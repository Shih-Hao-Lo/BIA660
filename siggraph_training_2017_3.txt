Peter Tieryas:Henry Garcia:Stacey Truman:Evan Bonifacio	In Pixar's Lou, a combination of lost and found items comes to life, multiple pieces assembling to create the eponymous character. There were many visual and technical challenges to creating a character that can take on almost any form, using many of the random objects around him to convey emotion and feeling. We've highlighted several of the ways animation, modeling, rigging, simulation, and shading worked in conjunction to develop artistic and technical solutions to make Lou feel as real as the world around him.	Bringing Lou to life: a study in creating Lou	NA:NA:NA:NA	2017
George Nguyen:Peter Tieryas:Jae Hyung Kim:Josh Holtsclaw	Cars 3's main antagonist is Jackson Storm, the first of a new breed of Next Gen racers. Lightning McQueen is definitely a classic, but Storm represents a sea change in many ways. Much of our specialized technical work on Storm reflects this progression to support his characterization on screen.	Revving up a storm: a talk on creating Jackson storm	NA:NA:NA:NA	2017
Kim Keech:Rachel Bibb:Brian Whited:Brett Achorn	The art direction of Moana called for the use of hand-drawn animation to be intimately mixed with the primarily CG film. This direction pushed us to develop new workflows in order to not only achieve the very specific look but also allow for the direct interaction between the various CG and hand-drawn elements. Exploring these workflows has further bridged the gap between hand-drawn animation and CG animation, opening the way for continued exploration into hybrid animation.	The role of hand-drawn animation in Disney's Moana	NA:NA:NA:NA	2017
Tomohiro Hasegawa	In this session, we will explain the issues that we set our sights on when creating monster art (for the Behemoth and Leviathan) given the advanced and sophisticated expressiveness that is now afforded by present-day consoles, as well as the opinions/initiatives we tackled in order to solve these issues.	A fantasy based on reality the art of Final Fantasy XV	NA	2017
Jens Jebens:Damien Gray:Simon Bull:Aidan Sarsfield	The demand for asset complexity has increased by several orders of magnitude since The LEGO Movie. This has resulted in the need for the team at Animal Logic to further develop their proprietary render and shading pipeline, while significantly optimising nearly all aspects of asset creation. Animal Logic's already extensive library of LEGO bricks was expanded considerably, and centralised for use across multiple shows and multiple locations. Continued development of asset creation tools, and significant increases in pipeline automation ensured increased review cycles, greater consistency and minimal duplication of effort.	Evolving complexity management on "the LEGO Batman movie"	NA:NA:NA:NA	2017
Hannes Ricklefs:Stefan Puschendorf:Sandilya Bhamidipati:Brian Eriksson:Akshay Pushparaja	VFX production companies are currently challenged by the increasing complexity of visual effects shots combined with constant schedule demands. The ability to execute in an efficient and cost-effective manner requires extensive coordination between different sites, different departments, and different artists. This coordination demands data-intensive analysis of VFX workflows beyond standard project management practices and existing tools. In this paper, we propose a novel solution centered around a general evaluation data model and APIs that convert production data (job/scene/shot/schedule/task) to business intelligence insights enabling performance analytics and generation of data summarization for process controlling. These analytics provide an impact measuring framework for analyzing performance over time, with the introduction of new production technologies, and across separate jobs. Finally, we show how the historical production data can be used to create predictive analytics for the accurate forecasting of future VFX production process performance.	From VFX project management to predictive forecasting	NA:NA:NA:NA:NA	2017
Dhruv Govil	In this talk, we describe the use of depth based compositing to accelerate collaboration between multiple artists on single shots. This workflow allows for increasingly complex shots to be finished in shorter timelines, even when dealing with large numbers of hero characters. SplitComp is a workflow for artists using depth compositing. It was originally developed for Cloudy With a Chance of Meatballs 2, but has since been used on every Sony Pictures Imageworks animated feature by animators and the cloth simulation team. It saves several hours of production time per artist every day of production. Over the course of a single feature film, it saves teams entire weeks worth of time. Older collaboration systems required artists to export their geometry so that it may be imported into other scene files, which is both time consuming and required artists to make a conscious effort. SplitComp, however, only requires playblasts which are a natural product of the artists workflow. This allows for significantly less effort from the artists and more frequent updates.	Animation collaboration with depth compositing	NA	2017
Daniel Heckenberg:Luke Emrose:Matthew Reid:Michael Balzer:Antoine Roille:Max Liani	The technical and creative challenges of The LEGO Batman Movie motivated many changes to rendering at Animal Logic. The project was the first feature animation to be entirely rendered with the studio's proprietary path-tracer, Glimpse. Brick-based modelling, animation and destruction techniques taken to the extents of Gotham City required extraordinary scalability and control. The desire to separate complexity from artistic intent led to the development of a novel material composition system. Lensing and lighting choices also drove technical development for efficient in-render lens distortion, depth-of-field effects and accelerated handling of thousands of city and interior lights.	Rendering the darkness: glimpse on the LEGO Batman movie	NA:NA:NA:NA:NA:NA	2017
Ciaran Moloney:Jamie Haydock:Mathew Puchala:Miguel Perez Senent	The Jedha sequence involved a colossal wave of destruction emanating from the epicenter of the Death Star attack on the planet Jedha. At ILM London, our task was to carefully plan the evolution of planet scale destruction from initial impact to final escape. Tectonic sized plates of rock, earth and sand had to rise up into the sky and form a wave 30,000 feet high. To achieve this we created a wide range of elements using well established ILM workflows. The scale of the scenes and simulations also meant that new workflows needed to be developed and the setups needed to be efficiently art directable.	Rogue One: A Star Wars Story - Jedha destruction	NA:NA:NA:NA	2017
Marc Bryant:Ian Coony:Jonathan Garcia	For Disney's Moana, the challenges presented by our story's fiery foe, Te Kā, required cross-departmental collaboration and the creation of new pipeline technology. From raging fires and erupting molten lava to churning pyroclastic plumes of steam and smoke, Te Kā was comprised of a large number of layered environmental elements. Effects artists composed heavily art-directed simulations alongside reusable effects assets. This hybrid approach allowed artists to quickly block in and visualize large portions of their shot prior to simulation or rendering. This Foundation Effects (or FFX) workflow became a core strategy for delivering Te Kā's complex effects across multiple sequences.	Moana: Foundation of a Lava Monster	NA:NA:NA	2017
Dong Joo Byun:Shant Ergenian:Gregory Culp	In the "Lair of Tamatoa" sequence of our latest movie Moana, we had 56 disco ball lighting effects shots. Our effects and lighting departments collaborated closely to create the bizarre and ludicrous environment of the scene. We developed a geometry-based lighting pipeline which allowed us to interactively design the light effects..	Moana: geometry based disco ball lighting for tamatoa's lair	NA:NA:NA	2017
Matt Ebb:Richard Sutherland:Daniel Heckenberg:Miles Green	Building the digital sets for Guardians of the Galaxy Vol. 2 presented a unique challenge for Animal Logic's asset and FX teams. The creative brief involved two separate alien environments made from complex mathematical shapes, with an unprecedented amount of detail. As an additional challenge the environments had to match the look and feel of specifically styled concept art with a grand, monumental design. To meet the artistic requirements required a high level of creative control and manipulation of set elements that were to be rendered alongside many other highly detailed objects, and propagated quickly through the pipeline for fast feedback iterations. The team used a novel approach to modelling fractal objects using point clouds, taking advantage of pipeline capabilities to integrate FX objects with environment set-pieces. Additionally the team, leveraged instancing and high levels of geometric complexity using the in-house renderer Glimpse.	Building detailed fractal sets for "Guardians of the Galaxy Vol. 2"	NA:NA:NA:NA	2017
Jim Malmros	In this technical post mortem we talk about how The Coalition implemented new custom graphics features and optimized the rendering technology to achieve the performance needed for the high visual bar for Gears Of War 41.	Gears of War 4: custom high-end graphics features and performance techniques	NA	2017
Colin Matisz:Andy Yi Shen	Gears of War 4 is one of the first titles to take full advantage of the High Dynamic Range (HDR) TV output of the Xbox One S. We developed techniques & technologies to do this, including using HDR reference photography, a modified tonemapper, lumen-based lighting, HDR sky materials, camera exposure ranges, post processes and a tuning environment for emissive surfaces and visual effects. We built a strong foundation of physically based materials and other shader techniques to create a wide variety of realistic surfaces that react beautifully to light. Our artists had the challenge of achieving the goals of art direction, while meeting the performance goals of 1080p30 in single player and 1080p60 in multiplayer gameplay. Using available lighting techniques in UE4 + custom tools, we implemented best practices to optimize our artist workflow resulting in stunning visuals.	HDR TV output and lighting Gears of War 4	NA:NA	2017
Prasert Prasertvithyakarn:Tatsuhiro Joudan:Hidekazu Kato:Seiji Nanase:Masayoshi Miyamoto:Isamu Hasegawa	In FINAL FANTASY XV, a triple-A open world RPG, we have proposed a new method of smart gameplay sharing by introducing a novel mechanism of automatic gameplay photograph generation. Unlike the classic screenshots that most players are familiar with, the photographs generated are depicted as though they were seen from the perspective of the in-game AI companion "Prompto". This system enhances the photos with several features such as shot facing, facial-body motion exaggeration, auto triggering, auto framing, auto focusing, auto post-filtering and auto album management. The system is capable of generating photographs that are stylish and unique, yet represent your gameplay in a new way no other games have accomplished before. With an in-game social network posting interface, generated photos can be easily shared. As a result, since the release of the game, our photos are flooding Facebook and Twitter, while creating a new benchmark to the world in the field of smart gameplay sharing.	Procedural photograph generation from actual gameplay: snapshot AI in FINAL FANTASY XV	NA:NA:NA:NA:NA:NA	2017
Kleber Garcia	Circular Separable Convolution Depth of Field (CSC DoF) is a mathematical adaptation and implementation of a separable circular filter, which utilizes complex plane phasors to create very accurate and fast bokeh. At its core, this technique convolves a circular pattern blur in the frequency domain using a horizontal and a vertical pass, representing the frame buffer using complex numbers. This technique renders at magnitudes faster than brute-force and sprite-based approaches, since it is a separable convolution. Important properties of this technique include convolution separability, low memory bandwidth and large radii circles. The technique has been shipped on Madden NFL 15, Madden NFL 16, Madden NFL 17, Fifa 17 and PGA Tour Rory McIlroy. The implementation includes an offline shader code generation step containing pre-computed frequency domain filters, multiple weighted passes for imaginary and real number processing. We will present the mathematical derivation and some caveats to achieve the required precision for intermediate frequency domain frame buffers.	Circular separable convolution depth of field	NA	2017
Daniel Heckenberg:Steve Agland:Jean Pascal leBlanc:Raphael Barth	We created an efficient pipeline for automated, HDR light probes for the hybrid live-action / animated feature film Peter Rabbit. A specially developed "360°" spherical camera allows on-set acquisition at more positions and in less time than traditional techniques. Reduced capture time, drastically simplified stitching and a custom multiple-exposure raw to HDR process minimizes artefacts in the resulting images. A semi-automated system recovers clipped radiance in direct sunlight using surfaces with known properties. By recording capture location and orientation and combining with other scene data we produce automated rendering setups using the light probes for illumination and projection onto 3d render geometry.	Automated light probes from capture to render for Peter Rabbit	NA:NA:NA:NA	2017
Lucio Moser:Darren Hendler:Doug Roble	We present Masquerade, a novel modular and expandable tool for adding fine-scale details to facial motion capture data from head-mounted cameras. After studying two important related works we developed a framework to reproduce the original approaches as well as to test equally promising alternatives. This framework has been vital for understanding the limitations of previous approaches and to explore ways to improve the results. Our final solution was a combination of algorithms and data representations that produced better results than previous works when tested with our evaluation data. Since then, Masquerade is being actively used in production for enhancing marker data with fine-scale details.	Masquerade: fine-scale details for head-mounted camera motion capture data	NA:NA:NA	2017
Andrew Feng:Evan Suma:Ari Shapiro	We demonstrate a system that can generate a photorealistic, interactive 3D character from a human subject that is capable of movement, emotion, speech and gesture in less than 20 minutes without the need for 3D artist intervention or specialized technical knowledge through a near automatic process. Our method uses mostly commodity- or of-the-shelf hardware. We demonstrate the just-in-time use of generating such 3D models for virtual and augmented reality, games, simulation and communication. We anticipate that the inexpensive generation of such photorealistic models will be useful in many venues where a just-in-time 3D construction of digital avatars that resemble particular human subjects is necessary. Figure 1 shows the overall workflow of our virtual character creation pipeline.	Just-in-time, viable, 3d avatars from scans	NA:NA:NA	2017
Adrien Kaiser:Jose Alonso Ybanez Zepeda:Tamy Boubekeur	Modern RGB-D sensors are widely used for indoor 3D capture, with applications ranging from modeling to robotics, through gaming. Nevertheless, their use is limited by their low resolution, with frames often corrupted with noise, missing data and temporal inconsistencies. In order to cope with all these issues, we present Proxy Clouds, a multiplanar superstructure for unified real-time processing of RGB-D data. By generating and updating through time a single set of rich statistics parameterized over planar proxies from raw RGB-D data, several processing primitives can be applied to improve the quality of the RGB-D stream on-the-fly or lighten further operations. We illustrate the use of Proxy Clouds on several applications, including noise and temporal flickering removal, hole filling, resampling, color processing and compression. We present experiments performed with our framework in indoor scenes of different natures captured with a consumer depth sensor.	Proxy clouds for RGB-D stream processing: an insight	NA:NA:NA	2017
Matthew Cong:Lana Lan:Ronald Fedkiw	For Kong: Skull Island, Industrial Light & Magic created an anatomically motivated facial simulation model for Kong that includes the facial skeleton and musculature. We applied a muscle simulation framework that allowed us to target facial shapes while maintaining desirable physical properties to ensure that the simulations stayed on-model. This allowed muscle simulations to be used as a powerful tool for adding physical detail to and improving the anatomical validity of both blendshapes and blendshape animations in order to achieve more realistic facial animation with less hand sculpting.	Muscle simulation for facial animation in Kong: Skull Island	NA:NA:NA	2017
David Bollo	In this talk, we present three of the techniques that we developed to deliver high performance animation in the Gears of War 41 video game. First, we present a "warp point"-driven system for dealing with character traversal through an irregular environment. This system builds on previous motion warping work and is over twice as fast as a more traditional blend space based approach. Next, we introduce a novel approach to handling motion transitions that eliminates traditional blended transitions and replaces them with an animation post-processing step that is 60% cheaper to compute overall. Finally, we introduce a fast but effective heuristic for improving the quality of these motion transitions by automatically matching the locomotion foot phase between the transitioning animations.	High performance animation in Gears of War 4	NA	2017
Gene S. Lee:Christian Eisenacher:Andy Lin:Noel Villegas	This paper presents a conservative, uniform method for handling scene constraints, such as look at and parent, in a pose-based caching system. The constraints are organized into a dependency graph where nodes represent the control caches of a rig, and directed arcs link the rigs to specific control caches. For any animation update, the dependency graph indicates which cache to clear and which other rigs to subsequently update. This method supports pre-evaluation, avoids expensive state tracking, and is easy to implement. The result is a seamless experience that works with all types of constraints while preserving real-time performance.	Handling scene constraints for pose-based caching	NA:NA:NA:NA	2017
Pilar Molina Lopez:Jake Richards	Eyes are often the most important feature in a character's performance, conveying emotion, timing and intention as well as hints about what comes next in the story. Stories are driven by characters and audience investment comes from their empathy for those characters. Unless the viewer is making a concerted effort to look elsewhere on screen, they usually concentrate on the eyes of the main character. Therefore, a great amount of effort and time is spent making the eyes of our characters look as expressive as possible. When done improperly, the eyes will make a character look dead and unappealing. Our technology utilized to create our characters' eyes gives artists the flexibility to push the boundaries of their craft; it helps them portray characters that communicate the emotions that a story requires. In order to achieve this we designed a set of techniques that compose our eye pipeline. It has been refined over many years in a continued effort and collaboration among several departments at our studio, from modeling to lighting passing through animation and rigging. It allows animators to follow their expressive style while also providing materials artists and lighters with the necessary input to achieve a realistic look.	The eyes have it: comprehensive eye control for animated characters	NA:NA	2017
Marc Thyng:Christopher Evart:Toby Jones:Aleka McAdams	Beginning with the early concept art, Moana featured characters with long curly hair interacting heavily with both the characters and their environment. This level of complexity in hair interactions and dynamics presented demanding simulation needs which led to changes throughout the hair simulation pipeline, from grooming to technical animation. In order to overcome these challenges we implemented a new hair model and data type, as well as overhauled how we handled hair collisions. We discuss the motivation and details of our hair simulation and technical animation process, as well as the implications of the new model both to artist interactions and our overall pipeline.	The art and technology of hair simulation in Disney's Moana	NA:NA:NA:NA	2017
Brian Missey:Amaury Aubel:Arunachalam Somasundaram:Megha Davalath	Hair plays a feature role in the film Trolls. It is a crucial part of the overall character design of the Trolls themselves, typically composing over half the silhouette of the character. However, the use of hair on the show went well beyond the standard coif and bled into acting beats, traditional effects, environments, and set pieces. This talk presents the wide variety of unique and challenging hair effects in the film and the techniques used to create them.	Hairy effects in Trolls	NA:NA:NA:NA	2017
Chloe LeGendre:Loc Hyunh:Shanhe Wang:Paul Debevec	We present a technique for modeling the vellus hair over the face based on observations of asperity scattering along a subject's silhouette. We photograph the backlit subject in profile and three-quarters views with a high-resolution DSLR camera to observe the vellus hair on the side and front of the face and separately acquire a 3D scan of the face geometry and texture. We render a library of backlit vellus hair patch samples with different geometric parameters such as density, orientation, and curvature, and we compute image statistics for each set of parameters. We trace the silhouette contour in each face image and straighten the backlit hair silhouettes using image resampling. We compute image statistics for each section of the facial silhouette and determine which set of hair modeling parameters best matches the statistics. We then generate a complete set of vellus hairs for the face by interpolating and extrapolating the matched parameters over the skin. We add the modeled vellus hairs to the 3D facial scan and generate renderings under novel lighting conditions, generally matching the appearance of real photographs.	Modeling vellus facial hair from asperity scattering silhouettes	NA:NA:NA:NA	2017
Elias Saliba:Mustafa Barkaoui:Hind Wakil	Lighthouse VFX production, and post production house is specialized in visual effects.	Behind the scenes of VFX in the Middle East & Syria: "in art we trust"	NA:NA:NA	2017
Sean Palmer:Jonathan Garcia:Sara Drakeley:Patrick Kelly:Ralf Habel	Disney's Moana was the largest and most complex water project the studio had ever undertaken. Over 900 shots required ocean interaction, which included boat wakes, splashes, shorelines, walls of water, and highly art-directed sentient water. Our previous films' water techniques would not scale to address the complexity and volume of work required by Moana and staffing and time constraints necessitated automating large parts of the process. We redesigned our pipeline to provide a flexible authoring process for a lightweight implicit ocean representation. This new workflow allowed artists to visualize and edit specific parts of the water setup and easily share their updates with other departments.	The ocean and water pipeline of Disney's Moana	NA:NA:NA:NA:NA	2017
Ben Frost:Alexey Stomakhin:Hiroaki Narita	For Disney's Moana, water was a dominant part of island life, in fact it had a life of itfis own. Presenting itself as a character, water was ever present, in a multitude of shapes and scales. An end-to-end water pipeline was developed for this film [Garcia et al. 2016], including the creation of proprietary fluid APIC solver [Jiang et al. 2015] named Splash. This gave us physically accurate simulations. The challenge with performing water was to provide art-directed simulations, defying physics, yet remaining in a grounded sense of possibility. Incorporating natural swells and flows to support the building of designed shapes limited anthropomorphic features, and played to our goal of communicating that this character is the ocean as a whole.	Moana: performing water	NA:NA:NA	2017
Rob Hopper:Kai Wolter	For Pirates of the Caribbean: Dead Men Tell no Tales MPC faced the creative challenge to produce highly believable ocean and water effects interacting with full-CG ships and characters. This included pirate ships emerging from the bottom of the sea, a parting ocean giving space to an enormous three dimensional set, and a model ship in a bottle containing a full-sized ocean. The varied scale and nature of these effects required us to rethink our simulation techniques and toolset. In this talk we present our approaches to animate, simulate and render these using our newly developed ocean toolkit and tighter integration of Autodesk Bifrost and SideFX Houdini into our FX and rendering pipeline.	The water effects of Pirates of the Caribbean: Dead Men Tell no Tales	NA:NA	2017
Stephen Marshall:Tim Speltz:Greg Gladstone:Krzysztof Rost:Jon Reisch	The world of Disney Pixar's Cars 3 finds our hero Lightning McQueen on a journey to reconnect to the roots of "real" racing as he struggles to stage a comeback in a sport which is quickly evolving past him. Over the course of the film, our characters race on beaches alongside lapping waves, in abandoned ghost tracks, through winding mountain forests, and even in a raucus, muddy demolition derby. Providing a sense of believable interaction between our characters and these varied environments in over 600 shots was one of the key responsibilities of our FX team on Cars 3. In order to achieve the scope and scale of this work efficiently, we built on sequence-wide workflows and independent "clustered" simulations presented last year in (Reisch et al. 2016), extending these strategies to effects unique to the show. Creating a common shared core to our simulation and effects-asset rigs provided artists with a familiar starting point regardless of whether they were working on volumetric dust, rigid-body debris, point-based dynamics sand, or even viscous mud simulations. A focus on stability, artist experience, and optimized workflows which scaled to take advantage of our render farm, allowed our team to achieve visually consistent, high quality results on an accelerated schedule.	Racing to the finish line: effects challenges on Cars 3	NA:NA:NA:NA:NA	2017
Alejandro Conty Estevez:Christopher Kulla	We present a technique to importance sample large collections of lights. A bounding volume hierarchy over all lights is traversed at each shading point using a single random number in a way that importance samples their predicted contribution. We further improve the performance of the algorithm by forcing splitting until the importance of a cluster is sufficiently representative of its contents.	Importance sampling of many lights with adaptive tree splitting	NA:NA	2017
Alexander Keller:Carsten Wächter:Matthias Raab:Daniel Seibert:Dietger van Antwerpen:Johann Korndörfer:Lutz Kettner	While ray tracing has become increasingly common and path tracing is well understood by now, a major challenge consists of crafting an easy-to-use and efficient system implementing these technologies. Following a purely physically-based paradigm while still allowing for artistic workflows, the Iray light transport simulation and rendering system allows for rendering complex scenes by the push of a button and thus makes accurate light transport simulation widely available. We discuss the challenges and implementation choices that follow from our primary design decisions, demonstrating that such a rendering system can be made a practical, scalable, and efficient real-world application that is in use by many industry professionals today.	The iray light transport simulation and rendering system	NA:NA:NA:NA:NA:NA:NA	2017
Beibei Wang:Nicolas Holzschuch	Illumination simulation involving participating media is computationally intensive. The overall aspect of the material depends on simulating a large number of scattering events inside the material. Combined, the contributions of these scattering events are a smooth illumination. Computing them using ray-tracing or photon-mapping algorithms is expensive: convergence time is high, and pictures before convergence are low quality (see Figure 1). In this paper, we precompute the result of multiple scattering events, assuming an infinite medium, and store it in two 4D tables. These precomputed tables can be used with many rendering algorithms, such as Virtual Ray Lights (VRL), Unified Point Beams and Paths (UPBP) or Manifold Exploration Metropolis Light Transport (MEMLT), greatly reducing the convergence time. The original algorithm takes care of low order scattering (single and double scattering), while our precomputations are used for multiple scattering (more than two scattering events).	Precomputed multiple scattering for light simulation in participating medium	NA:NA	2017
Norbert Bus:Tamy Boubekeur	We propose a novel representation of the light field tailored to improve importance sampling for Monte Carlo rendering. The domain of the light field i.e., the product space of spatial positions and directions is hierarchically subdivided into subsets on which local models characterize the light transport.The data structure is based on double trees, and only approximates the exact light field, but enables efficient queries for importance sampling and easy setup by tracing photons in the scene. The framework is simple yet flexible, supports any type of local model for representing the light field, provided it can be efficiently importance sampled, and progressive refinement with an arbitrary number of photons. Last, we provide a reference open source implementation of our method.	Double hierarchies for efficient sampling in Monte Carlo rendering	NA:NA	2017
Colin Penty:Ian Wong	We have created a new material system for Gears of War 4 inside Unreal Engine that allows artists to layer dozens of materials with complete material tuning control and flexibility - then cook out the results in-engine for efficient run-time performance.	Gears of War 4: creating a layered material system for 60fps	NA:NA	2017
Priyamvad Deshmukh:Feng Xie:Eric Tabellion	Since Shrek 2, DreamWorks artists have used the fabric model developed by [Glumac and Doepp 2004] extensively on cloth material shading. Even after we developed the physically based microcylinderical cloth model by [Sadeghi et al. 2013], they continued to prefer the intuitive control of the DreamWorks fabric shading model, which is also a cyindrical shading model, with easy to use artistic controls for highlights, and highlight directions.	DreamWorks fabric shading model: from artist friendly to physically plausible	NA:NA:NA	2017
Lutz Kettner	Using a term rewriting system, simplifications of physically-based materials described in a declarative programming language can be created automatically. Sets of rules for the term rewriting system allow for customizing simplifications according to use cases. Examples include automatic level-of-detail generation or simplification of materials for faster rendering in realtime viewports and games.	Fast automatic level of detail for physically-based materials	NA	2017
Yuxiao Du:Ergun Akleman	In this work, we have developed an approach to include any cross-hatching technique into any rendering system with global illumination effects (see Figure 1). Our new approach provide a robust computation to obtain hand-drawn effects for a wide variety of diffuse and specular materials. Our contributions can be summarized as follows: (1) A Barycentric shader that can provide generalized cross-hatching with multi-textures; and (2) A texture synthesis method that can automatically produce crosshatching textures from any given image.	Designing look-and-feel using generalized crosshatching	NA:NA	2017
Dong Joo Byun:Alexey Stomakhin	We used two different solutions for generating crashing waves for more than 40 shots in Moana. Our profile curve based wave deformer was developed and used for art-directed design of shapes, motion, and composition of running and crashing waves. In contrast to previously developed wave deformers, we designed a cross section shape animation by providing a series of profile curves which represented the animation keys. These profile curves could be hand plotted curves or mathematically calculated changing profiles, which means any kind of choreographic touch could be applied for designing the wave shapes. We could design multiple crashing waves for huge scale tsunami scenes and we could art direct the timing and composition of the waves which would fit well with the character animation and camera works. For scenarios demanding more realism, motion complexity and physical accuracy, we adopted a fully simulated approach. Our APIC-based fluid solver [Jiang et al. 2015] was equipped with control mechanisms allowing us to precisely choreograph the motion of breaking waves to the needs of a specific shot. Though more expensive than procedural approaches, this solution was much more preferable for "hero" shots with close up interaction with boats and characters.	Moana: crashing waves	NA:NA	2017
Gergely Klár:Jeff Budsberg:Matt Titus:Stephen Jones:Ken Museth	We present two complementary techniques for Material Point Method (MPM) based simulations to improve their performance and to allow for fine-grained artistic control. Our entirely GPU-based solver is able perform up to five times faster than its multithreaded CPU counterpart as a result of our novel particle and grid transfer algorithms. On top of this, we introduce Adaptive Particle Activation, that both makes it possible to simulate only a reduced number of particles, and to give artists means for fine direction over the simulation.	Production ready MPM simulations	NA:NA:NA:NA:NA	2017
Todd Keeler:Robert Bridson	We propose a novel method of compressing a fluid effect for realtime playback by using a compact mathematical representation of the spatio-temporal fluid surface. To create the surface representation we use as input a set of fluid meshes from standard techniques along with the simulation's surface velocity to construct a spatially adaptive and temporally coherent Lagrangian least-squares representation of the surface. We then compress the Lagrangian point data using a technique called Fourier extensions for further compression gains. The resulting surface is easily decompressed and amenable to being evaluated in parallel. We demonstrate real-time and interactive decompression and meshing of surfaces using a dual-contouring method that efficiently uses the decompressed particle data and least-squares representation to create a view dependent triangulation.	Compact iso-surface representation and compression for fluid phenomena	NA:NA	2017
Michael B. Nielsen:Konstantinos Stamatelos:Adrian Graham:Marcus Nordenstam:Robert Bridson	A guided liquid simulation [Nielsen and Bridson 2011] re-simulates a thin surface layer of an existing liquid simulation at higher resolution, or simulates just a thin layer near the surface of an animated input sequence produced e.g. by hand or by spectral wave methods. The movement of the simulated surface layer is guided by the underlying animation and this technique has been used to achieve high surface detail and art-directed water effects on several movies including "Hobbit - The Desolation of Smaug" and "Tintin - Secret of the Unicorn". Despite the successful application of guided simulations in production, the method as originally proposed requires a fair amount of manual setup time and can be computationally costly in scenarios where the area of focus (such as a moving ship) covers a large and non-regular area over time. In this talk we present an outline of a novel set of algorithms facilitating localized guided simulations, where the guided simulation takes place only within a - possibly animated - local region specified by the user. We have implemented our algorithms in Autodesk Maya's procedural Bifrost framework and integrated them with Bifrost's adaptive FLIP solver. We demonstrate with several examples the benefits of our approach in terms of computational efficiency and ease of use. Additionally our tool was utilized by Moving Picture Company (MPC) to create high resolution art-directed water simulations on Pirates of the Caribbean - Dead Men Tell No Tales.	Localized guided liquid simulations in bifrost	NA:NA:NA:NA:NA	2017
Chris Kramer	NA	Evolution of AR in Pokémon go	NA	2017
Kent Bye	NA	How VR changes the sense of ourselves & reality	NA	2017
Graham Roberts	NA	A new (virtual) reality at the New York Times	NA	2017
Mike Jutan:Steve Ellis	For Rogue One: A Star Wars Story, executive producer John Knoll wanted the all-CG shots to feel consistent with the signature handheld camera style that director Gareth Edwards captured on set. To achieve this, the Industrial Light & Magic (ILM) R&D team created a director-centric virtual camera system that encourages open set exploration of the all-CG Star Wars worlds. We enabled the director to achieve his artistic vision via our low footprint, flexible, iteration-based production toolset.	Director-centric virtual camera production tools for rogue one	NA:NA	2017
Vincent Serritella:David Lally:Brian Larsen:Farhez Rayani:Jason Kim:Matt Silas	In 2016, Pixar launched an internal, experimental storytelling initiative to enable new creative voices, as well as explore alternative storytelling techniques, pipelines, and workflows in production. Filmmakers are granted total creative freedom to develop a story, design a world, and produce a short film, within six months, and without any executive supervision. Smash and Grab is a seven minute short film that explores the use of comic book sketches, virtual production, motion/camera capture and procedural shading and lighting techniques. With the backdrop of a busy feature studio, limited resources, and a minimal crew, this talk is the story of our journey.	Smash and grab: off the rails filmmaking at Pixar	NA:NA:NA:NA:NA:NA	2017
Jeff Stringer:Owen Nelson:Tony Aiello	To manage the increased complexity in its hybrid stop-motion/CG animated features, LAIKA built a custom production scheduling system, converting the traditional tactile and laborious approach of planning a stop-motion shooting schedule on a big board into a fully digital, live and interactive experience connecting the entire crew.	LAIKA's digital big boards	NA:NA:NA	2017
Kenneth Vanhoey:Carlos Eduardo Porto de Oliveira:Hayko Riemenschneider:András Bódis-Szomorú:Santiago Manén:Danda Pani Paudel:Michael Gygli:Nikolay Kobyshev:Till Kroeger:Dengxin Dai:Luc Van Gool	VarCity - the Video is a short documentary-style CGI movie explaining the main outcomes of the 5-year Computer Vision research project VarCity. Besides a coarse overview of the research, we present the challenges that were faced in its production, induced by two factors: i) usage of imperfect research data produced by automatic algorithms, and ii) human factors, like federating researchers and a CG artist around a similar goal many had a different conception of, while no one had a detailed overview of all the content. Successive achievement was driven by some ad-hoc technical developments but more importantly of detailed and abundant communication and agreement on common best practices.	VarCity - the video: the struggles and triumphs of leveraging fundamental research results in a graphics video production	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2017
Andrzej Zarzycki:Martina Decker	Adaptive designs and intelligent spaces are in the forefront of the current architectural and product design discourse. They engage users in interactive dialogue, allow for public domain authoring, and are critical factors in sustainable designs where buildings monitor their own performance and respond to environmental factors or user needs (figure 1).	Programmable buildings: architecture as an interaction interface powered with programmable matter	NA:NA	2017
Jochen Suessmuth:Sky Asay:Conor Fitzgerald:Mario Poerner:Davoud Ohadi:Detlef Mueller	The aim of the adidas digital creation program is to build an industry leading system of 3-D tools that empowers creativity and connects creators digitally. Our pipeline is tailored for a product design and creation community that is non-technical. Our users benefit from better visualization and information for decision-making such as concept reviews or improved factory handovers. The brand benefits from more useful assets for sell-in, and consumers benefit from products that it ever changing trends. 3-D tools for product design that enable constant change through non-destructive behavior at a speed of workflow that is competitive with traditional 2-D methods haven't been available in the past. In this talk, we'll share how working side by side with our footwear designers, we were able to find novel workflows that inspire new tools and integrations.	Concept through creation: establishing a 3-D design process in the footwear industry	NA:NA:NA:NA:NA:NA	2017
Don Derek Haddad:Gershon Dublon:Brian Mayton:Spencer Russell:Xiao Xiao:Ken Perlin:Joseph A. Paradiso	The rise of ubiquitous sensing enables the harvesting of massive amounts of data from the physical world. This data is often used to drive the behavior of devices, and when presented to users, it is most commonly visualized quantitatively, as graphs and charts. Another approach for the representation of sensor network data presents the data within a rich, virtual environment. These scenes can be generated based on the physical environment, and their appearance can change based on the state of sensor nodes. By freely exploring these environments, users gain a vivid, multi-modal, and experiential perspective into large, multi-dimensional datasets. This paper presents the concept of "Resynthesizing Reality" through a case study we have created based on a network of environmental sensors deployed at a large-scale wetland restoration site. We describe the technical implementation of our system, present techniques to visualize sensor data within the virtual environment, and discuss potential applications for such Resynthesized Realities.	Resynthesizing reality: driving vivid virtual environments from sensor networks	NA:NA:NA:NA:NA:NA:NA	2017
Daniela Hasenbring:Jeremy Hoey	Sprout is our proprietary Maya-based tool for hand-dressing digital environments with large quantities of high-resolution assets like trees, plants and rocks. It was developed at Sony Picture Imageworks (SPI) to address the need for an interactive artist-friendly tool that was fully integrated into SPIfis existing pipeline. Prior to the development of Sprout, environment dressing at SPI was done primarily in Houdini or procedurally at render-time and was thus the province of FX TDs. In Sprout, artists can load any asset and quickly fipaintfi instances onto any other geometry using a brush paradigm familiar to anyone who has used Photoshop. Sophisticated lightweight OpenGL representations keep performance nimble, and all instances remain fully editable by the artist to allow for highly art-directed environment dressing. Sprout has made environment dressing at SPI available to a larger variety of artists, being leveraged most recently for photoreal jungle environments for an upcoming VFX motion picture as shown in Figure 1.	Interactive environment creation with sprout	NA:NA	2017
James Bartolozzi:Matt Kuruc	The curve hierarchy skeleton has been a foundational component of Pixar's vegetation pipeline since Cars 2 (2011). This skeleton is leveraged when building flow fields for texture synthesis, generating procedural secondary vegetation detail, and as a basis for simulation rigs. Our current skeletonization pipeline is built around mesh contraction [Shek et al. 2010] [Au et al. 2008] which is sensitive to the underlying topology. This method creates undesirable curve structures when modelers add musculature to the trunk mesh. Technical directors would reapply manual fixes to the skeleton over the course of iterating on the model. Recent work examining constructing skeletons using point clouds and volumetrics inspired us to develop a new hybrid approach. This alternative to the traditional mesh contraction algorithm has shown to be fast, reliable, accurate, and minimizes constraints on our modeling artists.	A hybrid approach to procedural tree skeletonization	NA:NA	2017
Wanho Choi:Nayoung Kim:Julie Jang:Sanghun Kim:Dohyun Yang	Although there is commercially available software for producing digital fur and feathers, creating photorealistic digital creatures under a low budget is still no trivial matter. Because no software could fulfill our purposes at the time of the making of our first movie Mr. Go, we decided to develop our own custom solution, ZelosFur [Choi et al. 2013]. While it made possible furry digital creature creation for subsequent projects, this prototypical system lacked the flexibility to easily add new features with backward compatibility and did not provide artists with enough freedom or control over the grooming process. Zelos Node Network (ZENN) is a new procedural solution that allows for quick, easy, and art-directable creation of all kinds of body coverings for digital creatures (e.g. fur, feathers, scales, etc.) By extension, it can also be used to create forests, rocks, and verdant landscapes for digital environments. In this talk, we discuss how to design and implement a procedural grooming workflow within ZENN and briefly address our caching and rendering process.	Build your own procedural grooming pipeline	NA:NA:NA:NA:NA	2017
Arunachalam Somasundaram	We present FurCollide, a fast, robust, and artist friendly tool used for collision detection and collision resolution of fur curves with meshes. The tool helps artists interact with and control tens of thousands of curves with ease while providing high fidelity realistic and/or artistic collision results. This tool is in use at DreamWorks Animation and has been used in a wide variety of fur and/or grass collision situations in various films.	FurCollide: fast, robust, and controllable fur collisions with meshes	NA	2017
Andy Rowan-Robinson	"Field Trip to Mars" is the first-ever headset-free group virtual reality vehicle experience. Taking the literal shape of a classic yellow school bus, the vehicle is home to an immersive virtual experience that transports school children to the surface of the Red Planet.	Field trip to Mars	NA	2017
Oculus	In many ways, Oculus Story Studio's VR experience, "Dear Angelica" as shown in Figure 1, is a unique project. We wanted to immerse the viewers inside a series of hand-drawn illustrations and tell a story by artfully transitioning between the drawings, and breathe life into these drawings by adding various animations and visual effects. In trying to create this ground-breaking experience, we had to create a brand-new pipeline: from inventing the tools to create these 3D illustrations, to processing and animating the drawings, to rendering them in virtual reality.	Dear angelica: breathing life into VR illustrations	NA	2017
Dave Mauriello:Jason Kirk:Jeremy Fernsler	Presented are two novel approaches to visualizing both internal and external anatomy of the heart through the cardiac cycle. The first uses "windows" manually cut through each chamber of a virtual heart model. The use of windows allows for internal and external views simultaneously while showing the varying thickness of the ventricular walls through the cardiac cycle. Internal structures such as both semilunar valves, both AV valves, chordae tendineae, and papillary muscles are kept intact and can be visualized in motion. The second approach is a rigging and control system that allows for independent rotation directions for the base, midpoint and apex of each ventricle both internally and externally, allowing for a more accurate wringing motion.	Two novel approaches to visualizing internal and external anatomy of the cardiac cycle with a windowed virtual heart model	NA:NA:NA	2017
Matthew Chambers:Justin Israel:Andy Wright	To ensure peak utilization of hardware resources, as well as handle the increasingly dynamic demands placed on its render farm infrastructure, WETA Digital developed custom queuing, scheduling, job description and submission systems - which work in concert to maximize the available cores across a large range of non-uniform task types. The render farm is one of the most important, high traffic components of a modern VFX pipeline. Beyond the hardware itself a render farm requires careful management and maintenance to ensure it is operating at peak efficiency. In WETAs case this hardware consists of a mix of over 80,000 CPU cores and a number of GPU resources, and as this has grown it has introduced many interesting scalability challenges. In this talk we aim to present our end-to-end solutions in the render farm space, from the structure of the resource and the inherent problems introduced at this scale, through the development of Plow - our management, queuing and monitoring software. Finally we will detail the deployment process and production benefits realized. Within each section we intend to present the scalability issues encountered, and detail our strategy, process and results in solving these problems. The ever increasing complexity and computational demands of modern VFX drives WETAs need to innovate in all areas, from surfacing, rendering and simulation but also to core pipeline infrastructure.	Large scale VFX pipelines	NA:NA:NA	2017
Daniel Bergel:Craig Dibble:Pauline Koh:James Pearson:Hannes Ricklefs	Disney's The Jungle Book required MPC to deliver work of an unprecedented visual complexity and quality. To enable Disney to fully realise their creative vision, MPC wanted to ensure it had burst compute capacity available through flexible and scalable cloud based resources. The major technical challenge was to provide this burst capacity whilst meeting the strict security requirements of our client, something which had not previously been achieved for a production of this scale or sensitivity. The project needed dedicated resources across Technology, Operations and Production to holistically capture and address everyone's requirements and process constraints. Across all these domains the project was considered a huge success. This talk presents the key challenges faced including a technical overview of the architecture, the essential management tools, and the interaction with production from identifying appropriate job types to effective utilisation of these virtual resources.	Cloudy with a chance of rendering	NA:NA:NA:NA:NA	2017
Mungo Pay:Damien Maupu:Martin Pražák	The complexity of crowd shots can vary greatly, from simple vignetting tasks that add life to an environment, to large and complex battle sequences involving thousands of characters. For this reason, a "one size fits all" crowd solution might not be optimal, both in terms of design and usability, but also allocation of crew. In this talk we present a suite of tools, developed across multiple platforms, each optimised for specific crowd tasks. These are underpinned by a data interchange library to allow for modification at any stage of the pipeline.	Flexible pipeline for crowd production	NA:NA:NA	2017
Ton Roosendaal:Francesco Siddi	For "Cosmos Laundromat" - CAF 2016 Jury Award winner - the Blender team, headed by CG pioneer and producer Ton Roosendaal, developed and used a complete open source creation pipeline. The team released several other shorts since then, including a 360-degrees VR experience and a pitch for the feature animation film "Agent 327". Developing and sharing open source technologies is a great challenge, and leads to great benefits for the small and medium animation studios.	Beyond "cosmos laundromat": blender's open source studio pipeline	NA:NA	2017
Dominik P. Käser:Evan Parker:Adam Glazier:Mike Podwal:Matt Seegmiller:Chun-Po Wang:Per Karlsson:Nadav Ashkenazi:Joanna Kim:Andre Le:Matthias Bühlmann:Joshua Moshier	One of the great promises of virtual reality is that it can allow people to visit places in the world that they might otherwise be unable to. Since the recent renaissance of virtual reality, content creators have exercised various techniques such as 360-degree cameras and photogrammetry to make this promise come true. At Google, we spent more than 10 years capturing every part of the world as part of the Google Earth project. The result is a rich 3D mesh that contains trillions of triangles [Kontkanen and Parker 2014] and as such is predestined to be a good data source for VR content. In [Kaeser and Buehlmann 2016] we discussed some of our early experiments with bringing Google Earth to virtual reality, but without a focus on developing a product. Following these experiments, we worked extensively to create a well-rounded product, Google Earth VR, which we eventually launched to the world in November 2016. Google Earth VR quickly became one of the most actively used VR applications in the market and has won several awards since. This talk discusses the journey of the Google Earth VR project from its early prototypes to its final launched stage.	The making of Google earth VR	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2017
Bruna Berford:Carlos Diaz-Padron:Terry Kaleas:Irem Oz:Devon Penney	Penrose Studios is an animation studio that produces content for VR and AR devices. There are a multitude of fascinating new artistic challenges facing animators and directors creating this type of content, such as animating without a fixed camera and very few cuts. Studios currently leverage existing animation and production tools to create content rendered in real time. However, this new medium is hampered by traditional processes that greatly reduce artist productivity and creativity. For instance, animating in traditional third party tools and importing into VR often leads to unexpected results, which results in a suboptimal iteration loop. We present a variety of tools that improve the traditional animation pipeline to address these issues and reduce the frequency of context switches between virtual reality and 2D monitors. The result is a toolset where artists feel empowered to explore animation in this new art form without being crippled by inefficient workflows.	Building an animation pipeline for VR stories	NA:NA:NA:NA:NA	2017
Chris Healer	Chris discusses the VR creation process using examples ranging from Hollywood VR projects to those from The New York Times. He'll dive into live-action VR photography and photogrammetric reality capture techniques, and will explore the linkage between tried and true VFX trickery and the illusion of a Virtual Reality. From this presentation audience members will take away a greater understanding of the VR Post Production process, especially in relation to VFX workflows. He'll use recent recognizable VR projects as examples, including "Lincoln in the Bardo" from The New York Times, Doug Liman's "Invisible," and others.	Visual effects for VR	NA	2017
Brett Achorn:Sean Palmer:Larry Wu	In Walt Disney Animation Studio's Moana, a key moment in the story calls for the protagonists to confront a floating island barge crewed by coconut-clad pirates: the Kakamora. Combining a large moving set on an animated ocean with active crowds, sails, ropes and effects simulations required coordination across multiple departments and some on-the-fly innovations. To handle the challenges the sequence presented, we made extensive use of automation and procedural tools along with some creative design decisions to save artist time and allow for greater reuse.	Building moana's kakamora barge	NA:NA:NA	2017
Melt van der Spuy	On Storks, we were asked to design and animate complex structures composed of hundreds of interacting characters. This lead us to develop PackIT - an artist focused approach to efficiently deal with large numbers of characters. Our approach is in contrast to other methods which often rely on simulation or group behaviors. We treat each character individually and efficiently manage the conversion between rigs and geometry caches, putting the control back with the artist. Using the extensive library of in-house building blocks we reduce development time and impact on the pipeline. As a result of the huge performance gains and reduced scene load times, a single artists can easily complete shots involving hundreds of characters.	PackIT: animating complicated character groups easily	NA	2017
Greg Mourino:Mason Evans:Kevin Edzenga:Svetla Cavaleri:Mark Adams:Justin Bisceglio	With the help of new tools, we streamlined our review and render processes for crowds to triple our shot count on our latest show, Ferdinand. At the same time, we integrated some novel approaches to complex deformation features for cloth and facial animation, which elevated the quality of our crowd animations.	Populating the crowds in Ferdinand	NA:NA:NA:NA:NA:NA	2017
Damien Maupu:Emanuele Goffredo:Nile Hylton:Mungo Pay:Martin Pražák	While crowd simulation frameworks can be very powerful for virtual crowd generation, in a VFX context they can also be unwieldy due to their chaotic nature. Small changes on the inputs can produce markedly different results, which can be problematic when attempting to adhere to a director's vision. Artist driven tools allow much more flexibility when constructing scenes, speed up turn-around time and can produce extremely dynamic crowd shots. To generate virtual crowds, Double Negative VFX (Dneg) has recently transitioned from an in-house standalone simulation-based solution to an artist-driven framework integrated into SideFX's Houdini.	Artist-driven crowd authoring tools	NA:NA:NA:NA:NA	2017
Curtis Andrus:Endre Balint:Chong Deng:Simon Coupe	In The Mummy, much of MPC's work involved augmenting the Ahmanet character with various CG elements. This includes, eye splitting, runes, rotten/torn skin, etc. See Figure 1 for an example. These elements needed to be added on top of a live performance, so tracking a 3D model to Ahamanet's face was necessary. Doing this sort of work isn't uncommon, but with the high volume of shots MPC did for this show, it was clear that some new tools would be necessary to help simplify this process.	Optical flow-based face tracking in The Mummy	NA:NA:NA:NA	2017
Andreas Bauer	Ray-traced contours are inherently challenged by highly detailed geometry, as commonly found in organic shapes. Existing contour methods cannot reflect such complexity in an artistically pleasing way (Figure 1 A), and animations are prone to flicker. After a brief explanation of contour generation and its inherent challenges, this talk presents a novel approach to rendering aesthetic and flicker-free contours on highly detailed geometry (Figure 1 B). The new method employs sub-pixel-level sub-sampling to achieve a high level of detail quality, and supports contours in transparency, reflection and refraction. The implementation uses mental ray (Unified Sampling mode) but could be realized in other ray tracing renderers as well.	A new contour method for highly detailed geometry	NA	2017
Xinling Chen:Christopher Kulla:Lucas Miller:Alan Chen	In Smurfs: The Lost Village, the enchanted forest is central to the environment and story. The task was to create millions of exotic plants of hundreds of different species spreading across the forest. The plants in the forest emit light when in the shade, and stop when they are hit by sunlight. We revisited the way in which our lighting tool creates lights to scale to huge light counts without compromising on the ability to fine tune lighting through features like light linking. This lead to simplified workflows for the artists who were able to easily manipulate shots with up to several millions of individual lights.	Lighting up the smurfs enchanted forest	NA:NA:NA:NA	2017
Ken Dahm:Alexander Keller	We introduce a rendering algorithm that is as simple as a path tracer but dramatically improves light transport simulation and even outperforms the Metropolis light transport algorithm. The underlying method of importance sampling learns where radiance is coming from and in fact coincides with reinforcement learning. The cost for the improvement is a data structure similar to irradiance volumes as used in realtime games.	Learning light transport the reinforced way	NA:NA	2017
Ken Museth	We present a new efficient algorithm for computing signed distance fields by means of the Fast Sweeping Method. Unlike existing algorithms ours is explicitly designed to explore the benefits of sparse (vs dense) grids as well as concurrency, i.e. mutli-threading.	Novel algorithm for sparse and parallel fast sweeping: efficient computation of sparse signed distance fields	NA	2017
Ran Dong:Dongsheng Cai:Nobuyoshi Asai	Human motions (especially, dance motions) are very noisy and it is difficult to analyze the motions. To resolve this problem, we propose a new method to decompose and edit the motions using the Hilbert-Huang transform (HHT). The HHT decomposes a chromatic signal into "monochromatic" signals that are the so-called Intrinsic Mode Functions (IMFs) using an Empirical Mode Decomposition (EMD)[Huang 2014]. The HHT has the advantage to analyze non-stationary and nonlinear signals like human joint motions over the FFT or Wavelet transform. In the present research, we propose a new framework to analyze a famous Japanese threesome pop singer group "Perfume". Then using the NA-MEMD, we decompose dance motions into motion (choreographic) primitives or IMFs, which can be scaled, combined, subtracted, exchanged, and modified self-consistently.	Dance motion analysis and editing using hilbert-huang transform	NA:NA:NA	2017
Danil Nagy	This talk will describe a recent collaboration between our group and the aircraft manufacturer Airbus for the design of a new aerospace component which uses cutting-edge design and fabrication techniques to radically reduce the weight of the component while maintaining the same structural performance. To achieve these results, we developed a novel computational geometry system which combines a bottom-up growth strategy based on slime mold behavior from nature, with a top-down genetic algorithm framework for optimizing the final design.	Nature-based hybrid computational geometry system for optimizing the interior structure of aerospace components	NA	2017
Nitish Padmanaban:Robert Konrad:Emily A. Cooper:Gordon Wetzstein	Personal computing devices have evolved steadily, from desktops to mobile devices, and now to emerging trends in wearable computing. Wearables are expected to be integral to consumer electronics, with the primary mode of interaction often being a near-eye display. However, current-generation near-eye displays are unable to provide fully natural focus cues for all users, which often leads to discomfort. This core limitation is due to the optics of the systems themselves, with current displays being unable to change focus as required by natural vision. Furthermore, the form factor often makes it difficult for users to wear corrective eyewear. With two prototype near-eye displays, we address these issues using display modes that adapt to the user via computational optics. These prototypes make use of focus-tunable lenses, mechanically actuated displays, and gaze tracking technology to correct common refractive errors per user, and provide natural focus cues by dynamically updating scene depth based on where a user looks. Recent advances in computational optics hint at a future in which some users experience better vision in the virtual world than in the real one.	Optimizing VR for all users through adaptive focus displays	NA:NA:NA:NA	2017
Nathan Matsuda:Alexander Fix:Douglas Lanman	Optimization-based design of optical systems can yield configurations that would be impractical to achieve with manual parameter adjustment. Nonetheless, most approaches are geared toward one-time, offline generation of static configurations to be fabricated physically. Recently, challenging computational imaging problems, such as seeing around corners or through scattering media, have utilized dynamically addressable optical elements to probe scene light transport. A new class of optimization techniques targeted at these dynamic applications has emerged in which stochastic raytracing replaces the fixed operators applied with conventional optimization methods. By modeling optical systems as raytracing operators, more complex non-linear phenomena and larger problem sizes can be considered. We introduce a simple raytracing-in-the-loop optimization model for a head-mounted display (HMD) containing a spatial light modulator (SLM). Using this approach, we are able to compute color images to be displayed in concert with spatially varying SLM phase maps at a resolution that would otherwise be computationally in-feasible. We also consider extensions of this model that may further enhance the performance of the target system.	A case study on raytracing-in-the-loop optimization: focal surface displays	NA:NA:NA	2017
Konrad Tollmar:Pietro Lungaro:Alfredo Fanghella Valero:Ashutosh Mittal	Smart Eye-tracking Enabled Networking (SEEN) is a novel end-to-end framework using real-time eye-gaze information beyond state-of-the-art solutions. Our approach can effectively combine the computational savings of foveal rendering with the bandwidth savings required to enable future mobile VR content provision.	Beyond foveal rendering: smart eye-tracking enabled networking (SEEN)	NA:NA:NA:NA	2017
