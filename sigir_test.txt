Jinfeng Rao:Ferhan Ture:Jimmy Lin	A recently-introduced product of Comcast, a large cable company in the United States, is a "voice remote" that accepts spoken queries from viewers. We present an analysis of a large query log from this service to answer the question: "What do viewers say to their TVs?" In addition to a descriptive characterization of queries and sessions, we describe two complementary types of analyses to support query understanding. First, we propose a domain-specific intent taxonomy to characterize viewer behavior: as expected, most intents revolve around watching programs---both direct navigation as well as browsing---but there is a non-trivial fraction of non-viewing intents as well. Second, we propose a domain-specific tagging scheme for labeling query tokens, that when combined with intent and program prediction, provides a multi-faceted approach to understand voice queries directed at entertainment systems.	What Do Viewers Say to Their TVs?: An Analysis of Voice Queries to Entertainment Systems	NA:NA:NA	2018
Vikas Yadav:Rebecca Sharp:Mihai Surdeanu	While increasingly complex approaches to question answering (QA) have been proposed, the true gain of these systems, particularly with respect to their expensive training requirements, can be in- flated when they are not compared to adequate baselines. Here we propose an unsupervised, simple, and fast alignment and informa- tion retrieval baseline that incorporates two novel contributions: a one-to-many alignment between query and document terms and negative alignment as a proxy for discriminative information. Our approach not only outperforms all conventional baselines as well as many supervised recurrent neural networks, but also approaches the state of the art for supervised systems on three QA datasets. With only three hyperparameters, we achieve 47% [email protected] on an 8th grade Science QA dataset, 32.9% [email protected] on a Yahoo! answers QA dataset and 64% MAP on WikiQA.	Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering	NA:NA:NA	2018
Myungha Jang:James Allan	In an era in which new controversies rapidly emerge and evolve on social media, navigating social media platforms to learn about a new controversy can be an overwhelming task. In this light, there has been significant work that studies how to identify and measure controversy online. However, we currently lack a tool for effectively understanding controversy in social media. For example, users have to manually examine postings to find the arguments of conflicting stances that make up the controversy. In this paper, we study methods to generate a stance-aware summary that explains a given controversy by collecting arguments of two conflicting stances. We focus on Twitter and treat the stance summarization as a ranking problem of finding the top k tweets that best summarize the two conflicting stances of a controversial topic. We formalize the characteristics of a good stance summary and propose a ranking model accordingly. We first evaluate our methods on five controversial topics on Twitter. Our user study shows that our methods consistently outperform other baseline techniques in generating a summary that explains the given controversy.	Explaining Controversy on Social Media via Stance Summarization	NA:NA	2018
Vaibhav Kumar:Dhruv Khattar:Siddhartha Gairola:Yash Kumar Lal:Vasudeva Varma	Online media outlets, in a bid to expand their reach and subsequently increase revenue through ad monetisation, have begun adopting clickbait techniques to lure readers to click on articles. The article fails to fulfill the promise made by the headline. Traditional methods for clickbait detection have relied heavily on feature engineering which, in turn, is dependent on the dataset it is built for. The application of neural networks for this task has only been explored partially. We propose a novel approach considering all information found in a social media post. We train a bidirectional LSTM with an attention mechanism to learn the extent to which a word contributes to the post's clickbait score in a differential manner. We also employ a Siamese net to capture the similarity between source and target information. Information gleaned from images has not been considered in previous approaches. We learn image embeddings from large amounts of data using Convolutional Neural Networks to add another layer of complexity to our model. Finally, we concatenate the outputs from the three separate components, serving it as input to a fully connected layer. We conduct experiments over a test corpus of 19538 social media posts, attaining an F1 score of 65.37% on the dataset bettering the previous state-of-the-art, as well as other proposed approaches, feature engineering or otherwise.	Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks	NA:NA:NA:NA:NA	2018
Penghui Wei:Junjie Lin:Wenji Mao	Stance detection aims at inferring from text whether the author is in favor of, against, or neutral towards a target entity. Most of the existing studies consider different target entities separately. However, in many scenarios, stance targets are closely related, such as several candidates in a general election and different brands of the same product. Multi-target stance detection, in contrast, aims at jointly detecting stances towards multiple related targets. As stance expression regarding a target can provide additional information to help identify the stances towards other related targets, modeling expressions regarding multiple targets jointly is beneficial for improving the overall performance compared to single-target scheme. In this paper, we propose a dynamic memory-augmented network DMAN for multi-target stance detection. DMAN utilizes a shared external memory, which is dynamically updated through the learning process, to capture and store stance-indicative information for multiple related targets. It then jointly predicts stances towards these targets in a multitask manner. Experimental results show the effectiveness of our DMAN model.	Multi-Target Stance Detection via a Dynamic Memory-Augmented Network	NA:NA:NA	2018
Stefano Mizzaro:Josiane Mothe:Kevin Roitero:Md Zia Ullah	Some methods have been developed for automatic effectiveness evaluation without relevance judgments. We propose to use those methods, and their combination based on a machine learning approach, for query performance prediction. Moreover, since predicting average precision as it is usually done in query performance prediction literature is sensitive to the reference system that is chosen, we focus on predicting the average of average precision values over several systems. Results of an extensive experimental evaluation on ten TREC collections show that our proposed methods outperform state-of-the-art query performance predictors.	Query Performance Prediction and Effectiveness Evaluation Without Relevance Judgments: Two Sides of the Same Coin	NA:NA:NA:NA	2018
Fanghong Jian:Jimmy Xiangji Huang:Jiashu Zhao:Tingting He	In probabilistic BM25, term frequency normalization is one of the key components. It is often controlled by parameters $k_1$ and b , which need to be optimized for each given data set. In this paper, we assume and show empirically that term frequency normalization should be specific with query length in order to optimize retrieval performance. Following this intuition, we first propose a new term frequency normalization with query length for probabilistic information retrieval, namely \textttBM25\tiny QL . Then \textttBM25\tiny QL is incorporated into the state-of-the-art models CRTER riptsize 2 and LDA-BM25, denoted as $\textttCRTER riptsize 2 ^\texttt\tiny QL $ and \textttLDA-BM25\tiny QL respectively. A series of experiments show that our proposed approaches \textttBM25\tiny QL , $\textttCRTER riptsize 2 ^\texttt\tiny QL $ and \textttLDA-BM25\tiny QL are comparable to BM25, CRTER riptsize 2 and LDA-BM25 with the optimal b setting in terms of MAP on all the data sets.	A New Term Frequency Normalization Model for Probabilistic Information Retrieval	NA:NA:NA:NA	2018
Alexander Moore:Vanessa Murdock:Yaxiong Cai:Kristine Jones	Every day more technologies and services are backed by complex machine-learned models, consuming large amounts of data to provide a myriad of useful services. While users are willing to provide personal data to enable these services, their trust in and engagement with the systems could be improved by providing insight into how the machine learned decisions were made. Complex ML systems are highly effective but many of them are black boxes and give no insight into how they make the choices they make. Moreover, those that do often do so at the model-level rather than the instance-level. In this work we present a method for deriving explanations for instance-level decisions in tree ensembles. As this family of models accounts for a large portion of industrial machine learning, this work opens up the possibility for transparent models at scale.	Transparent Tree Ensembles	NA:NA:NA:NA	2018
Parikshit Sondhi:Mohit Sharma:Pranam Kolari:ChengXiang Zhai	Understanding the search tasks and search behavior of users is necessary for optimizing search engine results. While much work has been done on understanding the users in Web search, little knowledge is available about the search tasks and behavior of users in the E-Commerce (E-Com) search applications. In this paper, we share the first empirical study of the queries and search behavior of users in E-Com search by analyzing search log from a major E-Com search engine. The analysis results show that E-Com queries can be categorized into five categories, each with distinctive search behaviors: (1) Shallow Exploration Queries are short vague queries that a user may use initially in exploring the product space. (2) Targeted Purchase Queries are queries used by users to purchase items that they are generally familiar with, thus without much decision making. (3) Major-Item Shopping Queries are used by users to shop for a major item which is often relatively expensive and thus requires some serious exploration, but typically in a limited scope of choices. (4) Minor-Item Shopping Queries are used by users to shop for minor items that are generally not very expensive, but still require some exploration of choices. (5) Hard-Choice Shopping Queries are used by users who want to deeply explore all the candidate products before finalizing the choice often appropriate when multiple products must be carefully compared with each other. These five categories form a taxonomy for E-Com queries and can shed light on how we may develop customized search technologies for each type of search queries to improve search engine utility.	A Taxonomy of Queries for E-commerce Search	NA:NA:NA:NA	2018
Ali Montazeralghaem:Hamed Zamani:Azadeh Shakery	Axiomatic analysis is a well-defined theoretical framework for analytical evaluation of information retrieval models. The current studies in axiomatic analysis implicitly assume that the constraints (axioms) are independent. In this paper, we revisit this assumption and hypothesize that there might be interdependence relationships between the existing constraints. As a preliminary study, we focus on the pseudo-relevance feedback (PRF) models that have been theoretically studied using the axiomatic analysis approach. In this paper, we introduce two novel interdependent PRF constraints which emphasize on the effect of existing constraints on each other. We further modify two state-of-the-art PRF models, log-logistic and relevance models, in order to satisfy the proposed constraints. Experiments on three TREC newswire and web collections demonstrate that the proposed modifications significantly outperform the baselines, in all cases.	Theoretical Analysis of Interdependent Constraints in Pseudo-Relevance Feedback	NA:NA:NA	2018
Robert Litschko:Goran Glavaš:Simone Paolo Ponzetto:Ivan Vulić	We propose a fully unsupervised framework for ad-hoc cross-lingual information retrieval (CLIR) which requires no bilingual data at all. The framework leverages shared cross-lingual word embedding spaces in which terms, queries, and documents can be represented, irrespective of their actual language. The shared embedding spaces are induced solely on the basis of monolingual corpora in two languages through an iterative process based on adversarial neural networks. Our experiments on the standard CLEF CLIR collections for three language pairs of varying degrees of language similarity (English-Dutch/Italian/Finnish) demonstrate the usefulness of the proposed fully unsupervised approach. Our CLIR models with unsupervised cross-lingual embeddings outperform baselines that utilize cross-lingual embeddings induced relying on word-level and document-level alignments. We then demonstrate that further improvements can be achieved by unsupervised ensemble CLIR models. We believe that the proposed framework is the first step towards development of effective CLIR models for language pairs and domains where parallel data are scarce or non-existent.	Unsupervised Cross-Lingual Information Retrieval Using Monolingual Data Only	NA:NA:NA:NA	2018
Johannes Kiesel:Arefeh Bahrami:Benno Stein:Avishek Anand:Matthias Hagen	Query suggestions are a standard means to clarify the intent of underspecified queries. In a voice-based search setting, the compilation of query suggestions is not straightforward, and user-centric research targeting query underspecification is lacking so far. Our paper analyses a specific type of ambiguous voice queries and studies the impact of various kinds of voice query clarifications offered by the system and its impact on user satisfaction. We conduct a user study that measures the satisfaction for clarifications that are explicitly invoked and presented by seven different methods. Our findings include that (1) user experience depends on language proficiency levels, (2) users are not dissatisfied when prompted for clarifications (in fact, enjoy it sometimes), and (3) the most effective way of query clarification depends on the number and lengths of the possible answers.	Toward Voice Query Clarification	NA:NA:NA:NA:NA	2018
Daniel Locke:Guido Zuccon	Test collection based evaluation represents the standard of evalua- tion for information retrieval systems. Legal IR, more speci cally case law retrieval, has no such standard test collection for evalua- tion. In this paper, we present a test collection for use in evaluating case law search, being the retrieval of judicial decisions relevant to a particular legal question. The collection is made available at ielab.io/caselaw.	A Test Collection for Evaluating Legal Case Law Search	NA:NA	2018
Sindunuraga Rikarno Putra:Felipe Moraes:Claudia Hauff	Collaborative search has been an active area of research within the IR community for many years. While for "single-user'' research a variety of up-to-date open-source search systems exist, few "multi-user'' search tools are open-source and even fewer are being maintained. In this paper, we present SearchX, an open-source collaborative search system we are currently developing-and using for our research. We designed and built SearchX using the modern Web stack (and are thus not siloed by an operating system or a particular browser type), enabling efficient research across platforms (Desktop, mobile) and with online users (e.g. crowdworkers). A video, describing the demo can be found at https: //www.youtube.com/watch?v=uf24m6p3vts.	SearchX: Empowering Collaborative Search Research	NA:NA:NA	2018
Mónica Marrero:Claudia Hauff	In order to improve long-term retention, ad conversion rates, and so on, A/B testing has become the norm within Web portals, enabling efficient large-scale experimentation. While A/B testing is also increasingly used by academic researchers (with crowd-working platforms offering a large pool of artificial users), few platforms are freely available to this end. Academic researchers usually develop adhoc solutions, leading to many duplicated efforts and time spent on work not directly related to one's research. As an alternative, we have developed and open sourced APONE, an A cademic P latform for ON line Experiments. APONE uses PlanOut, a framework and high-level language, to specify online experiments, and offers Web services and a Web GUI to easily create, manage and monitor them. By building a user friendly Web application, we enable not only experts to conduct valid A/B experiments. In particular as a secondary use case, we envision large classrooms to also benefit from the deployment of APONE, a vision we put into practice in a graduate Information Retrieval course. We open-source APONE at https://marrerom.github.io/APONE. A demo version is running at http://ireplatform.ewi.tudelft.nl:8080/APONE.	A/B Testing with APONE	NA:NA	2018
Minh C. Phan:Aixin Sun	We present CoNEREL, a system for collective named entity recognition and entity linking focusing on news articles and readers' comments. Different from other systems, CoNEREL processes articles and comments in batch mode, to make the best use of the shared contexts of multiple news stories and their comments. Particularly, a news article provides context for all its comments. To improve named entity recognition, CoNEREL utilizes co-reference of mentions to refine their class labels ( e.g. , person, location). To link the recognized entities to Wikipedia, our system implements Pair-Linking, a state-of-the-art entity linking algorithm. Furthermore, CoNEREL provides an interactive visualization of the Pair-Linking process. From the visualization, one can understand how Pair-Linking achieves decent linking performance through iterative evidence building, while being extremely fast and efficient. The graph formed by the Pair-Linking process naturally becomes a good summary of entity relations, making CoNEREL a useful tool to study the relationships between the entities mentioned in an article, as well as the ones that are discussed in its comments.	CoNEREL: Collective Information Extraction in News Articles	NA:NA	2018
Sarvnaz Karimi:Vincent Nguyen:Falk Scholer:Brian Jin:Sara Falamaki	Clinical Decision Support (CDS) systems aim to assist clinicians in their daily decision-making related to diagnosis, tests, and treatments of patients by providing relevant evidence from the scientific literature. This promise however is yet to be fulfilled, with search for relevant literature for a given patient condition still being an active research topic. The TREC CDS track was designed to address this research gap. We developed a platform to facilitate experimentation and hypothesis testing for information retrieval researchers working on this topic. It provides a large range of query and document processing techniques that are explored in the biomedical search domain.	A2A: Benchmark Your Clinical Decision Support Search	NA:NA:NA:NA:NA	2018
Harrisen Scells:Daniel Locke:Guido Zuccon	We present a framework for constructing and executing information retrieval experiment pipelines. The framework as a whole is built primarily for domain specific applications such as medical literature search for systematic reviews, or finding factually or legally applicable case law in the legal domain; however it can also be used for more general tasks. There are a number of pre-implemented components that enable common information retrieval experiments such as ad-hoc retrieval or query analysis through query performance predictors. In addition, this collection of tools seeks to be user friendly, well documented, and easily extendible. Finally, the entire pipeline can be distributed as a single binary with no dependencies, ready to use with a simple domain specific language (DSL) for constructing pipelines.	An Information Retrieval Experiment Framework for Domain Specific Applications	NA:NA:NA	2018
Jeffrey Dalton:Victor Ajayi:Richard Main	Conversational search and recommendation systems that use natural language interfaces are an increasingly important area raising a number of research and interface design questions. Despite the increasing popularity of digital personal assistants, the number of conversational recommendation systems is limited and their functionality basic. In this demonstration we introduce Vote Goat, a conversational recommendation agent built using Google's DialogFlow framework. The demonstration provides an interactive movie recommendation system using a speech-based natural language interface. The main intents span search and recommendation tasks including: rating movies, receiving recommendations, retrieval over movie metadata, and viewing crowdsourced statistics. Vote Goat uses gamification to incentivize movie voting interactions with the 'Greatest Of All Time' (GOAT) movies derived from user ratings. The demo includes important functionality for research applications with logging of interactions for building test collections as well as A/B testing to allow researchers to experiment with system parameters.	Vote Goat: Conversational Movie Recommendation	NA:NA:NA	2018
Luyan Xu:Zeon Trevor Fernando:Xuan Zhou:Wolfgang Nejdl	In this demo paper, we introduce LogCanvas, a platform for user search history visualization.Different from the existing visualization tools, LogCanvas focuses on helping users re-construct the semantic relationship among their search activities. LogCanvas segments a user's search history into different sessions and generates a knowledge graph to represent the information exploration process in each session.A knowledge graph is composed of the most important concepts or entities discovered by each search query as well as their relationships. It thus captures the semantic relationship among the queries.LogCanvas offers a session timeline viewer and a snippets viewer to enable users to re-find their previous search results efficiently. LogCanvas also provides a collaborative perspective to support a group of users in sharing search results and experience.	LogCanvas: Visualizing Search History Using Knowledge Graphs	NA:NA:NA:NA	2018
Jing Li:Aixin Sun:Zhenchang Xing:Lei Han	Application programming interface (API) documentation well describes an API and how to use it. However, official documentation does not describe "how not to use it" or the different kinds of errors when an API is used wrongly. Programming caveats are negative usages of an API. When these caveats are overlooked, errors may emerge, leading to heavy discussions on Q&A websites like Stack Overflow. In this demonstration, we present API Caveat Explorer, a search system to explore API caveats that are mined from large-scale unstructured discussions on Stack Overflow. API Caveat Explorer takes API-oriented queries such as "HashMap" and retrieves API caveats by text summarization techniques. API caveats are represented by sentences, which are context-independent, prominent, semantically diverse and non-redundant. The system provides a web-based interface that allows users to interactively explore the full picture of all discovered caveats of an API, and the details of each. The potential users of API Caveat Explorer are programmers and educators for learning and teaching APIs.	API Caveat Explorer -- Surfacing Negative Usages from Practice: An API-oriented Interactive Exploratory Search System for Programmers	NA:NA:NA:NA	2018
Shuo Zhang:Vugar Abdul Zada:Krisztian Balog	We introduce SmartTable, an online spreadsheet application that is equipped with intelligent assistance capabilities. With a focus on relational tables, describing entities along with their attributes, we offer assistance in two flavors: (i) for populating the table with additional entities (rows) and (ii) for extending it with additional entity attributes (columns). We provide details of our implementation, which is also released as open source. The application is available at http://smarttable.cc.	SmartTable: A Spreadsheet Program with Intelligent Assistance	NA:NA:NA	2018
Tuukka Ruotsalo:Antti Lipsanen	Medical information retrieval suffers from a dual problem: users struggle in describing what they are experiencing from a medical perspective and the search engine is struggling in retrieving the information exactly matching what users are experiencing. We demonstrate interactive symptom elicitation for diagnostic information retrieval. Interactive symptom elicitation builds a model from the user's initial description of the symptoms and interactively elicitates new information about symptoms by posing questions of related, but uncertain, symptoms for the user. As a result, the system interactively learns the estimates of symptoms while controlling the uncertainties related to the diagnostic process. The learned model is then used to rank the associated diagnoses that the user might be experiencing. Our preliminary experimental results show that interactive symptom elicitation can significantly improve user's capability to describe their symptoms, increase the confidence of the model, and enable effective diagnostic information retrieval.	Interactive Symptom Elicitation for Diagnostic Information Retrieval	NA:NA	2018
Asmelash Teka Hadgu:Sallam Abualhaija:Claudia Niederée	The observation of social media provides an important complementing source of information about an unfolding event such as a crisis situation. For this purpose we have developed and demonstrate Sover!, a system to monitor real-time dynamic events via Twitter targeting the needs of aid organizations. At its core it builds upon an effective adaptive crawler, which combines two social media streams in a Bayesian inference framework and after each time-window updates the probabilities of whether given keywords are relevant for an event. Sover! also exposes the crawling functionality so a user can actively influence the evolving selection of keywords. The crawling activity feeds a rich dashboard, which enables the user to get a better understanding of a crisis situation as it unfolds in real-time.	Sover! Social Media Observer	NA:NA:NA	2018
Craig Macdonald	Experimentation using IR systems has traditionally been a procedural and laborious process. Queries must be run on an index, with any parameters of the retrieval models suitably tuned. With the advent of learning-to-rank, such experimental processes (including the appropriate folding of queries to achieve cross-fold validation) have resulted in complicated experimental designs and hence scripting. At the same time, machine learning platforms such as Scikit Learn and Apache Spark have pioneered the notion of an experimental pipeline , which naturally allows a supervised classification experiment to be expressed a series of stages, which can be learned or transformed. In this demonstration, we detail Terrier-Spark, a recent adaptation to the Terrier Information Retrieval platform which permits it to be used within the experimental pipelines of Spark. We argue that this (1) provides an agile experimental platform for information retrieval, comparable to that enjoyed by other branches of data science; (2) aids research reproducibility in information retrieval by facilitating easily-distributable notebooks containing conducted experiments; and (3) facilitates the teaching of information retrieval experiments in educational environments.	Combining Terrier with Apache Spark to create Agile Experimental Information Retrieval Pipelines	NA	2018
Kuldeep Singh:Ioanna Lytra:Arun Sethupat Radhakrishna:Akhilesh Vyas:Maria-Esther Vidal	Question answering (QA) systems provide user-friendly interfaces for retrieving answers from structured and unstructured data given natural language questions. Several QA systems, as well as related components, have been contributed by the industry and research community in recent years. However, most of these efforts have been performed independently from each other and with different focuses, and their synergies in the scope of QA have not been addressed adequately. FRANKENSTEIN is a novel framework for developing QA systems over knowledge bases by integrating existing state-of-the-art QA components performing different tasks. It incorporates several reusable QA components, employs machine learning techniques to predict best performing components and QA pipelines for a given question, and generates static and dynamic executable QA pipelines. In this paper, we illustrate different functionalities of FRANKENSTEIN for performing independent QA component execution, QA component prediction, given an input question as well as the static and dynamic composition of different QA pipelines.	Dynamic Composition of Question Answering Pipelines with FRANKENSTEIN	NA:NA:NA:NA:NA	2018
Mustafa Abualsaud:Nimesh Ghelani:Haotian Zhang:Mark D. Smucker:Gordon V. Cormack:Maura R. Grossman	The goal of high-recall information retrieval (HRIR) is to find all or nearly all relevant documents for a search topic. In this paper, we present the design of our system that affords efficient high-recall retrieval. HRIR systems commonly rely on iterative relevance feedback. Our system uses a state-of-the-art implementation of continuous active learning (CAL), and is designed to allow other feedback systems to be attached with little work. Our system allows users to judge documents as fast as possible with no perceptible interface lag. We also support the integration of a search engine for users who would like to interactively search and judge documents. In addition to detailing the design of our system, we report on user feedback collected as part of a 50 participants user study. While we have found that users find the most relevant documents when we restrict user interaction, a majority of participants prefer having flexibility in user interaction. Our work has implications on how to build effective assessment systems and what features of the system are believed to be useful by users.	A System for Efficient High-Recall Retrieval	NA:NA:NA:NA:NA:NA	2018
Norman Meuschke:Vincent Stange:Moritz Schubotz:Bela Gipp	Current plagiarism detection systems reliably find instances of copied and moderately altered text, but often fail to detect strong paraphrases, translations, and the reuse of non-textual content and ideas. To improve upon the detection capabilities for such concealed content reuse in academic publications, we make four contributions: i) We present the first plagiarism detection approach that combines the analysis of mathematical expressions, images, citations and text. ii) We describe the implementation of this hybrid detection approach in the research prototype HyPlag. iii) We present novel visualization and interaction concepts to aid users in reviewing content similarities identified by the hybrid detection approach. iv) We demonstrate the usefulness of the hybrid detection and result visualization approaches by using HyPlag to analyze a confirmed case of content reuse present in a retracted research publication.	HyPlag: A Hybrid Approach to Academic Plagiarism Detection	NA:NA:NA:NA	2018
Mir Anamul Hasan:Daniel G. Schwartz	This demo presents RecAdvisor, a prototype recommender system for finding and recommending potential Ph.D. supervisors for students by identifying different criteria to consider when selecting a supervisor.	RecAdvisor: Criteria-based Ph.D. Supervisor Recommendation	NA:NA	2018
Andrew Jie Zhou:Grace Hui Yang	In this paper, we introduce a Virtual Reality (VR) search engine interface. Virtual reality has been explored in the game industry and in the multimedia community. When wearing a VR device, a realistic experience is simulated around the user. In the working environment, VR's potential is still understudied. As a first step to enable VR-supported working environment, we present a search engine with a virtual reality interface. In our system, users can read, search and interact with the search engine with novel experiences. They only need to use their hands to interact with digital content, just like what is shown in the "minority report" movie.	Minority Report by Lemur: Supporting Search Engine with Virtual Reality	NA:NA	2018
Puxuan Yu:Wasi Uddin Ahmad:Hongning Wang	We develop Hide-n-Seek, an intent-aware privacy protection plugin for personalized web search. In addition to users' genuine search queries, Hide-n-Seek submits k cover queries and corresponding clicks to an external search engine to disguise a user's search intent grounded and reinforced in a search session by mimicking the true query sequence. The cover queries are synthesized and randomly sampled from a topic hierarchy, where each node represents a coherent search topic estimated by both n-gram and neural language models constructed over crawled web documents. Hide-n-Seek also personalizes the returned search results by re-ranking them based on the genuine user profile developed and maintained on the client side. With a variety of graphical user interfaces, we present the topic-based query obfuscation mechanism to the end users for them to digest how their search privacy is protected.	Hide-n-Seek: An Intent-aware Privacy Protection Plugin for Personalized Web Search	NA:NA:NA	2018
Rajeev Rastogi	In this talk, I will first provide an overview of key problem areas where we are applying Machine Learning techniques within Amazon such as product demand forecasting, product search, and information extraction from reviews, and associated technical challenges. I will then talk about two specific applications where we use a variety of methods to learn semantically rich representations of data: question answering where we use deep learning techniques and product size recommendations where we use probabilistic models. Rajeev Rastogi is a Director of Machine Learning at Amazon where he is developing ML platforms and applications for the e-commerce domain. Previously, he was Vice President of Yahoo! Labs Bangalore and the founding Director of the Bell Labs Research Center in Bangalore, India. Rajeev is an ACM Fellow and a Bell Labs Fellow. He is active in the fields of databases, data mining, and networking, and has served on the program committees of several conferences in these areas. He currently serves on the editorial boards of the CACM, VLDB Journal and ACM Computing Surveys, and has been an Associate editor for IEEE Transactions on Knowledge and Data Engineering in the past. He has published over 125 papers, and holds over 50 patents. Rajeev received his B. Tech degree from IIT Bombay, and a PhD degree in Computer Science from the University of Texas, Austin.	Machine Learning @ Amazon	NA	2018
Charu C. Aggarwal	Many streaming applications in social networks, communication networks, and information networks are built on top of large graphs Such large networks contain continuously occurring processes, which lead to streams of edge interactions and posts. For example, the messages sent by participants on Facebook to one another can be viewed as content-rich interactions along edges. Such edge-centric streams are referred to as graph streams or social streams. The aggregate volume of these interactions can scale up super-linearly with the number of nodes in the network, which makes the problem more pressing for rapidly growing networks. These continuous streams may be mined for useful insights. In these cases, real-time analysis is crucial because of the time-sensitive nature of the interactions. However, generalizing conventional mining applications to such graphs turns out to be a challenge because of the expensive nature of graph mining algorithms. We discuss recent advances in several graph mining applications like clustering, classification, link prediction, event detection, and anomaly detection in real-time graph streams.	Extracting Real-Time Insights from Graphs and Social Streams	NA	2018
Jieping Ye	Didi Chuxing is the largest ride-sharing platform in China, providing transportation services for over 400 million users. Every day, Didi Chuxing's platform generates over 100 TB worth of data, processes more than 40 billion routing requests, and produces over 15 billion location points. In this talk, I will explain how Didi Chuxing applies big data and AI technologies to analyze big transportation data and improve the travel experience for millions of users.	Big Data at Didi Chuxing	NA	2018
Xian-Sheng Hua	A city is an aggregate of a huge amount of heterogeneous data. However, extracting meaningful values from that data remains challenging. City Brain is an end-to-end system whose goal is to glean irreplaceable values from big city data, specifically from videos, with the assistance of rapidly evolving AI technologies and fast-growing computing capacity. From cognition to optimization, to decision-making, from search to prediction and ultimately, to intervention, City Brain improves the way we manage the city, as well as the way we live in it. In this talk, firstly we will introduce current practices of the City Brain platform in a few cities in China, including what we can do to achieve the goal and make it a reality. Then we will focus on visual search technologies and applications that we can apply on the city data. Last, a few video demos will be shown, followed by highlighting a few future directions of city computing.	The City Brain: Towards Real-Time Search for the Real-World	NA	2018
Emre Kıcıman	Many people use web search engines for expectation exploration: exploring what might happen if they take some action, or how they should expect some situation to evolve. While search engines have databases to provide structured answers to many questions, there is no database about the outcomes of actions or the evolution of situations. The information we need to answer such questions, however, is already being recorded. On social media, for example, hundreds of millions of people are publicly reporting about the actions they take and the situations they are in, and an increasing range of events and activities experienced in their lives over time. In this presentation, we show how causal inference methods can be applied to such individual-level, longitudinal records to generate answers for expectation exploration queries.	Causal Inference over Longitudinal Data to Support Expectation Exploration	NA	2018
Ted Tao Yuan:Zezhong Zhang	To recommend relevant merchandises for seasonal retail events, we rely on item retrieval from marketplace inventory. With feedback to expand query scope, we discuss keyword expansion candidate selection using word embedding similarity, and an enhanced tf-idf formula for expanded words in search ranking.	Merchandise Recommendation for Retail Events with Word Embedding Weighted Tf-idf and Dynamic Query Expansion	NA:NA	2018
David Carmel:Liane Lewin-Eytan:Yoelle Maarek	Alexa is an intelligent personal assistant developed by Amazon, that can provide many services through voice interaction such as music playback, news, question-answering, and on-line shopping. The Alexa shopping research team in Amazon is a new emerging group of scientists who investigate revolutionary shopping experience through Alexa, while devising new search paradigms beyond traditional catalog search.	Product Question Answering Using Customer Generated Content - Research Challenges	NA:NA:NA	2018
Konstantine Arkoudas:Mohamed Yahya	The Bloomberg Terminal is the leading source of information and news in the finance industry. Through hundreds of functions that provide access to a vast wealth of structured and semi-structured data, the terminal is able to satisfy a wide range of information needs. Users can find what they need by constructing queries, plotting charts, creating alerts, and so on. Until recently, most queries to the terminal were constructed through dedicated GUIs. For instance, if users wanted to screen for technology companies that met certain criteria, they would specify the criteria by filling out a form via a sequence of interactions with GUI elements such as drop-down lists, checkboxes, radio and toggle buttons, etc. To facilitate information retrieval in the terminal, we are equipping it with the ability to understand and answer queries expressed in natural language. Our QA (question answering) systems map structurally complex questions like the above to a logical meaning representation which can then be translated to an executable query language (such as SQL or SPARQL). At that point we can execute the queries against a suitable back end, obtain the results, and present them to the users. Adding a natural-language interface to a data repository introduces usability challenges of its own, chief amongst them being this: How can the user know what the system can and cannot understand and answer (without needing to undergo extensive training)? We can unpack this question into two separate parts: 1) How can we convey the full range of the system's abilities? 2) How can we convey its limitations? We use auto-complete as a tool to help meet both challenges. Specifically, the first question pertains to the general issue of discoverability: We want at least some of the suggested completions to act as vehicles for discovering data and functionality of which users may have not been previously aware. The second question pertains to expectation management. Naturally, no QA system can attain perfect performance; limiting factors include representational shortcomings and various kinds of incompleteness of the underlying data sources, as well as NLP technology limitations. We want to stop generating completions as a signal indicating that we are not able to understand and/or answer what is being typed.	Auto-completion for Question Answering Systems at Bloomberg	NA:NA	2018
Sahin Cem Geyik:Qi Guo:Bo Hu:Cagri Ozcaglar:Ketan Thakkar:Xianren Wu:Krishnaram Kenthapadi	In this talk, we present the overall system design and architecture, the challenges encountered in practice, and the lessons learned from the production deployment of the talent search and recommendation systems at LinkedIn. By presenting our experiences of applying techniques at the intersection of recommender systems, information retrieval, machine learning, and statistical modeling in a large-scale industrial setting and highlighting the open problems, we hope to stimulate further research and collaborations within the SIGIR community.	Talent Search and Recommendation Systems at LinkedIn: Practical Challenges and Lessons Learned	NA:NA:NA:NA:NA:NA:NA	2018
Ajeet Grewal:Jimmy Lin	We present a broad overview of personalized content recommendations at Twitter, discussing how our approach has evolved over the years, represented by several generations of systems. Historically, content analysis of Tweets has not been a priority, and instead engineering efforts have focused on graph-based recommendation techniques that exploit structural properties of the follow graph and engagement signals from users. These represent "low hanging fruits" that have enabled high-quality recommendations using simple algorithms. As deployed systems have grown in maturity and our understanding of the problem space has become more refined, we have begun to look for other opportunities to further improve recommendation quality. We overview recent investments in content analysis, particularly named-entity recognition techniques built around recurrent neural networks, and discuss how they integrate with existing graph-based capabilities to open up the design space of content recommendation algorithms.	The Evolution of Content Analysis for Personalized Recommendations at Twitter	NA:NA	2018
James Wong:Brendan Collins:Ganesh Venkataraman	Airbnb is an online marketplace which connects hosts and guests all over the world. Our inventory includes over 4.5 million listings, which enable the travel of over 300 million guests. The growth team at Airbnb is responsible for helping travelers find Airbnb, in part by participating in ad auctions on major search platforms such as Google and Bing. In this talk, we will describe how ad- vertising efficiently on these platforms requires solving several information retrieval and machine learning problems, including query understanding, click value estimation, and realtime pacing of our expenditure.	Large Scale Search Engine Marketing (SEM) at Airbnb	NA:NA:NA	2018
Inho Kang	NAVER, Korea's No. 1 internet company, and LINE, the Japan-based global messenger platform, have teamed up to use their extensive online content databases (documents, maps, encyclopedia, etc.) and user information to develop an AI platform called Clova (Cloud-based Virtual Assistant). Clova's deep-learning based methods have made tremendous progress in image classification and intelligent Q&A response. For example, in Naver, users can procure product or location information by showing pictures. Keyword-based matching is enhanced using semantic analysis, and retrieving top links is changed to answering questions by refining user queries through dialogue. In addition to PC and mobile, Clova is evolving to enable a user to access the relevant information using multiple devices and interfaces. Line released a smart speaker called Clova Friends that includes Line's free voice-call function and infrared home-appliance control. IPTV set-top box powered-by the Clova AI platform which can recommend movies based on Naver's user reviews. In this talk, I will cover some efforts and challenges in understanding and satisfying users on each device with sophisticated natural language processing technologies. I will introduce the technically challenging problems that we are currently tackling and future AI developments.	Clova: Services and Devices Powered by AI	NA	2018
Manoj Kumar Chinnakotla:Puneet Agrawal	In this work, we highlight some interesting challenges faced when trying to build a large-scale commercial IR-based chatbot, Ruuh, for an emerging market like India which has unique characteristics such as high linguistic and cultural diversity, large section of young population and the second largest mobile market in the world. We set out to build a "human-like" AI agent which aspires to become the trusted friend of every Indian youth. To meet this objective, we realised that we need to think beyond the utilitarian notion of merely generating "relevant" responses and enable the agent to comprehend and meet a wider range of user social needs, like expressing happiness when user's favourite team wins, sharing a cute comment on showing the pictures of the user's pet and so on. The agent should also be well-versed with the informal language of the urban Indian youth which often includes slang and code-mixing across two or more languages (English and their native language). Finally, in order to be their trusted friend, the agent has to communicate with respect without offending their sentiments and emotions. Some of the above objectives pose significant research challenges in the areas of NLP, IR and AI. We take the audience through our journey of how we tackled some of the above challenges while building a large-scale commercial IR-based conversational agent. Our attempts to solve some of the above challenges have also resulted in some interesting research contributions in the form of publications and patents in the above areas. Our chat-bot currently has more than 1M users who have engaged in more than 70M conversations.	Lessons from Building a Large-scale Commercial IR-based Chatbot for an Emerging Market	NA:NA	2018
Perry Samson:Charles Bassam	A new educational service has been prototyped by Echo360 that uses natural language processing to analyze students notes and provide personalized recommendations on how to both improve note-taking and scaffold learning. The LessonWare middleware system uses computer-generated transcriptions from class captures and available student notes to identify key terms mentioned during class sessions. The combination of analyzed key terms and corresponding timestamps allows contextual linkages to be created between educational resources. Student notes are automatically augmented with corresponding moments in class captures, specific pages in the course eTextbook or open education resources or specific adaptive learning assets.	LessonWare: Mining Student Notes to Provide Personalized Feedback	NA:NA	2018
Jun Xu:Xiangnan He:Hang Li	Matching is the key problem in both search and recommendation, that is to measure the relevance of a document to a query or the interest of a user on an item. Previously, machine learning methods have been exploited to address the problem, which learns a matching function from labeled data, also referred to as "learning to match''. In recent years, deep learning has been successfully applied to matching and significant progresses have been made. Deep semantic matching models for search and neural collaborative filtering models for recommendation are becoming the state-of-the-art technologies. The key to the success of the deep learning approach is its strong ability in learning of representations and generalization of matching patterns from raw data (e.g., queries, documents, users, and items, particularly in their raw forms). In this tutorial, we aim to give a comprehensive survey on recent progress in deep learning for matching in search and recommendation. Our tutorial is unique in that we try to give a unified view on search and recommendation. In this way, we expect researchers from the two fields can get deep understanding and accurate insight on the spaces, stimulate more ideas and discussions, and promote developments of technologies. The tutorial mainly consists of three parts. Firstly, we introduce the general problem of matching, which is fundamental in both search and recommendation. Secondly, we explain how traditional machine learning techniques are utilized to address the matching problem in search and recommendation. Lastly, we elaborate how deep learning can be effectively used to solve the matching problems in both tasks.	Deep Learning for Matching in Search and Recommendation	NA:NA:NA	2018
Tetsuya Sakai	This hands-on half-day tutorial consists of two 90-minute sessions. Part I covers the following topics: paired and two-sample t -tests, confidence intervals (with Excel and R); familywise error rate, multiple comparison procedures; ANOVA (with Excel and R); Tukey's HSD test, simultaneous confidence intervals (with R). Part II covers the following topics: randomised Tukey HSD test (with Discpower); what's wrong with statistical significance tests?; effect sizes, statistical power; topic set size design (with Excel); power analysis (with R); summary: how to report your results. Participants should have some prior knowledge about the very basics of statistical significance testing and are strongly encouraged to bring a laptop with R already installed. The tutorial participants will be able to design and conduct statistical significance tests for comparing the mean effectiveness scores of two or more systems appropriately, and to report on the test results in an informative manner.	Conducting Laboratory Experiments Properly with Statistical Tools: An Easy Hands-on Tutorial	NA	2018
Jianfeng Gao:Michel Galley:Lihong Li	This tutorial surveys neural approaches to conversational AI that were developed in the last few years. We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) social bots. For each category, we present a review of state-of-the-art neural approaches, draw the connection between neural approaches and traditional symbolic approaches, and discuss the progress we have made and challenges we are facing, using specific systems and models as case studies.	Neural Approaches to Conversational AI	NA:NA:NA	2018
Weinan Zhang	Generative adversarial nets (GANs) have been widely studied during the recent development of deep learning and unsupervised learning. With an adversarial training mechanism, GAN manages to train a generative model to fit the underlying unknown real data distribution under the guidance of the discriminative model estimating whether a data instance is real or generated. Such a framework is originally proposed for fitting continuous data distribution such as images, thus it is not straightforward to be directly applied to information retrieval scenarios where the data is mostly discrete, such as IDs, text and graphs. In this tutorial, we focus on discussing the GAN techniques and the variants on discrete data fitting in various information retrieval scenarios. (i) We introduce the fundamentals of GAN framework and its theoretic properties; (ii) we carefully study the promising solutions to extend GAN onto discrete data generation; (iii) we introduce IRGAN, the fundamental GAN framework of fitting single ID data distribution and the direct application on information retrieval; (iv) we further discuss the task of sequential discrete data generation tasks, e.g., text generation, and the corresponding GAN solutions; (v) we present the most recent work on graph/network data fitting with node embedding techniques by GANs. Meanwhile, we also introduce the relevant open-source platforms such as IRGAN and Texygen to help audience conduct research experiments on GANs in information retrieval. Finally, we conclude this tutorial with a comprehensive summarization and a prospect of further research directions for GANs in information retrieval.	Generative Adversarial Nets for Information Retrieval: Fundamentals and Advances	NA	2018
Zhaochun Ren:Xiangnan He:Dawei Yin:Maarten de Rijke	E-commerce (electronic commerce or EC) is the buying and selling of goods and services, or the transmitting of funds or data online. E-commerce platforms come in many kinds, with global players such as Amazon, Airbnb, Alibaba, eBay, JD.com and platforms targeting specific markets such as Bol.com and Booking.com. Information retrieval has a natural role to play in e-commerce, especially in connecting people to goods and services. Information discovery in e-commerce concerns different types of search (exploratory search vs. lookup tasks), recommender systems, and natural language processing in e-commerce portals. Recently, the explosive popularity of e-commerce sites has made research on information discovery in e-commerce more important and more popular. There is increased attention for e-commerce information discovery methods in the community as witnessed by an increase in publications and dedicated workshops in this space. Methods for information discovery in e-commerce largely focus on improving the performance of e-commerce search and recommender systems, on enriching and using knowledge graphs to support e-commerce, and on developing innovative question-answering and bot-based solutions that help to connect people to goods and services. Below we describe why we believe that the time is right for an introductory tutorial on information discovery in e-commerce, the objectives of the proposed tutorial, its relevance, as well as more practical details, such as the format, schedule and support materials.	Information Discovery in E-commerce: Half-day SIGIR 2018 Tutorial	NA:NA:NA:NA	2018
Oren Kurland:J. Shane Culpepper	Fusion is an important and central concept in Information Retrieval. The goal of fusion methods is to merge different sources of information so as to address a retrieval task. For example, in the adhoc retrieval setting, fusion methods have been applied to merge multiple document lists retrieved for a query. The lists could be retrieved using different query representations, document representations, ranking functions and corpora. The goal of this half day, intermediate-level, tutorial is to provide a methodological view of the theoretical foundations of fusion approaches, the numerous fusion methods that have been devised and a variety of applications for which fusion techniques have been applied.	Fusion in Information Retrieval: SIGIR 2018 Half-Day Tutorial	NA:NA	2018
Laura Dietz:Alexander Kotov:Edgar Meij	The past decade has witnessed the emergence of several publicly available and proprietary knowledge graphs (KGs). The depth and breadth of content in these KGs made them not only rich sources of structured knowledge by themselves, but also valuable resources for search systems. A surge of recent developments in entity linking and entity retrieval methods gave rise to a new line of research that aims at utilizing KGs for text-centric retrieval applications. This tutorial is the first to summarize and disseminate the progress in this emerging area to industry practitioners and researchers.	Utilizing Knowledge Graphs for Text-Centric Information Retrieval	NA:NA:NA	2018
Guido Zuccon:Bevan Koopman	The HS2018 tutorial will cover topics from an area of information retrieval (IR) with significant societal impact --- health search. Whether it is searching patient records, helping medical professionals find best-practice evidence, or helping the public locate reliable and readable health information online, health search is a challenging area for IR research with an actively growing community and many open problems. This tutorial will provide attendees with a full stack of knowledge on health search, from understanding users and their problems to practical, hands-on sessions on current tools and techniques, current campaigns and evaluation resources, as well as important open questions and future directions.	SIGIR 2018 Tutorial on Health Search (HS2018): A Full-day from Consumers to Clinicians	NA:NA	2018
ChengXiang Zhai:Chase Geigle	As text data continues to grow quickly, it is increasingly important to develop intelligent systems to help people manage and make use of vast amounts of text data ("big text data''). As a new family of effective general approaches to text data retrieval and analysis, probabilistic topic models---notably Probabilistic Latent Semantic Analysis (PLSA), Latent Dirichlet Allocations (LDA), and their many extensions---have been studied actively in the past decade with widespread applications. These topic models are powerful tools for extracting and analyzing latent topics contained in text data; they also provide a general and robust latent semantic representation of text data, thus improving many applications in information retrieval and text mining. Since they are general and robust, they can be applied to text data in any natural language and about any topics. This tutorial systematically reviews the major research progress in probabilistic topic models and discuss their applications in text retrieval and text mining. The tutorial provides (1) an in-depth explanation of the basic concepts, underlying principles, and the two basic topic models (i.e., PLSA and LDA) that have widespread applications, (2) an introduction to EM algorithms and Bayesian inference algorithms for topic models, (3) a hands-on exercise to allow the tutorial attendants to learn how to use the topic models implemented in the MeTA Open Source Toolkit and experiment with provided data sets, (4) a broad overview of all the major representative topic models that extend PLSA or LDA, and (5) a discussion of major challenges and future research directions.	A Tutorial on Probabilistic Topic Models for Text Data Retrieval and Analysis	NA:NA	2018
Soumen Chakrabarti	Systems for structured knowledge extraction and inference have made giant strides in the last decade. Starting from shallow linguistic tagging and coarse-grained recognition of named entities at the resolution of people, places, organizations, and times, modern systems link billions of pages of unstructured text with knowledge graphs having hundreds of millions of entities belonging to tens of thousands of types, and related by tens of thousands of relations. Via deep learning, systems build continuous representations of words, entities, types, and relations, and use these to continually discover new facts to add to the knowledge graph, and support search systems that go far beyond page-level "ten blue links''. We will present a comprehensive catalog of the best practices in traditional and deep knowledge extraction, inference and search. We will trace the development of diverse families of techniques, explore their interrelationships, and point out various loose ends.	Knowledge Extraction and Inference from Text: Shallow, Deep, and Everything in Between	NA	2018
Nicola Tonellotto:Craig Macdonald	Typically, techniques that benefit effectiveness of information retrieval (IR) systems have a negative impact on efficiency. Yet, with the large scale of Web search engines, there is a need to deploy efficient query processing techniques to reduce the cost of the infrastructure required. This tutorial aims to provide a detailed overview of the infrastructure of an IR system devoted to the efficient yet effective processing of user queries. This tutorial guides the attendees through the main ideas, approaches and algorithms developed in the last 30 years in query processing. In particular, we illustrate, with detailed examples and simplified pseudo-code, the most important query processing strategies adopted in major search engines, with a particular focus on dynamic pruning techniques. Moreover, we present and discuss the state-of-the-art innovations in query processing, such as impact-sorted and blockmax indexes. We also describe how modern search engines exploit such algorithms with learning-to-rank (LtR) models to produce effective results, exploiting new approaches in LtR query processing. Finally, this tutorial introduces query efficiency predictors for dynamic pruning, and discusses their main applications to scheduling, routing, selective processing and parallelisation of query processing, as deployed by a major search engine.	Efficient Query Processing Infrastructures: A half-day tutorial at SIGIR 2018	NA:NA	2018
Jon Degenhardt:Pino Di Fabbrizio:Surya Kallumadi:Mohit Kumar:Yiu-Chang Lin:Andrew Trotman:Huasha Zhao	eCommerce Information Retrieval has received little attention in the academic literature, yet it is an essential component of some of the largest web sites (such as eBay, Amazon, Airbnb, Alibaba, Taobao, Target, Facebook, and others). SIGIR has for several years seen sponsorship from these kinds of organisations, who clearly value the importance of research into Information Retrieval. The purpose of this workshop is to bring together researchers and practitioners of eCommerce IR to discuss topics unique to it, to set a research agenda, and to examine how to build datasets for research into this fascinating topic. eCommerce IR is ripe for research and has a unique set of problems. For example, in eCommerce search there may be no hypertext links between documents (products); there is a click stream, but more importantly, there is often a buy stream. eCommerce problems are wide in scope and range from user interaction modalities (the kinds of search seen in when buying are different from those of web-page search (i.e. it is not clear how shopping and buying relate to the standard web-search interaction models)) through to dynamic updates of a rapidly changing collection on auction sites, and the experienceness of some products (such as Airbnb bookings). This workshop is a follow up to the "SIGIR 2017 workshop on eCommerce (ECOM17)", which was organized at SIGIR 2017, Tokyo. In the 2018 workshop, in addition to a data challenge, we will be following up on multiple aspects that were discussed in the 2017 workshop.	SIGIR 2018 Workshop on eCommerce (ECOM18)	NA:NA:NA:NA:NA:NA:NA	2018
Yongfeng Zhang:Yi Zhang:Min Zhang	Explainable recommendation and search attempt to develop models or methods that not only generate high-quality recommendation or search results, but also intuitive explanations of the results for users or system designers, which can help to improve the system transparency, persuasiveness, trustworthiness, and effectiveness, etc. This is even more important in personalized search and recommendation scenarios, where users would like to know why a particular product, web page, news report, or friend suggestion exists in his or her own search and recommendation lists. The motivation of the workshop is to promote the research and application of Explainable Recommendation and Search, under the background of Explainable AI in a more general sense. Early recommendation and search systems adopted intuitive yet easily explainable models to generate recommendation and search lists, such as user-based and item-based collaborative filtering for recommendation, which provide recommendations based on similar users or items, or TF-IDF based retrieval models for search, which provide document ranking lists according to word similarity between different documents. However, state-of-the-art recommendation and search models extensively rely on complex machine learning and latent representation models such as matrix factorization or even deep neural networks, and they work with various types of information sources such as ratings, text, images, audio or video signals. The complexity nature of state-of-the-art models make search and recommendation systems as blank-boxes for end users, and the lack of explainability weakens the persuasiveness and trustworthiness of the system for users, making explainable recommendation and search important research issues to the IR community. In a broader sense, researchers in the whole artificial intelligence community have also realized the importance of Explainable AI, which aims to address a wide range of AI explainability problems in deep learning, computer vision, automatic driving systems, and natural language processing tasks. As an important branch of AI research, this further highlights the importance and urgency for our IR/RecSys community to address the explainability issues of various recommendation and search systems.	SIGIR 2018 Workshop on ExplainAble Recommendation and Search (EARS 2018)	NA:NA:NA	2018
Muthu Kumar Chandrasekaran:Kokil Jaidka:Philipp Mayr	The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Information retrieval~(IR), bibliometric and natural language processing (NLP) techniques could enhance scholarly search, retrieval and user experience but are not yet widely used. To this purpose, we propose the third iteration of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL). The workshop is intended to stimulate IR, NLP researchers and Digital Library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics, text mining and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop will incorporate multiple invited talks, paper sessions, a poster session and the 4th edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.	Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2018)	NA:NA:NA	2018
Paul Groth:Laura Koesten:Philipp Mayr:Maarten de Rijke:Elena Simperl	This half day workshop explores challenges in data search, with a particular focus on data on the web. We want to stimulate an interdisciplinary discussion around how to improve the description, discovery, ranking and presentation of structured and semi-structured data, across data formats and domain applications. We welcome contributions describing algorithms and systems, as well as frameworks and studies in human data interaction. The workshop aims to bring together communities interested in making the web of data more discoverable, easier to search and more user friendly.	DATA: SEARCH'18 - Searching Data on the Web	NA:NA:NA:NA:NA	2018
Laura Dietz:Chenyan Xiong:Jeff Dalton:Edgar Meij	Semantic technologies such as controlled vocabularies, thesauri, and knowledge graphs have been used throughout the history of information retrieval for a variety of tasks. Recent advances in knowledge acquisition, alignment, and utilization have given rise to a body of new approaches for utilizing knowledge graphs in text retrieval tasks and it is therefore time to consolidate the community efforts and study how such technologies can be employed in information retrieval systems in the most effective way. It is also time to start and deepen the dialogue between researchers and practitioners in order to ensure that breakthroughs, technologies, and algorithms in this space are widely disseminated. The goal of this workshop is to bring together and grow a community of researchers and practitioners who are interested in using, aligning, and constructing knowledge graphs and similar semantic resources for information retrieval applications.	The Second Workshop on Knowledge Graphs and Semantics for Text Retrieval, Analysis, and Understanding (KG4IR)	NA:NA:NA:NA	2018
Xi Niu:Wlodek Zadrozny:Kazjon Grace:Weimao Ke	The concept of surprise is central to human learning and development. However, compared to accuracy, surprise has received little attention in the IR community, yet it is an essential component of the information seeking process. This workshop brings together researchers and practitioners of IR to discuss the topic of computational surprise, to set a research agenda, and to examine how to build datasets for research into this fascinating topic. The themes in this workshop include discussion of what can be learned from some well-known surprise models in other fields, such as Bayesian surprise; how to evaluate surprise based on user experience; and how computational surprise is related to the newly emerging areas, such as fake news detection, computational contradiction, clickbait detection, etc.	Computational Surprise in Information Retrieval	NA:NA:NA:NA	2018
Suzan Verberne:Jiyin He:Udo Kruschwitz:Birger Larsen:Tony Russell-Rose:Arjen P. de Vries	Professional search is a problem area in which many facets of information retrieval are addressed, both system-related (e.g. distributed search) and user-related (e.g. complex information needs), and the interface between user and system (e.g. supporting exploratory search tasks). Professional search tasks have specific requirements, different from the requirements of generic web search. The aim of this workshop is to bring together researchers to work on the requirements and challenges of professional search from different angles. We will have an interactive workshop where researchers not only present their scientific results but also work together on the definition of future challenges and solutions with input from information professionals. The workshop will deliver a roadmap of research directions for the years to come.	First International Workshop on Professional Search (ProfS2018)	NA:NA:NA:NA:NA:NA	2018
Jaime Arguello:Filip Radlinski:Hideo Joho:Damiano Spina:Julia Kiseleva	The CAIR'18 workshop will bring together academic and industrial researchers to create a forum for research on conversational approaches to search and recommendation. A specific focus will be on techniques that support complex and multi-turn user-machine dialogues for information access and retrieval, and multi-modal interfaces for interacting with such systems.	Second International Workshop on Conversational Approaches to Information Retrieval (CAIR'18): Workshop at SIGIR 2018	NA:NA:NA:NA:NA	2018
Hamed Zamani:Mostafa Dehghani:Fernando Diaz:Hang Li:Nick Craswell	In recent years, machine learning approaches, and in particular deep neural networks, have yielded significant improvements on several natural language processing and computer vision tasks; however, such breakthroughs have not yet been observed in the area of information retrieval. Besides the complexity of IR tasks, such as understanding the user's information needs, a main reason is the lack of high-quality and/or large-scale training data for many IR tasks. This necessitates studying how to design and train machine learning algorithms where there is no large-scale or high-quality data in hand. Therefore, considering the quick progress in development of machine learning models, this is an ideal time for a workshop that especially focuses on learning in such an important and challenging setting for IR tasks. The goal of this workshop is to bring together researchers from industry---where data is plentiful but noisy---with researchers from academia---where data is sparse but clean to discuss solutions to these related problems.	SIGIR 2018 Workshop on Learning from Limited or Noisy Data for Information Retrieval	NA:NA:NA:NA:NA	2018
Yan Liu:Zhenhui Li:Wei Ai:Lingyu Zhang	We propose a half-day workshop at SIGIR 2018 for the professionals, researchers, and practitioners who are interested in mining and understanding big and heterogeneous data generated in transportation to improve the transportation system. We plan to have both paper presentations and invited talks.	SIGIR 2018 Workshop on Intelligent Transportation Informatics	NA:NA:NA:NA	2018
Steven Zimmerman	The interplay between human biases and the underlying data collection and algorithmic methods to present users with relevant information in information retrieval (IR) systems have undesirable side effects, such as filter bubbles, censorship and developing beliefs in false information. Previous work in the areas of interactive information retrieval, document classification, behavioral economics and user profiling provide the foundation for our research. Using existing knowledge about human bias and profile data, we propose leveraging this information to raise awareness to users about their behavior in the frame of IR systems and inferences made. It is our goal to understand to what extent user behavior is changed. Our position is that education and awareness are much better approaches to address the ethical and human rights concerns when compared to regulatory measures and non-transparent changes to IR algorithms. It is believed the approach outlined below has the potential to dampen the effects of filter bubbles, reduce consumption of misleading and potentially hateful content, to broaden perspectives and protect the fundamental human right to freedom of expression.	Exploring Potential Pathways to Address Bias and Ethics in IR	NA	2018
Shuo Zhang	Tables are one of those "universal tools'' that are practical and useful in many application scenarios. Tables can be used to collect and organize information from multiple sources and then turn that information into knowledge (and ultimately to support decision-making) by performing various operations, like sorting, filtering, and joins. Because of this, a large number of tables exist already out there on the Web, which represent a vast and rich source of structured information and could be utilized as resources. Recently, a growing body of work has begun to tap into utilizing the knowledge contained in tables. A wide and diverse range of tasks have been undertaken, including but not limited to (i) searching for tables[4], (ii) extracting knowledge from tables, and (iii) augmenting tables (e.g., with new columns and rows[1,3] ). The objective of this research is to develop a set of components for a tool called SmartTable, which is aimed at assisting the user in completing a complex task by providing intelligent assistance for working with tables. Imagine the scenario that a user is working with a table, and has already entered some data in the table. We can provide recommendations for the empty table cells, search for similar tables that can serve as a blueprint, or even generate automatically the entire table that the user needs. The table-making task can thus be simplified into just a few button clicks. Motivated by the above scenario, we propose a set of novel tasks such as row and column heading population, table search, and table generation. The following specific research questions are addressed: ( RQ1 ) How to populate table rows and column heading labels? ( RQ2 ) How to find relevant tables given a keyword query? ( RQ3 ) How to find tables relevant to the table the user is currently working on? ( RQ4 ) How to generate an output table as response to a free text query? For RQ1, the task of row population [1,3] relates to the task of entity set expansion, where a given set of entities is to be completed with additional entities. Row population focuses on populating entities in the "core column'' of a relational table. We develop a two-step pipeline for this task utilizing a table corpus and a knowledge base. In the first step, candidate entities sharing the same categories with seed entities or co-occurring in similar tables are selected. In the second step, they are ranked by a probabilistic model. Column population shares similarities with the problem of schema complement, where a seed table is to be extended with additional columns. For column population, we regard column headings from similar tables as candidates and rank them using a probabilistic model. For RQ2 and RQ3, we address the problem of table search. This task is not only interesting on its own but is also being used as a fundamental building block in many other table-based information access scenarios, such as table completion or table mining. To search related tables, the query could be some keywords [2,4] or it can also be an existing (incomplete) table. Based on the query type, this task is divided into two sub-tasks, which are table retrieval for keyword query and query-by-table respectively. For RQ4, we introduce and address the task of the on-the-fly table generation: given a query, generate a relational table that contains relevant entities (as rows) along with their key properties (as columns) [5]. In terms of the table elements in a relational table, this task boils downing to core column entity ranking, schema determination and value look-up. We propose a feature-based approach for entity ranking and schema determination, combing deep semantic features with task-specific signals. For value lookup, we combine information from existing tables and a knowledge base. So far, we have proposed methods and evaluation resources for addressing the tasks of row/column population, table search, and table generation. Future research directions for this project include looking up table values, interacting with tables using natural language, and generating table embeddings.	SmartTable: Equipping Spreadsheets with Intelligent AssistanceFunctionalities	NA	2018
Luke Gallagher	NA	Efficiency-Effectiveness Trade-Offs in Machine Learned Models for Information Retrieval	NA	2018
Stefano Marchesin	We propose a research that aims at improving the effectiveness of case-based retrieval systems through the use of automatically created document-level semantic networks. The proposed research leverages the recent advancements in information extraction and relational learning to revisit and advance the core ideas of concept-centered hypertext models. The automatic extraction of semantic relations from documents --- and their centrality in the creation and exploitation of the documents' semantic networks --- represents our attempt to go one step further than previous approaches.	Case-Based Retrieval Using Document-Level Semantic Networks	NA	2018
Eilon Sheetrit	Our main goal is studying the merits of using inter-passage similarities for the task of focused retrieval; i.e., ranking passages in documents by their relevance to an information need expressed by a query. As an initial research direction we study the cluster hypothesis for passage (focused) retrieval. We propose a novel suite of cluster hypothesis tests that employ inter-passage similarities and demonstrate that the cluster hypothesis holds for passages. In addition, we present several future directions we intend to pursue.	Utilizing Inter-Passage Similarities for Focused Retrieval	NA	2018
Anirban Chakraborty	Making recommendations to a user which are more refined, personalized and contextually appropriate, has become an important problem in a variety of domains. Contextual Point of Interest (POI) recommendation is of particular interest for travel and tourism, to suggest places to visit for a user traveling to a new city. In this paper, I propose an approach which frames contextual POI recommendation as a traditional document ranking problem, where I represent each POI as a document. A significant aspect of this research investigates the generation of an enriched document representation for a POI, by harvesting information such as descriptions, users' reviews, and ratings from the web. I propose a log-linear retrieval model for document ranking using Kernel Density Estimation (KDE) which will eventually rank POIs. Additionally, I propose to use social media, such as Twitter, to supplement the recommendation process. People's spontaneous and unfiltered opinions about a POI give valuable information about that POI which is different from other online reviews. This unique type of user data will be used to enrich the document representation for POIs, and it is possible to incorporate the effect of social media into my retrieval model. I follow the TREC Contextual Suggestion track setup for my evaluation methodology.	Enhanced Contextual Recommendation using Social Media Data	NA	2018
Darío Garigliotti	Web search is a human experience of a very rapid and impactful evolution. It has become a key technology on which people rely daily for getting information about almost everything. This evolution of the search experience has also shaped the expectations of people about it. The user sees the search engine as a wise interpreter capable of understanding the intent and meaning behind a search query, realizing her current context, and responding to it directly and appropriately[1]. Search by meaning, or semantic search, rather than just literal matches, became possible by a large portion of IR research devoted to study semantically more meaningful representations of the information need expressed by the user query. Major commercial search engines have indeed responded to user expectations, capitalizing on query semantics, or query understanding. They introduced features which not only provide information directly but also engage the user to stay interacting in the search engine result page (SERP). Direct displays (weather, flight offers, exchange rates, etc.), rich vertical content (images, videos, news, etc.), and knowledge panels, are examples of this recent evolution trend into answer engines [10].  Our notion of semantics is inherently based on the one of structure. Given the large portion of web search queries looking for entities[11], entities and their properties---attributes, types, and relationships---are first-class citizens in our space of structured knowledge. Semantic search can then be seen as a rich toolbox. Multiple techniques recognize these essential knowledge units in queries, identify them uniquely in underlying knowledge repositories, and exploit them to address a particular aspect of query understanding[2]. Query recommendations are another remarkable approach, whose suggestion feedback points to provide hints for improving the articulation of the search query. >This research focuses on utilizing techniques from semantic search in the next evolution stage of search engines, namely, the support for task completion. Search is usually performed with a specific goal underlying the query. This goal, in many cases, consists in a nontrivial task to be completed. Indeed, task-based search corresponds to a considerable portion of query volume[5]. Addressing task-based search can have a large impact on search behavior, yet the interaction processes behind performing a complex task are very far to be fully understood. [12] Current search engines allow to solve a small set of basic tasks, and most of the knowledge-intensive workload for supporting more complex tasks is on the user. The ultimate challenge is then to build useful systems "to achieve work task completion''. [13] Rather than modeling explicitly search tasks, we strive for extending and enhancing solid strategies of semantic search to help users achieve their tasks. One component we focus on in this research is utilizing entity type information, to gain a better understanding of how entity type information can be exploited in entity retrieval. [7,9] The second component is concerned with understanding query intents. Specifically, understanding what entity-oriented queries ask for, and how they can be fulfilled. [8] The third component is about generating query suggestions to support task-based search [4,6]. The search goal, often complex and knowledge-intensive, may lead the user to issue multiple queries to eventually complete her underlying task. We envisage the capability of the three identified components to complement each other for supporting task completion.	A Semantic Search Approach to Task-Completion Engines	NA	2018
Kristine M. Rogers	A user with a standing need for updates on current events uses a structured exploration process for finding and reviewing new documents, with the user comparing document information to her mental model. To avoid missing key changes on the topic, the user should see some documents on each of the subtopics available that day. This research includes a system and evaluation approach for this standing need use case.	Addressing News-Related Standing Information Needs	NA	2018
Harrisen Scells	Systematic reviews, in particular medical systematic reviews, are time consuming and costly to produce but are of value for clinical decision making, policy, and regulations. The largest contributing factors to the time and monetary costs are the searching (including the formulation of queries) and screening processes. These initial processes involve researchers reading the abstracts of thousands and sometimes hundreds of thousands of research articles to determine if the retrieved articles should be included or excluded from the systematic review. This research explores automatic methodologies to reduce the workload relating to the searching and initial screening processes. The objective of this research is to use Information Retrieval techniques to improve the retrieval of literature for medical systematic reviews.	Improving Systematic Review Creation With Information Retrieval	NA	2018
Unni Krishnan	NA	Design and Evaluation of Query Auto Completion Mechanisms	NA	2018
Sudeshna Das	Almost every part of the world relies on textbooks as the primary medium of imparting education. The quality of education, thus, is correlated with the quality of textbooks. In general, the quality of content in the textbooks used in the less-developed countries of the world is not up to the mark [1]. In addition to their intended purpose of delivering information, textbooks also promote behaviours that adults wish to pass on to the next generation [7]. It is, thus, important to ensure that textbooks are helpful in effective learning and do not condone undesirable social mores. The task of evaluating textbooks against these parameters is not trivial: experts must go through the entire content manually. This exercise being not only laborious, but also expensive [3]. This thesis attempts to propose a feasible computational alternative to this process The dataset that we are working with is comprised of school textbooks collected from different boards of education of two south-east Asian countries that are widely regarded as 'developing countries'[4]. In general, a digitized textbook throws a large number of computational problems that require ideas from a number of disciplines such as Natural Language Processing, Information Retrieval, Human Computer Interaction and Cognitive Science. In this thesis we focus on the following research questions.  Research Question 1 How can we automatically identify the text fragments that reflect gender bias from textbooks? The prevalence of gender bias has been reported in textbooks from many parts of the world [5,6]. Computational efforts to contain the effects of such biases have been proposed [2], but the detection of presence of gender bias, and identifying text passages that exhibit such biases is an unexplored problem. We model the task as a binary classification problem, with one class being biased text, and the other being unbiased text. For classifying text fragments, we propose a boosted memory-based model to learn the hidden patterns of biases from a small amount of labelled data. We are currently working on the sub-problem of identifying the gender of named human entities in textbooks in the absence of clear linguistic markers of gender such as 'sister', 'himself', etc. Our methodology is based on leveraging contextual data in the form of phrase vectors, along with sentence structure.  Research Question 2 How can we enhance the learning experience of a student through an optimal ordering of concepts, and ensure the coverage of all necessary topics? We propose to represent textbook sections in the form of concept graphs [8], and utilize the linked structure of Wikipedia to determine necessary and sufficient concepts whose inclusion ensures completeness of the graph, i.e., prerequisite concepts [3] are not left out, while minimizing redundancy. For example, on the event of the presence of outgoing edges from an excluded concept C 1 to a concept C 2 included in the textbook, we suggest that C 1 be also included to ensure a comprehensive coverage of the subject matter. We also propose assigning weights to these edges, denoting the amount of dependence a concept has on another, to facilitate our aim of finding an optimal ordering of the concepts.  Research Question 3 How can we automatically identify and accumulate resources of the same difficulty level as the original text from external repositories, to facilitate betterunderstanding of the content among students? Students often refer to online resources to better understand textbook content. A major hurdle associated with this practice is the wide mismatch in difficulty levels of the textbook, and the hits returned by a search engine [1]. Our aim is to learn to distinguish between texts of different levels of difficulty by using textbooks as the training data. We propose to apply a transfer learning-based approach to use the parameters learned during training to identify suitable web content. At present, we are planning to work with school-level basic science textbooks.	Better Textbooks with Human Language Technology	NA	2018
