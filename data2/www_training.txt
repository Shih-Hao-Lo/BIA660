Olivier Chapelle:Ya Zhang	As with any application of machine learning, web search ranking requires labeled data. The labels usually come in the form of relevance assessments made by editors. Click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. The main difficulty however comes from the so called position bias - urls appearing in lower positions are less likely to be clicked even if they are relevant. In this paper, we propose a Dynamic Bayesian Network which aims at providing us with unbiased estimation of the relevance from the click logs. Experiments show that the proposed click model outperforms other existing click models in predicting both click-through rate and relevance.	A dynamic bayesian network click model for web search ranking	NA:NA	2009
Fan Guo:Chao Liu:Anitha Kannan:Tom Minka:Michael Taylor:Yi-Min Wang:Christos Faloutsos	Given a terabyte click log, can we build an efficient and effective click model? It is commonly believed that web search click logs are a gold mine for search business, because they reflect users' preference over web documents presented by the search engine. Click models provide a principled approach to inferring user-perceived relevance of web documents, which can be leveraged in numerous applications in search businesses. Due to the huge volume of click data, scalability is a must. We present the click chain model (CCM), which is based on a solid, Bayesian framework. It is both scalable and incremental, perfectly meeting the computational challenges imposed by the voluminous click logs that constantly grow. We conduct an extensive experimental study on a data set containing 8.8 million query sessions obtained in July 2008 from a commercial search engine. CCM consistently outperforms two state-of-the-art competitors in a number of metrics, with over 9.7% better log-likelihood, over 6.2% better click perplexity and much more robust (up to 30%) prediction of the first and the last clicked position.	Click chain model in web search	NA:NA:NA:NA:NA:NA:NA	2009
Deepak Agarwal:Bee-Chung Chen:Pradheep Elango	We propose novel spatio-temporal models to estimate click-through rates in the context of content recommendation. We track article CTR at a fixed location over time through a dynamic Gamma-Poisson model and combine information from correlated locations through dynamic linear regressions, significantly improving on per-location model. Our models adjust for user fatigue through an exponential tilt to the first-view CTR (probability of click on first article exposure) that is based only on user-specific repeat-exposure features. We illustrate our approach on data obtained from a module (Today Module) published regularly on Yahoo! Front Page and demonstrate significant improvement over commonly used baseline methods. Large scale simulation experiments to study the performance of our models under different scenarios provide encouraging results. Throughout, all modeling assumptions are validated via rigorous exploratory data analysis.	Spatio-temporal models for estimating click-through rate	NA:NA:NA	2009
Purnamrita Sarkar:Andrew W. Moore	In this paper we consider the problem of re-ranking search results by incorporating user feedback. We present a graph theoretic measure for discriminating irrelevant results from relevant results using a few labeled examples provided by the user. The key intuition is that nodes relatively closer (in graph topology) to the relevant nodes than the irrelevant nodes are more likely to be relevant. We present a simple sampling algorithm to evaluate this measure at specific nodes of interest, and an efficient branch and bound algorithm to compute the top k nodes from the entire graph under this measure. On quantifiable prediction tasks the introduced measure outperforms other diffusion-based proximity measures which take only the positive relevance feedback into account. On the Entity-Relation graph built from the authors and papers of the entire DBLP citation corpus (1.4 million nodes and 2.2 million edges) our branch and bound algorithm takes about 1.5 seconds to retrieve the top 10 nodes w.r.t. this measure with 10 labeled nodes.	Fast dynamic reranking in large graphs	NA:NA	2009
Ziv Bar-Yossef:Maxim Gurevich	The ImpressionRank of a web page (or, more generally, of a web site) is the number of times users viewed the page while browsing search results. ImpressionRank captures the visibility of pages and sites in search engines and is thus an important measure, which is of interest to web site owners, competitors, market analysts, and end users. All previous approaches to estimating the ImpressionRank of a page rely on privileged access to private data sources, like the search engine's query log. In this paper we present the first external algorithm for estimating the ImpressionRank of a web page. This algorithm relies on access to three public data sources: the search engine, the query suggestion service of the search engine, and the web. In addition, the algorithm is local and uses modest resources. It can therefore be used by almost any party to estimate the ImpressionRank of any page on any search engine. En route to estimating the ImpressionRank of a page, our algorithm solves a novel variant of the keyword extraction problem: it finds the most popular search keywords that drive impressions of a page. Empirical analysis of the algorithm on the Google and Yahoo! search engines indicates that it is accurate and provides interesting insights about sites and search queries.	Estimating the impressionrank of web pages	NA:NA	2009
Jiang Bian:Yandong Liu:Ding Zhou:Eugene Agichtein:Hongyuan Zha	Community Question Answering (CQA) has emerged as a popular forum for users to pose questions for other users to answer. Over the last few years, CQA portals such as Naver and Yahoo! Answers have exploded in popularity, and now provide a viable alternative to general purpose Web search. At the same time, the answers to past questions submitted in CQA sites comprise a valuable knowledge repository which could be a gold mine for information retrieval and automatic question answering. Unfortunately, the quality of the submitted questions and answers varies widely - increasingly so that a large fraction of the content is not usable for answering queries. Previous approaches for retrieving relevant and high quality content have been proposed, but they require large amounts of manually labeled data -- which limits the applicability of the supervised approaches to new sites and domains. In this paper we address this problem by developing a semi-supervised coupled mutual reinforcement framework for simultaneously calculating content quality and user reputation, that requires relatively few labeled examples to initialize the training process. Results of a large scale evaluation demonstrate that our methods are more effective than previous approaches for finding high-quality answers, questions, and users. More importantly, our quality estimation significantly improves the accuracy of search over CQA archives over the state-of-the-art methods.	Learning to recognize reliable users and content in social media with coupled mutual reinforcement	NA:NA:NA:NA:NA	2009
Ossama Abdel Hamid:Behshad Behzadi:Stefan Christoph:Monika Henzinger	In the origin detection problem an algorithm is given a set S of documents, ordered by creation time, and a query document D. It needs to output for every consecutive sequence of k alphanumeric terms in D the earliest document in $S$ in which the sequence appeared (if such a document exists). Algorithms for the origin detection problem can, for example, be used to detect the "origin" of text segments in D and thus to detect novel content in D. They can also find the document from which the author of D has copied the most (or show that D is mostly original.) We concentrate on solutions that use only a fixed amount of memory. We propose novel algorithms for this problem and evaluate them together with a large number of previously published algorithms. Our results show that (1) detecting the origin of text segments efficiently can be done with very high accuracy even when the space used is less than 1% of the size of the documents in $S$, (2) the precision degrades smoothly with the amount of available space, (3) various estimation techniques can be used to increase the performance of the algorithms.	Detecting the origin of text segments efficiently	NA:NA:NA:NA	2009
Liangda Li:Ke Zhou:Gui-Rong Xue:Hongyuan Zha:Yong Yu	Document summarization plays an increasingly important role with the exponential growth of documents on the Web. Many supervised and unsupervised approaches have been proposed to generate summaries from documents. However, these approaches seldom simultaneously consider summary diversity, coverage, and balance issues which to a large extent determine the quality of summaries. In this paper, we consider extract-based summarization emphasizing the following three requirements: 1) diversity in summarization, which seeks to reduce redundancy among sentences in the summary; 2) sufficient coverage, which focuses on avoiding the loss of the document's main information when generating the summary; and 3) balance, which demands that different aspects of the document need to have about the same relative importance in the summary. We formulate the extract-based summarization problem as learning a mapping from a set of sentences of a given document to a subset of the sentences that satisfies the above three requirements. The mapping is learned by incorporating several constraints in a structure learning framework, and we explore the graph structure of the output variables and employ structural SVM for solving the resulted optimization problem. Experiments on the DUC2001 data sets demonstrate significant performance improvements in terms of F1 and ROUGE metrics.	Enhancing diversity, coverage and balance for summarization through structure learning	NA:NA:NA:NA:NA	2009
Jong Wook Kim:K. Selçuk Candan:Junichi Tatemura	The use of blogs to track and comment on real world (political, news, entertainment) events is growing. Similarly, as more individuals start relying on the Web as their primary information source and as more traditional media outlets try reaching consumers through alternative venues, the number of news sites on the Web is also continuously increasing. Content-reuse, whether in the form of extensive quotations or content borrowing across media outlets, is very common in blogs and news entries outlets tracking the same real-world event. Knowledge about which web entries re-use content from which others can be an effective asset when organizing these entries for presentation. On the other hand, this knowledge is not cheap to acquire: considering the size of the related space web entries, it is essential that the techniques developed for identifying re-use are fast and scalable. Furthermore, the dynamic nature of blog and news entries necessitates incremental processing for reuse detection. In this paper, we develop a novel qSign algorithm that efficiently and effectively analyze the blogosphere for quotation and reuse identification. Experiment results show that with qSign processing time gains from 10X to 100X are possible while maintaining reuse detection rates of upto 90%. Furthermore, processing time gains can be pushed multiple orders of magnitude (from 100X to 1000X) for 70% recall.	Efficient overlap and content reuse detection in blogs and online news articles	NA:NA:NA	2009
Sihong Xie:Wei Fan:Jing Peng:Olivier Verscheure:Jiangtao Ren	Transferring knowledge from one domain to another is challenging due to a number of reasons. Since both conditional and marginal distribution of the training data and test data are non-identical, model trained in one domain, when directly applied to a different domain, is usually low in accuracy. For many applications with large feature sets, such as text document, sequence data, medical data, image data of different resolutions, etc. two domains usually do not contain exactly the same features, thus introducing large numbers of "missing values" when considered over the union of features from both domains. In other words, its marginal distributions are at most overlapping. In the same time, these problems are usually high dimensional, such as, several thousands of features. Thus, the combination of high dimensionality and missing values make the relationship in conditional probabilities between two domains hard to measure and model. To address these challenges, we propose a framework that first brings the marginal distributions of two domains closer by "filling up" those missing values of disjoint features. Afterwards, it looks for those comparable sub-structures in the "latent-space" as mapped from the expanded feature vector, where both marginal and conditional distribution are similar. With these sub-structures in latent space, the proposed approach then find common concepts that are transferable across domains with high probability. During prediction, unlabeled instances are treated as "queries", the mostly related labeled instances from out-domain are retrieved, and the classification is made by weighted voting using retrieved out-domain examples. We formally show that importing feature values across domains and latent semantic index can jointly make the distributions of two related domains easier to measure than in original feature space, the nearest neighbor method employed to retrieve related out domain examples is bounded in error when predicting in-domain examples. Software and datasets are available for download.	Latent space domain transfer between high dimensional overlapping distributions	NA:NA:NA:NA:NA	2009
Jun Zhu:Zaiqing Nie:Xiaojiang Liu:Bo Zhang:Ji-Rong Wen	Traditional relation extraction methods require pre-specified relations and relation-specific human-tagged examples. Bootstrapping systems significantly reduce the number of training examples, but they usually apply heuristic-based methods to combine a set of strict hard rules, which limit the ability to generalize and thus generate a low recall. Furthermore, existing bootstrapping methods do not perform open information extraction (Open IE), which can identify various types of relations without requiring pre-specifications. In this paper, we propose a statistical extraction framework called Statistical Snowball (StatSnowball), which is a bootstrapping system and can perform both traditional relation extraction and Open IE. StatSnowball uses the discriminative Markov logic networks (MLNs) and softens hard rules by learning their weights in a maximum likelihood estimate sense. MLN is a general model, and can be configured to perform different levels of relation extraction. In StatSnwoball, pattern selection is performed by solving an l1-norm penalized maximum likelihood estimation, which enjoys well-founded theories and efficient solvers. We extensively evaluate the performance of StatSnowball in different configurations on both a small but fully labeled data set and large-scale Web data. Empirical results show that StatSnowball can achieve a significantly higher recall without sacrificing the high precision during iterations with a small number of seeds, and the joint inference of MLN can improve the performance. Finally, StatSnowball is efficient and we have developed a working entity relation search engine called Renlifang based on it.	StatSnowball: a statistical approach to extracting entity relationships	NA:NA:NA:NA:NA	2009
David H. Stern:Ralf Herbrich:Thore Graepel	We present a probabilistic model for generating personalised recommendations of items to users of a web service. The Matchbox system makes use of content information in the form of user and item meta data in combination with collaborative filtering information from previous user behavior in order to predict the value of an item for a user. Users and items are represented by feature vectors which are mapped into a low-dimensional `trait space' in which similarity is measured in terms of inner products. The model can be trained from different types of feedback in order to learn user-item preferences. Here we present three alternatives: direct observation of an absolute rating each user gives to some items, observation of a binary preference (like/ don't like) and observation of a set of ordinal ratings on a user-specific scale. Efficient inference is achieved by approximate message passing involving a combination of Expectation Propagation (EP) and Variational Message Passing. We also include a dynamics model which allows an item's popularity, a user's taste or a user's personal rating scale to drift over time. By using Assumed-Density Filtering (ADF) for training, the model requires only a single pass through the training data. This is an on-line learning algorithm capable of incrementally taking account of new data so the system can immediately reflect the latest user preferences. We evaluate the performance of the algorithm on the MovieLens and Netflix data sets consisting of approximately 1,000,000 and 100,000,000 ratings respectively. This demonstrates that training the model using the on-line ADF approach yields state-of-the-art performance with the option of improving performance further if computational resources are available by performing multiple EP passes over the training data.	Matchbox: large scale online bayesian recommendations	NA:NA:NA	2009
Paul N. Bennett:David Maxwell Chickering:Anton Mityagin	We consider the problem of identifying the consensus ranking for the results of a query, given preferences among those results from a set of individual users. Once consensus rankings are identified for a set of queries, these rankings can serve for both evaluation and training of retrieval and learning systems. We present a novel approach to collecting the individual user preferences over image-search results: we use a collaborative game in which players are rewarded for agreeing on which image result is best for a query. Our approach is distinct from other labeling games because we are able to elicit directly the preferences of interest with respect to image queries extracted from query logs. As a source of relevance judgments, this data provides a useful complement to click data. Furthermore, the data is free of positional biases and is collected by the game without the risk of frustrating users with non-relevant results; this risk is prevalent in standard mechanisms for debiasing clicks. We describe data collected over 34 days from a deployed version of this game that amounts to about 18 million expressed preferences between pairs. Finally, we present several approaches to modeling this data in order to extract the consensus rankings from the preferences and better sort the search results for targeted queries.	Learning consensus opinion: mining data from a labeling game	NA:NA:NA	2009
Yue Lu:ChengXiang Zhai:Neel Sundaresan	Web 2.0 technologies have enabled more and more people to freely comment on different kinds of entities (e.g. sellers, products, services). The large scale of information poses the need and challenge of automatic summarization. In many cases, each of the user-generated short comments comes with an overall rating. In this paper, we study the problem of generating a ``rated aspect summary'' of short comments, which is a decomposed view of the overall ratings for the major aspects so that a user could gain different perspectives towards the target entity. We formally define the problem and decompose the solution into three steps. We demonstrate the effectiveness of our methods by using eBay sellers' feedback comments. We also quantitatively evaluate each step of our methods and study how well human agree on such a summarization task. The proposed methods are quite general and can be used to generate rated aspect summary automatically given any collection of short comments each associated with an overall rating.	Rated aspect summarization of short comments	NA:NA:NA	2009
Cristian Danescu-Niculescu-Mizil:Gueorgi Kossinets:Jon Kleinberg:Lillian Lee	There are many on-line settings in which users publicly express opinions. A number of these offer mechanisms for other users to evaluate these opinions; a canonical example is Amazon.com, where reviews come with annotations like "26 of 32 people found the following review helpful." Opinion evaluation appears in many off-line settings as well, including market research and political campaigns. Reasoning about the evaluation of an opinion is fundamentally different from reasoning about the opinion itself: rather than asking, "What did Y think of X?", we are asking, "What did Z think of Y's opinion of X?" Here we develop a framework for analyzing and modeling opinion evaluation, using a large-scale collection of Amazon book reviews as a dataset. We find that the perceived helpfulness of a review depends not just on its content but also but also in subtle ways on how the expressed evaluation relates to other evaluations of the same product. As part of our approach, we develop novel methods that take advantage of the phenomenon of review "plagiarism" to control for the effects of text in opinion evaluation, and we provide a simple and natural mathematical model consistent with our findings. Our analysis also allows us to distinguish among the predictions of competing theories from sociology and social psychology, and to discover unexpected differences in the collective opinion-evaluation behavior of user populations from ifferent countries.	How opinions are received by online communities: a case study on amazon.com helpfulness votes	NA:NA:NA:NA	2009
Surajit Chaudhuri:Venkatesh Ganti:Dong Xin	Tasks recognizing named entities such as products, people names, or locations from documents have recently received significant attention in the literature. Many solutions to these tasks assume the existence of reference entity tables. An important challenge that needs to be addressed in the entity extraction task is that of ascertaining whether or not a candidate string approximately matches with a named entity in a given reference table. Prior approaches have relied on string-based similarity which only compare a candidate string and an entity it matches with. In this paper, we exploit web search engines in order to define new similarity functions. We then develop efficient techniques to facilitate approximate matching in the context of our proposed similarity functions. In an extensive experimental evaluation, we demonstrate the accuracy and efficiency of our techniques.	Exploiting web search to generate synonyms for entities	NA:NA:NA	2009
Murat Ali Bayir:Ismail Hakki Toroslu:Ahmet Cosar:Guven Fidan	In this paper, we propose a novel framework called Smart-Miner for web usage mining problem which uses link information for producing accurate user sessions and frequent navigation patterns. Unlike the simple session concepts in the time and navigation based approaches, where sessions are sequences of web pages requested from the server or viewed in the browser, Smart Miner sessions are set of paths traversed in the web graph that corresponds to users' navigations among web pages. We have modeled session construction as a new graph problem and utilized a new algorithm, Smart-SRA, to solve this problem efficiently. For the pattern discovery phase, we have developed an efficient version of the Apriori-All technique which uses the structure of web graph to increase the performance. From the experiments that we have performed on both real and simulated data, we have observed that Smart-Miner produces at least 30% more accurate web usage patterns than other approaches including previous session construction methods. We have also studied the effect of having the referrer information in the web server logs to show that different versions of Smart-SRA produce similar results. Our another contribution is that we have implemented distributed version of the Smart Miner framework by employing Map/Reduce Paradigm. We conclude that we can efficiently process terabytes of web server logs belonging to multiple web sites by our scalable framework.	Smart Miner: a new framework for mining large scale web usage data	NA:NA:NA:NA	2009
Aleksandra Korolova:Krishnaram Kenthapadi:Nina Mishra:Alexandros Ntoulas	The question of how to publish an anonymized search log was brought to the forefront by a well-intentioned, but privacy-unaware AOL search log release. Since then a series of ad-hoc techniques have been proposed in the literature, though none are known to be provably private. In this paper, we take a major step towards a solution: we show how queries, clicks and their associated perturbed counts can be published in a manner that rigorously preserves privacy. Our algorithm is decidedly simple to state, but non-trivial to analyze. On the opposite side of privacy is the question of whether the data we can safely publish is of any use. Our findings offer a glimmer of hope: we demonstrate that a non-negligible fraction of queries and clicks can indeed be safely published via a collection of experiments on a real search log. In addition, we select an application, keyword generation, and show that the keyword suggestions generated from the perturbed data resemble those generated from the original data.	Releasing search queries and clicks privately	NA:NA:NA:NA	2009
Jiang-Ming Yang:Rui Cai:Yida Wang:Jun Zhu:Lei Zhang:Wei-Ying Ma	Web forums have become an important data resource for many web applications, but extracting structured data from unstructured web forum pages is still a challenging task due to both complex page layout designs and unrestricted user created posts. In this paper, we study the problem of structured data extraction from various web forum sites. Our target is to find a solution as general as possible to extract structured data, such as post title, post author, post time, and post content from any forum site. In contrast to most existing information extraction methods, which only leverage the knowledge inside an individual page, we incorporate both page-level and site-level knowledge and employ Markov logic networks (MLNs) to effectively integrate all useful evidence by learning their importance automatically. Site-level knowledge includes (1) the linkages among different object pages, such as list pages and post pages, and (2) the interrelationships of pages belonging to the same object. The experimental results on 20 forums show a very encouraging information extraction performance, and demonstrate the ability of the proposed approach on various forums. We also show that the performance is limited if only page-level knowledge is used, while when incorporating the site-level knowledge both precision and recall can be significantly improved.	Incorporating site-level knowledge to extract structured data from web forums	NA:NA:NA:NA:NA:NA	2009
Huanhuan Cao:Daxin Jiang:Jian Pei:Enhong Chen:Hang Li	Capturing the context of a user's query from the previous queries and clicks in the same session may help understand the user's information need. A context-aware approach to document re-ranking, query suggestion, and URL recommendation may improve users' search experience substantially. In this paper, we propose a general approach to context-aware search. To capture contexts of queries, we learn a variable length Hidden Markov Model (vlHMM) from search sessions extracted from log data. Although the mathematical model is intuitive, how to learn a large vlHMM with millions of states from hundreds of millions of search sessions poses a grand challenge. We develop a strategy for parameter initialization in vlHMM learning which can greatly reduce the number of parameters to be estimated in practice. We also devise a method for distributed vlHMM learning under the map-reduce model. We test our approach on a real data set consisting of 1.8 billion queries, 2.6 billion clicks, and 840 million search sessions, and evaluate the effectiveness of the vlHMM learned from the real data on three search applications: document re-ranking, query suggestion, and URL recommendation. The experimental results show that our approach is both effective and efficient.	Towards context-aware search by learning a very large variable length hidden markov model from search logs	NA:NA:NA:NA:NA	2009
Hu Guan:Jingyu Zhou:Minyi Guo	Automated text categorization is an important technique for many web applications, such as document indexing, document filtering, and cataloging web resources. Many different approaches have been proposed for the automated text categorization problem. Among them, centroid-based approaches have the advantages of short training time and testing time due to its computational efficiency. As a result, centroid-based classifiers have been widely used in many web applications. However, the accuracy of centroid-based classifiers is inferior to SVM, mainly because centroids found during construction are far from perfect locations. We design a fast Class-Feature-Centroid (CFC) classifier for multi-class, single-label text categorization. In CFC, a centroid is built from two important class distributions: inter-class term index and inner-class term index. CFC proposes a novel combination of these indices and employs a denormalized cosine measure to calculate the similarity score between a text vector and a centroid. Experiments on the Reuters-21578 corpus and 20-newsgroup email collection show that CFC consistently outperforms the state-of-the-art SVM classifiers on both micro-F1 and macro-F1 scores. Particularly, CFC is more effective and robust than SVM when data is sparse.	A class-feature-centroid classifier for text categorization	NA:NA:NA	2009
Lei Tang:Suju Rajan:Vijay K. Narayanan	The explosion of online content has made the management of such content non-trivial. Web-related tasks such as web page categorization, news filtering, query categorization, tag recommendation, etc. often involve the construction of multi-label categorization systems on a large scale. Existing multi-label classification methods either do not scale or have unsatisfactory performance. In this work, we propose MetaLabeler to automatically determine the relevant set of labels for each instance without intensive human involvement or expensive cross-validation. Extensive experiments conducted on benchmark data show that the MetaLabeler tends to outperform existing methods. Moreover, MetaLabeler scales to millions of multi-labeled instances and can be deployed easily. This enables us to apply the MetaLabeler to a large scale query categorization problem in Yahoo!, yielding a significant improvement in performance.	Large scale multi-label classification via metalabeler	NA:NA:NA	2009
Ashish Goel:Kamesh Munagala	Search auctions have become a dominant source of revenue generation on the Internet. Such auctions have typically used per-click bidding and pricing. We propose the use of hybrid auctions where an advertiser can make a per-impression as well as a per-click bid, and the auctioneer then chooses one of the two as the pricing mechanism. We assume that the advertiser and the auctioneer both have separate beliefs (called priors) on the click-probability of an advertisement. We first prove that the hybrid auction is truthful, assuming that the advertisers are risk-neutral. We then show that this auction is superior to the existing per-click auction in multiple ways: We show that risk-seeking advertisers will choose only a per-impression bid whereas risk-averse advertisers will choose only a per-click bid, and argue that both kind of advertisers arise naturally. Hence, the ability to bid in a hybrid fashion is important to account for the risk characteristics of the advertisers. For obscure keywords, the auctioneer is unlikely to have a very sharp prior on the click-probabilities. In such situations, we show that having the extra information from the advertisers in the form of a per-impression bid can result in significantly higher revenue. An advertiser who believes that its click-probability is much higher than the auctioneer's estimate can use per-impression bids to correct the auctioneer's prior without incurring any extra cost. The hybrid auction can allow the advertiser and auctioneer to implement complex dynamic programming strategies to deal with the uncertainty in the click-probability using the same basic auction. The per-click and per-impression bidding schemes can only be used to implement two extreme cases of these strategies. As Internet commerce matures, we need more sophisticated pricing models to exploit all the information held by each of the participants. We believe that hybrid auctions could be an important step in this direction. The hybrid auction easily extends to multiple slots, and is also applicable to scenarios where the hybrid bidding is per-impression and per-action (i.e. CPM and CPA), or per-click and per-action (i.e. CPC and CPA).	Hybrid keyword search auctions	NA:NA	2009
Eyal Even Dar:Vahab S. Mirrokni:S. Muthukrishnan:Yishay Mansour:Uri Nadav	Ad auctions in sponsored search support "broad match" that allows an advertiser to target a large number of queries while bidding only on a limited number. While giving more expressiveness to advertisers, this feature makes it challenging to optimize bids to maximize their returns: choosing to bid on a query as a broad match because it provides high profit results in one bidding for related queries which may yield low or even negative profits. We abstract and study the complexity of the {\em bid optimization problem} which is to determine an advertiser's bids on a subset of keywords (possibly using broad match) so that her profit is maximized. In the query language model when the advertiser is allowed to bid on all queries as broad match, we present a linear programming (LP)-based polynomial-time algorithm that gets the optimal profit. In the model in which an advertiser can only bid on keywords, ie., a subset of keywords as an exact or broad match, we show that this problem is not approximable within any reasonable approximation factor unless P=NP. To deal with this hardness result, we present a constant-factor approximation when the optimal profit significantly exceeds the cost. This algorithm is based on rounding a natural LP formulation of the problem. Finally, we study a budgeted variant of the problem, and show that in the query language model, one can find two budget constrained ad campaigns in polynomial time that implement the optimal bidding strategy. Our results are the first to address bid optimization under the broad match feature which is common in ad auctions.	Bid optimization for broad match ad auctions	NA:NA:NA:NA:NA	2009
Gagan Aggarwal:S. Muthukrishnan:Dávid Pál:Martin Pál	In sponsored search, a number of advertising slots is available on a search results page, and have to be allocated among a set of advertisers competing to display an ad on the page. This gives rise to a bipartite matching market that is typically cleared by the way of an automated auction. Several auction mechanisms have been proposed, with variants of the Generalized Second Price (GSP) being widely used in practice. There is a rich body of work on bipartite matching markets that builds upon the stable marriage model of Gale and Shapley and the assignment model of Shapley and Shubik. This line of research offers deep insights into the structure of stable outcomes in such markets and their incentive properties. In this paper, we model advertising auctions in terms of an assignment model with linear utilities, extended with bidder and item specific maximum and minimum prices. Auction mechanisms like the commonly used GSP or the well-known Vickrey-Clarke-Groves (VCG) can be interpreted as simply computing a bidder-optimal stable matching in this model, for a suitably defined set of bidder preferences, but our model includes much richer bidders and preferences. We prove that in our model the existence of a stable matching is guaranteed, and under a non-degeneracy assumption a bidder-optimal stable matching exists as well. We give an algorithm to find such matching in polynomial time, and use it to design truthful mechanism that generalizes GSP, is truthful for profit-maximizing bidders, correctly implements features like bidder-specific minimum prices and position-specific bids, and works for rich mixtures of bidders and preferences. Our main technical contributions are the existence of bidder-optimal matchings and strategyproofness of the resulting mechanism, and are proved by induction on the progress of the matching algorithm.	General auction mechanism for search advertising	NA:NA:NA:NA	2009
Arpita Ghosh:Benjamin I.P. Rubinstein:Sergei Vassilvitskii:Martin Zinkevich	Motivated by the emergence of auction-based marketplaces for display ads such as the Right Media Exchange, we study the design of a bidding agent that implements a display advertising campaign by bidding in such a marketplace. The bidding agent must acquire a given number of impressions with a given target spend, when the highest external bid in the marketplace is drawn from an unknown distribution P. The quantity and spend constraints arise from the fact that display ads are usually sold on a CPM basis. We consider both the full information setting, where the winning price in each auction is announced publicly, and the partially observable setting where only the winner obtains information about the distribution; these differ in the penalty incurred by the agent while attempting to learn the distribution. We provide algorithms for both settings, and prove performance guarantees using bounds on uniform closeness from statistics, and techniques from online learning. We experimentally evaluate these algorithms: both algorithms perform very well with respect to both target quantity and spend; further, our algorithm for the partially observable case performs nearly as well as that for the fully observable setting despite the higher penalty incurred during learning.	Adaptive bidding for display advertising	NA:NA:NA:NA	2009
Jun Yan:Ning Liu:Gang Wang:Wen Zhang:Yun Jiang:Zheng Chen	Behavioral Targeting (BT) is a technique used by online advertisers to increase the effectiveness of their campaigns, and is playing an increasingly important role in the online advertising market. However, it is underexplored in academia when looking at how much BT can truly help online advertising in commercial search engines. To answer this question, in this paper we provide an empirical study on the click-through log of advertisements collected from a commercial search engine. From the comprehensively experiment results on the sponsored search log of the commercial search engine over a period of seven days, we can draw three important conclusions: (1) Users who clicked the same ad will truly have similar behaviors on the Web; (2) Click-Through Rate (CTR) of an ad can be averagely improved as high as 670% by properly segmenting users for behavioral targeted advertising in a sponsored search; (3) Using the short term user behaviors to represent users is more effective than using the long term user behaviors for BT. The statistical t-test verifies that all conclusions drawn in the paper are statistically significant. To the best of our knowledge, this work is the first empirical study for BT on the click-through log of real world ads.	How much can behavioral targeting help online advertising?	NA:NA:NA:NA:NA:NA	2009
Thomas Meinl:Benjamin Blau	Web service development and usage has shifted from simple information processing services to high-value business services that are crucial to productivity and success. In order to deal with an increasing risk of unavailability or failure of mission-critical Web services we argue the need for advanced reservation of services in the form of derivatives. The contribution of this paper is twofold: First we provide an abstract model of a market design that enables the trade of derivatives for mission-critical Web services. Our model satisfies requirements that result from service characteristics such as intangibility and the impossibility to inventor services in order to meet fluctuating demand. It comprehends principles from models of incomplete markets such as the absence of a tradeable underlying and consistent arbitrage-free derivative pricing. Furthermore we provide an architecture for a Web service market that implements our model and describes the strategy space and interaction of market participants in the trading process of service derivatives. We compare the underlying pricing processes to existing derivative models in energy exchanges, discuss eventual shortcomings, and apply Wavelets to analyze actual data and extract long- and short-term trends.	Web service derivatives	NA:NA	2009
Zakaria Al-Qudah:Hussein A. Alzoubi:Mark Allman:Michael Rabinovich:Vincenzo Liberatore	Web hosting providers are increasingly looking into dynamic hosting to reduce costs and improve the performance of their platforms. Instead of provisioning fixed resources to each customer, dynamic hosting maintains a variable number of application instances to satisfy current demand. While existing research in this area has mostly focused on the algorithms that decide on the number and location of application instances, we address the problem of efficient enactment of these decisions once they are made. We propose a new approach to application placement and experimentally show that it dramatically reduces the cost of application placement, which in turn improves the end-to-end agility of the hosting platform in reacting to demand changes.	Efficient application placement in a dynamic hosting platform	NA:NA:NA:NA:NA	2009
Jeffrey Erman:Alexandre Gerber:Mohammad T. Hajiaghayi:Dan Pei:Oliver Spatscheck	This paper proposes and evaluates a Network Aware Forward Caching approach for determining the optimal deployment strategy of forward caches to a network. A key advantage of this approach is that we can reduce the network costs associated with forward caching to maximize the benefit obtained from their deployment. We show in our simulation that a 37% increase to net benefits could be achieved over the standard method of full cache deployment to cache all POPs traffic. In addition, we show that this maximal point occurs when only 68% of the total traffic is cached. Another contribution of this paper is the analysis we use to motivate and evaluate this problem. We characterize the Internet traffic of 100K subscribers of a US residential broadband provider. We use both layer 4 and layer 7 analysis to investigate the traffic volumes of the flows as well as study the general characteristics of the applications used. We show that HTTP is a dominant protocol and account for 68% of the total downstream traffic and that 34% of that traffic is multimedia. In addition, we show that multimedia content using HTTP exhibits a 83% annualized growth rate and other HTTP traffic has a 53% growth rate versus the 26% over all annual growth rate of broadband traffic. This shows that HTTP traffic will become ever more dominent and increase the potential caching opportunities. Furthermore, we characterize the core backbone traffic of this broadband provider to measure the distance travelled by content and traffic. We find that CDN traffic is much more efficient than P2P content and that there is large skew in the Air Miles between POP in a typical network. Our findings show that there are many opportunties in broadband provider networks to optimize how traffic is delivered and cached.	Network-aware forward caching	NA:NA:NA:NA:NA	2009
Zakaria Al-Qudah:Seungjoon Lee:Michael Rabinovich:Oliver Spatscheck:Jacobus Van der Merwe	Anycast-based content delivery networks (CDNs) have many properties that make them ideal for the large scale distribution of content on the Internet. However, because routing changes can result in a change of the endpoint that terminates the TCP session, TCP session disruption remains a concern for anycast CDNs, especially for large file downloads. In this paper we demonstrate that this problem does not require any complex solutions. In particular, we present the design of a simple, yet efficient, mechanism to handle session disruptions due to endpoint changes. With our mechanism, a client can continue the download of the content from the point at which it was before the endpoint change. Furthermore, CDN servers purge the TCP connection state quickly to handle frequent switching with low system overhead. We demonstrate experimentally the effectiveness of our proposed mechanism and show that more complex mechanisms are not required. Specifically, we find that our mechanism maintains high download throughput even with a reasonably high rate of endpoint switching, which is attractive for load balancing scenarios. Moreover, our results show that edge servers can purge TCP connection state after a single timeout-triggered retransmission without any tangible impact on ongoing connections. Besides improving server performance, this behavior improves the resiliency of the CDN to certain denial of service attacks.	Anycast-aware transport for content delivery networks	NA:NA:NA:NA:NA	2009
Lyndon Kennedy:Mor Naaman	We describe a system for synchronization and organization of user-contributed content from live music events. We start with a set of short video clips taken at a single event by multiple contributors, who were using a varied set of capture devices. Using audio fingerprints, we synchronize these clips such that overlapping clips can be displayed simultaneously. Furthermore, we use the timing and link structure generated by the synchronization algorithm to improve the findability and representation of the event content, including identifying key moments of interest and descriptive text for important captured segments of the show. We also identify the preferred audio track when multiple clips overlap. We thus create a much improved representation of the event that builds on the automatic content match. Our work demonstrates important principles in the use of content analysis techniques for social media content on the Web, and applies those principles in the domain of live music capture.	Less talk, more rock: automated organization of community-contributed collections of concert videos	NA:NA	2009
Alberto Messina:Maurizio Montagnuolo	Current Web technology has enabled the distribution of informative content through dynamic media platforms. In addition, the availability of the same content in the form of digital multimedia data has dramatically increased. Content-based, cross-media retrieval applications are needed to efficiently access desired information from this variety of data sources. This paper presents a novel approach for cross-media information aggregation, and describes a prototype system implementing this approach. The prototype adopts online newspaper articles and TV newscasts as information sources, to deliver a service made up of items including both contributions. Extensive experiments prove the effectiveness of the proposed approach in a real-world business context.	A generalised cross-modal clustering method applied to multimedia news semantic indexing and retrieval	NA:NA	2009
Munmun De Choudhury:Hari Sundaram:Ajita John:Dorée Duncan Seligmann	Rich media social networks promote not only creation and consumption of media, but also communication about the posted media item. What causes a conversation to be interesting, that prompts a user to participate in the discussion on a posted video? We conjecture that people participate in conversations when they find the conversation theme interesting, see comments by people whom they are familiar with, or observe an engaging dialogue between two or more people (absorbing back and forth exchange of comments). Importantly, a conversation that is interesting must be consequential - i.e. it must impact the social network itself. Our framework has three parts: characterizing themes, characterizing participants for determining interestingness and measures of consequences of a conversation deemed to be interesting. First, we detect conversational themes using a mixture model approach. Second, we determine interestingness of participants and interestingness of conversations based on a random walk model. Third, we measure the consequence of a conversation by measuring how interestingness affects the following three variables - participation in related themes, participant cohesiveness and theme diffusion. We have conducted extensive experiments using dataset from the popular video sharing site, YouTube. Our results show that our method of interestingness maximizes the mutual information, and is significantly better (twice as large) than three other baseline methods (number of comments, number of new participants and PageRank based assessment).	What makes conversations interesting?: themes, participants and consequences of conversations in online social media	NA:NA:NA:NA	2009
Reinier H. van Leuken:Lluis Garcia:Ximena Olivares:Roelof van Zwol	Due to the reliance on the textual information associated with an image, image search engines on the Web lack the discriminative power to deliver visually diverse search results. The textual descriptions are key to retrieve relevant results for a given user query, but at the same time provide little information about the rich image content. In this paper we investigate three methods for visual diversification of image search results. The methods deploy lightweight clustering techniques in combination with a dynamic weighting function of the visual features, to best capture the discriminative aspects of the resulting set of images that is retrieved. A representative image is selected from each cluster, which together form a diverse result set. Based on a performance evaluation we find that the outcome of the methods closely resembles human perception of diversity, which was established in an extensive clustering experiment carried out by human assessors.	Visual diversification of image search results	NA:NA:NA:NA	2009
Dong Liu:Xian-Sheng Hua:Linjun Yang:Meng Wang:Hong-Jiang Zhang	Social media sharing web sites like Flickr allow users to annotate images with free tags, which significantly facilitate Web image search and organization. However, the tags associated with an image generally are in a random order without any importance or relevance information, which limits the effectiveness of these tags in search and other applications. In this paper, we propose a tag ranking scheme, aiming to automatically rank the tags associated with a given image according to their relevance to the image content. We first estimate initial relevance scores for the tags based on probability density estimation, and then perform a random walk over a tag similarity graph to refine the relevance scores. Experimental results on a 50, 000 Flickr photo collection show that the proposed tag ranking method is both effective and efficient. We also apply tag ranking into three applications: (1) tag-based image search, (2) tag recommendation, and (3) group recommendation, which demonstrates that the proposed tag ranking approach really boosts the performances of social-tagging related applications.	Tag ranking	NA:NA:NA:NA:NA	2009
Lei Wu:Linjun Yang:Nenghai Yu:Xian-Sheng Hua	Social tagging provides valuable and crucial information for large-scale web image retrieval. It is ontology-free and easy to obtain; however, irrelevant tags frequently appear, and users typically will not tag all semantic objects in the image, which is also called semantic loss. To avoid noises and compensate for the semantic loss, tag recommendation is proposed in literature. However, current recommendation simply ranks the related tags based on the single modality of tag co-occurrence on the whole dataset, which ignores other modalities, such as visual correlation. This paper proposes a multi-modality recommendation based on both tag and visual correlation, and formulates the tag recommendation as a learning problem. Each modality is used to generate a ranking feature, and Rankboost algorithm is applied to learn an optimal combination of these ranking features from different modalities. Experiments on Flickr data demonstrate the effectiveness of this learning-based multi-modality recommendation strategy.	Learning to tag	NA:NA:NA:NA	2009
Shengyue Ji:Guoliang Li:Chen Li:Jianhua Feng	Traditional information systems return answers after a user submits a complete query. Users often feel "left in the dark" when they have limited knowledge about the underlying data, and have to use a try-and-see approach for finding information. A recent trend of supporting autocomplete in these systems is a first step towards solving this problem. In this paper, we study a new information-access paradigm, called "interactive, fuzzy search," in which the system searches the underlying data "on the fly" as the user types in query keywords. It extends autocomplete interfaces by (1) allowing keywords to appear in multiple attributes (in an arbitrary order) of the underlying data; and (2) finding relevant records that have keywords matching query keywords approximately. This framework allows users to explore data as they type, even in the presence of minor errors. We study research challenges in this framework for large amounts of data. Since each keystroke of the user could invoke a query on the backend, we need efficient algorithms to process each query within milliseconds. We develop various incremental-search algorithms using previously computed and cached results in order to achieve an interactive speed. We have deployed several real prototypes using these techniques. One of them has been deployed to support interactive search on the UC Irvine people directory, which has been used regularly and well received by users due to its friendly interface and high efficiency.	Efficient interactive fuzzy keyword search	NA:NA:NA:NA	2009
Sreenivas Gollapudi:Aneesh Sharma	Understanding user intent is key to designing an effective ranking system in a search engine. In the absence of any explicit knowledge of user intent, search engines want to diversify results to improve user satisfaction. In such a setting, the probability ranking principle-based approach of presenting the most relevant results on top can be sub-optimal, and hence the search engine would like to trade-off relevance for diversity in the results. In analogy to prior work on ranking and clustering systems, we use the axiomatic approach to characterize and design diversification systems. We develop a set of natural axioms that a diversification system is expected to satisfy, and show that no diversification function can satisfy all the axioms simultaneously. We illustrate the use of the axiomatic framework by providing three example diversification objectives that satisfy different subsets of the axioms. We also uncover a rich link to the facility dispersion problem that results in algorithms for a number of diversification objectives. Finally, we propose an evaluation methodology to characterize the objectives and the underlying axioms. We conduct a large scale evaluation of our objectives based on two data sets: a data set derived from the Wikipedia disambiguation pages and a product database.	An axiomatic approach for result diversification	NA:NA	2009
Deepayan Chakrabarti:Ravi Kumar:Kunal Punera	Quicklinks for a website are navigational shortcuts displayed below the website homepage on a search results page, and that let the users directly jump to selected points inside the website. Since the real-estate on a search results page is constrained and valuable, picking the best set of quicklinks to maximize the benefits for a majority of the users becomes an important problem for search engines. Using user browsing trails obtained from browser toolbars, and a simple probabilistic model, we formulate the quicklink selection problem as a combinatorial optimizaton problem. We first demonstrate the hardness of the objective, and then propose an algorithm that is provably within a factor of 1-1/e of the optimal. We also propose a different algorithm that works on trees and that can find the optimal solution; unlike the previous algorithm, this algorithm can incorporate natural constraints on the set of chosen quicklinks. The efficacy of our methods is demonstrated via empirical results on both a manually labeled set of websites and a set for which quicklink click-through rates for several webpages were obtained from a real-world search engine.	Quicklink selection for navigational query results	NA:NA:NA	2009
Hao Yan:Shuai Ding:Torsten Suel	Web search engines use highly optimized compression schemes to decrease inverted index size and improve query throughput, and many index compression techniques have been studied in the literature. One approach taken by several recent studies first performs a renumbering of the document IDs in the collection that groups similar documents together, and then applies standard compression techniques. It is known that this can significantly improve index compression compared to a random document ordering. We study index compression and query processing techniques for such reordered indexes. Previous work has focused on determining the best possible ordering of documents. In contrast, we assume that such an ordering is already given, and focus on how to optimize compression methods and query processing for this case. We perform an extensive study of compression techniques for document IDs and present new optimizations of existing techniques which can achieve significant improvement in both compression and decompression performances. We also propose and evaluate techniques for compressing frequency values for this case. Finally, we study the effect of this approach on query processing performance. Our experiments show very significant improvements in index size and query processing speed on the TREC GOV2 collection of 25.2 million web pages.	Inverted index compression and query processing with optimized document ordering	NA:NA:NA	2009
Jay Chen:Lakshminarayanan Subramanian:Jinyang Li	The majority of people in rural developing regions do not have access to the World Wide Web. Traditional network connectivity technologies have proven to be prohibitively expensive in these areas. The emergence of new long-range wireless technologies provide hope for connecting these rural regions to the Internet. However, the network connectivity provided by these new solutions are by nature intermittent due to high network usage rates, frequent power-cuts and the use of delay tolerant links. Typical applications, especially interactive applications like web search, do not tolerate intermittent connectivity. In this paper, we present the design and implementation of RuralCafe, a system intended to support efficient web search over intermittent networks. RuralCafe enables users to perform web search asynchronously and find what they are looking for in one round of intermittency as opposed to multiple rounds of search/downloads. RuralCafe does this by providing an expanded search query interface which allows a user to specify additional query terms to maximize the utility of the results returned by a search query. Given knowledge of the limited available network resources, RuralCafe performs optimizations to prefetch pages to best satisfy a search query based on a user's search preferences. In addition, RuralCafe does not require modifications to the web browser, and can provide single round search results tailored to various types of networks and economic constraints. We have implemented and evaluated the effectiveness of RuralCafe using queries from logs made to a large search engine, queries made by users in an intermittent setting, and live queries from a small testbed deployment. We have also deployed a prototype of RuralCafe in Kerala, India.	RuralCafe: web search in the rural developing world	NA:NA:NA	2009
Shuai Ding:Jinru He:Hao Yan:Torsten Suel	Web search engines are facing formidable performance challenges due to data sizes and query loads. The major engines have to process tens of thousands of queries per second over tens of billions of documents. To deal with this heavy workload, such engines employ massively parallel systems consisting of thousands of machines. The significant cost of operating these systems has motivated a lot of recent research into more efficient query processing mechanisms. We investigate a new way to build such high performance IR systems using graphical processing units (GPUs). GPUs were originally designed to accelerate computer graphics applications through massive on-chip parallelism. Recently a number of researchers have studied how to use GPUs for other problem domains such as databases and scientific computing. Our contribution here is to design a basic system architecture for GPU-based high-performance IR, to develop suitable algorithms for subtasks such as inverted list compression, list intersection, and top-$k$ scoring, and to show how to achieve highly efficient query processing on GPU-based systems. Our experimental results for a prototype GPU-based system on $25.2$ million web pages indicate that significant gains in query processing performance can be obtained.	Using graphics processors for high performance IR query processing	NA:NA:NA:NA	2009
Qingqing Gan:Torsten Suel	Query processing is a major cost factor in operating large web search engines. In this paper, we study query result caching, one of the main techniques used to optimize query processing performance. Our first contribution is a study of result caching as a weighted caching problem. Most previous work has focused on optimizing cache hit ratios, but given that processing costs of queries can vary very significantly we argue that total cost savings also need to be considered. We describe and evaluate several algorithms for weighted result caching, and study the impact of Zipf-based query distributions on result caching. Our second and main contribution is a new set of feature-based cache eviction policies that achieve significant improvements over all previous methods, substantially narrowing the existing performance gap to the theoretically optimal (clairvoyant) method. Finally, using the same approach, we also obtain performance gains for the related problem of inverted list caching.	Improved techniques for result caching in web search engines	NA:NA	2009
Sandeep Pandey:Andrei Broder:Flavio Chierichetti:Vanja Josifovski:Ravi Kumar:Sergei Vassilvitskii	Motivated by contextual advertising systems and other web applications involving efficiency-accuracy tradeoffs, we study similarity caching. Here, a cache hit is said to occur if the requested item is similar but not necessarily equal to some cached item. We study two objectives that dictate the efficiency-accuracy tradeoff and provide our caching policies for these objectives. By conducting extensive experiments on real data we show similarity caching can significantly improve the efficiency of contextual advertising systems, with minimal impact on accuracy. Inspired by the above, we propose a simple generative model that embodies two fundamental characteristics of page requests arriving to advertising systems, namely, long-range dependences and similarities. We provide theoretical bounds on the gains of similarity caching in this model and demonstrate these gains empirically by fitting the actual data to the model.	Nearest-neighbor caching for content-match applications	NA:NA:NA:NA:NA:NA	2009
Flavio Chierichetti:Ravi Kumar:Prabhakar Raghavan	Web search engines use indexes to efficiently retrieve pages containing specified query terms, as well as pages linking to specified pages. The problem of compressed indexes that permit such fast retrieval has a long history. We consider the problem: assuming that the terms in (or links to) a page are generated from a probability distribution, how well compactly can we build such indexes that allow fast retrieval? Of particular interest is the case when the probability distribution is Zipfian (or a similar power law), since these are the distributions that arise on the web. We obtain sharp bounds on the space requirement of Boolean indexes for text documents that follow Zipf's law. In the process we develop a general technique that applies to any probability distribution, not necessarily a power law; this is the first analysis of compression in indexes under arbitrary distributions. Our bounds lead to quantitative versions of rules of thumb that are folklore in indexing. Our experiments on several document collections show that the distribution of terms appears to follow a double-Pareto law rather than Zipf's law. Despite widely varying sets of documents, the index sizes observed in the experiments conform well to our theoretical predictions.	Compressed web indexes	NA:NA:NA	2009
Eustache Diemert:Gilles Vandelle	Automatic categorization of user queries is an important component of general purpose (Web) search engines, particularly for triggering rich, query-specific content and sponsored links. We propose an unsupervised learning scheme that reduces dramatically the cost of setting up and maintaining such a categorizer, while retaining good categorization power. The model is stored as a graph of concepts where graph edges represent the cross-reference between the concepts. Concepts and relations are extracted from query logs by an offline Web mining process, which uses a search engine as a powerful summarizer for building a concept graph. Empirical evaluation indicates that the system compares favorably on publicly available data sets (such as KDD Cup 2005) as well as on portions of the current query stream of Yahoo! Search, where it is already changing the experience of millions of Web search users.	Unsupervised query categorization using automatically-built concept graphs	NA:NA	2009
Jian Hu:Gang Wang:Fred Lochovsky:Jian-tao Sun:Zheng Chen	Understanding the intent behind a user's query can help search engine to automatically route the query to some corresponding vertical search engines to obtain particularly relevant contents, thus, greatly improving user satisfaction. There are three major challenges to the query intent classification problem: (1) Intent representation; (2) Domain coverage and (3) Semantic interpretation. Current approaches to predict the user's intent mainly utilize machine learning techniques. However, it is difficult and often requires many human efforts to meet all these challenges by the statistical machine learning approaches. In this paper, we propose a general methodology to the problem of query intent classification. With very little human effort, our method can discover large quantities of intent concepts by leveraging Wikipedia, one of the best human knowledge base. The Wikipedia concepts are used as the intent representation space, thus, each intent domain is represented as a set of Wikipedia articles and categories. The intent of any input query is identified through mapping the query into the Wikipedia representation space. Compared with previous approaches, our proposed method can achieve much better coverage to classify queries in an intent domain even through the number of seed intent examples is very small. Moreover, the method is very general and can be easily applied to various intent domains. We demonstrate the effectiveness of this method in three different applications, i.e., travel, job, and person name. In each of the three cases, only a couple of seed intent queries are provided. We perform the quantitative evaluations in comparison with two baseline methods, and the experimental results shows that our method significantly outperforms other methods in each intent domain.	Understanding user's query intent with wikipedia	NA:NA:NA:NA:NA	2009
Xing Yi:Hema Raghavan:Chris Leggetter	Discovering users' specific and implicit geographic intention in web search can greatly help satisfy users' information needs. We build a geo intent analysis system that uses minimal supervision to learn a model from large amounts of web-search logs for this discovery. We build a city language model, which is a probabilistic representation of the language surrounding the mention of a city in web queries. We use several features derived from these language models to: (1) identify users' implicit geo intent and pinpoint the city corresponding to this intent, (2) determine whether the geo-intent is localized around the users' current geographic location, (3) predict cities for queries that have a mention of an entity that is located in a specific place. Experimental results demonstrate the effectiveness of using features derived from the city language model. We find that (1) the system has over 90% precision and more than 74% accuracy for the task of detecting users' implicit city level geo intent (2) the system achieves more than 96% accuracy in determining whether implicit geo queries are local geo queries, neighbor region geo queries or none-of-these (3) the city language model can effectively retrieve cities in location-specific queries with high precision (88%) and recall (74%); human evaluation shows that the language model predicts city labels for location-specific queries with high accuracy (84.5%).	Discovering users' specific geo intention in web search	NA:NA:NA	2009
Xuerui Wang:Andrei Broder:Marcus Fontoura:Vanja Josifovski	Contextual advertising (also called content match) refers to the placement of small textual ads within the content of a generic web page. It has become a significant source of revenue for publishers ranging from individual bloggers to major newspapers. At the same time it is an important way for advertisers to reach their intended audience. This reach depends on the total number of exposures of the ad (impressions) and its click-through-rate (CTR) that can be viewed as the probability of an end-user clicking on the ad when shown. These two orthogonal, critical factors are both difficult to estimate and even individually can still be very informative and useful in planning and budgeting advertising campaigns. In this paper, we address the problem of forecasting the number of impressions for new or changed ads in the system. Producing such forecasts, even within large margins of error, is quite challenging: 1) ad selection in contextual advertising is a complicated process based on tens or even hundreds of page and ad features; 2) the publishers' content and traffic vary over time; and 3) the scale of the problem is daunting: over a course of a week it involves billions of impressions, hundreds of millions of distinct pages, hundreds of millions of ads, and varying bids of other competing advertisers. We tackle these complexities by simulating the presence of a given ad with its associated bid over weeks of historical data. We obtain an impression estimate by counting how many times the ad would have been displayed if it were in the system over that period of time. We estimate this count by an efficient two-level search algorithm over the distinct pages in the data set. Experimental results show that our approach can accurately forecast the expected number of impressions of contextual ads in real time. We also show how this method can be used in tools for bid selection and ad evaluation.	A search-based method for forecasting ad impression in contextual advertising	NA:NA:NA:NA	2009
Sanjay Agrawal:Kaushik Chakrabarti:Surajit Chaudhuri:Venkatesh Ganti:Arnd Christian Konig:Dong Xin	Web search engines often federate many user queries to relevant structured databases. For example, a product related query might be federated to a product database containing their descriptions and specifications. The relevant structured data items are then returned to the user along with web search results. However, each structured database is searched in isolation. Hence, the search often produces empty or incomplete results as the database may not contain the required information to answer the query. In this paper, we propose a novel integrated search architecture. We establish and exploit the relationships between web search results and the items in structured databases to identify the relevant structured data items for a much wider range of queries.Our architecture leverages existing search engine components to implement this functionality at very low overhead. We demonstrate the quality and efficiency of our techniques through an extensive experimental study.	Exploiting web search engines to search structured databases	NA:NA:NA:NA:NA:NA	2009
Andrei Broder:Peter Ciccolo:Evgeniy Gabrilovich:Vanja Josifovski:Donald Metzler:Lance Riedel:Jeffrey Yuan	Sponsored search systems are tasked with matching queries to relevant advertisements. The current state-of-the-art matching algorithms expand the user's query using a variety of external resources, such as Web search results. While these expansion-based algorithms are highly effective, they are largely inefficient and cannot be applied in real-time. In practice, such algorithms are applied offline to popular queries, with the results of the expensive operations cached for fast access at query time. In this paper, we describe an efficient and effective approach for matching ads against rare queries that were not processed offline. The approach builds an expanded query representation by leveraging offline processing done for related popular queries. Our experimental results show that our approach significantly improves the effectiveness of advertising on rare queries with only a negligible increase in computational cost.	Online expansion of rare queries for sponsored search	NA:NA:NA:NA:NA:NA:NA	2009
Anna Cinzia Squicciarini:Mohamed Shehab:Federica Paci	Social Networking is one of the major technological phenomena of the Web 2.0, with hundreds of millions of people participating. Social networks enable a form of self expression for users, and help them to socialize and share content with other users. In spite of the fact that content sharing represents one of the prominent features of existing Social Network sites, Social Networks yet do not support any mechanism for collaborative management of privacy settings for shared content. In this paper, we model the problem of collaborative enforcement of privacy policies on shared data by using game theory. In particular, we propose a solution that offers automated ways to share images based on an extended notion of content ownership. Building upon the Clarke-Tax mechanism, we describe a simple mechanism that promotes truthfulness, and that rewards users who promote co-ownership. We integrate our design with inference techniques that free the users from the burden of manually selecting privacy preferences for each picture. To the best of our knowledge this is the first time such a protection mechanism for Social Networking has been proposed. In the paper, we also show a proof-of-concept application, which we implemented in the context of Facebook, one of today's most popular social networks. We show that supporting these type of solutions is not also feasible, but can be implemented through a minimal increase in overhead to end-users.	Collective privacy management in social networks	NA:NA:NA	2009
Elena Zheleva:Lise Getoor	In order to address privacy concerns, many social media websites allow users to hide their personal profiles from the public. In this work, we show how an adversary can exploit an online social network with a mixture of public and private user profiles to predict the private attributes of users. We map this problem to a relational classification problem and we propose practical models that use friendship and group membership information (which is often not hidden) to infer sensitive attributes. The key novel idea is that in addition to friendship links, groups can be carriers of significant information. We show that on several well-known social media sites, we can easily and accurately recover the information of private-profile users. To the best of our knowledge, this is the first work that uses link-based and group-based classification to study privacy implications in social networks with mixed public and private user profiles.	To join or not to join: the illusion of privacy in social networks with mixed public and private user profiles	NA:NA	2009
Balachander Krishnamurthy:Craig Wills	For the last few years we have been studying the diffusion of private information for users as they visit various Web sites triggering data gathering aggregation by third parties. This paper reports on our longitudinal study consisting of multiple snapshots of our examination of such diffusion over four years. We examine the various technical ways by which third-party aggregators acquire data and the depth of user-related information acquired. We study techniques for protecting privacy diffusion as well as limitations of such techniques. We introduce the concept of secondary privacy damage. Our results show increasing aggregation of user-related data by a steadily decreasing number of entities. A handful of companies are able to track users' movement across almost all of the popular Web sites. Virtually all the protection techniques have significant limitations highlighting the seriousness of the problem and the need for alternate solutions.	Privacy diffusion on the web: a longitudinal perspective	NA:NA	2009
Leyla Bilge:Thorsten Strufe:Davide Balzarotti:Engin Kirda	Social networking sites have been increasingly gaining popularity. Well-known sites such as Facebook have been reporting growth rates as high as 3% per week. Many social networking sites have millions of registered users who use these sites to share photographs, contact long-lost friends, establish new business contacts and to keep in touch. In this paper, we investigate how easy it would be for a potential attacker to launch automated crawling and identity theft attacks against a number of popular social networking sites in order to gain access to a large volume of personal user information. The first attack we present is the automated identity theft of existing user profiles and sending of friend requests to the contacts of the cloned victim. The hope, from the attacker's point of view, is that the contacted users simply trust and accept the friend request. By establishing a friendship relationship with the contacts of a victim, the attacker is able to access the sensitive personal information provided by them. In the second, more advanced attack we present, we show that it is effective and feasible to launch an automated, cross-site profile cloning attack. In this attack, we are able to automatically create a forged profile in a network where the victim is not registered yet and contact the victim's friends who are registered on both networks. Our experimental results with real users show that the automated attacks we present are effective and feasible in practice.	All your contacts are belong to us: automated identity theft attacks on social networks	NA:NA:NA:NA	2009
Arjun Guha:Shriram Krishnamurthi:Trevor Jim	We present a static control-flow analysis for JavaScript programs running in a web browser. Our analysis tackles numerous challenges posed by modern web applications including asynchronous communication, frameworks, and dynamic code generation. We use our analysis to extract a model of expected client behavior as seen from the server, and build an intrusion-prevention proxy for the server: the proxy intercepts client requests and disables those that do not meet the expected behavior. We insert random asynchronous requests to foil mimicry attacks. Finally, we evaluate our technique against several real applications and show that it protects against an attack in a widely-used web application.	Using static analysis for Ajax intrusion detection	NA:NA:NA	2009
Guang Xiang:Jason I. Hong	Phishing is a significant security threat to the Internet, which causes tremendous economic loss every year. In this paper, we proposed a novel hybrid phish detection method based on information extraction (IE) and information retrieval (IR) techniques. The identity-based component of our method detects phishing webpages by directly discovering the inconsistency between their identity and the identity they are imitating. The keywords-retrieval component utilizes IR algorithms exploiting the power of search engines to identify phish. Our method requires no training data, no prior knowledge of phishing signatures and specific implementations, and thus is able to adapt quickly to constantly appearing new phishing patterns. Comprehensive experiments over a diverse spectrum of data sources with 11449 pages show that both components have a low false positive rate and the stacked approach achieves a true positive rate of 90.06% with a false positive rate of 1.95%.	A hybrid phish detection approach by identity discovery and keywords retrieval	NA:NA	2009
Danh Le-Phuoc:Axel Polleres:Manfred Hauswirth:Giovanni Tummarello:Christian Morbidoni	The use of RDF data published on the Web for applications is still a cumbersome and resource-intensive task due to the limited software support and the lack of standard programming paradigms to deal with everyday problems such as combination of RDF data from dierent sources, object identifier consolidation, ontology alignment and mediation, or plain querying and filtering tasks. In this paper we present a framework, Semantic Web Pipes, that supports fast implementation of Semantic data mash-ups while preserving desirable properties such as abstraction, encapsulation, component-orientation, code re-usability and maintainability which are common and well supported in other application areas.	Rapid prototyping of semantic mash-ups through semantic web pipes	NA:NA:NA:NA:NA	2009
Philippe Cudré-Mauroux:Parisa Haghani:Michael Jost:Karl Aberer:Hermann De Meer	We tackle the problem of disambiguating entities on the Web. We propose a user-driven scheme where graphs of entities -- represented by globally identifiable declarative artifacts -- self-organize in a dynamic and probabilistic manner. Our solution has the following two desirable properties: i) it lets end-users freely define associations between arbitrary entities and ii) it probabilistically infers entity relationships based on uncertain links using constraint-satisfaction mechanisms. We outline the interface between our scheme and the current data Web, and show how higher-layer applications can take advantage of our approach to enhance search and update of information relating to online entities. We describe a decentralized infrastructure supporting efficient and scalable entity disambiguation and demonstrate the practicability of our approach in a deployment over several hundreds of machines.	idMesh: graph-based disambiguation of linked data	NA:NA:NA:NA:NA	2009
Senlin Liang:Paul Fodor:Hui Wan:Michael Kifer	The Semantic Web initiative has led to an upsurge of the interest in rules as a general and powerful way of processing, combining, and analyzing semantic information. Since several of the technologies underlying rule-based systems are already quite mature, it is important to understand how such systems might perform on the Web scale. OpenRuleBench is a suite of benchmarks for analyzing the performance and scalability of different rule engines. Currently the study spans five different technologies and eleven systems, but OpenRuleBench is an open community resource, and contributions from the community are welcome. In this paper, we describe the tested systems and technologies, the methodology used in testing, and analyze the results.	OpenRuleBench: an analysis of the performance of rule engines	NA:NA:NA:NA	2009
Jorge Gracia:Mathieu d'Aquin:Eduardo Mena	Nowadays, the increasing amount of semantic data available on the Web leads to a new stage in the potential of Semantic Web applications. However, it also introduces new issues due to the heterogeneity of the available semantic resources. One of the most remarkable is redundancy, that is, the excess of different semantic descriptions, coming from different sources, to describe the same intended meaning. In this paper, we propose a technique to perform a large scale integration of senses (expressed as ontology terms), in order to cluster the most similar ones, when indexing large amounts of online semantic information. It can dramatically reduce the redundancy problem on the current Semantic Web. In order to make this objective feasible, we have studied the adaptability and scalability of our previous work on sense integration, to be translated to the much larger scenario of the Semantic Web. Our evaluation shows a good behaviour of these techniques when used in large scale experiments, then making feasible the proposed approach.	Large scale integration of senses for the semantic web	NA:NA:NA	2009
Sören Auer:Sebastian Dietzold:Jens Lehmann:Sebastian Hellmann:David Aumueller	In this paper we present Triplify - a simplistic but effective approach to publish Linked Data from relational databases. Triplify is based on mapping HTTP-URI requests onto relational database queries. Triplify transforms the resulting relations into RDF statements and publishes the data on the Web in various RDF serializations, in particular as Linked Data. The rationale for developing Triplify is that the largest part of information on the Web is already stored in structured form, often as data contained in relational databases, but usually published by Web applications only as HTML mixing structure, layout and content. In order to reveal the pure structured information behind the current Web, we have implemented Triplify as a light-weight software component, which can be easily integrated into and deployed by the numerous, widely installed Web applications. Our approach includes a method for publishing update logs to enable incremental crawling of linked data sources. Triplify is complemented by a library of configurations for common relational schemata and a REST-enabled data source registry. Triplify configurations containing mappings are provided for many popular Web applications, including osCommerce, WordPress, Drupal, Gallery, and phpBB. We will show that despite its light-weight architecture Triplify is usable to publish very large datasets, such as 160GB of geo data from the OpenStreetMap project.	Triplify: light-weight linked data publication from relational databases	NA:NA:NA:NA:NA	2009
Fabian M. Suchanek:Mauro Sozio:Gerhard Weikum	This paper presents SOFIE, a system for automated ontology extension. SOFIE can parse natural language documents, extract ontological facts from them and link the facts into an ontology. SOFIE uses logical reasoning on the existing knowledge and on the new knowledge in order to disambiguate words to their most probable meaning, to reason on the meaning of text patterns and to take into account world knowledge axioms. This allows SOFIE to check the plausibility of hypotheses and to avoid inconsistencies with the ontology. The framework of SOFIE unites the paradigms of pattern matching, word sense disambiguation and ontological reasoning in one unified model. Our experiments show that SOFIE delivers high-quality output, even from unstructured Internet documents.	SOFIE: a self-organizing framework for information extraction	NA:NA:NA	2009
Benjamin Markines:Ciro Cattuto:Filippo Menczer:Dominik Benz:Andreas Hotho:Gerd Stumme	Social bookmarking systems are becoming increasingly important data sources for bootstrapping and maintaining Semantic Web applications. Their emergent information structures have become known as folksonomies. A key question for harvesting semantics from these systems is how to extend and adapt traditional notions of similarity to folksonomies, and which measures are best suited for applications such as community detection, navigation support, semantic search, user profiling and ontology learning. Here we build an evaluation framework to compare various general folksonomy-based similarity measures, which are derived from several established information-theoretic, statistical, and practical measures. Our framework deals generally and symmetrically with users, tags, and resources. For evaluation purposes we focus on similarity between tags and between resources and consider different methods to aggregate annotations across users. After comparing the ability of several tag similarity measures to predict user-created tag relations, we provide an external grounding by user-validated semantic proxies based on WordNet and the Open Directory Project. We also investigate the issue of scalability. We find that mutual information with distributional micro-aggregation across users yields the highest accuracy, but is not scalable; per-user projection with collaborative aggregation provides the best scalable approach via incremental computations. The results are consistent across resource and tag similarity.	Evaluating similarity measures for emergent semantics of social tagging	NA:NA:NA:NA:NA:NA	2009
Danushka T. Bollegala:Yutaka Matsuo:Mitsuru Ishizuka	Measuring the similarity between semantic relations that hold among entities is an important and necessary step in various Web related tasks such as relation extraction, information retrieval and analogy detection. For example, consider the case in which a person knows a pair of entities (e.g. Google, YouTube), between which a particular relation holds (e.g. acquisition). The person is interested in retrieving other such pairs with similar relations (e.g. Microsoft, Powerset). Existing keyword-based search engines cannot be applied directly in this case because, in keyword-based search, the goal is to retrieve documents that are relevant to the words used in a query -- not necessarily to the relations implied by a pair of words. We propose a relational similarity measure, using a Web search engine, to compute the similarity between semantic relations implied by two pairs of words. Our method has three components: representing the various semantic relations that exist between a pair of words using automatically extracted lexical patterns, clustering the extracted lexical patterns to identify the different patterns that express a particular semantic relation, and measuring the similarity between semantic relations using a metric learning approach. We evaluate the proposed method in two tasks: classifying semantic relations between named entities, and solving word-analogy questions. The proposed method outperforms all baselines in a relation classification task with a statistically significant average precision score of 0.74. Moreover, it reduces the time taken by Latent Relational Analysis to process 374 word-analogy questions from 9 days to less than 6 hours, with an SAT score of 51%.	Measuring the similarity between implicit semantic relations from the web	NA:NA:NA	2009
Maria Grineva:Maxim Grinev:Dmitry Lizorkin	We present a novel method for key term extraction from text documents. In our method, document is modeled as a graph of semantic relationships between terms of that document. We exploit the following remarkable feature of the graph: the terms related to the main topics of the document tend to bunch up into densely interconnected subgraphs or communities, while non-important terms fall into weakly interconnected communities, or even become isolated vertices. We apply graph community detection techniques to partition the graph into thematically cohesive groups of terms. We introduce a criterion function to select groups that contain key terms discarding groups with unimportant terms. To weight terms and determine semantic relatedness between them we exploit information extracted from Wikipedia. Using such an approach gives us the following two advantages. First, it allows effectively processing multi-theme documents. Second, it is good at filtering out noise information in the document, such as, for example, navigational bars or headers in web pages. Evaluations of the method show that it outperforms existing methods producing key terms with higher precision and recall. Additional experiments on web pages prove that our method appears to be substantially more effective on noisy and multi-theme documents than existing methods.	Extracting key terms from noisy and multitheme documents	NA:NA:NA	2009
Shilad Sen:Jesse Vig:John Riedl	Tagging has emerged as a powerful mechanism that enables users to find, organize, and understand online entities. Recommender systems similarly enable users to efficiently navigate vast collections of items. Algorithms combining tags with recommenders may deliver both the automation inherent in recommenders, and the flexibility and conceptual comprehensibility inherent in tagging systems. In this paper we explore tagommenders, recommender algorithms that predict users' preferences for items based on their inferred preferences for tags. We describe tag preference inference algorithms based on users' interactions with tags and movies, and evaluate these algorithms based on tag preference ratings collected from 995 MovieLens users. We design and evaluate algorithms that predict users' ratings for movies based on their inferred tag preferences. Our tag-based algorithms generate better recommendation rankings than state-of-the-art algorithms, and they may lead to flexible recommender systems that leverage the characteristics of items users find most important.	Tagommenders: connecting users to items through tags	NA:NA:NA	2009
Wen-Yen Chen:Jon-Chyuan Chu:Junyi Luan:Hongjie Bai:Yi Wang:Edward Y. Chang	Users of social networking services can connect with each other by forming communities for online interaction. Yet as the number of communities hosted by such websites grows over time, users have even greater need for effective community recommendations in order to meet more users. In this paper, we investigate two algorithms from very different domains and evaluate their effectiveness for personalized community recommendation. First is association rule mining (ARM), which discovers associations between sets of communities that are shared across many users. Second is latent Dirichlet allocation (LDA), which models user-community co-occurrences using latent aspects. In comparing LDA with ARM, we are interested in discovering whether modeling low-rank latent structure is more effective for recommendations than directly mining rules from the observed data. We experiment on an Orkut data set consisting of 492,104 users and 118,002 communities. Our empirical comparisons using the top-k recommendations metric show that LDA performs consistently better than ARM for the community recommendation task when recommending a list of 4 or more communities. However, for recommendation lists of up to 3 communities, ARM is still a bit better. We analyze examples of the latent information learned by LDA to explain this finding. To efficiently handle the large-scale data set, we parallelize LDA on distributed computers and demonstrate our parallel implementation's scalability with varying numbers of machines.	Collaborative filtering for orkut communities: discovery of user latent behavior	NA:NA:NA:NA:NA:NA	2009
Wei Chu:Seung-Taek Park	In Web-based services of dynamic content (such as news articles), recommender systems face the difficulty of timely identifying new items of high-quality and providing recommendations for new users. We propose a feature-based machine learning approach to personalized recommendation that is capable of handling the cold-start issue effectively. We maintain profiles of content of interest, in which temporal characteristics of the content, e.g. popularity and freshness, are updated in real-time manner. We also maintain profiles of users including demographic information and a summary of user activities within Yahoo! properties. Based on all features in user and content profiles, we develop predictive bilinear regression models to provide accurate personalized recommendations of new items for both existing and new users. This approach results in an offline model with light computational overhead compared with other recommender systems that require online re-training. The proposed framework is general and flexible for other personalized tasks. The superior performance of our approach is verified on a large-scale data set collected from the Today-Module on Yahoo! Front Page, with comparison against six competitive approaches.	Personalized recommendation on dynamic content using predictive bilinear models	NA:NA	2009
Sharad Goel:Roby Muhamad:Duncan Watts	The "algorithmic small-world hypothesis" states that not only are pairs of individuals in a large social network connected by short paths, but that ordinary individuals can find these paths. Although theoretically plausible, empirical evidence for the hypothesis is limited, as most chains in "small-world" experiments fail to complete, thereby biasing estimates of "true" chain lengths. Using data from two recent small-world experiments, comprising a total of 162,328 message chains, and directed at one of 30 "targets" spread across 19 countries, we model heterogeneity in chain attrition rates as a function of individual attributes. We then introduce a rigorous way of estimating true chain lengths that is provably unbiased, and can account for empirically-observed variation in attrition rates. Our findings provide mixed support for the algorithmic hypothesis. On the one hand, it appears that roughly half of all chains can be completed in 6-7 steps--thus supporting the "six degrees of separation" assertion--but on the other hand, estimates of the mean are much longer, suggesting that for at least some of the population, the world is not "small" in the algorithmic sense. We conclude that search distances in social networks are fundamentally different from topological distances, for which the mean and median of the shortest path lengths between nodes tend to be similar.	Social search in "Small-World" experiments	NA:NA:NA	2009
Thomas Karagiannis:Milan Vojnovic	We examine the behavioral patterns of email usage in a large-scale enterprise over a three-month period. In particular, we focus on two main questions: (Q1) what do replies depend on? and (Q2) what is the gain of augmenting contacts through the friends of friends from the email social graph? For Q1, we identify and evaluate the significance of several factors that affect the reply probability and the email response time. We find that all factors of our considered set are significant, provide their relative ordering, and identify the recipient list size, and the intensity of email communication between the correspondents as the dominant factors. We highlight various novel threshold behaviors and provide support for existing hypotheses such as that of the least-effort reply. For Q2, we find that the number of new contacts extracted from the friends-of-friends relationships amounts to a large number, but which is still a limited portion of the total enterprise size. We believe that our results provide significant insights towards informed design of advanced email features, including those of social-networking type.	Behavioral profiles for advanced email features	NA:NA	2009
Meeyoung Cha:Alan Mislove:Krishna P. Gummadi	Online social networking sites like MySpace, Facebook, and Flickr have become a popular way to share and disseminate content. Their massive popularity has led to viral marketing techniques that attempt to spread content, products, and ideas on these sites. However, there is little data publicly available on viral propagation in the real world and few studies have characterized how information spreads over current online social networks. In this paper, we collect and analyze large-scale traces of information dissemination in the Flickr social network. Our analysis, based on crawls of the favorite markings of 2.5 million users on 11 million photos, aims at answering three key questions: (a) how widely does information propagate in the social network? (b) how quickly does information propagate? and (c) what is the role of word-of-mouth exchanges between friends in the overall propagation of information in the network? Contrary to viral marketing ``intuition,'' we find that (a) even popular photos do not spread widely throughout the network, (b) even popular photos spread slowly through the network, and (c) information exchanged between friends is likely to account for over 50 of all favorite-markings, but with a significant delay at each hop.	A measurement-driven analysis of information propagation in the flickr social network	NA:NA:NA	2009
Ulrik Brandes:Patrick Kenis:Jürgen Lerner:Denise van Raaij	In this paper we give models and algorithms to describe and analyze the collaboration among authors of Wikipedia from a network analytical perspective. The edit network encodes who interacts how with whom when editing an article; it significantly extends previous network models that code author communities in Wikipedia. Several characteristics summarizing some aspects of the organization process and allowing the analyst to identify certain types of authors can be obtained from the edit network. Moreover, we propose several indicators characterizing the global network structure and methods to visualize edit networks. It is shown that the structural network indicators are correlated with quality labels of the associated Wikipedia articles.	Network analysis of collaboration structure in Wikipedia	NA:NA:NA:NA	2009
Jérôme Kunegis:Andreas Lommatzsch:Christian Bauckhage	We analyse the corpus of user relationships of the Slashdot technology news site. The data was collected from the Slashdot Zoo feature where users of the website can tag other users as friends and foes, providing positive and negative endorsements. We adapt social network analysis techniques to the problem of negative edge weights. In particular, we consider signed variants of global network characteristics such as the clustering coefficient, node-level characteristics such as centrality and popularity measures, and link-level characteristics such as distances and similarity measures. We evaluate these measures on the task of identifying unpopular users, as well as on the task of predicting the sign of links and show that the network exhibits multiplicative transitivity which allows algebraic methods based on matrix multiplication to be used. We compare our methods to traditional methods which are only suitable for positively weighted edges.	The slashdot zoo: mining a social network with negative edges	NA:NA:NA	2009
Yutaka Matsuo:Hikaru Yamamoto	Several attempts have been made to analyze customer behavior on online E-commerce sites. Some studies particularly emphasize the social networks of customers. Users' reviews and ratings of a product exert effects on other consumers' purchasing behavior. Whether a user refers to other users' ratings depends on the trust accorded by a user to the reviewer. On the other hand, the trust that is felt by a user for another user correlates with the similarity of two users' ratings. This bidirectional interaction that involves trust and rating is an important aspect of understanding consumer behavior in online communities because it suggests clustering of similar users and the evolution of strong communities. This paper presents a theoretical model along with analyses of an actual online E-commerce site. We analyzed a large community site in Japan: @cosme. The noteworthy characteristics of @cosme are that users can bookmark their trusted users; in addition, they can post their own ratings of products, which facilitates our analyses of the ratings' bidirectional effects on trust and ratings. We describe an overview of the data in @cosme, analyses of effects from trust to rating and vice versa, and our proposition of a measure of community gravity, which measures how strongly a user might be attracted to a community. Our study is based on the @cosme dataset in addition to the Epinions dataset. It elucidates important insights and proposes a potentially important measure for mining online social networks.	Community gravity: measuring bidirectional effects by trust and rating on online social networks	NA:NA	2009
David J. Crandall:Lars Backstrom:Daniel Huttenlocher:Jon Kleinberg	We investigate how to organize a large collection of geotagged photos, working with a dataset of about 35 million images collected from Flickr. Our approach combines content analysis based on text tags and image data with structural analysis based on geospatial data. We use the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places. We then study the interplay between this structure and the content, using classification methods for predicting such locations from visual, textual and temporal features of the photos. We find that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features. We illustrate using these techniques to organize a large photo collection, while also revealing various interesting properties about popular cities and landmarks at a global scale.	Mapping the world's photos	NA:NA:NA:NA	2009
Jose San Pedro:Stefan Siersdorfer	Web 2.0 applications like Flickr, YouTube, or Del.icio.us are increasingly popular online communities for creating, editing and sharing content. The growing size of these folksonomies poses new challenges in terms of search and data mining. In this paper we introduce a novel methodology for automatically ranking and classifying photos according to their attractiveness for folksonomy members. To this end, we exploit image features known for having significant effects on the visual quality perceived by humans (e.g. sharpness and colorfulness) as well as textual meta data, in what is a multi-modal approach. Using feedback and annotations available in the Web 2.0 photo sharing system Flickr, we assign relevance values to the photos and train classification and regression models based on these relevance assignments. With the resulting machine learning models we categorize and rank photos according to their attractiveness. Applications include enhanced ranking functions for search and recommender methods for attractive content. Large scale experiments on a collection of Flickr photos demonstrate the viability of our approach.	Ranking and classifying attractiveness of photos in folksonomies	NA:NA	2009
Anon Plangprasopchok:Kristina Lerman	Automatic folksonomy construction from tags has attracted much attention recently. However, inferring hierarchical relations between concepts from tags has a drawback in that it is difficult to distinguish between more popular and more general concepts. Instead of tags we propose to use user-specified relations for learning folksonomy. We explore two statistical frameworks for aggregating many shallow individual hierarchies, expressed through the collection/set relations on the social photosharing site Flickr, into a common deeper folksonomy that reflects how a community organizes knowledge. Our approach addresses a number of challenges that arise while aggregating information from diverse users, namely noisy vocabulary, and variations in the granularity level of the concepts expressed. Our second contribution is a method for automatically evaluating learned folksonomy by comparing it to a reference taxonomy, e.g., the Web directory created by the Open Directory Project. Our empirical results suggest that user-specified relations are a good source of evidence for learning folksonomies.	Constructing folksonomies from user-specified relations on flickr	NA:NA	2009
Yu Zheng:Lizhu Zhang:Xing Xie:Wei-Ying Ma	The increasing availability of GPS-enabled devices is changing the way people interact with the Web, and brings us a large amount of GPS trajectories representing people's location histories. In this paper, based on multiple users' GPS trajectories, we aim to mine interesting locations and classical travel sequences in a given geospatial region. Here, interesting locations mean the culturally important places, such as Tiananmen Square in Beijing, and frequented public areas, like shopping malls and restaurants, etc. Such information can help users understand surrounding locations, and would enable travel recommendation. In this work, we first model multiple individuals' location histories with a tree-based hierarchical graph (TBHG). Second, based on the TBHG, we propose a HITS (Hypertext Induced Topic Search)-based inference model, which regards an individual's access on a location as a directed link from the user to that location. This model infers the interest of a location by taking into account the following three factors. 1) The interest of a location depends on not only the number of users visiting this location but also these users' travel experiences. 2) Users' travel experiences and location interests have a mutual reinforcement relationship. 3) The interest of a location and the travel experience of a user are relative values and are region-related. Third, we mine the classical travel sequences among locations considering the interests of these locations and users' travel experiences. We evaluated our system using a large GPS dataset collected by 107 users over a period of one year in the real world. As a result, our HITS-based inference model outperformed baseline approaches like rank-by-count and rank-by-frequency. Meanwhile, when considering the users' travel experiences and location interests, we achieved a better performance beyond baselines, such as rank-by-count and rank-by-interest, etc.	Mining interesting locations and travel sequences from GPS trajectories	NA:NA:NA:NA	2009
Maryam Kamvar:Melanie Kellar:Rajan Patel:Ya Xu	We present a logs-based comparison of search patterns across three platforms: computers, iPhones and conventional mobile phones. Our goal is to understand how mobile search users differ from computer-based search users, and we focus heavily on the distribution and variability of tasks that users perform from each platform. The results suggest that search usage is much more focused for the average mobile user than for the average computer-based user. However, search behavior on high-end phones resembles computer-based search behavior more so than mobile search behavior. A wide variety of implications follow from these findings. First, there is no single search interface which is suitable for all mobile phones. We suggest that for the higher-end phones, a close integration with the standard computer-based interface (in terms of personalization and available feature set) would be beneficial for the user, since these phones seem to be treated as an extension of the users' computer. For all other phones, there is a huge opportunity for personalizing the search experience for the user's "mobile needs", as these users are likely to repeatedly search for a single type of information need on their phone.	Computers and iphones and mobile phones, oh my!: a logs-based comparison of search users on different devices	NA:NA:NA:NA	2009
Yuki Arase:Xing Xie:Manni Duan:Takahiro Hara:Shojiro Nishio	Geographical context is very important for images. Millions of images on the Web have been already assigned latitude and longitude information. Due to the rapid proliferation of such images with geographical context, it is still difficult to effectively search and browse them, since we do not have ways to decide their relevance. In this paper, we focus on the geographical relevance of images, which is defined as to what extent the main objects in an image match landmarks at the location where the image was taken. Recently, researchers have proposed to use game based approaches to label large scale data such as Web images. However, previous works have not shown the quality of collected game logs in detail and how the logs can improve existing applications. To answer these questions, we design and implement a Web-based and multi-player game to collect human knowledge while people are enjoying the game. Then we thoroughly analyze the game logs obtained during a three week study with 147 participants and propose methods to determine the image geographical relevance. In addition, we conduct an experiment to compare our methods with a commercial search engine. Experimental results show that our methods dramatically improve image search relevance. Furthermore, we show that we can derive geographically relevant objects and their salient portion in images, which is valuable for a number of applications such as image location recognition.	A game based approach to assign geographical relevance to web images	NA:NA:NA:NA:NA	2009
Joshua Hailpern:Loretta Guarino-Reid:Richard Boardman:Srinivas Annam	With the advent of Web 2.0 technologies, websites have evolved from static pages to dynamic, interactive Web-based applications with the ability to replicate common desktop functionality. However, for blind and visually impaired individuals who rely upon screen readers, Web 2.0 applications force them to adapt to an inaccessible use model. Many technologies, including WAI-ARIA, AJAX, and improved screen reader support, are rapidly evolving to improve this situation. However, simply combining them does not solve the problems of screen reader users. The main contributions of this paper are two models of interaction for screen reader users, for both traditional websites and Web 2.0 applications. Further contributions are a discussion of accessibility difficulties screen reader users encounter when interacting with Web 2.0 applications, a user workflow design model for improving Web 2.0 accessibility, and a set of design requirements for developers to ease the user's burden and increase accessibility. These models, accessibility difficulties, and design implications are based directly on responses and lessons learned from usability research focusing on Web 2.0 usage and screen reader users. Without the conscious effort of Web engineers and designers, most blind and visually impaired users will shy away from using new Web 2.0 technology in favor of desktop based applications.	Web 2.0: blind to an accessible new world	NA:NA:NA:NA	2009
Cameron Braganza:Kim Marriott:Peter Moulder:Michael Wybrow:Tim Dwyer	The standard layout model used by web browsers is to lay text out in a vertical scroll using a single column. The horizontal-scroll layout model--in which text is laid out in columns whose height is set to that of the browser window and the viewer scrolls horizontally - seems well-suited to multi-column layout on electronic devices. We describe a study that examines how people read and, in particular, the strategies they use for scrolling with these two models when reading large textual documents on a standard computer monitor. We compare usability of the models and evaluate both user preferences and the effect of the model on performance. Also interesting is the description of the browser and its user interface which we used for the study.	Scrolling behaviour with single- and multi-column layout	NA:NA:NA:NA:NA	2009
Rich Gossweiler:Maryam Kamvar:Shumeet Baluja	We present a new CAPTCHA which is based on identifying an image's upright orientation. This task requires analysis of the often complex contents of an image, a task which humans usually perform well and machines generally do not. Given a large repository of images, such as those from a web search result, we use a suite of automated orientation detectors to prune those images that can be automatically set upright easily. We then apply a social feedback mechanism to verify that the remaining images have a human-recognizable upright orientation. The main advantages of our CAPTCHA technique over the traditional text recognition techniques are that it is language-independent, does not require text-entry (e.g. for a mobile device), and employs another domain for CAPTCHA generation beyond character obfuscation. This CAPTCHA lends itself to rapid implementation and has an almost limitless supply of images. We conducted extensive experiments to measure the viability of this technique.	What's up CAPTCHA?: a CAPTCHA based on image orientation	NA:NA:NA	2009
Woralak Kongdenfha:Boualem Benatallah:Julien Vayssière:Régis Saint-Paul:Fabio Casati	The rapid growth of social networking sites and web communities have motivated web sites to expose their APIs to external developers who create mashups by assembling existing functionalities. Current APIs, however, aim toward developers with programming expertise; they are not directly usable by wider class of users who do not have programming background, but would nevertheless like to build their own mashups. To address this need, we propose a spreadsheet-based Web mashups development framework, which enables users to develop mashups in the popular spreadsheet environment. First, we provide a mechanism that makes structured data first class values of spreadsheet cells. Second, we propose a new component model that can be used to develop fairly sophisticated mashups, involving joining data sources and keeping spreadsheet data up to date. Third, to simplify mashup development, we provide a collection of spreadsheet-based mashup patterns that captures common Web data access and spreadsheet presentation functionalities. Users can reuse and customize these patterns to build spreadsheet-based Web mashups instead of developing them from scratch. Fourth, we enable users to manipulate structured data presented on spreadsheet in a drag-and-drop fashion. Finally, we have developed and tested a proof-of-concept prototype to demonstrate the utility of the proposed framework.	Rapid development of spreadsheet-based web mashups	NA:NA:NA:NA:NA	2009
Guiling Wang:Shaohua Yang:Yanbo Han	This paper presents an end-user-oriented programming environment called Mashroom. Major contributions herein include an end-user programming model with an expressive data structure as well as a set of formally-defined mashup operators. The data structure takes advantage of nested table, and maintains the intuitiveness while allowing users to express complex data objects. The mashup operators are visualized with contextual menu and formula bar and can be directly applied on the data. Experiments and case studies reveal that end users have little difficulty in effectively and efficiently using Mashroom to build mashup applications.	Mashroom: end-user mashup programming using nested tables	NA:NA:NA	2009
Jalal Mahmud:Yevgen Borodin:I. V. Ramakrishnan:C. R. Ramakrishnan	Screen readers, the dominant assistive technology used by visually impaired people to access the Web, function by speaking out the content of the screen serially. Using screen readers for conducting online transactions can cause considerable information overload, because transactions, such as shopping and paying bills, typically involve a number of steps spanning several web pages. One can combat this overload by using a transaction model for web accessibility that presents only fragments of web pages that are needed for doing transactions. We can realize such a model by coupling a process automaton, encoding states of a transaction, with concept classifiers that identify page fragments "relevant" to a particular state of the transaction. In this paper we present a fully automated process that synergistically combines several techniques for transforming unlabeled click-stream data generated by transactions into a transactionmodel. These techniques include web content analysis to partition a web page into segments consisting of semantically related content, contextual analysis of data surrounding clickable objects in a page, and machine learning methods, such as clustering of page segments based on contextual analysis, statistical classification, and automata learning. The use of unlabeled click streams in building transaction models has important benefits: (i) visually impaired users do not have to depend on sighted users for creating manually labeled training data to construct the models; (ii) it is possible to mine personalized models from unlabeled transaction click-streams associated with sites that visually impaired users visit regularly; (iii) since unlabeled data is relatively easy to obtain, it is feasible to scale up the construction of domain-specific transaction models (e.g., separate models for shopping, airline reservations, bill payments, etc.); (iv) adjusting the performance of deployed models over timtime with new training data is also doable. We provide preliminary experimental evidence of the practical effectiveness of both domain-specific, as well as personalized accessibility transaction models built using our approach. Finally, this approach is applicable for building transaction models for mobile devices with limited-size displays, as well as for creating wrappers for information extraction from web sites.	Automated construction of web accessibility models from transaction click-streams	NA:NA:NA:NA	2009
Mohammad Alrifai:Thomas Risse	The run-time binding of web services has been recently put forward in order to support rapid and dynamic web service compositions. With the growing number of alternative web services that provide the same functionality but differ in quality parameters, the service composition becomes a decision problem on which component services should be selected such that user's end-to-end QoS requirements (e.g. availability, response time) and preferences (e.g. price) are satisfied. Although very efficient, local selection strategy fails short in handling global QoS requirements. Solutions based on global optimization, on the other hand, can handle global constraints, but their poor performance renders them inappropriate for applications with dynamic and real-time requirements. In this paper we address this problem and propose a solution that combines global optimization with local selection techniques to benefit from the advantages of both worlds. The proposed solution consists of two steps: first, we use mixed integer programming (MIP) to find the optimal decomposition of global QoS constraints into local constraints. Second, we use distributed local selection to find the best web services that satisfy these local constraints. The results of experimental evaluation indicate that our approach significantly outperforms existing solutions in terms of computation time while achieving close-to-optimal results.	Combining global optimization with local selection for efficient QoS-aware service composition	NA:NA	2009
William Conner:Arun Iyengar:Thomas Mikalsen:Isabelle Rouvellou:Klara Nahrstedt	Many reputation management systems have been developed under the assumption that each entity in the system will use a variant of the same scoring function. Much of the previous work in reputation management has focused on providing robustness and improving performance for a given reputation scheme. In this paper, we present a reputation-based trust management framework that supports the synthesis of trust-related feedback from many different entities while also providing each entity with the flexibility to apply different scoring functions over the same feedback data for customized trust evaluations. We also propose a novel scheme to cache trust values based on recent client activity. To evaluate our approach, we implemented our trust management service and tested it on a realistic application scenario in both LAN and WAN distributed environments. Our results indicate that our trust management service can effectively support multiple scoring functions with low overhead and high availability.	A trust management framework for service-oriented environments	NA:NA:NA:NA:NA	2009
Lijun Mei:Zhenyu Zhang:W. K. Chan:T. H. Tse	Regression testing assures the quality of modified service-oriented business applications against unintended changes. However, a typical regression test suite is large in size. Earlier execution of those test cases that may detect failures is attractive. Many existing prioritization techniques order test cases according to their respective coverage of program statements in a previous version of the application. On the other hand, industrial service-oriented business applications are typically written in orchestration languages such as WS-BPEL and integrated with workflow steps and web services via XPath and WSDL. Faults in these artifacts may cause the application to extract wrong data from messages, leading to failures in service compositions. Surprisingly, current regression testing research hardly considers these artifacts. We propose a multilevel coverage model to capture the business process, XPath, and WSDL from the perspective of regression testing. We develop a family of test case prioritization techniques atop the model. Empirical results show that our techniques can achieve significantly higher rates of fault detection than existing techniques.	Test case prioritization for regression testing of service-oriented business applications	NA:NA:NA:NA	2009
Cesare Pautasso:Erik Wilde	Loose coupling is often quoted as a desirable property of systems architectures. One of the main goals of building systems using Web technologies is to achieve loose coupling. However, given the lack of a widely accepted definition of this term, it becomes hard to use coupling as a criterion to evaluate alternative Web technology choices, as all options may exhibit, and claim to provide, some kind of "loose" coupling effects. This paper presents a systematic study of the degree of coupling found in service-oriented systems based on a multi-faceted approach. Thanks to the metric introduced in this paper, coupling is no longer a one-dimensional concept with loose coupling found somewhere in between tight coupling and no coupling. The paper shows how the metric can be applied to real-world examples in order to support and improve the design process of service-oriented systems.	Why is the web loosely coupled?: a multi-faceted metric for service design	NA:NA	2009
Toyotaro Suzumura:Michiaki Tatsubori:Scott Trent:Akihiko Tozawa:Tamiya Onodera	The performance of server-side applications is becoming increasingly important as more applications exploit the Web application model. Extensive work has been done to improve the performance of individual software components such as Web servers and programming language runtimes. This paper describes a novel approach to boost Web application performance by improving inter-process communication between a programming language runtime and Web server runtime. The approach reduces redundant processing for memory copying and the context switch overhead between user space and kernel space by exploiting the zero-copy data transfer methodology, such as the sendfile system call. In order to transparently utilize this optimization feature with existing Web applications, we propose enhancements of the PHP runtime, FastCGI protocol, and Web server. Our proposed approach achieves a 126% performance improvement with micro-benchmarks and a 44% performance improvement for a standard Web benchmark, SPECweb2005.	Highly scalable web applications with zero-copy data transfer	NA:NA:NA:NA:NA	2009
Heiko Ludwig:Jim Laredo:Kamal Bhattacharya:Liliana Pasquale:Bruno Wassermann	Applications increasingly make use of the distributed platform that the World Wide Web provides - be it as a Software-as-a-Service such as salesforce.com, an application infrastructure such as facebook.com, or a computing infrastructure such as a "cloud". A common characteristic of applications of this kind is that they are deployed on infrastructure or make use of components that reside in different management domains. Current service management approaches and systems, however, often rely on a centrally managed configuration management database (CMDB), which is the basis for centrally orchestrated service management processes, in particular change management and incident management. The distribution of management responsibility of WWW based applications requires a decentralized approach to service management. This paper proposes an approach of decentralized service management based on distributed configuration management and service process co-ordination, making use RESTful access to configuration information and ATOM-based distribution of updates as a novel foundation for service management processes.	REST-based management of loosely coupled services	NA:NA:NA:NA:NA	2009
Dietwig Lowet:Daniel Goergen	Collaborative browsing, or co-browsing, is the co-navigation of the web with other people at-a-distance, supported by software that takes care of synchronizing the browsers. Current state-of-the-art solutions are able to do co-browsing of "static web pages", and do not support the synchronization of JavaScript interactions. However, currently many web pages use JavaScript and Ajax techniques to create highly dynamic and interactive web applications. In this paper, we describe two approaches for co-browsing that both support the synchronization of the JavaScript and Ajax interactions of dynamic web pages. One approach is based on synchronizing the output of the JavaScript engine by sending over the changes made on the DOM tree. The other approach is based on synchronizing the input of the JavaScript engine by synchronizing UI events and incoming data. Since the latter solution offers a better user experience and is more scalable, it is elaborated in more detail. An important aspect of both approaches is that they operate at the DOM level. Therefore, the client-side can be implemented in JavaScript and no browser extensions are required. To the best of the authors' knowledge this is the first DOM-level co-browsing solution that also enables co-browsing of the dynamic interaction parts of web pages. The presented co-browsing solution has been implemented in a research demonstrator which allows users to do co-browsing of web-applications on browser-based networked televisions.	Co-browsing dynamic web pages	NA:NA	2009
Michiaki Tatsubori:Toyotaro Suzumura	Web applications often use HTML templates to separate the webpage presentation from its underlying business logic and objects. This is now the de facto standard programming model for Web application development. This paper proposes a novel implementation for existing server-side template engines, FlyingTemplate, for (a) reduced bandwidth consumption in Web application servers, and (b) off-loading HTML generation tasks to Web clients. Instead of producing a fully-generated HTML page, the proposed template engine produces a skeletal script which includes only the dynamic values of the template parameters and the bootstrap code that runs on a Web browser at the client side. It retrieves a client-side template engine and the payload templates separately. With the goals of efficiency, implementation transparency, security, and standards compliance in mind, we developed FlyingTemplate with two design principles: effective browser cache usage, and reasonable compromises which restrict the template usage patterns and relax the security policies slightly but in a controllable way. This approach allows typical template-based Web applications to run effectively with FlyingTemplate. As an experiment, we tested the SPECweb2005 banking application using FlyingTemplate without any other modifications and saw throughput improvements from 1.6x to 2.0x in its best mode. In addition, FlyingTemplate can enforce compliance with a simple security policy, thus addressing the security problems of client-server partitioning in the Web environment.	HTML templates that fly: a template engine approach to automated offloading from server to client	NA:NA	2009
Chuan Yue:Haining Wang	JavaScript is an interpreted programming language most often used for enhancing webpage interactivity and functionality. It has powerful capabilities to interact with webpage documents and browser windows, however, it has also opened the door for many browser-based security attacks. Insecure engineering practices of using JavaScript may not directly lead to security breaches, but they can create new attack vectors and greatly increase the risks of browser-based attacks. In this paper, we present the first measurement study on insecure practices of using JavaScript on the Web. Our focus is on the insecure practices of JavaScript inclusion and dynamic generation, and we examine their severity and nature on 6,805 unique websites. Our measurement results reveal that insecure JavaScript practices are common at various websites: (1) at least 66.4% of the measured websites manifest the insecure practices of including JavaScript files from external domains into the top-level documents of their webpages; (2) over 44.4% of the measured websites use the dangerous eval() function to dynamically generate and execute JavaScript code on their webpages; and (3) in JavaScript dynamic generation, using the document.write() method and the innerHTML property is much more popular than using the relatively secure technique of creating script elements via DOM methods. Our analysis indicates that safe alternatives to these insecure practices exist in common cases and ought to be adopted by website developers and administrators for reducing potential security risks.	Characterizing insecure javascript practices on the web	NA:NA	2009
Jeff Pasternack:Dan Roth	Much of the information on the Web is found in articles from online news outlets, magazines, encyclopedias, review collections, and other sources. However, extracting this content from the original HTML document is complicated by the large amount of less informative and typically unrelated material such as navigation menus, forms, user comments, and ads. Existing approaches tend to be either brittle and demand significant expert knowledge and time (manual or tool-assisted generation of rules or code), necessitate labeled examples for every different page structure to be processed (wrapper induction), require relatively uniform layout (template detection), or, as with Visual Page Segmentation (VIPS), are computationally expensive. We introduce maximum subsequence segmentation, a method of global optimization over token-level local classifiers, and apply it to the domain of news websites. Training examples are easy to obtain, both learning and prediction are linear time, and results are excellent (our semi-supervised algorithm yields an overall F1-score of 97.947%), surpassing even those produced by VIPS with a hypothetical perfect block-selection heuristic. We also evaluate against the recent CleanEval shared task with surprisingly good cross-task performance cleaning general web pages, exceeding the top "text-only" score (based on Levenshtein distance), 87.8% versus 84.1%.	Extracting article text from the web with maximum subsequence segmentation	NA:NA	2009
Gengxin Miao:Junichi Tatemura:Wang-Pin Hsiung:Arsany Sawires:Louise E. Moser	Fully automatic methods that extract lists of objects from the Web have been studied extensively. Record extraction, the first step of this object extraction process, identifies a set of Web page segments, each of which represents an individual object (e.g., a product). State-of-the-art methods suffice for simple search, but they often fail to handle more complicated or noisy Web page structures due to a key limitation -- their greedy manner of identifying a list of records through pairwise comparison (i.e., similarity match) of consecutive segments. This paper introduces a new method for record extraction that captures a list of objects in a more robust way based on a holistic analysis of a Web page. The method focuses on how a distinct tag path appears repeatedly in the DOM tree of the Web document. Instead of comparing a pair of individual segments, it compares a pair of tag path occurrence patterns (called visual signals) to estimate how likely these two tag paths represent the same list of objects. The paper introduces a similarity measure that captures how closely the visual signals appear and interleave. Clustering of tag paths is then performed based on this similarity measure, and sets of tag paths that form the structure of data records are extracted. Experiments show that this method achieves higher accuracy than previous methods.	Extracting data records from the web using tag path clustering	NA:NA:NA:NA:NA	2009
Uri Schonfeld:Narayanan Shivakumar	Comprehensive coverage of the public web is crucial to web search engines. Search engines use crawlers to retrieve pages and then discover new ones by extracting the pages' outgoing links. However, the set of pages reachable from the publicly linked web is estimated to be significantly smaller than the invisible web, the set of documents that have no incoming links and can only be retrieved through web applications and web forms. The Sitemaps protocol is a fast-growing web protocol supported jointly by major search engines to help content creators and search engines unlock this hidden data by making it available to search engines. In this paper, we perform a detailed study of how "classic" discovery crawling compares with Sitemaps, in key measures such as coverage and freshness over key representative websites as well as over billions of URLs seen at Google. We observe that Sitemaps and discovery crawling complement each other very well, and offer different tradeoffs.	Sitemaps: above and beyond the crawl of duty	NA:NA	2009
Huayu Wu:Tok Wang Ling:Liang Xu:Zhifeng Bao	Since more and more business data are represented in XML format, there is a compelling need of supporting analytical operations in XML queries. Particularly, the latest version of XQuery proposed by W3C, XQuery 1.1, introduces a new construct to explicitly express grouping operation in FLWOR expression. Existing works in XML query processing mainly focus on physically matching query structure over XML document. Given the explicit grouping operation in a query, how to efficiently compute grouping and aggregate functions over XML document is not well studied yet. In this paper, we extend our previous XML query processing algorithm, VERT, to efficiently perform grouping and aggregate function in queries. The main technique of our approach is introducing relational tables to index values. Query pattern matching and aggregation computing are both conducted with table indices. We also propose two semantic optimizations to further improve the query performance. Finally we present experimental results to validate the efficiency of our approach, over other existing approaches.	Performing grouping and aggregate functions in XML queries	NA:NA:NA:NA	2009
Ghislain Fourny:Markus Pilman:Daniela Florescu:Donald Kossmann:Tim Kraska:Darin McBeath	Since the invention of the Web, the browser has become more and more powerful. By now, it is a programming and execution environment in itself. The predominant language to program applications in the browser today is JavaScript. With browsers becoming more powerful, JavaScript has been extended and new layers have been added (e.g., DOM-Support and XPath). Today, JavaScript is very successful and applications and GUI features implemented in the browser have become increasingly complex. The purpose of this paper is to improve the programmability of Web browsers by enabling the execution of XQuery programs in the browser. Although it has the potential to ideally replace JavaScript, it is possible to run it in addition to JavaScript for more flexibility. Furthermore, it allows instant code migration from the server to the client and vice-versa. This enables a significant simplification of the technology stack. The intuition is that programming the browser involves mostly XML (i.e., DOM) navigation and manipulation, and the XQuery family of W3C standards were designed exactly for that purpose. The paper proposes extensions to XQuery for Web browsers and gives a number of examples that demonstrate the usefulness of XQuery for the development of AJAX-style applications. Furthermore, the paper presents the design of an XQuery plug-in for Microsoft's Internet Explorer. The paper also gives examples of applications which were developed with the help of this plug-in.	XQuery in the browser	NA:NA:NA:NA:NA:NA	2009
Xiangfu Meng:Z. M. Ma:Li Yan	To deal with the problem of empty or too little answers returned from a Web database in response to a user query, this paper proposes a novel approach to provide relevant and ranked query results. Based on the user original query, we speculate how much the user cares about each specified attribute and assign a corresponding weight to it. This original query is then rewritten as an approximate query by relaxing the query criteria range. The relaxation order of all specified attributes and the relaxed degree on each specified attribute are varied with the attribute weights. For the approximate query results, we generate users' contextual preferences from database workload and use them to create a priori orders of tuples in an off-line preprocessing step. Only a few representative orders are saved, each corresponding to a set of contexts. Then, these orders and associated contexts are used at query time to expeditiously provide ranked answers. Results of a preliminary user study demonstrate that our query relaxation and results ranking methods can capture the user's preferences effectively. The efficiency and effectiveness of our approach is also demonstrated by experimental result.	Answering approximate queries over autonomous web databases	NA:NA:NA	2009
Adriano Pereira:Diego Duarte:Wagner Meira, Jr.:Virgilio Almeida:Paulo Góes	E-commerce is growing at an exponential rate. In the last decade, there has been an explosion of online commercial activity enabled by World Wide Web (WWW). These days, many consumers are less attracted to online auctions, preferring to buy merchandise quickly using fixed-price negotiations. Sales at Amazon.com, the leader in online sales of fixed-price goods, rose 37% in the first quarter of 2008. At eBay, where auctions make up 58% of the site's sales, revenue rose 14%. In Brazil, probably by cultural influence, online auctions are not been popular. This work presents a characterization and analysis of fixed-price online negotiations. Using actual data from a Brazilian marketplace, we analyze seller practices, considering seller profiles and strategies. We show that different sellers adopt strategies according to their interests, abilities and experience. Moreover, we confirm that choosing a selling strategy is not simple, since it is important to consider the seller's characteristics to evaluate the applicability of a strategy. The work also provides a comparative analysis of some selling practices in Brazil with popular worldwide marketplaces.	Analyzing seller practices in a Brazilian marketplace	NA:NA:NA:NA:NA	2009
Guilherme Vale Menezes:Nivio Ziviani:Alberto H.F. Laender:Virgílio Almeida	We analyze knowledge production in Computer Science by means of coauthorship networks. For this, we consider 30 graduate programs of different regions of the world, being 8 programs in Brazil, 16 in North America (3 in Canada and 13 in the United States), and 6 in Europe (2 in France, 1 in Switzerland and 3 in the United Kingdom). We use a dataset that consists of 176,537 authors and 352,766 publication entries distributed among 2,176 publication venues. The results obtained for different metrics of collaboration social networks indicate the process of knowledge creation has  changed differently for each region. Research is increasingly done in teams across different fields of Computer Science. The size of the giant component indicates the existence of isolated collaboration groups in the European network, contrasting to the degree of connectivity found in the Brazilian and North-American counterparts. We also analyzed the temporal evolution of the social networks representing the three regions. The number of authors per paper experienced an increase in a time span of 12 years. We observe that the number of collaborations between authors grows faster than the number of authors, benefiting from the existing network structure. The temporal evolution shows differences between well-established fields, such as Databases and Computer Architecture, and emerging fields, like Bioinformatics and Geoinformatics. The patterns of collaboration analyzed in this paper contribute to an overall understanding of Computer Science research in different geographical regions that could not be achieved without the use of complex networks and a large publication database.	A geographical analysis of knowledge production in computer science	NA:NA:NA:NA	2009
Gang Wang:Jian Hu:Yunzhang Zhu:Hua Li:Zheng Chen	Existing keyword suggestion tools from various search engine companies could automatically suggest keywords related to the advertisers' products or services, counting in simple statistics of the keywords, such as search volume, cost per click (CPC), etc. However, the nature of the generalized Second Price Auction suggests that better understanding the competitors' keyword selection and bidding strategies better helps to win the auction, other than only relying on general search statistics. In this paper, we propose a novel keyword suggestion strategy, called Competitive Analysis, to explore the keyword based competition relationships among advertisers and eventually help advertisers to build campaigns with better performance. The experimental results demonstrate that the proposed Competitive Analysis can both help advertisers to promote their product selling and generate more revenue to the search engine companies.	Competitive analysis from click-through log	NA:NA:NA:NA:NA	2009
Manish Gupta	Click Through Rate (CTR) is an important metric for ad systems, job portals, recommendation systems. CTR impacts publisher's revenue, advertiser's bid amounts in "pay for performance" business models. We learn regression models using features of the job, optional click history of job, features of "related" jobs. We show that our models predict CTR much better than predicting avg. CTR for all job listings, even in absence of the click history for the job listing.	Predicting click through rate for job listings	NA	2009
Jeonghee Yi:Farzin Maghoul	In this paper we describe a problem of discovering query clusters from a click-through graph of web search logs. The graph consists of a set of web search queries, a set of pages selected for the queries, and a set of directed edges that connects a query node and a page node clicked by a user for the query. The proposed method extracts all maximal bipartite cliques (bicliques) from a click-through graph and compute an equivalence set of queries (i.e., a query cluster) from the maximal bicliques. A cluster of queries is formed from the queries in a biclique. We present a scalable algorithm that enumerates all maximal bicliques from the click-through graph. We have conducted experiments on Yahoo web search queries and the result is promising.	Query clustering using click-through graph	NA:NA	2009
Jihyun Lee:Jun-Ki Min:Chin-Wan Chung	In this paper, we present a semantic search technique considering the type of desired Web resources and the semantic relationships between the resources and the query keywords in the ontology. In order to effectively retrieve the most relevant top-k resources, we propose a novel ranking model. To do this, we devise a measure to determine the weight of the semantic relationship. In addition, we consider the number of meaningful semantic relationships between a resource and keywords, the coverage of keywords, and the distinguishability of keywords. Through experiments using real datasets, we observe that our ranking model provides more accurate semantic search results compared to existing ranking models.	An effective semantic search technique using ontology	NA:NA:NA	2009
Sunitha Ramanujam:Anubha Gupta:Latifur Khan:Steven Seida:Bhavani Thuraisingham	The emergence of Semantic Web technologies and standards such as Resource Description Framework (RDF) has introduced novel data storage models such as the RDF Graph Model. In this paper, we present a research effort called R2D, which attempts to bridge the gap between RDF and RDBMS concepts by presenting a relational view of RDF data stores. Thus, R2D is essentially a relational wrapper around RDF stores that aims to make the variety of stable relational tools that are currently in the market available to RDF stores without data duplication and synchronization issues.	Relationalizing RDF stores for tools reusability	NA:NA:NA:NA:NA	2009
Davide Francesco Barbieri:Daniele Braga:Stefano Ceri:Emanuele Della Valle:Michael Grossniklaus	C-SPARQL is an extension of SPARQL to support continuous queries, registered and continuously executed over RDF data streams, considering windows of such streams. Supporting streams in RDF format guarantees interoperability and opens up important applications, in which reasoners can deal with knowledge that evolves over time. We present C-SPARQL by means of examples in Urban Computing.	C-SPARQL: SPARQL for continuous querying	NA:NA:NA:NA:NA	2009
Guoliang Li:Jianhua Feng:Lizhu Zhou	In a traditional keyword-search system over XML data, a user composes a keyword query, submits it to the system, and retrieves relevant subtrees. In the case where the user has limited knowledge about the data, often the user feels "left in the dark" when issuing queries, and has to use a try-and-see approach for finding information. In this paper, we study a new information-access paradigm for XML data, called "Inks," in which the system searches on the underlying data "on the fly" as the user types in query keywords. Inks extends existing XML keyword search methods by interactively answering queries. We propose effective indices, early-termination techniques, and efficient search algorithms to achieve a high interactive speed. We have implemented our algorithm, and the experimental results show that our method achieves high search efficiency and result quality.	Interactive search in XML data	NA:NA:NA	2009
Harry Halpin	There has recently been an upsurge of interest in the possibilities of combining structured data and ad-hoc information retrieval from traditional hypertext. In this experiment, we run queries extracted from a query log of a major search engine against the Semantic Web to discover if the Semantic Web has anything of interest to the average user. We show that there is indeed much information on the Semantic Web that could be relevant for many queries for people, places and even abstract concepts, although they are overwhelmingly clustered around a Semantic Web-enabled export of Wikipedia known as DBPedia.	Is there anything worth finding on the semantic web?	NA	2009
Pedro Oliveira:Paulo Gomes	Most of the approaches for dealing with uncertainty in the Semantic Web rely on the principle that this uncertainty is already asserted. In this paper, we propose a new approach to learn and reason about uncertainty in the Semantic Web. Using instance data, we learn the uncertainty of an OWL ontology, and use that information to perform probabilistic reasoning on it. For this purpose, we use Markov logic, a new representation formalism that combines logic with probabilistic graphical models.	Instance-based probabilistic reasoning in the semantic web	NA:NA	2009
Bernhard Kruepl:Wolfgang Holzinger:Yansen Darmaputra:Robert Baumgartner	We demonstrate a flight meta-search engine that is based on the Metamorph framework. Metamorph provides mechanisms to model web forms together with the interactions which are needed to fulfil a request, and can generate interaction sequences that pose queries using these web forms and collect the results. In this paper, we discuss an interesting new feature that makes use of the forms themselves as an information source. We show how data can be extracted from web forms (rather than the data behind web forms) to generate a graph of flight connections between cities. The flight connection graph allows us to vastly reduce the number of queries that the engine sends to airline websites in the most interesting search scenarios; those that involve the controversial practice of creative ticketing, in which agencies attempt to find lower price fares by using more than one airline for a journey. We describe a system which attains data from a number of websites to identify promising routes and prune the search tree. Heuristics that make use of geographical information and an estimation of cost based on historical data are employed. The results are then made available to improve the quality of future search requests.	A flight meta-search engine with metamorph	NA:NA:NA:NA	2009
Ali Dasdan:Chris Drome:Santanu Kolay	Human computation is an effective way to channel human effort spent playing games to solving computational problems that are easy for humans but difficult for computers to automate. We propose Thumbs-Up, a new game for human computation with the purpose of playing to rank search result. Our experience from users shows that Thumbs-Up is not only fun to play, but produces more relevant rankings than both a major search engine and optimal rank aggregation using the Kemeny rule.	Thumbs-up: a game for playing to rank search results	NA:NA:NA	2009
Ranieri Baraglia:Fidel Cacheda:Victor Carneiro:Vreixo Formoso:Raffaele Perego:Fabrizio Silvestri	Giving suggestions to users of Web-based services is a common practice aimed at enhancing their navigation experience. Major Web Search Engines usually provide "Suggestions" under the form of queries that are, to some extent, related to the current query typed by the user, and the knowledge learned from the past usage of the system. In this work we introduce "Search Shortcuts" as "Successful" queries allowed, in the past, users to satisfy their information needs. Differently from conventional suggestion techniques, our search shortcuts allows to evaluate effectiveness by exploiting a simple train-and-test approach. We have applied several Collaborative Filtering algorithms to this problem, evaluating them on a real query log data. We generate the shortcuts from all user sessions belonging to the testing set, and measure the quality of the shortcuts suggested by considering the similarity between them and the navigational user behavior.	Search shortcuts: driving users towards their goals	NA:NA:NA:NA:NA:NA	2009
Ning Liu:Jun Yan:Zheng Chen	In this paper, we propose to model the blended search problem by assuming conditional dependencies among queries, VSEs and search results. The probability distributions of this model are learned from search engine query log through unigram language model. Our experimental exploration shows that, (1) a large number of queries in generic Web search have vertical search intentions; and (2) our proposed algorithm can effectively blend vertical search results into generic Web search, which can improve the Mean Average Precision (MAP) by as much as 16% compared to traditional Web search without blending.	A probabilistic model based approach for blended search	NA:NA:NA	2009
Francisco P. Romero:Jesus Serrano-Guerrero:Jose A. Olivas	In this poster, a tool named BUCEFALO is presented. This tool is specially designed to improve the information retrieval tasks in web-based Personal Health Records (PHR). This tool implements semantic and multilingual query expansion techniques and information filtering algorithms in order to help users find the most valuable information about a specific clinical case. The filtering model is based on fuzzy prototypes based filtering, data quality measures, user profiles and healthcare ontologies. The first experimental results illustrate the feasibility of this tool.	Bucefalo: a tool for intelligent search and filtering for web-based personal health records	NA:NA:NA	2009
Haofen Wang:Qiaoling Liu:Gui-Rong Xue:Yong Yu:Lei Zhang:Yue Pan	More and more structured information in the form of semantic data is nowadays available. It offers a wide range of new possibilities especially for semantic search and Web data integration. However, their effective exploitation still brings about a number of challenges, e.g. usability, scalability and uncertainty. In this paper, we present Dataplorer, a solution designed to address these challenges. We consider the usability through the use of hybrid queries and faceted search, while still preserving the scalability thanks to an extension of inverted index to support this type of query. Moreover, Dataplorer deals with uncertainty by means of a powerful ranking scheme to find relevant results. Our experimental results show that our proposed approach is promising and it makes us believe that it is possible to extend the current IR infrastructure to query and search the Web of data.	Dataplorer: a scalable search engine for the data web	NA:NA:NA:NA:NA:NA	2009
Xiaofeng He:Lei Duan:Yiping Zhou:Byron Dom	We propose a novel cost-efficient approach to threshold selection for binary web-page classification problems with imbalanced class distributions. In many binary-classification tasks the distribution of classes is highly skewed. In such problems, using uniform random sampling in constructing sample sets for threshold setting requires large sample sizes in order to include a statistically sufficient number of examples of the minority class. On the other hand, manually labeling examples is expensive and budgetary considerations require that the size of sample sets be limited. These conflicting requirements make threshold selection a challenging problem. Our method of sample-set construction is a novel approach based on stratified sampling, in which manually labeled examples are expanded to reflect the true class distribution of the web-page population. Our experimental results show that using false positive rate as the criterion for threshold setting results in lower-variance threshold estimates than using other widely used accuracy measures such as F1 and precision.	Threshold selection for web-page classification with highly skewed class distribution	NA:NA:NA:NA	2009
Congle Zhang:Gui-Rong Xue:Yong Yu:Hongyuan Zha	Traditional Naive Bayes Classifier performs miserably on web-scale taxonomies. In this paper, we investigate the reasons behind such bad performance. We discover that the low performance are not completely caused by the intrinsic limitations of Naive Bayes, but mainly comes from two largely ignored problems: contradiction pair problem and discriminative evidence cancelation problem. We propose modifications that can alleviate the two problems while preserving the advantages of Naive Bayes. The experimental results show our modified Naive Bayes can significantly improve the performance on real web-scale taxonomies.	Web-scale classification with naive bayes	NA:NA:NA:NA	2009
Junfeng Wang:Xiaofei He:Can Wang:Jian Pei:Jiajun Bu:Chun Chen:Ziyu Guan:Gang Lu	We consider the problem of template-independent news extraction. The state-of-the-art news extraction method is based on template-level wrapper induction, which has two serious limitations. 1) It cannot correctly extract pages belonging to an unseen template until the wrapper for that template has been generated. 2) It is costly to maintain up-to-date wrappers for hundreds of websites, because any change of a template may lead to the invalidation of the corresponding wrapper. In this paper we formalize news extraction as a machine learning problem and learn a template-independent wrapper using a very small number of labeled news pages from a single site. Novel features dedicated to news titles and bodies are developed respectively. Correlations between the news title and the news body are exploited. Our template-independent wrapper can extract news pages from different sites regardless of templates. In experiments, a wrapper is learned from 40 pages from a single news site. It achieved 98.1% accuracy over 3,973 news pages from 12 news sites.	News article extraction with template-independent wrapper	NA:NA:NA:NA:NA:NA:NA:NA	2009
Ralitsa Angelova:Gjergji Kasneci:Fabian M. Suchanek:Gerhard Weikum	We introduce a multi-label classification model and algorithm for labeling heterogeneous networks, where nodes belong to different types and different types have different sets of classification labels. We present a graph-based approach which models the mutual influence between nodes in the network as a random walk. When viewing class labels as "colors", the random surfer is "spraying" different node types with different color palettes; hence the name Graffiti. We demonstrate the performance gains of our method by comparing it to three state-of-the-art techniques for graph-based classification.	Graffiti: node labeling in heterogeneous networks	NA:NA:NA:NA	2009
Shuyi Zheng:Pavel Dmitriev:C. Lee Giles	This paper identifies and explores the problem of seed selection in a web-scale crawler. We argue that seed selection is not a trivial but very important problem. Selecting proper seeds can increase the number of pages a crawler will discover, and can result in a collection with more ``good" and less "bad" pages. Based on the analysis of the graph structure of the web, we propose several seed selection algorithms. Effectiveness of these algorithms is proved by our experimental results on real web data.	Graph based crawler seed selection	NA:NA:NA	2009
Jyh-Ren Shieh:Yung-Huan Hsieh:Yang-Ting Yeh:Tse-Chung Su:Ching-Yung Lin:Ja-Ling Wu	This paper proposes an effective approach to provide relevant search terms for conceptual Web search. 'Semantic Term Suggestion' function has been included so that users can find the most appropriate query term to what they really need. Conventional approaches for term suggestion involve extracting frequently occurring key terms from retrieved documents. They must deal with term extraction difficulties and interference from irrelevant documents. In this paper, we propose a semantic term suggestion function called Collective Intelligence based Term Suggestion (CITS). CITS provides a novel social-network based framework for relevant terms suggestion with a semantic graph of the search term without limiting to the specific query term. A visualization of semantic graph is presented to the users to help browsing search results from related terms in the semantic graph. The search results are ranked each time according to their relevance to the related terms in the entire query session. Comparing to two popular commercial search engines, a user study of 18 users on 50 search terms showed better user satisfactions and indicated the potential usefulness of proposed method in real-world search applications.	Building term suggestion relational graphs from collective intelligence	NA:NA:NA:NA:NA:NA	2009
William Chang:Patrick Pantel:Ana-Maria Popescu:Evgeniy Gabrilovich	In online advertising, pervasive in commercial search engines, advertisers typically bid on few terms, and the scarcity of data makes ad matching difficult. Suggesting additional bidterms can significantly improve ad clickability and conversion rates. In this paper, we present a large-scale bidterm suggestion system that models an advertiser's intent and finds new bidterms consistent with that intent. Preliminary experiments show that our system significantly increases the coverage of a state of the art production system used at Yahoo while maintaining comparable precision.	Towards intent-driven bidterm suggestion	NA:NA:NA:NA	2009
Hao Wu:Guang Qiu:Xiaofei He:Yuan Shi:Mingcheng Qu:Jing Shen:Jiajun Bu:Chun Chen	This paper proposes an efficient relevance feedback based interactive model for keyword generation in sponsored search advertising. We formulate the ranking of relevant terms as a supervised learning problem and suggest new terms for the seed by leveraging user relevance feedback information. Active learning is employed to select the most informative samples from a set of candidate terms for user labeling. Experiments show our approach improves the relevance of generated terms significantly with little user effort required.	Advertising keyword generation using active learning	NA:NA:NA:NA:NA:NA:NA:NA	2009
Lusong Li:Tao Mei:Chris Liu:Xian-Sheng Hua	This paper presents a novel game-like advertising system called GameSense, which is driven by the compelling contents of online images. Given a Web page which typically contains images, GameSense is able to select suitable images to create online in-image games for advertising. The contextually relevant ads (i.e., product logos) are embedded at appropriate positions within the online games. The ads are selected based on not only textual relevance but also visual content similarity. The game is able to provide viewers rich experience and thus promote the embedded ads to provide more effective advertising.	Gamesense	NA:NA:NA:NA	2009
Dan Shen:Xiaoyuan Wu:Alvaro Bolivar	As the largest online marketplace in the world, eBay has a huge inventory where there are plenty of great rare items with potentially large, even rapturous buyers. These items are obscured in long tail of eBay item listing and hard to find through existing searching or browsing methods. It is observed that there are great rarity demands from users according to eBay query log. To keep up with the demands, the paper proposes a method to automatically detect rare items in eBay online listing. A large set of features relevant to the task are investigated to filter items and further measure item rareness. The experiments on the most rarity-demand-intensitive domains show that the method may effectively detect rare items (>90% precision).	Rare item detection in e-commerce site	NA:NA:NA	2009
Oktie Hassanzadeh:Lipyeow Lim:Anastasios Kementsietsidis:Min Wang	In this paper, we present a framework for online discovery of semantic links from relational data. Our framework is based on declarative specification of the linkage requirements by the user, that allows matching data items in many real-world scenarios. These requirements are translated to queries that can run over the relational data source, potentially using the semantic knowledge to enhance the accuracy of link discovery. Our framework lets data publishers to easily find and publish high-quality links to other data sources, and therefore could significantly enhance the value of the data in the next generation of web.	A declarative framework for semantic link discovery over relational data	NA:NA:NA:NA	2009
Chen Lin:Jiang-Ming Yang:Rui Cai:Xin-Jing Wang:Wei Wang:Lei Zhang	The abundant knowledge in web communities has motivated the research interests in discussion threads. The dynamic nature of discussion threads poses interesting and challenging problems for computer scientists. Although techniques such as semantic models or structural models have been shown to be useful in a number of areas, they are inefficient in understanding discussion threads due to the temporal dependence among posts in a discussion thread. Such dependence causes that semantics and structure coupled with each other in discussion threads. In this paper, we propose a sparse coding-based model named SMSS to Simultaneously Model Semantic and Structure of discussion threads.	Modeling semantics and structure of discussion threads	NA:NA:NA:NA:NA:NA	2009
Kirill Nikolaev:Ekaterina Zudina:Andrey Gorshkov	In order to artificially boost the rank of commercial pages in search engine results, search engine optimizers pay for links to these pages on other websites. Identifying paid links is important for a web search engine to produce highly relevant results. In this paper we introduce a novel method of identifying such links. We start with training a classifier of anchor text topics and analyzing web pages for diversity of their outgoing commercial links. Then we use this information and analyze link graph of the Russian Web to find pages that sell links and sites that buy links and to identify the paid links. Testing on manually marked samples showed high efficiency of the algorithm.	Combining anchor text categorization and graph analysis for paid link detection	NA:NA:NA	2009
Sebastian Michel:Ingmar Weber	We show how a number of novel email search features can be implemented without any kind of natural language processing (NLP) or advanced data mining. Our approach inspects the email headers of all messages a user has ever sent or received and it creates simple per-contact summaries, including simple information about the message exchange history, the domain of the sender or even the sender's gender. With these summaries advanced questions/tasks such as "Who do I still need to reply to?" or "Find 'fun' messages sent by friends." become possible. As a proof of concept, we implemented a Mozilla-Thunderbird extension, adding powerful people search to the popular email client.	Rethinking email message and people search	NA:NA	2009
Eda Baykan:Monika Henzinger:Ludmila Marian:Ingmar Weber	Given only the URL of a web page, can we identify its topic? This is the question that we examine in this paper. Usually, web pages are classified using their content, but a URL-only classifier is preferable, (i) when speed is crucial, (ii) to enable content filtering before an (objection-able) web page is downloaded, (iii) when a page's content is hidden in images, (iv) to annotate hyperlinks in a personalized web browser, without fetching the target page, and (v) when a focused crawler wants to infer the topic of a target page before devoting bandwidth to download it. We apply a machine learning approach to the topic identification task and evaluate its performance in extensive experiments on categorized web pages from the Open Directory Project (ODP). When training separate binary classifiers for each topic, we achieve typical F-measure values between 80 and 85, and a typical precision of around 85. We also ran experiments on a small data set of university web pages. For the task of classifying these pages into faculty, student, course and project pages, our methods improve over previous approaches by 13.8 points of F-measure.	Purely URL-based topic classification	NA:NA:NA:NA	2009
Kaimin Zhang:Lu Wang:Xiaolin Guo:Aimin Pan:Bin Benjamin Zhu	In this paper, a benchmark called WPBench is reported to evaluate the responsiveness of Web browsers for modern Web 2.0 applications. In WPBench, variations of servers and networks are removed and the benchmark result is the closest to what Web users would perceive. To achieve these, WPBench records users' interactions with typical Web 2.0 applications, and then replays Web navigations when benchmarking browsers. The replay mechanism can emulate the actual user interactions and the characteristics of the servers and the networks in a consistent way independent of browsers so that any browser compliant to the standards can be benchmarked fairly. In addition to describing the design and generation of WPBench, we also report the WPBench comparison results on the responsiveness performance for three popular Web browsers: Internet Explorer, Firefox and Chrome.	WPBench: a benchmark for evaluating the client-side performance of web 2.0 applications	NA:NA:NA:NA:NA	2009
Vipul Mathur:Sanket Dhopeshwarkar:Varsha Apte	Many overload control mechanisms for Web based applications aim to prevent overload by setting limits on factors such as admitted load, number of server threads, buffer size. For this they need online measurements of metrics such as response time, throughput, and resource utilization. This requires instrumentation of the server by modifying server code, which may not be feasible or desirable. An alternate approach is to use a proxy between the clients and servers. We have developed a proxy-based overload control platform called MASTH Proxy--Multi-class Admission-controlled Self-Tuning HTTP Proxy. It records detailed measurements, supports multiple request classes, manages queues of HTTP requests, provides tunable parameters and enables easy implementation of dynamic overload control. This gives designers of overload control schemes a platform where they can concentrate on developing the core control logic, without the need to modify upstream server code.	MASTH proxy: an extensible platform for web overload control	NA:NA:NA	2009
Stéphane Sire:Micaël Paquier:Alain Vagner:Jérôme Bogaerts	Widget containers are used everywhere on the Web, for instance as customizable start pages to Web desktops. In this poster, we describe the extension of a widget container with an inter-widgets communication layer, as well as the subsequent application programming interfaces (APIs) added to the Widget object to support this feature. We present the benefits of a drag and drop facility within widgets and conclude by a call for standardization of inter-widgets communication on the Web.	A messaging API for inter-widgets communication	NA:NA:NA:NA	2009
Atsuyuki Morishima:Akiyoshi Nakamizo:Toshinari Iida:Shigeo Sugimoto:Hiroyuki Kitagawa	This paper addresses the problem of finding new locations of moved Web pages. We discuss why the content-based approach has a limitation in solving the problem and why it is important to exploit the knowledge on where to search for the pages.	Why are moved web pages difficult to find?: the WISH approach	NA:NA:NA:NA:NA	2009
Taehyung Lee:Jinil Kim:Jin Wook Kim:Sung-Ryul Kim:Kunsoo Park	A soft error redirection is a URL redirection to a page that returns the HTTP status code 200 (OK) but has actually no relevant content to the client request. Since such redirections degrade the performance of web search engines in many ways, it is highly desirable to remove as many of them as possible. We propose a novel approach to detect soft error redirections by analyzing redirection logs collected during crawling operation. Experimental results on huge crawl data show that our measure can classify soft error redirections effectively.	Detecting soft errors by redirection classification	NA:NA:NA:NA:NA	2009
Hyunyoung Kil:Wonhong Nam:Dongwon Lee	The behavioral description based Web Service Composition (WSC) problem aims at the automatic construction of a coordinator web service that controls a set of web services to reach a goal state. However, solving the WSC problem exactly with a realistic model is doubly-exponential in the number of variables in web service descriptions. In this paper, we propose a novel efficient approximation-based algorithm using automatic abstraction and refinement to dramatically reduce the number of variables needed to solve the problem.	Automatic web service composition with abstraction and refinement	NA:NA:NA	2009
Bo Jiang:W. K. Chan:Zhenyu Zhang:T. H. Tse	Peer services depend on one another to accomplish their tasks, and their structures may evolve. A service composition may be designed to replace its member services whenever the quality of the composite service fails to meet certain quality-of-service (QoS) requirements. Finding services and service invocation endpoints having the greatest impact on the quality are important to guide subsequent service adaptations. This paper proposes a technique that samples the QoS of composite services and continually analyzes them to identify artifacts for service adaptation. The preliminary results show that our technique has the potential to effectively find such artifacts in services.	Where to adapt dynamic service compositions	NA:NA:NA:NA	2009
Sourish Dasgupta:Satish Bhat:Yugyung Lee	Today's Web becomes a platform for services to be dynamically interconnected to produce a desired outcome. It is important to formalize the semantics of the contextual elements of web services. In this paper, we propose a novel technique called Semantic Genome Propagation Scheme (SGPS) for measuring similarity between semantic concepts. We show how SGPS is used to compute a multi-dimensional similarity between two services. We evaluate the SGPS similarity measurement in terms of the similarity performance and scalability.	SGPS: a semantic scheme for web service similarity	NA:NA:NA	2009
Ting Deng:Jinpeng Huai:Xianxian Li:Zongxia Du:Huipeng Guo	In this paper, we propose a novel approach for composing existing web services to satisfy the correctness constraints to the design, including freeness of deadlock and unspecified reception, and temporal constraints in Computation Tree Logic formula. An automated synthesis algorithm based on learning algorithm is introduced, which guarantees that the composite service is the most general way of coordinating services so that the correctness is ensured. We have implemented a prototype system evaluating the effectiveness and efficiency of our synthesis approach through an experimental study.	Automated synthesis of composite services with correctness guarantee	NA:NA:NA:NA:NA	2009
Ali Dasdan:Xinh Huynh	In order to return relevant search results, a search engine must keep its local repository synchronized to the Web, but it is usually impossible to attain perfect freshness. Hence, it is vital for a production search engine continually to monitor and improve repository freshness. Most previous freshness metrics, formulated in the context of developing better synchronization policies, focused on the web crawler while ignoring other parts of a search engine. But, the freshness of documents in a web crawler does not necessarily translate directly into the freshness of search results as seen by users. We propose metrics for measuring freshness from a user's perspective, which take into account the latency between when documents are crawled and when they are viewed by users, as well as the variation in user click and view frequency among different documents. We also describe a practical implementation of these metrics that were used in a production search engine.	User-centric content freshness metrics for search engines	NA:NA	2009
Chao-Jung Hsu:Chin-Yu Huang	In the past, some researches suggested that engineers can use combined software reliability growth models (SRGMs) to obtain more accurate reliability prediction during testing. In this paper, three weighted combinational models, namely, equal, linear, and nonlinear weight, are proposed for reliability estimation of web-based software. We further investigate the estimation accuracy of using genetic algorithm to determine the weight assignment for the proposed models. Preliminary result shows that the linearly and nonlinearly weighted combinational models have better prediction capability than single SRGM and equally weighted combinational model for web-based software.	Reliability analysis using weighted combinational models for web-based software	NA:NA	2009
Bin Lu:Zhaohui Wu:Yuan Ni:Guotong Xie:Chunying Zhou:Huajun Chen	With the proliferation of data APIs, it is not uncommon that users who have no clear ideas about data APIs will encounter difficulties to build Mashups to satisfy their requirements. In this paper, we present a semantic-based mashup navigation system, sMash that makes mashup building easy by constructing and visualizing a real-life data API network. We build a sample network by gathering more than 300 popular APIs and find that the relationships between them are so complex that our system will play an important role in navigating users and give them inspiration to build interesting mashups easily. The system is accessible at: http://www.dart.zju.edu.cn/mashup.	sMash: semantic-based mashup navigation for data API network	NA:NA:NA:NA:NA:NA	2009
Toufeeq Hussain:Rajesh Balakrishnan:Amar Viswanathan	This paper formulates a collaborative system for modeling business application. The system uses a Semantic Wiki to enable collaboration between the various stakeholders involved in the design of the system and translates the captured intelligence into business models which are used for designing a business system.	Semantic wiki aided business process specification	NA:NA:NA	2009
Jean-Sebastien Brunner:Patrick Gatellier	In this paper, we describe the fully dynamic semantic portal we implemented, integrating Semantic Web technologies and Service Oriented Architecture (SOA). The goals of the portal are twofold: first it helps administrators to easily propose new features in the portal using semantics to ease the orchestration process; secondly it automatically generates a customized user interface for these scenarios. This user interface takes into account different devices and assists end-users in the use of the portal taking benefit of context awareness. All the added-value of this portal is based on a core semantics defined by an ontology. We present here the main features of this portal and how it was implemented using state-of-the-art technologies and frameworks.	Raise semantics at the user level for dynamic and interactive SOA-based portals	NA:NA	2009
Yang Li:Tian-Bo Lu:Li Guo:Zhi-Hong Tian:Qin-Wu Nie	In this poster, based on our previous work in building a lightweight DDoS (Distributed Denial-of-Services) attacks detection mechanism for web server using TCM-KNN (Transductive Confidence Machines for K-Nearest Neighbors) and genetic algorithm based instance selection methods, we further propose a more efficient and effective instance selection method, named E-FCM (Extend Fuzzy C-Means). By using this method, we can obtain much cheaper training time for TCM-KNN while ensuring high detection performance. Therefore, the optimized mechanism is more suitable for lightweight DDoS attacks detection in real network environment.	Towards lightweight and efficient DDOS attacks detection for web server	NA:NA:NA:NA:NA	2009
Wei Wang:Florent Masseglia:Thomas Guyet:Rene Quiniou:Marie-Odile Cordier	Detection of web attacks is an important issue in current defense-in-depth security framework. In this paper, we propose a novel general framework for adaptive and online detection of web attacks. The general framework can be based on any online clustering methods. A detection model based on the framework is able to learn online and deal with "concept drift" in web audit data streams. Str-DBSCAN that we extended DBSCAN to streaming data as well as StrAP are both used to validate the framework. The detection model based on the framework automatically labels the web audit data and adapts to normal behavior changes while identifies attacks through dynamical clustering of the streaming data. A very large size of real HTTP Log data collected in our institute is used to validate the framework and the model. The preliminary testing results demonstrated its effectiveness.	A general framework for adaptive and online detection of web attacks	NA:NA:NA:NA:NA	2009
Yutaka Oiwa:Hiromitsu Takagi:Hajime Watanabe:Hirofumi Suzuki	We developed a new Web authentication protocol with password-based mutual authentication which prevents various kinds of phishing attacks. This protocol provides a protection of user's passwords against any phishers even if a dictionary attack is employed, and prevents phishers from imitating a false sense of successful authentication to users. The protocol is designed considering interoperability with many recent Web applications which requires many features which current HTTP authentication does not provide. The protocol is proposed as an Internet Draft submitted to IETF, and implemented in both server side (as an Apache extension) and client side (as a Mozilla-based browser and an IE-based one).	PAKE-based mutual HTTP authentication for preventing phishing attacks	NA:NA:NA:NA	2009
Jack Lindamood:Raymond Heatherly:Murat Kantarcioglu:Bhavani Thuraisingham	On-line social networks, such as Facebook, are increasingly utilized by many users. These networks allow people to publish details about themselves and connect to their friends. Some of the information revealed inside these networks is private and it is possible that corporations could use learning algorithms on the released data to predict undisclosed private information. In this paper, we explore how to launch inference attacks using released social networking data to predict undisclosed private information about individuals. We then explore the effectiveness of possible sanitization techniques that can be used to combat such inference attacks under different scenarios.	Inferring private information using social network data	NA:NA:NA:NA	2009
Ayman Farahat	We describe an optimize-and-dispatch approach for delivering pay-per-impression advertisements in online advertising. The platform provider for an advertising network commits to showing advertisers' banner ads while capping the number of advertising message shown to a unique user as the user transitions through the network. The traditional approach for enforcing frequency caps has been to use cross-site cookies to track users. However,cross-site cookies and other tracking mechanisms can infringe on the user privacy. In this paper, we propose a novel linear programming approach that decides when to show an ad to the user based solely on the page currently viewed by the users. We show that the frequency caps are fulfilled in expectation. We show the efficacy of that approach using simulation results.	Privacy preserving frequency capping in internet banner advertising	NA	2009
Andreas Juffinger:Elisabeth Lex	People use weblogs to express thoughts, present ideas and share knowledge, therefore weblogs are extraordinarily valuable resources, amongs others, for trend analysis. Trends are derived from the chronological sequence of blog post count per topic. The comparison with a reference corpus allows qualitative statements over identified trends. We propose a crosslanguage blog mining and trend visualisation system to analyse blogs across languages and topics. The trend visualisation facilitates the identification of trends and the comparison with the reference news article corpus. To prove the correctness of our system we computed the correlation between trends in blogs and news articles for a subset of blogs and topics. The evaluation corroborated our hypothesis of a high correlation coefficient for these subsets and therefore the correctness of our system for different languages and topics is proven.	Crosslanguage blog mining and trend visualisation	NA:NA	2009
Satoshi Sato	Automatic compilation of lexicon is a dream of lexicon compilers as well as lexicon users. This paper proposes a system that crawls English-Japanese person-name transliterations from the Web, which works a back-end collector for automatic compilation of bilingual person-name lexicon. Our crawler collected 561K transliterations in five months. From them, an English-Japanese person-name lexicon with 406K entries has been compiled by an automatic post processing. This lexicon is much larger than other similar resources including English-Japanese lexicon of HeiNER obtained from Wikipedia.	Crawling English-Japanese person-name transliterations from the web	NA	2009
Martin Atkinson:Erik Van der Goot	This paper presents a near real-time multilingual news monitoring and analysis system that forms the backbone of our research work. The system integrates technologies to address the problems related to information extraction and analysis of open source intelligence on the World Wide Web. By chaining together different techniques in text mining, automated machine learning and statistical analysis, we can automatically determine who, where and, to a certain extent, what is being reported in news articles.	Near real time information mining in multilingual news	NA:NA	2009
Xiaochuan Ni:Jian-Tao Sun:Jian Hu:Zheng Chen	In this paper, we try to leverage a large-scale and multilingual knowledge base, Wikipedia, to help effectively analyze and organize Web information written in different languages. Based on the observation that one Wikipedia concept may be described by articles in different languages, we adapt existing topic modeling algorithm for mining multilingual topics from this knowledge base. The extracted 'universal' topics have multiple types of representations, with each type corresponding to one language. Accordingly, new documents of different languages can be represented in a space using a group of universal topics, which makes various multilingual Web applications feasible.	Mining multilingual topics from wikipedia	NA:NA:NA:NA	2009
Philipp Scholl:Renato Domínguez García:Doreen Böhnstedt:Christoph Rensing:Ralf Steinmetz	The term web genre denotes the type of a given web resource, in contrast to the topic of its content. In this research, we focus on recognizing the web genres blog, wiki and forum. We present a set of features that exploit the hierarchical structure of the web page's HTML mark-up and thus, in contrast to related approaches, do not depend on a linguistic analysis of the page's content. Our results show that it is possible to achieve a very good accuracy for a fully language independent detection of structured web genres.	Towards language-independent web genre detection	NA:NA:NA:NA:NA	2009
Sukwon Chung:Dungjit Shiowattana:Pavel Dmitriev:Su Chan	In this paper, we report on a large-scale study of structural differences among the national webs. The study is based on a web-scale crawl conducted in the summer 2008. More specifically, we study two graphs derived from this crawl, the nation graph, with nodes corresponding to nations and edges - to links among nations, and the host graph, with nodes corresponding to hosts and edges - to hyperlinks among pages on the hosts. Contrary to some of the previous work [2], our results show that webs of different nations are often very different from each other, both in terms of their internal structure, and in terms of their connectivity with other nations.	The web of nations	NA:NA:NA:NA	2009
Matthias Keller:Martin Nussbaumer	In this paper we present an approach of generating Cascading Style Sheet documents automatically if the desired effect on the content elements is specified. While a Web user agent resolves the CSS rules and computes their effect, our approach handles the way back. We argue, that this can remarkably improve CSS productivity, since the process of CSS authoring always involves this direction implicitly. Our approach claims a new and innovative way to reuse chunks of markup together with its presentation. It furthermore bears potential for the optimization and reorganization of CSS documents. We describe criteria for CSS code quality we oriented on, including a quantitative indicator for the abstractness of a CSS presentation specification. An evaluation and recomputation of the CSS for 25.000 HTML documents shows that concerning these criteria the automatically generated code comes close to manually authored code.	Cascading style sheets: a novel approach towards productive styling with today's standards	NA:NA	2009
Guilherme A. Toda:Eli Cortez:Filipe Mesquita:Altigran S. da Silva:Edleno Moura:Marden Neubert	On the web of today the most prevalent solution for users to interact with data-intensive applications is the use of form-based interfaces composed by several data input fields, such as text boxes, radio buttons, pull-down lists, check boxes, etc. Although these interfaces are popular and effective, in many cases, free text interfaces are preferred over form-based ones. In this paper we discuss the proposal and the implementation of a novel IR-based method for using data rich free text to interact with form-based interfaces. Our solution takes a free text as input, extracts implicitly data values from it and fills appropriate fields using them. For this task, we rely on values of previous submissions for each field, which are freely obtained from the usage of form-based interfaces	Automatically filling form-based web interfaces with free text inputs	NA:NA:NA:NA:NA:NA	2009
Christian Kohlschütter	What makes template content in the Web so special that we need to remove it? In this paper I present a large-scale aggregate analysis of textual Web content, corroborating statistical laws from the field of Quantitative Linguistics. I analyze the idiosyncrasy of template content compared to regular "full text" content and derive a simple yet suitable quantitative model.	A densitometric analysis of web template content	NA	2009
Marta Gatius:Meritxell González	In this paper, we study how the performance and usability of web dialogue systems could be enhanced by using an appropriate representation of the different types of knowledge involved in communication: general dialogue mechanisms, specific domain-restricted linguistic and conceptual knowledge and information on how well the communication process is doing. We describe the experiments carried out to analyze how to improve this knowledge representation in the web dialogue system we developed.	A flexible dialogue system for enhancing web usability	NA:NA	2009
Thomas Gottron:Ludger Martin	Nowadays, information is primarily searched on the WWW. From a user perspective, the readability is an important criterion for measuring the accessibility and thereby the quality of an information. We show that modern content extraction algorithms help to estimate the readability of a web document quite accurate.	Estimating web site readability using content extraction	NA:NA	2009
Miquel Termens:Mireia Ribera:Mercè Porras:Marc Boldú:Andreu Sulé:Pilar Paris	This poster explains the changes introduced in the Web Content Accessibility Guidelines (WCAG) 2.0 from WCAG 1.0 and proposes a checklist for adapting existing websites. Finally, it describes the most common criticisms of the WCAG and places them in the context of its origin and initial aims.	Web content accessibility guidelines: from 1.0 to 2.0	NA:NA:NA:NA:NA:NA	2009
Keiji Yanai:Bingyu Qiu	We propose a novel method to detect cultural differences over the world automatically by using a large amount of geotagged images on the photo sharingWeb sites such as Flickr. We employ the state-of-the-art object recognition technique developed in the research community of computer vision to mine representative photos of the given concept for representative local regions from a large-scale unorganized collection of consumer-generated geotagged photos. The results help us understand how objects, scenes or events corresponding to the same given concept are visually different depending on local regions over the world.	Mining cultural differences from a large number of geotagged photos	NA:NA	2009
Zheng-Bin Dong:Guo-Jie Song:Kun-Qing Xie:Jing-Yao Wang	Mobile social network is a typical social network where one or more individuals of similar interests or commonalities, conversing and connecting with one another using the mobile phone. Our works in this paper focus on the experimental study for this kind of social network with the support of large-scale real mobile call data. The main contributions can be summarized as three-fold: firstly, a large-scale real mobile phone call log of one city has been extracted from a mobile phone carrier in China to construct mobile social network; secondly, common features of traditional social networks, such as power law distribution and small diameter etc, have been experimented, with which we confirm that the mobile social network is a typical scale-free network and has small-world phenomenon; lastly, different from traditional analytical methods, important properties of the actors, such as gender and age, have been introduced into our experiments with some interesting findings about human behavior, for example, the middle-age people are more active than the young and old people, and the female is unusual more active than the male while in the old age.	An experimental study of large-scale mobile social network	NA:NA:NA:NA	2009
Yang Li:Yi-Chuan Wu:Jian-Ying Zhang:Jin Peng:Hong-Luan Liao:Yun-Fei Zhang	In this poster, we present a novel P2P (Peer to Peer) based distributed services network (DSN), which is a next generation operable and manageable distributed core network architecture and functional structure, proposed by China Mobile for telecommunication services and wireless Internet. Our preliminary implementations of P2P VoIP (Voice over Internet Protocol) system over DSN platform demonstrate its effectiveness and promising future.	A P2P based distributed services network for next generation mobile internet communications	NA:NA:NA:NA:NA:NA	2009
Ning An:Raja Chatterjee:Mike Horhammer:Siva Ravada	In this paper, we briefly describe the implementation of various Open Geospatial Consortium Web Service Interface Standards in Oracle Spatial 11g. We highlight how we utilize Oracle's implementation of OASIS Web Services Security (WSS) to provide a robust security framework for these OGC Web Services. We also discuss our future direction in supporting OGC Web Service Interface Standards.	Securely implementing open geospatial consortium web service interface standards in oracle spatial	NA:NA:NA:NA	2009
Roberto Manca:Francesco Massidda:Davide Carboni	In this work, a novel mobile browser for geo-referenced pictures is introduced and described. We use the term browser to denote a system aimed at browsing pictures selected from a large set like Internet photo sharing services. The criteria to filter a subset of pictures to browse are three: the user's actual position, the user's actual heading, and the user's preferences. In this work we only focus on the first two criteria leaving the integration of user's preferences for future developments.	Visualization of Geo-annotated pictures in mobile phones	NA:NA:NA	2009
Adrian Popescu:Gregory Grefenstette	Uploading tourist photos is a popular activity on photo sharing platforms. These photographs and their associated metadata (tags, geo-tags, and temporal information) should be useful for mining information about the sites visited. However, user-supplied metadata are often noisy and efficient filtering methods are needed before extracting useful knowledge. We focus here on exploiting temporal information, associated with tourist sites that appear in Flickr. From automatically filtered sets of geo-tagged photos, we deduce answers to questions like "how long does it take to visit a tourist attraction?" or "what can I visit in one day in this city?" Our method is evaluated and validated by comparing the automatically obtained visit duration times to manual estimations.	Deducing trip related information from flickr	NA:NA	2009
Guang-Gang Geng:Qiudan Li:Xinchang Zhang	Robust statistical learning based web spam detection system often requires large amounts of labeled training data. However, labeled samples are more difficult, expensive and time consuming to obtain than unlabeled ones. This paper proposed link based semi-supervised learning algorithms to boost the performance of a classifier, which integrates the traditional Self-training with the topological dependency based link learning. The experiments with a few labeled samples on standard WEBSPAM-UK2006 benchmark showed that the algorithms are effective.	Link based small sample learning for web spam detection	NA:NA:NA	2009
Haiqiang Zuo:Weiming Hu:Ou Wu:Yunfei Chen:Guan Luo	Image spam is a new obfuscating method which spammers invented to more effectively bypass conventional text based spam filters. In this paper, we extract local invariant features of images and run a one-class SVM classifier which uses the pyramid match kernel as the kernel function to detect image spam. Experimental results demonstrate that our algorithm is effective for fighting image spam.	Detecting image spam using local invariant features and pyramid match kernel	NA:NA:NA:NA:NA	2009
Mingmin Chi:Peiwu Zhang:Yingbin Zhao:Rui Feng:Xiangyang Xue	General image retrieval is often carried out by a text-based search engine, such as Google Image Search. In this case, natural language queries are used as input to the search engine. Usually, the user queries are quite ambiguous and the returned results are not well-organized as the ranking often done by the popularity of an image. In order to address these problems, we propose to use both textual and visual contents of retrieved images to reRank web retrieved results. In particular, a machine learning technique, a multi-view clustering algorithm is proposed to reorganize the original results provided by the text-based search engine. Preliminary results validate the effectiveness of the proposed framework.	Web image retrieval reranking with multi-view clustering	NA:NA:NA:NA:NA	2009
Siddharth Mitra:Mayank Agrawal:Amit Yadav:Niklas Carlsson:Derek Eager:Anirban Mahanti	NA	Characterizing web-based video sharing workloads	NA:NA:NA:NA:NA:NA	2009
Kerstin Bischoff:Claudiu S. Firan:Raluca Paiu	Music theme annotations would be really beneficial for supporting retrieval, but are often neglected by users while annotating. Thus, in order to support users in tagging and to fill the gaps in the tag space, in this paper we develop algorithms for recommending theme annotations. Our methods exploit already existing user tags, the lyrics of music tracks, as well as combinations of both. We compare the results for our recommended theme annotations against genre and style recommendations - a much easier and already studied task. We evaluate the quality of our recommended tags against an expert ground truth data set. Our results are promising and provide interesting insights into possible extensions for music tagging systems to support music search.	Deriving music theme annotations from user tags	NA:NA:NA	2009
Junyan Zhu:Can Wang:Xiaofei He:Jiajun Bu:Chun Chen:Shujie Shang:Mingcheng Qu:Gang Lu	Social annotations on a Web document are highly generalized description of topics contained in that page. Their tagged frequency indicates the user attentions with various degrees. This makes annotations a good resource for summarizing multiple topics in a Web page. In this paper, we present a tag-oriented Web document summarization approach by using both document content and the tags annotated on that document. To improve summarization performance, a new tag ranking algorithm named EigenTag is proposed in this paper to reduce noise in tags. Meanwhile, association mining technique is employed to expand tag set to tackle the sparsity problem. Experimental results show our tag-oriented summarization has a significant improvement over those not using tags.	Tag-oriented document summarization	NA:NA:NA:NA:NA:NA:NA:NA	2009
Jun Yan:Ning Liu:Elaine Qing Chang:Lei Ji:Zheng Chen	Both search engine click-through log and social annotation have been utilized as user feedback for search result re-ranking. However, to our best knowledge, no previous study has explored the correlation between these two factors for the task of search result ranking. In this paper, we show that the gap between search queries and social tags of the same Web page can well reflect its user preference score. Motivated by this observation, we propose a novel algorithm, called Query-Tag-Gap (QTG), to re-rank search results for better user satisfaction. Intuitively, on one hand, the search users' intentions are generally described by their queries before they read the search results. On the other hand, the Web annotators semantically tag Web pages after they read the content of the pages. The difference between users' recognition of the same page before and after they read it is a good reflection of user satisfaction. In this extended abstract, we formally define the query set and tag set of the same page as users' pre- and post- knowledge respectively. We empirically show the strong correlation between user satisfaction and user's knowledge gap before and after reading the page. Based on this gap, experiments have shown outstanding performance of our proposed QTG algorithm in search result re-ranking.	Search result re-ranking based on gap between search queries and social tags	NA:NA:NA:NA:NA	2009
Takeharu Eda:Toshio Uchiyama:Tadasu Uchiyama:Masatoshi Yoshikawa	In order to create more attractive tagclouds that get people interested in tagged content, we propose a simple but novel tagcloud where font size is determined by tag's entropy value, not the popularity to its content. Our method raises users' emotional interest in the content by emphasizing more emotional tags. Our initial experiments show that emotional tagclouds attract more attention than normal tagclouds at first look; thus they will enhance the role of tagcloud as a social signaller.	Signaling emotion in tagclouds	NA:NA:NA:NA	2009
Lili Jiang:Jianyong Wang:Ning An:Shengyuan Wang:Jian Zhan:Lian Li	The ever growing volume of Web data makes it increasingly challenging to accurately find relevant information about a specific person on the Web. To address the challenge caused by name ambiguity in Web people search, this paper explores a novel graph-based framework to both disambiguate and tag people entities in Web search results. Experimental results demonstrate the effectiveness of the proposed framework in tag discovery and name disambiguation.	Two birds with one stone: a graph-based framework for disambiguating and tagging people names in web search	NA:NA:NA:NA:NA:NA	2009
Santanu Kolay:Ali Dasdan	Social bookmarking has emerged as a growing source of human generated content on the web. In essence, bookmarking involves URLs and tags on them. In this paper, we perform a large scale study of the usefulness of bookmarked URLs from the top social bookmarking site Delicious. Instead of focusing on the dimension of tags, which has been covered in the previous work, we explore social bookmarking from the dimension of URLs. More specifically, we investigate the Delicious URLs and their content to quantify their value to a search engine. For their value in leading to good content, we show that the Delicious URLs have higher quality content and more external outlinks. For their value in satisfying users, we show that the Delicious URLs have more clicked URLs as well as get more clicks. We suggest that based on their value, the Delicious URLs should be used as another source of seed URLs for crawlers.	The value of socially tagged urls for a search engine	NA:NA	2009
Dell Zhang:Robert Mao:Wei Li	How often do tags recur? How hard is predicting tag recurrence? What tags are likely to recur? We try to answer these questions by analysing the RSDC08 dataset, in both individual and collective settings. Our findings provide useful insights for the development of tag suggestion techniques etc.	The recurrence dynamics of social tagging	NA:NA:NA	2009
Markus Krause:Hidir Aras	Collaborative Tagging is a powerful method to create folksonomies that can be used to grasp/filter user preferences or enhance web search. Recent research has shown that depending on the number of users and the quality of user-provided tags powerful community-driven semantics or "ontologies" can emerge - as it was evident analyzing user data from social web applications such as del.icio.us or Flickr. Unfortunately, most web pages do not contain tags and, thus, no vocabulary that describes the information provided. A common problem in web page annotation is to motivate users for constant participation, i.e. tagging. In this paper we describe our approach of a binary verification game that embeds collaborative tagging into on-line games in order to produce domain specific folksonomies.	Playful tagging: folksonomy generation using online games	NA:NA	2009
Ning Liu:Jun Yan:Weiguo Fan:Qiang Yang:Zheng Chen	A pressing task during the unification process is to identify a user's vertical search intention based on the user's query. In this paper, we propose a novel method to propagate social annotation, which includes user-supplied tag data, to both queries and VSEs for semantically bridging them. Our proposed algorithm consists of three key steps: query annotation, vertical annotation and query intention identification. Our algorithm, referred to as TagQV, verifies that the social tagging can be propagated to represent Web objects such as queries and VSEs besides Web pages. Experiments on real Web search queries demonstrate the effectiveness of TagQV in query intention identification.	Identifying vertical search intention of query through social tagging propagation	NA:NA:NA:NA:NA	2009
Einat Amitay:David Carmel:Nadav Har'El:Shila Ofek-Koifman:Aya Soffer:Sivan Yogev:Nadav Golbandi	We explore new ways of improving a search engine using data from Web 2.0 applications such as blogs and social bookmarks. This data contains entities such as documents, people and tags, and relationships between them. We propose a simple yet effective method, based on faceted search, that treats all entities in a unified manner: returning all of them (documents, people and tags) on every search, and allowing all of them to be used as search terms. We describe an implementation of such a social search engine on the intranet of a large enterprise, and present large-scale experiments which verify the validity of our approach.	Social search and discovery using a unified approach	NA:NA:NA:NA:NA:NA:NA	2009
Yu-Ru Lin:Jimeng Sun:Paul Castro:Ravi Konuru:Hari Sundaram:Aisling Kelliher	Social media websites promote diverse user interaction on media objects as well as user actions with respect to other users. The goal of this work is to discover community structure in rich media social networks, and observe how it evolves over time, through analysis of multi-relational data. The problem is important in the enterprise domain where extracting emergent community structure on enterprise social media, can help in forming new collaborative teams, aid in expertise discovery, and guide long term enterprise reorganization. Our approach consists of three main parts: (1) a relational hypergraph model for modeling various social context and interactions; (2) a novel hypergraph factorization method for community extraction on multi-relational social data; (3) an on-line method to handle temporal evolution through incremental hypergraph factorization. Extensive experiments on real-world enterprise data suggest that our technique is scalable and can extract meaningful communities. To evaluate the quality of our mining results, we use our method to predict users' future interests. Our prediction outperforms baseline methods (frequency counts, pLSA) by 36-250% on the average, indicating the utility of leveraging multi-relational social context by using our method.	Extracting community structure through relational hypergraphs	NA:NA:NA:NA:NA:NA	2009
Hyoseop Shin:Jeehoon Lee	Searching posts effectively has become an important issue in large-scale online communities. Especially, if search users have different inclinations when they search posts, they have different kinds of posts in their minds. To address this problem, in this paper, we propose a scheme of ranking posts based on search users' inclination. User ranking score is employed to capture posts that are relevant to a specific user inclination. Specifically, we present a scheme to rank posts in terms of user expertise and popularity. Experimental results show that different user inclinations can produce quite different search results and the proposed scheme achieves about 70% accuracy.	Ranking user-created contents by search user's inclination in online communities	NA:NA	2009
Praphul Chandra:Ajay Gupta	Web is being extensively used for personal expression, which includes ratings, reviews, recommendations, blogs. This user created content, e.g. book review on Amazon.com, becomes the property of the website, and the user often does not have easy access to it. In some cases, user's feedback may get averaged with feedback from other users e.g. ratings of a video. We argue that the creator of such content needs to be able to retain (a link to) her created content. We introduce the concept of MEB which is a user controlled store of such retained links. A MEB allows a user to access/share all the reviews she has given on different websites. With this capability users can allow their friends to search through their feedback. Searching through one's social network allows harnessing the power of social networks where known relationships provide the context & trust necessary to interpret feedback.	Retaining personal expression for social search	NA:NA	2009
Dewei Chen:Jie Tang:Juanzi Li:Lizhu Zhou	In this paper, we study a novel problem of staring people discovery from social networks, which is concerned with finding people who are not only authoritative but also sociable in the social network. We formalize this problem as an optimization programming problem. Taking the co-author network as a case study, we define three objective functions and propose two methods to combine these objective functions. A genetic algorithm based method is further presented to solve this problem. Experimental results show that the proposed solution can effectively find the staring people from social networks.	Discovering the staring people from social networks	NA:NA:NA:NA	2009
Dmitry Lizorkin:Olena Medelyan:Maria Grineva	We present the results of a community detection analysis of the Wikipedia graph. Distinct communities in Wikipedia contain semantically closely related articles. The central topic of a community can be identified using PageRank. Extracted communities can be organized hierarchically similar to manually created Wikipedia category structure.	Analysis of community structure in Wikipedia	NA:NA:NA	2009
Akiyo Nadamoto:Eiji Aramaki:Takeshi Abekawa:Yohei Murakami	In community-type content such as blogs and SNSs, we call the user's unawareness of information as a "content hole" and the search for this information as a "content hole search." A content hole search differs from similarity searching and has a variety of types. In this paper, we propose different types of content holes and define each type. We also propose an analysis of dialogue related to community-type content and introduce content hole search by using Wikipedia as an example.	Content hole search in community-type content	NA:NA:NA:NA	2009
Manolis Platakis:Dimitrios Kotsakos:Dimitrios Gunopulos	Over the last few years, blogs (web logs) have gained massive popularity and have become one of the most influential web social media in our times. Every blog post in the Blogosphere has a well defined timestamp, which is not taken into account by search engines. By conducting research regarding this feature of the Blogosphere, we can attempt to discover bursty terms and correlations between them during a time interval. We apply Kleinberg's automaton on extracted titles of blog posts to discover bursty terms, we introduce a novel representation of a term's burstiness evolution called State Series and we employ a Euclidean-based distance metric to discover potential correlations between terms without taking into account their context. We evaluate the results trying to match them with real life events. Finally, we propose some ideas for further evaluation techniques and future research in the field.	Searching for events in the blogosphere	NA:NA:NA	2009
Xudong Tu:Xin-Jing Wang:Dan Feng:Lei Zhang	Due to the lexical gap between questions and answers, automatically detecting right answers becomes very challenging for community question-answering sites. In this paper, we propose an analogical reasoning-based method. It treats questions and answers as relational data and ranks an answer by measuring the analogy of its link to a query with the links embedded in previous relevant knowledge; the answer that links in the most analogous way to the new question is assumed to be the best answer. We based our experiments on 29.8 million Yahoo!Answer question-answer threads and showed the effectiveness of the approach.	Ranking community answers via analogical reasoning	NA:NA:NA:NA	2009
Mingcheng Qu:Guang Qiu:Xiaofei He:Cheng Zhang:Hao Wu:Jiajun Bu:Chun Chen	User-Interactive Question Answering (QA) communities such as Yahoo! Answers are growing in popularity. However, as these QA sites always have thousands of new questions posted daily, it is difficult for users to find the questions that are of interest to them. Consequently, this may delay the answering of the new questions. This gives rise to question recommendation techniques that help users locate interesting questions. In this paper, we adopt the Probabilistic Latent Semantic Analysis (PLSA) model for question recommendation and propose a novel metric to evaluate the performance of our approach. The experimental results show our recommendation approach is effective.	Probabilistic question recommendation for question answering communities	NA:NA:NA:NA:NA:NA:NA	2009
Nish Parikh:Neel Sundaresan	In this paper, we describe a buzz-based recommender system based on a large source of queries in an eCommerce application. The system detects bursts in query trends. These bursts are linked to external entities like news and inventory information to find the queries currently in-demand which we refer to as buzz queries. The system follows the paradigm of limited quantity merchandising, in the sense that on a per-day basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity, and improving activity and stickiness on the site. A semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood.	Buzz-based recommender system	NA:NA	2009
Riddhiman Ghosh:Mohamed Dekhil	In this paper we describe techniques for the discovery and construction of user profiles. Leveraging from the emergent data web, our system addresses the problem of sparseness of user profile information currently faced by both asserted and inferred profile systems. A profile mediator, that dynamically builds the most suitable user profile for a particular service or interaction in real-time, is employed in our prototype implementation.	Discovering user profiles	NA:NA	2009
Ganesh Agarwal:Govind Kabra:Kevin Chen-Chuan Chang	We propose to mine structured query templates from search logs, for enabling rich query interpretation that recognizes both query intents and associated attributes. We formalize the notion of template as a sequence of keywords and domain attributes, and our objective is to discover templates with high precision and recall for matching queries in a domain of interest. Our solution bootstraps from small seed input knowledge to discover relevant query templates, by harnessing the wealth of information available in search logs. We model this information in a tri-partite QueST network of queries, sites, and templates. We propose a probabilistic inferencing framework based on the dual metrics of precision and recall- and we show that the dual inferencing correspond respectively to the random walks in backward and forward directions. We deployed and tested our algorithm over a real-world search log of 15 million queries. The algorithm achieved accuracy of as high as 90% (on F-measure), with little seed knowledge and even with incomplete domain schema.	Towards rich query interpretation: walking back and forth for mining query templates	NA:NA:NA	2010
Mohammad Alrifai:Dimitrios Skoutas:Thomas Risse	Web service composition enables seamless and dynamic integration of business applications on the web. The performance of the composed application is determined by the performance of the involved web services. Therefore, non-functional, quality of service aspects are crucial for selecting the web services to take part in the composition. Identifying the best candidate web services from a set of functionally-equivalent services is a multi-criteria decision making problem. The selected services should optimize the overall QoS of the composed application, while satisfying all the constraints specified by the client on individual QoS parameters. In this paper, we propose an approach based on the notion of skyline to effectively and efficiently select services for composition, reducing the number of candidate services to be considered. We also discuss how a provider can improve its service to become more competitive and increase its potential of being included in composite applications. We evaluate our approach experimentally using both real and synthetically generated datasets.	Selecting skyline services for QoS-based web service composition	NA:NA:NA	2010
Nikolay Archak	Crowdsourcing is a new Web phenomenon, in which a firm takes a function once performed in-house and outsources it to a crowd, usually in the form of an open contest. Designing efficient crowdsourcing mechanisms is not possible without deep understanding of incentives and strategic choices of all participants. This paper presents an empirical analysis of determinants of individual performance in multiple simultaneous crowdsourcing contests using a unique dataset for the world's largest competitive software development portal: TopCoder.com. Special attention is given to studying the effects of the reputation system currently used by TopCoder.com on behavior of contestants. We find that individual specific traits together with the project payment and the number of project requirements are significant predictors of the final project quality. Furthermore, we find significant evidence of strategic behavior of contestants. High rated contestants face tougher competition from their opponents in the competition phase of the contest. In order to soften the competition, they move first in the registration phase of the contest, signing up early for particular projects. Although registration in TopCoder contests is non-binding, it deters entry of opponents in the same contest; our lower bound estimate shows that this strategy generates significant surplus gain to high rated contestants. We conjecture that the reputation + cheap talk mechanism employed by TopCoder has a positive effect on allocative efficiency of simultaneous all-pay contests and should be considered for adoption in other crowdsourcing platforms.	Money, glory and cheap talk: analyzing strategic behavior of contestants in simultaneous crowdsourcing contests on TopCoder.com	NA	2010
Nikolay Archak:Vahab S. Mirrokni:S. Muthukrishnan	Consider an online ad campaign run by an advertiser. The ad serving companies that handle such campaigns record users' behavior that leads to impressions of campaign ads, as well as users' responses to such impressions. This is summarized and reported to the advertisers to help them evaluate the performance of their campaigns and make better budget allocation decisions. The most popular reporting statistics are the click-through rate and the conversion rate. While these are indicative of the effectiveness of an ad campaign, the advertisers often seek to understand more sophisticated long-term effects of their ads on the brand awareness and the user behavior that leads to the conversion, thus creating a need for the reporting measures that can capture both the duration and the frequency of the pathways to user conversions. In this paper, we propose an alternative data mining framework for analyzing user-level advertising data. In the aggregation step, we compress individual user histories into a graph structure, called the adgraph, representing local correlations between ad events. For the reporting step, we introduce several scoring rules, called the adfactors (AF), that can capture global role of ads and ad paths in the adgraph, in particular, the structural correlation between an ad impression and the user conversion. We present scalable local algorithms for computing the adfactors; all algorithms were implemented using the MapReduce programming model and the Pregel framework. Using an anonymous user-level dataset of sponsored search campaigns for eight different advertisers, we evaluate our framework with different adgraphs and adfactors in terms of their statistical fit to the data, and show its value for mining the long-term behavioral patterns in the advertising data.	Mining advertiser-specific user behavior using adfactors	NA:NA:NA	2010
Medha Atre:Vineet Chaoji:Mohammed J. Zaki:James A. Hendler	The Semantic Web community, until now, has used traditional database systems for the storage and querying of RDF data. The SPARQL query language also closely follows SQL syntax. As a natural consequence, most of the SPARQL query processing techniques are based on database query processing and optimization techniques. For SPARQL join query optimization, previous works like RDF-3X and Hexastore have proposed to use 6-way indexes on the RDF data. Although these indexes speed up merge-joins by orders of magnitude, for complex join queries generating large intermediate join results, the scalability of the query processor still remains a challenge. In this paper, we introduce (i) BitMat - a compressed bit-matrix structure for storing huge RDF graphs, and (ii) a novel, light-weight SPARQL join query processing method that employs an initial pruning technique, followed by a variable-binding-matching algorithm on BitMats to produce the final results. Our query processing method does not build intermediate join tables and works directly on the compressed data. We have demonstrated our method against RDF graphs of upto 1.33 billion triples - the largest among results published until now (single-node, non-parallel systems), and have compared our method with the state-of-the-art RDF stores - RDF-3X and MonetDB. Our results show that the competing methods are most effective with highly selective queries. On the other hand, BitMat can deliver 2-3 orders of magnitude better performance on complex, low-selectivity queries over massive data.	Matrix "Bit" loaded: a scalable lightweight join query processor for RDF data	NA:NA:NA:NA	2010
Anne Aula:Rehan M. Khan:Zhiwei Guan:Paul Fontes:Peter Hong	We investigated the efficacy of visual and textual web page previews in predicting the helpfulness of web pages related to a specific topic. We ran two studies in the usability lab and collected data through an online survey. Participants (total of 245) were asked to rate the expected helpfulness of a web page based on a preview (four different thumbnail variations: a textual web page summary, a thumbnail/title/URL combination, a title/URL combination). In the lab studies, the same participants also rated the helpfulness of the actual web pages themselves. In the online study, the web page ratings were collected from a separate group of participants. Our results show that thumbnails add information about the relevance of web pages that is not available in the textual summaries of web pages (title, snippet & URL). However, showing only thumbnails, with no textual information, results in poorer performance than showing only textual summaries. The prediction inaccuracy caused by textual vs. visual previews was different: textual previews tended to make users overestimate the helpfulness of web pages, whereas thumbnails made users underestimate the helpfulness of web pages in most cases. In our study, the best performance was obtained by combining sufficiently large thumbnails (at least 200x200 pixels) with page titles and URLs - and it was better to make users focus primarily on the thumbnail by placing the title and URL below the thumbnail. Our studies highlighted four key aspects that affect the performance of previews: the visual/textual mode of the previews, the zoom level and size of the thumbnail, as well as the positioning of key information elements.	A comparison of visual and textual page previews in judging the helpfulness of web pages	NA:NA:NA:NA:NA	2010
Lars Backstrom:Eric Sun:Cameron Marlow	Geography and social relationships are inextricably intertwined; the people we interact with on a daily basis almost always live near us. As people spend more time online, data regarding these two dimensions -- geography and social relationships -- are becoming increasingly precise, allowing us to build reliable models to describe their interaction. These models have important implications in the design of location-based services, security intrusion detection, and social media supporting local communities. Using user-supplied address data and the network of associations between members of the Facebook social network, we can directly observe and measure the relationship between geography and friendship. Using these measurements, we introduce an algorithm that predicts the location of an individual from a sparse set of located users with performance that exceeds IP-based geolocation. This algorithm is efficient and scalable, and could be run on a network containing hundreds of millions of users.	Find me if you can: improving geographical prediction with social and spatial proximity	NA:NA:NA	2010
Hongji Bao:Edward Y. Chang	In this paper, we present AdHeat, a social ad model considering user influence in addition to relevance for matching ads. Traditionally, ad placement employs the relevance model. Such a model matches ads with Web page content, user interests, or both. We have observed, however, on social networks that the relevance model suffers from two shortcomings. First, influential users (users who contribute opinions) seldom click ads that are highly relevant to their expertise. Second, because influential users' contents and activities are attractive to other users, hint words summarizing their expertise and activities may be widely preferred. Therefore, we propose AdHeat, which diffuses hint words of influential users to others and then matches ads for each user with aggregated hints. We performed experiments on a large online Q&A community with half a million users. The experimental results show that AdHeat outperforms the relevance model on CTR (click through rate) by significant margins.	AdHeat: an influence-based diffusion model for propagating hints to match ads	NA:NA	2010
John J. Barton:Jan Odvarko	Breakpoints are perhaps the quintessential feature of a de-bugger: they allow a developer to stop time and study the program state. Breakpoints are typically specified by selecting a line of source code. For large, complex, web pages with multiple developers, the relevant source line for a given user interface problem may not be known to the developer. In this paper we describe the implementation of breakpoints in dynamically created source, and on error messages, network events, DOMmutation, DOMobject property changes, and CSS style rule updates. Adding these domain-specific breakpoints to a general-purpose debugger for Javascript allows the developer to initiate the debugging process via Web page abstractions rather than lower level source code views. The breakpoints are implemented in the open source Fire-bug project, version 1.5, for the Firefox Web browser.	Dynamic and graphical web page breakpoints	NA:NA	2010
Daniel Bates:Adam Barth:Collin Jackson	Cross-site scripting flaws have now surpassed buffer overflows as the world's most common publicly-reported security vulnerability. In recent years, browser vendors and researchers have tried to develop client-side filters to mitigate these attacks. We analyze the best existing filters and find them to be either unacceptably slow or easily circumvented. Worse, some of these filters could introduce vulnerabilities into sites that were previously bug-free. We propose a new filter design that achieves both high performance and high precision by blocking scripts after HTML parsing but before execution. Compared to previous approaches, our approach is faster, protects against more vulnerabilities, and is harder for attackers to abuse. We have contributed an implementation of our filter design to the WebKit open source rendering engine, and the filter is now enabled by default in the Google Chrome browser.	Regular expressions considered harmful in client-side XSS filters	NA:NA:NA	2010
Michael Bendersky:Evgeniy Gabrilovich:Vanja Josifovski:Donald Metzler	The core task of sponsored search is to retrieve relevant ads for the user's query. Ads can be retrieved either by exact match, when their bid term is identical to the query, or by advanced match, which indexes ads as documents and is similar to standard information retrieval (IR). Recently, there has been a great deal of research into developing advanced match ranking algorithms. However, no previous research has addressed the ad indexing problem. Unlike most traditional search problems, the ad corpus is defined hierarchically in terms of advertiser accounts, campaigns, and ad groups, which further consist of creatives and bid terms. This hierarchical structure makes indexing highly non-trivial, as naively indexing all possible "displayable" ads leads to a prohibitively large and ineffective index. We show that ad retrieval using such an index is not only slow, but its precision is suboptimal as well. We investigate various strategies for compact, hierarchy-aware indexing of sponsored search ads through adaptation of standard IR indexing techniques. We also propose a new ad retrieval method that yields more relevant ads by exploiting the structured nature of the ad corpus. Experiments carried out over a large ad test collection from a commercial search engine show that our proposed methods are highly effective and efficient compared to more standard indexing and retrieval approaches.	The anatomy of an ad: structured indexing and retrieval for sponsored search	NA:NA:NA:NA	2010
Paul N. Bennett:Krysta Svore:Susan T. Dumais	Many have speculated that classifying web pages can improve a search engine's ranking of results. Intuitively results should be more relevant when they match the class of a query. We present a simple framework for classification-enhanced ranking that uses clicks in combination with the classification of web pages to derive a class distribution for the query. We then go on to define a variety of features that capture the match between the class distributions of a web page and a query, the ambiguity of a query, and the coverage of a retrieved result relative to a query's set of classes. Experimental results demonstrate that a ranker learned with these features significantly improves ranking over a competitive baseline. Furthermore, our methodology is agnostic with respect to the classification space and can be used to derive query classes for a variety of different taxonomies.	Classification-enhanced ranking	NA:NA:NA	2010
Edward Benson:Adam Marcus:David Karger:Samuel Madden	We introduce a client-server toolkit called Sync Kit that demonstrates how client-side database storage can improve the performance of data intensive websites. Sync Kit is designed to make use of the embedded relational database defined in the upcoming HTML5 standard to offload some data storage and processing from a web server onto the web browsers to which it serves content. Our toolkit provides various strategies for synchronizing relational database tables between the browser and the web server, along with a client-side template library so that portions web applications may be executed client-side. Unlike prior work in this area, Sync Kit persists both templates and data in the browser across web sessions, increasing the number of concurrent connections a server can handle by up to a factor of four versus that of a traditional server-only web stack and a factor of three versus a recent template caching approach.	Sync kit: a persistent client-side database caching toolkit for data intensive websites	NA:NA:NA:NA	2010
Jiang Bian:Xin Li:Fan Li:Zhaohui Zheng:Hongyuan Zha	Many ranking algorithms applying machine learning techniques have been proposed in informational retrieval and Web search. However, most of existing approaches do not explicitly take into account the fact that queries vary significantly in terms of ranking and entail different treatments regarding the ranking models. In this paper, we apply a divide-and-conquer framework for ranking specialization, i.e. learning multiple ranking models by addressing query difference. We first generate query representation by aggregating ranking features through pseudo feedbacks, and employ unsupervised clustering methods to identify a set of ranking-sensitive query topics based on training queries. To learn multiple ranking models for respective ranking-sensitive query topics, we define a global loss function by combining the ranking risks of all query topics, and we propose a unified SVM-based learning process to minimize the global loss. Moreover, we employ an ensemble approach to generate the ranking result for each test query by applying a set of ranking models of the most appropriate query topics. We conduct experiments using a benchmark dataset for learning ranking functions as well as a dataset from a commercial search engine. Experimental results show that our proposed approach can significantly improve the ranking performance over existing single-model approaches as well as straightforward local ranking approaches, and the automatically identified ranking-sensitive topics are more useful for enhancing ranking performance than pre-defined query categorization.	Ranking specialization for web search: a divide-and-conquer approach by using topical RankSVM	NA:NA:NA:NA:NA	2010
Domenico Bianculli:Walter Binder:Mauro Luigi Drago	Middleware for Web service compositions, such as BPEL engines, provides the execution environment for services as well as additional functionalities, such as monitoring and self-tuning. Given its role in service provisioning, it is very important to assess the performance of middleware in the context of a SOA. This paper presents SOABench, a framework for the automatic generation and execution of testbeds for benchmarking middleware for composite Web services and for assessing the performance of existing SOA infrastructures. SOABench defines a testbed model characterized by the composite services to execute, the workload to generate, the deployment configuration to use, the performance metrics to gather, the data analyses to perform on them, and the reports to produce. We have validated SOABench by benchmarking the performance of different BPEL engines.	Automated performance assessment for service-oriented middleware: a case study on BPEL engines	NA:NA:NA	2010
Danushka Tarupathi Bollegala:Yutaka Matsuo:Mitsuru Ishizuka	Extracting semantic relations among entities is an important first step in various tasks in Web mining and natural language processing such as information extraction, relation detection, and social network mining. A relation can be expressed extensionally by stating all the instances of that relation or intensionally by defining all the paraphrases of that relation. For example, consider the ACQUISITION relation between two companies. An extensional definition of ACQUISITION contains all pairs of companies in which one company is acquired by another (e.g. (YouTube, Google) or (Powerset, Microsoft)). On the other hand we can intensionally define ACQUISITION as the relation described by lexical patterns such as X is acquired by Y, or Y purchased X, where X and Y denote two companies. We use this dual representation of semantic relations to propose a novel sequential co-clustering algorithm that can extract numerous relations efficiently from unlabeled data. We provide an efficient heuristic to find the parameters of the proposed coclustering algorithm. Using the clusters produced by the algorithm, we train an L1 regularized logistic regression model to identify the representative patterns that describe the relation expressed by each cluster. We evaluate the proposed method in three different tasks: measuring relational similarity between entity pairs, open information extraction (Open IE), and classifying relations in a social network system. Experiments conducted using a benchmark dataset show that the proposed method improves existing relational similarity measures. Moreover, the proposed method significantly outperforms the current state-of-the-art Open IE systems in terms of both precision and recall. The proposed method correctly classifies 53 relation types in an online social network containing 470; 671 nodes and 35; 652; 475 edges, thereby demonstrating its efficacy in real-world relation detection tasks.	Relational duality: unsupervised extraction of semantic relations between entities on the web	NA:NA:NA	2010
Alessandro Bozzon:Marco Brambilla:Stefano Ceri:Piero Fraternali	In this paper we propose the Liquid Query paradigm, to support users in finding responses to multi-domain queries through exploratory information seeking across structured information sources (Web documents, deep Web data, and personal data repositories), wrapped by means of a uniform notion of search service. Liquid Query aims at filling the gap between general-purpose search engines, which are unable to find information spanning multiple topics, and domain-specific search systems, which cannot go beyond their domain limits. The Liquid Query interface consists of interaction primitives that let users pose questions and explore results spanning over multiple sources incrementally, thus getting closer and closer to the sought information. We demonstrate our approach with a prototype built upon the YQL (Yahoo! Query Language) framework.	Liquid query: multi-domain exploratory search on the web	NA:NA:NA:NA	2010
Falk Brauer:Michael Huber:Gregor Hackenbroich:Ulf Leser:Felix Naumann:Wojciech M. Barczynski	Enterprise Search (ES) is different from traditional IR due to a number of reasons, among which the high level of ambiguity of terms in queries and documents and existence of graph-structured enterprise data (ontologies) that describe the concepts of interest and their relationships to each other, are the most important ones. Our method identifies concepts from the enterprise ontology in the query and corpus. We propose a ranking scheme for ontology sub-graphs on top of approximately matched token q-grams. The ranking leverages the graph-structure of the ontology to incorporate not explicitly mentioned concepts. It improves previous solutions by using a fine-grained ranking function that is specifically designed to cope with high levels of ambiguity. This method is able to capture much more of the semantics of queries and documents than previous techniques. We prove this claim by an evaluation of our method in three real-life scenarios from two different domains, and found it to consistently be superior both in terms of precision and recall.	Graph-based concept identification and disambiguation for enterprise search	NA:NA:NA:NA:NA:NA	2010
Berkant Barla Cambazoglu:Flavio P. Junqueira:Vassilis Plachouras:Scott Banachowski:Baoqiu Cui:Swee Lim:Bill Bridge	Commercial Web search engines have to process user queries over huge Web indexes under tight latency constraints. In practice, to achieve low latency, large result caches are employed and a portion of the query traffic is served using previously computed results. Moreover, search engines need to update their indexes frequently to incorporate changes to the Web. After every index update, however, the content of cache entries may become stale, thus decreasing the freshness of served results. In this work, we first argue that the real problem in today's caching for large-scale search engines is not eviction policies, but the ability to cope with changes to the index, i.e., cache freshness. We then introduce a novel algorithm that uses a time-to-live value to set cache entries to expire and selectively refreshes cached results by issuing refresh queries to back-end search clusters. The algorithm prioritizes the entries to refresh according to a heuristic that combines the frequency of access with the age of an entry in the cache. In addition, for setting the rate at which refresh queries are issued, we present a mechanism that takes into account idle cycles of back-end servers. Evaluation using a real workload shows that our algorithm can achieve hit rate improvements as well as reduction in average hit ages. An implementation of this algorithm is currently in production use at Yahoo!.	A refreshing perspective of search engine caching	NA:NA:NA:NA:NA:NA:NA	2010
Brett Cannon:Eric Wohlstadter	Traditionally web applications have required an internet connection in order to work with data. Browsers have lacked any mechanisms to allow web applications to operate offline with a set of data to provide constant access to applications. Recently, through browser plug-ins such as Google Gears, browsers have gained the ability to persist data for offline use. However, until now it's been difficult for a web developer using these plug-ins to manage persisting data both locally for offline use and in the internet cloud due to: synchronization requirements, managing throughput and latency to the cloud, and making it work within the confines of a standards-compliant web browser. Historically in non-browser environments, programming language environments have offered automated object persistence to shield the developer from these complexities. In our research we have created a framework which introduces automated persistence of data objects for JavaScript utilizing the internet. Unlike traditional object persistence solutions, ours relies only on existing or forthcoming internet standards and does not rely upon specific runtime mechanisms such as OS or interpreter/compiler support. A new design was required in order to be suitable to the internet's unique characteristics of varying connection quality and a browser's specific restrictions. We validate our approach using benchmarks which show that our framework can handle thousands of data objects automatically, reducing the amount of work needed by developers to support offline Web applications.	Automated object persistence for JavaScript	NA:NA	2010
Xin Cao:Gao Cong:Bin Cui:Christian S. Jensen	Community Question Answering (CQA) has emerged as a popular type of service where users ask and answer questions and access historical question-answer pairs. CQA archives contain very large volumes of questions organized into a hierarchy of categories. As an essential function of CQA services, question retrieval in a CQA archive aims to retrieve historical question-answer pairs that are relevant to a query question. In this paper, we present a new approach to exploiting category information of questions for improving the performance of question retrieval, and we apply the approach to existing question retrieval models, including a state-of-the-art question retrieval model. Experiments conducted on real CQA data demonstrate that the proposed techniques are capable of outperforming a variety of baseline methods significantly.	A generalized framework of exploring category information for question retrieval in community question answer archives	NA:NA:NA:NA	2010
Deepayan Chakrabarti:Rupesh Mehta	An unsupervised clustering of the webpages on a website is a primary requirement for most wrapper induction and automated data extraction methods. Since page content can vary drastically across pages of one cluster (e.g., all product pages on amazon.com), traditional clustering methods typically use some distance function between the DOM trees representing a pair of webpages. However, without knowing which portions of the DOM tree are "important," such distance functions might discriminate between similar pages based on trivial features (e.g., differing number of reviews on two product pages), or club together distinct types of pages based on superficial features present in the DOM trees of both (e.g., matching footer/copyright), leading to poor clustering performance. We propose using search logs to automatically find paths in the DOM trees that mark out important portions of pages, e.g., the product title in a product page. Such paths are identified via a global analysis of the entire website, whereby search data for popular pages can be used to infer good paths even for other pages that receive little or no search traffic. The webpages on the website are then clustered using these "key" paths. Our algorithm only requires information on search queries, and the webpages clicked in response to them; there is no need for human input, and it does not need to be told which portion of a webpage the user found interesting. The resulting clusterings achieve an adjusted RAND score of over 0.9 on half of the websites (a score of 1 indicating a perfect clustering), and 59% better scores on average than competing algorithms. Besides leading to refined clusterings, these key paths can be useful in the wrapper induction process itself, as shown by the high degree of match between the key paths and the manually identified paths used in existing wrappers for these sites (90% average precision).	The paths more taken: matching DOM trees to search logs for accurate webpage clustering	NA:NA	2010
Zhicong Cheng:Bin Gao:Tie-Yan Liu	This paper is concerned with actively predicting search intent from user browsing behavior data. In recent years, great attention has been paid to predicting user search intent. However, the prediction was mostly passive because it was performed only after users submitted their queries to search engines. It is not considered why users issued these queries, and what triggered their information needs. According to our study, many information needs of users were actually triggered by what they have browsed. That is, after reading a page, if a user found something interesting or unclear, he/she might have the intent to obtain further information and accordingly formulate a search query. Actively predicting such search intent can benefit both search engines and their users. In this paper, we propose a series of technologies to fulfill this task. First, we extract all the queries that users issued after reading a given page from user browsing behavior data. Second, we learn a model to effectively rank these queries according to their likelihoods of being triggered by the page. Third, since search intents can be quite diverse even if triggered by the same page, we propose an optimization algorithm to diversify the ranked list of queries obtained in the second step, and then suggest the list to users. We have tested our approach on large-scale user browsing behavior data obtained from a commercial search engine. The experimental results have shown that our approach can predict meaningful queries for a given page, and the search performance for these queries can be significantly improved by using the triggering page as contextual information.	Actively predicting diverse search intent from user browsing behaviors	NA:NA:NA	2010
Flavio Chierichetti:Ravi Kumar:Andrew Tomkins	The NP-hard Max-k-cover problem requires selecting k sets from a collection so as to maximize the size of the union. This classic problem occurs commonly in many settings in web search and advertising. For moderately-sized instances, a greedy algorithm gives an approximation of (1-1/e). However, the greedy algorithm requires updating scores of arbitrary elements after each step, and hence becomes intractable for large datasets. We give the first max cover algorithm designed for today's large-scale commodity clusters. Our algorithm has provably almost the same approximation as greedy, but runs much faster. Furthermore, it can be easily expressed in the MapReduce programming paradigm, and requires only polylogarithmically many passes over the data. Our experiments on five large problem instances show that our algorithm is practical and can achieve good speedups compared to the sequential greedy algorithm.	Max-cover in map-reduce	NA:NA:NA	2010
Flavio Chierichetti:Ravi Kumar:Andrew Tomkins	We present a model of tabbed browsing that represents a hybrid between a Markov process capturing the graph of hyperlinks, and a branching process capturing the birth and death of tabs. We present a mathematical criterion to characterize whether the process has a steady state independent of initial conditions, and we show how to characterize the limiting behavior in both cases. We perform a series of experiments to compare our tabbed browsing model with pagerank, and show that tabbed browsing is able to explain 15-25% of the deviation between actual measured browsing behavior and the behavior predicted by the simple pagerank model. We find this to be a surprising result, as the tabbed browsing model does not make use of any notion of site popularity, but simply captures deviations in user likelihood to open and close tabs from a particular node in the graph.	Stochastic models for tabbed browsing	NA:NA:NA	2010
Yejin Choi:Marcus Fontoura:Evgeniy Gabrilovich:Vanja Josifovski:Mauricio Mediano:Bo Pang	We explore the use of the landing page content in sponsored search ad selection. Specifically, we compare the use of the ad's intrinsic content to augmenting the ad with the whole, or parts, of the landing page. We explore two types of extractive summarization techniques to select useful regions from the landing pages: out-of-context and in-context methods. Out-of-context methods select salient regions from the landing page by analyzing the content alone, without taking into account the ad associated with the landing page. In-context methods use the ad context (including its title, creative, and bid phrases) to help identify regions of the landing page that should be used by the ad selection engine. In addition, we introduce a simple yet effective unsupervised algorithm to enrich the ad context to further improve the ad selection. Experimental evaluation confirms that the use of landing pages can significantly improve the quality of ad selection. We also find that our extractive summarization techniques reduce the size of landing pages substantially, while retaining or even improving the performance of ad retrieval over the method that utilize the entire landing page.	Using landing pages for sponsored search ad selection	NA:NA:NA:NA:NA:NA	2010
Karen Church:Joachim Neumann:Mauro Cherubini:Nuria Oliver	As the mobile Internet continues to grow, there is an increasing need to provide users with effective search and information access services. In order to build more effective mobile search services, we must first understand the impact that various interface choices have on mobile users. For example, the majority of mobile location-based search services are built on top of a map visualization, but is this intuitive design-decision the optimal interface choice from a human centric perspective? In order to tackle this fundamental design question, we have developed two proactive mobile search interfaces (one map-based and the other text-based) that utilize key mobile contexts to improve the search and information discovery experience of mobile users. In this paper, we present the results of an exploratory field study of these two interfaces - involving 34 users over a 1 month period - where we focus in particular on the impact that the type of user interface (e.g. map vs text) has on the search and information discovery experience of mobile users. We highlight the main usage results - including that maps are not the interface of choice for certain information access tasks - and outline key implications for the design of next generation mobile search services.	The "Map Trap"?: an evaluation of map versus text-based interfaces for location-based mobile search services	NA:NA:NA:NA	2010
Gregory Conti:Edward Sobiesk	In an ideal world, interface design is the art and science of helping users accomplish tasks in a timely, efficient, and pleasurable manner. This paper studies the inverse situation, the vast emergence of deliberately constructed malicious interfaces that violate design best practices in order to accomplish goals counter to those of the user. This has become a commonplace occurrence both on and off the desktop, particularly on the web. A primary objective of this paper is to formally define this problem, including construction of a taxonomy of malicious interface techniques and a preliminary analysis of their impact on users. Findings are presented that gauge the self-reported tolerance and expectation levels of users with regard to malicious interfaces as well as the effectiveness and ease of use of existing countermeasures. A second objective of this paper is to increase awareness, dialogue, and research in a domain that we consider largely unexplored but critical to future usability of the WWW. Our results were accomplished through significant compilation of malicious interface techniques based on review of thousands of web sites and by conducting three surveys. Ultimately, this paper concludes that malicious interfaces are a ubiquitous problem that demands intervention by the security and human computer interaction communities in order to reduce the negative impact on the global user population.	Malicious interface design: exploiting the user	NA:NA	2010
Marco Cova:Christopher Kruegel:Giovanni Vigna	JavaScript is a browser scripting language that allows developers to create sophisticated client-side interfaces for web applications. However, JavaScript code is also used to carry out attacks against the user's browser and its extensions. These attacks usually result in the download of additional malware that takes complete control of the victim's platform, and are, therefore, called "drive-by downloads." Unfortunately, the dynamic nature of the JavaScript language and its tight integration with the browser make it difficult to detect and block malicious JavaScript code. This paper presents a novel approach to the detection and analysis of malicious JavaScript code. Our approach combines anomaly detection with emulation to automatically identify malicious JavaScript code and to support its analysis. We developed a system that uses a number of features and machine-learning techniques to establish the characteristics of normal JavaScript code. Then, during detection, the system is able to identify anomalous JavaScript code by emulating its behavior and comparing it to the established profiles. In addition to identifying malicious code, the system is able to support the analysis of obfuscated code and to generate detection signatures for signature-based systems. The system has been made publicly available and has been used by thousands of analysts.	Detection and analysis of drive-by-download attacks and malicious JavaScript code	NA:NA:NA	2010
Cristian Danescu-Niculescu-Mizil:Andrei Z. Broder:Evgeniy Gabrilovich:Vanja Josifovski:Bo Pang	Queries on major Web search engines produce complex result pages, primarily composed of two types of information: organic results, that is, short descriptions and links to relevant Web pages, and sponsored search results, the small textual advertisements often displayed above or to the right of the organic results. Strategies for optimizing each type of result in isolation and the consequent user reaction have been extensively studied; however, the interplay between these two complementary sources of information has been ignored, a situation we aim to change. Our findings indicate that their perceived relative usefulness (as evidenced by user clicks) depends on the nature of the query. Specifically, we found that, when both sources focus on the same intent, for navigational queries there is a clear competition between ads and organic results, while for non-navigational queries this competition turns into synergy. We also investigate the relationship between the perceived usefulness of the ads and their textual similarity to the organic results, and propose a model that formalizes this relationship. To this end, we introduce the notion of responsive ads, which directly address the user's information need, and incidental ads, which are only tangentially related to that need. Our findings support the hypothesis that in the case of navigational queries, which are usually fully satisfied by the top organic result, incidental ads are perceived as more valuable than responsive ads, which are likely to be duplicative. On the other hand, in the case of non-navigational queries, incidental ads are perceived as less beneficial, possibly because they diverge too far from the actual user need. We hope that our findings and further research in this area will allow search engines to tune ad selection for an increased synergy between organic and sponsored results, leading to both higher user satisfaction and better monetization.	Competing for users' attention: on the interplay between organic and sponsored search results	NA:NA:NA:NA:NA	2010
Munmun De Choudhury:Winter A. Mason:Jake M. Hofman:Duncan J. Watts	Researchers increasingly use electronic communication data to construct and study large social networks, effectively inferring unobserved ties (e.g. i is connected to j) from observed communication events (e.g. i emails j). Often overlooked, however, is the impact of tie definition on the corresponding network, and in turn the relevance of the inferred network to the research question of interest. Here we study the problem of network inference and relevance for two email data sets of different size and origin. In each case, we generate a family of networks parameterized by a threshold condition on the frequency of emails exchanged between pairs of individuals. After demonstrating that different choices of the threshold correspond to dramatically different network structures, we then formulate the relevance of these networks in terms of a series of prediction tasks that depend on various network features. In general, we find: a) that prediction accuracy is maximized over a non-trivial range of thresholds corresponding to 5-10 reciprocated emails per year; b) that for any prediction task, choosing the optimal value of the threshold yields a sizable (~30%) boost in accuracy over naive choices; and c) that the optimal threshold value appears to be (somewhat surprisingly) consistent across data sets and prediction tasks. We emphasize the practical utility in defining ties via their relevance to the prediction task(s) at hand and discuss implications of our empirical results.	Inferring relevant social networks from interpersonal communication	NA:NA:NA:NA	2010
Shuai Ding:Josh Attenberg:Torsten Suel	Web search engines depend on the full-text inverted index data structure. Because the query processing performance is so dependent on the size of the inverted index, a plethora of research has focused on fast end effective techniques for compressing this structure. Recently, several authors have proposed techniques for improving index compression by optimizing the assignment of document identifiers to the documents in the collection, leading to significant reduction in overall index size. In this paper, we propose improved techniques for document identifier assignment. Previous work includes simple and fast heuristics such as sorting by URL, as well as more involved approaches based on the Traveling Salesman Problem or on graph partitioning. These techniques achieve good compression but do not scale to larger document collections. We propose a new framework based on performing a Traveling Salesman computation on a reduced sparse graph obtained through Locality Sensitive Hashing. This technique achieves improved compression while scaling to tens of millions of documents. Based on this framework, we describe a number of new algorithms, and perform a detailed evaluation on three large data sets showing improvements in index size.	Scalable techniques for document identifier assignment in inverted indexes	NA:NA:NA	2010
Debora Donato:Francesco Bonchi:Tom Chi:Yoelle Maarek	Addressing user's information needs has been one of the main goals of Web search engines since their early days. In some cases, users cannot see their needs immediately answered by search results, simply because these needs are too complex and involve multiple aspects that are not covered by a single Web or search results page. This typically happens when users investigate a certain topic in domains such as education, travel or health, which often require collecting facts and information from many pages. We refer to this type of activities as "research missions". These research missions account for 10% of users' sessions and more than 25% of all query volume, as verified by a manual analysis that was conducted by Yahoo! editors. We demonstrate in this paper that such missions can be automatically identified on-the-fly, as the user interacts with the search engine, through careful runtime analysis of query flows and query sessions. The on-the-fly automatic identification of research missions has been implemented in Search Pad, a novel Yahoo! application that was launched in 2009, and that we present in this paper. Search Pad helps users keeping trace of results they have consulted. Its novelty however is that unlike previous notes taking products, it is automatically triggered only when the system decides, with a fair level of confidence, that the user is undertaking a research mission and thus is in the right context for gathering notes. Beyond the Search Pad specific application, we believe that changing the level of granularity of query modeling, from an isolated query to a list of queries pertaining to the same research missions, so as to better reflect a certain type of information needs, can be beneficial in a number of other Web search applications. Session-awareness is growing and it is likely to play, in the near future, a fundamental role in many on-line tasks: this paper presents a first step on this path.	Do you want to take notes?: identifying research missions in Yahoo! search pad	NA:NA:NA:NA	2010
Anlei Dong:Ruiqiang Zhang:Pranam Kolari:Jing Bai:Fernando Diaz:Yi Chang:Zhaohui Zheng:Hongyuan Zha	Realtime web search refers to the retrieval of very fresh content which is in high demand. An effective portal web search engine must support a variety of search needs, including realtime web search. However, supporting realtime web search introduces two challenges not encountered in non-realtime web search: quickly crawling relevant content and ranking documents with impoverished link and click information. In this paper, we advocate the use of realtime micro-blogging data for addressing both of these problems. We propose a method to use the micro-blogging data stream to detect fresh URLs. We also use micro-blogging data to compute novel and effective features for ranking fresh URLs. We demonstrate these methods improve effective of the portal web search engine for realtime web search.	Time is of the essence: improving recency ranking using Twitter data	NA:NA:NA:NA:NA:NA:NA:NA	2010
Rob Ennals:Beth Trushkowsky:John Mark Agosta	We describe Dispute Finder, a browser extension that alerts a user when information they read online is disputed by a source that they might trust. Dispute Finder examines the text on the page that the user is browsing and highlights any phrases that resemble known disputed claims. If a user clicks on a highlighted phrase then Dispute Finder shows them a list of articles that support other points of view. Dispute Finder builds a database of known disputed claims by crawling web sites that already maintain lists of disputed claims, and by allowing users to enter claims that they believe are disputed. Dispute Finder identifies snippets that make known disputed claims by running a simple textual entailment algorithm inside the browser extension, referring to a cached local copy of the claim database. In this paper, we explain the design of Dispute Finder, and the trade-offs between the various design decisions that we explored.	Highlighting disputed claims on the web	NA:NA:NA	2010
Lujun Fang:Kristen LeFevre	Privacy is an enormous problem in online social networking sites. While sites such as Facebook allow users fine-grained control over who can see their profiles, it is difficult for average users to specify this kind of detailed policy. In this paper, we propose a template for the design of a social networking privacy wizard. The intuition for the design comes from the observation that real users conceive their privacy preferences (which friends should be able to see which information) based on an implicit set of rules. Thus, with a limited amount of user input, it is usually possible to build a machine learning model that concisely describes a particular user's preferences, and then use this model to configure the user's privacy settings automatically. As an instance of this general framework, we have built a wizard based on an active learning paradigm called uncertainty sampling. The wizard iteratively asks the user to assign privacy "labels" to selected ("informative") friends, and it uses this input to construct a classifier, which can in turn be used to automatically assign privileges to the rest of the user's (unlabeled) friends. To evaluate our approach, we collected detailed privacy preference data from 45 real Facebook users. Our study revealed two important things. First, real users tend to conceive their privacy preferences in terms of communities, which can easily be extracted from a social network graph using existing techniques. Second, our active learning wizard, using communities as features, is able to recommend high-accuracy privacy settings using less user input than existing policy-specification tools.	Privacy wizards for social networking sites	NA:NA	2010
George Forman:Evan Kirshenbaum:Shyamsundar Rajaram	Using a clickstream sample of 2 billion URLs from many thousand volunteer Web users, we wish to analyze typical usage of keyword searches across the Web. In order to do this, we need to be able to determine whether a given URL represents a keyword search and, if so, which field contains the query. Although it is easy to recognize 'q' as the query field in 'http://www.google.com/search?hl=en&q=music', we must do this automatically for the long tail of diverse websites. This problem is the focus of this paper. Since the names, types and number of fields differ across sites, this does not conform to traditional text classification or to multi-class problem formulations. The problem also exhibits highly non-uniform importance across websites, since traffic follows a Zipf distribution. We developed a solution based on manually identifying the query fields on the most popular sites, followed by an adaptation of machine learning for the rest. It involves an interesting case-instances structure: labeling each website case usually involves selecting at most one of the field instances as positive, based on seeing sample field values. This problem structure and soft constraint - which we believe has broader applicability - can be used to greatly reduce the manual labeling effort. We employed active learning and judicious GUI presentation to efficiently train a classifier with accuracy estimated at 96%, beating several baseline alternatives.	A novel traffic analysis for identifying search fields in the long tail of web sites	NA:NA:NA	2010
Arpita Ghosh:Amin Sayedi	When online ads are shown together, they compete for user attention and conversions, imposing negative externalities on each other. While the competition for user attention in sponsored search can be captured via models of clickthrough rates, the post-click competition for conversions cannot: since the value-per-click of an advertiser is proportional to the conversion probability conditional on a click, which depends on the other ads displayed, the private value of an advertiser is no longer one-dimensional, and the GSP mechanism is not adequately expressive. We study the design of expressive GSP-like mechanisms for the simplest form that an advertiser's private value can have in the presence of such externalities- an advertiser's value depends on exclusivity, i.e., whether her ad is shown exclusively, or along with other ads. Our auctions take as input two-dimensional (per-click) bids for exclusive and nonexclusive display, and have two types of outcomes: either a single ad is displayed exclusively, or multiple ads are simultaneously shown. We design two expressive auctions that are both extensions of GSP- the first auction, GSP2D, is designed with the property that the allocation and pricing are identical to GSP when multiple ads are shown; the second auction, NP2D, is designed to be a next price auction. We show that both auctions have high efficiency and revenue in all reasonable equilibria; further, the NP2D auction is guaranteed to always have an equilibrium with revenue at least as much as the current GSP mechanism. However, we find that unlike with one-dimensional valuations, the GSP-like auctions for these richer valuations do not always preserve efficiency and revenue with respect to the VCG mechanism.	Expressive auctions for externalities in online advertising	NA:NA	2010
David F. Gleich:Paul G. Constantine:Abraham D. Flaxman:Asela Gunawardana	PageRank computes the importance of each node in a directed graph under a random surfer model governed by a teleportation parameter. Commonly denoted alpha, this parameter models the probability of following an edge inside the graph or, when the graph comes from a network of web pages and links, clicking a link on a web page. We empirically measure the teleportation parameter based on browser toolbar logs and a click trail analysis. For a particular user or machine, such analysis produces a value of alpha. We find that these values nicely fit a Beta distribution with mean edge-following probability between 0.3 and 0.7, depending on the site. Using these distributions, we compute PageRank scores where PageRank is computed with respect to a distribution as the teleportation parameter, rather than a constant teleportation parameter. These new metrics are evaluated on the graph of pages in Wikipedia.	Tracking the random surfer: empirically measured teleportation parameters in PageRank	NA:NA:NA:NA	2010
Ziyu Guan:Can Wang:Jiajun Bu:Chun Chen:Kun Yang:Deng Cai:Xiaofei He	Social tagging services allow users to annotate various online resources with freely chosen keywords (tags). They not only facilitate the users in finding and organizing online resources, but also provide meaningful collaborative semantic data which can potentially be exploited by recommender systems. Traditional studies on recommender systems focused on user rating data, while recently social tagging data is becoming more and more prevalent. How to perform resource recommendation based on tagging data is an emerging research topic. In this paper we consider the problem of document (e.g. Web pages, research papers) recommendation using purely tagging data. That is, we only have data containing users, tags, documents and the relationships among them. We propose a novel graph-based representation learning algorithm for this purpose. The users, tags and documents are represented in the same semantic space in which two related objects are close to each other. For a given user, we recommend those documents that are sufficiently close to him/her. Experimental results on two data sets crawled from Del.icio.us and CiteULike show that our algorithm can generate promising recommendations and outperforms traditional recommendation algorithms.	Document recommendation in social tagging services	NA:NA:NA:NA:NA:NA:NA	2010
Qiang Hao:Rui Cai:Changhu Wang:Rong Xiao:Jiang-Ming Yang:Yanwei Pang:Lei Zhang	With the prosperity of tourism and Web 2.0 technologies, more and more people have willingness to share their travel experiences on the Web (e.g., weblogs, forums, or Web 2.0 communities). These so-called travelogues contain rich information, particularly including location-representative knowledge such as attractions (e.g., Golden Gate Bridge), styles (e.g., beach, history), and activities (e.g., diving, surfing). The location-representative information in travelogues can greatly facilitate other tourists' trip planning, if it can be correctly extracted and summarized. However, since most travelogues are unstructured and contain much noise, it is difficult for common users to utilize such knowledge effectively. In this paper, to mine location-representative knowledge from a large collection of travelogues, we propose a probabilistic topic model, named as Location-Topic model. This model has the advantages of (1) differentiability between two kinds of topics, i.e., local topics which characterize locations and global topics which represent other common themes shared by various locations, and (2) representation of locations in the local topic space to encode both location-representative knowledge and similarities between locations. Some novel applications are developed based on the proposed model, including (1) destination recommendation for on flexible queries, (2) characteristic summarization for a given destination with representative tags and snippets, and (3) identification of informative parts of a travelogue and enriching such highlights with related images. Based on a large collection of travelogues, the proposed framework is evaluated using both objective and subjective evaluation methods and shows promising results.	Equip tourists with knowledge mined from travelogues	NA:NA:NA:NA:NA:NA:NA	2010
Andreas Harth:Katja Hose:Marcel Karnstedt:Axel Polleres:Kai-Uwe Sattler:Jürgen Umbrich	Typical approaches for querying structured Web Data collect (crawl) and pre-process (index) large amounts of data in a central data repository before allowing for query answering. However, this time-consuming pre-processing phase however leverages the benefits of Linked Data -- where structured data is accessible live and up-to-date at distributed Web resources that may change constantly -- only to a limited degree, as query results can never be current. An ideal query answering system for Linked Data should return current answers in a reasonable amount of time, even on corpora as large as the Web. Query processors evaluating queries directly on the live sources require knowledge of the contents of data sources. In this paper, we develop and evaluate an approximate index structure summarising graph-structured content of sources adhering to Linked Data principles, provide an algorithm for answering conjunctive queries over Linked Data on theWeb exploiting the source summary, and evaluate the system using synthetically generated queries. The experimental results show that our lightweight index structure enables complete and up-to-date query results over Linked Data, while keeping the overhead for querying low and providing a satisfying source ranking at no additional cost.	Data summaries for on-demand queries over linked data	NA:NA:NA:NA:NA:NA	2010
Qi He:Jian Pei:Daniel Kifer:Prasenjit Mitra:Lee Giles	When you write papers, how many times do you want to make some citations at a place but you are not sure which papers to cite? Do you wish to have a recommendation system which can recommend a small number of good candidates for every place that you want to make some citations? In this paper, we present our initiative of building a context-aware citation recommendation system. High quality citation recommendation is challenging: not only should the citations recommended be relevant to the paper under composition, but also should match the local contexts of the places citations are made. Moreover, it is far from trivial to model how the topic of the whole paper and the contexts of the citation places should affect the selection and ranking of citations. To tackle the problem, we develop a context-aware approach. The core idea is to design a novel non-parametric probabilistic model which can measure the context-based relevance between a citation context and a document. Our approach can recommend citations for a context effectively. Moreover, it can recommend a set of citations for a paper with high quality. We implement a prototype system in CiteSeerX. An extensive empirical evaluation in the CiteSeerX digital library against many baselines demonstrates the effectiveness and the scalability of our approach.	Context-aware citation recommendation	NA:NA:NA:NA:NA	2010
Damon Horowitz:Sepandar D. Kamvar	We present Aardvark, a social search engine. With Aardvark, users ask a question, either by instant message, email, web input, text message, or voice. Aardvark then routes the question to the person in the user's extended social network most likely to be able to answer that question. As compared to a traditional web search engine, where the challenge lies in finding the right document to satisfy a user's information need, the challenge in a social search engine like Aardvark lies in finding the right person to satisfy a user's information need. Further, while trust in a traditional search engine is based on authority, in a social search engine like Aardvark, trust is based on intimacy. We describe how these considerations inform the architecture, algorithms, and user interface of Aardvark, and how they are reflected in the behavior of Aardvark users.	The anatomy of a large-scale social search engine	NA:NA	2010
Matthijs Hovelynck:Boris Chidlovskii	We propose a method for improving classification performance in a one-class setting by combining classifiers of different modalities. We apply the method to the problem of distinguishing responsive documents in a corpus of e-mails, like Enron Corpus. We extract the social network of actors which is implicit in a large body of electronic communication and turn it into valuable features for classifying the exchanged documents. Working in a one-class setting we folow a semi-supervised approach based on the Mapping Convergence framework. We propose an alternative interpretation, that allows for broader applicability when positive and negative items are not naturally separable. We propose an extension to the one-class evaluation framework in truly one-case cases when only some positive training examples are available. We extent the one-class setting to the co-training principle that enables us to take advantage of multiple views on the data. We report evaluation results of this extension on three different corpora including Enron Corpus.	Multi-modality in one-class classification	NA:NA	2010
Jian Huang:Jianfeng Gao:Jiangbo Miao:Xiaolong Li:Kuansan Wang:Fritz Behr:C. Lee Giles	It has been widely observed that search queries are composed in a very different style from that of the body or the title of a document. Many techniques explicitly accounting for this language style discrepancy have shown promising results for information retrieval, yet a large scale analysis on the extent of the language differences has been lacking. In this paper, we present an extensive study on this issue by examining the language model properties of search queries and the three text streams associated with each web document: the body, the title, and the anchor text. Our information theoretical analysis shows that queries seem to be composed in a way most similar to how authors summarize documents in anchor texts or titles, offering a quantitative explanation to the observations in past work. We apply these web scale n-gram language models to three search query processing (SQP) tasks: query spelling correction, query bracketing and long query segmentation. By controlling the size and the order of different language models, we find that the perplexity metric to be a good accuracy indicator for these query processing tasks. We show that using smoothed language models yields significant accuracy gains for query bracketing for instance, compared to using web counts as in the literature. We also demonstrate that applying web-scale language models can have marked accuracy advantage over smaller ones.	Exploring web scale language models for search query processing	NA:NA:NA:NA:NA:NA:NA	2010
Utku Irmak:Reiner Kraft	Named entity recognition studies the problem of locating and classifying parts of free text into a set of predefined categories. Although extensive research has focused on the detection of person, location and organization entities, there are many other entities of interest, including phone numbers, dates, times and currencies (to name a few examples). We refer to these types of entities as "semi-structured named entities", since they usually follow certain syntactic formats according to some conventions, although their structure is typically not well-defined. Regular expression solutions require significant amount of manual effort and supervised machine learning approaches rely on large sets of labeled training data. Therefore, these approaches do not scale when we need to support many semi-structured entity types in many languages and regions. In this paper, we study this problem and propose a novel three-level bootstrapping framework for the detection of semi-structured entities. We describe the proposed techniques for phone, date and time entities, and perform extensive evaluations on English, German, Polish, Swedish and Turkish documents. Despite the minimal input from the user, our approach can achieve 95% precision and 84% recall for phone entities, and 94% precision and 81% recall for date and time entities, on average. We also discuss implementation details and report run time performance results, which show significant improvements over regular expression based solutions.	A scalable machine-learning approach for semi-structured named entity recognition	NA:NA	2010
Dejun Jiang:Guillaume Pierre:Chi-Hung Chi	Dynamic resource provisioning aims at maintaining the end-to-end response time of a web application within a pre-defined SLA. Although the topic has been well studied for monolithic applications, provisioning resources for applications composed of multiple services remains a challenge. When the SLA is violated, one must decide which service(s) should be reprovisioned for optimal effect. We propose to assign an SLA only to the front-end service. Other services are not given any particular response time objectives. Services are autonomously responsible for their own provisioning operations and collaboratively negotiate performance objectives with each other to decide the provisioning service(s). We demonstrate through extensive experiments that our system can add/remove/shift both servers and caches within an entire multi-service application under varying workloads to meet the SLA target and improve resource utilization.	Autonomous resource provisioning for multi-service web applications	NA:NA:NA	2010
Xin Jin:Scott Spangler:Rui Ma:Jiawei Han	In this paper we introduce a new Web mining and search technique - Topic Initiator Detection (TID) on the Web. Given a topic query on the Internet and the resulting collection of time-stamped web documents which contain the query keywords, the task of TID is to automatically return which web document (or its author) initiated the topic or was the first to discuss about the topic. To deal with the TID problem, we design a system framework and propose algorithm InitRank (Initiator Ranking) to rank the web documents by their possibility to be the topic initiator. We first extract features from the web documents and design several topic initiator indicators. Then, we propose a TCL graph which integrates the Time, Content and Link information and design an optimization framework over the graph to compute InitRank. Experiments show that compared with baseline methods, such as direct time sorting, well-known link based ranking algorithms PageRank and HITS, InitRank achieves the best overall performance with high effectiveness and robustness. In case studies, we successfully detected (1) the first web document related to a famous rumor of an Australia product banned in USA and (2) the pre-release of IBM and Google Cloud Computing collaboration before the official announcement.	Topic initiator detection on the world wide web	NA:NA:NA:NA	2010
Kaimin Zhang:Lu Wang:Aimin Pan:Bin Benjamin Zhu	This paper presents smart caching schemes for Web browsers. For modern Web applications, the style formatting and layout calculation often account for substantial amounts of the local computation in order to render a Web page. In this paper, we propose two caching schemes to reduce the computation of style formatting and layout calculation, named smart style caching and layout caching, respectively. The stable style data and layout data for DOM (Document Object Model) elements are recorded to construct the caches when a Web page is browsed. The cached data is checked in the granularity of DOM elements and applied directly if the identified DOM element is not changed in the sequent visits to the same page. We have implemented a prototype of the proposed caching schemes based on the Webkit layout engine. The experimental results with Web pages from the Top 25 Web sites show that, with the smart style caching scheme enabled, the time consumed for style formatting is reduced by 64% on average; with both the smart style caching scheme and layout caching scheme enabled, the time consumed for layout calculation are reduced by 61% on average, and the overall performance improvement is about 46%.	Smart caching for web browsers	NA:NA:NA:NA	2010
Hongwen Kang:Kuansan Wang:David Soukal:Fritz Behr:Zijian Zheng	In this paper, we propose a semi-supervised learning approach for classifying program (bot) generated web search traffic from that of genuine human users. The work is motivated by the challenge that the enormous amount of search data pose to traditional approaches that rely on fully annotated training samples. We propose a semi-supervised framework that addresses the problem in multiple fronts. First, we use the CAPTCHA technique and simple heuristics to extract from the data logs a large set of training samples with initial labels, though directly using these training data is problematic because the data thus sampled are biased. To tackle this problem, we further develop a semi-supervised learning algorithm to take advantage of the unlabeled data to improve the classification performance. These two proposed algorithms can be seamlessly combined and very cost efficient to scale the training process. In our experiment, the proposed approach showed significant (i.e. 2:1) improvement compared to the traditional supervised approach.	Large-scale bot detection for search engines	NA:NA:NA:NA:NA	2010
Georgia Koloniari:Evaggelia Pitoura	In this paper, we address the problem of database selection for XML document collections, that is, given a set of collections and a user query, how to rank the collections based on their goodness to the query. Goodness is determined by the relevance of the documents in the collection to the query. We consider keyword queries and support Lowest Common Ancestor (LCA) semantics for defining query results, where the relevance of each document to a query is determined by properties of the LCA of those nodes in the XML document that contain the query keywords. To avoid evaluating queries against each document in a collection, we propose maintaining in a preprocessing phase, information about the LCAs of all pairs of keywords in a document and use it to approximate the properties of the LCA-based results of a query. To improve storage and processing efficiency, we use appropriate summaries of the LCA information based on Bloom filters. We address both a boolean and a weighted version of the database selection problem. Our experimental results show that our approach incurs low errors in the estimation of the goodness of a collection and provides rankings that are very close to the actual ones.	LCA-based selection for XML document collections	NA:NA	2010
Christian Körner:Dominik Benz:Andreas Hotho:Markus Strohmaier:Gerd Stumme	Recent research provides evidence for the presence of emergent semantics in collaborative tagging systems. While several methods have been proposed, little is known about the factors that influence the evolution of semantic structures in these systems. A natural hypothesis is that the quality of the emergent semantics depends on the pragmatics of tagging: Users with certain usage patterns might contribute more to the resulting semantics than others. In this work, we propose several measures which enable a pragmatic differentiation of taggers by their degree of contribution to emerging semantic structures. We distinguish between categorizers, who typically use a small set of tags as a replacement for hierarchical classification schemes, and describers, who are annotating resources with a wealth of freely associated, descriptive keywords. To study our hypothesis, we apply semantic similarity measures to 64 different partitions of a real-world and large-scale folksonomy containing different ratios of categorizers and describers. Our results not only show that "verbose" taggers are most useful for the emergence of tag semantics, but also that a subset containing only 40% of the most 'verbose' taggers can produce results that match and even outperform the semantic precision obtained from the whole dataset. Moreover, the results suggest that there exists a causal link between the pragmatics of tagging and resulting emergent semantics. This work is relevant for designers and analysts of tagging systems interested (i) in fostering the semantic development of their platforms, (ii) in identifying users introducing "semantic noise", and (iii) in learning ontologies.	Stop thinking, start tagging: tag semantics emerge from collaborative verbosity	NA:NA:NA:NA:NA	2010
Spyros Kotoulas:Eyal Oren:Frank van Harmelen	Semantic Web data exhibits very skewed frequency distributions among terms. Efficient large-scale distributed reasoning methods should maintain load-balance in the face of such highly skewed distribution of input data. We show that term-based partitioning, used by most distributed reasoning approaches, has limited scalability due to load-balancing problems. We address this problem with a method for data distribution based on clustering in elastic regions. Instead of as- signing data to fixed peers, data flows semi-randomly in the network. Data items "speed-date" while being temporarily collocated in the same peer. We introduce a bias in the routing to allow semantically clustered neighborhoods to emerge. Our approach is self-organising, efficient and does not require any central coordination. We have implemented this method on the MaRVIN platform and have performed experiments on large real-world datasets, using a cluster of up to 64 nodes. We compute the RDFS closure over different datasets and show that our clustering algorithm drastically reduces computation time, calculating the RDFS closure of 200 million triples in 7.2 minutes.	Mind the data skew: distributed inferencing by speeddating in elastic regions	NA:NA:NA	2010
Alexander Kotov:ChengXiang Zhai	Web search is generally motivated by an information need. Since asking well-formulated questions is the fastest and the most natural way to obtain information for human beings, almost all queries posed to search engines correspond to some underlying questions, which reflect the user's information need. Accurate determination of these questions may substantially improve the quality of search results and usability of search interfaces. In this paper, we propose a new framework for question-guided search, in which a retrieval system would automatically generate potentially interesting questions to users based on the search results of a query. Since the answers to such questions are known to exist in the search results, these questions can potentially guide users directly to the answers that they are looking for, eliminating the need to scan the documents in the result list. Moreover, in case of imprecise or ambiguous queries, automatically generated questions can naturally engage users into a feedback cycle to refine their information need and guide them towards their search goals. Implementation of the proposed strategy raises new challenges in content indexing, question generation, ranking and feedback. We propose new methods to address these challenges and evaluated them with a prototype system on a subset of Wikipedia. Evaluation results show the promise of this new question-guided search strategy.	Towards natural question guided search	NA:NA	2010
Akshay Krishnamurthy:Adrian Mettler:David Wagner	We present a programming model for building web applications with security properties that can be confidently verified during a security review. In our model, applications are divided into isolated, privilege-separated components, enabling rich security policies to be enforced in a way that can be checked by reviewers. In our model, the web framework enforces privilege separation and isolation of web applications by requiring the use of an object-capability language and providing interfaces that expose limited, explicitly-specified privileges to application components. This approach restricts what each component of the application can do and quarantines buggy or compromised code. It also provides a way to more safely integrate third-party, less-trusted code into a web application. We have implemented a prototype of this model based upon the Java Servlet framework and used it to build a webmail application. Our experience with this example suggests that the approach is viable and helpful at establishing reviewable application-specific security properties.	Fine-grained privilege separation for web applications	NA:NA:NA	2010
Ravi Kumar:Andrew Tomkins	In this paper, we undertake a large-scale study of online user behavior based on search and toolbar logs. We propose a new CCS taxonomy of pageviews consisting of Content (news, portals, games, verticals, multimedia), Communication (email, social networking, forums, blogs, chat), and Search (Web search, item search, multimedia search). We show that roughly half of all pageviews online are content, one-third are communications, and the remaining one-sixth are search. We then give further breakdowns to characterize the pageviews within each high-level category. We then study the extent to which pages of certain types are revisited by the same user over time, and the mechanisms by which users move from page to page, within and across hosts, and within and across page types. We consider robust schemes for assigning responsibility for a pageview to ancestors along the chain of referrals. We show that mail, news, and social networking pageviews are insular in nature, appearing primarily in homogeneous sessions of one type. Search pageviews, on the other hand, appear on the path to a disproportionate number of pageviews, but cannot be viewed as the principal mechanism by which those pageviews were reached. Finally, we study the burstiness of pageviews associated with a URL, and show that by and large, online browsing behavior is not significantly affected by "breaking" material with non-uniform visit frequency.	A characterization of online browsing behavior	NA:NA	2010
Ravi Kumar:Sergei Vassilvitskii	Spearman's footrule and Kendall's tau are two well established distances between rankings. They, however, fail to take into account concepts crucial to evaluating a result set in information retrieval: element relevance and positional information. That is, changing the rank of a highly-relevant document should result in a higher penalty than changing the rank of an irrelevant document; a similar logic holds for the top versus the bottom of the result ordering. In this work, we extend both of these metrics to those with position and element weights, and show that a variant of the Diaconis-Graham inequality still holds - the generalized two measures remain within a constant factor of each other for all permutations. We continue by extending the element weights into a distance metric between elements. For example, in search evaluation, swapping the order of two nearly duplicate results should result in little penalty, even if these two are highly relevant and appear at the top of the list. We extend the distance measures to this more general case and show that they remain within a constant factor of each other. We conclude by conducting simple experiments on web search data with the proposed measures. Our experiments show that the weighted generalizations are more robust and consistent with each other than their unweighted counter-parts.	Generalized distances between rankings	NA:NA	2010
Peep Küngas:Marlon Dumas	This paper addresses the problem of identifying redundant data in large-scale service-oriented information systems. Specifically, the paper puts forward an automated method to pinpoint potentially redundant data attributes from a given collection of semantically-annotated Web service interfaces. The key idea is to construct a service network to represent all input and output dependencies between data attributes and operations captured in the service interfaces, and to apply centrality measures from network theory in order to quantify the degree to which an attribute belongs to a given subsystem. The proposed method was tested on a federated governmental information system consisting of 58 independently-maintained information systems providing altogether about 1000 service operations described in WSDL. The accuracy of the method is evaluated in terms of precision and recall.	Redundancy detection in service-oriented systems	NA:NA	2010
Haewoon Kwak:Changhyun Lee:Hosung Park:Sue Moon	Twitter, a microblogging service less than three years old, commands more than 41 million users as of July 2009 and is growing fast. Twitter users tweet about any topic within the 140-character limit and follow others to receive their tweets. The goal of this paper is to study the topological characteristics of Twitter and its power as a new medium of information sharing. We have crawled the entire Twitter site and obtained 41.7 million user profiles, 1.47 billion social relations, 4,262 trending topics, and 106 million tweets. In its follower-following topology analysis we have found a non-power-law follower distribution, a short effective diameter, and low reciprocity, which all mark a deviation from known characteristics of human social networks [28]. In order to identify influentials on Twitter, we have ranked users by the number of followers and by PageRank and found two rankings to be similar. Ranking by retweets differs from the previous two rankings, indicating a gap in influence inferred from the number of followers and that from the popularity of one's tweets. We have analyzed the tweets of top trending topics and reported on their temporal behavior and user participation. We have classified the trending topics based on the active period and the tweets and show that the majority (over 85%) of topics are headline news or persistent news in nature. A closer look at retweets reveals that any retweeted tweet is to reach an average of 1,000 users no matter what the number of followers is of the original tweet. Once retweeted, a tweet gets retweeted almost instantly on next hops, signifying fast diffusion of information after the 1st retweet. To the best of our knowledge this work is the first quantitative study on the entire Twittersphere and information diffusion on it.	What is Twitter, a social network or a news media?	NA:NA:NA:NA	2010
Timothy La Fond:Jennifer Neville	Relational autocorrelation is ubiquitous in relational domains. This observed correlation between class labels of linked instances in a network (e.g., two friends are more likely to share political beliefs than two randomly selected people) can be due to the effects of two different social processes. If social influence effects are present, instances are likely to change their attributes to conform to their neighbor values. If homophily effects are present, instances are likely to link to other individuals with similar attribute values. Both these effects will result in autocorrelated attribute values. When analyzing static relational networks it is impossible to determine how much of the observed correlation is due each of these factors. However, the recent surge of interest in social networks has increased the availability of dynamic network data. In this paper, we present a randomization technique for temporal network data where the attributes and links change over time. Given data from two time steps, we measure the gain in correlation and assess whether a significant portion of this gain is due to influence and/or homophily. We demonstrate the efficacy of our method on semi-synthetic data and then apply the method to a real-world social networks dataset, showing the impact of both influence and homophily effects.	Randomization tests for distinguishing social influence and homophily effects	NA:NA	2010
Tao Lei:Rui Cai:Jiang-Ming Yang:Yan Ke:Xiaodong Fan:Lei Zhang	Duplicate URLs have brought serious troubles to the whole pipeline of a search engine, from crawling, indexing, to result serving. URL normalization is to transform duplicate URLs to a canonical form using a set of rewrite rules. Nowadays URL normalization has attracted significant attention as it is lightweight and can be flexibly integrated into both the online (e.g. crawling) and the offline (e.g. index compression) parts of a search engine. To deal with a large scale of websites, automatic approaches are highly desired to learn rewrite rules for various kinds of duplicate URLs. In this paper, we rethink the problem of URL normalization from a global perspective and propose a pattern tree-based approach, which is remarkably different from existing approaches. Most current approaches learn rewrite rules by iteratively inducing local duplicate pairs to more general forms, and inevitably suffer from noisy training data and are practically inefficient. Given a training set of URLs partitioned into duplicate clusters for a targeted website, we develop a simple yet efficient algorithm to automatically construct a URL pattern tree. With the pattern tree, the statistical information from all the training samples is leveraged to make the learning process more robust and reliable. The learning process is also accelerated as rules are directly summarized based on pattern tree nodes. In addition, from an engineering perspective, the pattern tree helps select deployable rules by removing conflicts and redundancies. An evaluation on more than 70 million duplicate URLs from 200 websites showed that the proposed approach achieves very promising performance, in terms of both de-duping effectiveness and computational efficiency.	A pattern tree-based approach to learning URL normalization rules	NA:NA:NA:NA:NA:NA	2010
Kristina Lerman:Tad Hogg	Popularity of content in social media is unequally distributed, with some items receiving a disproportionate share of attention from users. Predicting which newly-submitted items will become popular is critically important for both companies that host social media sites and their users. Accurate and timely prediction would enable the companies to maximize revenue through differential pricing for access to content or ad placement. Prediction would also give consumers an important tool for filtering the ever-growing amount of content. Predicting popularity of content in social media, however, is challenging due to the complex interactions among content quality, how the social media site chooses to highlight content, and influence among users. While these factors make it difficult to predict popularity a priori, we show that stochastic models of user behavior on these sites allows predicting popularity based on early user reactions to new content. By incorporating aspects of the web site design, such models improve on predictions based on simply extrapolating from the early votes. We validate this claim on the social news portal Digg using a previously-developed model of social voting based on the Digg user interface.	Using a model of social dynamics to predict popularity of news	NA:NA	2010
Jure Leskovec:Kevin J. Lang:Michael Mahoney	Detecting clusters or communities in large real-world graphs such as large social or information networks is a problem of considerable interest. In practice, one typically chooses an objective function that captures the intuition of a network cluster as set of nodes with better internal connectivity than external connectivity, and then one applies approximation algorithms or heuristics to extract sets of nodes that are related to the objective function and that "look like" good communities for the application of interest. In this paper, we explore a range of network community detection methods in order to compare them and to understand their relative performance and the systematic biases in the clusters they identify. We evaluate several common objective functions that are used to formalize the notion of a network community, and we examine several different classes of approximation algorithms that aim to optimize such objective functions. In addition, rather than simply fixing an objective and asking for an approximation to the best cluster of any size, we consider a size-resolved version of the optimization problem. Considering community quality as a function of its size provides a much finer lens with which to examine community detection algorithms, since objective functions and approximation algorithms often have non-obvious size-dependent behavior.	Empirical comparison of algorithms for network community detection	NA:NA:NA	2010
Jure Leskovec:Daniel Huttenlocher:Jon Kleinberg	We study online social networks in which relationships can be either positive (indicating relations such as friendship) or negative (indicating relations such as opposition or antagonism). Such a mix of positive and negative links arise in a variety of online settings; we study datasets from Epinions, Slashdot and Wikipedia. We find that the signs of links in the underlying social networks can be predicted with high accuracy, using models that generalize across this diverse range of sites. These models provide insight into some of the fundamental principles that drive the formation of signed links in networks, shedding light on theories of balance and status from social psychology; they also suggest social computing applications by which the attitude of one user toward another can be estimated from evidence provided by their relationships with other members of the surrounding social network.	Predicting positive and negative links in online social networks	NA:NA:NA	2010
Chengkai Li:Ning Yan:Senjuti B. Roy:Lekhendro Lisham:Gautam Das	This paper proposes Facetedpedia, a faceted retrieval system for information discovery and exploration in Wikipedia. Given the set of Wikipedia articles resulting from a keyword query, Facetedpedia generates a faceted interface for navigating the result articles. Compared with other faceted retrieval systems, Facetedpedia is fully automatic and dynamic in both facet generation and hierarchy construction, and the facets are based on the rich semantic information from Wikipedia. The essence of our approach is to build upon the collaborative vocabulary in Wikipedia, more specifically the intensive internal structures (hyperlinks) and folksonomy (category system). Given the sheer size and complexity of this corpus, the space of possible choices of faceted interfaces is prohibitively large. We propose metrics for ranking individual facet hierarchies by user's navigational cost, and metrics for ranking interfaces (each with k facets) by both their average pairwise similarities and average navigational costs. We thus develop faceted interface discovery algorithms that optimize the ranking metrics. Our experimental evaluation and user study verify the effectiveness of the system.	Facetedpedia: dynamic generation of query-dependent faceted interfaces for wikipedia	NA:NA:NA:NA:NA	2010
Lihong Li:Wei Chu:John Langford:Robert E. Schapire	Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks. The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.	A contextual-bandit approach to personalized news article recommendation	NA:NA:NA:NA	2010
Ping Li:Christian König	This paper establishes the theoretical framework of b-bit minwise hashing. The original minwise hashing method has become a standard technique for estimating set similarity (e.g., resemblance) with applications in information retrieval, data management, computational advertising, etc. By only storing b bits of each hashed value (e.g., b=1 or 2), we gain substantial advantages in terms of storage space. We prove the basic theoretical results and provide an unbiased estimator of the resemblance for any b. We demonstrate that, even in the least favorable scenario, using b=1 may reduce the storage space at least by a factor of 21.3 (or 10.7) compared to b=64 (or b=32), if one is interested in resemblance >0.5.	b-Bit minwise hashing	NA:NA	2010
Chao Liu:Hung-chih Yang:Jinliang Fan:Li-Wei He:Yi-Min Wang	The Web abounds with dyadic data that keeps increasing by every single second. Previous work has repeatedly shown the usefulness of extracting the interaction structure inside dyadic data [21, 9, 8]. A commonly used tool in extracting the underlying structure is the matrix factorization, whose fame was further boosted in the Netflix challenge [26]. When we were trying to replicate the same success on real-world Web dyadic data, we were seriously challenged by the scalability of available tools. We therefore in this paper report our efforts on scaling up the nonnegative matrix factorization (NMF) technique. We show that by carefully partitioning the data and arranging the computations to maximize data locality and parallelism, factorizing a tens of millions by hundreds of millions matrix with billions of nonzero cells can be accomplished within tens of hours. This result effectively assures practitioners of the scalability of NMF on Web-scale dyadic data.	Distributed nonnegative matrix factorization for web-scale dyadic data analysis on mapreduce	NA:NA:NA:NA:NA	2010
Yue Lu:Panayiotis Tsaparas:Alexandros Ntoulas:Livia Polanyi	Online reviews in which users publish detailed commentary about their experiences and opinions with products, services, or events are extremely valuable to users who rely on them to make informed decisions. However, reviews vary greatly in quality and are constantly increasing in number, therefore, automatic assessment of review helpfulness is of growing importance. Previous work has addressed the problem by treating a review as a stand-alone document, extracting features from the review text, and learning a function based on these features for predicting the review quality. In this work, we exploit contextual information about authors' identities and social networks for improving review quality prediction. We propose a generic framework for incorporating social context information by adding regularization constraints to the text-based predictor. Our approach can effectively use the social context information available for large quantities of unlabeled reviews. It also has the advantage that the resulting predictor is usable even when social context is unavailable. We validate our framework within a real commerce portal and experimentally demonstrate that using social context information can help improve the accuracy of review quality prediction especially when the available training data is sparse.	Exploiting social context for review quality prediction	NA:NA:NA:NA	2010
Arun S. Maiya:Tanya Y. Berger-Wolf	We propose a novel method, based on concepts from expander graphs, to sample communities in networks. We show that our sampling method, unlike previous techniques, produces subgraphs representative of community structure in the original network. These generated subgraphs may be viewed as stratified samples in that they consist of members from most or all communities in the network. Using samples produced by our method, we show that the problem of community detection may be recast into a case of statistical relational learning. We empirically evaluate our approach against several real-world datasets and demonstrate that our sampling method can effectively be used to infer and approximate community affiliation in the larger network.	Sampling community structure	NA:NA	2010
Leo A. Meyerovich:Rastislav Bodik	The web browser is a CPU-intensive program. Especially on mobile devices, webpages load too slowly, expending significant time in processing a document's appearance. Due to power constraints, most hardware-driven speedups will come in the form of parallel architectures. This is also true of mobile devices such as phones and e-books. In this paper, we introduce new algorithms for CSS selector matching, layout solving, and font rendering, which represent key components for a fast layout engine. Evaluation on popular sites shows speedups as high as 80x. We also formulate the layout problem with attribute grammars, enabling us to not only parallelize our algorithm but prove that it computes in O(log) time and without reflow.	Fast and parallel webpage layout	NA:NA	2010
Leo A. Meyerovich:Adrienne Porter Felt:Mark S. Miller	Browsers do not currently support the secure sharing of JavaScript objects between principals. We present this problem as the need for object views, which are consistent and controllable versions of objects. Multiple views can be made for the same object and customized for the recipients. We implement object views with a JavaScript library that wraps shared objects and interposes on all access attempts. The security challenge is to fully mediate access to objects shared through a view and prevent privilege escalation. We discuss how object views can be deployed in two settings: same-origin sharing with rewriting-based JavaScript isolation systems like Google Caja, and inter-origin sharing between browser frames over a message-passing channel. To facilitate simple document sharing, we build a policy system for declaratively defining policies for document object views. Notably, our document policy system makes it possible to hide elements without breaking document structure invariants. Developers can control the fine-grained behavior of object views with an aspect system that accepts programmatic policies.	Object views: fine-grained sharing in browsers	NA:NA:NA	2010
Hamid Reza Motahari Nezhad:Guang Yuan Xu:Boualem Benatallah	With the rapid growth in the number of online Web services, the problem of service adaptation has received significant attention. In matching and adaptation, the functional description of services including interface and data as well as behavioral descriptions are important. Existing work on matching and adaptation focuses only on one aspect. In this paper, we present a semi-automated matching approach that considers both service descriptions. We introduce two protocol-aware service interface matching algorithms, i.e. depth-based interface matching and iterative reference-based interface matching. These algorithms refine the results of interface matching by incorporating the ordering constraints imposed by business protocol definitions on service operations. We have implemented a prototype and performed experiments using the specification of synthetic and real-world Web services. Experiments show that the proposed approaches lead to a significant improvement in the quality of matching between services.	Protocol-aware matching of web service interfaces for adapter development	NA:NA:NA	2010
Oded Nov:David Anderson:Ofer Arazy	Volunteer computing is a powerful way to harness distributed resources to perform large-scale tasks, similarly to other types of community-based initiatives. Volunteer computing is based on two pillars: the first is computational - allocating and managing large computing tasks; the second is participative - making large numbers of individuals volunteer their computer resources to a project. While the computational aspects of volunteer computing received much research attention, the participative aspect remains largely unexplored. In this study we aim to address this gap: by drawing on social psychology and online communities research, we develop and test a three-dimensional model of the factors determining volunteer computing users' contribution. We investigate one of the largest volunteer computing projects - [email protected] - by linking survey data about contributors' motivations to their activity logs. Our findings highlight the differences between volunteer computing and other forms of community-based projects, and reveal the intricate relationship between individual motivations, social affiliation, tenure in the project, and resource contribution. Implications for research and practice are discussed.	Volunteer computing: a model of the factors determining contribution to community-based scientific research	NA:NA:NA	2010
Sinno Jialin Pan:Xiaochuan Ni:Jian-Tao Sun:Qiang Yang:Zheng Chen	Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of users publishing sentiment data (e.g., reviews, blogs). Although traditional classification algorithms can be used to train sentiment classifiers from manually labeled text data, the labeling work can be time-consuming and expensive. Meanwhile, users often use some different words when they express sentiment in different domains. If we directly apply a classifier trained in one domain to other domains, the performance will be very low due to the differences between these domains. In this work, we develop a general solution to sentiment classification when we do not have any labels in a target domain but have some labeled data in a different domain, regarded as source domain. In this cross-domain sentiment classification setting, to bridge the gap between the domains, we propose a spectral feature alignment (SFA) algorithm to align domain-specific words from different domains into unified clusters, with the help of domain-independent words as a bridge. In this way, the clusters can be used to reduce the gap between domain-specific words of the two domains, which can be used to train sentiment classifiers in the target domain accurately. Compared to previous approaches, SFA can discover a robust representation for cross-domain data by fully exploiting the relationship between the domain-specific and domain-independent words via simultaneously co-clustering them in a common latent space. We perform extensive experiments on two real world datasets, and demonstrate that SFA significantly outperforms previous approaches to cross-domain sentiment classification.	Cross-domain sentiment classification via spectral feature alignment	NA:NA:NA:NA:NA	2010
Niko P. Popitsch:Bernhard Haslhofer	The Web of Data has emerged as a way of exposing structured linked data on the Web. It builds on the central building blocks of the Web (URIs, HTTP) and benefits from its simplicity and wide-spread adoption. It does, however, also inherit the unresolved issues such as the broken link problem. Broken links constitute a major challenge for actors consuming Linked Data as they require them to deal with reduced accessibility of data. We believe that the broken link problem is a major threat to the whole Web of Data idea and that both Linked Data consumers and providers will require solutions that deal with this problem. Since no general solutions for fixing such links in the Web of Data have emerged, we make three contributions into this direction: first, we provide a concise definition of the broken link problem and a comprehensive analysis of existing approaches. Second, we present DSNotify, a generic framework able to assist human and machine actors in fixing broken links. It uses heuristic feature comparison and employs a time-interval-based blocking technique for the underlying instance matching problem. Third, we derived benchmark datasets from knowledge bases such as DBpedia and evaluated the effectiveness of our approach with respect to the broken link problem. Our results show the feasibility of a time-interval-based blocking approach for systems that aim at detecting and fixing broken links in the Web of Data.	DSNotify: handling broken links in the web of data	NA:NA	2010
Jeffrey Pound:Peter Mika:Hugo Zaragoza	Semantic Search refers to a loose set of concepts, challenges and techniques having to do with harnessing the information of the growing Web of Data (WoD) for Web search. Here we propose a formal model of one specific semantic search task: ad-hoc object retrieval. We show that this task provides a solid framework to study some of the semantic search problems currently tackled by commercial Web search engines. We connect this task to the traditional ad-hoc document retrieval and discuss appropriate evaluation metrics. Finally, we carry out a realistic evaluation of this task in the context of a Web search application.	Ad-hoc object retrieval in the web of data	NA:NA:NA	2010
Davood Rafiei:Krishna Bharat:Anand Shukla	Result diversity is a topic of great importance as more facets of queries are discovered and users expect to find their desired facets in the first page of the results. However, the underlying questions of how 'diversity' interplays with 'quality' and when preference should be given to one or both are not well-understood. In this work, we model the problem as expectation maximization and study the challenges of estimating the model parameters and reaching an equilibrium. One model parameter, for example, is correlations between pages which we estimate using textual contents of pages and click data (when available). We conduct experiments on diversifying randomly selected queries from a query log and the queries chosen from the disambiguation topics of Wikipedia. Our algorithm improves upon Google in terms of the diversity of random queries, retrieving 14% to 38% more aspects of queries in top 5, while maintaining a precision very close to Google. On a more selective set of queries that are expected to benefit from diversification, our algorithm improves upon Google in terms of precision and diversity of the results, and significantly outperforms another baseline system for result diversification.	Diversifying web search results	NA:NA:NA	2010
Suju Rajan:Dragomir Yankov:Scott J. Gaffney:Adwait Ratnaparkhi	Many web applications such as ad matching systems, vertical search engines, and page categorization systems require the identification of a particular type or class of pages on the Web. The sheer number and diversity of the pages on the Web, however, makes the problem of obtaining a good sample of the class of interest hard. In this paper, we describe a successfully deployed end-to-end system that starts from a biased training sample and makes use of several state-of-the-art machine learning algorithms working in tandem, including a powerful active learning component, in order to achieve a good classification system. The system is evaluated on traffic from a real-world ad-matching platform and is shown to achieve high categorization effectiveness with a significant reduction in editorial effort and labeling time.	A large-scale active learning system for topical categorization on the web	NA:NA:NA:NA	2010
Azarias Reda:Brian Noble:Yidnekachew Haile	Developing countries face significant challenges in network access, making even simple network tasks unpleasant. Many standard techniques - caching and predictive prefetching - help somewhat, but provide little or no assistance for personal data that is needed only by a single user. Sulula addresses this problem by leveraging the near-ubiquity of cellular phones able to send and receive simple SMS messages. Rather than visit a kiosk and fetch data on demand - a tiresome process at best - users request a future visit. If capacity exists, the kiosk can schedule secure retrieval of that user's data, saving time and more efficiently utilizing the kiosk's limited connectivity. When the user arrives at a provisioned kiosk, she need only obtain the session key on-demand, and thereafter has instant access. In addition, Sulula allows users to schedule data uploads. Experimental results show significant gains for the end user, saving tens of minutes of time for a typical email/news reading session. We also describe a small, ongoing deployment in-country for proof-of-concept, lessons learned from that experience, and provide a discussion on pricing and marketplace issues that remain to be addressed to make the system viable for developing-world access.	Distributing private data in challenged network environments	NA:NA:NA	2010
Steffen Rendle:Christoph Freudenthaler:Lars Schmidt-Thieme	Recommender systems are an important component of many websites. Two of the most popular approaches are based on matrix factorization (MF) and Markov chains (MC). MF methods learn the general taste of a user by factorizing the matrix over observed user-item preferences. On the other hand, MC methods model sequential behavior by learning a transition graph over items that is used to predict the next action based on the recent actions of a user. In this paper, we present a method bringing both approaches together. Our method is based on personalized transition graphs over underlying Markov chains. That means for each user an own transition matrix is learned - thus in total the method uses a transition cube. As the observations for estimating the transitions are usually very limited, our method factorizes the transition cube with a pairwise interaction model which is a special case of the Tucker Decomposition. We show that our factorized personalized MC (FPMC) model subsumes both a common Markov chain and the normal matrix factorization model. For learning the model parameters, we introduce an adaption of the Bayesian Personalized Ranking (BPR) framework for sequential basket data. Empirically, we show that our FPMC model outperforms both the common matrix factorization and the unpersonalized MC model both learned with and without factorization.	Factorizing personalized Markov chains for next-basket recommendation	NA:NA:NA	2010
Steven A. Ross:J. Alex Halderman:Adam Finkelstein	This paper introduces a captcha based on upright orientation of line drawings rendered from 3D models. The models are selected from a large database, and images are rendered from random viewpoints, affording many different drawings from a single 3D model. The captcha presents the user with a set of images, and the user must choose an upright orientation for each image. This task generally requires understanding of the semantic content of the image, which is believed to be difficult for automatic algorithms. We describe a process called covert filtering whereby the image database can be continually refreshed with drawings that are known to have a high success rate for humans, by inserting randomly into the captcha new images to be evaluated. Our analysis shows that covert filtering can ensure that captchas are likely to be solvable by humans while deterring attackers who wish to learn a portion of the database. We performed several user studies that evaluate how effectively people can solve the captcha. Comparing these results to an attack based on machine learning, we find that humans possess a substantial performance advantage over computers.	Sketcha: a captcha based on line drawings of 3D models	NA:NA:NA	2010
Carsten Saathoff:Ansgar Scherp	The semantics of rich multimedia presentations in the web such as SMIL, SVG, and Flash cannot or only to a very limited extend be understood by search engines today. This hampers the retrieval of such presentations and makes their archival and management a difficult task. Existing metadata models and metadata standards are either conceptually too narrow, focus on a specific media type only, cannot be used and combined together, or are not practically applicable for the semantic description of rich multimedia presentations. In this paper, we propose the Multimedia Metadata Ontology (M3O) for annotating rich, structured multimedia presentations. The M3O provides a generic modeling framework for representing sophisticated multimedia metadata. It allows for integrating the features provided by the existing metadata models and metadata standards. Our approach bases on Semantic Web technologies and can be easily integrated with multimedia formats such as the W3C standards SMIL and SVG. With the M3O, we unlock the semantics of rich multimedia presentations in the web by making the semantics machine-readable and machine-understandable. The M3O is used with our SemanticMM4U framework for the multi-channel generation of semantically-rich multimedia presentations.	Unlocking the semantics of multimedia presentations in the web with the multimedia metadata ontology	NA:NA	2010
Eldar Sadikov:Jayant Madhavan:Lu Wang:Alon Halevy	We address the problem of clustering the refinements of a user search query. The clusters computed by our proposed algorithm can be used to improve the selection and placement of the query suggestions proposed by a search engine, and can also serve to summarize the different aspects of information relevant to the original user query. Our algorithm clusters refinements based on their likely underlying user intents by combining document click and session co-occurrence information. At its core, our algorithm operates by performing multiple random walks on a Markov graph that approximates user search behavior. A user study performed on top search engine queries shows that our clusters are rated better than corresponding clusters computed using approaches that use only document click or only sessions co-occurrence information.	Clustering query refinements by user intent	NA:NA:NA:NA	2010
Takeshi Sakaki:Makoto Okazaki:Yutaka Matsuo	Twitter, a popular microblogging service, has received much attention recently. An important characteristic of Twitter is its real-time nature. For example, when an earthquake occurs, people make many Twitter posts (tweets) related to the earthquake, which enables detection of earthquake occurrence promptly, simply by observing the tweets. As described in this paper, we investigate the real-time interaction of events such as earthquakes in Twitter and propose an algorithm to monitor tweets and to detect a target event. To detect a target event, we devise a classifier of tweets based on features such as the keywords in a tweet, the number of words, and their context. Subsequently, we produce a probabilistic spatiotemporal model for the target event that can find the center and the trajectory of the event location. We consider each Twitter user as a sensor and apply Kalman filtering and particle filtering, which are widely used for location estimation in ubiquitous/pervasive computing. The particle filter works better than other comparable methods for estimating the centers of earthquakes and the trajectories of typhoons. As an application, we construct an earthquake reporting system in Japan. Because of the numerous earthquakes and the large number of Twitter users throughout the country, we can detect an earthquake with high probability (96% of earthquakes of Japan Meteorological Agency (JMA) seismic intensity scale 3 or more are detected) merely by monitoring tweets. Our system detects earthquakes promptly and sends e-mails to registered users. Notification is delivered much faster than the announcements that are broadcast by the JMA.	Earthquake shakes Twitter users: real-time event detection by social sensors	NA:NA:NA	2010
Alessandra Sala:Lili Cao:Christo Wilson:Robert Zablit:Haitao Zheng:Ben Y. Zhao	Access to realistic, complex graph datasets is critical to research on social networking systems and applications. Simulations on graph data provide critical evaluation of new systems and applications ranging from community detection to spam filtering and social web search. Due to the high time and resource costs of gathering real graph datasets through direct measurements, researchers are anonymizing and sharing a small number of valuable datasets with the community. However, performing experiments using shared real datasets faces three key disadvantages: concerns that graphs can be de-anonymized to reveal private information, increasing costs of distributing large datasets, and that a small number of available social graphs limits the statistical confidence in the results. The use of measurement-calibrated graph models is an attractive alternative to sharing datasets. Researchers can "fit" a graph model to a real social graph, extract a set of model parameters, and use them to generate multiple synthetic graphs statistically similar to the original graph. While numerous graph models have been proposed, it is unclear if they can produce synthetic graphs that accurately match the properties of the original graphs. In this paper, we explore the feasibility of measurement-calibrated synthetic graphs using six popular graph models and a variety of real social graphs gathered from the Facebook social network ranging from 30,000 to 3 million edges. We find that two models consistently produce synthetic graphs with common graph metric values similar to those of the original graphs. However, only one produces high fidelity results in our application-level benchmarks. While this shows that graph models can produce realistic synthetic graphs, it also highlights the fact that current graph metrics remain incomplete, and some applications expose graph properties that do not map to existing metrics.	Measurement-calibrated graph models for social network experiments	NA:NA:NA:NA:NA:NA	2010
Mark Sandler:S. Muthukrishnan	There are many online systems where millions of users post original content such as videos, reviews of items such as products, services and businesses, etc. While there are general rules for good behavior or even formal Terms of Service, there are still users who post content that is not suitable. Increasingly, online systems rely on other users who view the posted content to provide feedback. We study online systems where users report negative feedback, i.e., report abuse; these systems are quite distinct from much studied, traditional reputation systems that focus on eliciting popularity of content by various voting methods. The central problem that we study here is how to monitor the quality of negative feedback, that is, detect negative feedback which is incorrect, or perhaps even malicious. Systems address this problem by testing flags manually, which is an expensive operation. As a result, there is a tradeoff between the number of manual tests and the number of errors defined as the number of incorrect flags the monitoring system misses. Our contributions are as follows: We initiate a systematic study of negative feedbacks systems. Our framework is general enough to be applicable for a variety of systems. In this framework, the number of errors the system admits is bounded over the worst case of adversarial users while simultaneously the system performs only small amount of manual testing for multitude of standard users who might still err while reporting. Our main contribution is a randomized monitoring algorithm that we call Adaptive Probabilistic Testing (APT), that is simple to implement and has guarantees on expected number of errors. Even for adversarial users, the total expected error is bounded by "N over N flags for a given e < 0. Simultaneously, the number of tests performed by the algorithm is within a constant factor of the best possible algorithm for standard users. Finally, we present empirical study of our algorithm that shows its performance on both synthetic data and real data accumulated from a variety of negative feedback systems at Google. Our study indicates that the algorithm performs better than the analysis above shows.	Monitoring algorithms for negative feedback systems	NA:NA	2010
Rodrygo L.T. Santos:Craig Macdonald:Iadh Ounis	When a Web user's underlying information need is not clearly specified from the initial query, an effective approach is to diversify the results retrieved for this query. In this paper, we introduce a novel probabilistic framework for Web search result diversification, which explicitly accounts for the various aspects associated to an underspecified query. In particular, we diversify a document ranking by estimating how well a given document satisfies each uncovered aspect and the extent to which different aspects are satisfied by the ranking as a whole. We thoroughly evaluate our framework in the context of the diversity task of the TREC 2009 Web track. Moreover, we exploit query reformulations provided by three major Web search engines (WSEs) as a means to uncover different query aspects. The results attest the effectiveness of our framework when compared to state-of-the-art diversification approaches in the literature. Additionally, by simulating an upper-bound query reformulation mechanism from official TREC data, we draw useful insights regarding the effectiveness of the query reformulations generated by the different WSEs in promoting diversity.	Exploiting query reformulations for web search result diversification	NA:NA:NA	2010
Stefan Siersdorfer:Sergiu Chelaru:Wolfgang Nejdl:Jose San Pedro	An analysis of the social video sharing platform YouTube reveals a high amount of community feedback through comments for published videos as well as through meta ratings for these comments. In this paper, we present an in-depth study of commenting and comment rating behavior on a sample of more than 6 million comments on 67,000 YouTube videos for which we analyzed dependencies between comments, views, comment ratings and topic categories. In addition, we studied the influence of sentiment expressed in comments on the ratings for these comments using the SentiWordNet thesaurus, a lexical WordNet-based resource containing sentiment annotations. Finally, to predict community acceptance for comments not yet rated, we built different classifiers for the estimation of ratings for these comments. The results of our large-scale evaluations are promising and indicate that community feedback on already rated comments can help to filter new unrated comments or suggest particularly useful but still unrated comments.	How useful are your comments?: analyzing and predicting youtube comments and comment ratings	NA:NA:NA:NA	2010
Yang Song:Li-wei He	Query suggestion has been an effective approach to help users narrow down to the information they need. However, most of existing studies focused on only popular/head queries. Since rare queries possess much less information (e.g., clicks) than popular queries in the query logs, it is much more difficult to efficiently suggest relevant queries to a rare query. In this paper, we propose an optimal rare query suggestion framework by leveraging implicit feedbacks from users in the query logs. Our model resembles the principle of pseudo-relevance feedback which assumes that top-returned results by search engines are relevant. However, we argue that the clicked URLs and skipped URLs contain different levels of information and thus should be treated differently. Hence, our framework optimally combines both the click and skip information from users and uses a random walk model to optimize the query correlation. Our model specifically optimizes two parameters: (1) the restarting (jumping) rate of random walk, and (2) the combination ratio of click and skip information. Unlike the Rocchio algorithm, our learning process does not involve the content of the URLs but simply leverages the click and skip counts in the query-URL bipartite graphs. Consequently, our model is capable of scaling up to the need of commercial search engines. Experimental results on one-month query logs from a large commercial search engine with over 40 million rare queries demonstrate the superiority of our framework, with statistical significance, over the traditional random walk models and pseudo-relevance feedback models.	Optimal rare query suggestion with implicit user feedback	NA:NA	2010
Yaxiao Song:Gary Marchionini:Chi Young Oh	Video summarization is a mechanism for generating short summaries of the video to help people quickly make sense of the content of the video before downloading or seeking more detailed information. To produce reliable automatic video summarization algorithms, it is essential to first understand how human beings create video summaries with manual efforts. This paper focuses on a corpus of instructional documentary video, and seeks to improve automatic video summaries by understanding what features in the video catch the eyes and ears of human assessors, and using these findings to inform automatic summarization algorithms. The paper contributes a thorough and valuable methodology for performing automatic video summarization, and the methodology can be extended to inform summarization of other video corpuses.	What are the most eye-catching and ear-catching features in the video?: implications for video summarization	NA:NA:NA	2010
Sid Stamm:Brandon Sterne:Gervase Markham	The last three years have seen a dramatic increase in both awareness and exploitation of Web Application Vulnerabilities. 2008 and 2009 saw dozens of high-profile attacks against websites using Cross Site Scripting (XSS) and Cross Site Request Forgery (CSRF) for the purposes of information stealing, website defacement, malware planting, clickjacking, etc. While an ideal solution may be to develop web applications free from any exploitable vulnerabilities, real world security is usually provided in layers. We present content restrictions, and a content restrictions enforcement scheme called Content Security Policy (CSP), which intends to be one such layer. Content restrictions allow site designers or server administrators to specify how content interacts on their web sites-a security mechanism desperately needed by the untamed Web. These content restrictions rules are activated and enforced by supporting web browsers when a policy is provided for a site via HTTP, and we show how a system such as CSP can be effective to lock down sites and provide an early alert system for vulnerabilities on a web site. Our scheme is also easily deployed, which is made evident by our prototype implementation in Firefox and on the Mozilla Add-Ons web site.	Reining in the web with content security policy	NA:NA:NA	2010
Mingxuan Sun:Guy Lebanon:Kevyn Collins-Thompson	We introduce a new dissimilarity function for ranked lists, the expected weighted Hoeffding distance, that has several advantages over current dissimilarity measures for ranked search results. First, it is easily customized for users who pay varying degrees of attention to websites at different ranks. Second, unlike existing measures such as generalized Kendall's tau, it is based on a true metric, preserving meaningful embeddings when visualization techniques like multi-dimensional scaling are applied. Third, our measure can effectively handle partial or missing rank information while retaining a probabilistic interpretation. Finally, the measure can be made computationally tractable and we give a highly efficient algorithm for computing it. We then apply our new metric with multi-dimensional scaling to visualize and explore relationships between the result sets from different search engines, showing how the weighted Hoeffding distance can distinguish important differences in search engine behavior that are not apparent with other rank-distance metrics. Such visualizations are highly effective at summarizing and analyzing insights on which search engines to use, what search strategies users can employ, and how search results evolve over time. We demonstrate our techniques using a collection of popular search engines, a representative set of queries, and frequently used query manipulation methods.	Visualizing differences in web search algorithms using the expected weighted hoeffding distance	NA:NA:NA	2010
Shuo Tang:Chris Grier:Onur Aciicmez:Samuel T. King	Alhambra is a browser-based system designed to enforce and test web browser security policies. At the core of Alhambra is a policy-enhanced browser supporting fine-grain security policies that restrict web page contents and execution. Alhambra requires no server-side modifications or additions to the web application. Policies can restrict the construction of the document as well as the execution of JavaScript using access control rules and a taint-tracking engine. Using the Alhambra browser, we present two security policies that we have built using our architecture, both designed to prevent cross-site scripting. The first policy uses a taint-tracking engine to prevent cross-site scripting attacks that exploit bugs in the client-side of the web applications. The second one uses browsing history to create policies that restrict the contents of documents and prevent the inclusion of malicious content. Using Alhambra we analyze the impact of policies on the compatibility of web pages. To test compatibility, Alhambra supports revisiting user-generated browsing sessions and comparing multiple security policies in parallel to quickly and automatically evaluate security policies. To compare security policies for identical pages we have also developed useful comparison metrics that quantify differences between identical pages executed with different security policies. Not only do we show that our policies are effective with minimal compatibility cost, we also demonstrate that Alhambra can enforce strong security policies and provide quantitative evaluation of the differences introduced by security policies.	Alhambra: a system for creating, enforcing, and testing browser security policies	NA:NA:NA:NA	2010
Max Van Kleek:Brennan Moore:David R. Karger:Paul André:m.c. schraefel	The transition of personal information management (PIM) tools off the desktop to the Web presents an opportunity to augment these tools with capabilities provided by the wealth of real-time information readily available. In this paper, we describe a next-generation personal information assistance engine that lets end-users delegate to it various simple context- and activity-reactive tasks and reminders. Our system, Atomate, treats RSS/ATOM feeds from social networking and life-tracking sites as sensor streams, integrating information from such feeds into a simple unified RDF world model representing people, places and things and their timevarying states and activities. Combined with other information sources on the web, including the user's online calendar, web-based e-mail client, news feeds and messaging services, Atomate can be made to automatically carry out a variety of simple tasks for the user, ranging from context-aware filtering and messaging, to sharing and social coordination actions. Atomate's open architecture and world model easily accommodate new information sources and actions via the addition of feeds and web services. To make routine use of the system easy for non-programmers, Atomate provides a constrained-input natural language interface (CNLI) for behavior specification, and a direct-manipulation interface for inspecting and updating its world model.	Atomate it! end-user context-sensitive automation using heterogeneous information sources on the web	NA:NA:NA:NA:NA	2010
Roelof van Zwol:Börkur Sigurbjornsson:Ramu Adapala:Lluis Garcia Pueyo:Abhinav Katiyar:Kaushal Kurapati:Mridul Muralidharan:Sudar Muthu:Vanessa Murdock:Polly Ng:Anand Ramani:Anuj Sahai:Sriram Thiru Sathish:Hari Vasudev:Upendra Vuyyuru	This paper describes MediaFaces, a system that enables faceted exploration of media collections. The system processes semi-structured information sources to extract objects and facets, e.g. the relationships between two objects. Next, we rank the facets based on a statistical analysis of image search query logs, and the tagging behaviour of users annotating photos in Flickr. For a given object of interest, we can then retrieve the top-k most relevant facets and present them to the user. The system is currently deployed in production by Yahoo!'s image search engine1. We present the system architecture, its main components, and the application of the system as part of the image search experience.	Faceted exploration of image search results	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2010
Tim Weninger:William H. Hsu:Jiawei Han	We present Content Extraction via Tag Ratios (CETR) - a method to extract content text from diverse webpages by using the HTML document's tag ratios. We describe how to compute tag ratios on a line-by-line basis and then cluster the resulting histogram into content and non-content areas. Initially, we find that the tag ratio histogram is not easily clustered because of its one-dimensionality; therefore we extend the original approach in order to model the data in two dimensions. Next, we present a tailored clustering technique which operates on the two-dimensional model, and then evaluate our approach against a large set of alternative methods using standard accuracy, precision and recall metrics on a large and varied Web corpus. Finally, we show that, in most cases, CETR achieves better content extraction performance than existing methods, especially across varying web domains, languages and styles.	CETR: content extraction via tag ratios	NA:NA:NA	2010
Rongjing Xiang:Jennifer Neville:Monica Rogati	Previous work analyzing social networks has mainly focused on binary friendship relations. However, in online social networks the low cost of link formation can lead to networks with heterogeneous relationship strengths (e.g., acquaintances and best friends mixed together). In this case, the binary friendship indicator provides only a coarse representation of relationship information. In this work, we develop an unsupervised model to estimate relationship strength from interaction activity (e.g., communication, tagging) and user similarity. More specifically, we formulate a link-based latent variable model, along with a coordinate ascent optimization procedure for the inference. We evaluate our approach on real-world data from Facebook and LinkedIn, showing that the estimated link weights result in higher autocorrelation and lead to improved classification accuracy.	Modeling relationship strength in online social networks	NA:NA:NA	2010
Xiaoxin Yin:Wenzhao Tan:Xiao Li:Yi-Chin Tu	Today the major web search engines answer queries by showing ten result snippets, which need to be inspected by users for identifying relevant results. In this paper we investigate how to extract structured information from the web, in order to directly answer queries by showing the contents being searched for. We treat users' search trails (i.e., post-search browsing behaviors) as implicit labels on the relevance between web contents and user queries. Based on such labels we use information extraction approach to build wrappers and extract structured information. An important observation is that many web sites contain pages for name entities of certain categories (e.g., AOL Music contains a page for each musician), and these pages have the same format. This makes it possible to build wrappers from a small amount of implicit labels, and use them to extract structured information from many web pages for different name entities. We propose STRUCLICK, a fully automated system for extracting structured information for queries containing name entities of certain categories. It can identify important web sites from web search logs, build wrappers from users' search trails, filter out bad wrappers built from random user clicks, and combine structured information from different web sites for each query. Comparing with existing approaches on information extraction, STRUCLICK can assign semantics to extracted data without any human labeling or supervision. We perform comprehensive experiments, which show STRUCLICK achieves high accuracy and good scalability.	Automatic extraction of clickable structured web contents for name entity queries	NA:NA:NA:NA	2010
Xiaoxin Yin:Sarthak Shah	A significant portion of web search queries are name entity queries. The major search engines have been exploring various ways to provide better user experiences for name entity queries, such as showing "search tasks" (Bing search) and showing direct answers (Yahoo!, Kosmix). In order to provide the search tasks or direct answers that can satisfy most popular user intents, we need to capture these intents, together with relationships between them. In this paper we propose an approach for building a hierarchical taxonomy of the generic search intents for a class of name entities (e.g., musicians or cities). The proposed approach can find phrases representing generic intents from user queries, and organize these phrases into a tree, so that phrases indicating equivalent or similar meanings are on the same node, and the parent-child relationships of tree nodes represent the relationships between search intents and their sub-intents. Three different methods are proposed for tree building, which are based on directed maximum spanning tree, hierarchical agglomerative clustering, and pachinko allocation model. Our approaches are purely based on search logs, and do not utilize any existing taxonomies such as Wikipedia. With the evaluation by human judges (via Mechanical Turk), it is shown that our approaches can build trees of phrases that capture the relationships between important search intents.	Building taxonomy of web search intents for name entity queries	NA:NA	2010
Yisong Yue:Rajan Patel:Hein Roehrig	Leveraging clickthrough data has become a popular approach for evaluating and optimizing information retrieval systems. Although data is plentiful, one must take care when interpreting clicks, since user behavior can be affected by various sources of presentation bias. While the issue of position bias in clickthrough data has been the topic of much study, other presentation bias effects have received comparatively little attention. For instance, since users must decide whether to click on a result based on its summary (e.g., the title, URL and abstract), one might expect clicks to favor "more attractive" results. In this paper, we examine result summary attractiveness as a potential source of presentation bias. This study distinguishes itself from prior work by aiming to detect systematic biases in click behavior due to attractive summaries inflating perceived relevance. Our experiments conducted on the Google web search engine show substantial evidence of presentation bias in clicks towards results with more attractive titles.	Beyond position bias: examining result attractiveness as a source of presentation bias in clickthrough data	NA:NA:NA	2010
Elena Zheleva:John Guiver:Eduarda Mendes Rodrigues:Nataša Milić-Frayling	User experience in social media involves rich interactions with the media content and other participants in the community. In order to support such communities, it is important to understand the factors that drive the users' engagement. In this paper we show how to define statistical models of different complexity to describe patterns of song listening in an online music community. First, we adapt the LDA model to capture music taste from listening activities across users and identify both the groups of songs associated with the specific taste and the groups of listeners who share the same taste. Second, we define a graphical model that takes into account listening sessions and captures the listening moods of users in the community. Our session model leads to groups of songs and groups of listeners with similar behavior across listening sessions and enables faster inference when compared to the LDA model. Our experiments with the data from an online media site demonstrate that the session model is better in terms of the perplexity compared to two other models: the LDA-based taste model that does not incorporate cross-session information and a baseline model that does not use latent groupings of songs.	Statistical models of music-listening sessions in social media	NA:NA:NA:NA	2010
Vincent W. Zheng:Yu Zheng:Xing Xie:Qiang Yang	With the increasing popularity of location-based services, such as tour guide and location-based social network, we now have accumulated many location data on the Web. In this paper, we show that, by using the location data based on GPS and users' comments at various locations, we can discover interesting locations and possible activities that can be performed there for recommendations. Our research is highlighted in the following location-related queries in our daily life: 1) if we want to do something such as sightseeing or food-hunting in a large city such as Beijing, where should we go? 2) If we have already visited some places such as the Bird's Nest building in Beijing's Olympic park, what else can we do there? By using our system, for the first question, we can recommend her to visit a list of interesting locations such as Tiananmen Square, Bird's Nest, etc. For the second question, if the user visits Bird's Nest, we can recommend her to not only do sightseeing but also to experience its outdoor exercise facilities or try some nice food nearby. To achieve this goal, we first model the users' location and activity histories that we take as input. We then mine knowledge, such as the location features and activity-activity correlations from the geographical databases and the Web, to gather additional inputs. Finally, we apply a collective matrix factorization method to mine interesting locations and activities, and use them to recommend to the users where they can visit if they want to perform some specific activities and what they can do if they visit some specific places. We empirically evaluated our system using a large GPS dataset collected by 162 users over a period of 2.5 years in the real-world. We extensively evaluated our system and showed that our system can outperform several state-of-the-art baselines.	Collaborative location and activity recommendations with GPS history data	NA:NA:NA:NA	2010
Yingwu Zhu	In online content voting networks, aggregate user activities (e.g., submitting and rating content) make high-quality content thrive through the unprecedented scale, high dynamics and divergent quality of user generated content (UGC). To better understand the nature and impact of online content voting networks, we have analyzed Digg, a popular online social news aggregator and rating website. Based on a large amount of data collected, we provide an in-depth study of Digg. We study structural properties of Digg social network, revealing some strikingly distinct properties such as low link symmetry and the power-law distribution of node outdegree with truncated tails. We explore impact of the social network on user digging activities, and investigate the issues of content promotion, content filtering, vote spam and content censorship, which are inherent to content rating networks. We also provide insight into design of content promotion algorithms and recommendation-assisted content discovery. Overall, we believe that the results presented in this paper are crucial in understanding online content rating networks.	Measurement and analysis of an online content voting network: a case study of Digg	NA	2010
Jae-wook Ahn:Peter Brusilovsky	Information retrieval is one of the most popular information access methods for overcoming the information overload problem of the Web. However, its interaction model is still utilizing the old text-based ranked lists and static interaction algorithm. In this paper, we introduce our adaptive visualization approach for searching the Web, which we call Adaptive VIBE. It is an extended version of a reference point-based spatial visualization algorithm, and is designed to serve as a user interaction module for a personalized search system. Personalized search can incorporate dynamic user interests and different contexts, improving search results. When it is combined with adaptive visualization, it can encourage users to become involved in the search process more actively by exploring the information space and learning new facts for effective searching. In this paper, we introduce the rationale and functions of our adaptive visualization approach and discuss the approaches' potential to create a better search environment for the Web.	What you see is what you search: adaptive visual search framework for the web	NA:NA	2010
Rosa Alarcón:Erik Wilde	Service descriptions allow designers to document, understand, and use services, creating new useful and complex services with aggregated business value. Unlike RPC-based services, REST characteristics require a different approach to service description. We present the Resource Linking Language (ReLL) that introduces the concepts of media types, resource types, and link types as first class citizens for a service description. A proof of concept, a crawler called RESTler that crawls RESTful services based on ReLL descriptions, is also presented.	RESTler: crawling RESTful services	NA:NA	2010
Anthony P. Badali:Parham Aarabi:Ron Appel	NA	Intelligent ad resizing	NA:NA:NA	2010
Raju Balakrishnan:Subbarao Kambhampati	We consider the problem of deep web source selection and argue that existing source selection methods are inadequate as they are based on local similarity assessment. Specically, they fail to account for the fact that sources can vary in trustworthiness and individual results can vary in importance. In response, we formulate a global measure to calculate relevance and trustworthiness of a source based on agreement between the answers provided by different sources. Agreement is modeled as a graph with sources at the vertices. On this agreement graph, source quality scores - namely SourceRank - are calculated as the stationary visit probability of a weighted random walk. Our experiments on online databases and 675 book sources from Google Base show that SourceRank improves relevance of the results by 25-40% over existing methods and Google Base ranking. SourceRank also reduces linearly with the corruption levels of the sources.	SourceRank: relevance and trust assessment for deep web sources based on inter-source agreement	NA:NA	2010
Edward Benson:Adam Marcus:Fabian Howahl:David Karger	The web has dramatically enhanced people's ability to communicate ideas, knowledge, and opinions. But the authoring tools that most people understand, blogs and wikis, primarily guide users toward authoring text. In this work, we show that substantial gains in expressivity and communication would accrue if people could easily share richly structured information in meaningful visualizations. We then describe several extensions we have created for blogs and wikis that enable users to publish, share, and aggregate such structured information using the same workflows they apply to text. In particular, we aim to preserve those attributes that make blogs and wikis so effective: one-click access to the information, one-click publishing of content, natural authoring interfaces, and the ability to easily copy-and-paste information and visualizations from other sources.	Talking about data: sharing richly structured information through blogs and wikis	NA:NA:NA:NA	2010
Smriti Bhagat:Graham Cormode:Balachander Krishnamurthy:Divesh Srivastava	Anonymization of social networks before they are published or shared has become an important research question. Recent work on anonymizing social networks has looked at privacy preserving techniques for publishing a single instance of the network. However, social networks evolve and a single instance is inadequate for analyzing the evolution of the social network or for performing any longitudinal data analysis. We study the problem of repeatedly publishing social network data as the network evolves, while preserving privacy of users. Publishing multiple instances of the same network independently has privacy risks, since stitching the information together may allow an adversary to identify users in the networks. We propose methods to anonymize a dynamic network such that the privacy of users is preserved when new nodes and edges are added to the published network. These methods make use of link prediction algorithms to model the evolution of the social network. Using this predicted graph to perform group-based anonymization, the loss in privacy caused by new edges can be reduced. We evaluate the privacy loss on publishing multiple social network instances using our methods.	Privacy in dynamic social networks	NA:NA:NA:NA	2010
Sumit Bhatia:Prasenjit Mitra:C. Lee Giles	Algorithms are an integral part of computer science literature. However, none of the current search engines offer specialized algorithm search facility. We describe a vertical search engine that identifies the algorithms present in documents and extracts and indexes the related metadata and textual description of the identified algorithms. This algorithm specific information is then utilized for algorithm ranking in response to user queries. Experimental results show the superiority of our system on other popular search engines.	Finding algorithms in scientific articles	NA:NA:NA	2010
Lorenzo Blanco:Mirko Bronzi:Valter Crescenzi:Paolo Merialdo:Paolo Papotti	A large number of web sites publish pages containing structured information about recognizable concepts, but these data are only partially used by current applications. Although such information is spread across a myriad of sources, the web scale implies a relevant redundancy. We present a domain independent system that exploits the redundancy of information to automatically extract and integrate data from the Web. Our solution concentrates on sources that provide structured data about multiple instances from the same conceptual domain, e.g. financial data, product information. Our proposal is based on an original approach that exploits the mutual dependency between the data extraction and the data integration tasks. Experiments confirmed the quality and the feasibility of the approach.	Exploiting information redundancy to wring out structured data from the web	NA:NA:NA:NA:NA	2010
Roi Blanco:Edward Bortnikov:Flavio Junqueira:Ronny Lempel:Luca Telloli:Hugo Zaragoza	A Web search engine must update its index periodically to incorporate changes to the Web, and we argue in this work that index updates fundamentally impact the design of search engine result caches. Index updates lead to the problem of cache invalidation: invalidating cached entries of queries whose results have changed. To enable efficient invalidation of cached results, we propose a framework for developing invalidation predictors and some concrete predictors. Evaluation using Wikipedia documents and a query log from Yahoo! shows that selective invalidation of cached search results can lower the number of query re-evaluations by as much as 30% compared to a baseline time-to-live scheme, while returning results of similar freshness.	Caching search engine results over incremental indices	NA:NA:NA:NA:NA:NA	2010
Paul Bohunsky:Wolfgang Gatterbauer	Clustering and retrieval of web pages dominantly relies on analyzing either the content of individual web pages or the link structure between them. Some literature also suggests to use the structure of web pages, notably the structure of its DOM tree. However, little work considers the visual structure of web pages for clustering. In this paper (i) we motivate visual structure-based web page clustering and retrieval for a number of applications, (ii) we formalize a visual box model-based representation of web pages that supports new metrics of visual similarity, and (iii) we report on our current work on evaluating human perception of visual similarity of web pages and applying the learned visual similarity features to web page clustering and retrieval.	Visual structure-based web page clustering and retrieval	NA:NA	2010
Andrew Caniff:Lei Lu:Ningfang Mi:Ludmila Cherkasova:Evgenia Smirni	In this paper, we present Fastrack, a parameter-free algorithm for dynamic resource provisioning that uses simple statistics to promptly distill information about changes in workload burstiness. This information, coupled with the application's end-to-end response times and system bottleneck characteristics, guide resource allocation that shows to be very effective under a broad variety of burstiness profiles and bottleneck scenarios.	Efficient resource allocation and power saving in multi-tiered systems	NA:NA:NA:NA:NA	2010
Liangliang Cao:Andrey Del Pozo:Xin Jin:Jiebo Luo:Jiawei Han:Thomas S. Huang	With the explosive growth of digital cameras and online media, it has become crucial to design efficient methods that help users browse and search large image collections. The recent VisualRank algorithm [4] employs visual similarity to represent the link structure in a graph so that the classic PageRank algorithm can be applied to select the most relevant images. However, measuring visual similarity is difficult when there exist diversified semantics in the image collection, and the results from VisualRank cannot supply good visual summarization with diversity. This paper proposes to rank the images in a structural fashion, which aims to discover the diverse structure embedded in photo collections, and rank the images according to their similarity among local neighborhoods instead of across the entire photo collection. We design a novel algorithm named RankCompete, which generalizes the PageRank algorithm for the task of simultaneous ranking and clustering. The experimental results show that RankCompete outperforms VisualRank and provides an efficient but effective tool for organizing web photos.	RankCompete: simultaneous ranking and clustering of web photos	NA:NA:NA:NA:NA:NA	2010
Rongwei Cen:Yiqun Liu:Min Zhang:Liyun Ru:Shaoping Ma	Under different language contexts, people choose different terms or phrases to express their feelings and opinions. When a user is writing a paper or chatting with a friend, he/she applies a specific language model corresponding to the underlying goal. This paper presents a log-based study of analyzing the language models with specific goals. We exhibit the statistical information of terms and software programs, propose some methods to estimate the divergence of language models with specific user goals and measure the discrimination of these models. Experimental results show that the language models with different user goals have large divergence and different discrimination. These study conclusions can be applied to understand user needs and improve Human-Computer Interaction (HCI).	Study language models with specific user goals	NA:NA:NA:NA:NA	2010
Praphul Chandra:Geetha Manjunath	As the web grows in size, interfaces & interactions across websites diverge - for differentiation and arguably for a better user experience. However, this size & diversity is also a cognitive load for the user who has to learn a new user interface for every new website she visits. Several studies have confirmed the importance of well designed websites. In this paper, we propose a method for quantitative evaluation of the navigational complexity of user interactions on the web. Our approach of quantifying interaction complexity exploits the modeling of the web as a graph and uses the information theoretic definition of complexity. It enables us to measure the navigational complexity of web interaction in bits. Our approach is structural in nature and can be applied to both traditional paradigm of web interaction (browsing) and to emerging paradigms of web interaction like web widgets.	Navigational complexity in web interactions	NA:NA	2010
Tianqi Chen:Jun Yan:Guirong Xue:Zheng Chen	Recently, Behavioral Targeting (BT) is attracting much attention from both industry and academia due to its rapid growth in online advertising market. Though a basic assumption of BT, which is, the users who share similar Web browsing behaviors will have similar preference over ads, has been empirically verified, we argue that the users' ad click preference and Web browsing behavior are not reflecting the same user intent though they are correlated. In this paper, we propose to formulate BT as a transfer learning problem. We treat the users' preference over ads and Web browsing behaviors as two different user behavioral domains and propose to utilize transfer learning strategy across these two user behavioral domains to segment users for BT ads delivery. We show that some classical BT solutions could be formulated in transfer learning view. As an example, we propose to leverage translated learning, which is a recent proposed transfer learning algorithm, to benefit the BT ads delivery. Experimental results on real ad click data show that, BT user segmentation by the approach of transfer learning can outperform the classical user segmentation strategies for larger than 20% in terms of smoothed ad Click Through Rate(CTR).	Transfer learning for behavioral targeting	NA:NA:NA:NA	2010
Zhineng Chen:Juan Cao:YiCheng Song:Junbo Guo:Yongdong Zhang:Jintao Li	Tag recommendation is a common way to enrich the textual annotation of multimedia contents. However, state-of-the-art recommendation methods are built upon the pair-wised tag relevance, which hardly capture the context of the web video, i.e., when who are doing what at where. In this paper we propose the context-oriented tag recommendation (CtextR) approach, which expands tags for web videos under the context-consistent constraint. Given a web video, CtextR first collects the multi-form WWW resources describing the same event with the video, which produce an informative and consistent context; and then, the tag recommendation is conducted based on the obtained context. Experiments on an 80,031 web video collection show CtextR recommends various relevant tags to web videos. Moreover, the enriched tags improve the performance of web video categorization.	Context-oriented web video tag recommendation	NA:NA:NA:NA:NA:NA	2010
Eric Crestan:Patrick Pantel	A wealth of knowledge is encoded in the form of tables on the World Wide Web. We propose a classification algorithm and a rich feature set for automatically recognizing layout tables and attribute/value tables. We report the frequencies of these table types over a large analysis of the Web and propose open challenges for extracting from attribute/value tables semantic triples (knowledge). We then describe a solution to a key problem in extracting semantic triples: protagonist detection, i.e., finding the subject of the table that often is not present in the table itself. In 79% of our Web tables, our method finds the correct protagonist in its top three returned candidates.	Web-scale knowledge extraction from semi-structured tables	NA:NA	2010
Munmun De Choudhury:Moran Feldman:Sihem Amer-Yahia:Nadav Golbandi:Ronny Lempel:Cong Yu	Vacation planning is a frequent laborious task which requires skilled interaction with a multitude of resources. This paper develops an end-to-end approach for constructing intra-city travel itineraries automatically by tapping a latent source reflecting geo-temporal breadcrumbs left by millions of tourists. In particular, the popular rich media sharing site, Flickr, allows photos to be stamped by the date and time of when they were taken, and be mapped to Points Of Interest (POIs) by latitude-longitude information as well as semantic metadata (e.g., tags) that describe them. Our extensive user study on a "crowd-sourcing" marketplace (Amazon Mechanical Turk), indicates that high quality itineraries can be automatically constructed from Flickr data, when compared against popular professionally generated bus tours.	Constructing travel itineraries from tagged geo-temporal breadcrumbs	NA:NA:NA:NA:NA:NA	2010
Paul Dütting:Monika Henzinger:Ingmar Weber	Suppose you buy a new laptop and, simply because you like it so much, you recommend it to friends, encouraging them to purchase it as well. What would be an adequate price for the vendor of the laptop to pay for your recommendation? Personal recommendations like this are of considerable commercial interest, but unlike in sponsored search auctions there can be no truthful prices. Despite this "lack of truthfulness" the vendor of the product might still decide to pay you for recommendation e.g. because she wants to (i) provide you with an additional incentive to actually recommend her or to (ii) increase your satisfaction and/or brand loyalty. This leads us to investigate a pricing scheme based on the Shapley value [5] that satisfies certain "axioms of fairness". We find that it is vulnerable to manipulations and show how to overcome these difficulties using the anonymity-proof Shapley value of [4].	How much is your personal recommendation worth?	NA:NA:NA	2010
Ruslan R. Fayzrahmanov:Max C. Göbel:Wolfgang Holzinger:Bernhard Krüpl:Robert Baumgartner	Fast technological advancements and little compliance with accessibility standards by Web page authors pose serious obstacles to the Web experience of the blind user. We propose a unified Web document model that enables us to create a richer browsing experience and improved navigability for blind users. The model provides an integrated view on all aspects of a Web page and is leveraged to create a multi-axial user interface.	A unified ontology-based web page model for improving accessibility	NA:NA:NA:NA:NA	2010
Junlan Feng	Mobile voice search is a fast-growing business. It provides users an easier way to search for information using voice from mobile devices. In this paper, we describe a statistical approach to query parsing to assure search effectiveness. The task is to segment speech recognition (ASR) output, including ASR 1-Best and ASR word lattices, into segments and associate each segment with needed concepts in the application. We train the models including concept prior probability, query segment generation probability, and query subject probability from application data such as query log and source database. We apply the learned models on a mobile business search application and demonstrate the robustness of query parsing to ASR errors.	Query parsing in mobile voice search	NA	2010
Javier D. Fernández:Claudio Gutierrez:Miguel A. Martínez-Prieto	This paper studies the compressibility of RDF data sets. We show that big RDF data sets are highly compressible due to the structure of RDF graphs (power law), organization of URIs and RDF syntax verbosity. We present basic approaches to compress RDF data and test them with three well-known, real-world RDF data sets.	RDF compression: basic approaches	NA:NA:NA	2010
Max I. Fomitchev	We report the results of the analysis of website traffic logs and argue that both unique IP address and cookies vastly overestimate unique visitors, e.g. by factor of 8 in our studies. Google Analytics 'absolute unique visitors' measure is shown to produce similar 6x overestimation. To address the problem we present a new model for relating unique visitors to IP address or cookies.	How google analytics and conventional cookie tracking techniques overestimate unique visitors	NA	2010
Lisa M. Gandy:Nathan D. Nichols:Kristian J. Hammond	A useful approach for enabling computers to automatically create new content is utilizing the text, media, and information already present on the World Wide Web. The newly created content is known as "machine-generated content". For example, a machine-generated content system may create a multimedia news show with two animated anchors presenting a news story; one anchor reads the news story with text taken from an existing news article, and the other anchor regularly interrupts with his or her own opinion about the story. In this paper, we present such a system, and describe in detail its strategy for autonomously extracting and selecting the opinions given by the second anchor.	Shout out: integrating news and reader comments	NA:NA:NA	2010
Qi Gao:Geert-Jan Houben	With the enormous and still growing amount of data and user interaction on the Web, it becomes more and more necessary for data consumers to be able to assess the trustworthiness of data on the Web. In this paper, we present a framework for trust establishment and assessment on the Web of Data. Different from many approaches that build trust metrics within networks of people, we propose a model to represent the trust in concrete pieces of web data for a specific consumer, in which also the context is considered. Further more, we provide three strategies for trust assessment based on the principles of Linked Data, to overcome the shortcomings of the conventional web of documents: i.e. the lack of semantics and interlinking.	A framework for trust establishment and assessment on the web of data	NA:NA	2010
Guillermo Garrido:Francesco Bonchi:Aristides Gionis	In this paper we study the community structure of endorsement networks, i.e., social networks in which a directed edge u → v is asserting an action of support from user u to user v. Examples include scenarios in which a user u is favoring a photo, liking a post, or following the microblog of user v. Starting from the hypothesis that the footprint of a community in an endorsement network is a bipartite directed clique from a set of followers to a set of leaders, we apply frequent itemset mining techniques to discover such bicliques. Our analysis of real networks discovers that an interesting phenomenon is taking place: the leaders of a community are endorsing each other forming a very dense nucleus.	On the high density of leadership nuclei in endorsement social networks	NA:NA:NA	2010
C. Lee Giles:Yang Sun:Isaac G. Councill	Web crawlers are highly automated and seldom regulated manually. The diversity of crawler activities often leads to ethical problems such as spam and service attacks. In this research, quantitative models are proposed to measure the web crawler ethics based on their behaviors on web servers. We investigate and define rules to measure crawler ethics, referring to the extent to which web crawlers respect the regulations set forth in robots.txt configuration files. We propose a vector space model to represent crawler behavior and measure the ethics of web crawlers based on the behavior vectors. The results show that ethicality scores vary significantly among crawlers. Most commercial web crawlers' behaviors are ethical. However, many commercial crawlers still consistently violate or misinterpret certain robots.txt rules. We also measure the ethics of big search engine crawlers in terms of return on investment. The results show that Google has a higher score than other search engines for a US website but has a lower score than Baidu for Chinese websites.	Measuring the web crawler ethics	NA:NA:NA	2010
Liang Gou:Hung-Hsuan Chen:Jung-Hyun Kim:Xiaolong (Luke) Zhang:C. Lee Giles	To improve the search results for socially-connect users, we propose a ranking framework, Social Network Document Rank (SNDocRank). This framework considers both document contents and the similarity between a searcher and document owners in a social network and uses a Multi-level Actor Similarity (MAS) algorithm to efficiently calculate user similarity in a social network. Our experiment results based on YouTube data show that compared with the tf-idf algorithm, the SNDocRank method returns more relevant documents of interest. Our findings suggest that in this framework, a searcher can improve search by joining larger social networks, having more friends, and connecting larger local communities in a social network.	SNDocRank: document ranking based on social networks	NA:NA:NA:NA:NA	2010
Pankaj Gulhane:Rajeev Rastogi:Srinivasan H. Sengamedu:Ashwin Tengli	We propose a novel extraction approach that exploits content redundancy on the web to extract structured data from template-based web sites. We start by populating a seed database with records extracted from a few initial sites. We then identify values within the pages of each new site that match attribute values contained in the seed set of records. To filter out noisy attribute value matches, we exploit the fact that attribute values occur at fixed positions within template-based sites. We develop an efficient Apriori-style algorithm to systematically enumerate attribute position configurations with sufficient matching values across pages. Finally, we conduct an extensive experimental study with real-life web data to demonstrate the effectiveness of our extraction approach.	Exploiting content redundancy for web information extraction	NA:NA:NA:NA	2010
Qi Guo:Eugene Agichtein	An improved understanding of the relationship between search intent, result quality, and searcher behavior is crucial for improving the effectiveness of web search. While recent progress in user behavior mining has been largely focused on aggregate server-side click logs, we present a new search behavior model that incorporates fine-grained user interactions with the search results. We show that mining these interactions, such as mouse movements and scrolling, can enable more effective detection of the user's search intent. Potential applications include automatic search evaluation, improving search ranking, result presentation, and search advertising. As a case study, we report results on distinguishing between "research" and "purchase" variants of commercial intent, that show our method to be more effective than the current state-of-the-art.	Exploring searcher interactions for distinguishing types of commercial intent	NA:NA	2010
Hao Han:Junxia Guo:Takehiro Tokuda	In this paper, we present a description-based mashup framework for lightweight integration of Web contents. Our implementation shows that we can integrate not only the Web services but also the Web applications easily, even the Web contents dynamically generated by client-side scripts.	Deep mashup: a description-based framework for lightweight integration of web contents	NA:NA:NA	2010
Michal Holub:Maria Bielikova	Nowadays web portals contain large amount of information that is meant for various visitors or groups of visitors. To effectively navigate within the content the website needs to "know" its users in order to provide personalized content to them. We propose a method for automatic estimation of the user's interest in a web page he visits. This estimation is used for the recommendation of web portal pages (through presenting adaptive links) that the user might like. We conducted series of experiments in the domain of our faculty web portal to evaluate proposed approach.	Estimation of user interest in visited web page	NA:NA	2010
Guichun Hua:Min Zhang:Yiqun Liu:Shaoping Ma:Liyun Ru	Ranking is an essential part of information retrieval(IR) tasks such as Web search. Nowadays there are hundreds of features for ranking. So learning to rank(LTR), an interdisciplinary field of IR and machine learning(ML), has attracted increasing attention. Those features used in the IR are not always independent from each other, hence the feature selection, an important issue in ML, should be paid attention to for LTR. However, the state-of-the-art LTR approaches merely analyze the connection among the features from the aspects of feature selection. In this paper, we propose a hierarchical feature selection strategy containing 2 phases for ranking and learn ranking functions. The experimental results show that ranking functions based on the selected feature subset significantly outperform the ones based on all features.	Hierarchical feature selection for ranking	NA:NA:NA:NA:NA	2010
Hai Huang:Chengfei Liu	This paper focuses on selectivity estimation for SPARQL graph patterns, which is crucial to RDF query optimization. The previous work takes the join uniformity assumption, which would lead to high inaccurate estimation in the cases where properties in SPARQL graph patterns are correlated. We take into account the dependencies among properties in SPARQL graph patterns and propose a more accurate estimation model. We first focus on two common SPARQL graph patterns (star and chain patterns) and propose to use Bayesian network and chain histogram for estimating the selectivityof them. Then, for an arbitrary composite SPARQL graph pattern, we maximally combines the results of the star and chain patterns we have precomputed. The experiments show that our method outperforms existing approaches in accuracy.	Selectivity estimation for SPARQL graph pattern	NA:NA	2010
Won-Seok Hwang:Soo-Min Chae:Sang-Wook Kim:Gyun Woo	In this paper, we propose a new paper ranking algorithm that gives a high rank to papers which is credited by other authoritative papers or published in premier conferences or journals. Also, the proposed algorithm solves a problem that recent papers are rated poorly due to few citations.	Yet another paper ranking algorithm advocating recent publications	NA:NA:NA:NA	2010
Ekaterini Ioannou:Claudia Niederée:Yannis Velegrakis	Selecting and presenting content culled from multiple heterogeneous and physically distributed sources is a challenging task. The exponential growth of the web data in modern times has brought new requirements to such integration systems. Data is not any more produced by content providers alone, but also from regular users through the highly popular Web 2.0 social and semantic web applications. The plethora of the available web content increased its demand by regular users who could not any more wait the development of advanced integration tools. They wanted to be able to build in a short time their own specialized integration applications. Aggregators came to the risk of these users. They allowed them not only to combine distributed content, but also to process it in ways that generate new services available for further consumption. To cope with the heterogeneous data, the Linked Data initiative aims at the creation and exploitation of correspondences across data values. In this work, although we share the Linked Data community vision, we advocate that for the modern web, linking at the data value level is not enough. Aggregators should base their integration tasks on the concept of an entity, i.e., identifying whether different pieces of information correspond to the same real world entity, such as an event or a person. We describe our theory, system, and experimental results that illustrate the approach's effectiveness.	Enabling entity-based aggregators for web 2.0 data	NA:NA:NA	2010
Saral Jain:Stephan Seufert:Srikanta Bedathur	We study how to automatically extract tourist trips from large volumes of geo-tagged photographs. Working with more than 8 million of these photographs that are publicly available via photo- sharing communities such as Flickr and Panoramio, our goal is to satisfy the needs of a tourist who specifies a starting location (typically a hotel) together with a bounded travel distance and demands a tour that visits the popular sites along the way. Our system, named ANTOURAGE, solves this intractable problem using a novel adaptation of the max-min ant system (MMAS) meta-heuristic. Experiments using GPS metadata crawled from Flickr show that ANTOURAGE can generate high-quality tours.	Antourage: mining distance-constrained trips from flickr	NA:NA:NA	2010
Adam Jatowt:Hideki Kawai:Kensuke Kanazawa:Katsumi Tanaka:Kazuo Kunieda:Keiji Yamada	Humans have always desired to guess the future in order to adapt their behavior and maximize chances of success. In this paper, we conduct exploratory analysis of future-related information on the web. We focus on the future-related information which is grounded in time, that is, the information on forthcoming events whose expected occurrence dates are already known. We collect data by crawling search engine index and analyze collective view of future time-referenced events discussed on the web.	Analyzing collective view of future, time-referenced events on the web	NA:NA:NA:NA:NA:NA	2010
Sara Javanmardi:Jianfeng Gao:Kuansan Wang	Although higher order language models (LMs) have shown benefit of capturing word dependencies for Information retrieval(IR), the tuning of the increased number of free parameters remains a formidable engineering challenge. Consequently,in many real world retrieval systems, applying higher order LMs is an exception rather than the rule. In this study, we address the parameter tuning problem using a framework based on a linear ranking model in which different component models are incorporated as features. Using unigram and bigram LMs with 2 stage smoothing as examples, we show that our method leads to a bigram LM that outperforms significantly its unigram counterpart and the well-tuned BM25 model.	Optimizing two stage bigram language models for IR	NA:NA:NA	2010
Vasileios Kandylas:Ali Dasdan	Microblogging as introduced by Twitter is becoming a source of tracking real-time news. Although identifying the highest quality or most useful posts or tweets from Twitter for breaking news is still an open problem, major web search engines seem convinced of the value of such posts and have already started allocating part of their search results pages to them. In this paper, we study a different aspect of the problem for a search engine: instead of the value of the posts, we study the value of the (shortened) URLs referenced in these posts. Our results indicate that unlike frequently bookmarked URLs, which are generally of high quality, frequently tweeted URLs tend to fall in two opposite categories: they are either high in quality, or they are spam. Identifying the quality category of a URL is not trivial, but the combination of characteristics can reveal some trends.	The utility of tweeted URLs for web search	NA:NA	2010
Noriaki Kawamae:Ryuichiro Higashinaka	This paper presents a topic model that detects topic distributions over time. Our proposed model, Trend Detection Model (TDM) introduces a latent trend class variable into each document. The trend class has a probability distribution over topics and a continuous distribution over time. Experiments using our data set show that TDM is useful as a generative model in the analysis of the evolution of trends.	Trend detection model	NA:NA	2010
Julia Kiseleva:Qi Guo:Eugene Agichtein:Daniel Billsus:Wei Chai	We describe preliminary results of experiments with an unsupervised framework for query segmentation, transforming keyword queries into structured queries. The resulting queries can be used to more accurately search product databases, and potentially improve result presentation and query suggestion. The key to developing an accurate and scalable system for this task is to train a query segmentation or attribute detection system over labeled data, which can be acquired automatically from query and click-through logs. The main contribution of our work is a new method to automatically acquire such training data - resulting in significantly higher segmentation performance, compared to previously reported methods.	Unsupervised query segmentation using click data: preliminary results	NA:NA:NA:NA:NA	2010
Weize Kong:Yiqun Liu:Shaoping Ma:Liyun Ru	We consider the problem of detecting epidemic tendency by mining search logs. We propose an algorithm based on click-through information to select epidemic related queries/terms. We adopt linear regression to model epidemic occurrences and frequencies of epidemic related terms (ERTs) in search logs. The results show our algorithm is effective in finding ERTs which obtain a high correlation value with epidemic occurrences. We also find the proposed method performs better when combining different ERTs than using single ERT.	Detecting epidemic tendency by mining search logs	NA:NA:NA:NA	2010
Sai Krishna:Prasad Pingali:Vasudeva Varma	In this paper, we present a two-step language-independent spelling suggestion system. In the first step, candidate suggestions are generated using an Information Retrieval(IR) approach. In step two, candidate suggestions are re-ranked using a new string similarity measure that uses the length of the longest common substrings occurring at the beginning and end of the words. We obtained very impressive results by reranking candidate suggestions using the new similarity measure. The accuracy of first suggestion is 92.3%, 90.0% and 83.5% for Dutch, Danish and Bulgarian language datasets respectively.	An information retrieval approach to spelling suggestion	NA:NA:NA	2010
Changhyun Lee:Haewoon Kwak:Hosung Park:Sue Moon	Twitter offers an explicit mechanism to facilitate information diffusion and has emerged as a new medium for communication. Many approaches to find influentials have been proposed, but they do not consider the temporal order of information adoption. In this work, we propose a novel method to find influentials by considering both the link structure and the temporal order of information adoption in Twitter. Our method finds distinct influentials who are not discovered by other methods.	Finding influentials based on the temporal order of information adoption in twitter	NA:NA:NA:NA	2010
Kyumin Lee:James Caverlee:Steve Webb	We present the conceptual framework of the Social Honeypot Project for uncovering social spammers who target online communities and initial empirical results from Twitter and MySpace. Two of the key components of the Social Honeypot Project are: (1) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers.	The social honeypot project: protecting online communities from spammers	NA:NA:NA	2010
Cheng-Te Li:Hung-Che Lai:Chien-Tung Ho:Chien-Lin Tseng:Shou-De Lin	Micro-blogging services provide platforms for users to share their feelings and ideas on the go. Desiging to produce information stream in almost Micro-blogging services, although are capable of recording rich and diverse senses, still suffer from a drawback of not being able to provide deeper and summarized views. In this paper, we present a novel framework, Pusic, to musicalize micro-blogging messages for terms or users. Pusic can be used to (1) summarize users' messages into certain expression of emotions, (2) explore the emotions and senses and transform them into music, and (3) serve as a presentation of crowd net art. We generate the music from two aspects: emotion and harmony. The former is tackled by emotion detection from messages while the latter is estabilished by rule-based harmonic heuristics according to the detected emotions. Pusic has been announced online for people's experience and further investigation.	Pusic: musicalize microblog messages for summarization and exploration	NA:NA:NA:NA:NA	2010
Zhenhui Li:Ding Zhou:Yun-Fang Juan:Jiawei Han	Today, a huge amount of text is being generated for social purposes on social networking services on the Web. Unlike traditional documents, such text is usually extremely short and tends to be informal. Analysis of such text benefit many applications such as advertising, search, and content filtering. In this work, we study one traditional text mining task on such new form of text, that is extraction of meaningful keywords. We propose several intuitive yet useful features and experiment with various classification models. Evaluation is conducted on Facebook data. Performances of various features and models are reported and compared.	Keyword extraction for social snippets	NA:NA:NA:NA	2010
Cindy Xide Lin:Bo Zhao:Tim Weninger:Jiawei Han:Bing Liu	The World-Wide Web consists not only of a huge number of unstructured texts, but also a vast amount of valuable structured data. Web tables [2] are a typical type of structured information that are pervasive on the web, and Web-scale methods that automatically extract web tables have been studied extensively [1]. Many powerful systems (e.g.OCTOPUS [4], Mesa [3]) use extracted web tables as a fundamental component. In the database vernacular, a table is defined as a set of tuples which have the same attributes. Similarly, a web table is defined as a set of rows (corresponding to database tuples) which have the same column headers (corresponding to database attributes). Therefore, to extract a web table is to extract a relation on the web. In databases, tables often contain foreign keys which refer to other tables. Therefore, it follows that hyperlinks inside a web table sometimes function as foreign keys to other relations whose tuples are contained in the hyperlink's target pages. In this paper, we explore this idea by asking: can we discover new attributes for web tables by exploring hyperlinks inside web tables? This poster proposes a solution that takes a web table as input. Frequent patterns are generated as new candidate relations by following hyperlinks in the web table. The confidence of candidates are evaluated, and trustworthy candidates are selected to become new attributes for the table. Finally, we show the usefulness of our method by performing experiments on a variety of web domains.	Entity relation discovery from web tables and links	NA:NA:NA:NA:NA	2010
Nedim Lipka:Benno Stein	Wikipedia provides an information quality assessment model with criteria for human peer reviewers to identify featured articles. For this classification task "Is an article featured or not?" we present a machine learning approach that exploits an article's character trigram distribution. Our approach differs from existing research in that it aims to writing style rather than evaluating meta features like the edit history. The approach is robust, straightforward to implement, and outperforms existing solutions. We underpin these claims by an experiment design where, among others, the domain transferability is analyzed. The achieved performances in terms of the F-measure for featured articles are 0.964 within a single Wikipedia domain and 0.880 in a domain transfer situation.	Identifying featured articles in wikipedia: writing style matters	NA:NA	2010
Dong Liu:Xian-Sheng Hua:Meng Wang:Hong-Jiang Zhang	The tags on social media websites such as Flickr are frequently imprecise and incomplete, thus there is still a gap between these tags and the actual content of the images. This paper proposes a social image ``retagging'' scheme that aims at assigning images with better content descriptors. The refining process is formulated as an optimization framework based on the consistency between ``visual similarity'' and ``semantic similarity'' in social images. An effective iterative bound optimization algorithm is applied to learn the optimal tag assignment. In addition, as many tags are intrinsically not closely-related to the visual content of the images, we employ a knowledge-based method to differentiate visual content related from unrelated tags and then constrain the tagging vocabulary of our automatic algorithm within the content related tags. Experimental results on a Flickr image collection demonstrate the effectiveness of this approach.	Retagging social images based on visual and semantic consistency	NA:NA:NA:NA	2010
Jiahui Liu:Peter Jin Hong:Elin Rønby Pedersen	In this paper, we present a prototype system that helps users in early-stage web research to create and reestablish context across fragmented work process, without requiring them to explicitly collect and organize the material they visit. The system clusters a user's web history and shows it as research trails. We present two user interaction models with the research trails. The first interaction model is implemented as a standalone application, which presents a hierarchical view of research trails. The second interaction model is integrated with the web browser. It shows the user's research trails as selectable and manipulable visual streams when they open a new tab. Thereby, the NewTab page serves as a springboard in the browser for a user resuming an ongoing task.	Research trails: getting back where you left off	NA:NA:NA	2010
Xiaozhu Liu:Zhiyong Peng	To improve query performance and space efficiency, an efficient random access blocked inverted index (RABI) is proposed. RABI divides an inverted list into blocks and compresses different part of each block with the corresponding encoding method to decrease space consumption. RABI can provide fast addressing and random access functions on the compressed blocked inverted index with the novel hybrid compression method, which can provide both block level and inner block level skipping function and further enhance both space and time efficiencies without inserting any additional auxiliary information. Experimental results show that RABI achieves both high space efficiency and search efficiency, and outperforms the existing approach significantly.	An efficient random access inverted index for information retrieval	NA:NA	2010
Zhang Liu:Chaokun Wang:Jianmin Wang:Wei Zheng:Shengfei Shi	World wide web provides plenty of multimedia resources for creating rich media web applications. However, the collected music and other media resources always mismatch in the metric of time length. Existent music resizing approaches suffer from perceptual artifacts which degrade the performance of resized music. In this paper, a novel structure-aware music resizing approach is proposed. Through lyrics analysis, our approach can compress different parts of a music piece in variant compression rates. Experimental results show that the proposed method can effectively generate resized songs with good quality.	Structure-aware music resizing using lyrics	NA:NA:NA:NA:NA	2010
Alexandros Marinos:Erik Wilde:Jiannan Lu	Relational databases hold a vast quantity of information and making them accessible to the web is an big challenge. There is a need to make these databases accessible with as little difficulty as possible, opening them up to the power and serendipity of the Web. Our work presents a series of patterns that bridge the relational database model with the architecture of the Web along with an implementation of some of them. The aim is for relational databases to be made accessible with no intermediate steps and no extra metadata required. This approach can vastly increase the data available on the web, therefore making the Web itself all the more powerful, while enabling its users to seamlessly perform tasks that previously required bridging multiple domains and paradigms or were not possible.	HTTP database connector (HDBC): RESTful access to relational databases	NA:NA:NA	2010
Tsuyoshi Murata	Online social media such as delicious and digg are represented as tripartite networks whose vertices are users, tags, and resources. Detecting communities from such tripartite networks is practically important. Modularity is often used as the criteria for evaluating the goodness of network divisions into communities. For tripartite networks, Neubauer defines a tripartite modularity which extends Murata's bipartite modularity. However, Neubauer's tripartite modularity still uses projections and it will lose information that original tripartite networks have. This paper proposes new tripartite modularity for tripartite networks that do not use projections. Experimental results show that better community structures can be detected by optimizing our tripartite modularity.	Detecting communities from tripartite networks	NA	2010
Jose R. Perez-Aguera:Javier Arroyo:Jane Greenberg:Joaquin Perez-Iglesias:Victor Fresno	This paper presents a new collection based on DBpedia and INEX for evaluating semantic search performance. The proposed corpus is used to calculate the impact of considering document's structure on the retrieval performance of the Lucene and BM25 ranking functions. Results show that BM25 outperforms Lucene in all the considered metrics and that there is room for future improvements, which may be obtained using a hybrid approach combining both semantic technology and information retrieval ranking functions.	INEX+DBPEDIA: a corpus for semantic search evaluation	NA:NA:NA:NA:NA	2010
Tan Phan:Jun han:Garth Heward:Steve Versteeg	Current Web Service security standards have inadequate support for end-to-end protection of data when some receivers of the data are unknown to the sender. This paper presents an approach to aid collaborative partner services in properly protecting each other's data. Our approach allows each partner to derive an adequate protection mechanism with minimum performance overhead for each message it sends based on those of the corresponding messages it receives. We modify the message handling mechanisms of Web Service engines to dynamically gather protection requirements for a given outgoing message by aggregating requirements from original owners of message data.	Protecting data in multi-stakeholder web service system	NA:NA:NA:NA	2010
Anon Plangprasopchok:Kristina Lerman:Lise Getoor	Aggregating many personal hierarchies into a common taxonomy, also known as a folksonomy, presents several challenges due to its sparseness, ambiguity, noise, and inconsistency. We describe an approach to folksonomy learning based on relational clustering that addresses these challenges by exploiting structured metadata contained in personal hierarchies. Our approach clusters similar hierarchies using their structure and tag statistics, then incrementally weaves them into a deeper, bushier tree. We study folksonomy learning using social metadata extracted from the photo-sharing site Flickr. We evaluate the learned folksonomy quantitatively by automatically comparing it to a reference taxonomy created by the Open Directory Project. Our empirical results suggest that the proposed approach improves upon the state-of-the-art folksonomy learning method.	Constructing folksonomies by integrating structured metadata	NA:NA:NA	2010
Ana Maria Popescu:Patrick Pantel:Gilad Mishne	We describe improvements to the use of semantic lexicons by a state-of-the-art query interpretation system powering a major search engine. We successfully compute concept label importance information for lexicon strings; lexicon augmentation with such information leads to a 6.4% precision increase on affected queries with no query coverage loss. Finally, lexicon filtering based on label importance leads to a 13% precision increase, but at the expense of query coverage.	Semantic lexicon adaptation for use in query interpretation	NA:NA:NA	2010
Martin Potthast:Benno Stein:Steffen Becker	This paper investigates whether Web comments can be exploited for cross-media retrieval. Comparing Web items such as texts, images, videos, music, products, or personal profiles can be done at various levels of detail; our focus is on topic similarity. We propose to compare user-supplied comments on Web items in lieu of the commented items themselves. If this approach is feasible, the task of extracting and mapping features between arbitrary pairs of item types can be circumvented, and well-known text retrieval models can be applied instead - given that comments are available. We report on results of a preliminary, but nonetheless large-scale experiment which shows that, if comments on textual items are compared with comments on video items, topically similar pairs achieve a sufficiently high cross-media similarity.	Towards comment-based cross-media retrieval	NA:NA:NA	2010
Filip Radlinski:Martin Szummer:Nick Craswell	Many researchers have noted that web search queries are often ambiguous or unclear. We present an approach for identifying the popular meanings of queries using web search logs and user click behavior. We show our approach to produce more complete and user-centric intents than expert judges by evaluating on TREC queries. This approach was also used by the TREC 2009 Web Track judges to obtain more representative topic descriptions from real queries.	Inferring query intent from reformulations and clicks	NA:NA:NA	2010
Benjamin Rey:Ashvin Kannan	Advertisers use Sponsored Search to drive traffic to their site at a conversion rate and cost per conversion that provides value to them. However, very often advertisers bid at a constant price on a bundle of keywords, either for lack of enough data to fully optimize their bids at a keyword level, or indirectly by opting into Advanced Matching (AM) that allows an advertiser to reach a large number of queries while explicitly bidding only on a limited number. Then this single bid price reflects the return the advertiser gets from the full bundle. Under these conditions, the advertiser is competing too aggressively for some keyword auctions and with too low bids for others. In this paper, we propose a solution to improve the fairness of each keyword's bid prices within an AM bundle: adjusting the AM keyword bid by the ratio of its conversion rate to the conversion rate it would have reached had it been an Exact Match (EM). First we describe how we measure advertisers' conversion rates despite the opt-in nature of conversion tracking, and illustrate the need for bid adjustment in the context of AM. Then we present our approach to predict conversion rates in a robust manner. Our model uses a number of features capturing the quality of the match between the ad and the query. Then we describe how we adjust keyword bid prices to reflect their value to the advertiser thereby improving (1) the auction through fewer incorrectly high bids in the auction, (2) advertiser return through more auctions won by high value keywords and less by low value keywords, and (3) user satisfaction through higher conversion rate. Finally, we present experimental results from our live system.	Conversion rate based bid adjustment for sponsored search	NA:NA	2010
Sven Rizzotti:Helmar Burkhart	A lot of situations from daily life have found their counterpart in the internet. Buying or selling goods, discovering information or making social connections have been very successful in the virtual world and make the Web the central working place. Until now the purpose of a Web site has been defined at the time of creation. Extension of the available functionality or transfer of the functionality onto another site has only been possible with considerable additional effort. With useKit, we present a software platform that allows users to add individual selected functionalities to any Web site without installing software. useKit offers a new approach towards a collaborative, personalized and individually compiled view of the Web. useKit focuses on personalized applications and services that can be applied to any Web site. In analogy to file system permissions, this can be seen as an instance of the" "executable Web"" or Web 3.0 if Web 1.0 was ""read-only"", and Web 2.0 was "read-write"". User contributed code will morph online applications into omnipresent functional platforms with a single interface.	useKit: a step towards the executable web 3.0	NA:NA	2010
D. Sculley	We present two modifications to the popular k-means clustering algorithm to address the extreme requirements for latency, scalability, and sparsity encountered in user-facing web applications. First, we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent. Second, we achieve sparsity with projected gradient descent, and give a fast ε-accurate projection onto the L1-ball. Source code is freely available: http://code.google.com/p/sofia-ml	Web-scale k-means clustering	NA	2010
Mohamed Shehab:Gorrell Cheek:Hakim Touati:Anna C. Squicciarini:Pau-Chen Cheng	Online social networking sites are experiencing tremendous user growth with hundreds of millions of active users. As a result, there is a tremendous amount of user profile data online, e.g., name, birthdate, etc. Protecting this data is a challenge. The task of access policy composition is a tedious and confusing effort for the average user having hundreds of friends. We propose an approach that assists users in composing and managing their access control policies. Our approach is based on a supervised learning mechanism that leverages user provided example policy settings as training sets to build classifiers that are the basis for auto-generated policies. Furthermore, we provide mechanisms to enable users to fuse policy decisions that are provided by their friends or others in the social network. These policies then regulate access to user profile objects. We implemented our approach and, through extensive experimentation, prove the accuracy of our proposed mechanisms.	Learning based access control in online social networks	NA:NA:NA:NA:NA	2010
Vivek K. Singh:Mingyan Gao:Ramesh Jain	Large volumes of spatio-temporal-thematic data being created using sites like Twitter and Jaiku, can potentially be combined to detect events, and understand various 'situations' as they are evolving at different spatio-temporal granularity across the world. Taking inspiration from traditional image pixels which represent aggregation of photon energies at a location, we consider aggregation of user interest levels at different geo-locations as social pixels. Combining such pixels spatio-temporally allows for creation of social images and video. Here, we describe how the use of relevant (media processing inspired) situation detection operators upon such 'images', and domain based rules can be used to decide relevant control actions. The ideas are showcased using a Swine flu monitoring application which uses Twitter data.	Situation detection and control using spatio-temporal analysis of microblogs	NA:NA:NA	2010
Vivek K. Singh:Ramesh Jain	Events are the fundamental abstractions to study the dynamic world. We believe that the next generation of web (i.e. event-web), will focus on interconnections between events as they occur across space and time [3]. In fact we argue that the real value of large volumes of microblog data being created daily lies in its inherent spatio-temporality, and its correlation with the real-world events. In this context, we studied the structural properties of a corpus of 5,835,237 Twitter microblogs, and found it to exhibit Power laws across space and time, much like those exhibited by events in multiple domains. The properties studied over microblogs on different topics can be applied to study relationships between related events, as well as data organization for event-based, real-time, and location-aware applications.	Structural analysis of the emerging event-web	NA:NA	2010
Adish Singla:Ingmar Weber	We consider the problem of optimal tagging for navigational purposes in one's own collection. What is the best that a forgetful user can hope for in terms of ease of retrieving a labeled object? We prove that the number of tags has to increase logarithmically in the collection size to maintain a manageable result set. Using Flickr data we then show that users do indeed apply more and more tags as their collection grows and that this is not due to a global increase in tagging activity. However, as the additional terms applied are not statistically independent, users of large collections still have to deal with larger and larger result sets, even when more tags are used as search terms. We pose optimal tag suggestion for navigational purposes as an open problem.	Tagging and navigability	NA:NA	2010
Adish Singla:Ryen W. White	Click data captures many users' document preferences for a query and has been shown to help significantly improve search engine ranking. However, most click data is noisy and of low frequency, with queries associated to documents via only one or a few clicks. This severely limits the usefulness of click data as a ranking signal. Given potentially noisy clicks comprising results with at most one click for a query, how do we extract high-quality clicks that may be useful for ranking? In this poster, we introduce a technique based on query entropy for noise reduction in click data. We study the effect of query entropy and as well as features such as user engagement and the match between the query and the document. Based on query entropy plus other features, we can sample noisy data to 15% of its overall size with 43% query recall and an average increase of 20% in precision for recalled queries.	Sampling high-quality clicks from noisy click data	NA:NA	2010
Yang Sun:C. Lee Giles	In this research, capture-recapture (CR) models are used to estimate the population of web robots based on web server access logs from different websites. Each robot is considered as an individual randomly surfing the web and each website is considered as a trap that records the visitation of robots. We use maximum likelihood estimator to fit the observation data. Results show that there are 3,860 identifiable robot User-Agent strings and 780,760 IP addresses being used by web robots around the world. We also examine the origination of the named robots by their IP addresses. The results suggest that over 50% of web robot IP addresses are from United States and China.	Estimating the web robot population	NA:NA	2010
Yusuke Takamatsu:Yuji Kosuga:Kenji Kono	Session fixation is a technique for obtaining the visitor's session identifier (SID) by forcing the visitor to use the SID supplied by the attacker. The attacker who obtains the victim's SID can masquerade as the visitor. In this paper, we propose a technique to automatically detect session fixation vulnerabilities in web applications. Our technique uses attack simulator that executes a real session fixation attack and check whether it is successful or not. In the experiment, our system successfully detected vulnerabilities in our original test cases and in a real world web application.	Automated detection of session fixation vulnerabilities	NA:NA:NA	2010
Arash Termehchy:Marianne Winslett	Key-value stores (KVSs) are the most prevalent storage systems for large scale web services. As they do not have the structural complexities of RDBMSs, they are more efficient. In this paper, we introduce the problem of keyword search over KVSs and analyze its differences with keyword search over documents and relational databases. We propose a novel method called Keyword Search over Key-value stores (KSTORE) to solve the problem. Our user study using two real life data sets shows that KSTORE provides better ranking than the extended versions of document and relational database keyword search proposals and poses reasonable overheads.	Keyword search over key-value stores	NA:NA	2010
Mikalai Tsytsarau:Themis Palpanas:Kerstin Denecke	Our study addresses the problem of large-scale contradiction detection and management, from data extracted from the Web. We describe the first systematic solution to the problem, based on a novel statistical measure for contradictions, which exploits first- and second-order moments of sentiments. Our approach enables the interactive analysis and online identification of contradictions under multiple levels of time granularity. The proposed algorithm can be used to analyze and track opinion evolution over time and to identify interesting trends and patterns. It uses an incrementally updatable data structure to achieve computational efficiency and scalability. Experiments with real datasets show promising time performance and accuracy.	Scalable discovery of contradictions on the web	NA:NA:NA	2010
Xiaojun Wan:Jianwu Yang	This poster briefly describes a practical system named FounderWISE for harvesting and monitoring hot topics on the Web. FounderWISE consists of five components: Web crawler, text classifier, topic detector, topic summarizer and topic analyzer. In this poster we present two key components of topic detector and topic analyzer. The system has been successfully deployed in a few Chinese major government departments.	A practical system for harvesting and monitoring hot topics on the web	NA:NA	2010
Dong Wang:Chenguang Zhu:Weizhu Chen:Gang Wang:Zheng Chen	Several relevance metrics, such as NDCG, precision and pSkip, are proposed to measure search relevance, where different metrics try to characterize search relevance from different perspectives. Yet we empirically find that the direct optimization of one metric cannot always achieve the optimal ranking of another metric. In this paper, we propose two novel relevance optimization approaches, which take different metrics into a global consideration where the objective is to achieve an ideal tradeoff between different metrics. To achieve this objective, we propose to co-optimize multiple relevance metrics and show their effectiveness.	Co-optimization of multiple relevance metrics in web search	NA:NA:NA:NA:NA	2010
Yong Wang:Daniel Burgener:Aleksandar Kuzmanovic:Gabriel Maciá-Fernández	Advertising has become an integral and inseparable part of the World Wide Web. However, neither public auditing nor monitoring mechanisms still exist in this emerging area. In this paper, we present our initial efforts on building a content-level auditing service for web-based ad networks. Our content-level measurements - understanding the ad distribution mechanisms and evaluating location-based and behavioral targeting approaches - bring useful auditing information to all entities involved in the online advertising business. We extensively evaluate Google's, AOL's, and Adblade's ad networks and demonstrate how their different design philosophies dominantly affect their performance at the content level.	Analyzing content-level properties of the web adversphere	NA:NA:NA:NA	2010
Zhen Wen:Ching-Yung Lin	Search and recommendation systems must effectively model user interests in order to provide personalized results. The proliferation of social software makes social network an increasingly important source for user interest modeling, because of the social influence and correlation among friends. However, there are large variations in people's contribution of social content. Therefore, it is impractical to accurately model interests for all users. As a result, applications need to decide whether to utilize a user interest model based on its accuracy. To address this challenge, we present a study on the accuracy of user interests inferred from three types of social content: social bookmarking, file sharing, and electronic communication, in an organizational social network within a large-scale enterprise. First, we demonstrate that combining different types of social content to infer user interests outperforms methods that use only one type of social content. Second, we present a technique to predict the inference accuracy based on easily observed network characteristics, including user activeness, network in-degree, out-degree, and betweenness centrality.	How accurately can one's interests be inferred from friends	NA:NA	2010
Ou Wu:Yunfei Chen:Bing Li:Weiming Hu	A beautiful and well-laid out Web page greatly facilitates users' accessing and enhances browsing experiences. We use "visual quality (VQ)" to denote the aesthetics of Web pages. In this paper, a computational aesthetics approach is proposed to learn the evaluation model for the visual quality of Web pages. First, a Web page layout extraction algorithm (V-LBE) is introduced to partition a Web page into major layout blocks. Then, regarding a Web page as a semi-structured image, features known to significantly affect the visual quality of a Web page are extracted to construct a feature vector. The experimental results show the initial success of our approach. Potential applications include Web search and Web design.	Learning to evaluate the visual quality of web pages	NA:NA:NA:NA	2010
Qianli Xing:Yiqun Liu:Rongwei Cen:Min Zhang:Shaoping Ma:Liyun Ru	In this paper, we study on the reliability of search engine users using click-through data. We proposed a graph-based approach to evaluate user reliability according to how users click on search result lists. We tried to incorporate this measure of reliability into relevance feedback for improving ranking performances. Experimental results indicate that the proposed approach is both effective and applicable.	Are search engine users equally reliable?	NA:NA:NA:NA:NA:NA	2010
Takehiro Yamamoto:Satoshi Nakamura:Katsumi Tanaka	This paper proposes a system called RerankEverything, which enables users to rerank search results in any search service, such as a Web search engine, an e-commerce site, a hotel reservation site and so on. In conventional search services, interactions between users and services are quite limited and complicated. In addition, search functions and interactions to refine search results differ depending on the services. By using RerankEverything, users can interactively explore search results in accordance with their interests by reranking search results from various viewpoints.	RerankEverything: a reranking interface for browsing search results	NA:NA:NA	2010
Zhijun Yin:Manish Gupta:Tim Weninger:Jiawei Han	With the phenomenal success of networking sites (e.g., Facebook, Twitter and LinkedIn), social networks have drawn substantial attention. On online social networking sites, link recommendation is a critical task that not only helps improve user experience but also plays an essential role in network growth. In this paper we propose several link recommendation criteria, based on both user attributes and graph structure. To discover the candidates that satisfy these criteria, link relevance is estimated using a random walk algorithm on an augmented social graph with both attribute and structure information. The global and local influence of the attributes is leveraged in the framework as well. Besides link recommendation, our framework can also rank attributes in a social network. Experiments on DBLP and IMDB data sets demonstrate that our method outperforms state-of-the-art methods based on network structure and node attribute information for link recommendation.	LINKREC: a unified framework for link recommendation with user attributes and graph structure	NA:NA:NA:NA	2010
Seok-Ho Yoon:Sang-Wook Kim:Sunju Park	In this paper, we propose a new approach to measure similarities among academic papers based on their references. Our similarity measure uses both in-link and out-link by transforming in-link and out-link into undirected links.	A link-based similarity measure for scientific literature	NA:NA:NA	2010
Jie Yu:Xin Jin:Jiawei Han:Jiebo Luo	Photo-sharing services have attracted millions of people and helped construct massive social networks on the Web. A popular trend is that users share their image collections within social groups, which greatly promotes the interactions between users and expands their social networks. Existing systems have difficulties in generating satisfactory social group suggestions because the images are classified independently and their relationship in a collection is ignored. In this work, we intend to produce suggestions of suitable photo-sharing groups from a user's personal photo collection by mining images on the Web and leveraging the collection context. Both visual content and textual annotations are integrated to generate initial prediction of the events or topics depicted in the images. A user collection-based label propagation method is proposed to improve the group suggestion by modeling the relationship of images in the same collection as a sparse weighted graph. Experiments on real user images and comparisons with the state-of-the-art techniques demonstrate the effectiveness of the proposed approaches.	Social group suggestion from user image collections	NA:NA:NA:NA	2010
Xiaohui Yu:Yang Liu:Xiangji Huang:Aijun An	Writing and publishing reviews online has become an increasingly popular way for people to express opinions and sentiments. Analyzing the large volume of online reviews available can produce useful knowledge that are of interest to vendors and other parties. Prior studies in the literature have shown that online reviews have a significant correlation with the sales of products, and therefore mining the reviews could help predict the sales performance of relevant products. However, those studies fail to consider one important factor that may significantly affect the accuracy of the prediction, i.e., the quality of the reviews. In this paper, we propose a regression model that explicitly takes into account the quality factor, and discusses how this quality information can be predicted when it is not readily available. Experimental results on a movie review dataset confirm the effectiveness of the proposed model.	A quality-aware model for sales prediction using reviews	NA:NA:NA:NA	2010
Richong Zhang:Thomas Tran	Automatically assessing the quality and helpfulness of consumer reviews is more and more desirable with the evolutionary development of online review systems. Existing helpfulness assessment methodologies make use of the positive vote fraction as a benchmark and heuristically find a "best guess" to estimate the helpfulness of review documents. This benchmarking methodology ignores the voter population size and treats the the same positive vote fraction as the same helpfulness value. We propose a review recommendation approach that make use of the probability density of the review helpfulness as the benchmark and exploit graphical model and Expectation Maximization (EM) algorithm for the inference of review helpfulness. The experimental results demonstrate that the proposed approach is superior to existing approaches.	Review recommendation with graphical model and EM algorithm	NA:NA	2010
Yaqian Zhou:Mengjing Jiang:Qi Zhang:Xuanjing Huang:Lide Wu	In this paper we propose a novel recrawling method based on navigation patterns called Selective Recrawling. The goal of selective recrawling is to automatically select page collections that have large coverage and little redundancy to a pre-defined vertical domain. It only requires several seed objects and can select a set of URL patterns to cover most objects. The selected set can be used to recrawl the web pages for quite a period of time and renewed periodically. Experiments on local event data show that our method can greatly reduce the downloading of web pages while keep the comparative object coverage.	Selective recrawling for object-level vertical search	NA:NA:NA:NA:NA	2010
Jia Zhu:Gabriel Fung:Xiaofang Zhou	Entity resolution (ER) is a problem that arises in many areas. In most of cases, it represents a task that multiple entities from different sources require to be identified if they refer to the same or different objects because there are not unique identifiers associated with them. In this paper, we propose a model using web pages identification to identify entities and merge those entities refer to one object together. We use a classical name disambiguation problem as case study and examine our model on a subset of digital library records as the first stage of our work. The favorable results indicated that our proposed approach is highly effective.	Efficient web pages identification for entity resolution	NA:NA:NA	2010
Yun Zhu:Li Xiong:Christopher Verdery	We study the problem of anonymizing user profiles so that user privacy is sufficiently protected while the anonymized profiles are still effective in enabling personalized web search. We propose a Bayes-optimal privacy notion to bound the prior and posterior probability of associating a user with an individual term in the anonymized user profile set. We also propose a novel bundling technique that clusters user profiles into groups by taking into account the semantic relationships between the terms while satisfying the privacy constraint. We evaluate our approach through a set of preliminary experiments using real data demonstrating its feasibility and effectiveness.	Anonymizing user profiles for personalized web search	NA:NA:NA	2010
Haiqiang Zuo:Weiming Hu:Ou Wu	Along with the explosive growth of the World Wide Web, an immense industry for the production and consumption of pornography has grown. Though the censorship and legal restraints on pornography are discriminating in different historical, cultural and national contexts, selling pornography to minors is not allowed in most cases. Detecting human skin tone is of utmost importance in pornography image filtering algorithms. In this paper, we propose two patch-based skin color detection algorithms: regular patch and irregular patch skin color detection algorithms. On the basis of skin detection, we extract 31-dimensional features from the input image, and these features are fed into a random forest classifier. Our algorithm has been incorporated into an adult-content filtering infrastructure, and is now in active use for preventing minors from accessing pornographic images via mobile phones.	Patch-based skin color detection and its application to pornography image filtering	NA:NA:NA	2010
Mikhail Bautin:Charles B. Ward:Akshay Patil:Steven S. Skiena	The social sciences strive to understand the political, social, and cultural world around us, but have been impaired by limited access to the quantitative data sources enjoyed by the hard sciences. Careful analysis of Web document streams holds enormous potential to solve longstanding problems in a variety of social science disciplines through massive data analysis. This paper introduces the TextMap Access system, which provides ready access to a wealth of interesting statistics on millions of people, places, and things across a number of interesting web corpora. Powered by a flexible and scalable distributed statistics computation framework using Hadoop, continually updated corpora include newspapers, blogs, patent records, legal documents, and scientific abstracts; well over a terabyte of raw text and growing daily. The Lydia Textmap Access system, available through http://www.textmap.com/access, provides instant access for students and scholars through a convenient web user-interface. We describe the architecture of the TextMap Access system, and its impact on current research in political science, sociology, and business/marketing.	Access: news and blog analysis for the social sciences	NA:NA:NA:NA	2010
Yevgen Borodin:Faisal Ahmed:Muhammad Asiful Islam:Yury Puzis:Valentyn Melnyk:Song Feng:I. V. Ramakrishnan:Glenn Dausch	This demo will present HearSay, a multi-modal non-visual web browser, which aims to bridge the growing Web Accessibility divide between individuals with visual impairments and their sighted counterparts, and to facilitate full participation of blind individuals in the growing Web-based society.	Hearsay: a new generation context-driven multi-modal assistive web browser	NA:NA:NA:NA:NA:NA:NA:NA	2010
Daniel Coffman:Danny Soroker:Chandra Narayanaswami:Aaron Zinman	As sophisticated enterprise applications move to the Web, some advanced user experiences become difficult to migrate due to prohibitively high computation, memory, and bandwidth requirements. State-dependent visualizations of large-scale data sets are particularly difficult since a change in the client's context necessitates a change in the displayed results. This paper describes a Web architecture where clients are served a session-specific image of the data, with this image divided into tiles dynamically generated by the server. This set of tiles is supplemented with a corpus of metadata describing the immediate vicinity of interest; additional metadata is delivered as needed in a progressive fashion in support and anticipation of the user's actions. We discuss how the design of this architecture was motivated by the goal of delivering a highly responsive user experience. As an example of a complete application built upon this architecture, we present OrgMaps, an interactive system for navigating hierarchical data, enabling fluid, low-latency navigation of trees of hundreds of thousands of nodes on standard Web browsers using only HTML and JavaScript.	A client-server architecture for state-dependent dynamic visualizations on the web	NA:NA:NA:NA	2010
Jean-Yves Delort	This paper presents a technique for visualizing large spatial data sets in Web Mapping Systems (WMS). The technique creates a hierarchical clustering tree, which is subsequently used to extract clusters that can be displayed at a given scale without cluttering the map. Voronoi polygons are used as aggregation symbols to represent the clusters. This technique retains hierarchical relationships between data items at different scales. In addition, aggregation symbols do not overlap, and their sizes and the number of points that they cover is controlled by the same parameter. A prototype has been implemented and tested showing the effectiveness of the method for visualizing large data sets in WMS.	Hierarchical cluster visualization in web mapping systems	NA	2010
Byron J. Gao:Joey Jan	With a Wiki-like search interface, users can edit ranks of search results and share the edits with the rest of the world. This is an effective way of personalization, as well as a practice of mass collaboration that allows users to vote for ranking and improve search performance. Currently, there are several ongoing experimentation efforts from the industry, e.g., SearchWiki by Google and U Rank by Microsoft. Beyond that, there is little published research on this new search paradigm. In this paper, we make an effort to establish a framework for rank editing and sharing in the context of web search, where we identify fundamental issues and propose principled solutions. Comparing to existing systems, for rank editing, our framework allows users to specify not only relative, but also absolute preferences. For edit sharing, our framework provides enhanced flexibility, allowing users to select arbitrarily aggregated views. In addition, edits can be shared among similar queries. We present a prototype system Rants, that implements the framework and provides search services through the Google web search API.	Rants: a framework for rank editing and sharing in web search	NA:NA	2010
Ullas Gargi:Rich Gossweiler	As traditional media and information devices integrate with the web, they must suddenly support a vastly larger database of relevant items. Many devices use remote controls with on-screen keyboards which are not well suited for text entry but are difficult to displace. We introduce a text entry method which significantly improves text entry speed for on-screen keyboards using the same simple Up/Down/Left/Right/Enter interface common to remote controls and gaming devices used to enter text. The paper describes QuickSuggest's novel adaptive user interface, demonstrates quantitative improvements from simulation results on millions of user queries and shows ease of use and efficiency with no learning curve in user experiments.	QuickSuggest: character prediction on web appliances	NA:NA	2010
Andreas Hartl:Klara Weiand:François Bry	KiWi is a semantic Wiki that combines the Wiki philosophy of collaborative content creation with the methods of the Semantic Web in order to enable effective knowledge management. Querying a Wiki must be simple enough for beginning users, yet powerful enough to accommodate experienced users. To this end, the keyword-based KiWi query language (KWQL) supports queries ranging from simple lists of keywords to expressive rules for selecting and reshaping Wiki (meta-)data. In this demo, we showcase visKWQL, a visual interface for the KWQL language aimed at supporting users in the query construction process. visKWQL and its editor are described, and their functionality is illustrated using example queries. visKWQL's editor provides guidance throughout the query construction process through hints, warnings and highlighting of syntactic errors. The editor enables round-tripping between the twin languages KWQL and visKWQL, meaning that users can switch freely between the textual and visual form when constructing or editing a query. It is implemented using HTML, JavaScript, and CSS, and can thus be used in (almost) any web browser without any additional software.	visKQWL, a visual renderer for a semantic web query language	NA:NA:NA	2010
Lili Jiang:Wei Shen:Jianyong Wang:Ning An	Name ambiguity is a big challenge in people information retrieval and has received considerable attention, especially with the increasing volume of Web data in recent years. In this demo, we present a system, GRAPE, which is capable of finding people related information over the Web. The salient features of our system are people name disambiguation and people tag presentation, which effectively distinguish different people entities sharing the same name and uniquely represent each namesake with a cluster of tags, such as occupation, birthdate, and organization.	GRAPE: a system for disambiguating and tagging people names in web search	NA:NA:NA:NA	2010
Xin Jin:Jiebo Luo:Jie Yu:Gang Wang:Dhiraj Joshi:Jiawei Han	In this demo, we present a system called iRIN designed for performing image retrieval in image-rich information networks. We first introduce MoK-SimRank to significantly improve the speed of SimRank, one of the most popular algorithms for computing node similarity in information networks. Next, we propose an algorithm called SimLearn to (1) extend MoK-SimRank to heterogeneous image-rich information network, and (2) account for both link-based and content-based similarities by seamlessly integrating reinforcement learning with feature learning.	iRIN: image retrieval in image-rich information networks	NA:NA:NA:NA:NA:NA	2010
Timothy Jones:David Hawking:Ramesh Sankaranarayana	There are significant barriers to academic research into user Web search preferences. Academic researchers are unable to manipulate the results shown by a major search engine to users and would have no access to the interaction data collected by the engine. Our initial approach to overcoming this was to ask participants to submit queries to an experimental search engine rather than their usual search tool. Over several different experiments we found that initial user buy-in was high but that people quickly drifted back to their old habits and stopped contributing data. Here, we report our investigation of possible reasons why this occurs. An alternative approach is exemplified by the Lemur browser toolbar, which allows local collection of user interaction data from search engine sessions, but does not allow result pages to be modified. We will demonstrate a new Firefox toolbar that we have developed to support experiments in which search results may be arbitrarily manipulated. Using our toolbar, academics can set up the experiments they want to conduct, while collecting (subject to human experimentation guidelines) queries, clicks and dwell times as well as optional explicit judgments.	Live web search experiments for the rest of us	NA:NA:NA	2010
Nabil Layaida:Pierre Geneves	We present a tool for helping XML schema designers to obtain a high quality level for their specifications. The tool allows one to analyze relations between classes of XML documents and formally prove them. For instance, the tool can be used to check forward and backward compatibilities of recommendations. When such a relation does not hold, the tool allows one to identify the reasons and reports detailed counter-examples that exemplify the problem. For this purpose, the tool relies on recent advances in logic-based automated theorem proving techniques that allow for efficient reasoning on very large sets of XML documents. We believe this tool can be of great value for standardization bodies that define specifications using various XML type definition languages (such as W3C specifications), and are concerned with quality assurance for their normative recommendations.	Debugging standard document formats	NA:NA	2010
Lusong Li:Tao Mei:Xiang Niu:Chong-Wah Ngo	This paper presents an innovative style-wise advertising platform for web page. Web page "style" mainly refers to visual effects, such as color and layout. Unlike the most popular ad-network such as Google AdSense which needs publishers to change the original structure of their pages and define the position and style of the embedded ads manually, style-wise page advertising aims to automatically deliver style-consistent ads at proper positions within the web page, without breaking the layout of the original page. Our system is motivated from the fact that almost 90% web pages contain blank regions without any content. Given a web page with some blank regions, style-wise page advertising is able to detect the suitable blank areas for advertising, rank the ads according to the semantic relevance and web page style, and embed relevant ads into these nonintrusive areas. Style-wise page advertising represents one of the first attempts towards contextual advertising which enables the publishers to save effort when applying online advertising services.	PageSense: style-wise web page advertising	NA:NA:NA:NA	2010
Tomas Linden:Tommi Heikkinen:Timo Ojala:Hannu Kukka:Marko Jurmu	In this paper we present a web-based framework for spatiotemporal screen real estate management of interactive public displays. The framework facilitates dynamic partitioning of the screen real estate into virtual screens assigned for multiple concurrent web applications. The framework is utilized in the implementation of so-called UBI-hotspot, which provides various information services via different interaction modalities including mobile. The framework facilitates seamless integration of third party web applications residing anywhere in the public Internet into the UBI-hotspot, thus catering for a scalable and open architecture. We report the deployment of a network of indoor and outdoor UBI-hotspots at downtown Oulu, Finland. The quantitative data on the usage of the UBI-hotspots implicitly speaks in favor of the practical applicability of the framework.	Web-based framework for spatiotemporal screen real estate management of interactive public displays	NA:NA:NA:NA:NA	2010
Giulio Mori:Maria Claudia Buzzi:Marina Buzzi:Barbara Leporini	Audio podcasting is increasingly present in the educational field and is especially appreciated as an ubiquitous/pervasive tool ("anywhere, anytime, at any pace") for acquiring or expanding knowledge. We designed and implemented a Web-based Text To Speech (TTS) system for automatic generation of a set of structured audio podcasts from a single text document. The system receives a document in input (doc, rtf, or txt), and in output provides a set of audio files that reflect the document's internal structure (one mp3 file for each document section), ready to be downloaded on portable mp3 players. Structured audio files are useful for everyone but are especially appreciated by blind users, who must explore content audially. Fully accessible for the blind, our system offers WAI-ARIA-based Web interfaces for easy navigation and interaction via screen reader and voice synthesizer, and produces a set of accessible audio files for Rockbox mp3 players (mp3 and talk files), allowing blind users to also listen to naturally spoken file names (instead of their spelled-out strings). In this demo, we will show how the system works when a user interacts via screen reader and voice synthesizer, showing the interaction with both our Web-based system and with an mp3 player.	Structured audio podcasts via web text-to-speech system	NA:NA:NA:NA	2010
Richard Reid:Edoardo Pignotti:Peter Edwards:Adele Laing	Web-based Virtual Research Environments (VREs) have been proposed as one way in which e-Science tools can be deployed to support and enhance the research process. We are exploring the use of Linked Data in combination with the Open Provenance Model (OPM) and Social Web concepts to facilitate interactions between people and data in the context of a VRE. In this demo we present the ourSpaces VRE and outline the technologies used to link together provenance, research artefacts, projects, geographical locations and social data in the context of interdisciplinary research.	ourSpaces: linking provenance and social data in a virtual research environment	NA:NA:NA:NA	2010
Yuheng Ren:Mo Yu:Xin-Jing Wang:Lei Zhang:Wei-Ying Ma	In this paper, we demonstrate a novel landmark photo search and browsing system: Agate, which ranks landmark image search results considering their relevance, diversity and quality. Agate learns from community photos the most interested aspects and related activities of a landmark, and generates adaptively a Table of Content (TOC) as a summary of the attractions to facilitate the user browsing. Image search results are thus re-ranked with the TOC so as to ensure a quick overview of the attractions of the landmarks. A novel non-parametric TOC generation and set-based ranking algorithm, MoM-DPM Sets, is proposed as the key technology of Agate. Experimental results based on human evaluation show the effectiveness of our model and users' preference for Agate.	Diversifying landmark image search results by learning interested views from community photos	NA:NA:NA:NA:NA	2010
Tom Rowlands:David Hawking:Ramesh Sankaranarayana	Web search engines discover indexable documents by recursively 'crawling' from a seed URL. Their rankings take into account link popularity. While this works well, it introduces biases towards older documents. Older documents are more likely to be the target of links, while new documents with few, or no, incoming links are unlikely to rank highly in search results. We describe a novel system for 'new-Web' search based on links retrieved from the Twitter micro-blogging service. The Twitter service allows individuals, organisations and governments to rapidly disseminate very short messages to a wide variety of interested parties. When a Twitter message contains a URL, we use the Twitter message as a description of the URL's target. As Twitter is frequently used for discussion of current events, these messages offer useful, up- to-date annotations and instantaneous popularity readings for a small, but timely, portion of the Web. Our working system is simple and fast and we believe may offer a significant advantage in revealing new information on the Web that would otherwise be hidden from searchers. Beyond the basic system, we anticipate the Twitter messages may add supplementary terms for a URL, or add weight to existing terms, and that the reputation or authority of each message sender may serve to weight both annotations and query-independent popularity.	New-web search with microblog annotations	NA:NA:NA	2010
Sherif Sakr:Ahmed Awad	We present a framework for querying and reusing graph-based business process models. The framework is based on a new visual query language for business processes called BPMN-Q. The language addresses processes definitions and extends the standard BPMN visual notations for modeling business processes for its concrete syntax. BPMN-Q is used to query process models by matching a process model graph to a query graph. Moreover, the reusing framework is enhanced with a semantic query expander component. This component provides the users with the flexibility to get not only the perfectly matched process models to their queries but also the models with high similarity. The query engine of the framework is built on top of traditional RDBMS. A novel decomposition based and selectivity-aware relational processing mechanism is employed to achieve an efficient and scalable performance for graph-based BPMN-Q queries.	A framework for querying graph-based business process models	NA:NA	2010
Giovanni Tummarello:Richard Cyganiak:Michele Catasta:Szymon Danielczyk:Renaud Delbru:Stefan Decker	We demonstrate Sig.ma, both a service and an end user application to access the Web of Data as an integrated information space. Sig.ma uses an holistic approach in which large scale semantic web indexing, logic reasoning, data aggregation heuristics, ad hoc ontology consolidation, external services and responsive user interaction all play together to create rich entity descriptions. These consolidated entity descriptions then form the base for embeddable data mashups, machine oriented services as well as data browsing services. Finally, we discuss Sig.ma's peculiar characteristics and report on lessions learned and ideas it inspires.	Sig.ma: live views on the web of data	NA:NA:NA:NA:NA:NA	2010
Franziska von dem Bussche:Klara Weiand:Benedikt Linse:Tim Furche:François Bry	Web crawlers are increasingly used for focused tasks such as the extraction of data from Wikipedia or the analysis of social networks like last.fm. In these cases, pages are far more uniformly structured than in the general Web and thus crawlers can use the structure of Web pages for more precise data extraction and more expressive analysis. In this demonstration, we present a focused, structure-based crawler generator, the "Not so Creepy Crawler" (nc2 ). What sets nc2 apart, is that all analysis and decision tasks of the crawling process are delegated to an (arbitrary) XML query engine such as XQuery or Xcerpt. Customizing crawlers just means writing (declarative) XML queries that can access the currently crawled document as well as the metadata of the crawl process. We identify four types of queries that together sufice to realize a wide variety of focused crawlers. We demonstrate nc2 with two applications: The first extracts data about cities from Wikipedia with a customizable set of attributes for selecting and reporting these cities. It illustrates the power of nc2 where data extraction from Wiki-style, fairly homogeneous knowledge sites is required. In contrast, the second use case demonstrates how easy nc2 makes even complex analysis tasks on social networking sites, here exemplified by last.fm.	Not so creepy crawler: easy crawler generation with standard xml queries	NA:NA:NA:NA:NA	2010
Changhu Wang:Zhiwei Li:Lei Zhang	In this technical demonstration, we showcase the MindFinder system $-$ a novel image search engine. Different from existing interactive image search engines, most of which only provide image-level relevance feedback, MindFinder enables users to sketch and tag query images at object level. By considering the image database as a huge repository, MindFinder is able to help users present and refine their initial thoughts in their mind, and finally turn thoughts to a beautiful image(s). Multiple actions are enabled for users to flexibly design their queries in a bilateral interactive manner by leveraging the whole image database, including tagging, refining query by dragging and dropping objects from search results, as well as editing objects. After each action, the search results will be updated in real time to provide users up-to-date materials to further formulate the query. By the deliberate but easy design of the query, MindFinder not only tries to enable users to present on the query panel whatever they are imagining, but also returns to users the most similar images to the picture in users' mind. By scaling up the image database to 10 million, MindFinder has the potential to reveal whatever in users' mind, that is where the name MindFinder comes from.	MindFinder: image search by interactive sketching and tagging	NA:NA:NA	2010
Ingo M. Weber:Hye-young Paik:Boualem Benatallah:Zifei Gong:Liangliang Zheng:Corren Vorwerk	In this paper we present FormSys, a Web-based system that service-enables form documents. It offers two main services: filling in forms based on Web services' incoming SOAP messages, and invoking Web services based on filled-in forms. This can be applied to benefit individuals to reduce the number of often repetitive form fields they have to complete manually in many scenarios. It can also help organisations to remove the need for manual data entry by automatically triggering business process implementations based on incoming case data from filled-in forms. While the concept applies to forms of any type of document, our implementation uses Adobe AcroForms due to its universal applicability, availability of a usable API, and end-user appeal. In the demo, we will show the two core functions, namely soap2pdf and pdf2soap, along with use case applications of the services developed from real world scenarios. Essentially, this work demonstrates how PDFs can be used as a channel for interacting with Web services.	FormSys: form-processing web services	NA:NA:NA:NA:NA:NA	2010
Gang Wu:Mengdong Yang:Ke Wu:Guilin Qi:Yuzhong Qu	Falconer is a semantic Web search engine enhanced SIOC (Semantically-Interlinked Online Communities) application, which is designed to demonstrate the ability of accelerating the creation and reuse process of semantic Web data with easy-to-use user interfaces. In this process, semantic Web search engines feed existing semantic data into the SIOC framework, where new semantic data are composed by the community and indexed again by those search engines. Compared to existing social (semantic) Web applications, Falconer inherently conforms to SIOC specification. It provides semantic search engine based user registration suggestion, friends auto-discovery, and semantic annotation for forum post content. Another distinctive feature is that it enables users to subscribe any resource having a URI as the topic they are interested in. The relationships among users, topics, and posts are further visualized for analyzing the topic trends in the community. As all semantic data are formatted in RDF and RDFa, they can be queried with SPARQL query language.	Falconer: once SIOC meets semantic search engine	NA:NA:NA:NA:NA	2010
Hao Xu:Jingdong Wang:Xian-Sheng Hua:Shipeng Li	In this demo, we present a novel interactive image search system, image search by 2D semantic map. This system enables users to indicate what semantic concepts are expected to appear and even how these concepts are spatially distributed in the desired images. To this end, we design an intuitive interface for users to formulate a query in the form of 2D semantic map, called concept map, by typing textual queries in a blank canvas. In the ranking process, by interpreting each textual concept as a set of representative visual instances, the concept map query is translated into a visual instance map, which is then used for comparison with the images in the database. Besides, in this demo, we also show an image search system with a simplest semantic map, a 2D color map, where the concepts are limited from the colors.	Interactive image search by 2D semantic map	NA:NA:NA:NA	2010
Wai Gen Yee:Andrew Yates:Ophir Frieder:Armin Moehrle	An increasing amount of Web information is in video format. Today's search technology allows videos to be found using graphical features and textual descriptions. However, the information gleaned from video features is coarse, while textual descriptions are often short and fail to capture the precise content of videos. We hypothesize that user comments contain supplemental information that effectively describes the content of a video. This information, once extracted, can be applied to a search engine index to improve video search accuracy. A complete comment-based indexing system must encourage users to post comments and be able to analyze comment data. To support research in the relevant areas of interface design, text analysis, and information retrieval, we present eduKEN, a flexible Web-based tool that allows comment collection, analysis, and search. To support the collection of real-world comment data in an educational setting, eduKEN includes an additional user interface designed for classroom use.	eduKEN: a tool for fine-grained video comment collection and analysis	NA:NA:NA:NA	2010
Kang Zhang:Haofen Wang:Duc Thanh Tran:Yong Yu	With the development of Semantic Web in recent years, an increasing amount of semantic data has been created in form of Resource Description Framework (RDF). Current visualization techniques help users quickly understand the underlying RDF data by displaying its structure in an overview. However, detailed information can only be accessed by further navigation. An alternative approach is to display the global context as well as the local details simultaneously in a unified view. This view supports the visualization and navigation on RDF data in an integrated way. In this demonstration, we present ZoomRDF, a framework that: i) adapts a space-optimized visualization algorithm for RDF, which allows more resources to be displayed, thus maximizes the utilization of display space, ii) combines the visualization with a fisheye zooming concept, which assigns more space to some individual nodes while still preserving the overview structure of the data, iii) considers both the importance of resources and the user interaction on them, which offers more display space to those elements the user may be interested in. We implement the framework based on the Gene Ontology and demonstrate that it facilitates tasks like RDF data exploration and editing.	ZoomRDF: semantic fisheye zooming on RDF data	NA:NA:NA:NA	2010
Dong Zhou:Ajay Chander:Hiroshi Inamura	This paper describes the design and prototype implementation of MIntOS (Mobile Interaction Optimization System), a system for improving mobile interaction in web-based activities. MIntOS monitors users' interactions both for gathering interaction history and for the runtime construction of interaction context. A simple approach based on interaction burstiness is used to break interaction sequences into Trails, which approximates user tasks. Such Trails are used to generate rules for online, context-sensitive prediction of future interaction sequences. Predicted user interaction sequences are then optimized to reduce the amount of user input and user wait time using techniques such as interaction short-cuts, automatic text copying and form-filling, as well as page pre-fetching. Such optimized interaction sequences are, at real-time, recommended to the user through UI enhancements in a non-intrusive manner.	Optimizing user interaction for web-based mobile tasks	NA:NA:NA	2010
Andrei Broder:Elizabeth F. Churchill:Marti Hearst:Barney Pell:Prabhakar Raghavan:Andrew Tomkins	Back in the heady days of 1999 and WWW8 (Toronto) we held a panel titled "Finding Anything in the Billion Page Web: Are Algorithms the Key?" In retrospect the answer to this question seems laughably obvious - the search industry has burgeoned on a foundation of algorithms, cloud computing and machine learning. As we move into the second decade of this millennium, we are confronted with a dizzying array of new paradigms for finding content, including social networks and location-based search and advertising. This panel pulls together senior experts from academia and the major search principals to debate whether search will continue to look anything like the 2-keywords-give-10-blue-links paradigm that Google has popularized. What do emerging approaches and paradigms - natural language search, social search, location-based search - mean for the future of search in general?	Search is dead!: long live search	NA:NA:NA:NA:NA:NA	2010
Jeff Ubois:Jamie Davidson:Marko Grobelnik:Paul Over:Hans Westerhof	This panel will debate various approaches to improving video search, and explore how professional cataloguing, crowd sourced metadata, and improvements in search algorithms will evolve over the next ten years. Panelists will explore the needs of large scale video archives, and compare these against the current capabilities of video search.	Video search: are algorithms all we need?	NA:NA:NA:NA:NA	2010
David A. Shamma:Seth Fitzsimmonds:Joe Gregorio:Adam Hupp:Ramesh Jain:Kevin Marks	This panel discusses how polling in the HTTPd protocol affects how we are building the next generation of the web and its applications. As other technologies (HTML, Javascript, etc.) move forward, we ask should the web's protocol also evolve or is it sufficient for the web to continue through just GET and POST?	What the web can't do	NA:NA:NA:NA:NA:NA	2010
Ali Dasdan:Kostas Tsioutsiouliklis:Emre Velipasaoglu	Search engines are important resources for finding information on the Web. They are also important for publishers and advertisers to present their content to users. Thus, user satisfaction is key and must be quantified. In this tutorial, we give a practical review of web search metrics from a user satisfaction point of view. We cover metrics for relevance, comprehensiveness, coverage, diversity, discovery freshness, content freshness, and presentation. We will also describe how these metrics can be mapped to proxy metrics for the stages of a generic search engine pipeline. The practitioners can apply these metrics readily and the researchers can get motivation for new problems to work on, especially in formalizing and refining metrics.	Web search engine metrics: (direct metrics to measure user satisfaction)	NA:NA:NA	2010
Pavel Dmitriev:Pavel Serdyukov:Sergey Chernov	With the growing amount of information on users' desktops and increasing scale and complexity of intranets, Enterprise and Desktop Search are becoming two increasingly important Information Retrieval applications. While the challenges arising there are not completely different from those that the web community has faced for years, advanced web search solutions are often unable to address them properly. In this tutorial we give a research prospective on distinctive features of both Enterprise and Desktop Search, explain typical search scenarios, and review existing ranking techniques and algorithms.	Enterprise and desktop search	NA:NA:NA	2010
Olaf Hartig:Juan Sequeda:Jamie Taylor:Patrick Sinclair	In the past two years, the amount of data published in RDF and following the Linked Data principles has increased dramatically. Everyday people are publishing datasets as Linked Data. However, applications that consume Linked Data are not mainstream yet. To overcome this issue, we present a beginners tutorial on consuming Linked Data. We will discuss existing techniques how users can currently consume Linked Data and use it in their current applications.	How to consume linked data on the web: tutorial description	NA:NA:NA:NA	2010
Nicole Immorlica:Vahab S. Mirrokni	We discuss the use of social networks in implementing viral marketing strategies. In the first part of this tutorial, we study influence maximization or how the structure of the social network affects the spread of behaviors and technologies. In the second part, we then consider how one might monopolize these natural processes to generate revenue in a revenue maximization setting.	Optimal marketing and pricing over social networks	NA:NA	2010
Daxin Jiang:Jian Pei:Hang Li	Huge amounts of search and browse log data has been accumulated in various search engines. Such massive search/browse log data, on the one hand, provides great opportunities to mine the wisdom of crowds and improve Web search as well as online advertisement. On the other hand, designing effective and efficient algorithms and tools to clean, model, and process large scale log data presents great challenges. In this tutorial, we give a systematic survey on the applications, challenges, fundamental principles and state-of-the-art methods of mining large scale search and browse log data. We start with an introduction of search and browse log data and an overview of various log mining applications. Then, we focus on four popular areas of log mining applications, namely query understanding, document understanding, query-document matching, and user understanding. For each area, we review the major tasks, analyze the challenges, and exemplify several representative solutions. Finally, we discuss several new directions in search/browse log mining. The tutorial slides are available at the authors' homepages after the tutorial is presented.	Web search/browse log mining: challenges, methods, and applications	NA:NA:NA	2010
Rosie Jones:Ted Drake	It costs about 300M dollars to just build a search system that scales to the web. Web services which open up web search results to the public allow academics, developers, and entrepreneurs to achieve instant web search parity for free and enable them to focus on building their additional secret sauce on top to create even grander relevant services. For example, a social network could leverage open search and their data about users to personalize web search. Additionally, one of the best ways to gather data about web search behavior is to build your own search system. Proto-type IR and web search systems based on open search can be used to gather user interaction data and test the applicability of research ideas. Open Source tools like Lucene and Nutch and open search services like from major search engines can greatly help developers implement these types of systems quickly, allowing for fast production and evaluation. We will give detailed overviews of the popular open search tools, showcasing examples of search and IR algorithms and systems implemented using them, as well as discussing how evaluation can be performed.	Applications of open search tools	NA:NA	2010
Irwin King:Michael R. Lyu:Hao Ma	As the exponential growth of information generated on the World Wide Web, Social Recommendation has emerged as one of the hot research topics recently. Social Recommendation forms a specific type of information filtering technique that attempts to suggest information (blogs, news, music, travel plans, web pages, images, tags, etc.) that are likely to interest the users. Social Recommendation involves the investigation of collective intelligence by using computational techniques such as machine learning, data mining, natural language processing, etc. on social behavior data collected from blogs, wikis, recommender systems, question & answer communities, query logs, tags, etc. from areas such as social networks, social search, social media, social bookmarks, social news, social knowledge sharing, and social games. In this tutorial, we will introduce Social Recommendation and elaborate on how the various characteristics and aspects are involved in the social platforms for collective intelligence. Moreover, we will discuss the challenging issues involved in Social Recommendation in the context of theory and models of social networks, methods to improve recommender systems using social contextual information, ways to deal with partial and incomplete information in the social context, scalability and algorithmic issues with social computational techniques.	Introduction to social recommendation	NA:NA:NA	2010
Marie-Francine Moens	This tutorial focuses on the task of automated information linking in text and multimedia sources. In any task where information is fused from different sources, this linking is a necessary step. To solve the problem we borrow methods from computational linguistics, computer vision and data mining. Although the main focus is on finding equivalence relations in the sources, the tutorial opens views on the recognition of other relation types.	Linking content in unstructured sources	NA	2010
Cesare Pautasso:Erik Wilde	Recent technology trends in Web services indicate that a solution eliminating the perceived complexity of the WS-* standard technology stack may be in sight: advocates of Representational State Transfer (REST) have come to believe that their ideas explaining why the World Wide Web works are just as applicable to solve enterprise application integration problems and to radically simplify the plumbing required to implement a Service-Oriented Architecture (SOA). In this tutorial we give an introduction to the REST architectural style as the foundation for RESTful Web services. The tutorial starts from the basic design principles of REST and how they are applied to service oriented computing. Service-orientation concentrates on identifying self-contained units of functionality, which should then be exposed as easily reusable and repurposable services. This tutorial focuses not on the identification of those units, but on how to design the services representing them. We explain how decisions on the SOA level already shape the architectural style that will be used for the eventual IT architecture, and how the SOA process itself has to be controlled to yield services which can then be implemented RESTfully. We do not claim that REST is the only architectural style that can be used for SOA design, but we do argue that it does have distinct advantages for loosely coupled services and massive scale, and that any SOA approach already has to be specifically RESTful on the business level to yield meaningful input for IT architecture design.	RESTful web services: principles, patterns, emerging technologies	NA:NA	2010
Davy Van Deursen:Raphaël Troncy:Erik Mannens:Silvia Pfeiffer:Yves Lafon:Rik Van de Walle	In this paper, we describe two examples of implementations of the Media Fragments URI specification which is currently being developed by the W3C Media Fragments Working Group. The group's mission is to create standard addressing schemes for media fragments on the Web using Uniform Resource Identifiers (URIs). We describe two scenarios to illustrate the implementations. More specifically, we show how User Agents (UA) will either be able to resolve media fragment URIs without help from the server, or will make use of a media fragments-aware server. Finally, we present some ongoing discussions and issues regarding the implementation of the Media Fragments specification.	Implementing the media fragments URI specification	NA:NA:NA:NA:NA:NA	2010
David Humphrey:Corban Brook:Alistair MacDonald	The HTML5 specification introduces the audio and video media elements, and with them the opportunity to change the way media is integrated on the web. The current HTML5 media API provides ways to play and get limited information about audio and video, but no way to programatically access or create such media. In this paper we present an enhanced API for these media elements, as well as details about a Mozilla Firefox implementation created by the authors, which allows web developers to read and write raw audio data.	Exposing audio data to the web: an API and prototype	NA:NA:NA	2010
Catherine Leung:Andor Salga	WebGL leverages the power of OpenGL to present accelerated 3D graphics on a webpage. The ability to put hardware- accelerated 3D content in the browser will provide a means for the creation of new web based applications that were previously the exclusive domain of the desktop environment. It will also allow the inclusion of features that standalone 3D applications do not have. While WebGL succeeds in bringing the power and low- level API of OpenGL to the browser, it also expects a lot of web developers, who are used to the DOM and JavaScript libraries like jQuery. This paper will look at how mid level APIs can help web developers create unique 3D content that is more than just duplicates of a standalone desktop application on a web page. We will present one such web application named Motionview, built with C3DL, that provides a new means for artist and motion capture studios to communicate with each other. We will also highlight some upcoming project ideas that make use of 3D browser technology in a way that would not have been possible in a desktop environment.	Enabling WebGL	NA:NA	2010
Arun Kumar:Sheetal K. Agarwal:Priyanka Manwani	It has been a constant aim of computer scientists, programming language designers and practitioners to raise the level of programming abstractions and bring them as close to the user's natural context as possible. The efforts started right from our transition from machine code programming to assembly language programming, from there to high level procedural languages, followed by object oriented programming. Nowadays, service oriented software development and composition are the norm. There have also been notable efforts such as Alice system from CMU to simplify the programming experience through the use of 3D virtual worlds. The holy grail has been to enable non-technical users such as kids or non-technical people to be able to understand and pick up programming and software development easily. We present a novel approach to software development that lets people use their voice to program or create new software through composition. We demonstrate some basic programming tasks achieved by simply talking to a system over an ordinary phone. Such programs constructed by talking can be created in user's local language and do not require IT literacy or even literacy as a prerequisite. We believe this approach will have a deep impact on software development, especially development of web based software in the very near future.	The spoken web: software development and programming through voice	NA:NA:NA	2010
Polychronis Ypodimatopoulos:Andrew Lippman	We leverage the ubiquity of bluetooth-enabled devices and propose a decentralized, web-based architecture that allows users to share their location by following each other in the style of Twitter. We demonstrate a prototype that operates in a large building which generates a dataset of detected bluetooth devices at a rate of ~30 new devices per day, including the respective location where they were last detected. Users then query the dataset using their unique bluetooth ID and share their current location with their followers by means of unique URIs that they control. Our separation between producers (the building) and consumers (the users) of bluetooth device location data allows us to create socially-aware applications that respect user's privacy while limiting the software necessary to run on mobile devices to just a web browser.	'Follow me': a web-based, location-sharing architecture for large, indoor environments	NA:NA	2010
Scott Rich	In this paper, we describe our experience in the Jazz project, beginning from a "Classical" repository- and Java-centric design and evolving towards an architecture which borrows heavily from the architecture of the Web. Along the way, we formed an open community to collaborate on adapting this architecture to various tool domains. Finally, we discuss our experience delivering the first generation of tools built and integrated using these techniques.	IBM's jazz integration architecture: building a tools integration architecture and community inspired by the web	NA	2010
Jianyong Wang:Hayato Yamana	NA	Session details: WWW 2014 PhD/doctoral presentations	NA:NA	2014
Hyun Joon Jung	We investigate a method of crowdsourced task routing based on matrix factorization. From a preliminary analysis of a real crowdsourced data, we begin an exploration of how to route crowdsourcing task via Matrix factorization (MF) which efficiently estimate missing values in a worker-task matrix. Our preliminary results show the benefits of task routing over random assignment, the strength of probabilistic MF over baseline methods.	Quality assurance in crowdsourcing via matrix factorization based task routing	NA	2014
Jeong-Hoon Park	As GPS-enabled mobile devices have advanced, the location-based service(LBS) became one of the most active subjects in the Web-based services. Major Web-based services such as Google Picasa, Twitter, Facebook, and Flicker employ LBS as one of their main features. Consequently, a large number of geotagged documents are generated by users in the Web-based services. Recently, there have been studies on the spatial keyword search which aims to find a set of documents in the Web-based services by evaluating the spatial relevance and keyword relevance. It is a combination of the spatial search and keyword search, each of which has been studied for a long time. In this paper, we address the spatial semantic search problem which is to find top k relevant sets of documents with spatial constraints and semantic constraints. For devising an effective solution of the spatial semantic search, we propose a hybrid index strategy, a ranking model and an efficient search algorithm. In addition, we present the current status of our research progress, and discuss remaining challenges and future works.	Spatial semantic search in location-based web services	NA	2014
Nam Khanh Tran	In the past, various studies have been proposed to acquire the capacity to perceive and comprehend language in articles or human communications. Recently, researchers focus on higher semantic levels to what human would need to understand the contents of articles. While human can smoothly interpret documents when they have knowledge of the context of documents, they have difficulty with those as their context is lost or changes. In this PhD proposal, we address three novel research questions: detecting uninterpretable pieces in documents, retrieving contextual information and constructing compact context for the documents, then propose approaches to these tasks, and discuss related issues.	Time-aware topic-based contextualization	NA	2014
Minghe Yu	With the emergence of massive information networks, graph data have become ubiquitous for various applications. Although many graph processing problems have been studied recently, entity linking on graph data has not received enough attention by the academia and industry, which finds vertex pairs that refer to the same entity from two graphs. There are two main research challenges arising in this problem. The first one is how to determine whether two vertices refer to the same entity which is rather hard for graph data, especially uncertain data, e.g., social networks. The second challenge is to efficiently link the vertices. As existing graph data are rather large, it is very important to devise efficient algorithms to achieve high performance. To address these challenges, in this paper we propose a similarity-based method which takes the vertex pairs with similarity larger than a given threshold as linked entities. We extend existing textual similarity and structural similarity to evaluate similarity between vertices from different graphs. To achieve high quality, we also combine them and propose a hybrid similarity. We also discuss new algorithms to efficiently link entities. We conduct experimental studies on real datasets and the results proves show that our hybrid method achieves high performance and outperform the baseline approaches.	Entity linking on graph data	NA	2014
Philipp Singer	Navigating websites represents a fundamental activity of users on the Web. Modeling this activity, i.e., understanding how predictable human navigation is and whether regularities can be detected has been of interest to researchers for nearly two decades. This is crucial for improving the Web experience of users by e.g., enhancing interfaces or information network structures. This thesis envisions to shedding light on human navigational patterns by trying to understand, leverage and improve human navigation on the Web. One main goal of this thesis is the construction of a versatile framework for modeling human navigational data with the use of Markov chains and for detecting the appropriate Markov chain order by using several advanced inference methods. It allows us to investigate memory and structure in human navigation patterns. Furthermore, we are interested in detecting whether pragmatic human navigational data can be leveraged by e.g., being useful for the task of calculating semantic relatedness between concepts. Finally, we want to find ways of enhancing human navigation models. Concretely, we plan on incorporating prior knowledge about the semantic relatedness between concepts to our Markov chain models as it is known that humans navigate the Web intuitively instead of randomly. Our experiments should be conducted on a variety of distinct navigational data including both goal oriented and free form navigation scenarios. We not only look at navigational paths over websites, but also abstract away to navigational paths over topics in order to get insights into cognitive patterns.	Understanding, leveraging and improving human navigation on the web	NA	2014
Shruti Chhabra	In recent times, focus of information retrieval community has shifted from traditional keyword-based retrieval to techniques utilizing the semantics in the text. Since such techniques require the understanding of relationships between entities, efforts are ongoing to organize the Web into large entity-relationship graphs. These graphs can be leveraged to answer complex relationship queries. However, most of the research has focused upon extracting structural information between entities such as a path, Steiner tree, or subgraphs. Little attention has been paid to the comprehension of these structural results, which is necessary for the user to understand relationships encapsulated in these structures. In this doctoral proposal, we pursue the idea of entity-centric summarization and propose a novel framework to produce entity-centric summaries which describe the relationships among input entities. We discuss the inherent challenges associated with each module in the framework and present an evaluation plan. Results from our preliminary experiments are encouraging and substantiate the feasibility of summarization problem.	Entity-centric summarization: generating text summaries for graph snippets	NA	2014
Eduardo Graells-Garrido	Many activities people perform on the Web are biased, including activities like reading news, searching for information and connecting with people. Sometimes these biases are inherent in social behavior (like homophily), and sometimes they are external as they affect the system (like media bias). In this thesis proposal, we describe our approach to use information visualization to enhance Web activities performed by regular people (i.e., non-experts) We understand enhancing as reducing bias effects and generating an engaging response from users. Our methodology is based on case studies. We select a Web activity, identify the biases that affect it, and evaluate how the biases affect a population from online social networks using web mining techniques, and then, we design a visualization following an interactive and playful design approach to diminish the previously identified biases. We propose to evaluate the effect of our visualization designs in user studies by comparing them with state-of-the-art techniques considering a playful experiences framework.	Enhancing web activities with information visualization	NA	2014
Saman Kamran	Taggers in social tagging systems have the main role in giving identities to the objects. Tagged objects also represent perception of their taggers about them and can define identities of their taggers in return. Consequently, identities that are assigned to the objects and taggers have effect on the quality of their categorization and communities formation around them. Tags in social semantic tagging systems have formal definitions because they are mapped to the concepts that are defined in ontologies. Semantic tags are not only able to improve quality of tag assignments by solving some common tags ambiguity problems related to classic folksonomy systems (i.e., in particular polysemy and synonymy), but also to provide some meta data on top of the social relations based on contribution of taggers around semantic tags. Those meta data may be exploited to form dynamic communities which addresses the problems of lack of commonly agreed and evolving meaning of tags in social semantic tagging systems. This paper proposes an approach to form dynamic communities of related taggers around the tagged objects. Because our perceptions in each specific area of knowledge is evolving over time, the goal of our approach is also to evolve the represented knowledge in semantic tagging systems dynamically according to the latest perception of the related users.	Dynamic communities formation through semantic tags	NA	2014
Yunkyu Sohn	For decades, scholars of various disciplines have been fretted over strategic interactions, presenting theoretical insights and empirical observations. Despite the central role played by strategic interactions in creating values in the Internet environment, our ability to understand them scientifically and to manage them in practice has remained limited. While engineering communities suffer from not having enough theoretical resource to formalize such phenomena, economics and social sciences lack adequate technology to properly operationalize their theoretical insights, thereby demanding an integrative solution. This project aims to develop a rational-choice-theory-driven framework for computational social science, focusing on social interactions on the Internet. In order to suggest theoretical foundations, validation of the predictions in a controlled environment, and verification of the results in actual platforms, general approaches and a few examples of ongoing research are presented.	Strategic foundation of computational social science	NA	2014
Ning Xu	With the increasing size of web data and widely adopted parallel cloud computing paradigm, distributed database and other distributed data processing systems, for example Pregel and GraphLab, use data partitioning technology to divide the large data set. By default, these systems use hash partitioning to randomly assign data to partitions, which leads to huge network traffic between partitions. Fine-grained partitioning can better allocate data and minimize the number of nodes involved within a transaction or job while balancing the workload across data nodes as well. In this paper, we propose a novel prototype system, Lute, to provide highly efficient fine-grained partitioning scheme for these distributed systems. Lute maintains a lookup table for each partitioned data set that maps a key to a set of partition ID(s). We use a novel lookup table technology that provides low cost of reading and writing lookup table. Lute provides transaction support and high concurrency writing with Multi Version Concurrency Control (MVCC) as well. We implemented a prototype distributed DBMS on Postgresql and used Lute as a middle-ware to provide fine-grained partitioning support. Extensive experiments conducted on a cluster demonstrate the advantage of the proposed approach. The evaluation results show that in comparison with other state-of-the-art lookup table salutations, our approach can significantly improve throughput by about 20% to 70% on TPC-C benchmark.	Fine-grained data partitioning framework for distributed database systems	NA	2014
Katerina Stamou	The cloud computing paradigm emerged with service oriented principles. In the cloud setting, organizations outsource their IT equipment and manage their business processes through virtual services that are typically exchanged over HTTP. Service Level Agreements (SLAs) depict the status of running services. SLAs represent operational contracts that allow providers to estimate their service avail- ability according to their resource capacity. The SLA data schema and content are operationally defined by the type, volume and relations of service elements that organizations operate on their physical resources. Current lack of a uniform SLA standardization leads to semantic and operational differences between SLAs, that are produced and consumed by different organizations. Such differences prohibit common business SLA practices in the cloud computing domain. Our research introduces systematic SLA data management to describe the formalization, storage and processing of SLAs over distributed computing environments. Services in scope are framed within the cloud computing context.	Systematic SLA data management	NA	2014
Jaewoo Kang:Yufei Tao	NA	Session details: WWW 2014 demonstrations	NA:NA	2014
Kwan Hui Lim:Ee-Peng Lim:Palakorn Achananuparp:Adrian Vu:Agus Trisnajaya Kwee:Feida Zhu	Tracking user browsing data and measuring the effectiveness of website design and web services are important to businesses that want to attract the consumers today who spend much more time online than before. Instead of using randomized controlled experiments, the existing approach simply tracks user browsing behaviors before and after a change is made to website design or web services, and evaluate the differences. To address the effects caused by hidden factors (e.g. promotion activities on the website) and to give fair comparison of different website designs, we propose the LASER system, a unified experimentation platform that enables randomized online controlled experiments to be easily conducted with minimal human effort and modifications to the experimented websites. More importantly, the LASER system manages the various aspects of online controlled experiments, namely the selection of participants into groups, exposure of different user interface features or recommendation algorithms to these groups, measuring their responses, and summarizing the results in the visual manner.	LASER: a living analytics experimentation system for large-scale online controlled experiments	NA:NA:NA:NA:NA:NA	2014
Matheus Araújo:Pollyanna Gonçalves:Meeyoung Cha:Fabrício Benevenuto	Sentiment analysis methods are used to detect polarity in thoughts and opinions of users in online social media. As businesses and companies are interested in knowing how social media users perceive their brands, sentiment analysis can help better evaluate their product and advertisement campaigns. In this paper, we present iFeel, a Web application that allows one to detect sentiments in any form of text including unstructured social media data. iFeel is free and gives access to seven existing sentiment analysis methods: SentiWordNet, Emoticons, PANAS-t, SASA, Happiness Index, SenticNet, and SentiStrength. With iFeel, users can also combine these methods and create a new Combined-Method that achieves high coverage and F-measure. iFeel provides a single platform to compare the strengths and weaknesses of various sentiment analysis methods with a user friendly interface such as file uploading, graphical visualizing, and weight tuning.	iFeel: a system that compares and combines sentiment analysis methods	NA:NA:NA:NA	2014
Jimmy Lin:Miles Efron	How do we conduct large-scale community-wide evaluations for information retrieval if we are unable to distribute the document collection? This was the challenge we faced in organizing a task on searching tweets at the Text Retrieval Conference (TREC), since Twitter's terms of service forbid redistribution of tweets. Our solution, which we call "evaluation as a service", was to provide an API through which the collection can be accessed for completing the evaluation task. This paper describes the infrastructure underlying the service and its deployment at TREC 2013. We discuss the merits of the approach and potential applicability to other evaluation scenarios.	Infrastructure support for evaluation as a service	NA:NA	2014
Sourav S. Bhowmick:Aixin Sun:Ba Quan Truong	Despite considerable progress in recent years on Tag-based Social Image Retrieval (TagIR), state-of-the-art TagIR systems fail to provide a systematic framework for end users to ask why certain images are not in the result set of a given query and provide an explanation for such missing results. However, such why-not questions are natural when expected images are missing in the query results returned by a TagIR system. In this demonstration, we present a system called WINE (Why-not questIon aNswering Engine) which takes the first step to systematically answer the why-not questions posed by end-users on TagIR systems. It is based on three explanation models, namely result reordering, query relaxation, and query substitution, that enable us to explain a variety of why-not questions. Our answer not only involves the reason why desired images are missing in the results but also suggestion on how the search query can be altered so that the user can view these missing images in sufficient number.	Why not, WINE?	NA:NA:NA	2014
Hyunjung Park:Jennifer Widom	CrowdFill is a system for collecting structured data from the crowd. Unlike a typical microtask-based approach, CrowdFill shows an entire partially-filled table to all participating workers; workers collaboratively complete the table by filling in empty cells, as well as upvoting and downvoting data entered by other workers, using CrowdFill's intuitive data entry interface. CrowdFill ensures data entry is leading to a final table that satisfies prespecified constraints, and its compensation scheme encourages workers to submit useful, high-quality work. We demonstrate how CrowdFill collects structured data from the crowd, from the perspective of a user as well as from the perspective of workers.	CrowdFill: a system for collecting structured data from the crowd	NA:NA	2014
Yang Song:Weiwei Cui:Shixia Liu:Kuansan Wang	We present a system to analyze user interests by analyzing their online behaviors from large-scale usage logs. We surmise that user interests can be characterized by a large collection of features we call the behavioral genes that can be deduced from both their explicit and implicit online behaviors. It is the goal of this research to sequence the entire behavioral genome for online population, namely, to identify the pertinent behavioral genes and uncover their relationships in explaining and predicting user behaviors, so that high quality user profiles can be created and the online services can be better customized using these profiles. Within the scope of this paper, we demonstrate the work using the partial genome derived from web search logs. Our demo system is supported by an open access web service we are releasing and sharing with the research community. The main functions of the web service are: (1) calculating query similarities based on their lexical, temporal and semantic scores, (2) clustering a group of user queries into tasks with the same search and browse intent, and (3) inferring user topical interests by providing a probability distribution over a search taxonomy.	Online behavioral genome sequencing from usage logs: decoding the search behaviors	NA:NA:NA:NA	2014
Hannah Bast:Florian Bäurle:Björn Buchhold:Elmar Haußmann	We demonstrate a system for fast and intuitive exploration of the Freebase dataset. This required solving several non-trivial problems, including: entity scores for proper ranking and name disambiguation, a unique meaningful name for every entity and every type, extraction of canonical binary relations from multi-way relations (which in Freebase are modeled via so-called mediator objects), computing the transitive hull of selected relations, and identifying and merging duplicates. Our contribution is two-fold. First, we provide for download an up-to-date version of the Freebase data, enriched and simplified as just sketched. Second, we offer a user interface for exploring and searching this data set. The data set, the user interface and a demo video are available from http://freebase-easy.cs.uni-freiburg.de.	Easy access to the freebase dataset	NA:NA:NA:NA	2014
Sebastian Heil:Stefan Wild:Martin Gaedke	An increasing share of today's work is knowledge work. Adaptive Case Management (ACM) assists knowledge workers in handling this collaborative, emergent and unpredictable type of work. Finding suitable workers for specific functions still relies on manual assessment and assignment by persons in charge, which does not scale well. In this paper we discuss a tool for ACM to facilitate this expert finding leveraging existing Web technology. We propose a method to automatically recommend a set of eligible workers utilizing linked data, enriched user profile data from distributed social networks and information gathered from case descriptions. This semantic recommendation method detects similarities between case requirements and worker profiles. The algorithm traverses distributed social graphs to retrieve a ranked list of suitable contributors to a case according to adaptable metrics. For this purpose, we introduce a vocabulary to specify case requirements and a vocabulary to describe skill sets and personal attributes of workers. The semantic recommendation method is demonstrated by a prototypical implementation using a WebID-based distributed social network.	Collaborative adaptive case management with linked data	NA:NA:NA	2014
Erdal Kuzey:Gerhard Weikum	We present EVIN: a system that extracts named events from news articles, reconciles them into canonicalized events, and organizes them into semantic classes to populate a knowledge base. EVIN exploits different kinds of similarity measures among news, referring to textual contents, entity occurrences, and temporal ordering. These similarities are captured in a multi-view attributed graph. To distill canonicalized events, EVIN coarsens the graph by iterative merging based on a judiciously designed loss function. To infer semantic classes of events, EVIN uses statistical language models. EVIN provides a GUI that allows users to query the constructed knowledge base of events, and to explore it in a visual manner.	EVIN: building a knowledge base of events	NA:NA	2014
Gregor Leban:Blaz Fortuna:Janez Brank:Marko Grobelnik	Event Registry is a system that can analyze news articles and identify in them mentioned world events. The system is able to identify groups of articles that describe the same event. It can identify groups of articles in different languages that describe the same event and represent them as a single event. From articles in each event it can then extract event's core information, such as event location, date, who is involved and what is it about. Extracted information is stored in a database. A user interface is available that allows users to search for events using extensive search options, to visualize and aggregate the search results, to inspect individual events and to identify related events.	Event registry: learning about world events from news	NA:NA:NA:NA	2014
Michael Krug:Fabian Wiedemann:Martin Gaedke	The opportunities of the Internet combined with new devices and technologies change the end users' habits in media consumption. While end users often search for related information to the currently watched TV show by themselves, we propose to improve this user experience by automatically enriching media using semantic extraction. In our recent work we focused on how to apply media enrichment to distributed screens. Based on the findings we made from our recent prototype we identify several problems and describe how we deal with them. We illustrate a way to achieve cross-platform real-time synchronization using several transport protocols. We propose the usage of sessions to handle multi-user, multi-screen scenarios and introduce techniques for new interaction and customization patterns. We extend our recent approach with the extraction of keywords from given subtitles by utilizing statistical algorithms and natural language processing technologies, which are then used to discover and display related content from the Web. The prototype presented in this paper reflects the improvements of our work. We discuss next research steps and define challenges for further research.	Enhancing media enrichment by semantic extraction	NA:NA:NA	2014
Dimitris Kontokostas:Patrick Westphal:Sören Auer:Sebastian Hellmann:Jens Lehmann:Roland Cornelissen	Linked Open Data (LOD) comprises of an unprecedented volume of structured data on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowd-sourced or extracted data of often relatively low quality. We present Databugger, a framework for test-driven quality assessment of Linked Data, which is inspired by test-driven software development. Databugger ensures a basic level of quality by accompanying vocabularies, ontologies and knowledge bases with a number of test cases. The formalization behind the tool employs SPARQL query templates, which are instantiated into concrete quality test queries. The test queries can be instantiated automatically based on a vocabulary or manually based on the data semantics. One of the main advantages of our approach is that domain specific semantics can be encoded in the data quality test cases, thus being able to discover data quality problems beyond conventional quality heuristics.	Databugger: a test-driven framework for debugging the web of data	NA:NA:NA:NA:NA:NA	2014
Pavel Arapov:Michel Buffa:Amel Ben Othmane	The proposed demonstration requests DBPedia.org, gets the results and uses them to populate wiki pages with semantic annotations using RDFaLite. These annotations are persisted in a RDF store and we will show how this data can be reused by other applications, e.g. for a semantic mashup that displays all collected metadata about cities on a single map page. It has been developed using WikiNEXT, a mix between a semantic wiki and a web-based IDE. The tool is online , open source ; screencasts are available on YouTube (look for "WikiNext").	Semantic mashup with the online IDE WikiNEXT	NA:NA:NA	2014
Marcelo Arenas:Bernardo Cuenca Grau:Evgeny Kharlamov:Sarunas Marciuska:Dmitriy Zheleznyakov:Ernesto Jimenez-Ruiz	In this paper we demonstrate a system SemFacet, that is a proof of concept prototype for our semantic faceted search approach. SemFacet is implemented on top of the Yago knowledge base, powered by the OWL 2 RL triple store RDFox, and the full text search engine Lucene. SemFacet has provided very encouraging results. Via logical reasoning SemFacet can automatically (i) extract facets, (ii) update the faceted query interface with facets relevant for the current stage of the users query construction's session. SemFacet supports faceted queries that are much more expressive than the ones of traditional faceted search applications; in particular SemFacet allows to (i) relate several collections of documents, and (ii) change the focus of queries (and, thus, SemFacet provides control over the documents in the query output to be displayed on the screen). Our approach is fully declarative: the same backend implementation can be used to power faceted search over any application, provided that metadata and knowledge are represented in RDF and OWL 2.	SemFacet: semantic faceted search over yago	NA:NA:NA:NA:NA:NA	2014
Alan G. Labouseur:Paul W. Olsen:Kyuseo Park:Jeong-Hyon Hwang	Evolving networks can be modeled as series of graphs that represent those networks at different points in time. Our G* system enables efficient storage and querying of these graph snapshots by taking advantage of their commonalities. In extending G* for scalable and robust operation, we found the classic challenges of data distribution and replication to be imbued with renewed significance. If multiple graph snapshots are commonly queried together, traditional techniques that distribute data over all servers or create identical data replicas result in inefficient query execution. We propose to verify, using live demonstrations, the benefits of our graph snapshot distribution and replication techniques. Our distribution technique adjusts the set of servers storing each graph snapshot in a manner optimized for popular queries. Our replication technique maintains each snapshot replica on a different number of servers, making available the most efficient replica configurations for different types of queries.	A demonstration of query-oriented distribution and replication techniques for dynamic graph data	NA:NA:NA:NA	2014
Alessio Bellino:Giorgio De Michelis:Flavio De Paoli	YouTube4Two is an application that exploits the YouTube media library (through its API) to demonstrate a new style of social interaction. Two co-present people can share a video and act autonomously to navigate the related-video and comment lists, and search for videos. The novelty is that they can use their own smartphones connected via Internet to control the shared application. The application has been designed according to the responsive-web-design (RWD) principle to smoothly pass from desktop interface (controlled by mouse and keyboard) to smartphone interface (with touch control). YouTube4Two introduces the multi-device responsive Web design (MD-RWD) style that extends the RWD style by introducing the separation between displayed content (on a shared screen) and displayed control commands (on personal smartphones) to support shared control over an application.	YouTube4Two: socializing video platform for two co-present people	NA:NA:NA	2014
Nam Giang:Minkeun Ha:Daeyoung Kim	Cross domain communication has been a long-discussed subject in the field of web-based application, especially for any sort of mashups where a single web app combines resources from different locations. This issue becomes more important in the Web of Things context, where every physical resources are exposed to the Web and mashed up by other web applications. In this paper we demonstrate a use case in which cross domain communication is applied in the Web of Things using the HTML5 Cross Document Messaging API (HTML5CDM). In addition, we contribute an advanced implementation of HTML5CDM that brings RESTful communication model to HTML5CDM and supports better concurrent message exchange, which we believe will be of much benefit to web developers. In addition, a time/space evaluation that measures CPU and Memory usage for the developed HTML5CDM library is carried out and the results has proved our implementation's practicability.	Cross domain communication in the web of things: a new context for the old problem	NA:NA:NA	2014
David E. Salt:Mourad Ouzzani:Eduard C. Dragut:Peter Baker:Srivathsava Rangarajan	We describe ionomicshub, iHUB for short, a large scale cyber-infrastructure to support end-to-end research that aims to improve our understanding of how plants take up, transport and store their nutrient and toxic elements.	iHUB: an information and collaborative management platform for life sciences	NA:NA:NA:NA:NA	2014
Haggai Roitman:Gilad Barkai:David Konopnicki:Aya Soffer	Measuring the effectiveness of marketing campaigns across different channels is one of the most challenging tasks for today's brand marketers. Such measurement usually relies on a combination of key performance indicators (KPIs), used for assessing various aspects of marketing outcomes. Recently, with the availability of social-media sources, new options for collecting KPIs have become available and numerous social-media monitoring tools and services have emerged. Yet, given the vast media spectrum, which goes beyond social-media channels, existing solutions fail to generalize well and the curation of marketing performance KPIs for most marketing channels still relies on labor intensive means such as surveys and questionnaires. Trying to address the challenges, we propose to demonstrate a novel solution we have developed in IBM: Multi-channel Marketing Monitoring Platform (M3P for short). M3P is better tailored for the marketing performance domain, where online chatter is being harnessed for effective collection of meaningful marketing KPIs across all possible channels. We describe M3P's main challenges and review some of its novel KPIs. We then describe the M3P solution, focusing on its KPI extraction process. Finally, we describe the planned demonstration using a real-world marketing use-case.	Measuring the effectiveness of multi-channel marketing campaigns using online chatter	NA:NA:NA:NA	2014
Chun-Yen Yeh:Yu-Ming Hsu:Hsinfu Huang:Hong-Wun Jheng:Yu-Chuan Su:Tzu-Hsuan Chiu:Winston Hsu	In this demo, we present a scalable mobile video recognition system, named "Me-link," based on progressive fusion of light-weight audio visual features. With our system, users only have to point the mobile camera to the video they are interested in. The system will capture the frames and sounds, then retrieve relevant information immediately. As the users hold the mobile longer, the system progressively aggregates the cues temporally and then returns more accurate results. We also consider the real world noisy environment, where users may not get clear visual or audio signals. In the aggregation step of audio and visual cues, our system automatically detects the available channel for the final rank. On the server side, users can upload the videos with information via website. Besides, we also link the streaming signals so that users can get the real time broadcasting with ``Me-link".	Me-link: link me to the media -- fusing audio and visual cues for robust and efficient mobile media interaction	NA:NA:NA:NA:NA:NA:NA	2014
Donggeun Yoo:Kyunghyun Paeng:Sunggyun Park:Jungin Lee:Seungwook Paek:Sung-Eui Yoon:In So Kweon	Multiple color search technology helps users find fashion products in a more intuitive manner. Although fashion product images can be represented not only by a set of dominant colors but also by the relative ratio of colors, current online fashion shopping malls often provide rather simple color filters. In this demo, we present PRISM (Perceptual Representation of Image SiMilarity), a weighted multi-color browsing system for fashion products retrieval. Our system combines widely accepted backend web service stacks and various computer vision techniques including a product area parsing and a compact yet effective multi-color description. Finally, we demonstrate the benefits of PRISM system via web service in which users freely browse fashion products.	PRISM: a system for weighted multi-color browsing of fashion products	NA:NA:NA:NA:NA:NA:NA	2014
Anna Cinzia Squicciarini:Jules Dupont:Ruyan Chen	In this demo, we present Abuse User Analytics (AuA), an analytical framework aiming to provide key information about the behavior of online social network users. AuA efficiently processes data from users' discussions, and renders information about users' activities in a easy to-understand graphical fashion with the goal of identifying deviant or abusive activities. Using animated graphics, AuA visualizes users' degree of abusiveness, measured by several key metrics, over user selected time intervals. It is therefore possible to visualize how users' activities lead to complex interaction networks, and highlight the degenerative connections among users and within certain threads.	Online abusive users analytics through visualization	NA:NA:NA	2014
Muhammad Imran:Carlos Castillo:Ji Lucas:Patrick Meier:Sarah Vieweg	We present AIDR (Artificial Intelligence for Disaster Response), a platform designed to perform automatic classification of crisis-related microblog communications. AIDR enables humans and machines to work together to apply human intelligence to large-scale data at high speed. The objective of AIDR is to classify messages that people post during disasters into a set of user-defined categories of information (e.g., "needs", "damage", etc.) For this purpose, the system continuously ingests data from Twitter, processes it (i.e., using machine learning classification techniques) and leverages human-participation (through crowdsourcing) in real-time. AIDR has been successfully tested to classify informative vs. non-informative tweets posted during the 2013 Pakistan Earthquake. Overall, we achieved a classification quality (measured using AUC) of 80%. AIDR is available at http://aidr.qcri.org/.	AIDR: artificial intelligence for disaster response	NA:NA:NA:NA:NA	2014
Alberto Del Bimbo:Andrea Ferracani:Daniele Pezzatini:Federico D'Amato:Martina Sereni	It would be very difficult even for a resident to characterise the social dynamics of a city and to reveal to foreigners the evolving activity patterns which occur in its various areas. To address this problem, however, large amount of data produced by location-based social networks (LBSNs) can be exploited and combined effectively with techniques of user profiling. The key idea we introduce in this demo is to improve city areas and venues classification using semantics extracted both from places and from the online profiles of people who frequent those places. We present the results of our methodology in LiveCities, a web application which shows the hidden character of several italian cities through clustering and information visualisations paradigms. In particular we give in-depth insights of the city of Florence, IT, for which the majority of the data in our dataset have been collected. The system provides personal recommendation of areas and venues matching user interests and allows the free exploration of urban social dynamics in terms of people lifestyle, business, demographics, transport etc. with the objective to uncover the real `pulse' of the city. We conducted a qualitative validation through an online questionnaire with 28 residents of Florence to understand the shared perception of city areas by its inhabitants and to check if their mental maps align to our results. Our evaluation shows how considering also contextual semantics like people profiles of interests in venues categorisation can improve clustering algorithms and give good insights of the endemic characteristics and behaviours of the detected areas.	LiveCities: revealing the pulse of cities by location-based social networks venues and users analysis	NA:NA:NA:NA:NA	2014
Chaolun Xia:Raz Schwartz:Ke Xie:Adam Krebs:Andrew Langdon:Jeremy Ting:Mor Naaman	With the increasing volume of location-annotated content from various social media platforms like Twitter, Instagram and Foursquare, we now have real-time access to people's daily documentation of local activities, interests and attention. In this demo paper, we present CityBeat, a real-time visualization of hyper-local social media content for cities. The main objective of CityBeat is to provide users -- with a specific focus on journalists -- with information about the city's ongoings, and alert them to unusual activities. The system collects a stream of geo-tagged photos as input, uses time series analysis and classification techniques to detect hyper-local events, and compute trends and statistics. The demo includes a visualization of this information that is designed to be installed on a large-screen in a newsroom, as an ambient display.	CityBeat: real-time social media visualization of hyper-local city data	NA:NA:NA:NA:NA:NA:NA	2014
Subhabrata Mukherjee:Sachindra Joshi	In this work, we describe an unsupervised framework for creating self-assist systems which can serve as virtual call center agents to guide the customer in performing different domain-dependent tasks (like troubleshooting a problem, changing settings etc.). We describe a framework for creating an intent graph from a corpus of knowledge articles from a given domain which is used in creating the dialogue system. To the best of our knowledge, this is the first work in creating virtual self-assist agents.	Help yourself: a virtual self-assist system	NA:NA	2014
Claus Stadler:Michael Martin:Sören Auer	The majority of data (including data published on the Web as Linked Open Data) has a spatial dimension. However, the efficient, user friendly exploration of spatial data remains a major challenge. We present Facete, a web-based exploration and visualization application enabling the spatial-faceted browsing of data with a spatial dimension. Facete implements a novel spatial data exploration paradigm based on the following three key components: First, a domain independent faceted filtering module, which operates directly on SPARQL and supports nested facets. Second, an algorithm that efficiently detects spatial information related to those resources that satisfy the facet selection. The detected relations are used for automatically presenting data on a map. And third, a workflow for making the map display interact with data sources that contain large amounts of geometric information. We demonstrate Facete in large-scale, real world application scenarios.	Exploring the web of spatial data with facete	NA:NA:NA	2014
Jaewoo Kim:Meeyoung Cha:Thomas Sandholm	Location-based services, and in particular personal navigation systems, have become increasingly popular with the widespread use of GPS technology in smart devices. Existing navigation systems are designed to suggest routes based on the shortest distance or the fastest time to a target. In this paper, we propose a new type of route navigation based on regional context---primarily sentiments. Our system, called SocRoutes, aims to find a safer, friendlier, and more enjoyable route based on sentiments inferred from real-time, geotagged messages from Twitter. SocRoutes tailors routes by avoiding places with extremely negative sentiments, thereby potentially finding a safer and more enjoyable route with marginal increase in total distance compared to the shortest path. The system supports three types of traveling modes: walking, bicycling, and driving. We validated the idea based on crime history data from the City of Chicago Portal in December 2012, and sentiments extracted from geotagged tweets during the same time. We discovered that there was a significant correlation between regional Twitter posting sentiments and crime rate, in particular for high-crime and highly negative sentiment areas. We also demonstrated that SocRoutes, by solely utilizing social media sentiments, can find routes that bypass crime hotspots.	SocRoutes: safe routes based on tweet sentiments	NA:NA:NA	2014
Ricardo Baeza-Yates:B. Barla Cambazoglu	The main goals of a web search engine are quality, efficiency, and scalability. In this tutorial, we focus on the last two goals, providing a fairly comprehensive overview of the scalability and efficiency challenges in large-scale web search engines. In particular, the tutorial provides an in-depth architectural overview of a web search engine, mainly focusing on the web crawling, indexing, and query processing components. The scalability and efficiency issues encountered in these components are presented at four different granularities: at the level of a single computer, a cluster of computers, a single data center, and a multi-center search engine. The tutorial also points at open research problems and provides recommendations to researchers who are new to the field.	Scalability and efficiency challenges in large-scale web search engines	NA:NA	2014
Erik Cambria	The WWW'14 tutorial on Concept-Level Sentiment Analysis aims to provide its participants means to efficiently design models, techniques, tools, and services for concept-level sentiment analysis and their commercial realizations. The tutorial draws on insights resulting from the recent IEEE Intelligent Systems special issues on Concept-Level Opinion and Sentiment Analysis and the IEEE CIM special issue on Computational Intelligence for Natural Language Processing. The tutorial includes a hands-on session to illustrate how to build a concept-level opinion-mining engine step-by-step, from semantic parsing to concept-level reasoning.	Concept-level sentiment analysis: a world wide web conference 2014 tutorial	NA	2014
Atish Das Sarma:Nish Parikh:Neel Sundaresan	The focus of this tutorial will is e-commerce product search. Several challenges appear in this context, both from a research standpoint as well as an application standpoint. We present various approaches adopted in the industry, review well-known research techniques developed over the last decade, draw parallels to traditional web search highlighting the new challenges in this setting, and dig deep into some of the algorithmic and technical approaches developed. A specific approach that advances theoretical techniques and illustrates practical impact considered here is of identifying most suited results quickly from a large database. Settings span cold start users and advanced users for whom personalization is possible. In this context, top-$k$ and skylines are discussed as they form a key approach that spans the web, data mining, and database communities. These present powerful tools for search across multi-dimensional items with clear preferences within each attribute, like product search as opposed to regular web search.	E-commerce product search: personalization, diversification, and beyond	NA:NA:NA	2014
John Domingue:Alexander Mikroyannidis:Stefan Dietze	Following the latest developments in online learning and Linked Data, the scope of this tutorial will be two-fold: 1. New online learning methods will be taught for supporting the teaching of Linked Data. Additionally, the lessons learned and the best practices derived from designing and delivering a Linked Data curriculum by the EUCLID project will be discussed. 2. Ways in which Linked Data principles and technologies can be used to support online learning and create innovative educational services will be explained, based on the experience developed in the development of existing Linked Data applications for online learning. We will in particular rely on the data catalogue, use cases and applications considered by the LinkedUp project.	Online learning and linked data: lessons learned and best practices	NA:NA:NA	2014
Manish Gupta:Rui Li:Kevin Chen-Chuan Chang	Microblog data differs significantly from the traditional text data with respect to a variety of dimensions. Microblog data contains short documents, SMS kind of language, and is full of code mixing. Though a lot of it is mere social babble, it also contains fresh news coming from human sensors at a humungous rate. Given such interesting characteristics, the world wide web community has witnessed a large number of research tasks for microblogging platforms recently. Event detection on Twitter is one of the most popular such tasks with a large number of applications. The proposed tutorial on social analytics for Twitter will contain three parts. In the first part, we will discuss research efforts towards detection of events from Twitter using both the tweet content as well as other external sources. We will also discuss various applications for which event detection mechanisms have been put to use. Merely detecting events is not enough. Applications require that the detector must be able to provide a good description of the event as well. In the second part, we will focus on describing events using the best phrase, event type, event timespan, and credibility. In the third part, we will discuss user profiling for Twitter with a special focus on user location prediction. We will conclude with a summary and thoughts on future directions.	Towards a social media analytics platform: event detection and user profiling for twitter	NA:NA:NA	2014
Ido Guy	In recent years, with the proliferation of the social web, users are exposed to an intensively growing social overload. Social recommender systems aim to address this overload and are becoming integral part of virtually any leading website, playing a key factor in its success. In this tutorial, we will review the broad domain of social recommender systems, the underlying techniques and methodologies; the data in use, recommended entities, and target population; evaluation techniques; applications; and open issues and challenges.	Tutorial on social recommender systems	NA	2014
Shonali Krishnaswamy:Yuan-Fang Li	The combination of the versatility of smart devices and the capabilities of semantic technologies forms a great foundation for a mobile Semantic Web that will contribute to further realising the true potential of both disciplines. Motivated by a service discovery and matchmaking example, this tutorial provides an overview of background knowledge in ontology languages, basic reasoning problems, and how they are applicable in the mobile environment. It aims at presenting a timely survey of state-of-the-art development and challenges on mobile ontology reasoning, focusing on the reasoning and optimization techniques developed in the mTableaux framework. Finally, the tutorial closes with a summary of important research problems and an outlook of future research directions in this area.	The mobile semantic web	NA:NA	2014
Kyumin Lee:James Caverlee:Calton Pu	This tutorial will introduce peer-reviewed research work on information quality on social systems. Specifically, we will address new threats such as social spam, campaigns, misinformation and crowdturfing, and overview modern techniques to improve information quality by revealing and detecting malicious participants (e.g., social spammers, content polluters and crowdturfers) and low quality contents.	Social spam, campaigns, misinformation and crowdturfing	NA:NA:NA	2014
Lyndon Nixon:Vasileios Mezaris:Raphael Troncy	NA	Re-using media on the web	NA:NA:NA	2014
Kostas Stefanidis:Vasilis Efthymiou:Melanie Herschel:Vassilis Christophides	This tutorial provides an overview of the key research results in the area of entity resolution that are relevant to addressing the new challenges in entity resolution posed by the Web of data, in which real world entities are described by interlinked data rather than documents. Since such descriptions are usually partial, overlapping and sometimes evolving, entity resolution emerges as a central problem both to increase dataset linking but also to search the Web of data for entities and their relations.	Entity resolution in the web of data	NA:NA:NA:NA	2014
Jie Tang:Jimeng Sun	Social influence occurs when one's opinions, emotions, or behaviors are affected by others, intentionally or unintentionally. In this article, we survey recent research progress on social influence analysis. In particular, we first give a brief overview of related background knowledge, and then discuss what is social influence. We try to answer this question in terms of homophily and the process of influence and selection. After that, we focus on describing computational models for social influence including models for influence probability learning and influence diffusion. Finally, we discuss potential applications of social influence.	Computational models for social influence analysis: [extended abstract]	NA:NA	2014
Jiliang Tang:Huan Liu	The rapid development of social media exacerbates the information overload and credibility problems. Trust, providing information about with whom we can trust to share information and from whom we can accept information, plays an important role in helping users collect relevant and reliable information in social media. Trust has become a research topic of increasing importance and of practical significance. In this tutorial, we illustrate properties and representation models of trust, elucidate trust measurements with representative algorithms, and demonstrate real-world applications where trust is explicitly used. As a new dimension of the trust study, we discuss the concept of distrust and its roles in trust measurements and applications.	Trust in social computing	NA:NA	2014
Lidan Wang:Jimmy Lin:Donald Metzler:Jiawei Han	Ranking in response to user queries is a central problem in information retrieval, data mining, and machine learning. In the era of "Big data", traditional effectiveness-centric ranking techniques tend to get more and more costly (requiring additional hardware and energy costs) to sustain reasonable ranking speed on large data. The mentality of combating big data by throwing in more hardware/machines will quickly become highly expensive since data is growing at an extremely fast rate oblivious to any cost concerns from us. "Learning to efficiently rank" offers a cost-effective solution to ranking on large data (e.g., billions of documents). That is, it addresses a critically important question -- whether it is possible to improve ranking effectiveness on large data without incurring (too much) additional cost?	Learning to efficiently rank on big data	NA:NA:NA:NA	2014
AmirMahdi Ahmadinejad:Sina Dehghani:MohammadTaghi Hajiaghayi:Hamid Mahini:Saeed Seddighin:Sadra Yazdanbod	People make decisions and express their opinions according to their communities. An appropriate idea for controlling the diffusion of an opinion is to find influential people, and employ them to spread the desired opinion. We investigate an influencing problem when individuals' opinions are affected by their friends due to the model of Friedkin and Johnsen [4]. Our goal is to design efficient algorithms for finding opinion leaders such that changing their opinions has great impact on the overall opinion of the society. We define a set of problems like maximizing the sum of individual opinions or maximizing the number of individuals whose opinions are above a threshold. We discuss the complexity of the defined problems and design optimum algorithms for the non NP-hard variants of the problems. Furthermore, we run simulations on real-world social network data and show our proposed algorithm outperforms the classical algorithms such as degree-based, closeness-based, and pagerank-based algorithms.	How effectively can we form opinions?	NA:NA:NA:NA:NA:NA	2014
Ahmedin Mohammed Ahmed:Qiuyuan Yang:Nana Yaw Asabere:Tie Qiu:Feng Xia	Although existing replica allocation protocols perform well in most cases, some challenges still need to be addressed to further improve their performance. The success of such protocols for Ad-hoc Social Networks (ASNETs) depends on the performance of data accessibility and on the easy consistency management of available replica. We contribute to this line of research with replication protocol for maximizing availability of a data. Essentially, we propose ComPAS, a community-partitioning aware replica allocation method. Its goals include integration of social relationship for placing copy of the data in the community to achieve better efficiency and consistency by keeping the replica read cost, relocation cost and traffic as low as possible.	ComPAS: maximizing data availability with replication in ad-hoc social networks	NA:NA:NA:NA:NA	2014
Xiang Ao:Ping Luo:Chengkai Li:Fuzhen Zhuang:Qing He:Zhongzhi Shi	This paper studies the problem of discovering and learning sensational 2-episodes, i.e., pairs of co-occurring news events. To find all frequent episodes, we propose an efficient algorithm, MEELO, which significantly outperforms conventional methods. Given many frequent episodes, we rank them by their sensational effect. Instead of limiting ourselves to any individual subjective measure of sensational effect, we propose a learning-to-rank approach that exploits multiple features to capture the sensational effect of an episode from various aspects. An experimental study on real data verified our approach's efficiency and effectiveness.	Discovering and learning sensational episodes of news events	NA:NA:NA:NA:NA:NA	2014
Marcelo Arenas:Bernardo Cuenca Grau:Evgeny Evgeny:Sarunas Marciuska:Dmitriy Zheleznyakov	In this paper we present limitations of conventional faceted search in the way data, facets, and queries are modelled. We discuss how these limitations can be addressed with Semantic Web technologies such as RDF, OWL 2, and SPARQL 1.1. We also present a system, SemFacet, that is a proof-of-concept prototype of our approach implemented on top of Yago knowledge base, powered by the OWL 2 RL triple store RDFox, and the full text search engine Lucene.	Towards semantic faceted search	NA:NA:NA:NA:NA	2014
Xi Bai:Armin Haller:Ewan Klein:Dave Robertson	A growing number of approaches and tools have been utilised attempting at generating hypertext content with embedded metadata. However, little work has been carried out on finding a generic solution for publishing and styling Web pages with annotations derived from existing RDF data sets available in various formats. This paper proposes a metadata-driven publishing framework assisting publishers or webmasters in generating semantically-enriched content (HTML pages or snippets) by harnessing distributed RDF(a) documents or repositories with little human intervention. This framework also helps users to create and share so-called micro-themes, which is applicable to the above generated content for the purpose of page styling and also highly reusable thanks to the adopted semantic attribute selectors.	Metadata-driven hypertext content publishing and styling	NA:NA:NA:NA	2014
Christian Bauckhage:Kristian Kersting:Bashir Rastegarpanah	We investigate patterns of adoption of 175 social media services and Web businesses using data from Google Trends. For each service, we collect aggregated search frequencies from 45 countries as well as global averages. This results in more than 8.000 time series which we analyze using economic diffusion models. The models are found to provide accurate and statistically significant fits to the data and show that collective attention to social media grows and subsides in a highly regular manner. Regularities persist across regions, cultures, and topics and thus hint at general mechanisms that govern the adoption of Web-based services.	Collective attention to social media evolves according to diffusion models	NA:NA:NA	2014
Marina Boia:Claudiu Cristian Musat:Boi Faltings	Many Artificial Intelligence tasks need commonsense knowledge. Extracting this knowledge with statistical methods would require huge amounts of data, so human computation offers a better alternative. We acquire contextual knowledge for sentiment analysis by asking workers to indicate the contexts that influence the polarities of sentiment words. The increased complexity of the task causes some workers to give superficial answers. To increase motivation, we make the task more engaging by packaging it as a game. With the knowledge compiled from only a small set of answers, we already halve the gap between machine and human performance. This proves the strong potential of human computation for acquiring commonsense knowledge.	Acquiring commonsense knowledge for sentiment analysis using human computation	NA:NA:NA	2014
Paolo Boldi:Andrea Marino:Massimo Santini:Sebastiano Vigna	Although web crawlers have been around for twenty years by now, there is virtually no freely available, open-source crawling software that guarantees high throughput, overcomes the limits of single-machine tools and at the same time scales linearly with the amount of resources available. This paper aims at filling this gap.	BUbiNG: massive crawling for the masses	NA:NA:NA:NA	2014
Christina Brandt:Jure Leskovec	We examine the evolution of five social networking sites where complex networks of social relationships developed: Twitter, Flickr, DeviantArt, Delicious, and Yahoo! Answers. We study the differences and similarities in edge creation mechanisms in these social networks. We find large differences in edge reciprocation rates and overall structure of the underlying networks. We demonstrate that two mechanisms can explain these disparities: directed triadic closure, which leads to networks that show characteristics of status-oriented behavior, and reciprocation, which leads to friendship-oriented behavior. We develop a model that demonstrates how variances in these mechanisms lead to characteristic differences in the expression of network subgraph motifs. Lastly, we show how a user's future popularity, her indegree, can be predicted based on her initial edge creation behavior.	Status and friendship: mechanisms of social network evolution	NA:NA	2014
Pavel Arapov:Michel Buffa:Amel Ben Othmane	WikiNEXT is a wiki engine that enables users to write rapidly applications directly from the browser, in particular applications that can exploit the web of data. WikiNEXT relies on semantic web formalisms and technologies (RDF/RDFa lite) to describe wiki page content and embedded metadata, and to manipulate them (for example, using the SPARQL language). WikiNEXT is a mix between a web-based IDE (Integrated Development Environment) and a semantic wiki. It embeds several editors (a WYSIWYG editor, and an HTML/JavaScript editor + a JavaScript library manager) for coding in the browser, provides an API for exploiting semantic metadata, and uses a graph based data store and an object oriented database for persistence on the server side. It has been specially designed for writing online programming tutorials (i.e. an HTML5 tutorial, a semantic web tutorial on how to consume linked data, etc.), or more generally for developing web applications that can be mixed with more classical wiki documents (in fact all WikiNEXT pages are web applications). The tool is online, open source ; screencasts are available on YouTube (look for 'WikiNEXT').	A wiki way of programming for the web of data	NA:NA:NA	2014
Xiaochun Cao:Xiao Wang:Di Jin:Yixin Cao:Dongxiao He	The detection of communities in various networks has been considered by many researchers. Moreover, it is preferable for a community detection method to detect hubs and outliers as well. This becomes even more interesting and challenging when taking the unsupervised assumption, that is, we do not assume the prior knowledge of the number K of communities. In this poster, we define a novel model to identify overlapping communities as well as hubs and outliers. When K is given, we propose a normalized symmetric nonnegative matrix factorization algorithm to learn the parameters of the model. Otherwise, we introduce a Bayesian symmetric nonnegative matrix factorization to learn the parameters of the model, while determining K. Our experiment indicate its superior performance on various networks.	The (un)supervised detection of overlapping communities as well as hubs and outliers via (bayesian) NMF	NA:NA:NA:NA:NA	2014
Chih-Chun Chan:Yu-Chieh Lin:Ming-Syan Chen	Mobile devices, especially smart phones, have been popular in recent years. With users spending much time on mobile devices, service providers deliver advertising messages to mobile device users and look forward to increasing their revenue. However, delivery of proper advertising messages is challenging since strategies of advertising in TV, SMS, or website may not be applied to the banner-based advertising on mobile devices. In this work, we study how to properly recommend advertising messages for mobile device users. We propose a novel approach which simultaneously considers several important factors: user profile, apps used, and clicking history. We apply experiments on real-world mobile log data, and the results demonstrate the effectiveness of the proposed approach.	Recommendation for advertising messages on mobile devices	NA:NA:NA	2014
Fei Chen:Yiqun Liu:Jian Li:Min Zhang:Shaoping Ma	Given a number of possible sub-intents (also called subtopics) for a certain query and their corresponding search results, diversified search aims to return a single result list that could satisfy as many users' intents as possible. Previous studies have demonstrated that finding the optimal solution for diversified search is NP-hard. Therefore, several algorithms have been proposed to obtain a local optimal ranking with greedy approximations. In this paper, a pruned exhaustive search algorithm is proposed to decrease the complexity of the optimal search for the diversified search problem. Experimental results indicate that the proposed algorithm can decrease the computation complexity of exhaustive search without any performance loss.	A pruning algorithm for optimal diversified search	NA:NA:NA:NA:NA	2014
Li Chen:Feng Wang	Because of the important role of product reviews during users' decision process, we propose a novel explanation interface that particularly fuses the feature sentiments as extracted from reviews into explaining recommendations. Besides, it can explain multiple items altogether by revealing their similarity in respect of feature sentiments as well as static specifications, so as to support users' tradeoff making. Relative to existing works, we believe that this interface can be more effective, trustworthy, and persuasive.	Sentiment-enhanced explanation of product recommendations	NA:NA	2014
Zhiyuan Cheng:James Caverlee:Himanshu Barthwal:Vandana Bachani	We address the problem of identifying local experts on Twitter. Specifically, we propose a local expertise framework that integrates both users' topical expertise and their local authority by leveraging over 15 million geo-tagged Twitter lists. We evaluate the proposed approach across 16 queries coupled with over 2,000 individual judgments from Amazon Mechanical Turk. Our initial experiments find significant improvement over a naive local expert finding approach, suggesting the promise of exploiting geo-tagged Twitter lists for local expert finding.	Finding local experts on twitter	NA:NA:NA:NA	2014
Luca Costabello:Fabien Gandon	We present PRISSMA, a context-aware presentation layer for Linked Data. PRISSMA extends the Fresnel vocabulary with the notion of mobile context. Besides, it includes an algorithm that determines whether the sensed context is compatible with some context declarations.	Adaptive presentation of linked data on mobile	NA:NA	2014
Paolo Cremonesi:Franca Garzotto:Roberto Pagano:Massimo Quadrana	We discuss a comprehensive study exploring the impact of recommender systems when recommendations are forced to omit popular items (short head) and to use niche products only (long tail). This is an interesting issue in domains, such as e-tourism, where product availability is constrained, "best sellers" most popular items are the first ones to be consumed, and the short head may eventually become unavailable for recommendation purposes. Our work provides evidence that the effects resulting from item consumption may increase the utility of personalized recommendations.	Recommending without short head	NA:NA:NA:NA	2014
Atish Das Sarma:Si Si:Elizabeth F. Churchill:Neel Sundaresan	While recommendation profiles increasingly leverage social actions such as "shares", the predictive significance of such actions is unclear. To what extent do public shares correlate with other online behaviors such as searches, views and purchases? Based on an analysis of 950,000 users' behavioral, transactional, and social sharing data on a global online commerce platform, we show that social "shares", or publicly posted expressions of interest do not correlate with non-public behaviors such as views and purchases. A key takeaway is that there is a "gap" between public and non-public actions online, suggesting that marketers and advertisers need to be cautious in their estimation of the significance of social sharing.	The "expression gap": do you like what you share?	NA:NA:NA:NA	2014
Anastasia Dimou:Miel Vander Sande:Tom De Nies:Erik Mannens:Rik Van de Walle	The missing feedback loop is considered the reason for broken Data Cycles on current Linked Open Data ecosystems. Read-Write platforms are proposed, but they are restricted to capture modifications after the data is released as Linked Data. Triggering though a new iteration results in loosing the data consumers' modifications, as a new version of the source data is mapped, overwriting the currently published. We propose a prime solution that interprets the data consumers' feedback to update the mapping rules. This way, data publishers initiate a new iteration of the Data Cycle considering the data consumers' feedback when they map a new version of the published data.	RDF mapping rules refinements according to data consumers' feedback	NA:NA:NA:NA:NA	2014
Stephan Doerfel:Daniel Zoller:Philipp Singer:Thomas Niebler:Andreas Hotho:Markus Strohmaier	Social tagging systems have established themselves as an important part in today's web and have attracted the interest of our research community in a variety of investigations. This has led to several assumptions about tagging, such as that tagging systems exhibit a social component. In this work we overcome the previous absence of data for testing such an assumption. We thoroughly study social interaction, leveraging for the first time live log data gathered from the real-world public social tagging system \bibs. Our results indicate that sharing of resources constitutes an important and indeed social aspect of tagging.	How social is social tagging?	NA:NA:NA:NA:NA:NA	2014
Wei Dong:Minghui Qiu:Feida Zhu	Users often manage which aspects of their personal identities to be manifested on social network sites (SNS). Thus, the content of personal information disclosed on users' profiles can be influenced by a number of factors, such as motivation of using SNS and privacy concerns, both of which may vary depending on where users reside in. In this study, we compared the content of 2800 United States (US) and Singapore (SG) Twitter users' bios on their profile pages. We found US Twitter users were far more likely to disclose personal information that may reveal their true identity than SG users. The between country difference remained after we took bio length and user activity level into account. The results provide important insights on future studies to understand users' privacy concern in different regions of the world.	Who am I on twitter?: a cross-country comparison	NA:NA:NA	2014
Jiang Du:Peiquan Jin:Lizhou Zheng:Shouhong Wan:Lihua Yue	DBLP is a well-known online computer science bibliography. As nearly all important journals and conferences on computer science are tracked in DBLP, how to effectively search DBLP records has become a valuable topic for the computer science community. In this paper we present DBLP-Filter, a new DBLP search tool. The major features of DBLP-Filter are: (1) it provides new search options on concepts and literature importance; (2) it can maintain user profiles and can support user-area-aware search; (3) it provides the service of new literatures alert. Compared with the existing DBLP search tools, DBLP-Filter is more functional and also shows better effectiveness in terms of MAP and F-measure when tested under a set of randomly-selected queries.	DBLP-filter: effectively search on the DBLP bibliography	NA:NA:NA:NA:NA	2014
Jingfei Du:Yan Song:Chi-Ho Li	Query boundaries carry useful information for query segmentation, especially when analyzing queries in a language with no space, e.g., Chinese. This paper presents our research on Chinese query segmentation via averaged perceptron to model query boundaries through an L-R tagging scheme on a large amount of unlabeled queries. Experimental results indicate that query boundaries are very informative and they significantly improve supervised Chinese query segmentation when labeled training data is very limited.	Perceptron-based tagging of query boundaries for Chinese query segmentation	NA:NA:NA	2014
Kai Eckert:Dominique Ritze:Konstantin Baierer:Christian Bizer	In this paper, we present a workflow model together with an implementation following the Linked Data principles and the principles for RESTful web services. By means of RDF-based specifications of web services, workflows, and runtime information, we establish a full provenance chain for all resources created within these workflows.	RESTful open workflows for data provenance and reuse	NA:NA:NA:NA	2014
Besnik Fetahu:Stefan Dietze:Bernardo Pereira Nunes:Marco Antonio Casanova:Davide Taibi:Wolfgang Nejdl	NA	What's all the data about?: creating structured profiles of linked data on the web	NA:NA:NA:NA:NA:NA	2014
Hao Fu:Aston Zhang:Xing Xie	Recently, a number of anonymization algorithms have been developed to protect the privacy of social graph data. However, in order to satisfy higher level of privacy requirements, it is sometimes impossible to maintain sufficient utility. Is it really easy to de-anonymize "lightly" anonymized social graphs? Here "light" anonymization algorithms stand for those algorithms that maintain higher data utility. To answer this question, we proposed a de-anonymization algorithm based on a node similarity measurement. Using the proposed algorithm, we evaluated the privacy risk of several "light" anonymization algorithms on real datasets.	De-anonymizing social graphs via node similarity	NA:NA:NA	2014
Ariel Fuxman:Patrick Pantel:Yuanhua Lv:Ashok Chandra:Pradeep Chilakamarri:Michael Gamon:David Hamilton:Bernhard Kohlmeier:Dhyanesh Narayanan:Evangelos Papalexakis:Bo Zhao	In today's productivity environment, users are constantly researching topics while consuming or authoring content in applications such as e-readers, word processors, presentation programs, or social networks. However, none of these applications sufficiently enable users to do their research directly within the application. In fact, users typically have to switch to a browser and write a query on a search engine. Switching to a search engine is distracting and hurts productivity. Furthermore, the main problem is that the search engine is not aware of important user context such as the book that they are reading or the document they are authoring. To tackle this problem, we introduce the notion of contextual insights: providing users with information that is contextually relevant to the content that they are consuming or authoring. We then present Leibniz, a system that provides a solution for the contextual insights problem.	Contextual insights	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2014
Luis Galárraga:Katja Hose:Ralf Schenkel	The increasing interest in Semantic Web technologies has led not only to a rapid growth of semantic data on the Web but also to an increasing number of backend applications relying on efficient query processing. Confronted with such a trend, existing centralized state-of-the-art systems for storing RDF and processing SPARQL queries are no longer sufficient. In this paper, we introduce Partout, a distributed engine for fast RDF processing in a cluster of machines. We propose an effective approach for fragmenting RDF data sets based on a query log and allocating the fragments to hosts in a cluster of machines. Furthermore, Partout's query optimizer produces efficient query execution plans for ad-hoc SPARQL queries.	Partout: a distributed engine for efficient RDF processing	NA:NA:NA	2014
Shuai Gao:Jun Ma:Zhumin Chen	Predicting popularity of online contents is of remarkable practical value in various business and administrative applications. Existing studies mainly focus on finding the most effective features for prediction. However, some effective features, such as structural features which are extracted from the underlying user network, are hard to access. In this paper, we aim to identify features that are both effective and effortless (easy to obtain or compute). Experiments on Sina Weibo show the effectiveness and effortlessness of the temporal features and satisfying prediction performance can be obtained based on only the temporal features of first 10 retweets.	Effective and effortless features for popularity prediction in microblogging network	NA:NA:NA	2014
Yang Gao:Yue Xu:Yuefeng Li	Topic modelling has been widely used in the fields of information retrieval, text mining, machine learning, etc. In this paper, we propose a novel model, Pattern Enhanced Topic Model (PETM), which makes improvements to topic modelling by semantically representing topics with discriminative patterns, and also makes innovative contributions to information filtering by utilising the proposed PETM to determine document relevance based on topics distribution and maximum matched patterns proposed in this paper. Extensive experiments are conducted to evaluate the effectiveness of PETM by using the TREC data collection Reuters Corpus Volume 1. The results show that the proposed model significantly outperforms both state-of-the-art term-based models and pattern-based models.	A topic based document relevance ranking model	NA:NA:NA	2014
Viktors Garkavijs:RIka Okamoto:Tetsuo Ishikawa:Mayumi Toshima:Noriko Kando	We propose two methods for exploratory image search systems using gaze data for continuous learning of the users' interests and relevance calculation. The first system uses the fixation time over the images selected by gaze in the search results pages, whereas the second one utilizes the fixation time over the clicked images and fixations over the non-selected images on the results page. A user model is trained and continuously updated from the gaze input throughout the whole session in both systems. We conducted an experiment with 24 users, each performing four search tasks using the proposed systems and compared the results to a baseline system, which does not employ any gaze data. The Gaze feedback system users viewed 22.35% more images than the users of the baseline system. A high correlation between the number of saved images and the satisfaction with the results was observed in data collected from the users of a mouse feedback system enriched by gaze data. The results show that including the gaze data into the relevance calculation in both cases increases the degree of satisfaction with the search results compared with the baseline.	GLASE-IRUKA: gaze feedback improves satisfaction in exploratory image search	NA:NA:NA:NA:NA	2014
Tao Ge:Wenjie Li:Zhifang Sui	This paper proposes a semi-supervised self-learning method, which is based on a Naive Bayes classifier exploiting context features and PMI scores, to extract opinion targets. The experimental results indicate our bootstrapping framework is effective for this task and outperforms the state-of-the-art models on COAE2008 dataset2, especially in precision.	A semi-supervised method for opinion target extraction	NA:NA:NA	2014
Ekaterina Gladkikh:Kirill Nikolaev:Mikhail Nikitin	The paper describes the experience of resisting the large-scale solving of CAPTCHA through the CAPTCHA-farms and presents the results of experimenting with different types of textual CAPTCHA on the farm worker and casual user crowds. Localization of CAPTCHA led to cutting twice the absolute volume of CAPTCHA parsing, but introducing the semantics into the test complicated it to casual users and was not found promising.	Localized CAPTCHA testing on users and farms	NA:NA:NA	2014
Gagan Goel:Afshin Nikzad:Adish Singla	Designing optimal pricing policies and mechanisms for allocating tasks to workers is central to the online crowdsourcing markets. In this paper, we consider the following realistic setting of online crowdsourcing markets -- there is a requester with a limited budget and a heterogeneous set of tasks each requiring certain skills; there is a pool of workers and each worker has certain expertise and interests which define the set of tasks she can and is willing to do. Under the matching constraints given by this bipartite graph between workers and tasks, we design our incentive-compatible mechanism truthuniform which allocates the tasks to the workers, while ensuring budget feasibility and achieves near-optimal utility for the requester. Apart from strong theoretical guarantees, we carry out experiments on a realistic case study of Wikipedia translation project on Mechanical Turk. We note that this is the first paper to address this setting from a mechanism design perspective.	Allocating tasks to workers with matching constraints: truthful mechanisms for crowdsourcing markets	NA:NA:NA	2014
Eduardo Graells-Garrido:Mounia Lalmas:Daniele Quercia	In online social networks, people tend to connect with like-minded people and read agreeable information. Direct recommendation of challenging content has not worked well because users do not value diversity and avoid challenging content. In this poster, we investigate the possibility of an indirect approach by introducing intermediary topics, which are topics that are common to people having opposing views on sensitive issues, i.e., those issues that tend to divide people. Through a case study about a sensitive issue discussed in Twitter, we show that such intermediary topics exist, opening a path for future work in recommendation promoting diversity of content to be shared.	People of opposing views can share common interests	NA:NA:NA	2014
Mihajlo Grbovic:Slobodan Vucetic	Determining the right audience for an advertising campaign is a well-established problem, of central importance to many Internet companies. Two distinct targeting approaches exist, the model-based approach, which leverages machine learning, and the rule-based approach, which relies on manual generation of targeting rules. Common rules include identifying users that had interactions (website visits, emails received, etc.) with the companies related to the advertiser, or search queries related to their product. We consider a problem of discovering such rules from data using Constrained Sparse PCA. The constraints are put in place to account for cases when evidence in data suggests a relation that is not appropriate for advertising. Experiments on real-world data indicate the potential of the proposed approach.	Generating ad targeting rules using sparse principal component analysis with constraints	NA:NA	2014
Manish Gupta:Prashant Borole:Praful Hebbar:Rupesh Mehta:Niranjan Nayak	Given a query, the query-entity (QE) matching task involves identifying the best matching entity for the query. When modeling this task as a binary classification problem, two issues arise: (1) features in specific global markets (like de-at: German users in Austria) are quite sparse compared to other markets like en-us, and (2) training data is expensive to obtain in multiple markets and hence limited. Can we leverage some form of cross market data/features for effective query-entity matching in sparse markets? Our solution consists of three main modules: (1) Cross Market Training Data Leverage (CMTDL) (2) Cross Market Feature Leverage (CMFL), and (3) Cross Market Output Data Leverage (CMODL). Each of these parts perform "signal" sharing at different points during the classification process. Using a combination of these strategies, we show significant improvements in query-impression weighted coverage for the query-entity matching task.	Cross market modeling for query-entity matching	NA:NA:NA:NA:NA	2014
Harry Halpin:James Cheney	While the (Semantic) Web currently does have a way to exhibit static provenance information in the W3C PROV standards, the Web does not have a way to describe dynamic changes to data. While some provenance models and annotation techniques originally developed with databases or workflows in mind transfer readily to RDF, RDFS and SPARQL, these techniques do not readily adapt to describing changes in dynamic RDF datasets over time. In this paper we explore how to adapt the dynamic copy-paste provenance model of Buneman et al. to RDF datasets that change over time in response to SPARQL updates, how to represent the resulting provenance records themselves as RDF using named graphs in a manner compatible with W3C PROV, and how the provenance information can be provided as a SPARQL query. The primary contribution is a semantic framework that enables the semantics of SPARQL Update to be used as the basis for a `cut-and-paste' provenance model in a principled manner.	Dynamic provenance for SPARQL updates using named graphs	NA:NA	2014
Jonghyun Han:Hyunju Lee	It is often hard to accurately estimate interests of social media users because their messages do not have additional information, such as a category. In this paper, we propose an approach that estimates user interest from social media to provide personalized services. Our approach employs heterogeneous media to map social media onto categories. To describe the categories, we propose a hybrid method that integrates a topic model with TF-ICF for extracting both explicitly presented and implicitly latent features. Our evaluation result shows that it gives the highest performance, compared to other approaches. Thus, we expect that the proposed approach is helpful in advancing personalization of social media services.	Characterizing user interest using heterogeneous media	NA:NA	2014
Frederik Hogenboom:Michel Capelle:Marnix Moerland:Flavius Frasincar	Content-based news recommendation is traditionally performed using the cosine similarity and TF-IDF weighting scheme for terms occurring in news messages and user profiles. Semantics-driven variants such as SF-IDF additionally take into account term meaning by exploiting synsets from semantic lexicons. However, they ignore the various semantic relationships between synsets, providing only for a limited understanding of news semantics. Moreover, semantics-based weighting techniques are not able to handle - often crucial - named entities, which are usually not present in semantic lexicons. Hence, we extend SF-IDF by also considering the synset semantic relationships, and by employing named entity similarities using Bing page counts. Our proposed method, Bing-SF-IDF+, outperforms TF-IDF and SF-IDF in terms of F1 scores and kappa statistics.	Bing-SF-IDF+: semantics-driven news recommendation	NA:NA:NA:NA	2014
Hsun-Ping Hsieh:Cheng-Te Li	While mobile sensors are ubiquitous nowadays, the geographical activities of human beings are feasible to be collected and the geo-spatial interactions between people can be derived. As we know there is an underlying social network between mobile users, such social relationships are hidden and hold by service providers. Acquiring the social network over mobile users would enable lots of applications, such as friend recommendation and energy-saving mobile DB management. In this paper, we propose to infer the social relationships using the sensor data, which contains the encounter records between individuals, without any knowledge about the real friendships in prior. We propose a two-phase prediction method for the social inference. Experiments conducted on the CRAWDAD data demonstrate the encouraging results with satisfying prediction scores of precision and recall.	Inferring social relationships from mobile sensor data	NA:NA	2014
Hsun-Ping Hsieh:Cheng-Te Li	Online location-based services, such as Foursquare and Facebook, provide a great resource for location recommendation. As we know the time is one of the important factors on recommending places with proper time for users, since the pleasure of visiting a place could be diminished if arriving at wrong time, we propose to infer the visiting time distributions of locations. We assume the check-in data used is incomplete because in real-world scenarios it is hard or unavailable to collect all the temporal information of locations and the check-in behaviors might be abnormal. To tackle such problem, we devise a visiting time inference framework, VisTime-Miner, which considers the route-based visiting correlation of time labels to model the visiting behavior of a location. Experiments on a large-scaled Gowalla check-in data show a promising result.	Inferring visiting time distributions of locations from incomplete check-in data	NA:NA	2014
Qingbo Hu:Guan Wang:Philip S. Yu	Online video websites receive huge amount of videos daily from users all around the world. How to provide valuable recommendation of videos to viewers is important for video websites. Previous studies focus on analyzing the view count of a video, which measures the video's value in terms of popularity. However, the long-lasting value of an online video, namely longevity, is hidden behind the history that a video accumulates its "popularity" through time. Generally speaking, a longevous video tends to constantly draw society's attention. With a focus on Youtube, this paper proposes a scoring mechanism to quantify the longevity of videos. We introduce the concept of latent social impulses and use them to assess a video's longevity. In order to derive latent social impulses, we view the video website as a digital signal filter and formulate the task as a convex minimization problem. The proposed longevity computation is based on the derived social impulses. Unfortunately, the required information to derive social impulses is not always public, which disallows a third party to directly evaluate the longevity of all videos. Thus, we formulate a semi-supervised learning task by using videos of which the longevity scores are known to predict the unknown ones. We develop a Gaussian Random Markov Field model with Loopy Belief Propagation to solve it. The experiments on Youtube demonstrate that the proposed method significantly improves the prediction results comparing to two baseline models.	Deriving latent social impulses to determine longevous videos	NA:NA:NA	2014
Won-Seok Hwang:Shaoyu Li:Sang-Wook Kim:Kichun Lee	Recommendation methods suffer from the data sparsity and cold-start user problems, often resulting in low accuracy. To address these problems, we propose a novel imputation method, which effectively densifies a rating matrix by filling unevaluated ratings with probable values. In our method, we use a trust network to estimate the unevaluated ratings accurately. We conduct experiments on the Epinions dataset and demonstrate that our method helps provide better recommendation accuracy than previous methods, especially for cold-start users.	Data imputation using a trust network for recommendation	NA:NA:NA:NA	2014
Vidit Jain:Jay Mahadeokar	Usual text document representations such as tf-idf do not work well in classification tasks for short-text documents and across diverse data domains. Optimizing different representations for different data domains is infeasible in a practical setting on the Internet. Mining such representations from the data in an unsupervised manner is desirable. In this paper, we study a representation based on the multi-scale harmonic analysis of term-term co-occurrence graph. This representation is not only sparse, but also leads to the discovery of semantically coherent topics in data. In our experiments on user-generated short documents e.g., newsgroup messages, user comments, and meta-data, we found this representation to outperform other representations across different choice of classifiers. Similar improvements were also observed for data sets in Chinese and Portuguese languages.	Short-text representation using diffusion wavelets	NA:NA	2014
Min-Hee Jang:Christos Faloutsos:Sang-Wook Kim	We propose a novel method to predict accurately trust relationships of a target user even if he/she does not have much interaction information. The proposed method considers positive, implicit, and negative information of all users in a network based on belief propagation to predict trust relationships of a target user.	Trust prediction using positive, implicit, and negative information	NA:NA:NA	2014
Meng Jiang:Peng Cui:Alex Beutel:Christos Faloutsos:Shiqiang Yang	In a multimillion-node network of who-follows-whom like Twitter, since a high count of followers leads to higher profits, users have the incentive to boost their in-degree. Can we spot the suspicious following behavior, which may indicate zombie followers and suspicious followees? To answer the above question, we propose CatchSync, which exploits two tell-tale signs of the suspicious behavior: (a) synchronized behavior: the zombie followers have extremely similar following behavior pattern, because, say, they are generated by a script; and (b) abnormal behavior: their behavior pattern is very different from the majority. Our CatchSync introduces novel measures to quantify both concepts and catches the suspicious behavior. Moreover, we show it is effective in a real-world social network.	Detecting suspicious following behavior in multimillion-node social networks	NA:NA:NA:NA:NA	2014
Angel Jimenez-Molina:In-Young Ko	The proactive and spontaneous delivery of services for mobile users on the move can lead to the depletion of users' mental resources, affecting the normal processes of their physical activities. This is due to the competition for limited mental resources between the human-computer interactions required by services and the users' physical activities. To deal with this problem we propose a service selection method based on two theories from cognitive psychology. This mechanism assesses the degree of demand for mental resources by both the physical activities and the services. Additionally, a service binding and scheduling algorithm ensures less cognitively-taxing mobile service compositions.	Cognitive resource-aware web service selection in mobile computing environments	NA:NA	2014
Janani Kalyanam:Gert R.G. Lanckriet	Information in today's world is highly heterogeneous and unstructured. Learning and inferring from such data is challenging and is an active research topic. In this paper, we present and investigate an approach to learning from heterogeneous and unstructured multimedia data. Inspired by approaches in many fields including computer vision, we investigate a histogram based approach to represent multimodal unstructured data. While existing works have predominantly focused on histogram based approaches for unimodal data, we present a methodology to represent unstructured multimodal data. We explain how to discover the prototypical features or codewords over which these histograms are built. We present experimental results on classification and retrieval tasks performed on the histogram based representation.	Learning from unstructured multimedia data	NA:NA	2014
Pavan Kapanipathi:Prateek Jain:Chitra Venkataramani:Amit Sheth	Industry and researchers have identified numerous ways to monetize microblogs for personalization and recommendation. A common challenge across these different works is the identification of user interests. Although techniques have been developed to address this challenge, a flexible approach that spans multiple levels of granularity in user interests has not been forthcoming. In this work, we focus on exploiting hierarchical semantics of concepts to infer richer user interests expressed as a Hierarchical Interest Graph. To create such graphs, we utilize users' tweets to first ground potential user interests to structured background knowledge such as Wikipedia Category Graph. We then adapt spreading activation theory to assign user interest score to each category in the hierarchy. The Hierarchical Interest Graph not only comprises of users' explicitly mentioned interests determined from Twitter, but also their implicit interest categories inferred from the background knowledge source.	Hierarchical interest graph from tweets	NA:NA:NA:NA	2014
Makoto P. Kato:Takehiro Yamamoto:Hiroaki Ohshima:Katsumi Tanaka	This study investigated query formulations by users with Cognitive Search Intents (CSI), which are needs for the cognitive characteristics of documents to be retrieved, eg. comprehensibility, subjectivity, and concreteness. We proposed an example-based method of specifying search intents to observe unbiased query formulations. Our user study revealed that about half our subjects did not input any keywords representing CSIs, even though they were conscious of given CSIs.	Cognitive search intents hidden behind queries: a user study on query formulations	NA:NA:NA:NA	2014
Youngjoon Ki:Jiyoung Woo:Huy Kang Kim	Massively multiplayer online role-playing games (MMORPGs) simulate the real world and require highly complex user interaction. Many human behaviors can be observed in MMORPGs, such as social interactions, economic behaviors, and malicious behaviors. In this study, we primarily focus on malicious behavior, especially cheating using game bots. Bots can be diffused on the social network in an epidemic style. When bots diffuse on the social network, a user's influence on the diffusion process varies owing to different characteristics and network positions. We aim to identify the influentials in the game world and investigate how they differ from normal users. Identifying the influentials in the diffusion of malicious behaviors will enable the game company to act proactively and preventively towards malicious users such as game bot users.	Identifying spreaders of malicious behaviors in online games	NA:NA:NA	2014
Jungeun Kim:Minsoo Choy:Daehoon Kim:U Kang	Understanding of which new interactions among data objects are likely to occur in the future is crucial for a deeper understanding of network dynamics and evolution. This question is largely unexplored except a local neighborhood perspective, partly owing to the difficulty in finding major factors which heavily affect the link prediction problem. In this paper, we propose LPCSP, a novel link prediction method which exploits the generalized cluster information containing cluster relations and cluster evolution information. Experiments show that our proposed LPCSP is accurate, scalable, and useful for link prediction on real world graphs.	Link prediction based on generalized cluster information	NA:NA:NA:NA	2014
Kanghak Kim:Sunho Lee:Jeonghoon Son:Meeyoung Cha	Question & Answer (Q&A) behaviors on social media have huge potential as a rich source of information and knowledge online. However, little is known about how much diversity there exists in the topics covered in such Q&As and whether unstructured social media data can be made searchable. This paper seeks the feasibility of utilizing social media data for developing a Q&A service by examining the topic coverage in Twitter conversations. We propose a new framework to automatically extract informative Q&A content using machine learning techniques.	Finding informative Q&As on twitter	NA:NA:NA:NA	2014
Minkyoung Kim:David Newth:Peter Christen	This study proposes a model-free approach to infer macro-level information flow across online social systems in terms of the strength and directionality of influence among systems.	Macro-level information transfer across social networks	NA:NA:NA	2014
Yeooul Kim:Suin Kim:Alejandro Jaimes:Alice Oh	Agenda setting theory explains how media affects its audience. While traditional media studies have done extensive research on agenda setting, there are important limitations in those studies, including using a small set of issues, running costly surveys of public interest, and manually categorizing the articles into positive and negative frames. In this paper, we propose to tackle these limitations with a computational approach and a large dataset of online news. Overall, we demonstrate how to carry out a large-scale computational research of agenda setting with online news data using machine learning.	A computational analysis of agenda setting	NA:NA:NA:NA	2014
Yonghwan Kim:Dahee Lee:Jung Eun Hahm:Namgi Han:Min Song	In this paper we investigated the socio-cultural behavior of users reflected in the two different social media channels, YouTube and Twitter. We conducted the comparative analysis of the networks generated from the two channels. The relationship we set for each network is the relatedness on YouTube and the co-links on Twitter. From the results, we revealed that the social media influenced the distinct socio-cultural behaviors of their users. Specifically, Twitter network better showed the actual consumption of contents in the field of the k-pop culture than YouTube. From this study, we contributed to offer a novel approach for exploring the socio-cultural behavior of users on the social media.	Investigating socio-cultural behavior of users reflected in different social channels on K-pop	NA:NA:NA:NA:NA	2014
Jihoon Ko:Sangjin Shin:Sungkwang Eom:Minjae Song:Dong-Hoon Shin:Kyong-Ho Lee:Yongil Jang	To apply semantic search to smartphones, we propose an efficient semantic search method based on a lightweight mobile ontology. Through a prototype implementation of a semantic search engine on an android smartphone, experimental results show that the proposed method provides more accurate search results and a better user experience compared to the conventional method.	Semantically enhanced keyword search for smartphones	NA:NA:NA:NA:NA:NA:NA	2014
Minsam Ko:Seung-woo Choi:Joonwon Lee:Subin Yang:Uichin Lee:Aviv Segev:Junehwa Song	Recent advances of social TV services allow sports fans to watch sports games at any place and to lively interact with other fans via online chatting. Despite their popularity, however, so far little is known about the key properties of such mass interactions in online sports viewing. In this paper, we explore motives for mass interactions in online sports viewing and investigate how the motives are related to viewing and chatting behaviors, by studying Naver Sports, the largest online sports viewing service in Korea.	Motives for mass interactions in online sports viewing	NA:NA:NA:NA:NA:NA:NA	2014
Natalia Kudryashova	Internet search market of key words attracts much attention in conjunction with the legal proceedings against Google. It has been recognized that legal argumentation alone may not be sufficient to disentangle the complexity of the case. An approach that includes mathematical modeling is needed, to distinguish the effects of the factors intrinsic to the market and the consequences of anticompetitive practices. This pa- per proposes a modeling framework based on explicit treatment of users' switching between the search platforms in the environment set by the platforms' strategic decisions, and demonstrates some of its applications.	The market of internet sponsored links in the context of competition law: can modeling help?	NA	2014
Neeraj Kumar:Steven M. Seitz	We describe a system for searching your personal photos using an extremely wide range of text queries, including dates and holidays ("Halloween"), named and categorical places ("Empire State Building" or "park"), events and occasions ("Radiohead concert" or "wedding"), activities ("skiing"), object categories ("whales"), attributes ("outdoors"), and object instances ("Mona Lisa"), and any combination of these -- all with no manual labeling required. We accomplish this by correlating information in your photos -- the timestamps, GPS locations, and image pixels -- to information mined from the Internet. This includes matching dates to holidays listed on Wikipedia, GPS coordinates to places listed on Wikimapia, places and dates to find named events using Google, visual categories using classifiers either pre-trained on ImageNet or trained on-the-fly using results from Google Image Search, and object instances using interest point-based matching, again using results from Google Images. We tie all of these disparate sources of information together in a unified way, allowing for fast and accurate searches using whatever information you remember about a photo.	Photo recall: using the internet to label your photos	NA:NA	2014
Chi-Hoon Lee:HengShuai Yao:Xu He:Su Han Chan:JieYang Chang:Farzin Maghoul	Among the many tasks driven by very large scaled web search queries, it is an interesting task to predict how likely queries about a topic become popular (a.k.a. trending or buzzing) as the news in the near future, which is known as "Detecting trending queries." This task is nontrivial since the realization of buzzing trends of queries often requires sufficient statistics through users' activities. To address this challenge, we propose a novel framework that predicts whether queries become trending in the future. In principle, our system is built on the two learners. The first is to learn dynamics of time series for queries. The second, our decision maker, is to learn a binary classifier that determines whether queries become trending. Our framework is extremely efficient to be built taking advantage of the grid architecture that allows to deal with the large volume of data. In addition, it is flexible to continuously adapt as trending patterns evolve. The experiments results show that our approach achieves high quality of accuracy (over 77.5%} true positive rate) and yet detects much earlier (on average 29 hours advanced) than that of the baseline system.	Learning to predict trending queries: classification - based	NA:NA:NA:NA:NA:NA	2014
Lung-Hao Lee:Kuei-Ching Lee:Yen-Cheng Juan:Hsin-Hsi Chen:Yuen-Hsien Tseng	This study explores the users' web browsing behaviors that confront phishing situations for context-aware phishing detection. We extract discriminative features of each clicked URL, i.e., domain name, bag-of-words, generic Top-Level Domains, IP address, and port number, to develop a linear chain CRF model for users' behavioral prediction. Large-scale experiments show that our method achieves promising performance for predicting the phishing threats of users' next accesses. Error analysis indicates that our model results in a favorably low false positive rate. In practice, our solution is complementary to the existing anti-phishing techniques for cost-effectively blocking phishing threats from users' behavioral perspectives.	Users' behavioral prediction for phishing detection	NA:NA:NA:NA:NA	2014
Min-Joong Lee:Chin-Wan Chung	The betweenness centrality is a measure for the relative participation of the vertex in the shortest paths in the graph. In many cases, we are interested in the k-highest betweenness centrality vertices only rather than all the vertices in a graph. In this paper, we study an efficient algorithm for finding the exact k-highest betweenness centrality vertices.	Finding k-highest betweenness centrality vertices in graphs	NA:NA	2014
Yuming Lin:Tao Zhu:Xiaoling Wang:Jingwei Zhang:Aoying Zhou	User reviews play a crucial role in Web, since many decisions are made based on them. However, review spam would misled the users, which is extremely obnoxious. In this poster, we explore the problem of online review spam detection. Firstly, we devise six features to find the spam based on the review content and reviewer behaviors. Secondly, we apply supervised methods and an unsupervised one for spotting the review spam as early as possible. Finally, we carry out intensive experiments on a real-world review set to verify the proposed methods.	Towards online review spam detection	NA:NA:NA:NA:NA	2014
Chang Liu:Jacopo Urbani:Guilin Qi	In this paper, we study the problem of stream reasoning and propose a reasoning approach over large amounts of RDF data, which uses graphics processing units (GPU) to improve the performance. First, we show how the problem of stream reasoning can be reduced to a temporal reasoning problem. Then, we describe a number of algorithms to perform stream reasoning with GPUs.	Efficient RDF stream reasoning with graphics processingunits (GPUs)	NA:NA:NA	2014
Pengqi Liu:Javad Azimi:Ruofei Zhang	Contextual Advertising (CA) is an important area in the industry of online advertising. Typically, CA algorithms return a set of related ads based on some keywords extracted from the content of webpages. Therefore, extracting the best set of representative keywords from a given webpage is the key to the success of CA. In this paper, we introduce a new keywords generation approach that uses some novel NLP features including POS and named-entities tagging. Unlike most of the existing keyword extraction algorithms, our proposed framework is able to generate some related keywords which do not exist in the webpage. A monetization parameter, predicted from historical search keyword performance, is also used to rank potential keywords in order to balance the RPM (Revenue Per 1000 Matches) and relevance. Experimental results over a very large real-world data set shows that the proposed approach can outperform the state-of-the-art system in both relevance and monetization metrics.	Automatic keywords generation for contextual advertising	NA:NA:NA	2014
Sayandev Mukherjee:Ronald Sujithan:Pero Subasic	Many applications including realtime recommenders and ad-targeting systems have a need to identify trending concepts to prioritize the information presented to end-users. In this paper, we describe a novel approach that identifies trending concepts using the hourly Wikipedia page visitation statistics freely available for download. We describe a MapReduce framework that analyzes the raw hourly visitation logs and generates a ranked list of trending concepts on a daily basis. We validate this approach by extracting hourly lists of trending news articles, mapping these articles to Wikipedia concepts, and computing the similarity of the two lists according to several commonly used measures.	Detecting trending topics using page visitation statistics	NA:NA:NA	2014
Subhabrata Mukherjee:Jitendra Ajmera:Sachindra Joshi	In this work we propose an unsupervised approach to construct a domain-specific ontology from corpus. It is essential for Information Retrieval systems to identify important domain concepts and relationships between them. We identify important domain terms of which multi-words form an important component. Our approach identifies 40% of the domain terms, compared to 22% identified by WordNet on manually annotated smartphone data. We propose an approach to construct a shallow ontology from discovered domain terms by identifying four domain relations namely, Synonyms ('similar-to'), Type-Of ('is-a'), Action-On ('methods') and Feature-Of ('attributes'), where we achieve an F-Score of 49.14%, 65.5%, 65% and 80% respectively	Unsupervised approach for shallow domain ontology construction from corpus	NA:NA:NA	2014
Hyun-Kyo Oh:Yoohan Noh:Sang-Wook Kim:Sunju Park	This paper proposes SepaRating, a novel mechanism that separates a buyer's rating on a transaction into two kinds of scores: seller's score and item's score. SepaRating provides the reputation of sellers correctly based on the seller's score by repetitive separations, which helps potential buyers to find more reliable sellers. We verify the effectiveness of SepaRating via a series of experiments.	SepaRating: an approach to reputation computation based on rating separation in e-marketplace	NA:NA:NA:NA	2014
Jeong-Hoon Park:Chin-Wan Chung	The semantic Web is a promising future Web environment. In order to realize the semantic Web, the semantic annotation should be widely available. The studies for generating the semantic annotation do not provide a solution to the 'document evolution' requirement which is to maintain the consistency between semantic annotations and Web pages. In this paper, we propose an efficient solution to the requirement, that is to separately generate the long-term annotation and the short-term annotation. The experimental results show that our approach outperforms an existing approach which is the most efficient among the automatic approaches based on static Web pages.	Semantic annotation for dynamic web environment	NA:NA	2014
Baolin Peng:Wenge Rong:Yuanxin Ouyang:Chao Li:Zhang Xiong	One of the main research tasks in Community question answering (CQA) is to find most relevant questions for a given new query, thereby providing useful knowledge for the users. Traditionally used methods such as bag-of-words or latent semantic models consider queries, questions and answers in a same feature space. However, the correlations among queries, questions and answers imply that they lie in different feature spaces. In light of these issues, we proposed a tri-modal deep boltzmann machine (tri-DBM) to extract unified representation for query, question and answer. Experiments on Yahoo! Answers dataset reveal using these unified representation to train a classifier judging semantic matching level between query and question outperforms models using bag-of-words or LSA representation significantly.	Learning joint representation for community question answering with tri-modal DBM	NA:NA:NA:NA:NA	2014
Xianglan Piao:channoh Kim:Younghwan Oh:Hanjun Kim:Jae W. Lee	Modern web browsers are required to execute many complex, compute-intensive applications, mostly written in JavaScript. With widespread adoption of heterogeneous processors, recent JavaScript-based data-parallel programming models, such as River Trail and WebCL, support multiple types of processing elements including CPUs and GPUs. However, significant performance gains are still left on the table since the program kernel runs on only one compute device, typically selected at kernel invocation. This paper proposes a new framework for efficient work sharing between CPU and GPU for data-parallel JavaScript workloads. The work sharing scheduler partitions the input data into smaller chunks and dynamically dispatches them to both CPU and GPU for concurrent execution. For four data-parallel programs, our framework improves performance by up to 65% with a geometric mean speedup of 33% over GPU-only execution.	Efficient CPU-GPU work sharing for data-parallel JavaScript workloads	NA:NA:NA:NA:NA	2014
Philipp Pushnyakov:Gleb Gusev	Our work is devoted to Web revisitation patterns of individual users. Everybody revisits Web pages, but their reasons for doing so can differ. We analyzed Web interaction logs of millions users to characterize how people revisit Web content. We revealed that each user have its own distribution of revisitation times. This distribution follows Power Law with some exponent, which captures specific user peculiarities.	User profiles based on revisitation times	NA:NA	2014
Zhi Qiao:Peng Zhang:Jing He:Yanan Cao:Chuan Zhou:Li Guo	Recommender systems have attracted attentions lately due to their wide and successful applications in online advertising. In this paper, we propose a bayesian generative model to describe the generative process of rating, which combines geographical information of users and content of items. The generative model consists of two interacting LDA models, where one LDA model for location-based user groups (user dimension) and the other for the topics of content of items(item dimension). A Gibbs sampling algorithm is proposed for parameter estimation. Experiments have shown our proposed method outperforms baseline methods.	Combining geographical information of users and content of items for accurate rating prediction	NA:NA:NA:NA:NA:NA	2014
Jyothsna Rachapalli:Vaibhav Khadilkar:Murat Kantarcioglu:Bhavani Thuraisingham	With the advent of Semantic Web and Resource Description Framework (RDF), the web is likely to witness an unprecedented wealth of knowledge, resulting from seamless integration of various data sources. Data integration is one of the key features of RDF, however, absence of secure means for managing sensitive RDF data may prevent sharing of critical data altogether or may cause serious damage. Towards this end we present a language for sanitizing RDF graphs, which comprises a set of sanitization operations that transform a graph by concealing the sensitive data. These operations are modeled into a new SPARQL query form known as SANITIZE, which can also be leveraged towards fine grained access control and building advanced anonymization features.	RDF-X: a language for sanitizing RDF graphs	NA:NA:NA:NA	2014
Ryan A. Rossi:David F. Gleich:Assefaw H. Gebremedhin:Md. Mostofa Ali Patwary	We propose a fast, parallel maximum clique algorithm for large sparse graphs that is designed to exploit characteristics of social and information networks. Despite clique's status as an NP-hard problem with poor approximation guarantees, our method exhibits nearly linear runtime scaling over real-world networks ranging from 1000 to 100 million nodes. In a test on a social network with 1.8 billion edges, the algorithm finds the largest clique in about 20 minutes. Key to the efficiency of our algorithm are an initial heuristic procedure that finds a large clique quickly and a parallelized branch and bound strategy with aggressive pruning and ordering techniques. We use the algorithm to compute the largest temporal strong components of temporal contact networks.	Fast maximum clique algorithms for large graphs	NA:NA:NA:NA	2014
Kim Schouten:Flavius Frasincar	Implicit feature detection is a promising research direction that has not seen much research yet. Based on previous work, where co-occurrences between notional words and explicit features are used to find implicit features, this research critically reviews its underlying assumptions and proposes a revised algorithm, that directly uses the co-occurrences between implicit features and notional words. The revision is shown to perform better than the original method, but both methods are shown to fail in a more realistic scenario.	Implicit feature detection for sentiment analysis	NA:NA	2014
Boon-Siew Seah:Sourav S. Bhowmick:Aixin Sun	Most existing social image search engines present search results as a ranked list of images, which cannot be consumed by users in a natural and intuitive manner. Here, we present a novel algorithm that exploits both visual features and tags of the search results to generate high quality image search result summary. The summary not only breaks the results into visually and semantically coherent clusters, but it also maximizes the coverage of the original search results. We demonstrate the effectiveness of our method against state-of-the-art image summarization and clustering algorithms.	Summarizing social image search results	NA:NA:NA	2014
Jieying She:Lei Chen	On Twitter, hashtags are used to summarize topics of the tweet content and to help to categorize and search tweets. However, hashtags are created in a free style and thus heterogeneous, increasing difficulty of their usage. We propose TOMOHA, a supervised TOpic MOdel-based solution for HAshtag recommendation on Twitter. We treat hashtags as labels of topics, and develop a supervised topic model to discover relationship among words, hashtags and topics of tweets. We also novelly add user following relationship into the model. We infer the probability that a hashtag will be contained in a new tweet, and recommend k most probable ones. We propose parallel computing and pruning techniques to speed up model training and recommendation process. Experiments show that our method can properly and efficiently recommend hashtags.	TOMOHA: TOpic model-based HAshtag recommendation on twitter	NA:NA	2014
Yelong Shen:Xiaodong He:Jianfeng Gao:Li Deng:Grégoire Mesnil	This paper presents a series of new latent semantic models based on a convolutional neural network (CNN) to learn low-dimensional semantic vectors for search queries and Web documents. By using the convolution-max pooling operation, local contextual information at the word n-gram level is modeled first. Then, salient local fea-tures in a word sequence are combined to form a global feature vector. Finally, the high-level semantic information of the word sequence is extracted to form a global vector representation. The proposed models are trained on clickthrough data by maximizing the conditional likelihood of clicked documents given a query, us-ing stochastic gradient ascent. The new models are evaluated on a Web document ranking task using a large-scale, real-world data set. Results show that our model significantly outperforms other se-mantic models, which were state-of-the-art in retrieval performance prior to this work.	Learning semantic representations using convolutional neural networks for web search	NA:NA:NA:NA:NA	2014
Yilin Shen:Fengjiao Wang:Hongxia Jin	We study the first countermeasure against user identity linkage attack across multiple online social networks (OSNs). Our goal is to keep as much as user's information in public and meanwhile prevent their identities from being linked on different OSNs via k-anonymity. We develop a novel greedy algorithm, incorporating an efficient manner to compute the greedy function, and validate it in terms of both solution quality and robustness using real-world datasets.	Defending against user identity linkage attack across multiple online social networks	NA:NA:NA	2014
Si Si:Atish Das Sarma:Elizabeth F. Churchill:Neel Sundaresan	We study the problem of predicting sharing behavior from e-commerce sites to friends on social networks via share widgets. The contextual variation in an action that is private (like rating a movie on Netflix), to one shared with friends online (like sharing an item on Facebook), to one that is completely public (like commenting on a Youtube video) introduces behavioral differences that pose interesting challenges. In this paper, we show that users' interests manifest in actions that spill across different types of channels such as sharing, browsing, and purchasing. This motivates leveraging all such signals available from the e-commerce platform. We show that carefully incorporating signals from these interactions significantly improves share prediction accuracy.	Beyond modeling private actions: predicting social shares	NA:NA:NA:NA	2014
Terence Sim:Hossein Nejati:James Chua	A CAPTCHA is a Turing test to distinguish human users from automated scripts to defend against internet adversarial attacks. As text-based CAPTCHAs (TBC) have become increasingly difficult to solve, image-based CAPTCHAs, and particularly face recognition CAPTCHAs (FRC), offer a chance to overcome TBC limitations. In this paper, we systematically design and implement a practical FRC, informed by psychological findings. We use gray-scale and binary images, which are computationally inexpensive to generate and deploy. Furthermore, our FRC complies with CAPTCHA design guidelines, thereby ensuring its robustness.	Face recognition CAPTCHA made difficult	NA:NA:NA	2014
Nikita Spirin:Motahhare Eslami:Jie Ding:Pooja Jain:Brian Bailey:Karrie Karahalios	Examples are very important in design, but existing tools for design example search still do not cover many cases. For instance, long tail queries containing subtle and subjective design concepts, like "calm and quiet", "elegant", "dark background with a hint of color to make it less boring", are poorly supported. This is mainly due to the inherent complexity of the task, which so far has been tackled only algorithmically using general image search techniques. We propose a powerful new approach based on crowdsourcing, which complements existing algorithmic approaches and addresses their shortcomings. Out of many explored crowdsourcing configurations we found that (1) a design need should be represented via several query images and (2) AMT crowd workers should assess a query-specific relevance of a candidate example from a pre-built design collection. To test the utility of our approach, we compared it with Google Images in a query-by-example mode. Based on feedback from expert designers, the crowd selects more relevant design examples.	Searching for design examples with crowdsourcing	NA:NA:NA:NA:NA:NA	2014
Adam Styperek:Michal Ciesielczyk:Andrzej Szwabe	It is crucial to enable regular users to explore RDF-compliant data bases in an effective way, regardless their knowledge about the SPARQL or the underlying ontology. Natural language querying have been proposed to address this issue. However it has unavoidably lower accuracy, as compared to systems with graph-based querying interfaces, which, in turn, are usually still too difficult for regular users. This paper presents a search system of user interface that is more user-friendly than widely known graph-based solutions.	Semantic search engine with an intuitive user interface	NA:NA:NA	2014
Yukihiro Tagami:Toru Hotta:Yusuke Tanaka:Shingo Ono:Koji Tsukamoto:Akira Tajima	Contextual advertising has a key problem to determine how to select the ads that are relevant to the page content and/or the user information. We introduce a translation method that learns a mapping of contextual information to the textual features of ads by using past click data. This method is easy to implement and there is no need to modify an ordinary ad retrieval system because the contextual feature vector is simply transformed into a term vector with the learned matrix. We applied our approach with a real ad serving system and compared the online performance in A/B testing.	Translation method of contextual information into textual space of advertisements	NA:NA:NA:NA:NA:NA	2014
Christoph Trattner:Denis Parra:Lukas Eberhard:Xidao Wen	In this paper we present the latest results of a recently started project that aims at studying the extent to which links between buyers and sellers, i.e. trading interactions in online trading platforms, can be predicted from external knowledge sources such as online social networks. To that end, we conducted a large-scale experiment on data obtained from the virtual world Second Life. As our results reveal, online social network data bears a significant potential (28% over the baseline) to predict links between buyers and sellers in online trading platforms.	Who will trade with whom?: predicting buyer-seller interactions in online trading platforms through social networks	NA:NA:NA:NA	2014
Alexey Tschudnowsky:Stefan Pietschmann:Matthias Niederhausen:Martin Gaedke	Recent research in the field of user interface (UI) mashups has focused on so-called choreographed compositions, where communication between components is not pre-defined by a mashup designer, but rather emerges from the components' messaging capabilities. Though the mashup development process gets simplified, such solutions bear several problems related to awareness and control of the emerging message flow. This paper presents an approach to systematically extend choreographed mashups with visualization and tailoring facilities. A first user study demonstrates that usability of the resulting solutions increases if proposed awareness and control facilities are integrated.	Towards awareness and control in choreographed user interface mashups	NA:NA:NA:NA	2014
Damir Vandic:Lennart J. Nederstigt:Steven S. Aanen:Flavius Frasincar:Frederik Hogenboom	With the vast amount of information available on the Web, there is an increasing need to structure Web data in order to make it accessible to both users and machines. E-commerce is one of the areas in which growing data congestion on the Web has serious consequences. This paper proposes a framework that is capable of populating a product ontology using tabular product information from Web shops. By formalizing product information in this way, better product comparison or recommendation applications could be built. Our approach employs both lexical and syntactic matching for mapping properties and instantiating values. The performed evaluation shows that instantiating consumer electronics from Best Buy and Newegg.com results in an F1 score of approximately 77%.	Ontology population from web product information	NA:NA:NA:NA:NA	2014
Chetan Kumar Verma:Vijay Mahadevan:Nikhil Rasiwasia:Gaurav Aggarwal:Ravi Kant:Alejandro Jaimes:Sujit Dey	We present a data-driven approach for the construction of ontological graphs on a set of image tags obtained from annotated image corpus. We treat each tag as a node in a graph, and starting with a preliminary graph obtained using WordNet, we propose the graph construction as a refinement of the preliminary graph using corpus statistics. Towards this, we formulate an optimization problem which is solved using a local search based approach. To evaluate the constructed ontological graphs, we propose a novel task which involves associating test images with tags while observing partial set of associated tags.	Construction of tag ontological graphs by locally minimizing weighted average hops	NA:NA:NA:NA:NA:NA:NA	2014
Xin Wang:Jun Ling:Junhu Wang:Kewen Wang:Zhiyong Feng	This paper presents an automata-based algorithm for answering the \emph{provenance-aware} regular path queries (RPQs) over RDF graphs on the Semantic Web. The provenance-aware RPQs can explain why pairs of nodes in the classical semantics appear in the result of an RPQ. We implement a parallel version of the automata-based algorithm using the Pregel framework Giraph to efficiently evaluate provenance-aware RPQs on large RDF graphs. The experimental results show that our algorithms are effective and efficient to answer provenance-aware RPQs on large real-world RDF graphs.	Answering provenance-aware regular path queries on RDF graphs using an automata-based algorithm	NA:NA:NA:NA:NA	2014
Simon S. Woo:Beomjun Kim:Woochan Jun:Jingul Kim	Current 2D CAPTCHA mechanisms can be easily defeated by character recognition and segmentation attacks by automated machines. Recently, 3D CAPTCHA schemes have been proposed to overcome the weaknesses of 2D CAPTCHA for a few websites. However, researchers also demonstrate the offline pre-processing techniques to break 3D CAPTCHA. In this work, we propose a novel 3D object based CAPTCHA scheme that projects the CAPTCHA image over a 3D object. We develop the prototype and present the proof-of-concept of 3D object based CAPTCHA scheme to protect websites against automated attacks.	3DOC: 3D object CAPTCHA	NA:NA:NA:NA	2014
Liang Wu:Bin Cao:Yuanchun Zhou:Jianhui Li	Clustering-based methods are commonly used in Web search engines for query suggestion. Clustering is useful in reducing the sparseness of data. However, it also introduces noises and ignores the sequential information of query refinements in search sessions. In this paper, we propose to improve cluster based query suggestion from two perspectives: filtering out unrelated query candidates and predicting the refinement direction. We observe two major refinements behaviors. One is to simplify the original query and the other is to specify it. Both could be modeled by predicting the length (number of terms) of queries when candidates are being ranked. Two experimental results on the real query logs of a commercial search engine demonstrate the effectiveness of the proposed approaches.	Improving query suggestion through noise filtering and query length prediction	NA:NA:NA:NA	2014
Shan-Hung Wu:Man-Ju Chou:Chun-Hsiung Tseng:Yuh-Jye Lee:Kuan-Ta Chen	In this paper, we propose to use a continuous authentication approach to detect the in-situ identity fraud incidents, which occur when the attackers use the same devices and IP addresses as the victims. Using Facebook as a case study, we show that it is possible to detect such incidents by analyzing SNS users' browsing behavior. Our experiment results demonstrate that the approach can achieve reasonable accuracy given a few minutes of observation time.	Detecting in-situ identity fraud on social network services: a case study on facebook	NA:NA:NA:NA:NA	2014
Feng Xia:Haifeng Liu:Nana Yaw Asabere:Wei Wang:Zhuo Yang	This paper proposes a novel recommendation method called RecDI. In the multi-category item recommendation domain, RecDI is designed to combine user ratings with information involving user's direct and indirect neighborhood associations. Through relevant benchmarking experiments on two real-world datasets, we show that RecDI achieves better performance than other traditional recommendation methods, which demonstrates the effectiveness of RecDI.	Multi-category item recommendation using neighborhood associations in trust networks	NA:NA:NA:NA:NA	2014
Jia Xu:Patrick Shironoshita:Ubbo Visser:Nigel John:Mansur Kabuka	Instance checking is considered a central tool for data retrieval from description logic (DL) ontologies. In this paper, we propose a revised most specific concept (MSC) method for DL $\mathcal{SHI}$, which converts instance checking into subsumption problems. This revised method can generate small concepts that are specific-enough to answer a given query, and allow reasoning to explore only a subset of the ABox data to achieve efficiency. Experiments show effectiveness of our proposed method in terms of concept size reduction and the improvement in reasoning efficiency.	Optimizing the most specific concept method for efficient instance checking	NA:NA:NA:NA:NA	2014
Deqing Yang:Yanghua Xiao:Yangqiu Song:Junjun Zhang:Kezun Zhang:Wei Wang	Many real applications demand accurate cross-domain recommendation, e.g., recommending a Weibo (the largest Chinese Twitter) user with the products in an e-commerce Web site. Since many social media have rich tags on both items or users, tag-based profiling became popular for recommendation. However, most previous recommendation approaches have low effectiveness in handling sparse data or matching tags from different social media. Addressing these problems, we first propose an optimized local tag propagation algorithm to generate tags for profiling Weibo users and then use a Chinese knowledge graph accompanied by an improved ESA (explicit semantic analysis) for semantic matching of cross-domain tags. Empirical comparisons to the state-of-the-art approaches justify the efficiency and effectiveness of our approaches.	Tag propagation based recommendation across diverse social media	NA:NA:NA:NA:NA:NA	2014
Weilong Yao:Jing He:Guangyan Huang:Yanchun Zhang	Most existing learning to rank based recommendation methods only use user-item preferences to rank items, while neglecting social relations among users. In this paper, we propose a novel, effective and efficient model, SoRank, by integrating social information among users into listwise ranking model to improve quality of ranked list of items. In addition, with linear complexity to the number of observed ratings, SoRank is able to scale to very large dataset. Experimental results on publicly available dataset demonstrate the effectiveness of SoRank.	SoRank: incorporating social information into learning to rank models for recommendation	NA:NA:NA:NA	2014
Arjumand Younus:M. Atif Qureshi:Muhammad Saeed:Nasir Touheed:Colm O'Riordan:Gabriella Pasi	The use of Twitter as a discussion platform for political issues has led researchers to study its role in predicting election outcomes. This work studies a much neglected aspect of politics on Twitter namely "election trolling" whereby supporters of different political parties attack each other during election campaigns. We also propose a novel strategy to detect terms that are usually not associated with sentiment but are introduced by supporters of political parties to attack the opposing party. We demonstrate a lack of political maturity as evidenced through high percentage of political attacks in a developing region such as Pakistan.	Election trolling: analyzing sentiment in tweets during pakistan elections 2013	NA:NA:NA:NA:NA:NA	2014
Jianjun Yu:Yi Shen:Zhenglu Yang	Micro-blogging is experiencing fantastic success in the worldwide. However, during its rapid development, it has encountered the problem of information overload, which has troubled many users. In this paper, we mainly focus on the task of tweet recommendation to address this problem. We extend the session-based temporal graph (STG) approach as Topic-STG for tweet recommendation which comprehensively considers three types of features in Twitter: the textual information, the time factor, and the users' behavior. The experimental results conducted on a real dataset demonstrate the effectiveness of our approach.	Topic-STG: extending the session-based temporal graph approach for personalized tweet recommendation	NA:NA:NA	2014
Jianye Yu:Yuanzhuo Wang:Xiaolong Jin:Jingyuan Li:Xueqi Cheng	In this paper, we propose a social evolutionary game to investigate the evolution of social networks. Through comparison between simulation and empirical analysis on the social networks of Twitter and Sina Weibo, we validate the effectiveness of the proposed model and estimate the evolutionary phases of the two networks. We find that the users of Sina Weibo can withstand comparatively more costs than the users of Twitter. Therefore, they can perform more positive behavior and consider more about their reputation than Twitter users. Moreover, the evolutionary time of Sina Weibo to a stable state is longer than that of Twitter.	Evolutionary analysis on online social networks using a social evolutionary game	NA:NA:NA:NA:NA	2014
Peng Yu:Ching-man Au Yeung	In this poster, we present a new model for estimating the actual value of mobile apps to the users. The model assumes that users are implicitly evaluating the value of the apps in their smartphones when they choose to uninstall some apps. Our proposed method thus makes use of the install and uninstall log in a mobile app store to estimate the value of the apps. Experiments using data from a popular mobile app store show that our model is better in predicting the future download trend of the apps as well as the future uninstallation rate of the apps. We believe such model will be very useful in generating more credible and appropriate mobile app recommendations to users, or in generating features for machine learning systems in more complex prediction tasks.	App mining: finding the real value of mobile applications	NA:NA	2014
Huasha Zhao:Vivian Zhang:Ye Chen:John Canny:Tak Yan	Search advertising shows trends of vertical extension. Vertical ads typically offer better Return of Investment (ROI) to advertisers as a result of better user engagement. However, campaign and bids in vertical ads are not set at the keyword level. As a result, the matching between user query and ads suffers low recall rate and the match quality is heavily impacted by tail queries. In this paper, we propose a retail ads retrieval framework based on query rewrite using personal history data to improve ads recall rate. To insure ads quality, we also present a relevance model for matching rewritten queries with user search intent, with a particular focus on rare queries. Extensive experiments are carried out on large-scale logs collected from the Bing search engine, and results show our system achieves significant gains in ads retrieval rate without compromising ads quality. To our knowledge, this work is the first attempt to leverage user behavioral data in ad matching and apply it to the vertical ads domain.	Query augmentation based intent matching in retail vertical ads	NA:NA:NA:NA:NA	2014
Chuan Zhou:Peng Zhang:Jing Guo:Li Guo	Influence maximization [4] is NP-hard under the Linear Threshold (LT) model, where a line of greedy algorithms have been proposed. The simple greedy algorithm [4] guarantees accuracy rate of 1-1/e to the optimal solution; the advanced greedy algorithm, e.g., the CELF algorithm [6], runs 700 times faster by exploiting the submodular property of the spread function. However, both models lack efficiency due to heavy Monte-Carlo simulations during estimating the spread function. To this end, in this paper we derive an upper bound for the spread function under the LT model. Furthermore, we propose an efficient UBLF algorithm by incorporating the bound into CELF. Experimental results demonstrate that UBLF, compared with CELF, reduces about 98.9% Monte-Carlo simulations and achieves at least 5 times speed-raising when the size of seed set is small.	An upper bound based greedy algorithm for mining top-k influential nodes in social networks	NA:NA:NA:NA	2014
Chuan Zhou:Peng Zhang:Wenyu Zang:Li Guo	We address the problem of discovering the influential nodes in social networks under the voter model, which allows multiple activations to the same node, by defining an integral influence maximization problem in a long term. We analyze the problem formulation and present an exact solution to the maximization problem. We also provide a sufficient condition for the convergence of the integral influence. We experimentally compare the exact solution with other heuristic algorithms in the aspects of quality and efficiency.	Maximizing the long-term integral influence in social networks under the voter model	NA:NA:NA:NA	2014
Wendy Hall:Sung-Hyon Myaeng:Harith Alani:Matthew S. Weber	NA	Session details: WWW 2014 websci track	NA:NA:NA:NA	2014
Robert Meusel:Sebastiano Vigna:Oliver Lehmberg:Christian Bizer	Knowledge about the general graph structure of the World Wide Web is important for understanding the social mechanisms that govern its growth, for designing ranking methods, for devising better crawling algorithms, and for creating accurate models of its structure. In this paper, we describe and analyse a large, publicly accessible crawl of the web that was gathered by the Common Crawl Foundation in 2012 and that contains over 3.5 billion web pages and 128.7 billion links. This crawl makes it possible to observe the evolution of the underlying structure of the World Wide Web within the last 10 years: we analyse and compare, among other features, degree distributions, connectivity, average distances, and the structure of weakly/strongly connected components. Our analysis shows that, as evidenced by previous research, some of the features previously observed by Broder et al. are very dependent on artefacts of the crawling process, whereas other appear to be more structural. We confirm the existence of a giant strongly connected component; we however find, as observed by other researchers, very different proportions of nodes that can reach or that can be reached from the giant component, suggesting that the "bow-tie structure" is strongly dependent on the crawling process, and to the best of our current knowledge is not a structural property of the web. More importantly, statistical testing and visual inspection of size-rank plots show that the distributions of indegree, outdegree and sizes of strongly connected components are not power laws, contrarily to what was previously reported for much smaller crawls, although they might be heavy-tailed. We also provide for the first time accurate measurement of distance-based features, using recently introduced algorithms that scale to the size of our crawl.	Graph structure in the web --- revisited: a trick of the heavy tail	NA:NA:NA:NA	2014
Matthew Rowe:Markus Strohmaier	Despite their semantic-rich nature, online communities have, to date, largely been analysed through examining longitudinal changes in social networks, community uptake, or simple term-usage and language adoption. As a result, the evolution of communities on a semantic level, i.e. how concepts emerge, and how these concepts relate to previously discussed concepts, has largely been ignored. In this paper we present a graph-based exploration of the semantic evolution of online communities, thereby capturing dynamics of online communities on a conceptual level. We first examine how semantic graphs (concept graphs and entity graphs) of communities evolve, and then characterise such evolution using logistic population growth models. We demonstrate the value of such models by analysing how sample communities evolve and use our results to predict churn rates in community forums.	The semantic evolution of online communities	NA:NA	2014
Emilio Zagheni:Venkata Rama Kiran Garimella:Ingmar Weber:Bogdan State	Data about migration flows are largely inconsistent across countries, typically outdated, and often inexistent. Despite the importance of migration as a driver of demographic change, there is limited availability of migration statistics. Generally, researchers rely on census data to indirectly estimate flows. However, little can be inferred for specific years between censuses and for recent trends. The increasing availability of geolocated data from online sources has opened up new opportunities to track recent trends in migration patterns and to improve our understanding of the relationships between internal and international migration. In this paper, we use geolocated data for about 500,000 users of the social network website "Twitter". The data are for users in OECD countries during the period May 2011- April 2013. We evaluated, for the subsample of users who have posted geolocated tweets regularly, the geographic movements within and between countries for independent periods of four months, respectively. Since Twitter users are not representative of the OECD population, we cannot infer migration rates at a single point in time. However, we proposed a difference-in-differences approach to reduce selection bias when we infer trends in out-migration rates for single countries. Our results indicate that our approach is relevant to address two longstanding questions in the migration literature. First, our methods can be used to predict turning points in migration trends, which are particularly relevant for migration forecasting. Second, geolocated Twitter data can substantially improve our understanding of the relationships between internal and international migration. Our analysis relies uniquely on publicly available data that could be potentially available in real time and that could be used to monitor migration trends. The Web Science community is well-positioned to address, in future work, a number of methodological and substantive questions that we discuss in this article.	Inferring international and internal migration patterns from Twitter data	NA:NA:NA:NA	2014
Ramine Tinati:Paul Gaskell:Thanassis Tiropanis:Olivier Phillipe:Wendy Hall	The Web has grown to be an integral part of modern society offering novel ways for humans to communicate, interact, and share information. New collaborative platforms are forming which are providing individuals with new communities and knowledge bases and, at the same time, offering insights into human activity for researchers, policy-makers and engineers. On a global scale, the role of cultural and language barriers when studying such phenomena becomes particularly relevant and presents significant challenges: due to insufficient information, it is often hard to establish the cultural or language groups in which individuals belong, while there are technical difficulties in establishing the relevance and in analysing resources in different languages. This paper presents a framework to the end of addressing those issues by leveraging data on the use of Wikipedia. Resources available in different languages are explicitly correlated in Wikipedia along with time-stamped logs of access to its articles. This paper provides a framework to enable temporal page views in Wikipedia to be associated with specific geographic profiles. This framework is then used to examine the exchange of information between the English speaking and Chinese speaking localities and reports initial findings on the role of language and culture in diffusion in this context.	Examining Wikipedia across linguistic and temporal borders	NA:NA:NA:NA:NA	2014
Carmen Vaca Ruiz:Daniele Quercia:Luca Maria Aiello:Piero Fraternali	Urban resources are allocated according to socio-economic indicators, and rapid urbanization in developing countries calls for updating those indicators in a timely fashion. The prohibitive costs of census data collection make that very difficult. To avoid allocating resources upon outdated indicators, one could partly update or complement them using digital data. It has been shown that it is possible to use social media in developed countries (mainly UK and USA) for such a purpose. Here we show that this is the case for Brazil too. We analyze a random sample of a microblogging service popular in that country and accurately predict the GDPs of 45 Brazilian cities. To make these predictions, we exploit the sociological concept of glocality, which says that economically successful cities tend to be involved in interactions that are both local and global at the same time. We indeed show that a city's glocality, measured with social media data, effectively signals the city's economic well-being.	Taking Brazil's pulse: tracking growing urban economies from online attention	NA:NA:NA:NA	2014
Claudia López:Rosta Farzan	Using a large dataset of Yelp's online reviews for local businesses, we investigate how Word-of-Mouth research can inform the design of local online review systems and how these systems' data can extend our understanding of digital WOM in a local context. In this paper, we analyze how visual cues currently present in Yelp map to WOM concepts. We also show that these concepts are highly related to the perceived usefulness of the local reviews, which is aligned with prior WOM literature. Additionally, we found that local expertise, measured at the level of the neighborhood, strongly correlates with the perceived usefulness of reviews. Our findings augment the understanding of local online WOM and have design implications for local review systems.	Analysis of local online review systems as digital word-of-mouth	NA:NA	2014
Dominik Kowald:Paul Seitlinger:Christoph Trattner:Tobias Ley	In this paper, we introduce a tag recommendation algorithm that mimics the way humans draw on items in their long-term memory. This approach uses the frequency and recency of previous tag assignments to estimate the probability of reusing a particular tag. Using three real-world folksonomies gathered from bookmarks in BibSonomy, CiteULike and Flickr, we show how incorporating a time-dependent component outperforms conventional "most popular tags" approaches and another existing and very effective but less theory-driven, time-dependent recommendation mechanism. By combining our approach with a simple resource-specific frequency analysis, our algorithm outperforms other well-established algorithms, such as FolkRank, Pairwise Interaction Tensor Factorization and Collaborative Filtering. We conclude that our approach provides an accurate and computationally efficient model of a user's temporal tagging behavior. We demonstrate how effective principles of information retrieval can be designed and implemented if human memory processes are taken into account.	Long time no see: the probability of reusing tags as a function of frequency and recency	NA:NA:NA:NA	2014
Jagat Sastry Pudipeddi:Leman Akoglu:Hanghang Tong	Given a user on a Q&A site, how can we tell whether s/he is engaged with the site or is rather likely to leave? What are the most evidential factors that relate to users churning? Question and Answer (Q&A) sites form excellent repositories of collective knowledge. To make these sites self- sustainable and long-lasting, it is crucial to ensure that new users as well as the site veterans who provide most of the answers keep engaged with the site. As such, quantifying the engagement of users and preventing churn in Q&A sites are vital to improve the lifespan of these sites. We study a large data collection from stackoverflow.com to identify significant factors that correlate with newcomer user churn in the early stage and those that relate to veterans leaving in the later stage. We consider the problem under two settings: given (i) the first k posts, or (ii) first T days of activity of a user, we aim to identify evidential features to automatically classify users so as to spot those who are about to leave. We find that in both cases, the time gap between subsequent posts is the most significant indicator of diminishing interest of users, besides other indicative factors like answering speed, reputation of those who answer their questions, and number of answers received by the user.	User churn in focused question answering sites: characterizations and prediction	NA:NA:NA	2014
Dominic Difranzo:John S. Erickson:Marie Joan Kristine T. Gloria:Joanne S. Luciano:Deborah L. McGuinness:James Hendler	The multi-disciplinary nature of Web Science and the large size and diversity of data collected and studied by its practitioners has inspired a new type of Web resource known as the Web Observatory. Web observatories are platforms that enable researchers to collect, analyze and share data about the Web and to share tools for Web research. At the Boston Web Observatory Workshop 2013, a semantic model for describing Web Observatories was drafted and an extension to the schema.org microdata vocabulary collection was proposed. This paper details our implementation of the proposed extension, and how we have applied it to the Web Observatory Portal created by the Tetherless World Constellation at Rensselaer Polytechnic Institute (TWC RPI). We recognize this effort to be the "first-step" in the construction, evaluation and validation of the Web observatory model and not the final recommendation. Our hope is that this extension recommendation and our initial implementation sparks additional discussion among the Web Science community of on whether such direction enables Web Observatory curators to better expose and explain their individual Web Observatories to others, thereby enabling better collaboration between researchers across the Web Science community.	The web observatory extension: facilitating web science collaboration through semantic markup	NA:NA:NA:NA:NA:NA	2014
Elizabeth Sillence:Claire Hardy:Peter R. Harris:Pam Briggs	Patients now turn to other patients online for health information and advice in a phenomenon known as peer-to-peer healthcare. This paper describes a model of patients' peer-to-peer engagement, based upon qualitative studies of three patient or carer groups searching for online information and advice from their health peers. We describe a three-phase process through which patients engage with peer experience (PEx). In phase I (gating) patients determine the suitability and trustworthiness of the material they encounter; in phase II (engagement) they search out information, support and/or advice from others with similar or relevant experience; and in phase III (evaluation) they make judgments about the costs and benefits of engaging with particular websites in the longer term. This model provides a useful framework for understanding web based interactions in different patient groups.	Modeling patient engagement in peer-to-peer healthcare	NA:NA:NA:NA	2014
Alessandro Bozzon:Hariton Efstathiades:Geert-Jan Houben:Robert-Jan Sips	Understanding the impact of corporate information publicly distributed on the Web is becoming more and more crucial. In this paper we report the result of a study that involved 130 IBM employees: we explored the correctness and extent of organisational information that can be observed from the online profiles of a company's employees. Our work contributes new insights to the study of social networks by showing that, even by considering a small fraction of the available online data, it is possible to discover accurate information about an organisation, its structure, and the factors that characterise the social reach of their employees.	A study of the online profile of enterprise users in professional social networks	NA:NA:NA:NA	2014
Seth A. Myers:Aneesh Sharma:Pankaj Gupta:Jimmy Lin	In this paper, we provide a characterization of the topological features of the Twitter follow graph, analyzing properties such as degree distributions, connected components, shortest path lengths, clustering coefficients, and degree assortativity. For each of these properties, we compare and contrast with available data from other social networks. These analyses provide a set of authoritative statistics that the community can reference. In addition, we use these data to investigate an often-posed question: Is Twitter a social network or an information network? The "follow" relationship in Twitter is primarily about information consumption, yet many follows are built on social ties. Not surprisingly, we find that the Twitter follow graph exhibits structural characteristics of both an information network and a social network. Going beyond descriptive characterizations, we hypothesize that from an individual user's perspective, Twitter starts off more like an information network, but evolves to behave more like a social network. We provide preliminary evidence that may serve as a formal model of how a hybrid network like Twitter evolves.	Information network or social network?: the structure of the twitter follow graph	NA:NA:NA:NA	2014
Hong Huang:Jie Tang:Sen Wu:Lu Liu:Xiaoming fu	A closed triad is a group of three people who are connected with each other. It is the most basic unit for studying group phenomena in social networks. In this paper, we study how closed triads are formed in dynamic networks. More specifically, given three persons, what are the fundamental factors that trigger the formation of triadic closure? There are various factors that may influence the formation of a relationship between persons. Can we design a unified model to predict the formation of triadic closure? Employing a large microblogging network as the source in our study, we formally define the problem and conduct a systematic investigation. The study uncovers how user demographics and network topology influence the process of triadic closure. We also present a probabilistic graphical model to predict whether three persons will form a closed triad in dynamic networks. The experimental results on the microblogging data demonstrate the efficiency of our proposed model for the prediction of triadic closure formation.	Mining triadic closure patterns in social networks	NA:NA:NA:NA:NA	2014
Jiaoyan Chen:Huajun Chen:Guozhou Zheng:Jeff Z. Pan:Honghan Wu:Ningyu Zhang	Nowadays, people are increasingly concerned about smog disaster and the caused health hazard. However, the current methods for big smog analysis are usually based on the traditional lagging data sources or merely adopt physical environment observations, which limit the methods' accuracy and usability. The discipline of Web Science, the research fields of which include web of people and web of devices, provides real time web data as well as novel web data analysis approaches. In this paper, both social web data and device web data are proposed for smog disaster analysis. Firstly, we utilize social web data to define and calculate Individual Public Health Indexes (IPHIs) for smog caused health hazard quantification. Secondly, we integrate social web data and device web data to build standard health hazard rating reference and train smog-health models for health hazard prediction. Finally, we apply the rating reference and models to online and location-sensitive smog disaster monitoring, which can better guide people's behaviour and government's strategy design for disaster mitigation.	Big smog meets web science: smog disaster analysis based on social media and device data on the web	NA:NA:NA:NA:NA:NA	2014
Giang Binh Tran:Mohammad Alrifai	Wikipedia's Current Events Portal (WCEP) is a special part of Wikipedia that focuses on daily summaries of news events. The WikiTimes project provides structured access to WCEP by extracting and indexing all its daily news events. In this paper we study this part of Wikipedia and take a closer look into its content and the community behind it. First, we provide descriptive analysis of the collected news events. Second, we compare between the news summaries created by the WCEP crowd and the ones created by professional journalists on the same topics. Finally, we analyze the revision logs of news events over the past 7 years in order to characterize the WCEP crowd and their activities. The results show that WCEP has reached a stable state in terms of the volume of contributions as well as the size of its crowd, which makes it an important source of news summaries for the public and the research community.	Indexing and analyzing wikipedia's current events portal, the daily news summaries by the crowd	NA:NA	2014
Philipp Singer:Fabian Flöck:Clemens Meinhart:Elias Zeitfogel:Markus Strohmaier	In the past few years, Reddit -- a community-driven platform for submitting, commenting and rating links and text posts -- has grown exponentially, from a small community of users into one of the largest online communities on the Web. To the best of our knowledge, this work represents the most comprehensive longitudinal study of Reddit's evolution to date, studying both (i) how user submissions have evolved over time and (ii) how the community's allocation of attention and its perception of submissions have changed over 5 years based on an analysis of almost 60 million submissions. Our work reveals an ever-increasing diversification of topics accompanied by a simultaneous concentration towards a few selected domains both in terms of posted submissions as well as perception and attention. By and large, our investigations suggest that Reddit has transformed itself from a dedicated gateway to the Web to an increasingly self-referential community that focuses on and reinforces its own user-generated image- and textual content over external sources.	Evolution of reddit: from the front page of the internet to a self-referential community?	NA:NA:NA:NA:NA	2014
Masahiro Hamasaki:Masataka Goto:Tomoyasu Nakano	This paper describes a music browsing assistance service, Songrium (http://songrium.jp), which increases user enjoyment when listening to songs and allows visualization and exploration of a ``Web of Music''. We define a Web of Music in this paper to be a network of ``web-native music'', which we define in turn to be music that is published, shared, and remixed (has derivative works created) entirely on the web. Songrium was developed as an attempt to realize a Web of Music, by showing relations between both original songs and derivative works and offering an enriched listening experience. Songrium has analyzed over 600,000 music video clips on the most popular Japanese video-sharing service, Niconico, which contains original songs of web-native music and their derivative works such as covers and dance arrangements. Analysis of over 100,000 original songs reveals that over 500,000 derivative works were generated and have contributed to enrich the Web of Music.	Songrium: a music browsing assistance service with interactive visualization and exploration of protect a web of music	NA:NA:NA	2014
Jaimie Yejean Park:Jiyeon Jang:Alejandro Jaimes:Chin-Wan Chung:Sung-Hyon Myaeng	YouTube is the world's largest video sharing platform where both professional and non-professional users participate in creating, uploading, and viewing content. In this work, we analyze content in the music category created by the non-professionals, which we refer to as user-generated content (UGC). Non-professional users frequently upload content (UGC) that are parodies, remakes, or covers of the music videos uploaded by professionals, namely the official record labels. Along with the success of official music videos on YouTube, we find the increased participation of users in creating the UGC related to the music videos. In this study, we characterize the UGC uploading behavior in terms of what, where, and when. Furthermore, we measure the relationship between the popularity of the original content and creation of the related UGC. We find that the UGC uploading behavior is different depending on the types of the UGC and across different genres of music videos. We also find that UGC sharing is a highly global activity; popular UGC is created from all over the world despite the fact that the popular music videos originate from a very limited number of locations. Our findings imply that utilizing the information on re-created UGC is important in order to understand and to predict the popularity of the original content.	Exploring the user-generated content (UGC) uploading behavior on youtube	NA:NA:NA:NA:NA	2014
Michele Catasta:Alberto Tonon:Djellel Eddine Difallah:Gianluca Demartini:Karl Aberer:Philippe Cudre-Mauroux	Memory queries denote queries where the user is trying to recall from his/her past personal experiences. Neither Web search nor structured queries can effectively answer this type of queries, even when supported by Human Computation solutions. In this paper, we propose a new approach to answer memory queries that we call Transactive Search: The user-requested memory is reconstructed from a group of people by exchanging pieces of personal memories in order to reassemble the overall memory, which is stored in a distributed fashion among members of the group. We experimentally compare our proposed approach against a set of advanced search techniques including the use of Machine Learning methods over the Web of Data, online Social Networks, and Human Computation techniques. Experimental results show that Transactive Search significantly outperforms the effectiveness of existing search approaches for memory queries.	Hippocampus: answering memory queries using transactive search	NA:NA:NA:NA:NA:NA	2014
Olaf Hartig:M. Tamer Özsu	Traversal-based approaches to execute queries over data on the Web have recently been studied. These approaches make use of up-to-date data from initially unknown data sources and, thus, enable applications to tap the full potential of the Web. While existing work focuses primarily on implementation techniques, a principled analysis of subwebs that are reachable by such approaches is missing. Such an analysis may help to gain new insight into the problem of optimizing the response time of traversal-based query engines. Furthermore, a better understanding of characteristics of such subwebs may also inform approaches to benchmark these engines. This paper provides such an analysis. In particular, we identify typical graph-based properties of query-specific reachable subwebs and quantify their diversity. Furthermore, we investigate whether vertex scoring methods (e.g., PageRank) are able to predict query-relevance of data sources when applied to such subwebs.	Reachable subwebs for traversal-based query execution	NA:NA	2014
Thomas Steiner	Wikipedia is a global crowdsourced encyclopedia that at time of writing is available in 287 languages. Wikidata is a likewise global crowdsourced knowledge base that provides shared facts to be used by Wikipedias. In the context of this research, we have developed an application and an underlying Application Programming Interface (API) capable of monitoring realtime edit activity of all language versions of Wikipedia and Wikidata. This application allows us to easily analyze edits in order to answer questions such as "Bots vs. Wikipedians, who edits more?", "Which is the most anonymously edited Wikipedia?", or "Who are the bots and what do they edit?". To the best of our knowledge, this is the first time such an analysis could be done in realtime for Wikidata and for really all Wikipedias-large and small. Our application is available publicly online at the URL http://wikipedia-edits.herokuapp.com/, its code has been open-sourced under the Apache 2.0 license.	Bots vs. wikipedians, anons vs. logged-ins	NA	2014
Grégoire Burel:Yulan He	We describe the Joint Effort-Topic (JET) model and the Author Joint Effort-Topic (aJET) model that estimate the effort required for users to contribute on different topics. We propose to learn word-level effort taking into account term preference over time and use it to set the priors of our models. Since there is no gold standard which can be easily built, we evaluate them by measuring their abilities to validate expected behaviours such as correlations between user contributions and the associated effort.	Quantising contribution effort in online communities	NA:NA	2014
Evangelos Papalexakis:Konstantinos Pelechrinis:Christos Faloutsos	The proliferation of mobile devices that are capable of estimating their position, has lead to the emergence of a new class of social networks, namely location-based social networks (LBSNs for short). The main interaction between users in an LBSN is location sharing. While the latter can be realized through continuous tracking of a user's whereabouts from the service provider, the majority of LBSNs allow users to voluntarily share their location, through  check-ins. LBSNs provide incentives to users to perform check-ins. However, these incentives can also lead to people faking their location, thus, generating false information. In this work, we propose the use of tensor decomposition for spotting anomalies in the check-in behavior of users. To the best of our knowledge, this is the first attempt to model this problem using tensor analysis.	Spotting misbehaviors in location-based social networks using tensors	NA:NA:NA	2014
Claudia Wagner:Philipp Singer:Markus Strohmaier	Since food is one of the central elements of all human beings, a high interest exists in exploring temporal and spatial food and dietary patterns of humans. Predominantly, data for such investigations stem from consumer panels which continuously capture food consumption patterns from individuals and households. In this work we leverage data from a large online recipe platform which is frequently used in the German speaking regions in Europe and explore (i) the association between geographic proximity and shared food preferences and (ii) to what extent temporal information helps to predict the food preferences of users. Our results reveal that online food preferences of geographically closer regions are more similar than those of distant ones and show that specific types of ingredients are more popular on specific days of the week. The observed patterns can successfully be mapped to known real-world patterns which suggests that existing methods for the investigation of dietary and food patterns (e.g., consumer panels) may benefit from incorporating the vast amount of data generated by users browsing recipes on the Web.	Spatial and temporal patterns of online food preferences	NA:NA:NA	2014
Fred Morstatter:Jürgen Pfeffer:Huan Liu	Twitter shares a free 1% sample of its tweets through the "Streaming API". Recently, research has pointed to evidence of bias in this source. The methodologies proposed in previous work rely on the restrictive and expensive Firehose to find the bias in the Streaming API data. We tackle the problem of finding sample bias without costly and restrictive Firehose data. We propose a solution that focuses on using an open data source to find bias in the Streaming API.	When is it biased?: assessing the representativeness of twitter's streaming API	NA:NA:NA	2014
Ricardo Kawase:Patrick Siehndel:Eelco Herder	In this paper, we aim at finding out which users are likely to publicly demonstrate frustration towards their jobs on the microblogging platform Twitter - we will call these users haters. We show that the profiles of haters have specific characteristics in terms of vocabulary and connections. The implications of these findings may be used for the development of an early alert system that can help users to think twice before they post potentially self-harming content.	Haters gonna hate: job-related offenses in twitter	NA:NA:NA	2014
Kaweh Djafari Naini:Ricardo Kawase:Nattiya Kanhabua:Claudia Niederee	One of the core challenges of automatically creating Social Web summaries is to decide which posts to remember, i.e., to consider for summary inclusion and which to forget. Keeping everything would overwhelm the user and would also neglect the often intentionally ephemeral nature of Social Web posts. In this paper, we analyze high-impact features that characterize memorable posts as a first step for this selection process. Our work is based on a user evaluation for discovering human expectations towards content retention.	Characterizing high-impact features for content retention in social web applications	NA:NA:NA:NA	2014
Eunyoung Kim:Hwon Ihm:Sung-Hyon Myaeng	Location-based social network services (LBSNS) such as Foursquare are getting the highlight with the extensive spread of GPS-enabled mobile devices, and a large body of research has been conducted to devise methods for understanding and clustering places. However, in previous studies, the predefined set of semantic categories of places play a critical role in both discovery and evaluation of the results, despite its limited ability to represent the dynamics of the places. We explore beyond the predefined semantic categories of the places and discover topic-based place semantics through the use of Latent Dirichlet Allocation, by extracting topics from the text which people post on site. We also show the proposed method allows for understanding the temporal dynamics of the place semantics. The finding of this study is intended for, but not limited to, context aware services and place recommendation systems.	Topic-based place semantics discovered from microblogging text messages	NA:NA:NA	2014
Haewoon Kwak:Jong Gun Lee	We explore how research papers are shared in Twitter to understand its potential and limitation of the current practice that measures or predicts the scientific impact of research papers from the web. We track 54 second-level domains offering the top 100 journals listed in Google Scholar and collect 403,165 tweets sharing 75,677 unique research papers by 142,743 users over the course of 135 days. Our findings show the great potential of Twitter as a platform for paper sharing, but at the same time, indicate the limitations of measuring scientific impact through the lens of social media mainly due to the highly skewed and limited attention to few number of top journals.	Has much potential but biased: exploring the scholarly landscape in twitter	NA:NA	2014
Ramine Tinati:Leslie Carr:Susan Halford:Catherine Pope	NA	(Re)integrating the web: beyond 'socio-technical'	NA:NA:NA:NA	2014
Jasper Oosterman:Alessandro Bozzon:Geert-Jan Houben:Archana Nottamkandath:Chris Dijkshoorn:Lora Aroyo:Mieke H.R. Leyssen:Myriam C. Traub	The results of our exploratory study provide new insights to crowdsourcing knowledge intensive tasks. We designed and performed an annotation task on a print collection of the Rijksmuseum Amsterdam, involving experts and crowd workers in the domain-specific description of depicted flowers. We created a testbed to collect annotations from flower experts and crowd workers and analyzed these in regard to user agreement. The findings show promising results, demonstrating how, for given categories, nichesourcing can provide useful annotations by connecting crowdsourcing to domain expertise.	Crowd vs. experts: nichesourcing for knowledge intensive tasks in cultural heritage	NA:NA:NA:NA:NA:NA:NA:NA	2014
Claudia Orellana-Rodriguez:Wolfgang Nejdl:Ernesto Diaz-Aviles:Ismail Sengor Altingovde	User-generated content is a growing source of valuable information and its analysis can lead to a better understanding of the users needs and trends. In this paper, we leverage user feedback about YouTube videos for the task of affective video ranking. To this end, we follow a learning to rank approach, which allows us to compare the performance of different sets of features when the ranking task goes beyond mere relevance and requires an affective understanding of the videos. Our results show that, while basic video features, such as title and tags, lead to effective rankings in an affective-less setup, they do not perform as good when dealing with an affective ranking task.	Learning to rank for joy	NA:NA:NA:NA	2014
Willem Robert van Hage:Thomas Ploeger:Jesper Hoeksema	In this article we investigate the properties of the frequency distribution of numbers on the Web. We work with a part of the Common Crawl dataset comprising 3.8 billion Web documents and a recent dump of the English language Wikipedia. We show that, like words, numbers on the Web follow a Power law distribution, and obey Benford's law of first-digits. We show and explain regularities in the distribution, and compare the regularities in Common Crawl to those in Wikipedia. The comparison stresses which patterns in the frequency distributions follow from human thought.	Number frequency on the web	NA:NA:NA	2014
Rahul Parundekar	In this poster, we propose WebAlive, a new paradigm that brings any Thing in the realm of reality to `life' on the Web by creating an entangled virtual existence for it. For example, consider a physical object like the lamp on your desk. As a human, you are aware of its existence. You can see it, touch it, feel it, interact with it (turn it on, o, etc.), investigate its attributes (color, material, etc.). You can even alter or trash it. Now, imagine that the lamp had a virtual counterpart on the Web such that the two of them were entangled with each other. A software agent on the Web can now become aware of its existence, it can perform actions on it, learn about its attributes and even (with the right capability) alter it just how it was accessible to you. Referring to the most broadest definition of the word Thing, we can extend this entanglement concept beyond physical things like devices, objects, people, etc. to all intelligible things like entities, processes, concepts, ideas, etc. By doing so, we can open a new realm of possibilities where we could have digital access to the world around us. This poster provides an introduction to the paradigm and presents an approach to realize it using Web standards.	WebAlive: a new paradigm for bringing things to life on the web	NA	2014
Zhe Liu:Ingmar Weber	The rise in popularity of Twitter in recent years has in parallel led to an increase in online controversies. To monitor and control such conflicts early on, we design and evaluate a language-agnostic classifier to tell pairs of ideological friends from foes. We build the classifier using features from four different aspects: user-based, interaction-based, relationship-based and conflict-based. By experimenting with three large data sets containing diverse conflicts, we demonstrate the effectiveness of language-agnostic classification of ideological relation, achieving satisfactory results across all three data sets. Such a classifier potentially enables studies of diverse conflicts on Twitter on a large scale.	Predicting ideological friends and foes in Twitter conflicts	NA:NA	2014
Dave Raggett	The Web is still awkward when it comes to online editing. It is time to fix that, and make it easier for developers to create smarter browser based editing tools. This short paper presents work on a test framework for a cross browser open source library for browser based editing. The aim is to encourage a proliferation of different kinds of browser based editing for a wide range of purposes. The library steps around the considerable variations in the details of browser support for designMode and contentEditable, which have acted as a brake on realizing the full potential for browser based editing.	Testsuite and harness for browser based editing	NA	2014
Federico Tomassetti:Giuseppe Rizzo:Raphael Troncy	Nowadays, most of the web frameworks are developed using different programming languages, both for server and client side programmes. The typical scenario includes a general purpose language (e.g. Ruby, Python, Java) used together with different specialized languages: HTML, CSS, Javascript and SQL. All the artifacts are connected via different types of relations, most of which depend on the adopted framework. These cross-language relations are normally not captured by tools which require the developer to learn and to remember those associations in order to understand and maintain the application. This paper describes a library for detecting cross-language relations in polyglot frameworks. The library has been developed to be modular and to be easily integrated in existing IDEs. The library is publicly available at http://github.com/CrossLanguageProject/crosslanguagespotter	CrossLanguageSpotter: a library for detecting relations in polyglot frameworks	NA:NA:NA	2014
Sungjae Han:Geunseong Jung:Minsoo Ryu:Byung-Uk Choi:Jaehyuk Cha	This paper proposes a new voice web browser that can be operated in smart TV environments. Previous voice web browsers had the limitation of being run under limited conditions; for example, a list of the specific contents of a page was outputted by voice, or the user entered a search term by voice. In our method proposed in this paper, all the hierarchical menu areas on a web page are recognized and controlled with voice keywords so that page navigation according to a menu can be conveniently done in a voice supported web browser. Although many studies have been conducted on web page menu recognition, most of them provide insufficient information to recognize the hierarchical menu structure. In other words, most web pages in recent browsers showed submenus only as a result of a specific user interaction, since these previous studies had no way of recognizing or controlling the submenus. Therefore, in the web browser proposed in this study, a hierarchical menu structure, which is inserted dynamically via user interaction, is recognized and selected by voice, thus making it possible to maneuver on the web page. Furthermore, the core code of the browser is implemented in JavaScript, so it can be flexibly used not only for a web browser on Smart TVs, but also as functional extensions of existing web browsers in a PC environment.	A voice-controlled web browser to navigate hierarchical hidden menus of web pages in a smart-tv environment	NA:NA:NA:NA:NA	2014
Stefano Ortona:Luying Chen:Giorgio Orsi	Named entity extractors are a popular means for enriching documents with semantic annotations. Both the overlap and the increasing diversity in the capabilities and in the vocabularies of the annotators motivate the need for managing and integrating semantic annotations in a coherent and uniform fashion. ROSeAnn is a framework for the management and the reconciliation of semantic annotations. It provides end-users and programmers with a unified view over the results of multiple online and standalone annotators, linking them to an integrated ontology of their vocabularies, and supporting a variety of document formats such as: plain text, live Web pages, and PDF documents. Although ROSeAnn provides two predefined algorithms for conflict resolution -- one supervised, appropriate when representative training data is available, and one unsupervised -- it also allows application developers to define their own integration techniques, as well as extending the pool of annotators as new ones become available.	ROSeAnn: taming online semantic annotators	NA:NA:NA	2014
Kevin Borgolte:Christopher Kruegel:Giovanni Vigna	Tracking the evolution of websites has become fundamental to the understanding of today's Internet. The automatic reasoning of how and why websites change has become essential to developers and businesses alike, in particular because the manual reasoning has become impractical due to the sheer number of modifications that websites undergo during their operational lifetime, including but not limited to rotating advertisements, personalized content, insertion of new content, or removal of old content. Prior work in the area of change detection, such as XyDiff, X-Diff or AT&T's internet difference engine, focused mainly on ``diffing'' XML-encoded literary documents or XML-encoded databases. Only some previous work investigated the differences that must be taken into account to accurately extract the difference between HTML documents for which the markup language does not necessarily describe the content but is used to describe how the content is displayed instead. Additionally, prior work identifies all changes to a website, even those that might not be relevant to the overall analysis goal, in turn, they unnecessarily burden the analysis engine with additional workload. In this paper, we introduce a novel analysis framework, the Delta framework, that works by (i) extracting the modifications between two versions of the same website using a fuzzy tree difference algorithm, and (ii) using a machine-learning algorithm to derive a model of relevant website changes that can be used to cluster similar modifications to reduce the overall workload imposed on an analysis engine. Based on this model for example, the tracked content changes can be used to identify ongoing or even inactive web-based malware campaigns, or to automatically learn semantic translations of sentences or paragraphs by analyzing websites that are available in multiple languages. In prior work, we showed the effectiveness of the Delta framework by applying it to the detection and automatic identification of web-based malware campaigns on a data set of over 26 million pairs of websites that were crawled over a time span of four months. During this time, the system based on our framework successfully identified previously unknown web-based malware campaigns, such as a targeted campaign infecting installations of the Discuz!X Internet forum software.	Relevant change detection: a framework for the precise extraction of modified and novel web-based content as a filtering technique for analysis engines	NA:NA:NA	2014
Pavel Arapov:Michel Buffa:Amel Ben Othmane	WikiNEXT is a wiki engine 100% written in JavaScript that relies on recent APIs and frameworks. It has been designed to author web applications directly in a web browser, which can exploit the web of data. It combines the functionalities of a semantic wiki with those of a Web-based IDE (Integrated Development Environment) in order to develop web applications in addition to writing classic documents. It gives developers a rich internal API (Application Programming Interface) and provides several functionalities to exploit the web of data. Our approach uses templates, a special type of wiki pages that represent the semantic data model. Templates generate wiki pages with semantic annotations that are stored as quadruplets in a triple store engine. To query this semantic data, we provide a SPARQL endpoint. Screencasts are available on YouTube (look for WikiNEXT).	Developing web of data applications from the browser	NA:NA:NA	2014
Yoelle Maarek	In spite of personal communications moving more and more towards social and mobile, especially with younger generations, email traffic continues to grow. This growth is mostly attributed to (non-spam) machine-generated email, which, against common perception, is often extremely valuable. Indeed, together with monthly newsletters that can easily be ignored, inboxes contain flight itineraries, booking confirmations, receipts or invoices that are critical to many users. In this talk, I will discuss the new nature of consumer email, which is dominated by machine-generated messages of highly heterogeneous forms and value. I will show how the change has not been fully recognized yet by my most email clients (as an example, why should there still be a reply option associated with a message coming from a "[email protected]" address?). I will introduce some approaches for large-scale mail mining specifically tailored to machine-generated email. I will conclude by discussing possible applications and research directions.	When machines dominate humans: the challenges of mining and consuming machine-generated web mail	NA	2014
Pavel Serdyukov	Yandex is one of the largest internet companies in Europe, operating Russia's most popular search engine, generating 62\% of all search traffic in Russia, what means processing about 220 million queries from about 22 million users daily. Clearly, the amount and the variety of user behavioral data which we can monitor at search engines is rapidly increasing. Still, we do not always recognize its potential to help us solve the most challenging search problems and do not immediately know the ways to deal with it most effectively both for search quality evaluation and for its improvement. My talk will focus on various practical challenges arising from the need to "grok" search engine users and do something useful with the data they most generously, though almost unconsciously share with us. I will also present some answers to that by overviewing our latest research on user model based retrieval quality evaluation, implicit feedback mining and personalization. I will also summarize the experience we gained from organizing three data mining challenges at the series of workshops on using search click data (WSCD) organized in the scope of WSDM 2012 -- 2014 conferences. These challenges provided a unique opportunity to consolidate and scrutinize the work from search engines' industrial labs on analyzing behavioral data. Each year we publicly shared a fully anonymized dataset extracted from Yandex query logs and asked participants to predict editorial relevance labels of documents using search logs (in 2011), detect search engine switchings in search sessions (in 2012) and personalize web search using the long-term (user history based) and short-term (session-based) user context (in 2013).	Analyzing behavioral data for improving search experience	NA	2014
Martin Atzmueller:Alvin Chin:Christoph Trattner	NA	Session details: Modeling social media: mining big data in social media and the web (MSM 2014)	NA:NA:NA	2014
Daniele Quercia	Cities are attracting considerable research interest. The agenda behind smart cities is popular among computer scientists and engineers: new monitoring technologies promise to allocate urban resources (e.g., electricity, clean water, car traffic) more efficiently and, as such, make our cities 'smarter'. This talk offers a rare counterpoint to that dominant efficiency-driven narrative. It is about recent research on the relationship between happiness and cities [1]: which urban elements make people happy? To help answer that question, I built a web game with collaborators at the University of Cambridge in which users are shown ten pairs of urban scenes of London and, for each pair, a user needs to choose which one they consider to be most beautiful, quiet, and happy. Based on user votes, we are able to rank all urban scenes according to these three attributes. We recently analyzed the scenes with ratings using image processing tools [2]. We discovered that the amount of greenery in any given scene is associated with all the three attributes and that cars and fortress-like buildings are associated with sadness (we equated sadness to our measurement for the low end of our 'spectrum' of happiness). In contrast, public gardens and Victorian and red brick houses are associated with happiness. Our results (including those about distinctive and memorable areas [3]) all point in the same direction: urban elements that hinder social interactions are undesirable, while elements that increase interactions are the ones that should be integrated by urban planners to retrofit our cities for happiness. Now, as a computer scientist, you might wonder: can these findings be used to build better online tools? The answer is a definite 'Yes'! Existing mapping technologies, for example, return shortest directions. To complement them, we are designing new tools that return directions that are not only short but also tend to make urban walkers happy [4]. Another application comes from the mobile world. In mobile settings, geo-referenced content becomes increasingly important, and content about a neighborhood inherently depends on the way the neighborhood is perceived by people: whether it is, for instance, distinctive and beautiful or not. We are designing an application that identifies memorable city pictures by predicting which neighborhoods tend to be beautiful and which tend to make people happy [5].	The pursuit of urban happiness	NA	2014
Ramesh R. Sarukkai	Over the last 4 years, YouTube has grown from a viral video sharing site to a platform that fuels a win-win ecosystem for video content creators, advertisers and users. A key driving force behind this successful transformation is building out products/platforms that focus and optimize for the user. In this talk, we will talk about user-centric efforts such as launch of user-controlled skippable ads (TrueView Instream), and Dynamic Ad Loads (machine learning system that balances realtime user impact with advertising policies). Leveraging very large amounts of real-time activity data is paramount to successfully building and deploying such user-centric models. We conclude the talk with challenges and opportunities in this important area of real-time user analysis and large data modeling.	YouTube monetization: creating user-centric experiences using large scale data	NA	2014
Cody Buntain:Jennifer Golbeck	As social networks and the user-generated content that populates them continue to grow in prevalence, size, and influence, understanding how users interact and produce this content becomes increasingly important. Insight into these community dynamics could prove valuable for measuring content trust, providing role-based group recommendations, or evaluating group stability and growth. To this end, we explore user posting behavior on reddit, a large social networking site comprised of many sub-communities in which a user may participate simultaneously. We demonstrate that the well-known "answer-person" role is present in the reddit community, provide an exposition on an automated method for identifying this role based solely on user interactions (foregoing expensive content analysis), and show that users rarely exhibit significant participation in more than one communities.	Identifying social roles in reddit using network structure	NA:NA	2014
Simon Dooms:Toon De Pessemier:Luc Martens	While rating data is essential for all recommender systems research, there are only a few public rating datasets available, most of them years old and limited to the movie domain. With this work, we aim to end the lack of rating data by illustrating how vast amounts of ratings can be unambiguously collected from Twitter. We validate our approach by mining ratings from four major online websites focusing on movies, books, music and video clips. In a short mining period of 2 weeks, close to 3 million ratings were collected. Since some users turned up in more than one dataset, we believe this work to be amongst the first to provide a true cross-domain rating dataset.	Mining cross-domain rating datasets from structured data on twitter	NA:NA:NA	2014
Nathan Kallus	With public information becoming widely accessible and shared on today's web, greater insights are possible into crowd actions by citizens and non-state actors such as large protests and cyber activism. We present efforts to predict the occurrence, specific timeframe, and location of such actions before they occur based on public data collected from over 300,000 open content web sources in 7 languages, from all over the world, ranging from mainstream news to government publications to blogs and social media. Using natural language processing, event information is extracted from content such as type of event, what entities are involved and in what role, sentiment and tone, and the occurrence time range of the event discussed. Statements made on Twitter about a future date from the time of posting prove particularly indicative. We consider in particular the case of the 2013 Egyptian coup d'état. The study validates and quantifies the common intuition that data on social media (beyond mainstream news sources) are able to predict major events.	Predicting crowd behavior with big public data	NA	2014
Martin Atzmueller:Andreas Ernst:Friedrich Krebs:Christoph Scholz:Gerd Stumme	This paper focuses on the analysis of group evolution events in networks of face-to-face proximity. First, we analyze statistical properties of group evolution, e.g., individual activity and typical group sizes. Furthermore, we define a set of specific group evolution events. We analyze these using real-world data collected at the LWA 2010 conference using the Conferator system, and discuss patterns according to different phases of the conference.	On the evolution of social groups during coffee breaks	NA:NA:NA:NA:NA	2014
Christoph Scholz:Martin Atzmueller:Gerd Stumme	This paper focuses on the predictability of recurring links: These links are generated repeatedly in a network for different forms of social ties, e.g. by face-to-face interactions in offline social networks. In particular, we analyse the predictability of recurring links in networks of face-to-face proximity using several path-based measures, and compare these to network-proximity measures based on the nodes' neighbourhood. Furthermore, we show that the current tie strength is a good predictor for this link prediction task. In addition we show that the removal of weak ties improves the predictability for most of the considered network proximity measures. For our analysis we utilize three real-world datasets collected at different scientific conferences using the Conferator (http://www.conferator.org) system.	On the predictability of recurring links in networks of face-to-face proximity	NA:NA:NA	2014
KyoungMin Ryoo:Sue Moon	Geographic locations of users form an important axis in public polls and localized advertising, but are not available by default. The number of users who make their locations public or use GPS tagging is relatively small, compared to the huge number of users in online social networking services and social media platforms. In this work we propose a new framework to infer a user's main location of activities in Twitter using their textual contents. Our approach is based on a probabilistic generative model that filters local words, employs data binning for scalability, and applies a map projection technique for performance. For Korean Twitter users, we report that 60% of users are identified within 10 km of their locations, a significant improvement over existing approaches.	Inferring Twitter user locations with 10 km accuracy	NA:NA	2014
Patty Kostkova:Daniela Paolotti:John Brownstein	NA	Session details: Public health in the digital age: social media, crowdsourcing and participatory systems (2nd PHDA 2014)	NA:NA:NA	2014
Todd Bodnar:Victoria C. Barclay:Nilam Ram:Conrad S. Tucker:Marcel Salathé	Social media has been considered as a data source for tracking disease. However, most analyses are based on models that prioritize strong correlation with population-level disease rates over determining whether or not specific individual users are actually sick. Taking a different approach, we develop a novel system for social-media based disease detection at the individual level using a sample of professionally diagnosed individuals. Specifically, we develop a system for making an accurate influenza diagnosis based on an individual's publicly available Twitter data. We find that about half (17/35 = 48.57%) of the users in our sample that were sick explicitly discuss their disease on Twitter. By developing a meta classifier that combines text analysis, anomaly detection, and social network analysis, we are able to diagnose an individual with greater than 99% accuracy even if she does not discuss her health.	On the ground validation of online diagnosis with Twitter and medical records	NA:NA:NA:NA:NA	2014
Patty Kostkova:Stephan Garbin:Justin Moser:Wendy Pan	Traditional public health surveillance systems would benefit from integration with knowledge created by new situation-aware realtime signals from social media, online searches, mobile/sensor networks and citizens' participatory surveillance systems. However, the challenge of threat validation, cross-verification and information integration for risk assessment has so far been largely untackled. In this paper, we propose a new system, medi+board, monitoring epidemic intelligence sources and traditional case-based surveillance to better automate early warning, cross-validation of signals for outbreak detection and visualization of results on an interactive dashboard. This enables public health professionals to see all essential information at a glance. Modular and configurable to any 'event' defined by public health experts, medi+board scans multiple data sources, detects changing patterns and uses a configurable analysis module for signal detection to identify a threat. These can be validated by an analysis module and correlated with other sources to assess the reliability of the event classified as the reliability coefficient which is a real number between zero and one. Events are reported and visualized on the medi+board dashboard which integrates all information sources and can be navigated by a timescale widget. Simulation with three datasets from the swine flu 2009 pandemic (HPA surveillance, Google news, Twitter) demonstrates the potential of medi+board to automate data processing and visualization to assist public health experts in decision making on control and response measures.	Integration and visualization public health dashboard: the medi+board pilot project	NA:NA:NA:NA	2014
Patipat Susumpow:Patcharaporn Pansuwan:Nathalie Sajda:Adam W. Crawley	This paper reports the work in progress of incorporating a participatory disease detection mechanism into the existing web- and mobile device application DoctorMe in Thailand. As Southeast Asia has a high likelihood of hosting potential outbreaks of epidemics it is crucial to enable citizens to collectively contribute to improved public health through crowdsourced data, which is currently lacking. This paper focuses foremost on the localised approach, utilizing elements such as gamification, digital volunteerism and personalised health recommendations for participating users. DoctorMe's participatory disease detection approach aims to tap into the accelerating technological landscape in Thailand and to improve personal health and provide valuable data for institutional analysis that may prevent or decrease the impact of infectious disease outbreaks.	Participatory disease detection through digital volunteerism: how the doctorme application aims to capture data for faster disease detection in thailand	NA:NA:NA:NA	2014
Deleer Barazanji:Pär Bjelkmar	Information technology contributes greatly in improving people's health. Through our interaction with different communication channels such as social media, telephone calls or purchasing over-the-counter medicines, we emit signals and leave trails of information related to our health. Information that can be used to understand our health situation. Some of these communication channels are more structured, filtered and suited for evaluation, as a result they require less demanding filtration and analysis than others. One such channel is the Swedish National Telephone Health Service 1177, where professional healthcare personnel assist and give advice to callers. The main aim of this work is to detect point source outbreaks. For this purpose a project called Event-based Surveillance System (ESS) was initiated to develop a system for surveillance and detection using the former mentioned information source. The system is currently running and is used to notify local authorities whenever a deviation in the telephone traffic pattern is recorded.	System for surveillance and investigation of disease outbreaks	NA:NA	2014
Hans C. Ossebaard	Zoonoses are a class of infectious diseases causing growing concern of health authorities worldwide. Human and economic costs of zoonoses are substantial, especially in low-resource countries. New zoonoses emerge as a consequence of ecological, demographic, cultural, social and behavioral factors. Meanwhile, global antimicrobial resistance increases. This public health threat demands for a new approach to which the concept of "One Health" is emblematical. It emphasizes the interconnectedness of human, animal and environmental health. To protect and improve public health it is imperative that transdisciplinary collaboration and communication takes place between the human and the veterinary domain. This strategy is now widely endorsed by international, regional and national health policy and academic bodies. Nonetheless the contributions of both the social sciences and the new data sciences need more appreciation. Evidence is available that the methods and concepts they provide can budge "One Health".	One health informatics	NA	2014
Muhammad Imran:Carlos Castillo	Microblogging platforms such as Twitter have become a valuable resource for disease surveillance and monitoring. Automatic classification can be used to detect disease-related messages and to sort them into meaningful categories. In this paper, we show how the AIDR (Artificial Intelligence for Disaster Response) platform can be used to harvest and perform analysis of tweets in real-time using supervised machine learning techniques. AIDR is a volunteer-powered online social media content classification platform that automatically learns from a set of human-annotated examples to classify tweets into user-defined categories. In addition, it automatically increases classification accuracy as new examples become available. AIDR can be operated through a web interface without the need to deal with the complexity of the machine learning methods used.	Volunteer-powered automatic classification of social media messages for public health in AIDR	NA:NA	2014
Andrew R. McNeill:Pam Briggs	Twitter can be a powerful tool for the dissemination and discussion of public health information but how can we best describe its influence? In this paper we draw on social-psychological concepts such as social norms, social representations, emotions and rhetoric to explain how influence works both in terms of the spread of information and also its personal impact. Using tweets drawn from a range of health issues, we show that social psychological theory can be used in the qualitative analysis of Twitter data to further our understanding of how health behaviours can be affected by social media discourse.	Understanding Twitter influence in the health domain: a social-psychological contribution	NA:NA	2014
Aziz Mohaisen:Hyoungshick Kim:Yong Li	NA	Session details: Simplifying complex networks for practitioners 2014 workshop	NA:NA:NA	2014
Junzhou Zhao:John C.S. Lui:Don Towsley:Xiaohong Guan	As an important metric in graphs, group closeness centrality measures how close a group of vertices is to all other vertices in a graph, and it is used in numerous graph applications such as measuring the dominance and influence of a node group over the graph. However, when a large-scale graph contains hundreds of millions of nodes/edges which cannot reside entirely in computer's main memory, measuring and maximizing group closeness become challenging tasks. In this paper, we present a systematic solution for efficiently calculating and maximizing the group closeness for disk-resident graphs. Our solution first leverages a probabilistic counting method to efficiently estimate the group closeness with high accuracy, rather than exhaustively computing it in an exact fashion. In addition, we design an I/O-efficient greedy algorithm to find a node group that maximizes group closeness. Our proposed algorithm significantly reduces the number of random accesses to disk, thereby dramatically improving computational efficiency. Experiments on real-world big graphs demonstrate the efficacy of our approach.	Measuring and maximizing group closeness centrality over disk-resident graphs	NA:NA:NA:NA	2014
James Atwood:Bruno Ribeiro:Don Towsley	Preferential attachment (PA) models of network structure are widely used due to their explanatory power and conceptual simplicity. PA models are able to account for the scale-free degree distributions observed in many real-world large networks through the remarkably simple mechanism of sequentially introducing nodes that attach preferentially to high-degree nodes. The ability to efficiently generate instances from PA models is a key asset in understanding both the models themselves and the real networks that they represent. Surprisingly, little attention has been paid to the problem of efficient instance generation. In this paper, we show that the complexity of generating network instances from a PA model depends on the preference function of the model, provide efficient data structures that work under any preference function, and present empirical results from an implementation based on these data structures. We demonstrate that, by indexing growing networks with a simple augmented heap, we can implement a network generator which scales many orders of magnitude beyond existing capabilities (106 -- 108 nodes). We show the utility of an efficient and general PA network generator by investigating the consequences of varying the preference functions of an existing model. We also provide "quicknet", a freely-available open-source implementation of the methods described in this work.	Efficient network generation under general preferential attachment	NA:NA:NA	2014
Hyoungshick Kim:Konstantin Beznosov:Eiko Yoneki	The problem of spreading information is a topic of considerable recent interest, but the traditional influence maximization problem is inadequate for a typical viral marketer who cannot access the entire network topology. To fix this flawed assumption that the marketer can control any arbitrary k nodes in a network, we have developed a decentralized version of the influential maximization problem by influencing k neighbors rather than arbitrary users in the entire network. We present several reasonable neighbor selection schemes and evaluate their performance with a real dataset collected from Twitter. Unlike previous studies using network topology alone or synthetic parameters, we use real propagation rate for each node calculated from the Twitter messages during the 2010 UK election campaign. Our experimental results show that information can be efficiently propagated in online social networks using neighbors with a high propagation rate rather than those with a high number of neighbors.	Finding influential neighbors to maximize information diffusion in twitter	NA:NA:NA	2014
Matthew Thomas:Aziz Mohaisen	In this paper we focus on detecting and clustering distinct groupings of domain names that are queried by numerous sets of infected machines. We propose to analyze domain name system (DNS) traffic, such as Non-Existent Domain (NXDomain) queries, at several premier Top Level Domain (TLD) authoritative name servers to identify strongly connected cliques of malware related domains. We illustrate typical malware DNS lookup patterns when observed on a global scale and utilize this insight to engineer a system capable of detecting and accurately clustering malware domains to a particular variant or malware family without the need for obtaining a malware sample. Finally, the experimental results of our system will provide a unique perspective on the current state of globally distributed malware, particularly the ones that use DNS.	Kindred domains: detecting and clustering botnet domains using DNS traffic	NA:NA	2014
Ahmad Slim:Jarred Kozlick:Gregory L. Heileman:Jeff Wigdahl:Chaouki T. Abdallah	Crucial courses have a high impact on students progress at universities and ultimately on graduation rates. Detecting such courses should therefore be a major focus of decision makers at universities. Based on complex network analysis and graph theory, this paper proposes a new framework to not only detect such courses, but also quantify their cruciality. The experimental results conducted using data from the University of New Mexico (UNM) show that the distribution of course cruciality follows a power law distribution. The results also show that the ten most crucial courses at UNM are all in mathematics. Applications of the proposed framework are extended to study the complexity of curricula within colleges, which leads to a consideration of the creation of optimal curricula. Optimal curricula along with the earned letter grades of the courses are further exploited to analyze the student progress. This work is important as it presents a robust framework to ensure the ease of flow of students through curricula with the goal of improving a university's graduation rate.	Network analysis of university courses	NA:NA:NA:NA:NA	2014
Yeon-sup Lim:Bruno Ribeiro:Don Towsley	In this work, we develop techniques to identify the latent infected nodes in the presence of missing infection time-and-state data. Based on the likely epidemic paths predicted by the simple susceptible-infected epidemic model, we propose a measure (Infection Betweenness Centrality) for uncovering unknown infection states. Our experimental results using machine learning algorithms show that Infection Betweenness Centrality is the most effective feature for identifying latent infected nodes.	Classifying latent infection states in complex networks	NA:NA:NA	2014
Xiangming Zhu:Yong LI:Depeng Jin:Pan Hui	With the rapid emergence of applications in mobile networks, understanding and characterizing their properties becomes extremely important. In this paper, from the fundamental model of time-varying graphs, we introduce Temporal Capacity Graphs (TCG), which characterizes the maximum amount of the data that can be transmitted between any two nodes within any time, and consequently reveals the transmission capacity of the whole network. By applying TCG to several realistic mobile networks, we analyze their unique properties. Moreover, using TCG, we reveal the fundamental relationships and tradeoffs between the mobile network settings and system performance.	Temporal capacity graphs for time-varying mobile networks	NA:NA:NA:NA	2014
Shan Lu:Jieqi Kang:Weibo Gong:Don Towsley	In this paper, we proposed a network comparison method based on the mathematical theory of diffusion over manifolds using random walks over graphs. We show that our method not only distinguishes between graphs with different degree distributions, but also different graphs with the same degree distributions. We compare the undirected power law graphs generated by Barabasi-Albert model and directed power law graphs generated by Krapivsky's model to the random graphs generated by Erdos-Renyi model. We also compare power law graphs generated by four different generative models with the same degree distribution.	Complex network comparison using random walks	NA:NA:NA:NA	2014
Jae-wook Jang:Jiyoung Woo:Jaesung Yun:Huy Kang Kim	In this work, we aim to classify malware using automatic classifiers by employing graph metrics commonly used in social network analysis. First, we make a malicious system call dictionary that consists of system calls found in malware. To analyze the general structural information of malware and measure the influence of system calls found in malware, we adopt social network analysis. Thus, we use social network metrics such as the degree distribution, degree centrality, and average distance, which are implicitly equivalent to distinct behavioral characteristics. Our experiments demonstrate that the proposed system performs well in classifying malware families within each malware class with accuracy greater than 98%. As exploiting the social network properties of system calls found in malware, our proposed method can not only classify the malware with fewer features than previous methods adopting graph features but also enables us to build a quick and simple detection system against malware.	Mal-netminer: malware classification based on social network analysis of call graph	NA:NA:NA:NA	2014
Ryan H. Choi:Youngil Choi	A mobile cloud web browser is a web browser that enables mobile devices with constrained resources to support complex web pages by performing most of resource demanding operations on a cloud web server. In this paper, we present a design of a mobile web cloud browser with efficient data structure.	Designing a high-performance mobile cloud web browser	NA:NA	2014
Jae-wook Jang:Jaesung Yun:Jiyoung Woo:Huy Kang Kim	NA	Andro-profiler: anti-malware system based on behavior profiling of mobile malware	NA:NA:NA:NA	2014
Meeyoung Cha	Social media and blogging services have become extremely popular. Every day hundreds of millions of users share conversations on random thoughts, emotional expressions, political news, and social issues. Users interact by following each other's updates and passing along interesting pieces of information to their friends. Information therefore can diffuse widely and quickly through social links. Information propagation in networks like Twitter is unique in that traditional media sources and word-of-mouth propagation coexist. The availability of digitally-logged propagation events in social media help us better understand how user influence, tie strength, repeated exposures, conventions, and various other factors come into play in the way people generate and consume information in the modern society. In this talk, I will present several findings on how bad news [9], rumors [8], prominent events [11], conventions [6, 7], tags [1, 4], behaviors [12], and moods [10] propagate in social media based on a large amount of data collected from networks like Twitter, Flickr, Facebook, and Blogosphere. I will talk about the different roles of user types [2] and content types [5] in propagations as well as ways to measure their influence [3]. Among various findings, I will demonstrate that indegree of a user, a well-known measure of popularity, alone can reveal little about the influence.	Propagation phenomena in large social networks	NA	2014
Gianmarco De Francisci Morales:Luca Maria Aiello:Fabrizio Silvestri:Symeon Papadopoulos	NA	Session details: 2014 social news on the web workshop	NA:NA:NA:NA	2014
Christina Boididou:Symeon Papadopoulos:Yiannis Kompatsiaris:Steve Schifferes:Nic Newman	Fake or misleading multimedia content and its distribution through social networks such as Twitter constitutes an increasingly important and challenging problem, especially in the context of emergencies and critical situations. In this paper, the aim is to explore the challenges involved in applying a computational verification framework to automatically classify tweets with unreliable media content as fake or real. We created a data corpus of tweets around big events focusing on the ones linking to images (fake or real) of which the reliability could be verified by independent online sources. Extracting content and user features for each tweet, we explored the fake prediction accuracy performance using each set of features separately and in combination. We considered three approaches for evaluating the performance of the classifier, ranging from the use of standard cross-validation, to independent groups of tweets and to cross-event training. The obtained results included a 81% for tweet features and 75% for user ones in the case of cross-validation. When using different events for training and testing, the accuracy is much lower (up to %58) demonstrating that the generalization of the predictor is a very challenging issue.	Challenges of computational verification in social multimedia	NA:NA:NA:NA:NA	2014
Eva Jaho:Efstratios Tzoannos:Aris Papadopoulos:Nikos Sarris	There are both positive and negative aspects in the use of social media in news and information dissemination. To deal with the negative aspects, such as the spread of rumours and fake news, the flow of information should implicitly be filtered and marked to specific criteria such as credibility, trustworthiness, reputation, popularity, influence, and authenticity. This paper proposes an approach that can enhance trustworthiness and content validity in the presence of information overload. We introduce Alethiometer, a framework for assessing truthfulness in social media that can be used by professional and general news users alike. We present different measures that delve into the detailed analysis of the content, the contributors of the content and the underlying context. We further propose an approach for deriving a single metric that considers, in a unified manner, the quality of a contributor and of the content provided by that contributor. Finally, we present some preliminary statistical results from the examination of a set of 10 million twitter users, that provide useful insights on the characteristics of social media data.	Alethiometer: a framework for assessing trustworthiness and content validity in social media	NA:NA:NA:NA	2014
Minkyoung Kim:David Newth:Peter Christen	Information spreads across social media, bringing heterogeneous social networks interconnected and diffusion patterns varied in different topics of information. Studying such cross-population diffusion in various context helps us understand trends of information diffusion in a more accurate and consistent way. In this study, we focus on real-world news diffusion across online social systems such as mainstream news (News), social networking sites (SNS), and blogs (Blog), and we analyze behavioral patterns of the systems in terms of activity, reactivity, and heterogeneity. We found that News is the most active, SNS is the most reactive, and Blog is the most persistent, which governs time-evolving heterogeneity of these systems. Finally, we interpret the discovered crowd phenomena from various angles using our previous model-free and model-driven approaches, showing that the strength and directionality of influence reflect the behavioral patterns of the systems in news diffusion.	Trends of news diffusion in social media based on crowd phenomena	NA:NA:NA	2014
José Luis Redondoio Garcia:Laurens De Vocht:Raphael Troncy:Erik Mannens:Rik Van de Walle	Describing multimedia content in general and TV programs in particular is a hard problem. Relying on subtitles to extract named entities that can be used to index fragments of a program is a common method. However, this approach is limited to what is being said in a program and written in a subtitle, therefore lacking a broader context. Furthermore, this type of index is restricted to a flat list of entities. In this paper, we combine the power of non-structured documents with structured data coming from DBpedia to generate a much richer, context aware metadata of a TV program. We demonstrate that we can harvest a rich context by expanding an initial set of named entities detected in a TV fragment. We evaluate our approach on a TV news show.	Describing and contextualizing events in TV news show	NA:NA:NA:NA:NA	2014
Jochen Spangenberg:Nicolaus Heise	Information content provided by members of the general public (mostly online) is playing an ever-increasing role in the detection, production and distribution of news. This paper investigates the concepts of (1) grassroots journalism and (2) collaborative journalism. It looks into similarities and differences and shows how both phenomena have been influenced by the emergence of the Internet and digital technologies. Then, the consequences for journalism in general will be analysed. Ultimately, strategies to meet the new challenges are suggested in order to maintain the quality and reliability of news coverage in the future.	News from the crowd: grassroots and collaborative journalism in the digital age	NA:NA	2014
Jian Wang:Ido Guy:Li Chen	NA	Session details: Social recommender systems (SRS2014) workshop	NA:NA:NA	2014
Bin Yin:Yujiu Yang:Wenhuang Liu	Community-based recommender systems have attracted much research attention. Forming communities allows us to reduce data sparsity and focus on discovering the latent characteristics of communities instead of individuals. Previous work focused on how to detect the community using various algorithms. However, they failed to consider users' social attributes, such as social activeness and dynamic interest, which have strong correlations to users' preference and choice. Intuitively, people have different social activeness in a social network. Ratings from users with high activeness are more likely to be trustworthy. Temporal dynamic of interest is also significant to user's preference. In this paper, we propose a novel community-based framework. We first employ PLSA-based model incorporating social activeness and dynamic interest to discover communities. Then the state-of-the-art matrix factorization method is applied on each of the communities. The experiment results on two real world datasets validate the effectiveness of our method for improving recommendation performance.	Exploring social activeness and dynamic interest in community-based recommender system	NA:NA:NA	2014
Skanda Raj Vasudevan:Sutanu Chakraborti	Critiquing based recommenders are very commonly used to help users navigate through the product space to find the required product by tweaking/critiquing one or more features. By critiquing a product, the user gives an informative feedback(i.e, which feature needs to be modified) about why they rejected a product and preferred the other one. As a user interacts with such a system, trails are left behind. We propose ways of leveraging these trails to induce preference models of items which can be used to estimate the relative utilities of products which can be used in ranking the recommendations presented to the user. The idea is to effectively complement knowledge of explicit user interactions in traditional social recommenders with knowledge implicitly obtained from trails.	Mining user trails in critiquing based recommenders	NA:NA	2014
Feng Xia:Nana Yaw Asabere:Haifeng Liu:Nakema Deonauth:Fengqi Li	Due to the significant proliferation of scholarly papers in both conferences and journals, recommending relevant papers to researchers for academic learning has become a substantial problem. Conferences, in comparison to journals have an aspect of social learning, which allows personal familiarization through various interactions among researchers. In this paper, we improve the social awareness of participants of smart conferences by proposing an innovative folksonomy-based paper recommendation algorithm, namely, Socially-Aware Recommendation of Scholarly Papers (SARSP). Our proposed algorithm recommends scholarly papers, issued by Active Participants (APs), to other Group Profile participants at the same smart conference based on similarity of their research interests. Furthermore, through computation of social ties, SARSP generates effective recommendations of scholarly papers to participants who have strong social ties with an AP. Through a relevant real-world dataset, we evaluate our proposed algorithm. Our experimental results verify that SARSP has encouraging improvements over other existing methods.	Folksonomy based socially-aware recommendation of scholarly papers for conference participants	NA:NA:NA:NA:NA	2014
Kun Tu:Bruno Ribeiro:David Jensen:Don Towsley:Benyuan Liu:Hua Jiang:Xiaodong Wang	Recommendation systems for online dating have recently attracted much attention from the research community. In this paper we propose a two-side matching framework for online dating recommendations and design an Latent Dirichlet Allocation (LDA) model to learn the user preferences from the observed user messaging behavior and user profile features. Experimental results using data from a large online dating website shows that two-sided matching improves the rate of successful matches by as much as 45%. Finally, using simulated matching, we show that the LDA model can correctly capture user preferences.	Online dating recommendations: matching markets and learning preferences	NA:NA:NA:NA:NA:NA:NA	2014
Jong-Ryul Lee:Chin-Wan Chung	For predicting the diffusion process of information, we introduce and analyze a new correlation between the information adoptions of users sharing a friend in online social networks. Based on the correlation, we propose a probabilistic model to estimate the probability of a user's adoption using the naive Bayes classifier. Next, we build a recommendation method using the probabilistic model. Finally, we demonstrate the effectiveness of the proposed method with the data from Flickr and Movielens which are well-known web services. For all cases in the experiments, the proposed method is more accurate than comparison methods.	A new correlation-based information diffusion prediction	NA:NA	2014
Lionel Martin:Valentina Sintsova:Pearl Pu	People increasingly rely on other consumers' opinion to make online purchase decisions. Amazon alone provides access to millions of reviews, risking to cause information overload to an average user. Recent research has thus aimed at understanding and identifying reviews that are considered helpful. Most of such works analyzed the structure and connectivity of social networks to identify influential users. We believe that insight about influence can be gained from analyzing the affective content of the text as well as affect intensity. We employ text mining to extract the emotionality of 68,049 hotel reviews in order to investigate how those influencers behave, especially their choice of words. We analyze whether texts with words and phrases indicative of a writer's emotions, moods, and attitudes are more likely to trigger a genuine interest compared to more neutral texts. Our initial hypothesis was that influential writers are more likely to refrain themselves from expressing their sentiments in order to achieve a more perceived objectivity. But contrary to this initial assumption, our study shows that they use more affective words, both in terms of emotion variety and intensity. This work describes the first step towards building a helpfulness prediction algorithm using emotion lexicons.	Are influential writers more objective?: an analysis of emotionality in review comments	NA:NA:NA	2014
Noor Ifada:Richi Nayak	A common problem with the use of tensor modeling in generating quality recommendations for large datasets is scalability. In this paper, we propose the Tensor-based Recommendation using Probabilistic Ranking method that generates the reconstructed tensor using block-striped parallel matrix multiplication and then probabilistically calculates the preferences of user to rank the recommended items. Empirical analysis on two real-world datasets shows that the proposed method is scalable for large tensor datasets and is able to outperform the benchmarking methods in terms of accuracy.	Tensor-based item recommendation using probabilistic ranking in social tagging systems	NA:NA	2014
Colin Cooper:Sang Hyuk Lee:Tomasz Radzik:Yiannis Siantos	A recommender system uses information about known associations between users and items to compute for a given user an ordered recommendation list of items which this user might be interested in acquiring. We consider ordering rules based on various parameters of random walks on the graph representing associations between users and items. We experimentally compare the quality of recommendations and the required computational resources of two approaches: (i) calculate the exact values of the relevant random walk parameters using matrix algebra; (ii) estimate these values by simulating random walks. In our experiments we include methods proposed by Fouss et al. and Gori and Pucci, method P3, which is based on the distribution of the random walk after three steps, and method P3a, which generalises P3. We show that the simple method P3 can outperform previous methods and method P3a can offer further improvements. We show that the time- and memory-efficiency of direct simulation of random walks allows application of these methods to large datasets. We use in our experiments the three MovieLens datasets.	Random walks in recommender systems: exact computation and simulations	NA:NA:NA:NA	2014
Emanuel Lacic:Dominik Kowald:Denis Parra:Martin Kahr:Christoph Trattner	Recent research has unveiled the importance of online social networks for improving the quality of recommenders in several domains, what has encouraged the research community to investigate ways to better exploit the social information for recommendations. However, there is a lack of work that offers details of frameworks that allow an easy integration of social data with traditional recommendation algorithms in order to yield a straight-forward and scalable implementation of new and existing systems. Furthermore, it is rare to find details of performance evaluations of recommender systems such as hardware and software specifications or benchmarking results of server loading tests. In this paper we intend to bridge this gap by presenting the details of a social recommender engine for online marketplaces built upon the well-known search engine Apache Solr. We describe our architecture and also share implementation details to facilitate the re-use of our approach by people implementing recommender systems. In addition, we evaluate our framework from two perspectives: (a) recommendation algorithms and data sources, and (b) system performance under server stress tests. Using a dataset from the SecondLife virtual world that has both trading and social interactions, we contribute to research in social recommenders by showing how certain social features allow to improve recommendations in online marketplaces. On the platform implementation side, our evaluation results can serve as a baseline to people searching for performance references in terms of scalability, model training and testing trade-offs, real-time server performance and the impact of model updates in a production system.	Towards a scalable social recommender engine for online marketplaces: the case of apache solr	NA:NA:NA:NA:NA	2014
Marc Spaniol:Julien Masanès:Ricardo Baeza-Yates	NA	Session details: Temporal web analytics workshop (TempWeb'14)	NA:NA:NA	2014
Masashi Toyoda	The Web has involved diverse media services, such as blogs, photo/video/link sharing, social networks, and microblogs. These Web media react to and affect realworld events, while the mass media still has big influence on social activities. The Web and mass media now affect each other. Our use of media has evolved dynamically in the last decade, and this affects our societal behavior. For instance, the first photo of a plane crash landing during the "Miracle on the Hudson" on January 15, 2009 appeared and spread on Twitter and was then used in TV news. During the "Chelyabinsk Meteor" incident on February 15, 2013, many people reported videos of the incident on YouTube then mass media reused them on TV programs. Large scale collection, analysis, and visualization of those multiple media are strongly required for sociology, linguistics, risk management, and marketing researches. We are building a huge scale Japanese web archive, and various analytics engines with a large-scale display wall. Our archive consists of 30 billion web pages crawled for 14 years, 1 billion blog posts for 7 years, and 15 billion tweets for 3 years. In this talk, I present several analysis and visualization systems based on network analysis, natural language processing, image processing, and 3 dimensional visualization.	Multiple media analysis and visualization for understanding social activities	NA	2014
Sushma Bannur:Omar Alonso	There is a surge in the use of location activity in social media, in particular to broadcast the change of physical whereabouts. We are interested in analyzing the temporal characteristics of check-ins data from the user's perspective and also at the aggregate level for detecting patterns. In this paper we conduct a large study using check-in data from Facebook to analyze different temporal characteristics in four venue categories (restaurants, movies, shopping, and get-away). We present the results of such study and outline application areas where the conjunction of location and temporal-aware data can help in new search scenarios.	Analyzing temporal characteristics of check-in data	NA:NA	2014
Gaël Harry Dias:Mohammed Hasanuzzaman:Stéphane Ferrari:Yann Mathet	In this paper, we propose to build a temporal ontology, which may contribute to the success of time-related applications. Temporal classifiers are learned from a set of time-sensitive synsets and then applied to the whole WordNet to give rise to TempoWordNet. So, each synset is augmented with its intrinsic temporal value. To evaluate TempoWordNet, we use a semantic vector space representation for sentence temporal classification, which shows that improvements may be achieved with the time-augmented knowledge base against a bag-of-ngrams representation.	TempoWordNet for sentence time tagging	NA:NA:NA:NA	2014
Lars Döhling:Ulf Leser	Finding reliable information about a given event from large and dynamic text collections is a topic of great interest. For instance, rescue teams and insurance companies are interested in concise facts about damages after disasters, which can be found in web blogs, newspaper articles, social networks etc. However, finding, extracting, and condensing specific facts is a highly complex undertaking: It requires identifying appropriate textual sources, recognizing relevant facts within the sources, and aggregating extracted facts into a condensed answer despite inconsistencies, uncertainty, and changes over time. In this paper, we present a three-step framework providing techniques and solutions for each of these problems. We tested the feasibility of extracting time-associated event facts using our framework in a comprehensive case study: gathering data on particular earthquakes from web data sources. Our results show that it is, under certain circumstances, possible to automatically obtain reliable and timely data on natural disasters from the web.	Extracting and aggregating temporal events from text	NA:NA	2014
Hideo Joho:Adam Jatowt:Roi Blanco	Time is one of the key constructs of information quality. Following an upsurge of research in temporal aspects of information search, it has become clear that the community needs standardized evaluation benchmark for fostering research in Temporal Information Access. This paper introduces Temporalia (Temporal Information Access), a new pilot task run at NTCIR-11 to create re-usable datasets for those who are interested in temporal aspects of search technologies, and discusses its task design in detail.	NTCIR temporalia: a test collection for temporal information access research	NA:NA:NA	2014
Jimmy Lin:Milad Gholami:Jinfeng Rao	Web archiving initiatives around the world capture ephemeral web content to preserve our collective digital memory. However, unlocking the potential of web archives requires tools that support exploration and discovery of captured content. These tools need to be scalable and responsive, and to this end we believe that modern "big data" infrastructure can provide a solid foundation. We present Warcbase, an open-source platform for managing web archives built on the distributed datastore HBase. Our system provides a flexible data model for storing and managing raw content as well as metadata and extracted knowledge. Tight integration with Hadoop provides powerful tools for analytics and data processing. Relying on HBase for storage infrastructure simplifies the development of scalable and responsive applications. We describe a service that provides temporal browsing and an interactive visualization based on topic models that allows users to explore archived content.	Infrastructure for supporting exploration and discovery in web archives	NA:NA:NA	2014
Stewart Whiting:Joemon Jose:Omar Alonso	Wikipedia encyclopaedia projects, which consist of vast collections of user-edited articles covering a wide range of topics, are among some of the most popular websites on internet. With so many users working collaboratively, mainstream events are often very quickly reflected by both authors editing content and users reading articles. With temporal signals such as changing article content, page viewing activity and the link graph readily available, Wikipedia has gained attention in recent years as a source of temporal event information. This paper serves as an overview of the characteristics and past work which support Wikipedia (English, in this case) for time-aware information retrieval research. Furthermore, we discuss the main content and meta-data temporal signals available along with illustrative analysis. We briefly discuss the source and nature of each signal, and any issues that may complicate extraction and use. To encourage further temporal research based on Wikipedia, we have released all the distilled datasets referred to in this paper.	Wikipedia as a time machine	NA:NA:NA	2014
Marc Spaniol:Julien Masanès:Ricardo Baeza-Yates	In this paper we give an overview on the 4th Temporal Web Analytics Workshop (TempWeb). The goal of TempWeb is to provide a venue for researchers of all domains (IE/IR, Web mining, etc.) where the temporal dimension opens up an entirely new range of challenges and possibilities. The workshop's ambition is to help shaping a community of interest on the research challenges and possibilities resulting from the introduction of the time dimension in web analysis. Having a dedicated workshop will help, we believe, to take a rich and cross-domain approach to this new research challenge with a strong focus on the temporal dimension. For the fourth time, TempWeb has been organized in conjunction with the International World Wide Web (WWW) conference, being held on April 8, 2014 in Seoul, Korea.	The 4th temporal web analytics workshop (TempWeb'14)	NA:NA:NA	2014
Nigel Shadbolt:Jim Hendler:Noshir Contractor:Elena Simperl	NA	Session details: Theory and practice of social machines 2014 workshop	NA:NA:NA:NA	2014
Vanilson Buregio:Leandro Nascimento:Nelson Rosa:Silvio Meira	In this paper, we extend the initial classification scheme for Social Machines (SM) by including Personal APIs as a new SM-related topic of research inquiry. Personal APIs basically refer to the use of Open Application Programming Interfaces Open APIs) to programmatically access information about a person (e.g., personal basic info, health-related statistics, busy data) and/or trigger his/her human capabilities in a standardized way. Here, we provide an overview of some existing Personal APIs and show how this approach can be used to enable the design and implementation of people as individual SMs on the Web. A proof-of-concept system that demonstrates these ideas is also outlined in this paper.	Personal APIs as an enabler for designing and implementing people as social machines	NA:NA:NA:NA	2014
Leandro Marques do Nascimento:Vanilson A.A. Burégio:Vinicius C. Garcia:Silvio R.L. Meira	The term "Social Machine" (SM) has been commonly used as a synonym for what is known as the programmable web or web 3.0. Some definitions of a Social Machine have already been provided and they basically support the notion of relationships between distributed entities. The type of relationship molds which services would be provided or required by each machine, and under certain complex constraints. In order to deal with the complexity of this emerging web, we present a language that can describe networks of Social Machines, named SMADL -- the Social Machine Architecture Description Language. In few words, SMADL is as a relationship-driven language which can be used to describe the interactions between any number of machines in a multitude of ways, as a means to represent real machines interacting in the real web, such as, Twitter running on top of Amazon AWS or mash-ups built upon Google Maps, and obviously, as a means to represent interactions with other social machines too.	A new architecture description language for social machines	NA:NA:NA:NA	2014
Dave Murray-Rust:Dave Robertson	We present LSCitter, an implemented framework for supporting human interaction on social networks with formal models of interaction, designed as a generic tool for creating social machines on existing infrastructure. Interaction models can be used to choreograph distributed systems, providing points of coordination and communication between multiple interacting actors. While existing social networks specify how interactions happen---who messages go to and when, the effects of carrying out actions---these are typically implicit, opaque and non user-editable. Treating interaction models as first class objects allows the creation of electronic institutions, on which users can then choose the kinds of interaction they wish to engage in, with protocols which are explicit, visible and modifiable. However, there is typically a cost to users to engage with these institutions. In this paper we introduce the notion of "shadow institutions", where actions on existing social networks are mapped onto formal interaction protocols, allowing participants access to computational intelligence in a seamless, zero-cost manner to carry out computation and store information.	LSCitter: building social machines by augmenting existing social networks with interaction models	NA:NA	2014
Reuben Binns:David Matthews	This paper presents a case study of 'Terms-of-Service; Didn't Read', a social machine to curate, parse, and rate website terms and privacy policies. We examine the relationships between its human contributors and machine counterparts to determine community structure and information flow.	Community structure for efficient information flow in 'ToS;DR', a social machine for parsing legalese.	NA:NA	2014
Harry Halpin:Andrea Capocci	An intriguing hypothesis, first suggested by Tim Berners-Lee, is that the structure of online groups should conform to a power law distribution. We believe this is likely a consequence of the Dunbar Number, which is a supposed limit to the number of persistent social contacts a user can have in a group. As preliminary results, we show that the number of contacts of a typical Flickr user, the number of groups a user belongs to, and the size of Flickr groups all follow power law distributions. Furthermore, we find some unexpected differences in the internal structure of public and private Flickr groups. For further research, we further operationalize the Berners-Lee hypothesis to suppose that users with a group membership distribu	The Berners-Lee hypothesis: power laws and group structure in flickr	NA:NA	2014
Marco Brambilla:Stefano Ceri:Andrea Mauri:Riccardo Volonterio	This paper is focused on community-based crowdsourcing applications, i.e. the ability of spawning crowdsourcing tasks upon multiple communities of performers, thus leveraging the peculiar characteristics and capabilities of the community members. We show that dynamic adaptation of crowdsourcing campaigns to community behaviour is particularly relevant. We demonstrate that this approach can be very effective for obtaining answers from communities, with very different size, precision, delay and cost, by exploiting the social networking relations and the features of the crowdsourcing task. We show the approach at work within the CrowdSearcher platform, which allows configuring and dynamically adapting crowdsourcing campaigns tailored to different communities. We report on an experiment demonstrating the effectiveness of the approach.	Community-based crowdsourcing	NA:NA:NA:NA	2014
Amy Guy:Ewan Klein	Current discussions of social machines rightly emphasise a human's role as a crucial part of a system rather than a user of a system. The human `parts' are typically considered in terms of their aggregate outcomes and collective behaviours, but human participants are rarely all equal, even within a small system. We argue that due to the complex nature of online identity, understanding participants in a more granular way is crucial for social machine observation and design. We present the results of a study of the personas portrayed by participants in a social machine that produces creative media content, and discover that inconsistent or misleading representations of individuals do not necessarily undermine the system in which they are participating. We describe a preliminary framework for making sense of human participants in social machines, and the ongoing work that develops this further.	Constructed identity and social machines: a case study in creative media production	NA:NA	2014
Thanassis Tiropanis:Anni Rowland-Campbell:Wendy Hall	The Web is becoming increasingly pervasive throughout all aspects of human activity. As citizens and organisations adopt Web technologies, so governments are beginning to respond by themselves utilising the electronic space. Much of this has been reactive, and there is very little understanding of the impact that Web technologies are having on government systems and processes, let alone a proactive approach to designing systems that can ensure a positive and beneficial societal impact. The ecosystem which encompasses governments, citizens and communities is both evolving and adaptive, and the only way to examine and understand the development of Web-enabled government, and its possible implications, is to consider government itself as a "social machine" within a social machine ecosystem. In this light, there are significant opportunities and challenges for government that this paper identifies.	Government as a social machine in an ecosystem	NA:NA:NA	2014
Lei Zhang:Thanassis Tiropanis:Wendy Hall:Sung-Hyon Myaeng	In this paper, we propose the Ω-machine model for social machines. By introducing a cluster of "oracles" to a traditional Turing machine, the Ω-machine is capable of describing the interaction between human participants and mechanical machines. We also give two examples of social machines, collective intelligence and rumor spreading, and demonstrate how the general Ω-machine model could be used to simulate their computations.	Introducing the omega-machine	NA:NA:NA:NA	2014
Ségolène M. Tarte:David De Roure:Pip Willcox	Although Social Machines do not have yet a formalized definition, some efforts have been made to characterize them from a ``machinery'' point of view. In this paper, we present a methodology by which we attempt to reveal the sociality of Social Machines; to do so, we adopt the analogy of stories. By assimilating a Social Machine to a story, we can identify the stories within and about that machine and how this storytelling perspective might reveal the sociality of Social Machines. After illustrating this storytelling approach with a few examples, we then propose three axes of inquiry to evaluate the health of a social machine: (1) assessment of the sociality of a Social Machine through evaluation of its storytelling potential and realization; (2) assessment of the sustainability of a Social Machine through evaluation of its reactivity and interactivity; and (3) assessment of emergence through evaluation of the collaboration between authors and of the distributed/mixed nature of authority.	Working out the plot: the role of stories in social machines	NA:NA:NA	2014
Max Van Kleek:Daniel Alexander Smith:Ramine Tinati:Kieron O'Hara:Wendy Hall:Nigel R. Shadbolt	Web Observatories aim to develop techniques and methods to allow researchers to interrogate and answer questions about society through the multitudes of digital traces people now create. In this paper, we propose that a possible path towards surmounting the inevitable obstacle of personal privacy towards such a goal, is to keep data with individuals, under their own control, while enabling them to participate in Web Observatory-style analyses in situ. We discuss the kinds of applications such a global, distributed, linked network of Personal Web Observatories might have, a few of the many challenges that must be resolved towards realising such an architecture in practice, and finally, our work towards a fundamental reference building block of such a network.	7 billion home telescopes: observing social machines through personal data stores	NA:NA:NA:NA:NA:NA	2014
Hongbo Deng:Jiang Bian:Yi Chang:Neel Sundaresan	NA	Session details: Vertical search relevance 2014 workshop	NA:NA:NA:NA	2014
Pattisapu Nikhil Priyatam:Ajay Dubey:Krish Perumal:Sai Praneeth:Dharmesh Kakadia:Vasudeva Varma	The last two decades have witnessed an exponential rise in web content from a plethora of domains, which has necessitated the use of domain-specific search engines. Diversity of crawled content is one of the crucial aspects of a domain-specific search engine. To a large extent, diversity is governed by the initial set of seed URLs. Most of the existing approaches rely on manual effort for seed selection. In this work we automate this process using URLs posted on Twitter. We propose an algorithm to get a set of diverse seed URLs from a Twitter URL graph. We compare the performance of our approach against the baseline zero similarity seed selection method and find that our approach beats the baseline by a significant margin.	Seed selection for domain-specific search	NA:NA:NA:NA:NA:NA	2014
Ruben Verborgh:Thomas Steiner:Carlos Pedrinaci	NA	Session details: Web APIs and RESTful design 2014 workshop	NA:NA:NA	2014
Pete Gamache	Hypermedia API design is a method of creating APIs using hyperlinks to represent and publish an API's functionality. Hypermedia-based APIs bring theoretical advantages over many other designs, including the possibility of self-updating, generic API client software. Such hypermedia API clients only lately have come to exist, and the existing hypermedia client space did not compare favorably to custom API client libraries, requiring somewhat tedious manual access to HTTP resources. Nonetheless, the limitations in creating a compelling hypermedia client were few. This paper describes the design and implementation of HyperResource, a fully generic, production-ready Ruby client library for hypermedia APIs. The project leverages the inherent practicality of hypermedia design, demonstrates its immediate usefulness in creating self-generating API clients, enumerates several abstractions and strategies that help in creating hypermedia APIs and clients, and promotes hypermedia API design as the easiest option available to an API programmer.	Pragmatic hypermedia: creating a generic, self-inflating API client for production use	NA	2014
Hyunghun Cho:Sukyoung Ryu	In today's Web-centric era, embedded systems become mashup various web services via RESTful web services. RESTful web services use REST APIs that describe actions as resource state transfers via standard HTTP methods such as GET, PUT, POST, and DELETE. While RESTful web services are lightweight and executable on any platforms that support HTTP methods, writing programs composed of only such primitive methods is not a familiar concept to developers. Therefore, no single design strategy for (fully) RESTful APIs works for arbitrary domains, and current REST APIs are system dependent, incomplete, and likely to change. To help sever-side development of REST APIs, several domain-specific languages such as WADL, WSDL 2.0, and RDF provide automatic tools to generate REST APIs. However, client-side developers who often do not know the web services domain and do not understand RESTful web services suffer from the lack of any development help. In this paper, we present a new approach to build JavaScript APIs that are more accessible to client-side developers than REST APIs. We show a case study of our approach that uses JavaScript APIs and their wrapper implementation instead of REST APIs, and we describe the efficiency in the client-side development.	REST to JavaScript for better client-side development	NA:NA	2014
Guy Pardon:Cesare Pautasso	The REST architectural style supports the reliable interaction of clients with a single server. However, no guarantees can be made for more complex interactions which require to atomically transfer state among resources distributed across multiple servers. In this paper we describe a lightweight design for transactional composition of RESTful services. The approach -- based on the Try-Cancel/Confirm (TCC) pattern -- does not require any extension to the HTTP protocol. The design assumes that resources are designed to comply with the TCC pattern and ensures that the resources involved in the transaction are not aware of it. It delegates the responsibility of achieving the atomicity of the transaction to a coordinator which exposes a RESTful API.	Atomic distributed transactions: a RESTful design	NA:NA	2014
Nandana Mihindukulasooriya:Miguel Esteban-Gutiérrez:Raúl García-Castro	The REpresentational State Transfer (REST) architectural style describes the design principles that made the World Wide Web scalable and the same principles can be applied in enterprise context to do loosely coupled and scalable application integration. In recent years, RESTful services are gaining traction in the industry and are commonly used as a simpler alternative to SOAP Web Services. However, one of the main drawbacks of RESTful services is the lack of standard mechanisms to support advanced quality-of-service requirements that are common to enterprises. Transaction processing is one of the essential features of enterprise information systems and several transaction models have been proposed in the past years to fulfill the gap of transaction processing in RESTful services. The goal of this paper is to analyze the state-of-the-art RESTful transaction models and identify the current challenges.	Seven challenges for RESTful transaction models	NA:NA:NA	2014
Miel Vander Sande:Pieter Colpaert:Tom De Nies:Erik Mannens:Rik Van de Walle	Many organisations publish their data through a Web API. This stimulates use by Web applications, enabling reuse and enrichments. Recently, resource-oriented APIs are increasing in popularity because of their scalability. However, for organisations subject to data archiving, creating such an APIraises certain issues. Often, datasets are stored in different files and different formats. Therefore, tracking revisions is a challenging task and the API has to be custom built. Moreover, standard APIs only provide access to the current state of a resource. This creates time-based inconsistencies when they are combined. In this paper, we introduce an end-to-end solution for publishing a dataset as a time-based versioned REST API, with minimal input of the publisher. Furthermore, it publishes the provenance of each created resource. We propose a technology stack composed of prior work, which versions datasets, generates provenance, creates an API and adds Memento Datetime negotiation.	Publish data as time consistent web API with provenance	NA:NA:NA:NA:NA	2014
Harry Halpin	The W3C Web Cryptography API is the standard API for accessing cryptographic primitives in Javascript-based environments. We describe the motivations behind the creation of the W3C Web Cryptography API and give a high-level overview with motivating use-cases while addressing objections.	The W3C web cryptography API: motivation and overview	NA	2014
Masiar Babazadeh:Cesare Pautasso	Streaming applications have become more and more dynamic and heterogeneous thanks to new technologies which enable platforms like microcontrollers and Web browsers to be able to host part of a streaming topology. A dynamic heterogeneous streaming application should support load balancing and fault tolerance while being capable of adapting and rearranging topologies to user needs at runtime. In this paper we present a REST API to control dynamic heterogeneous streaming applications. By means of resources, their uniform interface and hypermedia we show how it is possible to monitor, change and adapt the deployment configuration of a streaming topology at runtime.	A RESTful API for controlling dynamic streaming topologies	NA:NA	2014
Juan Luis Pérez:Álvaro Villalba:David Carrera:Iker Larizgoitia:Vlad Trifa	The COMPOSE project aims to provide an open Marketplace for the Internet of Things as well as the necessary platform to support it. A necessary component of COMPOSE is an API that allows things, COMPOSE users and the platform to communicate. The COMPOSE API allows for things to push data to the platform, the platform to initiate asynchronous actions on the things, and COMPOSE users to retrieve and process data from the things. In this paper we present the design and implementation of the COMPOSE API, as well as a detailed description of the main key requirements that the API must satisfy. The API documentation and the source code for the platform are available online.	The COMPOSE API for the internet of things	NA:NA:NA:NA:NA	2014
Mahdi Bennara:Michaël Mrissa:Youssef Amghar	In this paper, we present an approach to compose linked services on the Web based on the principles of linked data and REST. Our contribution is a unified method for discovering both the interaction possibilities a service offers and the available semantic links to other services. Our composition engine is implemented as a generic client that allows exploring a service API and interacting with other services to answer user's goal. We rely on a typical scenario in order to illustrate the benefits of our composition approach. We implemented a prototype to demonstrate the applicability of our proposal, experiment and discuss the results obtained.	An approach for composing RESTful linked services on the web	NA:NA:NA	2014
Rajendra Akerkar:Pierre Maret:Laurent Vercouter	NA	Session details: Web intelligence and communities workshop (WI&C 2014)	NA:NA:NA	2014
Rajendra Akerkar:Pierre Maret:Laurent Vercouter	Web Intelligence is a multidisciplinary area dealing with utilizing data and services over the Web, to create new data and services using Information and Communication Technologies (ICT) and Intelligent techniques. The link to Networking and Web Communities (WCs) is apparent: the Web is a set of nodes, providing and consuming data and services; the permanent or temporary ties and exchanges in-between these nodes build the virtual communities; and the ICT and intelligent techniques influence the modeling and the processes, and it automates (or semi-automate) communication and cooperation. In this paper, we will explore one aspect of (Web) intelligence pertinent to the Web Communities. The "intelligent" features may emerge in a Web community from interactions and knowledge-transmissions between the community members. We will also introduce the WI&C'14 workshop's goal and structure.	Exploring intelligence of web communities	NA:NA:NA	2014
Sören Auer:Dimitris Kontokostas	A key success factor for the Web as a whole was and is its participatory nature. We discuss strategies for engaging human-intelligence to make the Web more semantic.	Towards web intelligence through the crowdsourcing of semantics	NA:NA	2014
Pablo Loyola:In-Young Ko	Open Source Software (OSS) has gained high amount of popularity during the last few years. It is becoming used by public and private institutions, even companies release portions of their code to obtain feedback from the community of voluntary developers. As OSS is based on the voluntary contributions of developers, the number of participants represents one of the key elements that impact the quality of the software. In order to understand how the the population of contributors evolve over time, we propose a methodology that adapts Lotka-Volterra-based biological models used for describing host-parasite interactions. Experiments based on data from the Github collaborative platform showed that the proposed approach performs effectively in terms of providing an estimation of the population of developers for each project over time.	Population dynamics in open source communities: an ecological approach applied to github	NA:NA	2014
Yasser Salem:Jun Hong:Weiru Liu	Product recommendation is an important aspect of many e-commerce systems. It provides an effective way to help users navigate complex product spaces. In this paper, we focus on critiquing-based recommenders. We present a new critiquing-based approach, History-Guided Recommendation (HGR), which is capable of using the recommendation pairs (item and critique) or critiques only so far in the current recommendation session to predict the most likely product recommendations and therefore short-cut the sometimes protracted recommendation sessions in standard critiquing approaches. The HGR approach shows a significant improvement in the interactions between the user and the recommender. It also enables successfully accepted recommendations to be made much earlier in the session.	History-guided conversational recommendation	NA:NA:NA	2014
Nikos Karacapilidis:Spyros Christodoulou:Manolis Tzagarakis:Georgia Tsiliki:Costas Pappis	Generally speaking, modern research becomes increasingly interdisciplinary and collaborative in nature. Researchers need to collaborate and make decisions by meaningfully assembling, mining and analyzing available large-scale volumes of complex multi-faceted data residing in different sources. At the same time, they need to efficiently and effectively exploit services available over the Web. Arguing that dealing with data-intensive and cognitively complex settings is not a technical problem alone, this paper presents a novel collaboration support platform for Web communities. The proposed solution adopts a hybrid approach that builds on the synergy between machine and human intelligence to facilitate the underlying sense-making and decision making processes. User experience shows that the platform enables stakeholders to make better, more informed and quicker decisions. The functionalities of the proposed platform are described through a real-world case from a biomedical research community.	Strengthening collaborative data analysis and decision making in web communities	NA:NA:NA:NA:NA	2014
Paolo Pareti:Ewan Klein:Adam Barker	This paper proposes a novel framework for representing community 'know-how' on the Semantic Web. Procedural knowledge generated by web communities typically takes the form of natural language instructions or videos and is largely unstructured. The absence of semantic structure impedes the deployment of many useful applications, in particular the ability to discover and integrate know-how automatically. We discuss the characteristics of community know-how and argue that existing knowledge representation frameworks fail to represent it adequately. We present a novel framework for representing the semantic structure of community know-how and demonstrate the feasibility of our approach by providing a concrete implementation which includes a method for automatically acquiring procedural knowledge for real-world tasks.	A semantic web of know-how: linked data for community-centric tasks	NA:NA:NA	2014
Somayeh Koohborfardhaghighi:Jörn Altmann	People-to-people interactions in the real world and in virtual environments (e.g., Facebook) can be represented through complex networks. Changes of the structural properties of these complex networks are caused through a variety of dynamic processes. While accepting the fact that variability in individual patterns of behavior (i.e., establishment of random or FOAF-type potential links) in social environments might lead to an increase or decrease in the structural properties of a complex network, in this paper, we focus on another factor that may contribute to such changes, namely the size of personal networks. Any personal network comes with the cost of maintaining individual connections. Despite the fact that technology has shrunk our world, there is also a limit to how many close friends one can keep and count on. It is a relatively small number. In this paper, we develop a multi-agent based model to capture, compare, and explain the structural changes within a growing social network (e.g., expanding the social relations beyond one's social circles). We aim to show that, in addition to various dynamic processes of human interactions, limitations on the size of personal networks can also lead to changes in the structural properties of networks (i.e., the average shortest-path length). Our simulation result shows that the famous small world theory of interconnectivity holds true or even can be shrunk, if people manage to utilize all their existing connections to reach other parties. In addition to this, it can clearly be observed that the network's average path length has a significantly smaller value, if the size of personal networks is set to larger values in our network growth model. Therefore, limitations on the size of personal networks in network growth models lead to an increase in the network's average path length.	How placing limitations on the size of personal networks changes the structural properties of complex networks	NA:NA	2014
Kevin Page:David De Roure:Wolfgang Nejdl	NA	Session details: Web observatory workshop (WOW2014)	NA:NA:NA	2014
Huanbo Luan:Juanzi Li:Maosong Sun:Tat-Seng Chua	With the emergence of social networks and their potential impact on society, many research groups and originations are collecting huge amount of social media data from various sites to serve different applications. These systems offer insights on different facets of society at different moments of time. Collectively they are known as social observatory systems. This paper describes the architecture and implementation of a live social observatory system named 'NExT-Live'. It aims to analyze the live online social media data streams to mine social senses, phenomena, influences and geographical trends dynamically. It incorporates an efficient and robust set of crawlers to continually crawl online social interactions on various social network sites. The data crawled are stored and processed in a distributed Hadoop architecture. It then performs the analysis on these social media streams jointly to generate analytics at different levels. In particular, it generates high-level analytics about the sense of different target entitles, including People, Locations, Topics and Organizations. NExT-Live offers a live observatory platform that enables people to know the happenings of the place in order to lead better life.	The design of a live social observatory system	NA:NA:NA:NA	2014
Matthew S. Weber	This paper discusses the challenges and opportunities for using archival Internet data in order to observe a host of social science phenomena. Specifically, this paper introduces HistoryTracker, a new tool for accessing and extracting archived data from the Internet Archive, the largest repository of archived Web data in existence. The HistoryTracker tool serves to create a Web observatory that allows scholars to study the history of the Web. HistoryTracker takes advantages of Hadoop processing capacity, and allows researchers to extract large swaths of archived data into a link list format that can be easily transferred to a number of other analytical tools. A brief illustration of the use of HistoryTracker is presented demonstrating the use of the tool. Finally, a number of continuing research challenges are discussed, and future research opportunities are outlined.	Observing the web by understanding the past: archival internet research	NA	2014
Mizuki Oka:Yasuhiro Hashimoto:Takashi Ikegami	A salient dynamic property of social media is bursting be- havior. In this paper, we study bursting behavior in relation to the structure of fluctuation, known as fluctuation-response relation, to reveal the origin of bursts. More specifically, we study the temporal relation between a preceding baseline fluctuation and the successive burst response using a frequency time series of 3,000 keywords on Twitter. We find three types of keyword time series in terms of the fluctuation-response relation. For the first type of keyword, the baseline fluctuation has a positive correlation with the burst size; as the preceding fluctuation increases, the burst size increases. These bursts are caused endogenously as a result of word-of-mouth interactions in a social network; the keyword is sensitive only to the internal context of the system. For the second type, there is a critical threshold in the fluctuation value up to which a positive correlation is observed. Beyond this value, the size of the bursts becomes independent from the fluctuation size. Our analysis shows that this critical threshold emerges because the bursts in the time series are endogenous and exogenous. This type of keyword is sensitive to internal and external stimuli. The third type is mainly bursts caused by exogenous bursts. This type of keyword is mostly sensitive only to external stimuli. These results are useful for characterizing how excitable a keyword is on Twitter and could be used, for example, for marketing purposes.	Fluctuation and burst response in social media	NA:NA:NA	2014
Gareth Paul Beeston:Manuel Leon Urrutia:Caroline Halcrow:Xianni Xiao:Lu Liu:Jinchuan Wang:Jinho Jay Kim:Kunwoo Park	This paper presents an analysis of humour use in Sina Weibo in reaction to the Chinese salt panic, which occurred as a result of the Fukushima disaster in March 2011. Basing the investigation on the humour Proximal Distancing Theory (PDT), and utilising a dataset from Sina Weibo in 2011, an examination of humour reactions is performed to identify the proximal spread of humourous Weibo posts in relation to the consequent salt panic in China. As a result of this method, we present a novel methodology for understanding humour reactions in social media, and provide recommendations on how such a method could be applied to a variety of other social media, crises, cultural and spatial settings.	Humour reactions in crisis: a proximal analysis of Chinese posts on sina weibo in reaction to the salt panic of march 2011	NA:NA:NA:NA:NA:NA:NA:NA	2014
Robert Simpson:Kevin R. Page:David De Roure	This paper introduces the Zooniverse citizen science project and software framework, outlining its structure from an observatory perspective: both as an observable web-based system in itself, and as an example of a platform iteratively developed according to real-world deployment and used at scale. We include details of the technical architecture of Zooniverse, including the mechanisms for data gathering across the Zooniverse operation, access, and analysis. We consider the lessons that can be drawn from the experience of designing and running Zooniverse, and how this might inform development of other web observatories.	Zooniverse: observing the world's largest citizen science platform	NA:NA:NA	2014
Paul Booth:Wendy Hall:Nicholas Gibbins:Spyros Galanis	Web Observatories use innovative analytic processes to gather insights from observed data and use the Web as a platform for publishing interactive data visualisations. Recordable events associated with interactivity on the Web provide an opportunity to openly evaluate the utility of these artefacts, assessing fitness for purpose and observing their use. The three principles presented in this paper propose a community evaluation approach to innovation in visual analytics and visualisation for Web Observatories through code sharing, the capturing of semantically enriched interaction data and by openly stating the intended goals of all visualisation work. The potential of this approach is exampled with a set of front-end tools suitable for adoption by the majority of Web Observatories as a means of visualising data on the Web as part the shared, open, and community-driven developmental process. The paper outlines the method for capturing user interaction data as a series of semantic events, which can be used to identify improvements in both the structure and functionality of visualisations. Such refinements in user behaviour are proposed as part of a new methodology that introduces Economics as an evaluation tool for visual analytics.	Visualising data in web observatories: a proposal for visual analytics development & evaluation	NA:NA:NA:NA	2014
Marie Joan Kristine T. Gloria:John S. Erickson:Joanne S. Luciano:Dominic DiFranzo:Deborah L. McGuinness	This paper explores the impact of health information technologies, including the Web, on society and advocates for the development of a Health Web Observatory (HWO) to collect, store and analyze new sources of health information. The paper begins with a high-level literature review from across domains to demonstrate the need for a multi-disciplinary pursuit when building web observatories. For as researchers in the social sciences and legal domains have highlighted, data carries assumptions of power, identity, governance, etc., which should not be overlooked. The paper then recommends example legal and ethical questions to consider when building any health web observatory. The goal is to insert social and regulatory concerns much earlier into the WO methodology	Legal and ethical considerations: step 1b in building a health web observatory	NA:NA:NA:NA:NA	2014
Ian C. Brown:Wendy Hall:Lisa Harris	In this paper, we propose an initial structure to support a taxonomy for Web Observatories (WO). The work is based on a small sample of cases drawn from the work of the Web Science Trust and the Web Science Institute and reflects aspects of academic, business and government Observatories. Whilst this is early work it is hoped, by drawing broad brushstrokes at the edges of different types of Observatory, that future work based on a more systematic review will refine this model and hence refine our understanding of the nature of Observatories. We also seek here to enhance a faceted classification scheme (which is thought to be weak in the area of visualisation) through the use of simplified concept maps.	Towards a taxonomy for web observatories	NA:NA:NA	2014
Irwin King:Bebo White	NA	Session details: Web-based education technologies workshop (WebET 2014)	NA:NA	2014
Peter Brusilovsky	Empirical studies of adaptive annotation in the educational context have demonstrated that it can help students to acquire knowledge faster, improve learning outcomes, reduce navigational overhead, and encourage non-sequential navigation. Over the last 8 years we have explored a lesser known effect of adaptive annotation -- its ability to significantly increase student engagement in working with non-mandatory educational content. In the presence of adaptive link annotation, students tend to access significantly more learning content; they stay with it longer, return to it more often and explore a wider variety of learning resources. This talk will present an overview of our exploration of the addictive links effect in many course-long studies, which we ran in several domains (C, SQL and Java programming), for several types of learning content (quizzes, problems, interactive examples). The first part of the talk will review our exploration of a more traditional knowledge-based personalization approach and the second part will focus on more recent studies of social navigation and open social student modeling.	Addictive links: engaging students through adaptive navigation support and open social student modeling	NA	2014
Ilona Nawrot:Antoine Doucet	The main objectives of massive open online courses (MOOC) are to foster knowledge through free high quality learning materials procurement; to create new knowledge through diverse users' interactions with the providing platform; and to empower research on learning. However, MOOC providers are also businesses (either profit or not-for-profit). They are still in the early stages of their development, but sooner or later, in order to secure their existence and assure their longterm growth, they will have to adapt a business model and monetize the services they provide. Nevertheless, despite their popularity MOOCs are characterized by a very high drop-out rate (about 90%), which may turn out to be a problem regardless of the adapted business model. Hence, MOOC providers can either assume the scale benefits to be sufficiently high to ignore the problem of low MOOC completion rate or tackle this problem. In this paper we explore the problem of the high drop-out rate in massive open online courses. First, we identify its main cause by conducting an online survey, namely bad time organization. Secondly, we provide suggestions to reduce the rate. Specifically, we argue that MOOC platforms should not only provide their users with high quality educational materials and interaction facilities. But they should also support and assist the users in their quest for knowledge. Thus, MOOC platforms should provide tools helping them optimize their time usage and subsequently develop metacognitive skills indispensable in proper time management of learning processes.	Building engagement for MOOC students: introducing support for time management on online learning platforms	NA:NA	2014
Sanjeev Kumar Saini:Senthil Anand S:Arivudainambi D:Krishnan C N	In this paper, we describe the details of an interactive online web-based degree program in the area of Computer Science with specialization in Free/Open Source Software (FOSS) that has been successfully running for two years in a leading technological university in India. The subjects taught as well as the tools and platforms used in delivering the course are exclusively FOSS and put together by the university team, as described here. We also describe the details of the program, its goals and purpose, the manner of its implementation, the learnings we have had and the challenges being faced in going forward.	A web-based degree program in open source education: a case study	NA:NA:NA:NA	2014
Neel Guha	Many studies have demonstrated the effectiveness of tutoring as a teaching strategy. Though much attention has recently been focussed on using the web to extend the reach of the university classroom to high achieving students, comparatively less attention has been paid to the potential of the web to bring personalized tutoring to at-risk students. In this paper, we describe Tutoring From the Desktop, a program in which high school students in California use Google Plus Hangouts to tutor students in Kohlapur, India. We show how a simple structured program can be used to over- come the barriers of time-zones, accents and much more.	Tutoring from the desktop: facilitating learning through Google+ hangouts	NA	2014
Sergey Butakov	In the era of exponentially growing web and exploding online education the problem of digital plagiarism has become one of the most burning ones in many areas. Efficient internet plagiarism detection tools should have a capacity similar to that of conventional web search engines. This requirement makes commercial plagiarism detection services expensive and therefore less accessible to smaller education institutions. This work-in-progress paper proposes the concept of crowdcrawling as a tool to distribute the most laborious part of the web search among community servers thus providing scalability and sustainability to the community driven plagiarism detection. It outlines roles for community members depending on the resources they are willing to contribute to the service.	Crowdcrawling approach for community based plagiarism detection service	NA	2014
Tak Pang Lau:Shuai Wang:Yuanyuan Man:Chi Fai Yuen:Irwin King	Writing is a vital issue for education as well as a fundamental skill in teaching and learning. With the development of information technologies, more and more professional writing tools emerge. As each of them mostly concentrates on addressing a specific issue, people need a one-stop platform, which could integrate multiply functions. In addition, with the supported concept of e-learning ecosystem for future education, a comprehensive platform will be more promising. Therefore, we introduce VeriGuide Platform, which provides a professional writing toolbox to promote the enhancement of teaching and learning in writing. It contains six vertical components, which could be split into two groups. The first group, Editing Assistance, facilitates students write papers and point out grammar and spelling errors. While the second group, Text Analysis, offers document analysis results, which enables students to achieve further writing improvement with explanatory feedbacks. Furthermore, we could do education data analytics to enhance the efficiency of teaching and learning. Specifically, the Editing Assistance contains well-organized writing and formatting, and grammar and spelling checking, while readability assessment, similarity detection, citation analysis, and sentiment analysis are included in the Text Analysis.	Language technologies for enhancement of teaching and learning in writing	NA:NA:NA:NA:NA	2014
Emanuele Lunadei:Christian Valdivia Torres:Erik Cambria	The way people create and share content has radically changed in the past few years thanks to the advent of social networks, web communities, blogs, wikis, and other online collaborative media. Such online social data are continuously growing in a way that makes it difficult to efficiently aggregate them, since they are the expression of a multitude of single content creators that most of the times show only a small percentage of originality. The act of 'sharing' is still tied to a pre-Internet fashion that sees it as a step following (and never preceding) content creation, as enforced by the rules of publishing and copyright. In the Internet era, the pieces of the puzzle of a valuable work might be scattered throughout the whole Web. In order to hinder the obsolete create-then-share trend that is killing creativity and usefulness of the Web, we propose a novel concept of copyright, which allows content to be shared while being created, in a way that they can gain increasing value as they become part of an increasingly richer puzzle.	Collective copyright: enabling the natural evolution of content creation in the web era	NA:NA:NA	2014
Adam Jatowt:Carlos Castillo:James Caverlee:Katsumi Tanaka	NA	Session details: WebQuality 2014 workshop	NA:NA:NA:NA	2014
Vlad Bulakh:Christopher W. Dunn:Minaxi Gupta	Fraudulent product promotion online, including online videos, is on the rise. In order to understand and defend against this ill, we engage in the fraudulent video economy for a popular video sharing website, YouTube, and collect a sample of over 3,300 fraudulently promoted videos and 500 bot profiles that promote them. We then characterize fraudulent videos and profiles and train supervised machine learning classifiers that can successfully differentiate fraudulent videos and profiles from legitimate ones.	Identifying fraudulently promoted online videos	NA:NA:NA	2014
Maria Rafalak:Katarzyna Abramczuk:Adam Wierzbicki	This paper describes the results of a study conducted in February 2013 on Amazon Mechanical Turk aimed at identifying various determinants of credibility evaluations. 2046 adult participants evaluated credibility of websites with diversified trustworthiness reference index. We concentrated on psychological factors that lead to the characteristic positive bias observed in many working social feedback systems on the Internet. We have used International Personality Item Pool (IPIP) and measured the following traits: trust, conformity, risk taking, need for cognition and intellect. Results suggest that trustworthiness and risk taking are factors clearly differentiating people with respect to tendency to overestimate, underestimate and judge accordingly websites' credibility. Intuitively people characterized by high general trust tend to be more generous in their credibility evaluations. On the other hand, people who are more willing to take risk, tend to be more critical of the Internet content. The latter indicates that high credibility evaluations are being treated as a default option, and lower ratings require special conditions. Other, more detailed psychological patterns related to websites' credibility evaluations are described in full paper.	Incredible: is (almost) all web content trustworthy? analysis of psychological factors related to website credibility evaluation	NA:NA:NA	2014
Lei Li:Chengzhi Zhang	In the social tagging system, users annotate different web resources according to their need of future information organization and retrieval, and users also annotate resources with different types of tags, such as objective tag, subjective tag, self-organized tag and so on. Because every web resource has its own characteristics, the tag types of each web resource are different. According to the web resource, the quality of each tag type is different. We should depend on resource types to evaluate the quality of tag types, in order to provide efficient tag recommendation service and design better user tagging interfaces. In this paper, we firstly selected five web resources, namely the blog, book, image, music and video, to explore the tag types when annotating different resources. Then we chose specific resource and tags to explore the quality of each tag type according to these five web resources and study the relationship between tag type and quality. The conclusion is that the quality of tag types for different web resources is different.	Quality evaluation of social tags according to web resource types	NA:NA	2014
Volha Bryl:Christian Bizer	In order to efficiently use the ever growing amounts of structured data on the web, methods and tools for quality-aware data integration should be devised. In this paper we propose an approach to automatically learn the conflict resolution strategies, which is a crucial step in large-scale data integration. The approach is implemented as an extension of the Sieve data quality assessment and fusion framework. We apply and evaluate our approach on the use case of fusing data from 10 language editions of DBpedia, a large-scale structured knowledge base extracted from Wikipedia. We also propose a method for extracting rich provenance metadata for each DBpedia fact, which is later used in data fusion.	Learning conflict resolution strategies for cross-language Wikipedia data fusion	NA:NA	2014
Aleksander Wawer:Radoslaw Nielek:Adam Wierzbicki	The article focuses on predicting trustworthiness from textual content of webpages. The recent work Olteanu et al. proposes a number of features (linguistic and social) to apply machine learning methods to recognize trust levels. We demonstrate that this approach can be substantially improved in two ways: by applying machine learning methods to vectors computed, using psychosocial and psycholinguistic features and in a high-dimensional bag-of-words paradigm of word occurrences. Following Olteanu et al., we test the methods in two classification settings, as a 2-class and 3-class scenario, and in a regression setting. In the 3-class scenario, the features compiled by Olteanu et al. achieve weighted precision of 0.63, while the methods proposed in our paper raise it to 0.66 and 0.70. We also examine coefficients of the models in order to discover words associated with low and high trust.	Predicting webpage credibility using linguistic features	NA:NA:NA	2014
U Kang:Leman Akoglu:Polo Chau:Christos Faloutsos	NA	Session details: Big graph mining 2014 workshop	NA:NA:NA:NA	2014
Joseph E. Gonzalez	From social networks to language modeling, the growing scale and importance of graph data has driven the development of new graph-parallel systems. In this talk, I will review the graph-parallel abstraction and describe how it can be used to express important machine learning and graph analytics algorithms like PageRank and Latent factor models. I will present how systems like GraphLab and Pregel exploit restrictions in the graph-parallel abstraction along with advances in distributed graph representation to efficiently execute iterative graph algorithms orders of magnitude faster than more general data-parallel systems. Unfortunately, the same restrictions that enable graph-parallel systems to achieve substantial performance gains also limit their ability to express many of the important stages in a typical graph-analytics pipeline. As a consequence, existing approaches to graph-analytics typically compose multiple systems through brittle and costly file interfaces. To fill the need for a holistic approach to graph-analytics we introduce GraphX, which unifies graph-parallel and data-parallel computation under a single API and system. I will show how a simple set of data-parallel operators can be used to express graph-parallel computation and how, by applying a collection of query optimizations derived from our work on graph-parallel systems, we can execute entire graph-analytics pipelines efficiently in a more general data-parallel distributed fault-tolerant system achieving performance comparable to specialized state-of-the-art systems.	From graphs to tables the design of scalable systems for graph analytics	NA	2014
Seungwhan Moon:Calvin McCarter:Yu-Hsin Kuo	In this paper, we propose a new active learning algorithm in which the learner chooses the samples to be queried from the unlabeled data points whose attributes are only partially observed. In addition, we propose a cost-driven decision framework where the learner chooses to query either the labels or the missing attributes. This problem statement addresses a common constraint when building large datasets and applying active learning techniques on them, where some of the attributes (including the labels) are significantly harder or more costly to acquire per data point. We take a novel approach to this problem, first by building an imputation model that maps from the partially featured data to the fully featured dimension, and then performing active learning on the projected input space combined with the estimated confidence of inference. We discuss that our approach is flexible and can work with graph mining tasks as well as conventional semi-supervised learning problems. The results suggest that the proposed algorithm facilitates more cost-efficient annotation than the baselines.	Active learning with partially featured data	NA:NA:NA	2014
Xiaoming Liu:Yadong Zhou:Chengchen Hu:Xiaohong Guan:Junyuan Leng	Community detection is a common problem in various types of big graphs. It is meaningful to understand the functions and dynamics of networks. The challenges of detecting community for big graphs include high computational cost, no prior information, etc.. In this work, we analyze the process of random walking in graphs, and find out that the weight of an edge gotten by processing the vertices visited by the walker could be an indicator to measure the closeness of vertex connection. Based on this idea, we propose a community detection algorithm for undirected big graphs which consists of three steps, including random walking using a single walker, weight calculating for edges and community detecting. Our algorithm is running in O(n2) without prior information. Experimental results show that our algorithm is capable of detecting the community structure and the overlapping parts of graphs in real-world effectively, and handling the challenges of community detection in big graph era.	Detecting community structure for undirected big graphs based on random walks	NA:NA:NA:NA:NA	2014
Jong-Ryul Lee:Chin-Wan Chung	This paper deals with a novel research work about a new efficient approximation algorithm for influence maximization, which was introduced to maximize the benefit of viral marketing. For efficiency, we devise two ways of exploiting the 2-hop influence spread which is the influence spread on nodes within 2-hops away from nodes in a seed set. Firstly, we propose a new greedy method for the influence maximization problem using the 2-hop influence spread. Secondly, to speed up the new greedy method, we devise an effective way of removing unnecessary nodes for influence maximization based on optimal seed's local influence heuristics. In our experiments, we evaluate our method with real-life datasets, and compare it with recent existing methods. From experimental results, the proposed method is at least an order of magnitude faster than the existing methods in all cases while achieving similar accuracy.	A fast approximation for influence maximization in large social networks	NA:NA	2014
Alireza Rezaei Mahdiraji:Peter Baumann	In this work-in-progress paper, we model scientific meshes as a multi-graph in Neo4j graph database using the graph property model. We conduct experiments to measure the performance of the graph database solution in processing mesh queries and compare it with GrAL mesh library and PostgreSQL database on synthetic and real mesh datasets. The experiments show that the databases outperform the mesh library. However, each of the databases perform better on specific query type, i.e, the graph database shows the best performance on global path-intensive queries and the relational database on local and field queries. Based on the experiments, we propose a mediator architecture for processing mesh queries by using the three database systems.	Processing scientific mesh queries in graph databases	NA:NA	2014
Feng Xia:Irwin King:Huan Liu	NA	Session details: 2014 big scholarly data: towards the web of scholars workshop	NA:NA:NA	2014
Hanghang Tong	In his world-widely renowned book, Nobel laureate Herbert Simon pointed out that it is more the complexity of the environment, than the complexity of the individual persons, that determines the complex behavior of humans. The emergence of online social network sites and web 2.0 applications provides a new connected environment/context, where people generate, share and search massive human knowledge; and interact and collaborate with each other to collectively perform some complex tasks. In this talk, we focus on how to make sense of the collaboration data in the context of graphs/networks. To be specific, we will introduce a suite of tools for querying complex patterns from such graphs. Exemplar questions we aim to answer include (a) what makes a team more successful than others, and how to find the best replacement if one of its team members becomes unavailable? (b) how to find a group of authors from databases, data mining and bioinformatics and they collaborate with each other in a star-shape? (c) given a set of querying authors of interest, how to find somebody who initiates the research field these querying authors belong to, and how to summarize and visualize the querying authors' (d) how to incorporate users' preference into these complex queries' We will also introduce the computational challenges behind these querying tools and how to remedy them.	Query complex graph patterns: tools and applications	NA	2014
Anastasia Dimou:Laurens De Vocht:Mathias Van Compernolle:Erik Mannens:Peter Mechant:Rik Van de Walle	As the Web evolves in an integrated and interlinked knowledge space thanks to the growing amount of published Linked Open Data, the need to find solutions that enable the scholars to discover, explore and analyse the underlying research data emerges. Scholars, typically non-expert technology users, lack of in-depth understanding of the underlying semantic technology which limits their ability to interpret and query the data. We present a visual workflow to connect scholars and scientific resources on the Web of Data. We allow scholars to move from exploratory analysis in academic social networks to exposing relations between these resources. We allow them to reveal experts in a particular field and discover relations in and beyond their research communities. This paper aims to evaluate the potential of such a visual workflow to be used by non-expert users to interact with the semantically enriched data and familiarize with the underlying dataset.	A visual workflow to explore the web of data for scholars	NA:NA:NA:NA:NA:NA	2014
Qiang Ma:S. Muthukrishnan:Brian Thompson:Graham Cormode	In this work, we aim to understand the mechanisms driving academic collaboration. We begin by building a model for how researchers split their effort between multiple papers, and how collaboration affects the number of citations a paper receives, supported by observations from a large real-world publication and citation dataset, which we call the h-Reinvestment model. Using tools from the field of Game Theory, we study researchers' collaborative behavior over time under this model, with the premise that each researcher wants to maximize his or her academic success. We find analytically that there is a strong incentive to collaborate rather than work in isolation, and that studying collaborative behavior through a game-theoretic lens is a promising approach to help us better understand the nature and dynamics of academic collaboration.	Modeling collaboration in academia: a game theoretic approach	NA:NA:NA:NA	2014
László Gulyás:Zsolt Jurányi:Sándor Soós:George Kampis	This paper reports the preliminary results of a project that aims at incorporating the analysis of the web presence (content) of research institutions into the scientometric analysis of these research institutions. The problem is to understand and predict the dynamics of academic activity and resource allocation using web presence. The present paper approaches this problem in two parts. First we develop a crawler and an archive of the web contents obtained from academic institutions, and present an early analysis of the records. Second, we use (currently off-line records to analyze the dynamics of resource allocation. Combination of the two parts is an ambition of ongoing work. The motivation in this study is twofold. First, we strongly believe that independent archiving, indexing and searching of (past) web content is an important task, even with regards to academic web presence. We are particularly interested in studying the dynamics of the "online scientific discourse", based on the assumption that the changing traces of web presence is an important factor that documents the intensity of activity. Second, we maintain that the trend-analysis of scientific activity represents a hitherto unused potential. We illustrate this by a pilot where, using 'offline' longitudinal datasets, we study whether past (i.e. cumulative) success can predict current (and future) activity in academia. Or, in short: do institutions invest and publish in areas where they have been successful? Answer to this question is, we believe, important to understanding and predicting research policies and their changes.	Can web presence predict academic performance?: the case of Eötvös university	NA:NA:NA:NA	2014
Hui Shi:Kurt Maly:Steven Zeil	Projects such as Libra and Cimple have built systems to capture knowledge in a research community and to respond to semantic queries. However, they lack the support for a knowledge base that can evolve over time while responding to queries requiring reasoning. We consider a semantic web that covers linked data about science research that are being harvested from the Web and are supplemented and edited by community members. We use ontologies to incorporate semantics to detect conflicts and resolve inconsistencies, and to infer new relations or proof statements with a reasoning engine. We consider a semantic web subject to changes in the knowledge base, the underlying ontology or the rule set that governs the reasoning. In this paper we explore the idea of trust where each change to the knowledge base is analyzed as to what subset of the knowledge base can still be trusted. We present algorithms that adapt the reasoner such that, when proving a goal, it does a simple retrieval when it encounters trusted items and backward chaining over untrusted items. We provide an evaluation of our proposed modifications that show that our algorithm is conservative and that it provides significant gains in performance for certain queries.	Trust and hybrid reasoning for ontological knowledge bases	NA:NA:NA	2014
Yu Liu:Zhen Huang:Jing Fang:Yizhou Yan	With the rapid increase of research papers, article-level metrics are of growing importance for helping researchers select papers. Classical metrics have a significant drawback of just using single factor, which limits the effectiveness of assessing papers in different periods after publication. Moreover, with the development of web 2.0, some new factors are introduced to assess papers. So, a novel article level metric in the context of research community (ALM_RC) is proposed. It integrates the impact of different factors comprehensively, because different factors have different time features and can complement each other in different periods after publication. In addition, as a research community is based on certain research directions, it is a relatively stable environment with related journals and scholars contributing their efforts to development of this research field. So in the context of research community, it is consistent, practical and reasonable to calculate the impact of the journals and scholars under relatively fair criteria. Experimental results show the novel metric is effective and robust in assessing papers.	An article level metric in the context of research community	NA:NA:NA:NA	2014
Laurens De Vocht:Selver Softic:Erik Mannens:Martin Ebner:Rik Van de Walle	Resources for research are not always easy to explore, and rarely come with strong support for identifying, linking and selecting those that can be of interest to scholars. In this work we introduce a model that uses state-of-the-art semantic technologies to interlink structured research data and data from Web collaboration tools, social media and Linked Open Data. We use this model to build a platform that connects scholars, using their profiles as a starting point to explore novel and relevant content for their research. Scholars can easily adapt to evolving trends by synchronizing new social media accounts or collaboration tools and integrate then with new datasets. We evaluate our approach by a scenario of personalized exploration of research repositories where we analyze real world scholar profiles and compare them to a reference profile.	Aligning web collaboration tools with research data for scholars	NA:NA:NA:NA:NA	2014
Jing Li:Feng Xia:Wei Wang:Zhen Chen:Nana Yaw Asabere:Huizhen Jiang	Recent academic procedures have depicted that work involving scientific research tends to be more prolific through collaboration and cooperation among researchers and research groups. On the other hand, discovering new collaborators who are smart enough to conduct joint-research work is accompanied with both difficulties and opportunities. One notable difficulty as well as opportunity is the big scholarly data. In this paper, we satisfy the demand of collaboration recommendation through co-authorship in an academic network. We propose a random walk model using three academic metrics as basics for recommending new collaborations. Each metric is studied through mutual paper co-authoring information and serves to compute the link importance such that a random walker is more likely to visit the valuable nodes. Our experiments on DBLP dataset show that our approach can improve the precision, recall rate and coverage rate of recommendation, compared with other state-of-the-art approaches.	ACRec: a co-authorship based random walk model for academic collaboration recommendation	NA:NA:NA:NA:NA:NA	2014
Suhendry Effendy:Irvan Jahja:Roland H.C. Yap	A large percentage of the research in computer science is published in conferences and workshops. We propose three methods which compute a "relatedness score" for conferences relative to a pivot conference, usually a top rated conference. We experiment with the DBLP bibliography to show that our relatedness ranking can be used to help understand the basis of conference reputation ratings, determine what conferences are related to an area and the classification of conferences into areas.	Relatedness measures between conferences in computer science: a preliminary study based on DBLP	NA:NA:NA	2014
Isaac Lera:Carlos Guerrero:Carlos Juiz	We have transformed five years of curriculum data of our academic staff from relational databases to a semantic model. Thanks to semantic queries, capabilities of NoSQL models, inference reasoners and data mining techniques we obtain knowledge that it improves the personal management of curriculum data, the quality and efficiency of exploitation tasks, and the transparency, dissemination and collaboration with citizens. The huge catalogue of CV data remains an underutilized resource. Private companies such as editorials have robust services based only on publications but academic institutions have the option of integrating other databases related with their staff to obtain more indicators. We analyse the transformation of data, highlighting the mapping process of authors, and we present two ways of exploitation using semantic queries and complex networks. Thus, institutions, researchers and citizens will have a quality data catalogue for diverse studies.	Indicators and functionalities of exploitation of academic staff CV using semantic web technologies	NA:NA:NA	2014
Graham Cormode:S. Muthukrishnan:Jinyun Yan	There are many situations that one needs to find comparable others for a given researcher. Examples include finding peer reviewers, programming committees for conferences, and comparable individual asked in recommendation letters for tenure evaluation. The task is often done on an ad hoc and informal basis. In this paper, we address an interesting problem that has not been adequately studied so far: mining cumulated large scale scholarly data to find comparable researchers. We propose a standard to quantify the quality of individual's research output, through the quality of publishing venues. We represent a researcher as a sequence of her publication records, and develop methods to compute the distance between two researchers through sequence matching. Multiple variations of distances are considered to target different scenarios. We define comparable relation by the distance, and conduct experiments on a large corpus and demonstrate the effectiveness our methods through examples. In the end of the paper, we identify several promising directions for further work.	People like us: mining scholarly data for comparable researchers	NA:NA:NA	2014
Emilio Ferrara:Yong-Yeol Ahn:Yana Volkovich:Young-Ho Eom	NA	Session details: Connecting online & offline life workshop (COOL 2014)	NA:NA:NA:NA	2014
Tian Zhou:Lixin Gao:Daiheng Ni	Road traffic conditions are typically affected by events such as extreme weather or sport games. With the advance of Web, events and weather conditions can be readily retrieved in real-time. In this paper, we propose a traffic condition prediction system incorporating both online and offline information. RFID-based system has been deployed for monitoring road traffic. By incorporating data from both road traffic monitoring system and online information, we propose a hierarchical Bayesian network to predict road traffic condition. Using historical data, we establish a hierarchical Bayesian network to characterize the relationships among events and road traffic conditions. To evaluate the model, we use the traffic data collected in Western Massachusetts as well as online information about events and weather. Our proposed prediction achieves an accuracy of 93% overall.	Road traffic prediction by incorporating online information	NA:NA:NA	2014
Daniele Quercia	For the last few years, I and my colleagues have been exploring the complex relationship between our offline and online worlds. This talk will show that, as online platforms become mature, the social behavior we have evolved over thousands of years is reflected on our actions on the web as well. It turns out that, in the context of social influence, finding the "(special) many" (of those who are able to spot trends early one) is more important than trying to find the "special few"[10]; that people with different personality traits take on different roles on both Twitter and Facebook [5,6]; that language, with its vocabulary and prescribed ways of communicating, is a symbolic resource that can be used on its own to influence others [4]; and that a Facebook relationship is more likely to break if it is not embedded in the same social circle, if it is between two people whose ages differ, and if one of the two is neurotic or introvert [3]. Interestingly, we also found that a relationship with a common female friend is more robust than that with a common male friend. More recently, we have also explored the relationship between offline and online worlds in the urban context. We have considered hypotheses put forward in the 1970s urban sociology literature [1,2] and, for the first time, we have been able to test them at scale.We have done so by building two crowdsourcing web games: one crowdsources Londoners' mental images of the city [8], and the other crowdsources the discovery of the urban elements that make people happy [7]. We have found that, as opposed to well-to-do areas, those suffering from social problems are rarely present in residents' mental maps of the city, and they tend to be characterized more by cars and fortress-like buildings than by greenery. This talk will conclude by showing how combining both web games with Flickr offers interesting applications for discovering emotionally-pleasant routes [9] and for ranking city pictures [11].	Evolutionary habits on the web	NA	2014
Munmun De Choudhury	Millions of people each year suffer from depression, which makes mental illness one of the most serious and widespread health challenges in our society today. There is therefore a need for effective policies, interventions, and prevention strategies that enable early detection and diagnosis of mental health concerns in populations. This talk reports some findings on the potential of leveraging social media postings as a new type of lens in understanding mental illness in individuals and populations. Information gleaned from social media bears potential to complement traditional survey techniques in its ability to provide finer grained measurements of behavior over time while radically expanding population sample sizes. The talk highlights how this research direction may be useful in developing tools for identifying the onset of depressive disorders, for use by healthcare agencies; or on behalf of individuals, enabling those suffering from mental illness to be more proactive about their mental health.	Can social media help us reason about mental health?	NA	2014
Haewoon Kwak	With the remarkable advances from isolated console games to massively multi-player online role-playing games, the online gaming world provides yet another place where people interact with each other. Online games have attracted attention from researchers, because i) the purpose of actions is relatively clear, and ii) actions are quantifiable. A wide range of predefined actions for supporting social interaction (e.g., friendship, communication, trade, enmity, aggression, and punishment) reflects either positive or negative connotations among game players, and is unobtrusively recorded by the game servers. These rich electronic footprints have become invaluable assets for the research of social dynamics. In particular, exploring negative behavior in online games is a key research direction because it directly influences gaming experience and user satisfaction. Even a few negative players can impact many others because of the design of multi-player games. For this reason these players are called toxic. The definition of toxic play is not cut and dry. Even if someone follows the game rules, he could be considered toxic. For example, killing one player repetitively is often deemed toxic behavior, although it does not break game rules at all. The vagueness of toxicity makes it hard to understand, detect, and prevent it. League of Legends (LoL), created by Riot Games with 70 million users as of 2012, offers a new way to understand toxic behavior. Riot Games develops a crowdsourcing framework, the Tribunal, to judge whether reported toxic behavior should be punished or not. Volunteered players review user reports and vote for either pardon or punishment. As of March 2013, 105 million votes had been collected in North America and Europe. We explore toxic playing and reaction based on large-scale data from the Tribunal[1]. We collect and investigate over 10 million user reports on 1.46 million toxic players and corresponding crowdsourced decisions made in the Tribunal. We crawl data from three different regions, North America, Western Europe, and Korea, to take regional differences of user behavior into account. To obtain the comprehensive view of toxic playing and reaction based on huge data collection, we answer following research questions in a bottom-up approach: how individuals react to toxic players, how teams interact with toxic players, how general toxic or non-toxic players behave across the match, and how crowds make a decision on toxic players. We find large-scale empirical support for some notoriously difficult theories to test in the wild, which are bystander effect, ingroup favoritism, black sheep effect, cohesion-performance relationships, and attribution theory. We also discover that regional differences affect the likelihood of being reported and the proportion of being punished of toxic players in the Tribunal. We then propose a supervised learning approach for predicting crowdsourced decisions on toxic behavior with large-scale labeled data collections[2]. Using the same sparse information available to the reviewers, we trained classifiers to detect the presence, and severity of toxicity. We built several models oriented around in-game performance, reports by victims of toxic behavior, and linguistic features of chat messages. We found that training with high agreement decisions resulted in more accuracy on low agreement decisions and that our classifier was adept in detecting clear cut innocence. Finally, we showed that our classifier is relatively robust across cultural regions; our classifier built from a North American dataset performed adequately on a European dataset. Ultimately, our work can be used as a foundation for the further study of toxic behavior.	Understanding toxic behavior in online games	NA	2014
Kwan Hui Lim:Binyan Jiang:Ee-Peng Lim:Palakorn Achananuparp	With the widespread adoption of the Web, many companies and organizations have established websites that provide information and support online transactions (e.g., buying products or viewing content). Unfortunately, users have limited attention to spare for interacting with online sites. Hence, it is of utmost importance to design sites that attract user attention and effectively guide users to the product or content items they like. Thus, we propose a novel and scalable experimentation approach to evaluate the effectiveness of online site designs. Our case study focuses on the effects of an authority message on visitors' browsing behavior on workshop and seminar online announcement sites. An authority message emphasizes a particular prominent speaker and his/her achievements. Through dividing users into control and treatment groups and carefully tracking their online activities, we observe that the authority message influences the way users interact with page elements on the website and increases their interests in the authority speakers.	Do you know the speaker?: an online experiment with authority messages on event websites	NA:NA:NA:NA	2014
Ting Wang	With the advances of sensory, satellite and mobile communication technologies in recent decades, locational data become widely available. A lot of work has been developed to find useful information from these data, and various approaches have been proposed. In this work, we aim to use one specific type of locational data -- network connection logs of mobile devices, which is widely available and easily accessible to telecom companies, to identify and extract active areas of users. This is a challenging topic due to the existence of inaccurate location and fluctuating log time intervals of this kind of data. In order to observe user behavior from this kind of data set, we propose a new algorithm, namely Behavior Observation Tool (BOT), which uses Convex Hull Algorithm with sliding time windows to model the user's movement, and thus knowledge about the user's lifestyle and habits can extracted from the mobile device network logs.	A behavior observation tool (BOT) for mobile device network connection logs	NA	2014
Petter Holme	NA	The social, economic and sexual networks of prostitution	NA	2014
Mohammad Jaber:Peter T. Wood:Panagiotis Papapetrou:Sven Helmer	Social networks can represent many different types of relationships between actors, some explicit and some implicit. For example, email communications between users may be represented explicitly in a network, while managerial relationships may not. In this paper we focus on analyzing explicit interactions among actors in order to detect hierarchical social relationships that may be implicit. We start by employing three well-known ranking-based methods, PageRank, Degree Centrality, and Rooted-PageRank (RPR) to infer such implicit relationships from interactions between actors. Then we propose two novel approaches which take into account the time-dimension of interactions in the process of detecting hierarchical ties. We experiment on two datasets, the Enron email dataset to infer manager-subordinate relationships from email exchanges, and a scientific publication co-authorship dataset to detect PhD advisor-advisee relationships from paper co-authorships. Our experiments show that time-based methods perform considerably better than ranking-based methods. In the Enron dataset, they detect 48% of manager-subordinate ties versus 32% found by Rooted-PageRank. Similarly, in co-author dataset, they detect 62% of advisor-advisee ties compared to only 39% by Rooted-PageRank.	Inferring offline hierarchical ties from online social networks	NA:NA:NA:NA	2014
Onur Varol:Filippo Menczer	Many species dream, yet there remain many open research questions in the study of dreams. The symbolism of dreams and their interpretation is present in cultures throughout history. Analysis of online data sources for dream interpretation using network science leads to understanding symbolism in dreams and their associated meaning. In this study, we introduce dream interpretation networks for English, Chinese and Arabic that represent different cultures from various parts of the world. We analyze communities in these networks, finding that symbols within a community are semantically related. The central nodes in communities give insight about cultures and symbols in dreams. The community structure of different networks highlights cultural similarities and differences. Interconnections between different networks are also identified by translating symbols from different languages into English. Structural correlations across networks point out relationships between cultures. Similarities between network communities are also investigated by analysis of sentiment in symbol interpretations. We find that interpretations within a community tend to have similar sentiment. Furthermore, we cluster communities based on their sentiment, yielding three main categories of positive, negative, and neutral dream symbols.	Connecting dream networks across cultures	NA:NA	2014
Valter Crescenzi:Tim Furche:Paolo Merialdo:Giorgio Orsi	NA	Session details: Data extraction and object search 2014 workshop	NA:NA:NA:NA	2014
Neil Anderson:Jun Hong	Automatically determining and assigning shared and meaningful text labels to data extracted from an e-Commerce web page is a challenging problem. An e-Commerce web page can display a list of data records, each of which can contain a combination of data items (e.g. product name and price) and explicit labels, which describe some of these data items. Recent advances in extraction techniques have made it much easier to precisely extract individual data items and labels from a web page, however, there are two open problems: 1. assigning an explicit label to a data item, and 2. determining labels for the remaining data items. Furthermore, improvements in the availability and coverage of vocabularies, especially in the context of e-Commerce web sites, means that we now have access to a bank of relevant, meaningful and shared labels which can be assigned to extracted data items. However, there is a need for a technique which will take as input a set of extracted data items and assign automatically to them the most relevant and meaningful labels from a shared vocabulary. We observe that the Information Extraction (IE) community has developed a great number of techniques which solve problems similar to our own. In this work-in-progress paper we propose our intention to theoretically and experimentally evaluate different IE techniques to ascertain which is most suitable to solve this problem.	Evaluation of information extraction techniques to label extracted data from e-commerce web pages	NA:NA	2014
Stefano Ortona	Today the web has become the largest available source of information. The automatic extraction of structured data from web is a challenging problem that has been widely investigated. However, after the extraction process, the problem of identifying duplicates among the extracted web records must be solved in order to present clean data to the final user. This problem, also known as record linkage or record matching, has been of central interest for the database community; however, only few works have addressed this problem in the web context. In this paper we present web object matching, the problem of identifying duplicates among records extracted from the web. We will show that in the web scenario we need to face all the problems of a classic record linkage setting plus the uncertainty introduced by the web. Indeed the records are the output of an extraction system that, rather than conventional databases or APIs, introduces semantic errors that are not due to a problem in the source. Most of the previous approaches rely on the fact that the records to match contain the correct information and we can use such information to identify duplicates. In this work we overview an approach that performs a validation step before the actual identification of duplicates, in order to check whether the information of the record can be trusted or not. We present an approach that works without any human supervision or training data and that deals with the problem not only in a record-by-record fashion (as other approaches), but also in a source-by-source fashion which allows detecting and possibly correcting systematic errors for an entire source. The only human effort required is the creation of a little knowledge about the domain of interest through a set of ontology constraints and an entity extraction system.	An analysis of duplicate on web extracted objects	NA	2014
Qian Chen:Mizuho Iwaihara	Entity type matching has many real world applications, especially in entity clustering, de-duplication and efficient query processing. Current methods to extract entities from text usually disregard regularities in the order of entities appearing in the text. In this paper, we focus on enumerative descriptions which enlist entity names in a certain hidden order, often occurring in web documents as listings and tables. We propose an algorithm to discover entity types from enumerative descriptions, where a type hierarchy is known but enumerating orders are hidden and heterogeneous, and partial entity-type mappings are given as seed instances. Our algorithm is iterative: We extract skeletons from syntactic patterns, then train a hidden Markov model to find an optimum enumerating order from seed instances and skeletons, to find a best-fit entity-type assignment.	Iterative algorithm for inferring entity types from enumerative descriptions	NA:NA	2014
Weifeng Su:Yafei Li:Frederick H. Lochovsky	Users submit queries to an online database via its query interface. Query interface parsing, which is important for many applications, understands the query capabilities of a query interface. Since most query interfaces are organized hierarchically, we present a novel query interface parsing method, StatParser (Statistical Parser), to automatically extract the hierarchical query capabilities of query interfaces. StatParser automatically learns from a set of parsed query interfaces and parses new query interfaces. StatParser starts from a small grammar and enhances the grammar with a set of probabilities learned from parsed query interfaces under the maximum-entropy principle. Given a new query interface, the probability-enhanced-grammar identifies the parse tree with the largest global probability to be the query capabilities of the query interface. Experimental results show that StatParser very accurately extracts the query capabilities and can effectively overcome the problems of existing query interface parsers.	Query interfaces understanding by statistical parsing	NA:NA:NA	2014
Disheng Qiu:Lorenzo Luce	The extraction and integration of data from many web sources in different domains is an open issue. Two promising solutions take on this challenge: top down approaches rely on a domain knowledge that is manually crafted by an expert to guide the process and bottom up approaches try to infer the schema from many web sources to make sense of the extracted data. The first solutions scale over the number of web sources, but for settings with different domains, an expert has to manually craft an ontology for each domain. The second solutions do not require a domain expert, but high quality is achieved only with a lot of human interactions both in the extraction and integration steps. We introduce a framework that takes the best from both approaches. The framework addresses synergically both extraction and integration of data from web sources. No domain expert is required, it exploits data from a seed knowledge base to enhance the automatic extraction and integration (top down). Human workers from crowdsourcing platforms are engaged to improve the quality and the coverage of the extracted data. The framework adopts techniques to automatically extract both the schema and the data from multiple web sources (bottom up). The extracted information is then used to bootstrap the seed knowledge base, reducing in this way the human effort for future tasks.	Extraction and integration of web sources with humans and domain knowledge	NA:NA	2014
Petar Petrovski:Volha Bryl:Christian Bizer	Large numbers of websites have started to markup their content using standards such as Microdata, Microformats, and RDFa. The marked-up content elements comprise descriptions of people, organizations, places, events, products, ratings, and reviews. This development has accelerated in last years as major search engines such as Google, Bing and Yahoo! use the markup to improve their search results. Embedding semantic markup facilitates identifying content elements on webpages. However, the markup is mostly not as fine-grained as desirable for applications that aim to integrate data from large numbers of websites. This paper discusses the challenges that arise in the task of integrating descriptions of electronic products from several thousand e-shops that offer Microdata markup. We present a solution for each step of the data integration process including Microdata extraction, product classification, product feature extraction, identity resolution, and data fusion. We evaluate our processing pipeline using 1.9 million product offers from 9240 e-shops which we extracted from the Common Crawl 2012, a large public Web corpus.	Integrating product data from websites offering microdata markup	NA:NA:NA	2014
Zhaochen Guo:Denilson Barbosa	Entity Linking (EL) consists in linking mentions in a document to their referent entities in a Knowledge Base. Current approaches fall into two main categories: local approaches, in which mentions are linked independently of each other, and global approaches, in which all mentions are linked collectively. Local approaches often ignore the semantic relatedness of entities, and while global approaches incorporate the semantic relatedness, they tend to focus only on directly connected entities, ignoring indirect connections which might be useful. We present a global EL approach that unifies the representation of the semantics of entities and documents--the probability distribution of entities being visited during a random walk on an entity graph--that accounts for direct and indirect connections. An experimental evaluation shows that our method outperforms five state-of-the-art EL systems and two very strong baselines.	Entity linking with a unified semantic representation	NA:NA	2014
Toyotaro Suzumura:Qi He:Yuanyuan Tian	NA	Session details: 2014 large scale network analysis workshop (LSNA'14)	NA:NA:NA	2014
Io Taxidou:Peter M. Fischer	The advent of social media has facilitated the study of information diffusion, user interaction and user influence over social networks. The research on analyzing information spreading focuses mostly on modeling, while analyses of real-life data have been limited to small, carefully cleaned datasets that are analyzed in an offline fashion. In this paper, we present an approach for online analysis of information diffusion in Twitter. We reconstruct so-called information cascades that model how information is being propagated from user to user from the stream of messages and the social graph. The results show that such an inference is feasible even on noisy, large-scale, rapidly produced data. We provide insights into the impact of incomplete data and the effect of different influence models on the cascades. The observed cascades show a significant amount of variety in scale and structure.	Online analysis of information diffusion in twitter	NA:NA	2014
Satoshi Kurihara	Twitter is a famous social networking service and has received attention recently. Twitter user have increased rapidly, and many users exchange information. When the East Japan great earthquake disaster occurred on March 11, 2011, many people could obtain important information from social networking service. Although twitter also played the important role a false rumor diffusion was pointed out. So, in this talk, I would like to focus on the false rumor diffusion phenomena, and introduce about our multi agent information diffusion model based on SIR model. And I would like to discuss about more rapid correction-tweet diffusion methodology.	The multi agent based information diffusion model for false rumordiffusion analysis	NA	2014
Toyotaro Suzumura:Shunsuke Nishii:Masaru Ganse	In recent years, real-time data mining for large-scale time-evolving graphs is becoming a hot research topic. Most of the prior arts target relatively static graphs and also process them in store-and-process batch processing model. In this paper we propose a method of applying on-the-fly and incremental graph stream computing model to such dynamic graph analysis. To process large-scale graph streams on a cluster of nodes dynamically in a scalable fashion, we propose an incremental large-scale graph processing model called "Incremental GIM-V (Generalized Iterative Matrix-Vector Multiplication)". We also design and implement UNICORN, a system that adopts the proposed incremental processing model on top of IBM InfoSphere Streams. Our performance evaluation demonstrates that our method achieves up to 48% speedup on PageRank with Scale 16 Log-normal Graph (vertexes=65,536, edges=8,364,525) with 4 nodes, 3023% speedup on Random walk with Restart with Kronecker Graph with Scale 18 (vertexes=262,144, edges=8,388,608) with 4 nodes against original GIM-V.	Towards large-scale graph stream processing platform	NA:NA:NA	2014
Hidefumi Ogata:Toyotaro Suzumura	The use of social application such as Twitter or FaceBook becomes popular in recent years. In particular, Twitter increases the number of the users rapidly from 2009 as the place that users can tweet anything in 140 characters. In the area of social network analysis, the user network of Twitter is frequently analyzed. Haewoon, et.al.,[1] analyzed the Twitter user network from various point of view in 2009, and they show that the Twitter user network has some different feature from conventional social networks. Bong- won, et.al., also made a collection of 74 millions tweets in 2010, and investigated the influence that "retweet" gives for diffusion of the information. Such analysis not only reveal the unique characteristics of Twitter user network, but also make some networking service such as finding users who are similar to someone, or the recommendation of commodities by using tweet information. There are some analysis such as clustering which needs entire data of the network. However, since social networks are increasing day by day, it becomes impossible to obtain the entire network by crawling. As a solution of this problem, there is the network analysis called link prediction. This enables to predict true network from a given part of the network. If we use link prediction, we can recover the entire network from the network data which we already obtained, and apply some analysis such as clustering to predicted net- work, then we may get the approximate result of the analysis for the entire network. In our research, we implemented one of the link prediction algorithm named Link Propagation in X10, which is a parallel programming language. And evaluated its scalability and precision with Twitter user network data.	Towards scalable X10 based link prediction for large scale social networks	NA:NA	2014
Chungmok Lee:Minh Pham:Norman Kim:Myong K. Jeong:Dennis K.J. Lin:Wanpracha Art Chavalitwongse	The link prediction problem is to predict the existence of a link between every node pair in the network based on the past observed networks arising in many practical applications such as recommender systems, information retrieval, and the marketing analysis of social networks. Here, we propose a new mathematical programming approach for predicting a future network utilizing the node degree distribution identified from historical observation of the past networks. We develop an integer programming problem for the link prediction problem, where the objective is to maximize the sum of link scores (probabilities) while respecting the node degree distribution of the networks. The performance of the proposed framework is tested on the real-life Facebook networks. The computational results show that the proposed approach can considerably improve the performance of previously published link prediction methods.	A novel link prediction approach for scale-free networks	NA:NA:NA:NA:NA:NA	2014
Flavio Chierichetti:Niloy Ganguly:Alessandra Sala	NA	Session details: Posters	NA:NA:NA	2015
Javad Azimi:Adnan Alam:Ruofei Zhang	Paid Search (PS) ads are one of the main revenue sources of online advertising companies where the goal is returning a set of relevant ads for a searched query in search engine websites such as Bing. Typical PS algorithms, return the ads which their Bided Keywords (BKs) are a subset of searched queries or relevant to them. However, there is a huge gap between BKs and searched queries as a considerable amount of BKs are rarely searched by the users. This is mostly due to the rare BKs provided by advertisers. In this paper, we propose an approach to rewrite the rare BKs to more commonly searched keywords, without compromising the original BKs intent, which increases the coverage and depth of PS ads and thus it delivers higher monetization power. In general, we first find the relevant web documents pertaining to the BKs and then extract common keywords using the web doc title and its summary snippets. Experimental results show the effectiveness of proposed algorithm in rewriting rare BKs and consequently providing us with a significant improvement in recall and thereby revenue.	Ads Keyword Rewriting Using Search Engine Results	NA:NA:NA	2015
Siddhartha Banerjee:Prasenjit Mitra:Kazunari Sugiyama	Automatic summarization techniques on meeting conversations developed so far have been primarily extractive, resulting in poor summaries. To improve this, we propose an approach to generate abstractive summaries by fusing important content from several utterances. Any meeting is generally comprised of several discussion topic segments. For each topic segment within a meeting conversation, we aim to generate a one sentence summary from the most important utterances using an integer linear programming-based sentence fusion approach. Experimental results show that our method can generate more informative summaries than the baselines.	Abstractive Meeting Summarization Using Dependency Graph Fusion	NA:NA:NA	2015
Piyush Bansal:Somay Jain:Vasudeva Varma	On various microblogging platforms like Twitter, the users post short text messages ranging from news and information to thoughts and daily chatter. These messages often contain keywords called Hashtags, which are semantico-syntactic constructs that enable topical classification of the microblog posts. In this poster, we propose and evaluate a novel method of semantic enrichment of microblogs for a particular type of entity search -- retrieving a ranked list of the top-k hashtags relevant to a user's query Q. Such a list can help the users track posts of their general interest. We show that our technique significantly improved microblog retrieval as well. We tested our approach on the publicly available Stanford sentiment analysis tweet corpus. We observed an improvement of more than 10% in NDCG for microblog retrieval task, and around 11% in mean average precision for hashtag retrieval task.	Towards Semantic Retrieval of Hashtags in Microblogs	NA:NA:NA	2015
Peng Bao:Hua-Wei Shen:Xiaolong Jin:Xue-Qi Cheng	The ability to model and predict the popularity dynamics of individual user generated items on online media has important implications in a wide range of areas. In this paper, we propose a probabilistic model using a Self-Excited Hawkes Process (SEHP) to characterize the process through which individual microblogs gain their popularity. This model explicitly captures the triggering effect of each forwarding, distinguishing itself from the reinforced Poisson process based model where all previous forwardings are simply aggregated as a single triggering effect. We validate the proposed model by applying it on Sina Weibo, the most popular microblogging network in China. Experimental results demonstrate that the SEHP model consistently outperforms the model based on reinforced Poisson process.	Modeling and Predicting Popularity Dynamics of Microblogs using Self-Excited Hawkes Processes	NA:NA:NA:NA	2015
Joel Barajas:Ram Akella:Marius Holtan	We propose a user targeting simulator for online display advertising. Based on the response of 37 million visiting users (targeted and non-targeted) and their features, we simulate different user targeting policies. We provide evidence that the standard conversion optimization policy shows similar effectiveness to that of a random targeting, and significantly inferior to other causally optimized targeting policies.	Evaluating User Targeting Policies: Simulation Based on Randomized Experiment Data	NA:NA:NA	2015
Florin Bulgarov:Cornelia Caragea	Keyphrases for a document provide a high-level topic description of the document. Given the number of documents growing exponentially on the Web in the past years, accurate methods for extracting keyphrases from such documents are greatly needed. In this study, we provide a comparison of existing supervised approaches to this task to determine the current best performing model. We use research articles on the Web as the case study.	A Comparison of Supervised Keyphrase Extraction Models	NA:NA	2015
Thomas Cerqueus:Eduardo Cunha de Almeida:Stefanie Scherzinger	In building software-as-a-service applications, a flexible development environment is key to shipping early and often. Therefore, schema-flexible data stores are becoming more and more popular. They can store data with heterogeneous structure, allowing for new releases to be pushed frequently, without having to migrate legacy data first. However, the current application code must continue to work with any legacy data that has already been persisted in production. To let legacy data structurally "catch up" with the latest application code, developers commonly employ object mapper libraries with life-cycle annotations. Yet when used without caution, they can cause runtime errors and even data loss. We present ControVol, an IDE plugin that detects evolutionary changes to the application code that are incompatible with legacy data. ControVol warns developers already at development time, and even suggests automatic fixes for lazily migrating legacy data when it is loaded into the application. Thus, ControVol ensures that the structure of legacy data can catch up with the structure expected by the latest software release.	ControVol: Let Yesterday's Data Catch Up with Today's Application Code	NA:NA:NA	2015
Angelos Charalambidis:Stasinos Konstantopoulos:Vangelis Karkaletsis	Dataset description vocabularies focus on provenance, versioning, licensing, and similar metadata. VoID is a notable exception, providing some expressivity for describing subsets and their contents and can, to some extent, be used for discovering relevant resources and for optimizing querying. In this poster we describe an extension of VoID that provides the expressivity needed in order to support the query planning methods typically used in federated querying.	Dataset Descriptions for Optimizing Federated Querying	NA:NA:NA	2015
Yiwei Chen:Katja Hofmann	Online learning to rank holds great promise for learning personalized search result rankings. First algorithms have been proposed, namely absolute feedback approaches, based on contextual bandits learning; and relative feedback approaches, based on gradient methods and inferred preferences between complete result rankings. Both types of approaches have shown promise, but they have not previously been compared to each other. It is therefore unclear which type of approach is the most suitable for which online learning to rank problems. In this work we present the first empirical comparison of absolute and relative online learning to rank approaches.	Online Learning to Rank: Absolute vs. Relative	NA:NA	2015
Daniela Chuda:Peter Kratky:Jozef Tvarozek	Behavioral biometrics based on mouse usage can be used to recognize one's identity, with special applications in anonymous Web browsing. Out of many features that describe browsing behavior, mouse clicks (or touches) as the most basic of navigation actions, provide a stable stream of behavioral data. The paper describes a method to recognize Web user according to three click features. The distance-based classification comparing cumulative distribution functions achieves high recognition accuracy even with hundreds of users.	Mouse Clicks Can Recognize Web Page Visitors!	NA:NA:NA	2015
Stefano Cresci:Davide Gazzè:Angelica Lo Duca:Andrea Marchetti:Maurizio Tesconi	In this paper we illustrate the Geo Data Annotator (GDA), a framework which helps a user to build a ground-truth dataset, starting from two separate geographical datasets. GDA exploits two kinds of indices to ease the task of manual annotation: geographical-based and string-based. GDA provides also a mechanism to evaluate the quality of the built ground-truth dataset. This is achieved through a collaborative platform, which allows many users to work to the same project. The quality evaluation is based on annotator agreement, which exploits the Fleiss' kappa statistic.	Geo Data Annotator: a Web Framework for Collaborative Annotation of Geographical Datasets	NA:NA:NA:NA:NA	2015
Soheila Dehghanzadeh:Alessandra Mileo:Daniele Dell'Aglio:Emanuele Della Valle:Shen Gao:Abraham Bernstein	In Web stream processing, there are queries that integrate Web data of various velocity, categorized broadly as streaming (i.e., fast changing) and background (i.e., slow changing) data. The introduction of local views on the background data speeds up the query answering process, but requires maintenance processes to keep the replicated data up-to-date. In this work, we study the problem of maintaining local views in a Web setting, where background data are usually stored remotely, are exposed through services with constraints on the data access (e.g., invocation rate limits and data access patterns) and, contrary to the database setting, do not provide streams with changes over their content. Then, we propose an initial solution: WBM, a method to maintain the content of the view with regards to query and user-defined constraints on accuracy and responsiveness.	Online View Maintenance for Continuous Query Evaluation	NA:NA:NA:NA:NA:NA	2015
Thomas Demeester:Dolf Trieschnigg:Dong Nguyen:Djoerd Hiemstra:Ke Zhou	This paper presents 'FedWeb Greatest Hits', a large new test collection for research in web information retrieval. As a combination and extension of the datasets used in the TREC Federated Web Search Track, this collection opens up new research possibilities on federated web search challenges, as well as on various other problems.	FedWeb Greatest Hits: Presenting the New Test Collection for Federated Web Search	NA:NA:NA:NA:NA	2015
Nemanja Djuric:Jing Zhou:Robin Morris:Mihajlo Grbovic:Vladan Radosavljevic:Narayan Bhamidipati	We address the problem of hate speech detection in online user comments. Hate speech, defined as an "abusive speech targeting specific group characteristics, such as ethnicity, religion, or gender", is an important problem plaguing websites that allow users to leave feedback, having a negative impact on their online business and overall user experience. We propose to learn distributed low-dimensional representations of comments using recently proposed neural language models, that can then be fed as inputs to a classification algorithm. Our approach addresses issues of high-dimensionality and sparsity that impact the current state-of-the-art, resulting in highly efficient and effective hate speech detectors.	Hate Speech Detection with Comment Embeddings	NA:NA:NA:NA:NA:NA	2015
Yuki Endo:Hiroyuki Toda:Yoshimasa Koike	Analyzing emerging topics from social media enables users to overview social movement and several web services to adopt current trends. Although existing studies mainly focus on extracting global emerging topics, efficient extraction of local ones related to a specific theme is still a challenging and unavoidable problem in social media analysis. We focus on extracting emerging social topics related to user-specified query words, and propose an extraction framework that uses a non-negative matrix factorization (NMF) modified for detecting temporal concentration and reducing noises. We conduct preliminary experiments for verifying our method using a Twitter dataset.	What's Hot in The Theme: Query Dependent Emerging Topic Extraction from Social Streams	NA:NA:NA	2015
Weiwei Feng:Peng Wang:Chuan Zhou:Li Guo:Peng Zhang	The distance dependent Chinese Restaurant Processes(dd-CRP), a nonparametric Bayesian model, can model distance sensitive data. Existing inference algorithms for dd-CRP, such as Markov Chain Monte Carlo (MCMC) and variational algorithms, are inefficient and unable to handle massive online data, because posterior distributions of dd-CRP are not marginal invariant. To solve this problem, we present a fast inference algorithm for dd-CRP based on the A-star search. Experimental results show that the new search algorithm is faster than existing dd-CRP inference algorithms with comparable results.	Fast Search for Distance Dependent Chinese Restaurant Processes	NA:NA:NA:NA:NA	2015
Sainyam Galhotra:Akhil Arora:Srinivas Virinchi:Shourya Roy	The steady growth of graph data from social networks has resulted in wide-spread research in finding solutions to the influence maximization problem. Although, TIM is one of the fastest existing algorithms, it cannot be deemed scalable owing to its exorbitantly high memory footprint.cIn this paper, we address the scalability aspect -- memory consumption and running time of the influence maximization problem. We propose ASIM, a scalable algorithm capable of running within practical compute times on commodity hardware. Empirically, ASIM is $6-8$ times faster when compared to CELF++ with similar memory consumption, while its memory footprint is $\approx 200$ times smaller when compared to TIM.	ASIM: A Scalable Algorithm for Influence Maximization under the Independent Cascade Model	NA:NA:NA:NA	2015
Mihajlo Grbovic:Nemanja Djuric:Vladan Radosavljevic:Narayan Bhamidipati	Determining user audience for online ad campaigns is a critical problem to companies competing in online advertising space. One of the most popular strategies is search retargeting, which involves targeting users that issued search queries related to advertiser's core business, commonly specified by advertisers themselves. However, advertisers often fail to include many relevant queries, which results in suboptimal campaigns and negatively impacts revenue for both advertisers and publishers. To address this issue, we use recently proposed neural language models to learn low-dimensional, distributed query embeddings, which can be used to expand query lists with related queries through simple nearest neighbor searches in the embedding space. Experiments on real-world data set strongly suggest benefits of the approach.	Search Retargeting using Directed Query Embeddings	NA:NA:NA:NA	2015
Srishti Gupta:Robert Pienta:Acar Tamersoy:Duen Horng Chau:Rahul C. Basole	Who can spot the next Google, Facebook, or Twitter? Who can discover the next billion-dollar startups? Measuring investor success is a challenging task, as investment strategies can vary widely. We propose InvestorRank, a novel method for identifying successful investors by analyzing how an investor's collaboration network change over time. InvestorRank captures the intuition that a successful investor achieves increasingly success in spotting great startups, or is able to keep doing so persistently. Our results show potential in discovering relatively unknown investors that may be the success stories of tomorrow.	Identifying Successful Investors in the Startup Ecosystem	NA:NA:NA:NA:NA	2015
Hao Han:Takashi Nakayama:Junxia Guo:Keizo Oyama	Like freshness date of food, Web information also has its "shelf life". In this paper, we exploratively study the reflection of shelf life of information in browsing behaviors. Our analysis shows that the satisfaction of browsing behavior is modified if the shelf life of information could be considered by search engines.	Towards Serving "Delicious" Information within Its Freshness Date	NA:NA:NA:NA	2015
Kotaro Hara:Mohammad T. Hajiaghayi:Benjamin B. Bederson	Improvements in image understanding technologies are making it possible for computers to pass traditional CAPTCHA tests with high probability. This suggests the need for new kinds of tasks that are easy to accomplish for humans but remain difficult for computers. In this paper, we introduce Fluency CAPTCHA (FluTCHA), a novel method to distinguish humans from computers using the fact that humans are better than machines at improving the fluency of sentences. We propose a way to let users work on FluTCHA tests and simultaneously complete useful linguistic tasks. Evaluation demonstrates the feasibility of using FluTCHA to distinguish humans from computers.	FluTCHA: Using Fluency to Distinguish Humans from Computers	NA:NA:NA	2015
Xiancai Ji:Zhi Qiao:Mingze Xu:Peng Zhang:Chuan Zhou:Li Guo	With the rapid growth of event-based social networks, the demand of event recommendation becomes increasingly important. While, the existing event recommendation approaches are batch learning fashion. Such approaches are impractical for real-world recommender systems where training data often arrive sequentially. Hence, we present an online event recommendation method. Experimental results on several real-world datasets demonstrate the utility of our method.	Online Event Recommendation for Event-based Social Networks	NA:NA:NA:NA:NA:NA	2015
Jyun-Yu Jiang:Pu-Jen Cheng:Chin-Yew Lin	The hierarchical structure of a knowledge base system can lead to various valuable applications; however, many knowledge base systems do not have such property. In this paper, we propose an entity-driven approach to automatically construct the hierarchical structure of entities for knowledge base systems. By deriving type dependencies from entity information, the initial graph of types will be constructed, and then modified to become a hierarchical structure by several graph algorithms. Experimental results show the effectiveness of our method in terms of constructing reasonable type hierarchy for knowledge base systems.	Entity-driven Type Hierarchy Construction for Freebase	NA:NA:NA	2015
Han-Gyu Ko:Joo-Sik Son:In-Young Ko	Since users often consider more than one aspect when they choose an item, relevant researches introduced multi-criteria recommender systems and showed that multi-criteria ratings add values to the existing CF-based recommender systems to provide more accurate recommendation results to users. However, all the previous works require multi-criteria ratings given by users explicitly while most of the existing datasets such as Netflix and MovieLens are a single criterion. Therefore, to take advantage of multi-criteria recommendation, there must be a way to extract necessary aspects and analyze users' preferences on those aspects from the given single-criterion type of dataset. In this paper, we propose an approach of utilizing semantic information of items to extract essential aspects to perform multi-aspect collaborative filtering to recommend users with items in a personalized manner.	Multi-Aspect Collaborative Filtering based on Linked Data for Personalized Recommendation	NA:NA:NA	2015
Tobias Kötter:Stephan Günnemann:Christos Faloutsos:Michael R. Berthold	Given a large bipartite graph that represents objects and their properties, how can we automatically extract semantic information that provides an overview of the data and -- at the same time -- enables us to drill down to specific parts for an in-depth analysis? In this work in-progress paper, we propose extracting a taxonomy that models the relation between the properties via an is-a hierarchy. The extracted taxonomy arranges the properties from general to specific providing different levels of abstraction.	Extracting Taxonomies from Bipartite Graphs	NA:NA:NA:NA	2015
Ralf Krestel:Thomas Werkmeister:Timur Pratama Wiradarma:Gjergji Kasneci	Twitter has become a prime source for disseminating news and opinions. However, the length of tweets prohibits detailed descriptions; instead, tweets sometimes contain URLs that link to detailed news articles. In this paper, we devise generic techniques for recommending tweets for any given news article. To evaluate and compare the different techniques, we collected tens of thousands of tweets and news articles and conducted a user study on the relevance of recommendations.	Tweet-Recommender: Finding Relevant Tweets for News Articles	NA:NA:NA:NA	2015
Tomasz Kusmierczyk:Christoph Trattner:Kjetil Nørvåg	In this paper, we present work-in-progress of a recently started research effort that aims at understanding the hidden temporal dynamics in online food communities. In this context, we have mined and analyzed temporal patterns in terms of recipe production and consumption in a large German community platform. As our preliminary results reveal, there are indeed a range of hidden temporal patterns in terms of food preferences and in particular in consumption and production. We believe that this kind of research can be important for future work in personalized Web-based information access and in particular recommender systems.	Temporality in Online Food Recipe Consumption and Production	NA:NA:NA	2015
Dahee Lee:Won Chul Kim:Min Song	Automatic information extraction techniques such as named entity recognition and relation extraction have been developed but it is yet rare to apply them to various document types. In this paper, we applied them to academic literature and social media's contents in the field of diabetes to find distinctions between the perceptions of biomedical experts and the public. We analyzed and compared the experts' and the public's networks constituted by the extracted entities and relations. The results confirmed that there are some differences in their views, i.e., biomedical entities that interest them and relations within their knowledge range.	Finding the Differences between the Perceptions of Experts and the Public in the Field of Diabetes	NA:NA:NA	2015
Sang-Chul Lee:Sang-Wook Kim:Sunju Park	In this paper, we propose a set of recommendation strategies and develop a graph-based framework for recommendation in online price-comparison services. We verify the superiority of the proposed framework by comparing it with existing methods using real-world data.	A Graph-Based Recommendation Framework for Price-Comparison Services	NA:NA:NA	2015
Huoran Li:Xuanzhe Liu:Wei Ai:Qiaozhu Mei:Feng Feng	Smartphone users adopt an increasing number of mobile applications (a.k.a., apps) in the recent years. Investigating how people manage mobile apps in their everyday lives creates a unique opportunity to understand the behaviors and preferences of mobile users. Existing literature provides very limited understanding about app management activities, due to the lack of user behavioral data at scale. This paper analyzes a very large collection of app management log of the users of a leading Android app marketplace in China. The data set covers one month of detailed activities of how users download, update, and uninstall the apps on their smart devices, involving 8,306,181 anonymized users and 394,661 apps. We characterize how these users manage the apps on their devices and identify behavioral patterns that correlate with users' online ratings of the apps.	A Descriptive Analysis of a Large-Scale Collection of App Management Activities	NA:NA:NA:NA:NA	2015
Jiguang Liang:Xiaofei Zhou:Li Guo:Shuo Bai	Feature selection is a critical task in both sentiment classification and topical text classification. However, most existing feature selection algorithms ignore a significant contextual difference between them that sentiment classification is commonly depended more on the words conveying sentiments. Based on this observation, a new feature selection method based on matrix factorization is proposed to identify the words with strong inter-sentiment distinguish-ability and intra-sentiment similarity. Furthermore, experiments show that our models require less features while still maintaining reasonable classification accuracy.	Feature Selection for Sentiment Classification Using Matrix Factorization	NA:NA:NA:NA	2015
Ankita Likhyani:Deepak Padmanabhan:Srikanta Bedathur:Sameep Mehta	Predicting the next location of a user based on their previous visiting pattern is one of the primary tasks over data from location based social networks (LBSNs) such as Foursquare. Many different aspects of these so-called "check-in" profiles of a user have been made use of in this task, including spatial and temporal information of check-ins as well as the social network information of the user. Building more sophisticated prediction models by enriching these check-in data by combining them with information from other sources is challenging due to the limited data that these LBSNs expose due to privacy concerns. In this paper, we propose a framework to use the location data from LBSNs, combine it with the data from maps for associating a set of venue categories with these locations. For example, if the user is found to be checking in at a mall that has cafes, cinemas and restaurants according to the map, all these information is associated. This category information is then leveraged to predict the next checkin location by the user. Our experiments with publicly available check-in dataset show that this approach improves on the state-of-the-art methods for location prediction.	Inferring and Exploiting Categories for Next Location Prediction	NA:NA:NA:NA	2015
Zheng Lin:Weiping Wang:Xiaolong Jin:Jiguang Liang:Dan Meng	Automatic opinion lexicon extraction has attracted lots of attention and many methods have thus been proposed. However, most existing methods depend on dictionaries (e.g., WordNet), which confines their applicability. For instance, the dictionary based methods are unable to find domain dependent opinion words, because the entries in a dictionary are usually domain-independent. There also exist corpus-based methods that directly extract opinion lexicons from reviews. However, they heavily rely on sentiment seed words that have limited sentiment information and the context information has not been fully considered. To overcome these problems, this paper presents a word vector and matrix factorization based method for automatically extracting opinion lexicons from reviews of different domains and further identifying the sentiment polarities of the words. Experiments on real datasets demonstrate that the proposed method is effective and performs better than the state-of-the-art methods.	A Word Vector and Matrix Factorization Based Method for Opinion Lexicon Extraction	NA:NA:NA:NA:NA	2015
Haichi Liu:Jintao Tang:Dengping Wei:Peilei Liu:Hong Ning:Ting Wang	Dataset interlinking is a great important problem in Linked Data. We consider this problem from the perspective of information retrieval in this paper, thus propose a learning to rank based framework, which combines various similarity measures to retrieve the relevant datasets for a given dataset. Specifically, inspired by the idea of collaborative filtering, an effective similarity measure called collaborative similarity is proposed. Experimental results show that the collaborative similarity measure is effective for dataset interlinking, and the learning to rank based framework can significantly increase the performance.	Collaborative Datasets Retrieval for Interlinking on Web of Data	NA:NA:NA:NA:NA:NA	2015
Pengqi Liu:Javad Azimi:Ruofei Zhang	Paid Search algorithms play an important role in online advertising where a set of related ads is returned based on a searched query. The Paid Search algorithms mostly consist of two main steps. First, a given searched query is converted to different sub-queries or similar phrases which preserve the core intent of the query. Second, the generated sub-queries are matched to the ads bidded keywords in the data set, and a set of ads with highest utility measuring relevance to the original query are returned. The focus of this paper is optimizing the first step by proposing a contextual query intent extraction algorithm to generate sub-queries online which preserve the intent of the original query the best. Experimental results over a very large real-world data set demonstrate the superb performance of proposed approach in optimizing both relevance and monetization metrics compared with one of the existing successful algorithms in our system.	Contextual Query Intent Extraction for Paid Search Selection	NA:NA:NA	2015
Rishabh Mehrotra:Emine Yilmaz	Current search systems do not provide adequate support for users tackling complex tasks due to which the cognitive burden of keeping track of such tasks is placed on the searcher. As opposed to recent approaches to search task extraction, a more naturalistic viewpoint would involve viewing query logs as hierarchies of tasks with each search task being decomposed into more focussed sub-tasks. In this work, we propose an efficient Bayesian nonparametric model for extracting hierarchies of such tasks & subtasks. The proposed approach makes use of the multi-relational aspect of query associations which are important in identifying query-task associations. We describe a greedy agglomerative model selection algorithm based on the Gamma-Poisson conjugate mixture that take just one pass through the data to learn a fully probabilistic, hierarchical model of trees that is capable of learning trees with arbitrary branching structures as opposed to the more common binary structured trees. We evaluate our method based on real world query log data based on query term prediction. To the best of our knowledge, this work is the first to consider hierarchies of search tasks and subtasks.	Towards Hierarchies of Search Tasks & Subtasks	NA:NA	2015
Cong Men:Wanwan Tang:Po Zhang:Junqi Hou	To better meet users' underlying navigational requirement, search engines like Baidu has developed general recommendation engine and provided related entities on the right side of the search engine results page(SERP). However, users' behavior have not been well investigated after the association of individual queries in search engine. To better understand users' navigational activities, we propose a new method to map users' behavior to an association graph and make graph analysis. Interesting properties like clustering and assortativity are found in this association graph. This study provides a new perspective on research of semantic network and users' navigational behavior on SERP.	On Topology of Baidu's Association Graph Based on General Recommendation Engine and Users' Behavior	NA:NA:NA:NA	2015
Saskia Metzler:Pauli Miettinen	The Resource Description Framework (RDF) represents information as subject--predicate--object triples. These triples are commonly interpreted as a directed labelled graph. We instead interpret the data as a 3-way Boolean tensor. Standard SPARQL queries then can be expressed using elementary Boolean algebra operations. We show how this representation helps to estimate the size of joins. Such estimates are valuable for query handling and our approach might yield more efficient implementations of SPARQL query processors.	Join Size Estimation on Boolean Tensors of RDF Data	NA:NA	2015
Robert Moro:Maria Bielikova	Searching a vast information space such as the Web presents a challenging task and even more so, if the domain is unknown and the character of the task is thus exploratory in its nature. We have proposed a method of exploratory navigation based on navigation leads, i.e. terms that help users to filter the information space of a digital library. In this paper, we focus on the selection of the leads considering their navigational value. We employ clustering based on topic modeling using LDA (Latent Dirichlet Allocation). We present results of a preliminary evaluation on the Annota dataset containing more than 50,000 research papers.	Navigation Leads Selection Considering Navigational Value of Keywords	NA:NA	2015
Fedelucio Narducci:Cataldo Musto:Marco Polignano:Marco de Gemmis:Pasquale Lops:Giovanni Semeraro	In this work we present a semantic recommender system able to suggest doctors and hospitals that best fit a specific patient profile. The recommender system is the core component of the social network named HealthNet (HN). The recommendation algorithm first computes similarities among patients, and then generates a ranked list of doctors and hospitals suitable for a given patient profile, by exploiting health data shared by the community. Accordingly, the HN user can find her most similar patients, look how they cured their diseases, and receive suggestions for solving her problem. Currently, the alpha version of HN is available only for Italian users, but in the next future we want to extend the platform to other languages. We organized three focus groups with patients, practitioners, and health organizations in order to obtain comments and suggestions. All of them proved to be very enthusiastic by using the HN platform.	A Recommender System for Connecting Patients to the Right Doctors in the HealthNet Social Network	NA:NA:NA:NA:NA:NA	2015
Nir Ofek:Lior Rokach:Cornelia Caragea:John Yen	Online health communities are a major source for patients and their informal caregivers in the process of gathering information and seeking social support. The Cancer Survivors Network of the American Cancer Society has many users and presents a large number of user interactions with regards to coping with cancer. Sentiment analysis is an important process in understanding members' needs and concerns and the impact of users' responses on other members. It aims to determine the participants' subjective attitude and reflect their emotions. Analyzing the sentiment of posts in online health communities enables the investigation of various factors such as what affects the sentiment change and discovery of sentiment change patterns. Since each writer has his or her own personality, and temporal emotional state, behavioral traits can be reflected in the writer's writing style. Pronouns are function-words which often convey some unique styling patterns into the texts. Drawing on a lexical approach to emotions, we conduct factor analysis on the use of pronouns in self-descriptions texts. Our analysis shows that the usage of pronouns has an effect on sentiment classification. Moreover, we evaluated the use of pronouns in our domain, and found it different than standard English usage.	The Importance of Pronouns to Sentiment Analysis: Online Cancer Survivor Network Case Study	NA:NA:NA:NA	2015
Vito Claudio Ostuni:Tommaso Di Noia:Eugenio Di Sciascio:Sergio Oramas:Xavier Serra	In this work we describe a hybrid recommendation approach for recommending sounds to users by exploiting and semantically enriching textual information such as tags and sounds descriptions. As a case study we used Freesound, a popular site for sharing sound samples which counts more than 4 million registered users. Tags and textual sound descriptions are exploited to extract and link entities to external ontologies such as WordNet and DBpedia. The enriched data are eventually merged with a domain specific tagging ontology to form a knowledge graph. Based on this latter, recommendations are then computed using a semantic version of the feature combination hybrid approach. An evaluation on historical data shows improvements with respect to state of the art collaborative algorithms.	A Semantic Hybrid Approach for Sound Recommendation	NA:NA:NA:NA:NA	2015
Jun Pang:Yang Zhang	Humans are social animals, they interact with different communities to conduct different activities. The literature has shown that human mobility is constrained by their social relations. In this work, we investigate the social impact on a user's mobility from his communities in order to conduct location prediction effectively. Through analysis of a real-life dataset, we demonstrate that (1) a user gets more influences from his communities than from all his friends; (2) his mobility is influenced only by a small subset of his communities; (3) influence from communities depends on social contexts. We further exploit a SVM to predict a user's future location based on his community information. Experimental results show that the model based on communities leads to more effective predictions than the one based on friends.	Exploring Communities for Effective Location Prediction	NA:NA	2015
Christos Perentis:Michele Vescovi:Bruno Lepri	Mobile devices, sensors and social networks have dramatically increased the collection and sharing of personal and contextual information of individuals. Hence, users constantly make disclosure decisions on the basis of a difficult trade-off between using services and data protection. Understanding the factors linked to the disclosure behavior of personal information is a step forward to assist users in their decisions. In this paper, we model the disclosure of personal information and investigate their relationships not only with demographic and self-reported individual characteristics, but also with real behavior inferred from mobile phone usage. Preliminary results show that real behavior captured from mobile data relates with actual sharing behavior, providing the basis for future predictive models.	Investigating Factors Affecting Personal Data Disclosure	NA:NA:NA	2015
Bryan Perozzi:Steven Skiena	Predicting accurate demographic information about the users of information systems is a problem of interest in personalized search, ad targeting, and other related fields. Despite such broad applications, most existing work only considers age prediction as one of classification, typically into only a few broad categories. Here, we consider the problem of exact age prediction in social networks as one of regression. Our proposed method learns social representations which capture community information for use as covariates. In our preliminary experiments on a large real-world social network, it can predict age within 4.15 years on average, strongly outperforming standard network regression techniques when labeled data is sparse.	Exact Age Prediction in Social Networks	NA:NA	2015
Natalia Prytkova:Gerhard Weikum:Marc Spaniol	Large collections of digital knowledge have become valuable assets for search and recommendation applications. The taxonomic type systems of such knowledge bases are often highly heterogeneous, as they reflect different cultures, languages, and intentions of usage. We present a novel method to the problem of multi-cultural knowledge alignment, which maps each node of a source taxonomy onto a ranked list of most suitable nodes in the target taxonomy. We model this task as combinatorial optimization problems, using integer linear programming and quadratic programming. The quality of the computed alignments is evaluated, using large heterogeneous taxonomies about book categories.	Aligning Multi-Cultural Knowledge Taxonomies by Combinatorial Optimization	NA:NA:NA	2015
Shuang Qiu:Jian Cheng:Xi Zhang:Hanqing Lu	To address the recommendation problems in the scenarios of multiple domains, in this paper, we propose a novel method, HMRec, which models both consistency and heterogeneity of users' multiple behaviors in a unified framework. Moreover, the decisive factors of each domain can also be captured by our approach successfully. Experiments on the real multi-domain dataset demonstrate the effectiveness of our model.	Exploring Heterogeneity for Multi-Domain Recommendation with Decisive Factors Selection	NA:NA:NA:NA	2015
Dimitrios Rafailidis:Alexandros Nanopoulos	We propose a new link-injection method aiming at boosting the overall diffusion of information in social networks. Our approach is based on a diffusion-coverage score of the ability of each user to spread information over the network. Candidate links for injection are identified by a matrix factorization technique and link injection is performed by attaching links to users according to their score. We additionally perform clustering to identify communities in order to inject links that cross the boundaries of such communities. In our experiments with five real world networks, we demonstrate that our method can significantly spread the information diffusion by performing limited link injection, essential to real-world applications	Crossing the Boundaries of Communities via Limited Link Injection for Information Diffusion In Social Networks	NA:NA	2015
Dimitrios Rafailidis:Alexandros Nanopoulos	We present a Coupled Tensor Factorization model to recommend items with repeat consumption over time. We introduce a measure that captures the rate with which the preferences of each user shift over time. Repeat consumption recommendations are generated based on factorizing the coupled tensor, by weighting the importance of past user preferences according to the captured rate. We also propose a variant, where the diversity of the side information is taken into account, by higher weighting users that have more rare side information. Our experiments with real-world datasets from last.fm and MovieLens demonstrate that the proposed models outperform several baselines.	Repeat Consumption Recommendation Based on Users Preference Dynamics and Side Information	NA:NA	2015
Maria-Evgenia G. Rossi:Fragkiskos D. Malliaros:Michalis Vazirgiannis	Understanding and controlling spreading dynamics in networks presupposes the identification of those influential nodes that will trigger an efficient information diffusion. It has been shown that the best spreaders are the ones located in the core of the network - as produced by the k-core decomposition. In this paper we further refine the set of the most influential nodes, showing that the nodes belonging to the best K-truss subgraph, as identified by the K-truss decomposition of the network, perform even better leading to faster and wider epidemic spreading.	Spread it Good, Spread it Fast: Identification of Influential Nodes in Social Networks	NA:NA:NA	2015
Rishiraj Saha Roy:Ritwik Sinha:Niyati Chhaya:Shiv Saini	Cookies and log in-based authentication often provide incomplete data for stitching website visitors across multiple sources, necessitating probabilistic deduplication. We address this challenge by formulating the problem as a binary classification task for pairs of anonymous visitors. We compute visitor proximity vectors by converting categorical variables like IP addresses, product search keywords and URLs with very high cardinalities to continuous numeric variables using the Jaccard coefficient for each attribute. Our method achieves about 90% AUC and F-scores in identifying whether two cookies map to the same visitor, while providing insights on the relative importance of available features in Web analytics towards the deduplication process.	Probabilistic Deduplication of Anonymous Web Traffic	NA:NA:NA:NA	2015
Tzanina Saveta:Evangelia Daskalaki:Giorgos Flouris:Irini Fundulaki:Melanie Herschel:Axel-Cyrille Ngonga Ngomo	The architectural choices behind the Data Web have led to the publication of large interrelated data sets that contain different descriptions for the same real-world objects. Due to the mere size of current online datasets, such duplicate instances are most commonly detected (semi-)automatically using instance matching frameworks. Choosing the right framework for this purpose remains tedious, as current instance matching benchmarks fail to provide end users and developers with the necessary insights pertaining to how current frameworks behave when dealing with real data. In this poster, we present the Semantic Publishing Instance Matching Benchmark (SPIMBENCH) which allows the benchmarking of instance matching systems against not only structure-based and value-based test cases, but also against semantics-aware test cases based on OWL axioms. SPIMBENCH features a scalable data generator and a weighted gold standard that can be used for debugging instance matching systems and for reporting how well they perform in various matching tasks.	Pushing the Limits of Instance Matching Systems: A Semantics-Aware Benchmark for Linked Data	NA:NA:NA:NA:NA:NA	2015
Fethi Burak Sazoglu:Özgür Ulusoy:Ismail Sengor Altingovde:Rifat Ozcan:Berkant Barla Cambazoglu	Detecting stale queries in a search engine result cache is an important problem. In this work, we propose a mechanism that propagates the expiration decision for a query to similar queries in the cache to re-adjust their time-to-live values.	Propagating Expiration Decisions in a Search Engine Result Cache	NA:NA:NA:NA:NA	2015
Kim Schouten:Nienke de Boer:Tjian Lam:Marijtje van Leeuwen:Ruud van Luijk:Flavius Frasincar	With consumer reviews becoming a mainstream part of e-commerce, a good method of detecting the product or service aspects that are discussed is desirable. This work focuses on detecting aspects that are not literally mentioned in the text, or implicit aspects. To this end, a co-occurrence matrix of synsets from WordNet and implicit aspects is constructed. The semantic relations that exist between synsets in WordNet are exploited to enrich the co-occurrence matrix with more contextual information. Comparing this method with a similar method which is not semantics-driven clearly shows the benefit of the proposed method. Especially corpora of limited size seem to benefit from the added semantic context.	Semantics-Driven Implicit Aspect Detection in Consumer Reviews	NA:NA:NA:NA:NA:NA	2015
Suvash Sedhain:Aditya Krishna Menon:Scott Sanner:Lexing Xie	This paper proposes AutoRec, a novel autoencoder framework for collaborative filtering (CF). Empirically, AutoRec's compact and efficiently trainable model outperforms state-of-the-art CF techniques (biased matrix factorization, RBM-CF and LLORMA) on the Movielens and Netflix datasets.	AutoRec: Autoencoders Meet Collaborative Filtering	NA:NA:NA:NA	2015
Dominic Seyler:Mohamed Yahya:Klaus Berberich	We propose an approach to generate natural language questions from knowledge graphs such as DBpedia and YAGO. We stage this in the setting of a quiz game. Our approach, though, is general enough to be applicable in other settings. Given a topic of interest (e.g., Soccer) and a difficulty (e.g., hard), our approach selects a query answer, generates a SPARQL query having the answer as its sole result, before verbalizing the question.	Generating Quiz Questions from Knowledge Graphs	NA:NA:NA	2015
Sanket S. Sharma:Munmun De Choudhury	Social media sites like Instagram have emerged as popular platforms for sharing ingestion and dining experiences. However research on characterizing the nutritional information embedded in such content is limited. In this paper, we develop a computational method to extract nutritional information, specifically calorific content from Instagram food posts. Next, we explore how the community reacts specifically to healthy versus non-healthy food postings. Based on a crowdsourced approach, our method was found to detect calorific content in posts with 89% accuracy. We further show the use of Instagram as a platform where sharing of moderately healthy food content is common, and such content also receives the most support from the community.	Measuring and Characterizing Nutritional Information of Food and Ingestion Content in Instagram	NA:NA	2015
Lisa Singh:Hui Yang:Micah Sherr:Yifang Wei:Andrew Hian-Cheong:Kevin Tian:Janet Zhu:Sicong Zhang:Tavish Vaidya:Elchin Asgarli	To help users better understand the potential risks associated with publishing data publicly, and the types of data that can be inferred by combining data from multiple online sources, we introduce a novel information exposure detection framework that generates and analyzes the web footprints users leave across the social web. We propose to use probabilistic operators, free text attribute extraction, and a population-based inference engine to generate the web footprints. Evaluation over public profiles from multiple sites shows that our framework successfully detects and quantifies information exposure using a small amount of non-sensitive initial knowledge.	Helping Users Understand Their Web Footprints	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2015
Shuangyong Song:Yao Meng	In this paper, we propose a Concept-level Emotion Cause Model (CECM), instead of the mere word-level models, to discover causes of microblogging users' diversified emotions on specific hot event. A modified topic-supervised biterm topic model is utilized in CECM to detect "emotion topics" in event-related tweets, and then context-sensitive topical PageRank is utilized to detect meaningful multiword expressions as emotion causes. Experimental results on a dataset from Sina Weibo, one of the largest microblogging websites in China, show CECM can better detect emotion causes than baseline methods.	Detecting Concept-level Emotion Cause in Microblogging	NA:NA	2015
Lucas Sterckx:Thomas Demeester:Johannes Deleu:Chris Develder	We propose an improvement on a state-of-the-art keyphrase extraction algorithm, Topical PageRank (TPR), incorporating topical information from topic models. While the original algorithm requires a random walk for each topic in the topic model being used, ours is independent of the topic model, computing but a single PageRank for each text regardless of the amount of topics in the model. This increases the speed drastically and enables it for use on large collections of text using vast topic models, while not altering performance of the original algorithm.	Topical Word Importance for Fast Keyphrase Extraction	NA:NA:NA:NA	2015
Lucas Sterckx:Thomas Demeester:Johannes Deleu:Chris Develder	We explore how the unsupervised extraction of topic-related keywords benefits from combining multiple topic models. We show that averaging multiple topic models, inferred from different corpora, leads to more accurate keyphrases than when using a single topic model and other state-of-the-art techniques. The experiments confirm the intuitive idea that a prerequisite for the significant benefit of combining multiple models is that the models should be sufficiently different, i.e., they should provide distinct contexts in terms of topical word importance.	When Topic Models Disagree: Keyphrase Extraction with Multiple Topic Models	NA:NA:NA:NA	2015
Yukihiro Tagami:Hayato Kobayashi:Shingo Ono:Akira Tajima	Modeling user activities on the Web is a key problem for various Web services, such as news article recommendation and ad click prediction. In this paper, we propose an approach that summarizes each sequence of user activities using the Paragraph Vector, considering users and activities as paragraphs and words, respectively. The learned user representations are used among the user-related prediction tasks in common. We evaluate this approach on two data sets based on logs from Web services of Yahoo! JAPAN. Experimental results demonstrate the effectiveness of our proposed methods.	Modeling User Activities on the Web using Paragraph Vector	NA:NA:NA:NA	2015
Niket Tandon:Gerhard Weikum:Gerard de Melo:Abir De	With the success of large knowledge graphs, research on automatically acquiring commonsense knowledge is revived. One kind of knowledge that has not received attention is that of human activities. This paper presents an information extraction pipeline for systematically distilling activity knowledge from a corpus of movie scripts. Our semantic frames capture activities together with their participating agents and their typical spatial, temporal and sequential contexts. The resulting knowledge base comprises about 250,000 activities with links to specific movie scenes where they occur.	Lights, Camera, Action: Knowledge Extraction from Movie Scripts	NA:NA:NA:NA	2015
Thomas Theodoridis:Symeon Papadopoulos:Yiannis Kompatsiaris	User profiling is an essential component of most modern online services offered upon user registration. Profiling typically involves the tracking and processing of users' online traces (e.g., page views/clicks) with the goal of inferring attributes of interest for them. The primary motivation behind profiling is to improve the effectiveness of advertising by targeting users with appropriately selected ads based on their profile attributes, e.g., interests, demographics, etc. Yet, there has been an increasing number of cases, where the advertising content users are exposed to is either irrelevant or not possible to explain based on their online activities. More disturbingly, automatically inferred user attributes are often used to make real-world decisions (e.g., job candidate selection) without the knowledge of users. We argue that many of these errors are inherent in the underlying user profiling process. To this end, we attempt to quantify the extent of such errors, focusing on a dataset of Facebook users and their likes, and conclude that profiling-based targeting is highly unreliable for a sizeable subset of users.	Assessing the Reliability of Facebook User Profiling	NA:NA:NA	2015
Thanh Tien Vu:Alistair Willis:Dawei Song	Recent research has shown that mining and modelling search tasks helps improve the performance of search personalisation. Some approaches have been proposed to model a search task using topics discussed in relevant documents, where the topics are usually obtained from human-generated online ontology such as Open Directory Project. A limitation of these approaches is that many documents may not contain the topics covered in the ontology. Moreover, the previous studies largely ignored the dynamic nature of the search task; with the change of time, the search intent and user interests may also change. This paper addresses these problems by modelling search tasks with time-awareness using latent topics, which are automatically extracted from the task's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation). In the experiments, we utilise the time-aware search task to re-rank result list returned by a commercial search engine and demonstrate a significant improvement in the ranking quality.	Modelling Time-aware Search Tasks for Search Personalisation	NA:NA:NA	2015
Pengyuan Wang:Dawei Yin:Jian Yang:Yi Chang:Marsha Meytlis	In online advertising, one of the central questions of ad campaign assessment is whether the ad truly adds values to the advertisers. To measure the incremental effect of ads, the ratio of the success rates of the users who were and who were not exposed to ads is usually calculated to represent ad effectiveness. Many existing campaigns simply target the users with high predicted success (e.g. purchases, searches) rate, which often neglect the fact that even without ad exposure, the targeted group of users might still perform the success actions, and hence show higher ratio than the true ad effectiveness. We call such phenomena `smart cheating'. Failure to discount smart cheating when assessing ad campaigns may favor the targeting plan that cheats hard, but such targeting does not lead to the maximal incremental success actions and results in wasted budget. In this paper we define and quantify smart cheating with a smart cheating ratio (SCR) through causal inference. We apply our approach to multiple real ad campaigns, and find that smart cheating exists extensively and can be rather severe in current advertising industry.	Rethink Targeting: Detect 'Smart Cheating' in Online Advertising through Causal Inference	NA:NA:NA:NA:NA	2015
Ryen W. White:Matthew Richardson:Wen-tau Yih	Search systems traditionally require searchers to formulate information needs as keywords rather than in a more natural form, such as questions. Recent studies have found that Web search engines are observing an increase in the fraction of queries phrased as natural language. As part of building better search engines, it is important to understand the nature and prevalence of these intentions, and the impact of this increase on search engine performance. In this work, we show that while 10.3% of queries issued to a search engine have direct question intent, only 3.2% of them are formulated as natural language questions. We investigate whether search engines perform better when search intent is stated as queries or questions, and we find that they perform equally well to both	Questions vs. Queries in Informational Search Tasks	NA:NA:NA	2015
Yuto Yamaguchi:Mitsuo Yoshida:Christos Faloutsos:Hiroyuki Kitagawa	Why does Smith follow Johnson on Twitter? In most cases, the reason why users follow other users is unavailable. In this work, we answer this question by proposing TagF, which analyzes the who-follows-whom network (matrix) and the who-tags-whom network (tensor) simultaneously. Concretely, our method decomposes a coupled tensor constructed from these matrix and tensor. The experimental results on million-scale Twitter networks show that TagF uncovers different, but explainable reasons why users follow other users.	Why Do You Follow Him?: Multilinear Analysis on Twitter	NA:NA:NA:NA	2015
Qipeng Yao:Ruisheng Shi:Chuan Zhou:Peng Wang:Li Guo	In this paper, we address the problem of minimizing the negative influence of undesirable things in a network by blocking a limited number of nodes from a topic modeling perspective. When undesirable thing such as a rumor or an infection emerges in a social network and part of users have already been infected, our goal is to minimize the size of ultimately infected users by blocking $k$ nodes outside the infected set. We first employ the HDP-LDA and KL divergence to analysis the influence and relevance from a topic modeling perspective. Then two topic-aware heuristics based on betweenness and out-degree for finding approximate solutions to this problem are proposed. Using two real networks, we demonstrate experimentally the high performance of the proposed models and learning schemes.	Topic-aware Social Influence Minimization	NA:NA:NA:NA:NA	2015
Wenyu Zang:Chuan Zhou:Li Guo:Peng Zhang	In this paper we address the problem of source locating in social networks from a topic modeling perspective. From the observation that the topic factor can help infer the propagation paths, we propose a topic-aware source locating method based on topic analysis of propagation items and participants. We evaluate our algorithm on both generated and real-world datasets. The experimental results show significant improvement over existing popular methods.	Topic-aware Source Locating in Social Networks	NA:NA:NA:NA	2015
Lei Zhang:Yunpeng Dong:Achim Rettinger	Linking words or phrases in unstructured text to entities in knowledge bases is the problem of entity recognition and disambiguation. In this paper, we focus on the task of entity recognition in Web text to address the challenges of entity correctness, completeness and emergence that existing approaches mainly suffer from. Experimental results show that our approach significantly outperforms the state-of-the-art approaches in terms of precision, F-measure, micro-accuracy and macro-accuracy, while still preserving high recall.	Towards Entity Correctness, Completeness and Emergence for Entity Recognition	NA:NA:NA	2015
Lu Zhou:Wenbo Wang:Keke Chen	Inappropriate tweets may cause severe damages on the authors' reputation or privacy. However, many users do not realize the potential damages when publishing such tweets. Published tweets have lasting effects that may not be completely eliminated by simple deletion, because other users may have read them or third-party tweet analysis platforms have cached them. In this paper, we study the problem of identifying regrettable tweets from normal individual users, with the ultimate goal of reducing the occurrences of regrettable tweets. We explore the contents of a set of tweets deleted by sample normal users to understand the regrettable tweets. With a set of features describing the identifiable reasons, we can develop classifiers to effectively distinguish such regrettable tweets from normal tweets.	Identifying Regrettable Messages from Tweets	NA:NA:NA	2015
Nishanth Sastry:Wagner Meira, Jr.	NA	Session details: Demonstrations	NA:NA	2015
Nitish Aggarwal:Kartik Asooja:Housam Ziad:Paul Buitelaar	In this demo, we present Entity Relatedness Graph (EnRG), a focused related entities explorer, which provides the users with a dynamic set of filters and facets. It gives a ranked lists of related entities to a given entity, and clusters them using the different filters. For instance, using EnRG, one can easily find the American vegans related to Brad Pitt or Irish universities related to Semantic Web. Moreover, EnRG helps a user in discovering the provenance for implicit relations between two entities. EnRG uses distributional semantics to obtain the relatedness scores between two entities.	Who are the American Vegans related to Brad Pitt?: Exploring Related Entities	NA:NA:NA:NA	2015
Julián Alarte:David Insa:Josep Silva:Salvador Tamarit	This paper presents and describes TeMex, a site-level web template extractor. TeMex is fully automatic, and it can work with online webpages without any preprocessing stage (no information about the template or the associated webpages is needed) and, more importantly, it does not need a predefined set of webpages to perform the analysis. TeMex only needs a URL. Contrarily to previous approaches, it includes a mechanism to identify webpage candidates that share the same template. This mechanism increases both recall and precision, and it also reduces the amount of webpages loaded and processed. We describe the tool and its internal architecture, and we present the results of its empirical evaluation.	TeMex: The Web Template Extractor	NA:NA:NA:NA	2015
Ahmad Assaf:Aline Senart:Raphaël Troncy	Data is being published by both the public and private sectors and covers a diverse set of domains ranging from life sciences to media or government data. An example is the Linked Open Data (LOD) cloud which is potentially a gold mine for organizations and individuals who are trying to leverage external data sources in order to produce more informed business decisions. Considering the significant variation in size, the languages used and the freshness of the data, one realizes that spotting spam datasets or simply finding useful datasets without prior knowledge is increasingly complicated. In this paper, we propose Roomba, a scalable automatic approach for extracting, validating, correcting and generating descriptive linked dataset profiles. While Roomba is generic, we target CKAN-based data portals and we validate our approach against a set of open data portals including the Linked Open Data (LOD) cloud as viewed on the DataHub. The results demonstrate that the general state of various datasets and groups, including the LOD cloud group, needs more attention as most of the datasets suffer from bad quality metadata and lack some informative metrics that are required to facilitate dataset search.	Roomba: Automatic Validation, Correction and Generation of Dataset Metadata	NA:NA:NA	2015
Shobana Balakrishnan:Surajit Chaudhuri:Vivek Narasayya	Personal photo collections are large and growing rapidly. Today, it is difficult to search such a photo collection for people who occur in them since it is tedious to manually associate face tags in photos. The key idea is to learn face models for friends and family of the user using tagged photos in a social graph such as Facebook as training examples. These face models are then used to automatically tag photos in the collection, thereby making it more searchable and easier to organize. To illustrate this idea we have developed a Windows app called AutoTag 'n Search My Photos. In this demo paper we describe the architecture, user interaction and controls, and our initial learnings from deploying the app.	AutoTag 'n Search My Photos: Leveraging the Social Graph for Photo Tagging	NA:NA:NA	2015
Alessio Bellino	Online services such as Google Maps or Open Street Maps allow the exploration of maps on smartphones and tablets. The gestures used are the pinch to adjust the zoom level and the drag to move the map. In this paper, two new gestures to adjust the zoom level of maps are presented. Both gestures with slight differences allow the identification of a target area to zoom, which is enlarged automatically up to cover the whole map container. The proposed gestures are added to the traditional ones (drag, pinch and flick) without any overlap. Therefore, users do not need to change their regular practices. They have just two more options to control the zoom level. One of the most relevant and appreciated advantages has to do with the gesture for smartphones (Tap&Tap): this allows users to control the zoom level with just one hand. The traditional pinch gesture, instead, needs two hands. According to the test results on new gestures in comparison with the traditional pinch, 30% of time is saved on tablets (Two-Finger-Tap gesture) whereas 14% on smartphones (Tap&Tap gesture).	Two New Gestures to Zoom: Enhancing Online Maps Services	NA	2015
Carlo Bernaschina:Ilio Catallo:Piero Fraternali:Davide Martinenghi:Marco Tagliasacchi	We present Champagne, a web tool for the execution of crowdsourcing campaigns. Through Champagne, task requesters can model crowdsourcing campaigns as a sequence of choices regarding different, independent crowdsourcing design decisions. Such decisions include, e.g., the possibility of qualifying some workers as expert reviewers, or of combining different quality assurance techniques to be used during campaign execution. In this regard, a walkthrough example showcasing the capabilities of the platform is reported. Moreover, we show that our modular approach in the design of campaigns overcomes many of the limitations exposed by the major platforms available in the market.	Champagne: A Web Tool for the Execution of Crowdsourcing Campaigns	NA:NA:NA:NA:NA	2015
Stefano Bocconi:Alessandro Bozzon:Achilleas Psyllidis:Christiaan Titos Bolivar:Geert-Jan Houben	This demo presents Social Glass, a novel web-based platform that supports the analysis, valorisation, integration, and visualisation of large-scale and heterogeneous urban data in the domains of city planning and decision-making. The platform systematically combines publicly available social datasets from municipalities together with social media streams (e.g. Twitter, Instagram and Foursquare) and resources from knowledge repositories. It further enables the mapping of demographic information, human movement patterns, place popularity, traffic conditions, as well as citizens' and visitors' opinions and preferences with regard to specific venues in the city. Social Glass will be demonstrated through several real-world case studies, that exemplify the framework's conceptual properties, and its potential value as a solution for urban analytics and city-scale event monitoring and assessment.	Social Glass: A Platform for Urban Analytics and Decision-making Through Heterogeneous Social Data	NA:NA:NA:NA:NA	2015
Sarah Chasins:Shaon Barman:Rastislav Bodik:Sumit Gulwani	To build a programming by demonstration (PBD) web scraping tool for end users, one needs two central components: a list finder, and a record and replay tool. A list finder extracts logical tables from a webpage. A record and replay (R+R) system records a user's interactions with a webpage, and replays them programmatically. The research community has invested substantial work in list finding --- variously called wrapper induction, structured data extraction, and template detection. In contrast, researchers largely considered the browser R+R problem solved until recently, when webpage complexity and interactivity began to rise. We argue that the increase in interactivity necessitates the use of new, more robust R+R approaches, which will facilitate the PBD web tools of the future. Because robust R+R is difficult to build and understand, we argue that tool developers need an R+R layer that they can treat as a black box. We have designed an easy-to-use API that allows programmers to use and even customize R+R, without having to understand R+R internals. We have instantiated our API in Ringer, our robust R+R tool. We use the API to implement WebCombine, a PBD scraping tool. A WebCombine user demonstrates how to collect the first row of a relational dataset, and the tool collects all remaining rows. WebCombine uses the Ringer API to handle navigation between pages, enabling users to scrape from modern, interaction-heavy pages. We demonstrate WebCombine by collecting a 3,787,146 row dataset from Google Scholar that allows us to explore the relationship between researchers' years of experience and their papers' citation counts.	Browser Record and Replay as a Building Block for End-User Web Automation Tools	NA:NA:NA:NA	2015
Zhe Chen:Michael Cafarella:Eytan Adar	A large amount of data is available only through data-driven diagrams such as bar charts and scatterplots. These diagrams are stylized mixtures of graphics and text and are the result of complicated data-centric production pipelines. Unfortunately, neither text nor image search engines exploit these diagram-specific properties, making it difficult for users to find relevant diagrams in a large corpus. In response, we propose DiagramFlyer, a search engine for finding data-driven diagrams on the web. By recovering the semantic roles of diagram components (e.g., axes, labels, etc.), we provide faceted indexing and retrieval for various statistical diagrams. A unique feature of DiagramFlyer is that it is able to "expand" queries to include not only exactly matching diagrams, but also diagrams that are likely to be related in terms of their production pipelines. We demonstrate the resulting search system by indexing over 300k images pulled from over 150k PDF documents.	DiagramFlyer: A Search Engine for Data-Driven Diagrams	NA:NA:NA	2015
Jaehoon Choi:Donghyeon Kim:Donghee Choi:Sangrak Lim:Seongsoon Kim:Jaewoo Kang:Youngjae Choi	Search engines have become an important decision-making tool today. Unfortunately, they still need to improve in answering complex queries. The answers to complex decision-making queries such as ``best burgers and fries'' and ``good restaurants for anniversary dinner,'' are often subjective. The most relevant answer to the query can be obtained by only collecting people's opinions about the query, which are expressed in various venues on the Web. Collected opinions are converted into a ``consensus'' list. All of this should be processed at query time, which is impossible under the current search paradigm. To address this problem, we introduce Smith, a novel opinion-based restaurant search engine. Smith actively processes opinions on the Web, blogs, review boards, and other forms of social media at index time, and produces consensus answers from opinions at query time. The Smith search app (iOS) is available for download at http://www.smithsearches.com/introduction/.	Smith Search: Opinion-Based Restaurant Search Engine	NA:NA:NA:NA:NA:NA:NA	2015
Fabian Flöck:Maribel Acosta	The visualization of editor interaction dynamics and provenance of content in revisioned, collaboratively written documents has the potential to allow for more transparency and intuitive understanding of the intricate mechanisms inherent to collective content production. Although approaches exist to build editor interactions from individual word changes in Wikipedia articles, they do not allow to inquire into individual interactions, and have yet to be implemented as usable end-user tools. We thus present whoVIS, a web tool to mine and visualize editor interactions in Wikipedia over time. whoVIS integrates novel features with existing methods, tailoring them to the use case of understanding intra-article disagreement between editors. Using real Wikipedia examples, our system demonstrates the combination of various visualization techniques to identify different social dynamics and explore the evolution of an article that would be particularly hard for end-users to investigate otherwise.	whoVIS: Visualizing Editor Interactions and Dynamics in Collaborative Writing Over Time	NA:NA	2015
Bahar Ghadiri Bashardoost:Christina Christodoulakis:Soheil Hassas Yeganeh:Renée J. Miller:Kelly Lyons:Oktie Hassanzadeh	Vizcurator permits the exploration, understanding and curation of open RDF data, its schema, and how it has been linked to other sources. We provide visualizations that enable one to seamlessly navigate through RDFS and RDF layers and quickly understand the open data, how it has been mapped or linked, how it has been structured (and could be restructured), and how deeply it has been related to other open data sources. More importantly, Vizcurator provides a rich set of tools for data curation. It suggests possible improvements to the structure of the data and enables curators to make informed decisions about enhancements to the exploration and exploitation of the data. Moreover, Vizcurator facilitates the mining of temporal resources and the definition of temporal constraints through which the curator can identify conflicting facts. Finally, Vizcurator can be used to create new binary temporal relations by reifying base facts and linking them to temporal resources. We will demonstrate Vizcurator using LinkedCT.org, a five-star open data set mapped from the XML NIH clinical trials data (clinicaltrials.gov) that we have been maintaining and curating for several years.	VizCurator: A Visual Tool for Curating Open Data	NA:NA:NA:NA:NA:NA	2015
Mihajlo Grbovic:Nemanja Djuric:Vladan Radosavljevic:Narayan Bhamidipati:Jordan Hawker:Caleb Johnson	Understanding interests expressed through user's search query is a task of critical importance for many internet applications. To help identify user interests, web engines commonly utilize classification of queries into one or more pre-defined interest categories. However, majority of the queries are noisy short texts, making accurate classification a challenging task. In this demonstration, we present queryCategorizr, a novel semi-supervised learning system that embeds queries into low-dimensional vector space using a neural language model applied on search log sessions, and classifies them into general interest categories while relying on a small set of labeled queries. Empirical results on large-scale data show that queryCategorizr outperforms the current state-of-the-art approaches. In addition, we describe a Graphical User Interface (GUI) that allows users to query the system and explore classification results in an interactive manner.	queryCategorizr: A Large-Scale Semi-Supervised System for Categorization of Web Search Queries	NA:NA:NA:NA:NA:NA	2015
Ziad Ismail:Danai Symeonidou:Fabian Suchanek	Internet users typically have several online accounts - such as mail accounts, cloud storage accounts, or social media accounts. The security of these accounts is often intricately linked: The password of one account can be reset by sending an email to another account; the data of one account can be backed up on another account; one account can only be accessed by two-factor authentication through a second account; and so forth. This poses three challenges: First, if a user loses one or several of his passwords, can he still access his data? Second, how many passwords does an attacker need in order to access the data? And finally, how many passwords does an attacker need in order to irreversibly delete the user's data? In this paper, we model the dependencies of online accounts in order to help the user discover security weaknesses. We have implemented our system and invite users to try it out on their real accounts.	DIVINA: Discovering Vulnerabilities of Internet Accounts	NA:NA:NA	2015
Michael Krug:Martin Gaedke	In this paper, we introduce the usage of enhanced Web Components to create web applications with multi-device capabilities by composition. By using the latest developments of the family of W3C standards called "Web Components" that we extent with dedicated communication and synchronization functionality, web developers are enabled to create web applications with ease. We enhance Web Components with an event-based communication channel, which is not limited to a single browser window. With our approach, applications using the extended SmartComponents and an additional synchronization service also support multi-device scenarios. In contrast to other widget-based approaches (W3C Widgets, OpenSocial containers), the usage of SmartComponents does not require a dedicated platform, like Apache Rave. SmartComponents are based on standard web technologies, are natively supported by recent web browsers and loosely coupled using our extension. This ensures a high level of reuse. We show how SmartComponents are structured, can be created and used. Furthermore, we explain how the communication aspect is integrated and multi-device communication is achieved. Finally, we describe our demonstration by outlining two example applications.	SmartComposition: Enhanced Web Components for a Better Future of Web Development	NA:NA	2015
Xiang Li:Zhiyong Feng:Keman Huang:Shizhan Chen	Web developers often use Ajax API to build the rich Internet application (RIA). Due to the uncertainty of the environment, automatically switching among different Ajax APIs with similar functionality is important to guarantee the end-to-end performance. However, it is challenging and time-consuming because it needs to manually modify codes based on the API documentation. In this paper, we propose a framework to address the self-adaption and difficulty in invoking Ajax API. The Ajax API wrapping model, consisting of the specific and abstract components, is proposed to automatically construct the grammatical and functional semantic relations between Ajax APIs. Then switching module is introduced to support the automatic switching among different Ajax APIs, according to the user preference and QoS of Ajax APIs. Taking the map APIs, i.e. Google Map, Baidu Map, Gaode Map, 51 Map and Tencent Map as an example, the demo shows that the framework can facilitate the construction of RIA and improve adaptability of the application. The process of selection and switching in the different Ajax APIs is automatic and transparent to the users.	Ajax API Self-adaptive Framework for End-to-end User	NA:NA:NA:NA	2015
Xiaotong Liu:Srinivasan Parthasarathy:Han-Wei Shen:Yifan Hu	The ever-increasing size and complexity of social networks place a fundamental challenge to visual exploration and analysis tasks. In this paper, we present \textit{GalaxyExplorer}, an influence-driven visual analysis system for exploring users of various influence and analyzing how they influence others in a social network. GalaxyExplorer reduces the size and complexity of a social network by dynamically retrieving theme-based graphs, and analyzing users' influence and passivity regarding specific themes and dynamics in response to disaster events. In GalaxyExplorer, a galaxy-based visual metaphor is introduced to simplify the visual complexity of a large graph with a focus+context view. Various interactions are supported for visual exploration. We present experimental results on real-world datasets that show the effectiveness of GalaxyExplorer in theme-aware influence analysis.	GalaxyExplorer: Influence-Driven Visual Exploration of Context-Specific Social Media Interactions	NA:NA:NA:NA	2015
Michael Martin:Konrad Abicht:Claus Stadler:Axel-Cyrille Ngonga Ngomo:Tommaso Soru:Sören Auer	CubeViz is a flexible exploration and visualization platform for statistical data represented adhering to the RDF Data Cube vocabulary. If statistical data is provided adhering to the Data Cube vocabulary, CubeViz exhibits a faceted browsing widget allowing to interactively filter observations to be visualized in charts. Based on the selected structural part, CubeViz offers suitable chart types and options for configuring the visualization by users. In this demo we present the CubeViz visualization architecture and components, sketch its underlying API and the libraries used to generate the desired output. By employing advanced introspection, analysis and visualization bootstrapping techniques CubeViz hides the schema complexity of the encoded data in order to support a user-friendly exploration experience.	CubeViz: Exploration and Visualization of Statistical Linked Data	NA:NA:NA:NA:NA:NA	2015
Arunav Mishra:Klaus Berberich	Recent increases in digitization and archiving efforts on news data have led to overwhelming amounts of online information for general users, thus making it difficult for them to retrospect on past events. One dimension along which past events can be effectively organized is time. Motivated by this idea, we introduce EXPOSÉ, an exploratory search system that explicitly uses temporal information associated with events to link different kinds of information sources for effective exploration of past events. In this demonstration, we use Wikipedia and news articles as two orthogonal sources. Wikipedia is viewed as an event directory that systematically lists seminal events in a year; news articles are viewed as a source of detailed information on each of these events. To this end, our demo includes several time-aware retrieval approaches that a user can employ for retrieving relevant news articles, as well as a timeline tool for temporal analysis and entity-based facets for filtering results.	EXPOSÉ: EXploring Past news fOr Seminal Events	NA:NA	2015
Bernardo Pereira Nunes:Terhi Nurmikko-Fuller:Giseli Rabello Lopes:Chiara Renso	ISCOOL is an interactive educational platform that helps users develop their skills for objective text analysis and interpretation. The tools incorporated into ISCOOL bridge various disparate sources, including reference datasets for people, and organizations, as well as gazetteers, dictionaries and collections of historical facts. This data serves as the basis for educating learners about the processes of evaluating the implicit and implied content of written material, whilst also providing a wider context in which this information is accessed, interpreted and understood. In the course of gameplay, the user is prompted to choose images that best capture content of a read passage. The interactive features of the game simultaneously test the user's existing knowledge, and ability to critically analyse the text. Results can be saved and shared, allowing the players to continue to interact with the data through conversations with their peers, friends, and family members, and to disseminate information throughout their communities. Users will be able to draw connections between the information they encounter in ISCOOL, and their daily realities - participants are empowered, informed and educated.	A Serious Game Powered by Semantic Web technologies	NA:NA:NA:NA	2015
Barak Pat:Yaron Kanza:Mor Naaman	Geographic search -- where the user provides keywords and receives relevant locations depicted on a map -- is a popular web application. Typically, such search is based on static geographic data. However, the abundant geotagged posts in microblogs such as Twitter and in social networks like Instagram, provide contemporary information that can be used to support geosocial search---geographic search based on user activities in social media. Such search can point out where people talk (or tweet) about different topics. For example, the search results may show where people refer to ``jogging'', to indicate popular jogging places. The difficulty in implementing such search is that there is no natural partition of the space into ``documents'' as in ordinary web search. Thus, it is not always clear how to present results and how to rank and filter results effectively. In this paper, we demonstrate a two-step process of first, quickly finding the relevant areas by using an arbitrary indexed partition of the space, and secondly, applying clustering on discovered areas, to present more accurate results. We introduce a system that utilizes geotagged posts in geographic search and illustrate how different ranking methods can be used, based on the proposed two-step search process. The system demonstrates the effectiveness and usefulness of the approach.	Geosocial Search: Finding Places based on Geotagged Social-Media Posts	NA:NA:NA	2015
Diego Reforgiato Recupero:Andrea Giovanni Nuzzolese:Sergio Consoli:Valentina Presutti:Misael Mongiovì:Silvio Peroni	SHELDON is the first true hybridization of NLP machine reading and the Semantic Web. It extracts RDF data from text using a machine reader: the extracted RDF graphs are compliant to Semantic Web and Linked Data. It goes further and applies Semantic Web practices and technologies to extend the current human-readable web. The input is represented by a sentence in any language. SHELDON includes different capabilities in order to extend machine reading to Semantic Web data: frame detection, topic ex- traction, named entity recognition, resolution and coreference, terminology extraction, sense tagging and disambiguation, taxonomy induction, semantic role labeling, type induction, sentiment analysis, citation inference, relation and event extraction, nice visualization tools which make use of the JavaScript infoVis Toolkit and RelFinder. A demo of SHELDON can be seen and used at http://wit.istc.cnr.it/stlab-tools/sheldon.	Extracting knowledge from text using SHELDON, a Semantic Holistic framEwork for LinkeD ONtology data	NA:NA:NA:NA:NA:NA	2015
Joel Scheuner:Jürgen Cito:Philipp Leitner:Harald Gall	Optimizing the deployment of applications in Infrastructure-as-a-Service clouds requires to evaluate the costs and performance of different combinations of cloud configurations which is unfortunately a cumbersome and error-prone process. In this paper, we present Cloud WorkBench (CWB), a concrete implementation of a cloud benchmarking Web service, which fosters the definition of reusable and representative benchmarks. We demonstrate the complete cycle of benchmarking an IaaS service with the sample benchmark SysBench. In distinction to existing work, our system is based on the notion of Infrastructure-as-Code, which is a state of the art concept to define IT infrastructure in a reproducible, well-defined, and testable way.	Cloud WorkBench: Benchmarking IaaS Providers based on Infrastructure-as-Code	NA:NA:NA:NA	2015
Arnab Sinha:Zhihong Shen:Yang Song:Hao Ma:Darrin Eide:Bo-June (Paul) Hsu:Kuansan Wang	In this paper we describe a new release of a Web scale entity graph that serves as the backbone of Microsoft Academic Service (MAS), a major production effort with a broadened scope to the namesake vertical search engine that has been publicly available since 2008 as a research prototype. At the core of MAS is a heterogeneous entity graph comprised of six types of entities that model the scholarly activities: field of study, author, institution, paper, venue, and event. In addition to obtaining these entities from the publisher feeds as in the previous effort, we in this version include data mining results from the Web index and an in-house knowledge base from Bing, a major commercial search engine. As a result of the Bing integration, the new MAS graph sees significant increase in size, with fresh information streaming in automatically following their discoveries by the search engine. In addition, the rich entity relations included in the knowledge base provide additional signals to disambiguate and enrich the entities within and beyond the academic domain. The number of papers indexed by MAS, for instance, has grown from low tens of millions to 83 million while maintaining an above 95% accuracy based on test data sets derived from academic activities at Microsoft Research. Based on the data set, we demonstrate two scenarios in this work: a knowledge driven, highly interactive dialog that seamlessly combines reactive search and proactive suggestion experience, and a proactive heterogeneous entity recommendation.	An Overview of Microsoft Academic Service (MAS) and Applications	NA:NA:NA:NA:NA:NA:NA	2015
Nam Khanh Tran:Andrea Ceroni:Nattiya Kanhabua:Claudia Niederée	Fully understanding an older news article requires context knowledge from the time of article creation. Finding information about such context is a tedious and time-consuming task, which distracts the reader. Simple contextualization via Wikification is not sufficient here. The retrieved context information has to be time-aware, concise (not full Wikipages) and focused on the coherence of the article topic. In this paper, we present Contextualizer, a web-based system that acquires additional information for supporting interpretations of a news article of interest that requires a mapping, in this case, a kind of time-travel translation between present context knowledge and context knowledge at time of text creation. For a given article, the system provides a GUI that allows users to highlight their interested keywords which are then used to construct appropriate queries for retrieving contextualization candidates. Contextualizer exploits different kinds of information such as temporal similarity and textual complementarity to re-rank the candidates and presents to users in a friendly and interactive web-based interface.	Time-travel Translator: Automatically Contextualizing News Articles	NA:NA:NA:NA	2015
Liang Wang:Sotiris Tasoulis:Teemu Roos:Jussi Kangasharju	The Internet is overloading its users with excessive information flows, so that effective content-based filtering becomes crucial in improving user experience and work efficiency. We build Kvasir, a semantic recommendation system, atop latent semantic analysis and other state-of-art technologies to seamlessly integrate an automated and proactive content provision service into web browsing. We utilize the power of Apache Spark to scale up Kvasir to a practical Internet service. Herein we present the architecture of Kvasir, along with our solutions to the technical challenges in the actual system implementation.	Kvasir: Seamless Integration of Latent Semantic Analysis-Based Content Provision into Web Browsing	NA:NA:NA:NA	2015
Fei Wu:Hongjian Wang:Zhenhui Li:Wang-Chien Lee:Zhuojie Huang	The wide adaptation of mobile devices embedded with modern positioning technology enables the collection of valuable mobility data from users. At the same time, the large-scale user-generated data from social media, such as geo-tagged tweets, provide rich semantic information about events and locations. The combination of the mobility data and social media data brings opportunities for us to study the semantics behind people's movement, i.e., understand why a person travels to a location at a particular time. Previous work have used map or POI (point of interest) database as source for semantics. However, those semantics are static, and thus missing important dynamic event information. To provide dynamic semantic annotation, we propose to use contextual social media. More specifically, the semantics could be landmark information (e.g., a museum or an arena) or event information (e.g., sports games or concerts). The SemMobi system implements our recently developed annotation method, which has been recently accepted to WWW 2015 conference. The annotation method annotates words to each mobility records based on local density of words, estimated by Kernel Density Estimation model. The annotated mobility data contain rich and interpretable information, therefore can benefit applications, such as personalized recommendation, targeted advertisement, and movement prediction. Our system is built upon large-scale tweet datasets. A user-friendly interface is designed to support interactive exploration of the result.	SemMobi: A Semantic Annotation System for Mobility Data	NA:NA:NA:NA:NA	2015
Zhong Zeng:Zhifeng Bao:Mong Li Lee:Tok Wang Ling	Keyword search over relational databases has been widely studied for the exploration of structured data in a user-friendly way. However, users typically have limited domain knowledge or are unable to precisely specify their search intention. Existing methods find the minimal units that contain all the query keywords, and largely ignore the interpretation of possible users' search intentions. As a result, users are often overwhelmed with a lot of irrelevant answers. Moreover, without a visually pleasing way to present the answers, users often have difficulty understanding the answers because of their complex structures. Therefore, we design an interactive yet visually pleasing search paradigm called ExpressQ. ExpressQ extends the keyword query language to include keywords that match meta-data, e.g., names of relations and attributes. These keywords are utilized to infer users' search intention. Each possible search intention is represented as a query pattern, whose meaning is described in human natural language. Through a series of user interactions, ExpressQ can determine the search intention of the user, and translate the corresponding query patterns into SQLs to retrieve answers to the query. The ExpressQ prototype is available at http://expressq.comp.nus.edu.sg.	Towards An Interactive Keyword Search over Relational Databases	NA:NA:NA:NA	2015
Lora Aroyo:Brooke Foucault Welles:Dame Wendy Hall:Jim Hendler	NA	Session details: WebSci Track Papers & Posters	NA:NA:NA:NA	2015
Rakesh Agrawal:Behzad Golshan:Evangelos Papalexakis	Google and Bing have emerged as the diarchy that arbitrates what documents are seen by Web searchers, particularly those desiring English language documents. We seek to study how distinctive are the top results presented to the users by the two search engines. A recent eye-tracking has shown that the web searchers decide whether to look at a document primarily based on the snippet and secondarily on the title of the document on the web search result page, and rarely based on the URL of the document. Given that the snippet and title generated by different search engines for the same document are often syntactically different, we first develop tools appropriate for conducting this study. Our empirical evaluation using these tools shows a surprising agreement in the results produced by the two engines for a wide variety of queries used in our study. Thus, this study raises the open question whether it is feasible to design a search engine that would produce results distinct from those produced by Google and Bing that the users will find helpful.	A Study of Distinctiveness in Web Results of Two Search Engines	NA:NA:NA	2015
Mirza Basim Baig:Leman Akoglu	Graph robustness is a measure of resilience to failures and targeted attacks. A large body of research on robustness focuses on how to attack a given network by deleting a few nodes so as to maximally disrupt its connectedness. As a result, literature contains a myriad of attack strategies that rank nodes by their relative importance for this task. How different are these strategies? Do they pick similar sets of target nodes, or do they differ significantly in their choices? In this paper, we perform the first large scale empirical correlation analysis of attack strategies, i.e., the node importance measures that they employ, for graph robustness. We approach this task in three ways; by analyzing similarities based on (i) their overall ranking of the nodes, (ii) the characteristics of top nodes that they pick, and (iii) the dynamics of disruption that they cause on the network. Our study of 15 different (randomized, local, distance-based, and spectral) strategies on 68 real-world networks reveals surprisingly high correlations among node-attack strategies, consistent across all three types of analysis, and identifies groups of comparable strategies. These findings suggest that some computationally complex strategies can be closely approximated by simpler ones, and a few strategies can be used as a close proxy of the consensus among all of them.	Correlation of Node Importance Measures: An Empirical Study through Graph Robustness	NA:NA	2015
Emma Cradock:David Millard:Sophie Stalla-Bourdillon	The current execution of privacy policies, as a mode of communicating information to users, is unsatisfactory. Social networking sites (SNS) exemplify this issue, attracting growing concerns regarding their use of personal data and its effect on user privacy. This demonstrates the need for more informative policies. However, SNS lack the incentives required to improve policies, which is exacerbated by the difficulties of creating a policy that is both concise and compliant. Standardization addresses many of these issues, providing benefits for users and SNS, although it is only possible if policies share attributes which can be standardized. This investigation used thematic analysis and cross- document structure theory, to assess the similarity of attributes between the privacy policies (as available in August 2014), of the six most frequently visited SNS globally. Using the Jaccard similarity coefficient, two types of attribute were measured; the clauses used by SNS and the coverage of forty recommendations made by the UK Information Commissioner's Office. Analysis showed that whilst similarity in the clauses used was low, similarity in the recommendations covered was high, indicating that SNS use different clauses, but to convey similar information. The analysis also showed that low similarity in the clauses was largely due to differences in semantics, elaboration and functionality between SNS. Therefore, this paper proposes that the policies of SNS already share attributes, indicating the feasibility of standardization and five recommendations are made to begin facilitating this, based on the findings of the investigation.	Investigating Similarity Between Privacy Policies of Social Networking Sites as a Precursor for Standardization	NA:NA:NA	2015
Manirupa Das:Micha Elsner:Arnab Nandi:Rajiv Ramnath	With the onset of social media and news aggregators on the Web, the newspaper industry is faced with a declining subscriber base. In order to retain customers both on-line and in print, it is therefore critical to predict and mitigate customer churn. Newspapers typically have heterogeneous sources of valuable data: circulation data, customer subscription information, news content, and search click log data. An ensemble of predictive models over multiple sources faces unique challenges -- ascertaining short-term versus long-term effects of features on churn, and determining mutual information properties across multiple data sources. We present TopChurn, a novel system that uses topic models as a means of extracting dominant features from user complaints and Web data for churn prediction. TopChurn uses a maximum entropy-based approach to identify features that are most indicative of subscribers likely to drop subscription within a specified period of time. We conduct temporal analyses to determine long-term versus short-term effects of status changes on subscriber accounts, included in our temporal models of churn; and topic and sentiment analyses on news and clicklogs, included in our Web models of churn. We then validate our insights via experiments over real data from The Columbus Dispatch, a mainstream daily newspaper, and demonstrate that our churn models significantly outperform baselines for various prediction windows.	TopChurn: Maximum Entropy Churn Prediction Using Topic Models Over Heterogeneous Signals	NA:NA:NA:NA	2015
Marco Guerini:Jacopo Staiano	This article provides a comprehensive investigation on the relations between virality of news articles and the emotions they are found to evoke. Virality, in our view, is a phenomenon with many facets, i.e. under this generic term several different effects of persuasive communication are comprised. By exploiting a high-coverage and bilingual corpus of documents containing metrics of their spread on social networks as well as a massive affective annotation provided by readers, we present a thorough analysis of the interplay between evoked emotions and viral facets. We highlight and discuss our findings in light of a cross-lingual approach: while we discover differences in evoked emotions and corresponding viral effects, we provide preliminary evidence of a generalized explanatory model rooted in the deep structure of emotions: the Valence-Arousal-Dominance (VAD) circumplex. We find that viral facets appear to be consistently affected by particular VAD configurations, and these configurations indicate a clear connection with distinct phenomena underlying persuasive communication.	Deep Feelings: A Massive Cross-Lingual Study on the Relation between Emotions and Virality	NA:NA	2015
Zhenyu Li:Gaogang Xie:Mohamed Ali Kaafar:Kave Salamatian	Streaming live content to mobile terminals has become prevalent. While there are extensive measurement studies of non-mobile live streaming (and in particular P2P live streaming) and video-on-demand (both mobile and non-mobile), user behavior in mobile live streaming systems is yet to be explored. This paper relies on over 4 million access logs collected from the PPTV live streaming system to study the viewing behavior and user activity pattern, with emphasis on the discrepancies that might exist when users access the live streaming system catalog from mobile and non-mobile terminals. We observe high rates of abandoned viewing sessions for mobile users and identify different reasons of that behavior for 3G- and WiFi-based views. We further examine the structure of abandoned sessions due to connection performance issues from the perspectives of time of day and mobile device types. To understand the user pattern, we analyze user activity distribution, user geographical distribution as well as user arrival/departure rates.	User Behavior Characterization of a Large-scale Mobile Live Streaming System	NA:NA:NA:NA	2015
Umashanthi Pavalanathan:Munmun De Choudhury	Social media is increasingly being adopted in health discourse. We examine the role played by identity in supporting discourse on socially stigmatized conditions. Specifically, we focus on mental health communities on reddit. We investigate the characteristics of mental health discourse manifested through reddit's characteristic 'throwaway' accounts, which are used as proxies of anonymity. For the purpose, we propose affective, cognitive, social, and linguistic style measures, drawing from literature in psychology. We observe that mental health discourse from throwaways is considerably disinhibiting and exhibits increased negativity, cognitive bias and self-attentional focus, and lowered self-esteem. Throwaways also seem to be six times more prevalent as an identity choice on mental health forums, compared to other reddit communities. We discuss the implications of our work in guiding mental health interventions, and in the design of online communities that can better cater to the needs of vulnerable populations. We conclude with thoughts on the role of identity manifestation on social media in behavioral therapy.	Identity Management and Mental Health Discourse in Social Media	NA:NA	2015
Will Radford:Matthias Gallé	Film and television play an important role in popular culture. However studies that require watching and annotating video are time-consuming and expensive to run at scale. We explore information mined from media database cast lists to explore the evolution of different roles over time. We focus on the gender distribution of those roles and how this changes over time. Finally, we compare real-life census gender distributions to our web-mediated onscreen gender data. We propose these methodologies are a useful adjunct to traditional analysis that allow researchers to explore the relationship between online and onscreen gender depictions.	"Roles for the Boys?": Mining Cast Lists for Gender and Role Distributions over Time	NA:NA	2015
Avi Segal:Ya'akov (Kobi) Gal:Robert J. Simpson:Victoria Victoria Homsy:Mark Hartswood:Kevin R. Page:Marina Jirotka	The majority of volunteers participating in citizen science projects perform only a few tasks each before leaving the system. We designed an intervention strategy to reduce disengagement in 16 different citizen science projects. Targeted users who had left the system received emails that directly addressed motivational factors that affect their engagement. Results show that participants receiving the emails were significantly more likely to return to productive activity when compared to a control group.	Improving Productivity in Citizen Science through Controlled Intervention	NA:NA:NA:NA:NA:NA:NA	2015
Paul Seitlinger:Dominik Kowald:Simone Kopeinik:Ilire Hasani-Mavriqi:Elisabeth Lex:Tobias Ley	Classic resource recommenders like Collaborative Filtering (CF) treat users as being just another entity, neglecting non-linear user-resource dynamics shaping attention and interpretation. In this paper, we propose a novel hybrid recommendation strategy that refines CF by capturing these dynamics. The evaluation results reveal that our approach substantially improves CF and, depending on the dataset, successfully competes with a computationally much more expensive Matrix Factorization variant.	Attention Please! A Hybrid Resource Recommender Mimicking Attention-Interpretation Dynamics	NA:NA:NA:NA:NA:NA	2015
Arkaitz Zubiaga:Maria Liakata:Rob Procter:Kalina Bontcheva:Peter Tolmie	Social media are frequently rife with rumours, and the study of rumour conversational aspects can provide valuable knowledge about how rumours evolve over time and are discussed by others who support or deny them. In this work, we present a new annotation scheme for capturing rumour-bearing conversational threads, as well as the crowdsourcing methodology used to create high quality, human annotated datasets of rumourous conversations from social media. The rumour annotation scheme is validated through comparison between crowdsourced and reference annotations. We also found that only a third of the tweets in rumourous conversations contribute towards determining the veracity of rumours, which reinforces the need for developing methods to extract the relevant pieces of information automatically.	Crowdsourcing the Annotation of Rumourous Conversations in Social Media	NA:NA:NA:NA:NA	2015
Alessandro Bessi:Fabio Petroni:Michela Del Vicario:Fabiana Zollo:Aris Anagnostopoulos:Antonio Scala:Guido Caldarelli:Walter Quattrociocchi	NA	Viral Misinformation: The Role of Homophily and Polarization	NA:NA:NA:NA:NA:NA:NA:NA	2015
Grégoire Burel:Paul Mulholland:Yulan He:Harith Alani	Value of online Question Answering (Q&A) communities is driven by the question-answering behaviour of its members. Finding the questions that members are willing to answer is therefore vital to the efficient operation of such communities. In this paper, we aim to identify the parameters that correlate with such behaviours. We train different models and construct effective predictions using various user, question and thread feature sets. We show that answering behaviour can be predicted with a high level of success.	Modelling Question Selection Behaviour in Online Communities	NA:NA:NA:NA	2015
Dominic DiFranzo:Marie Joan Kristine Gloria:James Hendler	As Web Science continues to mix methods from the many disciplines that study the web, we must begin to seriously look at mixing and linking data across the Qualitative and Quantitative divide. A large difficulty in this is in modeling and archiving Qualitative data. In this paper, we outline what these difficulties are in detail with a focus on the data practices of Ethnography. We describe how linked data technologies can address these issues. We demonstrate this with a case study in modeling data from audio interviews that were taken in an ethnographic study conducted in our lab. We conclude with a discussion on future work that needs to be done to better equip researchers with these tools and methods.	Linked Ethnographic Data: From Theory to Practice	NA:NA:NA	2015
Daniel Dünker:Jérôme Kunegis	Online pet social networks provide a unique opportunity to study an online social network in which a single user manages multiple user profiles, i.e. one for each pet they own. These types of multi-profile networks allow us to investigate two questions: (1) What is the relationship between the pet-level and human-level network, and (2) what is the relationship between friendship links and family ties? Concretely, we study the online pet social networks Catster, Dogster and Hamsterster, and show how the networks on the two levels interact, and perform experiments to find out whether knowledge about friendships on a profile-level alone can be used to predict which users are behind which profile.	Social Networking by Proxy: Analysis of Dogster, Catster and Hamsterster	NA:NA	2015
Xiwu Han:Antonio A.R. Ioris:Chenghua Lin	Web as corpus for NLP has been popular, and we now employed web as corpus for NLG, and made the online communication of tailored river information more effective and efficient. Evaluation and analysis shows that our generated texts were comparable to those written by domain experts and experienced users.	Web as Corpus Supporting Natural Language Generation for Online River Information Communication	NA:NA:NA	2015
Rishabh Mehrotra:Prasanta Bhattacharya	Video sharing and entertainment websites have rapidly grown in popularity and now constitute some of the most visited websites on the Internet. Despite the high usage and user engagement, most of recent research on online media platforms have restricted themselves to networking based social media sites like Facebook or Twitter. The current study is among the first to perform a large-scale empirical study using longitudinal video upload data from one of the largest online video sites. Unlike previous studies in the online media space that have focused exclusively on demand-side research questions, we model the supply-side of the crowd contributed video ecosystem on this platform. The modeling and subsequent prediction of video uploads is made complicated by the heterogeneity of video types (e.g. popular vs. niche video genres), and the inherent time trend effects. We identify distinct genre-clusters from our dataset and employ a self-exciting Hawkes point-process model on each of these clusters to fully specify and estimate the video upload process. Our findings show that using a relatively parsimonious point-process model, we are able to achieve higher model fit, and predict video uploads to the platform with a higher accuracy than competing models.	Modeling the Evolution of User-generated Content on a Large Video Sharing Platform	NA:NA	2015
Spiros Papadimitriou:Evangelos Papalexakis:Bin Liu:Hui Xiong	Concurrently with the recent, rapid adoption of 3D printing technologies, online sharing of 3D-printable designs is growing equally rapidly, even though it has received far less attention. We study remix relationships on Thingiverse, the dominant online repository and social network for 3D printing. We collected data of designs published over five years, and we find that remix ties exhibit both homophily and inverse-homophily across numerous key metrics, which is stronger compared to other kinds of social and content links. This may have implications on graph prediction tasks, as well as on the design of 3D-printable content repositories.	Remix in 3D Printing: What your Sources say About You	NA:NA:NA:NA	2015
Ramine Tinati:Markus Luczak-Roesch:Nigel Shadbolt:Wendy Hall	In this paper we examine WikiProjects, an emergent, community-driven feature of Wikipedia. We analysed 3.2 million Wikipedia articles associated with 618 active Wikipedia projects. The dataset contained the logs of over 115 million article revisions and 15 million talk entries both representing the activity of 15 million unique Wikipedians altogether. Our analysis revealed that per WikiProject, the number of article and talk contributions are increasing, as are the number of new Wikipedians contributing to individual WikiProjects. Based on these findings we consider how studying Wikipedia from a sub-community level may provide a means to measure Wikipedia activity.	Using WikiProjects to Measure the Health of Wikipedia	NA:NA:NA:NA	2015
Max Van Kleek:Daniel Smith:Nigel R. Shadbolt:Dave Murray-Rust:Amy Guy	Portraying matters as other than they truly are is an important part of everyday human communication. In this paper, we use a survey to examine ways in which people fabricate, omit or alter the truth online. Many reasons are found, including creative expression, hiding sensitive information, role-playing, and avoiding harassment or discrimination. The results suggest lying is often used for benign purposes, and we conclude that its use may be essential to maintaining a humane online society.	Self Curation, Social Partitioning, Escaping from Prejudice and Harassment: The Many Dimensions of Lying Online	NA:NA:NA:NA:NA	2015
Evelyne Viegas:Chris Welty	NA	Session details: Industrial Track	NA:NA	2015
Deepak Agarwal:Shaunak Chatterjee:Yang Yang:Liang Zhang	This paper considers an application of showing promotional widgets to web users on the homepage of a major professional social network site. The types of widgets include address book invitation, group join, friends' skill endorsement and so forth. The objective is to optimize user engagement under certain business constraints. User actions on each widget may have very different downstream utilities, and quantification of such utilities can sometimes be quite difficult. Since there are multiple widgets to rank when a user visits, launching a personalized model to simply optimize user engagement such as clicks is often inappropriate. In this paper we propose a scalable constrained optimization framework to solve this problem. We consider several different types of constraints according to the business needs for this application. We show through both offline experiments and online A/B tests that our optimization framework can lead to significant improvement in user engagement while satisfying the desired set of business objectives.	Constrained Optimization for Homepage Relevance	NA:NA:NA:NA	2015
Omar Alonso:Sushma Bannur:Kartikay Khandelwal:Shankar Kalyanaraman	Over the past couple of years, social networks such as Twitter and Facebook have become the primary source for consuming information on the Internet. One of the main differentiators of this content from traditional information sources available on the Web is the fact that these social networks surface individuals' perspectives. When social media users post and share updates with friends and followers, some of those short fragments of text contain a link and a personal comment about the web page, image or video. We are interested in mining the text around those links for a better understanding of what people are saying about the object they are referring to. Capturing the salient keywords from the crowd is rich metadata that we can use to augment a web page. This metadata can be used for many applications like ranking signals, query augmentation, indexing, and for organizing and categorizing content. In this paper, we present a technique called social signatures that given a link to a web page, pulls the most important keywords from the social chatter around it. That is, a high level representation of the web page from a social media perspective. Our findings indicate that the content of social signatures differs compared to those from a web page and therefore provides new insights. This difference is more prominent as the number of link shares increase. To showcase our work, we present the results of processing a dataset that contains around 1 Billion unique URLs shared in Twitter and Facebook over a two month period. We also provide data points that shed some light on the dynamics of content sharing in social media.	The World Conversation: Web Page Metadata Generation From Social Sources	NA:NA:NA:NA	2015
Shuo Chang:Peng Dai:Jilin Chen:Ed H. Chi	Online search and item recommendation systems are often based on being able to correctly label items with topical keywords. Typically, topical labelers analyze the main text associated with the item, but social media posts are often multimedia in nature and contain contents beyond the main text. Topic labeling for social media posts is therefore an important open problem for supporting effective social media search and recommendation. In this work, we present a novel solution to this problem for Google+ posts, in which we integrated a number of different entity extractors and annotators, each responsible for a part of the post (e.g. text body, embedded picture, video, or web link). To account for the varying quality of different annotator outputs, we first utilized crowdsourcing to measure the accuracy of individual entity annotators, and then used supervised machine learning to combine different entity annotators based on their relative accuracy. Evaluating using a ground truth data set, we found that our approach substantially outperforms topic labels obtained from the main text, as well as naive combinations of the individual annotators. By accurately applying topic labels according to their relevance to social media posts, the results enables better search and item recommendation.	Got Many Labels?: Deriving Topic Labels from Multiple Sources for Social Media Posts using Crowdsourcing and Ensemble Learning	NA:NA:NA:NA	2015
Guangyu Feng:Kun Xiong:Yang Tang:Anqi Cui:Jing Bai:Hang Li:Qiang Yang:Ming Li	A central task of computational linguistics is to decide if two pieces of texts have similar meanings. Ideally, this depends on an intuitive notion of semantic distance. While this semantic distance is most likely undefinable and uncomputable, in practice it is approximated heuristically, consciously or unconsciously. In this paper, we present a theory, and its implementation, to approximate the elusive semantic distance by the well-defined information distance. It is mathematically proven that any computable approximation of the intuitive concept of semantic distance is "covered" by our theory. We have implemented our theory to question answering (QA) and performed experiments based on data extracted from over 35 million question-answer pairs. Experiments demonstrate that our initial implementation of the theory produces convincingly fewer errors in classification compared to other academic models and commercial systems.	Question Classification by Approximating Semantics	NA:NA:NA:NA:NA:NA:NA:NA	2015
Hao Fu:Xing Xie:Yong Rui	Microblogging websites, e.g. Twitter and Sina Weibo, have become a popular platform for socializing and sharing information in recent years. Spammers have also discovered this new opportunity to unfairly overpower normal users with unsolicited content, namely social spams. While it is intuitive for everyone to follow legitimate users, recent studies show that both legitimate users and spammers follow spammers for different reasons. Evidence of users seeking for spammers on purpose is also observed. We regard this behavior as a useful information for spammer detection. In this paper, we approach the problem of spammer detection by leveraging the "carefulness" of users, which indicates how careful a user is when she is about to follow a potential spammer. We propose a framework to measure the carefulness, and develop a supervised learning algorithm to estimate it based on known spammers and legitimate users. We then illustrate how spammer detection can be improved in the aid of the proposed measure. Evaluation on a real dataset with millions of users and an online testing are performed on Sina Weibo. The results show that our approach indeed capture the carefulness, and it is effective to detect spammers. In addition, we find that the proposed measure is also beneficial for other applications, e.g. link prediction.	Leveraging Careful Microblog Users for Spammer Detection	NA:NA:NA	2015
Anastasios Noulas:Blake Shaw:Renaud Lambiotte:Cecilia Mascolo	Understanding the spatial networks formed by the trajectories of mobile users can be beneficial to applications ranging from epidemiology to local search. Despite the potential for impact in a number of fields, several aspects of human mobility networks remain largely unexplored due to the lack of large-scale data at a fine spatiotemporal resolution. Using a longitudinal dataset from the location-based service Foursquare, we perform an empirical analysis of the topological properties of place networks and note their resemblance to online social networks in terms of heavy-tailed degree distributions, triadic closure mechanisms and the small world property. Unlike social networks however, place networks present a mixture of connectivity trends in terms of assortativity that are surprisingly similar to those of the web graph. We take advantage of additional semantic information to interpret how nodes that take on functional roles such as 'travel hub', or 'food spot' behave in these networks. Finally, motivated by the large volume of new links appearing in place networks over time, we formulate the classic link prediction problem in this new domain. We propose a novel variant of gravity models that brings together three essential elements of inter-place connectivity in urban environments: network-level interactions, human mobility dynamics, and geographic distance. We evaluate this model and find it outperforms a number of baseline predictors and supervised learning algorithms on a task of predicting new links in a sample of one hundred popular cities.	Topological Properties and Temporal Dynamics of Place Networks in Urban Environments	NA:NA:NA:NA	2015
Xiang Ren:Tao Cheng	With the increasing use of entities in serving people's daily information needs, recognizing synonyms---different ways people refer to the same entity---has become a crucial task for many entity-leveraging applications. Previous works often take a "literal" view of the entity, i.e., its string name. In this work, we propose adopting a "structured" view of each entity by considering not only its string name, but also other important structured attributes. Unlike existing query log-based methods, we delve deeper to explore sub-queries, and exploit tailed synonyms and tailed web pages for harvesting more synonyms. A general, heterogeneous graph-based data model which encodes our problem insights is designed by capturing three key concepts (synonym candidate, web page and keyword) and different types of interactions between them. We cast the synonym discovery problem into a graph-based ranking problem and demonstrate the existence of a closed-form optimal solution for outputting entity synonym scores. Experiments on several real-life domains demonstrate the effectiveness of our proposed method.	Synonym Discovery for Structured Entities on Heterogeneous Graphs	NA:NA	2015
Houping Xiao:Jing Gao:Deepak S. Turaga:Long H. Vu:Alain Biem	In this paper, we investigate the problem of identifying inconsistent hosts in large-scale enterprise networks by mining multiple views of temporal data collected from the networks. The time-varying behavior of hosts is typically consistent across multiple views, and thus hosts that exhibit inconsistent behavior are possible anomalous points to be further investigated. To achieve this goal, we develop an effective approach that extracts common patterns hidden in multiple views and detects inconsistency by measuring the deviation from these common patterns. Specifically, we first apply various anomaly detectors on the raw data and form a three-way tensor (host, time, detector) for each view. We then develop a joint probabilistic tensor factorization method to derive the latent tensor subspace, which captures common time-varying behavior across views. Based on the extracted tensor subspace, an inconsistency score is calculated for each host that measures the deviation from common behavior. We demonstrate the effectiveness of the proposed approach on two enterprise-wide network-based anomaly detection tasks. An enterprise network consists of multiple hosts (servers, desktops, laptops) and each host sends/receives a time-varying number of bytes across network protocols (e.g.,TCP, UDP, ICMP) or send URL requests to DNS under various categories. The inconsistent behavior of a host is often a leading indicator of potential issues (e.g., instability, malicious behavior, or hardware malfunction). We perform experiments on real-world data collected from IBM enterprise networks, and demonstrate that the proposed method can find hosts with inconsistent behavior that are important to cybersecurity applications.	Temporal Multi-View Inconsistency Detection for Network Traffic Analysis	NA:NA:NA:NA:NA	2015
Anirban Dasgupta:Jay Chen:Oana Goga	NA	Session details: PhD Symposium	NA:NA:NA	2015
Maribel Acosta	Linked Open Data initiatives have fostered the publication of Linked Data sets, as well as the deployment of publicly available SPARQL endpoints as client-server querying infrastructures to access these data sets. However, recent studies reveal that SPARQL endpoints may exhibit significant limitations in supporting real-world applications, and public linked data sets can suffer of quality issues, e.g., data can be incomplete or incorrect. We tackle these problems and propose a novel hybrid architecture that relies on shipping policies to improve the performance of SPARQL endpoints, and exploits human and machine query processing computation to enhance the quality of Linked Data sets. We report on initial empirical results that suggest that the proposed techniques overcome current drawbacks, and may provide a novel solution to make these promising infrastructures available for real-world applications.	A Hybrid Approach to Perform Efficient and Effective Query Execution Against Public SPARQL Endpoints	NA	2015
Majid Ali AlShehry:Bruce Walker Ferguson	Crowdsourcing serves different needs of different sets of users. Most existing definitions and taxonomies of crowdsourcing address platform purpose while paying little attention to other parameters of this novel social phenomenon. In this paper, we analyze 41 crowdsourcing campaigns on 21 crowdsourcing platforms to derive 9 key parameters of successful crowdsourcing campaigns and introduce a comprehensive taxonomy of crowdsourcing. Using this taxonomy, we identify crowdsourcing trends in two parameters, platform purpose and contributor motivation. The paper highlights important advantages of using this conceptual model in planning crowdsourcing campaigns and concludes with a discussion of emerging challenges to such campaigns.	A Taxonomy of Crowdsourcing Campaigns	NA:NA	2015
Cody Buntain	My proposed research addresses fundamental deficiencies in social media-based event detection by discovering high-impact moments and evaluating their credibility rapidly. Results from my preliminary work demonstrate one can discover compelling moments by leveraging machine learning to characterize and detect bursts in keyword usage. Though this early work focused primarily on language-agnostic discovery in sporting events, it also showed promising results in adapting this work to earthquake detection. My dissertation will extend this research by adapting models to other types of high-impact events, exploring events with different temporal granularities, and finding methods to connect contextually related events into timelines. To ensure applicability of this research, I will also port these event discovery algorithms to stream processing platforms and evaluate their performance in the real-time context. To address issues of trust, my dissertation will also include developing algorithms that integrate the vast array of social media features to evaluate information credibility in near real time. Such features include structural signatures of information dissemination, the location from which a social media message was posted relative to the location of the event it describes, and metadata from related multimedia (e.g., pictures and video) shared about the event. My preliminary work also suggests methods that could be applied to social networks for stimulating trustworthy behavior and enhancing information quality. Contributions from my dissertation will primarily be practical algorithms for discovering events from various social media streams and algorithms for evaluating and enhancing the credibility of these events in near real time.	Discovering Credible Events in Near Real Time from Social Media Streams	NA	2015
Anila Sahar Butt	With the recent growth of Linked Data on the Web, there is an increased need for knowledge engineers to find ontologies to describe their data. Only limited work exists that addresses the problem of searching and ranking ontologies based on keyword queries. In this proposal we introduce the main challenges to find appropriate ontologies, and preliminary solutions to address these challenges. Our evaluation shows that the proposed solution performs significantly better than existing solutions on a benchmark ontology collection for the majority of the sample queries defined in the benchmark.	Ontology Search: Finding the Right Ontologies on the Web	NA	2015
Ujwal Gadiraju	Within the scope of this PhD proposal, we set out to investigate two pivotal aspects that influence the effectiveness of crowdsourcing: (i) microtask design, and (ii) workers behavior. Leveraging the dynamics of tasks that are crowdsourced on the one hand, and accounting for the behavior of workers on the other hand, can help in designing tasks efficiently. To help understand the intricacies of microtasks, we identify the need for a taxonomy of typically crowdsourced tasks. Based on an extensive study of 1000 workers on CrowdFlower, we propose a two-level categorization scheme for tasks. We present insights into the task affinity of workers, effort exerted by workers to complete tasks of various types, and their satisfaction with the monetary incentives. We also analyze the prevalent behavior of trustworthy and untrustworthy workers. Next, we propose behavioral metrics that can be used to measure and counter malicious activity in crowdsourced tasks. Finally, we present guidelines for the effective design of crowdsourced surveys and set important precedents for future work.	Make Hay While the Crowd Shines: Towards Efficient Crowdsourcing on the Web	NA	2015
Asmelash Teka Hadgu	The explosion of Web 2.0 platforms including social networking sites such as Twitter, blogs and wikis affects all web users: scholars included. As a result, there is a need for a comprehensive approach to gain a broader understanding and timely signals of scientific communication as well as how researchers interact on the social web. Most current work in this area deals with either a low number of researchers and heavily relies on manual annotation or large-scale analysis without deep understanding of the underlying researcher population. In this proposal, we present a holistic approach to solve these problems. This research proposes novel methods to collect, filter, analyze and make sense of scholars and scholarly communication by integrating heterogeneous data sources from fast social media streams as well as the academic web. Applying reproducible research, contributing applications and data sets, the thesis proposal strives to add value by mining the social web for social good.	Mining Scholarly Communication and Interaction on the Social Web	NA	2015
Dominik Kowald	With the emergence of Web 2.0, tag recommenders have become important tools, which aim to support users in finding descriptive tags for their bookmarked resources. Although current algorithms provide good results in terms of tag prediction accuracy, they are often designed in a data-driven way and thus, lack a thorough understanding of the cognitive processes that play a role when people assign tags to resources. This thesis aims at modeling these cognitive dynamics in social tagging in order to improve tag recommendations and to better understand the underlying processes. As a first attempt in this direction, we have implemented an interplay between individual micro-level (e.g., categorizing resources or temporal dynamics) and collective macro-level (e.g., imitating other users' tags) processes in the form of a novel tag recommender algorithm. The preliminary results for datasets gathered from BibSonomy, CiteULike and Delicious show that our proposed approach can outperform current state-of-the-art algorithms, such as Collaborative Filtering, FolkRank or Pairwise Interaction Tensor Factorization. We conclude that recommender systems can be improved by incorporating related principles of human cognition.	Modeling Cognitive Processes in Social Tagging to Improve Tag Recommendations	NA	2015
Jeongwoo Oh	Social media or social network service has attracted a great amount of interest of applied studies, because we can see how people connect, behave, and interact each other through it even at a glance. Individual usage of social media can be viewed from corporate level in the market. This paper starts from such interest as well, trying to verify the applicability of social media in the corporate finance study. The basic question is whether social media interaction of the firm or firm's executive can affect the performance of the corresponding firm. In the study of economics and finance, firm level network has been studied in different contexts, mainly involved with economic benefit. However, the online network has not been enough studied regarding its effect on the corporate performance. In general, firm's decision making process has been regarded as exclusive and confidential, rather than publicly observable, which resulted in focusing more on the closed network in person or between related firms. But we observe that many top executives are already active or even stars on the social media. Therefore, we take a close look at this to determine if networking on social media is personal activity or corporate behavior. In other words, we are interested in whether the internet-based life style with social media can possibly influence on the corporate performance in the market or the firm-level decision making process. We investigate this question by using both social media and market data with firm information. First of all, we identify the determinants of social network behavior of the firms' executives. And next, we estimate the value of social media network on the corporate performance, calculating abnormal returns and analyzing its dynamics in the long term. Finally, we also verify the value of social media network in the context of executives' personal compensation. We expect that this research will provide a new insight about the social network on the corporate performance by adopting online social media as a network variable. It is also expected to broaden the applicability of the online network data to the academic questions in finance research.	Social Media as Firm's Network and Its Influence on the Corporate Performance	NA	2015
Mohamed M. Sabri	Linked Data has been widely adopted over the last few years, with the size of the Linked Data cloud almost doubling every year. However, there is still no well-defined, efficient mechanism for querying such a Web of Data. We propose a framework that incorporates a set of optimizations to tackle various limitations in the state-of-the-art. The framework aims at combining the centralized query optimization capabilities of the data warehouse-based approaches with the result freshness and explorative data source discovery capabilities of link-traversal approaches. This is achieved by augmenting base-line link-traversal query execution with a set of optimization techniques. The proposed optimizations fall under two categories: metadata-based optimizations and semantics-based optimizations.	A Hybrid Framework for Online Execution of Linked Data Queries	NA	2015
Leonidas Anthopoulos:Marijn Janssen:Vishanth Weerakkody	NA	Session details: AW4CITY 2015	NA:NA:NA	2015
Leonidas G. Anthopoulos:Marijn Janssen:Vishanth Weerakkody	Smart cities have attracted an extensive and increasing interest from both science and industry with an increasing number of international examples emerging from across the world. However, despite the significant role that smart cities can play to deal with recent urban challenges, the concept has been criticized for being influenced by vendor hype. There are various attempts to conceptualize smart cities and various benchmarking methods have been developed to evaluate their impact. In this paper the modelling and benchmarking approaches are systematically compared. There are six common dimensions among the approaches, namely people, government, economy, mobility, environment and living. This paper utilizes existing smart city analysis models in order to review three representative smart city cases and useful outcomes are extrapolated from this comparison.	Comparing Smart Cities with different modeling approaches	NA:NA:NA	2015
Leonidas G. Anthopoulos:Panos Fitsilis	Smart cities have attracted the international scientific and business attention and a niche market is being evolved, which engages almost all the business sectors. In their attempt to empower and promote urban competitive advantages, local governments have approached the smart city context and they target habitants, visitors and investments. However, engaging the smart city context is not free-of-charge and corresponding investments are extensive and of high risk without the appropriate management. Moreover, investing in the smart city domain does not secure corresponding mission success and both governments and vendors require more effective instruments. This paper performs an investigation on the smart city business models and is a work in progress. Modeling can illustrate where corresponding profit comes from and how it flows, while a significant business model portfolio is eligible for smart city stakeholders.	Understanding Smart City Business Models: A Comparison	NA:NA	2015
Sergio Consoli:Diego Reforgiato Recupero:Misael Mongiovi:Valentina Presutti:Gianni Cataldi:Wladimiro Patatu	A good interaction between public administrations and citizens is imperative in modern smart cities. Semantic web technologies can aid in achieving such a goal. We present a smart urban fault reporting web platform to help citizens in reporting common urban problems, such as street faults, potholes or broken street lights, and to support the local public administration in responding and fixing those problems quickly. The tool is based on a semantic data model designed for the city, which integrates several distinct data sources, opportunely re-engineered to meet the principles of the Semantic Web and linked open data. The platform supports the whole process of road maintenance, from the fault reporting to the management of maintenance activities. The integration of multiple data sources enables increasing interoperability and heterogeneous information retrieval, thus favoring the development of effective smart urban fault reporting services. Our platform was evaluated in a real case study: a complete urban reporting and road maintenance system has been developed for the municipality of Catania. Our approach is completely generalizable and can be adopted by and customized for other cities. The final goal is to stimulate smart maintenance services in the "cities of the future".	An Urban Fault Reporting and Management Platform for Smart Cities	NA:NA:NA:NA:NA:NA	2015
Marion Gottschalk:Mathias Uslar	Urbanization grows steadily, i.e. more humans live at one place and rural areas are more unpopular. Urbanization faces challenges for city planning and development. Cities have to deal with large crowds, high energy consumption, large quantities of garbage etc. Thus, smart cities have to meet many requirements of different areas. Hence, realizing smart cities can be supported by linking different smart areas, such as smart girds and smart homes, to one large area. The linking is done by information and communication technologies, which are supported through a clear definition of functionalities and interfaces. Smart cities and further smart areas are under development, so, it is difficult to depict an overview on their functionalities, yet. Therefore, the two approaches, use case methodology and integration profiles, are introduced in this work, which are also realized by a web-based application.	Supporting the Development of Smart Cities using a Use Case Methodology	NA:NA	2015
Vincenzo Mighali:Giuseppe Del Fiore:Luigi Patrono:Luca Mainetti:Stefano Alletto:Giuseppe Serra:Rita Cucchiara	Smart cities are a trading topic in both the academic literature and industrial world. The capability to provide the users with added-value services through low-power and low-cost smart objects is very attractive in many fields. Among these, art and culture represent very interesting examples, as the tourism is one of the main driving engines of modern society. In this paper, we propose an IoT-aware architecture to improve the cultural experience of the user, by involving the most important recent innovations in the ICT field. The main components of the proposed architecture are: (i) an indoor localization service based on the Bluetooth Low Energy technology, (ii) a wearable device able to capture and process images related to the user's point of view, (iii) the user's mobile device useful to display customized cultural contents and to share multimedia data in the Cloud, and (iv) a processing center that manage the core of the whole business logic. In particular, it interacts with both wearable and mobile devices, and communicates with the outside world to retrieve contents from the Cloud and to provide services also to external users. The proposal is currently under development and it will be validated in the MUST museum in Lecce.	Innovative IoT-aware Services for a Smart Museum	NA:NA:NA:NA:NA:NA:NA	2015
Erich Ortner:Marco Mevius:Peter Wiedmann:Florian Kurz	Nowadays, the number of flexible and fast human to application system interactions is dramatically increasing. For instance, citizens interact with the help of the internet to organize surveys or meetings (in real-time) spontaneously. These interactions are supported by technologies and application systems such as free wireless networks, web -or mobile apps. Smart Cities aim at enabling their citizens to use these digital services, e.g., by providing enhanced networks and application infrastructures maintained by the public administration. However, looking beyond technology, there is still a significant lack of interaction and support between "normal" citizens and the public administration. For instance, democratic decision processes (e.g. how to allocate public disposable budgets) are often discussed by the public administration without citizen involvement. This paper introduces an approach, which describes the design of enhanced interactional web applications for Smart Cities based on dialogical logic process patterns. We demonstrate the approach with the help of a budgeting scenario as well as a summary and outlook on further research.	Design of Interactional End-to-End Web Applications for Smart Cities	NA:NA:NA:NA	2015
Alois Paulin	In this paper we search for and analyze the atomic components of general governance systems and discuss whether or not they can be informated, i.e. tangibly represented within the digital realm of information systems. We draw a framework based on the theories of Downs, Jellinek, and Hohfeld and find that the therein identified atomic components cannot be informated directly, but only indirectly, due to the inherent complexity of governance. We outline pending research questions to be addressed in the future.	Smart Cities Governance Informatability?: Let's First Understand the Atoms	NA	2015
Robert Seeliger:Christopher Krauss:Annette Wilson:Miggi Zwicklbauer:Stefan Arbanowski	The FI-CONTENT project aims at establishing the foundation of a European infrastructure for developing and testing novel smart city services. The Smart City Services Platform will develop enabling technology for SMEs and developer to create services offering residents and visitors to cities smart services that enhance their city visit or daily life. We have made use of generic, specific and common enablers to develop a reference implementation, the Smart City Guide web app. The basic information is provided by the Open City Database, an open source specific enabler that can be used for any city in Europe. Recommendation as a Service is an enabler that can be applied to lots use cases, here we describe how we integrated it into the Smart City Guide. The uses cases will be iteratively improved and upgraded during regular iterative cycles based on feedback gained in lab and field trials at the experimentation sites. As the app is transferable to any city, it will be tested at a number of experimentation sites.	Towards Personalized Smart City Guide Services in Future Internet Environments	NA:NA:NA:NA:NA	2015
Leyla Zhuhadar:Bryan Carson:Jerry Daday:Olfa Nasraoui	We describe a proposed universal design infrastructure that aims at promoting better opportunities for students with disabilities in STEM programs to understand multimedia teaching material. The Accessible Educational STEM Videos Project aims to transform learning and teaching for students with disabilities through integrating synchronized captioned educational videos into undergraduate and graduate STEM disciplines. This Universal Video Captioning (UVC) platform will serve as a repository for uploading videos and scripts. The proposed infrastructure is a web-based platform that uses the latest WebDAV technology (Web-based Distributed Authoring and Versioning) to identify resources, users, and content. It consists of three layers: (i) an administrative management system; (ii) a faculty/staff user interface; and (iii) a transcriber user interface. We anticipate that by enriching it with captions or transcripts, the multimodal presentation of materials promises to help students with disabilities in STEM programs master the subject better and increase retention.	A Universal Design Infrastructure for Multimodal Presentation of Materials in STEM Programs: Universal Design	NA:NA:NA:NA	2015
Feng Xia:Huan Liu:Irwin King	NA	Session details: BigScholar 2015	NA:NA:NA	2015
Kuansan Wang	Human is the only species on earth that has mastered the technologies in writing and printing to capture ephemeral thoughts and scientific discoveries. The capabilities to pass along knowledge, not only geographically but also generationally, have formed the bedrock of our civilizations. We are in the midst of a silent revolution driven by the technological advancements: no longer are computers just a fixture of our physical world but have they been so deeply woven into our daily routines that they are now occupying the center of our lives. No where are the phenomena more prominent than our reliance on the World Wide Web. More and more often, the web has become the primary source of fresh information and knowledge. In addition to general consumption, the availability of large amount of contents and behavioral data has also instigated new interdisciplinary research activities in the areas of information retrieval, natural language processing, machine learning, behavioral studies, social computing and data mining. This talk will use web search as an example to demonstrate how these new research activities and technologies have help the web evolve from a collection of documents to becoming the largest knowledge base in our history. During this evolution, the web is transformed from merely reacting to our needs to a living entity that can anticipate and push timely information to wherever and whenever we need it. How the scholarly activities and communications can be impacted will also be illustrated and elaborated, and some observations derived from a web scale data set, newly release to the public, will also be shared.	The Knowledge Web Meets Big Scholars	NA	2015
Zhen Chen:Feng Xia:Huizhen Jiang:Haifeng Liu:Jun Zhang	Academic venues act as the main platform of communities in academia and the bridge of connecting researchers, which have rapidly developed in recent years. However, information overload in big scholarly data creates tremendous challenges for mining useful and effective information in order to recommend researchers to acknowledge high quality and fruitful academic venues, thereby enabling them to participate in relevant academic conferences as well as contributing to important/influential journals. In this work, we propose AVER, a novel random walk based Academic VEnue Recommendation model. AVER runs a random walk with restart model on a co-publication network which contains two kinds of associations, coauthor relations and author-venue relations. Moreover, we define a transfer matrix with bias to drive the random walk by exploiting three academic factors, co-publication frequency, weight of relations and researchers' academic level. AVER is inspired from the fact that researchers are more likely to contact those who have high co-publication frequency and similar academic levels. Additionally, in AVER, we consider the difference of weights between two kinds of associations. We conduct extensive experiments on DBLP data set in order to evaluate the performance of AVER. The results demonstrate that, in comparison to relevant baseline approaches, AVER performs better in terms of precision, recall and F1.	AVER: Random Walk Based Academic Venue Recommendation	NA:NA:NA:NA:NA	2015
Subhajit Datta:Santonu Sarkar:A.S.M. Sajeev:Nishant Kumar	For researchers and practitioners of a relatively young discipline like software engineering, an enduring concern is to identify the acorns that will grow into oaks -- ideas remaining most current in the long run. Additionally, it is interesting to know how the ideas have risen in importance, and fallen, perhaps to rise again. We analyzed a corpus of 19,000+ papers written by 21,000+ authors across 16 software engineering publication venues from 1975 to 2010, to empirically determine the half-life of software engineering research topics. We adapted existing measures of half-life as well as defined a specific measure based on publication and citation counts. The results from this empirical study are a presented in this paper.	Discovering the Rise and Fall of Software Engineering Ideas from Scholarly Publication Data	NA:NA:NA:NA	2015
Yu Liu:Zhen Huang:Yizhou Yan:Yufeng Chen	With the advances of all research fields and web 2.0, scientific literature has been widely observed in digital libraries, citation databases, and social media. Its new properties, such as large volume, wide exhibition, and the complicated citation relationship in papers bring challenges to the management, analysis and exploring knowledge of scientific literature. In addition, although data mining techniques have been imported to scientific literature analysis tasks, they typically requires expert input and guidance, and returns static results to users after process, which makes them inflexible and not smart. Therefore, there is the need of a tool, which highly reflects article-level-metrics and combines human users and computer systems for analysis and exploring knowledge of scientific literature, as well as discovering and visualizing underlying interesting research topics. We design an online tool for literature navigation, filtering, and interactive data mining, named Science Navigation Map (SNM), which integrates information from online paper repositories, citation databases, etc. SNM provides visualization of article level metrics and interactive data mining which takes advantage of effective interaction between human users and computer systems to explore and extract knowledge from scientific literature and discover underlying interesting research topics. We also propose a multi-view non-negative matrix factorization and apply it to SNM as an interactive data mining tool, which can make better use of complicated multi-wise relationships in papers. In experiments, we visualize all the papers published at the journal of PLOS Biology from 2003 to 2012 in the navigation map and explore six relationship in papers for data mining. From this map, one can easily filter, analyse and explore knowledge of the papers through an interactive way.	Science Navigation Map: an Interactive Data Mining Tool for Literature Analysis	NA:NA:NA:NA	2015
Alexander G. Ororbia, II:Jian Wu:Madian Khabsa:Kyle WIlliams:Clyde Lee Giles	We examine CiteSeerX, an intelligent system designed with the goal of automatically acquiring and organizing large-scale collections of scholarly documents from the world wide web. From the perspective of automatic information extraction and modes of alternative search, we examine various functional aspects of this complex system with an eye towards ongoing and future research developments.	Big Scholarly Data in CiteSeerX: Information Extraction from the Web	NA:NA:NA:NA:NA	2015
Sabir Ribas:Berthier Ribeiro-Neto:Edmundo de Souza e Silva:Alberto Hideki Ueda:Nivio Ziviani	In this paper we discuss the problem of how to assess academic productivity based on publication outputs. We are interested in knowing how well a research group in an area of knowledge is doing relatively to a pre-selected set of reference groups, where each group is composed by academics or researchers. To assess academic productivity we adopt a new metric we propose, which we call P-score. We use P-score, citation counts and H-Index to obtain rankings of researchers in Brazil. Experimental results using data from the area of Computer Science show that P-score outperforms citation counts and H-Index when assessed against the official ranking produced by the Brazilian National Research Council (CNPq). This is of our interest for two reasons. First, it suggests that citation-based metrics, despite wide adoption, can be improved upon. Second, contrary to citation-based metrics, the P-score metric does not require access to the content of publications to be computed.	Using Reference Groups to Assess Academic Productivity in Computer Science	NA:NA:NA:NA:NA	2015
Qiu Fang Ying:Srinivasan Venkatramanan:Dah Ming Chiu	Scientific literature till date can be thought of as a partially revealed landscape, where scholars continue to unveil hidden knowledge by exploring novel research topics. How do scholars explore the scientific landscape, i.e., choose research topics to work on? We propose an agent-based model of topic mobility behavior where scholars migrate across research topics on the space of science following different strategies, seeking different utilities. We use this model to study whether strategies widely used in current scientific community can provide a balance between individual scientific success and the efficiency and diversity of the whole academic society. Through extensive simulations, we provide insights into the roles of different strategies, such as choosing topics according to research potential or the popularity. Our model provides a conceptual framework and a computational approach to analyze scholars' behavior and its impact on scientific production. We also discuss how such an agent-based modeling approach can be integrated with big real-world scholarly data.	Modeling and Analysis of Scholar Mobility on Scientific Landscape	NA:NA:NA	2015
Manuel Gomez-Rodriguez:Le Song:Hongyuan Zha	NA	Session details: DAEN 2015	NA:NA:NA	2015
Djellel Eddine Difallah:Michele Catasta:Gianluca Demartini:Panagiotis G. Ipeirotis:Philippe Cudré-Mauroux	Micro-task crowdsourcing is rapidly gaining popularity among research communities and businesses as a means to leverage Human Computation in their daily operations. Unlike any other service, a crowdsourcing platform is in fact a marketplace subject to human factors that affect its performance, both in terms of speed and quality. Indeed, such factors shape the \emph{dynamics} of the crowdsourcing market. For example, a known behavior of such markets is that increasing the reward of a set of tasks would lead to faster results. However, it is still unclear how different dimensions interact with each other: reward, task type, market competition, requester reputation, etc. In this paper, we adopt a data-driven approach to (A) perform a long-term analysis of a popular micro-task crowdsourcing platform and understand the evolution of its main actors (workers, requesters, tasks, and platform). (B) We leverage the main findings of our five year log analysis to propose features used in a predictive model aiming at determining the expected performance of any batch at a specific point in time. We show that the number of tasks left in a batch and how recent the batch is are two key features of the prediction. (C) Finally, we conduct an analysis of the demand (new tasks posted by the requesters) and supply (number of tasks completed by the workforce) and show how they affect task prices on the marketplace.	The Dynamics of Micro-Task Crowdsourcing: The Case of Amazon MTurk	NA:NA:NA:NA:NA	2015
Mehrdad Farajtabar:Manuel Gomez-Rodriguez:Yichen Wang:Shuang Li:Hongyuan Zha:Le Song	Information diffusion in online social networks is obviously affected by the underlying network topology, but it also has the power to change that topology. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the route of information spread. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics. In this project, we propose a probabilistic generative model, COEVOLVE, for the joint dynamics of these two processes, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate diffusion and network events from the co-evolutionary dynamics, and generate traces obeying common diffusion and network patterns observed in real-world networks. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives.	Co-evolutionary Dynamics of Information Diffusion and Network Structure	NA:NA:NA:NA:NA:NA	2015
Przemyslaw Grabowicz:Niloy Ganguly:Krishna Gummadi	A number of recent studies of information diffusion in social media, both empirical and theoretical, have been inspired by viral propagation models derived from epidemiology. These studies model propagation of memes, i.e., pieces of information, between users in a social network similarly to the way diseases spread in human society. Naturally, many of these studies emphasize social exposure, i.e., the number of friends or acquaintances of a user that have exposed a meme to her, as the primary metric for understanding, predicting, and controlling information diffusion. Intuitively, one would expect a meme to spread in a social network selectively, i.e., amongst the people who are interested in the meme. However, the importance of the alignment between the topicality of a meme and the topical interests of the potential adopters and influencers in the network has been less explored in the literature. In this paper, we quantify the impact of the topical alignment between memes and users on their adoption. Our analysis, using empirical data about two different types of memes, i.e., hashtags and URLs spreading through the Twitter social media platform, finds that topical alignment between memes and users is as crucial as the social exposure in understanding and predicting meme adoptions. Our results emphasize the need to look beyond social network-based viral propagation models and develop microscopic models of information diffusion that account for interests of users and topicality of information.	Microscopic Description and Prediction of Information Diffusion in Social Media: Quantifying the Impact of Topical Interests	NA:NA:NA	2015
Thibaut Horel:Yaron Singer	In many applications of influence maximization, one is restricted to select influencers from a set of users who engaged with the topic being promoted, and due to the structure of social networks, these users often rank low in terms of their influence potential. To alleviate this issue, one can consider an adaptive method which selects users in a manner which targets their influential neighbors. The advantage of such an approach is that it leverages the friendship paradox in social networks: while users are often not influential, they often know someone who is. Despite the various complexities in such optimization problems, we show that scalable adaptive seeding is achievable. To show the effectiveness of our methods we collected data from various verticals social network users follow, and applied our methods on it. Our experiments show that adaptive seeding is scalable, and that it obtains dramatic improvements over standard approaches of information dissemination.	Scalable Methods for Adaptively Seeding a Social Network	NA:NA	2015
Jean Pouget-Abadie:Thibaut Horel	In the Graph Inference problem, one seeks to recover the edges of an unknown graph from the observations of cascades propagating over this graph. We approach this problem from the sparse recovery perspective. We introduce a general model of cascades, including the voter model and the independent cascade model, for which we provide the first algorithm which recovers the graph's edges with high probability and O(s log m) measurements where s is the maximum degree of the graph and $m$ is the number of nodes. Furthermore, we show that our algorithm also recovers the edge weights (the parameters of the diffusion process) and is robust in the context of approximate sparsity. Finally we validate our approach empirically on synthetic graphs.	Inferring Graphs from Cascades: A Sparse Recovery Framework	NA:NA	2015
Estevam Hruschka:Michael Witbrock:Marko Grobelnik:Blaz Fortuna	NA	Session details: KET 2015	NA:NA:NA:NA	2015
Fabian M. Suchanek	In this talk, I will present our recent work in the area of knowledge bases. It covers 4 areas of research around ontologies and knowledge bases: The first area is the construction of the YAGO knowledge base. YAGO is now mulitlingual, and has grown into a larger project at the Max Planck Institute for Informatics and Télécom ParisTech. The second area is the alignment of knowledge bases. This includes the alignment of classes, instances, and relations across knowledge bases. The third area is rule mining. Our project finds semantic correlations in the form of Horn rules in the knowledge base. I will also talk about watermarking approaches to trace the provenance of ontological data. Finally, I will show applications of the knowledge base for mining news corpora.	A Hitchhiker's Guide to Ontology	NA	2015
Luka Bradesko:Janez Starc:Stefano Pacifico	This paper shows the implementation and evaluation of the Entity Linking or Named Entity Disambiguation system used and developed at Bloomberg. In particular, we present and evaluate a methodology and a system that do not require the use of Wikipedia as a knowledge base or training corpus. We present how we built features for disambiguation algorithms from the Bloomberg News corpus, and how we employed them for both single-entity and joint-entity disambiguation into a Bloomberg proprietary knowledge base of people and companies. Experimental results show high quality in the disambiguation of the available annotated corpus.	Isaac Bloomberg Meets Michael Bloomberg: Better EntityDisambiguation for the News	NA:NA:NA	2015
Alon Dayan:Osnat Mokryn:Tsvi Kuflik	This paper presents a new method for extracting unique features of items based on their textual reviews. The method is built of two similar iterations of applying a weighting scheme and then clustering the resultant set of vectors. In the first iteration, restaurants of similar food genres are grouped together into clusters. The second iteration reduces the importance of common terms in each such cluster, and highlights those that are unique to each specific restaurant. Clustering the restaurants again, now according to their unique features, reveals very interesting connections between the restaurants.	A Two-Iteration Clustering Method to Reveal Unique and Hidden Characteristics of Items Based on Text Reviews	NA:NA:NA	2015
Angel Luis Garrido:Pilar Blazquez:Maria G. Buey:Sergio Ilarri	Today, we can find a vast amount of textual information stored in proprietary data stores. The experience of searching information in these systems could be improved in a remarkable manner if we combine these private data stores with the information supplied by the Internet, merging both data sources to get new knowledge. In this paper, we propose an architecture with the goal of automatically obtaining knowledge about entities (e.g., persons, places, organizations, etc.) from a set of natural text documents, building smart data from raw data. We have tested the system in the context of the news archive of a real Media Group.	Knowledge Obtention Combining Information Extraction Techniques with Linked Data	NA:NA:NA:NA	2015
Changlin Ma:Meng Wang:Xuewen Chen	Opinion mining is an important research topic in data mining. Many current methods are coarse-grained, which are practically problemic due to insufficient feedback information and limited reference values. To address these problems, a novel topic and sentiment unification maximum entropy LDA model is proposed in this paper for fine-grained opinion mining of online reviews. In this model, a maximum entropy component is first added to the traditional LDA model to distinguish background words, aspect words and opinion words and further realize both the local and global extraction of these words. A sentiment layer is then inserted between a topic layer and a word layer to extend the proposed model to four layers. Sentiment polarity analysis is done based on the extraction of aspect words and opinion words to simultaneously acquire the sentiment polarity of the whole review and each topic, which leads to, fine-grained topic-sentiment abstract. Experimental results demonstrate the validity of the proposed model and theory.	Topic and Sentiment Unification Maximum Entropy Model for Online Review Analysis	NA:NA:NA	2015
Changlin Ma:Yong Zhang:Maoyuan Zhang	Protein-protein interaction plays an important role in understanding biological processes. In order to resolve the parsing error resulted from modal verb phrases and the noise interference brought by appositive dependency, an improved tree kernel-based PPI extraction method is proposed in this paper. Both modal verbs and appositive dependency features are considered to define some relevant processing rules which can effectively optimize and expand the shortest dependency path between two proteins in the new method. On the basis of these rules, the effective optimization and expanding path is used to direct the cutting of constituent parse tree, which makes the constituent parse tree for protein-protein interaction extraction more precise and concise. The experimental results show that the new method achieves better results on five commonly used corpora.	Tree Kernel-based Protein-Protein Interaction Extraction Considering both Modal Verb Phrases and Appositive Dependency Features	NA:NA:NA	2015
Sergio Oramas:Mohamed Sordo:Luis Espinosa-Anke	This paper presents a rule based approach to extracting relations from unstructured music text sources. The proposed approach identifies and disambiguates musical entities in text, such as songs, bands, persons, albums and music genres. Candidate relations are then obtained by traversing the dependency parsing tree of each sentence in the text with at least two identified entities. A set of syntactic rules based on part of speech tags are defined to filter out spurious and irrelevant relations. The extracted entities and relations are finally represented as a knowledge graph. We test our method on texts from songfacts.com, a website that provides tidbits with facts and stories about songs. The extracted relations are evaluated intrinsically by assessing their linguistic quality, as well as extrinsically by assessing the extent to which they map an existing music knowledge base. Our system produces a vast percentage of linguistically correct relations between entities, and is able to replicate a significant part of the knowledge base.	A Rule-Based Approach to Extracting Relations from Music Tidbits	NA:NA:NA	2015
Sagnik Ray Choudhury:Clyde Lee Giles	Scholarly documents contain multiple figures representing experimental findings. These figures are generated from data which is not reported anywhere else in the paper. We propose a modular architecture for analyzing such figures. Our architecture consists of the following modules: 1. An extractor for figures and associated metadata (figure captions and mentions) from PDF documents; 2. A Search engine on the extracted figures and metadata; 3. An image processing module for automated data extraction from the figures and 4. A natural language processing module to understand the semantics of the figure. We discuss the challenges in each step, report an extractor algorithm to extract vector graphics from scholarly documents and a classification algorithm for figures. Our extractor algorithm improves the state of the art by more than 10% and the classification process is very scalable, yet achieves 85\% accuracy. We also describe a semi-automatic system for data extraction from figures which is integrated with our search engine to improve user experience.	An Architecture for Information Extraction from Figures in Digital Libraries	NA:NA	2015
Dave Schneider:Michael J. Witbrock	In this paper, we discuss Semantic Construction Grammar (SCG), a system developed over the past several years to facilitate translation between natural language and logical representations. Crucially, SCG is designed to support a variety of different methods of representation, ranging from those that are fairly close to the NL structure (e.g. so-called "logical forms"), to those that are quite different from the NL structure, with higher-order and high-arity relations. Semantic constraints and checks on representations are integral to the process of NL understanding with SCG, and are easily carried out due to the SCG's integration with Cyc's Knowledge Base and inference engine [1] , [2].	Semantic Construction Grammar: Bridging the NL / Logic Divide	NA:NA	2015
Fouad Zablith:Ibrahim H. Osman	With the daily increase of the amount of published information, research in the area of text analytics is gaining more visibility. Text processing for improving analytics is being studied from different angles. In the literature, text dependencies have been employed to perform various tasks. This includes for example the identification of semantic relations and sentiment analysis. We observe that while text dependencies can boost text analytics, managing and preserving such dependencies in text documents that spread across various corpora and contexts is a challenging task. We present in this paper our work on linking text dependencies using the Resource Description Framework (RDF) specification, following the Stanford typed dependencies representation. We contribute to the field by providing analysts the means to query, extract, and reuse text dependencies for analytical purposes. We highlight how this additional layer can be used in the context of feedback analysis by applying a selection of queries passed to a triple-store containing the generated text dependencies graphs.	Linking Stanford Typed Dependencies to Support Text Analytics	NA:NA	2015
Stefan Dietze:Mathieu d'Aquin:Dragan Gasevic:Eelco Herder	NA	Session details: LiLE 2015	NA:NA:NA:NA	2015
Phil Barker:Lorna M. Campbell	The Learning Resource Metadata Initiative (LRMI) is a collaborative initiative that aims to make it easier for teachers and learners to find educational materials through major search engines and specialized resource discovery services. The approach taken by LRMI is to extend the schema.org ontology so that educationally significant characteristics and relationships can be expressed. This, of course, builds on a long history developing metadata standards for learning resources. The context for LRMI, however, is different to these in several respects. LRMI builds on schema.org, and schema.org is designed as a means for marking up web pages to make them more intelligible to search engines; the aim is for it to be present in a significant proportion of pages on the web, that is, implemented at scale not just by metadata professionals. LRMI may have applications that go beyond the core aims of schema.org: it is possible to create LRMI metadata that is independent of a web page for example as JSON-LD records or as EPUB3 metadata. The approach of extending schema.org has several advantages, starting with the ability to focus on how best to describe the educational characteristics of resources while others focus on other specialist aspects of the resource description. It also means that LRMI benefits from all the effort that goes into developing tools and community resources for schema.org. There are still some challenges for LRMI, one which is particularly pertinent is that of describing educational frameworks (e.g. common curricula or educational levels) to which the learning resources align. LRMI has developed the means for expressing an alignment statement such as "this resource is useful for teaching subject X" but we need more work on how to refer to the subject in that statement. This is challenge that conventional linked data for education could address.	LRMI, Learning Resource Metadata on the Web	NA:NA	2015
Tom De Nies:Frank Salliau:Ruben Verborgh:Erik Mannens:Rik Van de Walle	A popular way to log learning processes is by using the Experience API (abbreviated as xAPI), also referred to as Tin Can. While Tin Can is great for developers who need to log learning experiences in their applications, it is more challenging for data processors to interconnect and analyze the resulting data. An interoperable data model is missing to raise Tin Can to its full potential. We argue that in essence, these learning process logs are provenance. Therefore, the W3C PROV model can provide the much-needed interoperability. In this paper, we introduce a method to expose PROV using Tin Can statements. To achieve this, we made the following contributions: (1) a formal ontology of the xAPI vocabulary, (2) a context document to interpret xAPI statements as JSON-LD, (3) a mapping to convert xAPI JSON-LD statements into PROV, and (4) a tool implementing this mapping.  We preliminarily evaluate the approach by converting 20 xAPI statements taken from the public Tin Can Learning Record Store to valid PROV. Where the conversion succeeded, it did so without loss of valid information, therefore suggesting that the conversion process is reversible, as long as the original JSON is valid.	TinCan2PROV: Exposing Interoperable Provenance of Learning Processes through Experience API Logs	NA:NA:NA:NA:NA	2015
Dmitry Mouromtsev:Fedor Kozlov:Liubov Kovriguina:Olga Parkhimovich	The paper concerns estimation of students' knowledge based on their learning results in the ECOLE system. ECOLE is the online eLearning system which functionality is based on several ontologies. This system allows to interlink terms from different courses and domains and calculates several educational rates: term knowledge rate, total knowledge rate, domain knowledge rate and term significance rate. All of these rates are used to give the student recommendations about the activities he has to undertake to pass a course successfully.	ECOLE: Student Knowledge Assessment in the Education Process	NA:NA:NA:NA	2015
Enayat Rajabi:Ivana Marenzi	Linked Data promises access to a vast amount of resources for learners and teachers. Various research projects have focused on providing educational resources as Linked Data. In many of these projects the focus has been on interoperability of metadata and on linking them into the linked data cloud. In this paper we focus on the community aspect. We start from the observation that sharing data is most valuable within communities of practice with common interests and goals, and community members are interested in suitable resources to be used in specific learning scenarios. The community of practice we are focusing on is an English language teaching and learning community, which we have been supporting through the LearnWeb2.0 platform for the last two years. We analyse the requirements of this specific community as a basis to enrich the current collected materials with open educational resources taken from the Linked Data Cloud. To this aim, we performed an interlinking approach in order to enrich the learning resources exposed as RDF (Resource Description Framework) in the LearnWeb2.0 platform with additional information taken from the Web.	Linking a Community Platform to the Linked Open Data Cloud	NA:NA	2015
Davide Taibi:Giovanni Fulantelli:Stefan Dietze:Besnik Fetahu	The diversity of datasets published according to Linked Data (LD) principles has increased in the last few years and also led to the emergence of a wide range of data suitable in educational settings. However, sufficient insights into the state, coverage and scope of available educational Linked Data seem to be missing, for instance, about represented resource types or domains and topics. In this work, we analyse the scope and coverage of educational linked data on the Web, identifying the most popular resource types and topics, apparent gaps and underlining the strong correlation of resource types and topics. Our results indicate a prevalent bias to-wards data in areas such as the life sciences as well as computing-related topics.	Towards Analysing the Scope and Coverage of Educational Linked Data on the Web	NA:NA:NA:NA	2015
Fouad Zablith	Online environments are increasingly used as platforms to support and enhance learning experiences. In higher education, students enroll in programs that are usually formed of a set of courses and modules. Such courses are designed to cover a set of concepts and achieve specific learning objectives that count towards the related degree. However we observe that connections among courses and the way they conceptually interlink are hard to exploit. This is normal as courses are traditionally described using text in the form of documents such as syllabi and course catalogs. We believe that linked data can be used to create a conceptual layer around higher education programs to interlink courses in a granular and reusable manner. We present in this paper our work on creating a semantic linked data layer to conceptually connect courses taught in a higher education program. We highlight the linked data model we created to be collaboratively extended by course instructors and students using a semantic Mediawiki platform. We also present two applications that we built on top of the data to (1) showcase how learning material can now float around courses through their interlinked concepts in eLearning environments (we use moodle as a proof of concept); and (2) to support the process of higher education program reviews.	Interconnecting and Enriching Higher Education Programs using Linked Data	NA	2015
Lyndon Nixon:Johan Oomen:Raphaël Troncy	NA	Session details: LIME 2015	NA:NA:NA	2015
Harald Sack	Within the process of the production of a film or tv program a significant amount of metadata is created and - most times - lost again. As a consequence most of this valuable information has to be costly recreated in subsequent steps of media production, distribution, and archival. On the other hand, there is no commonly used metadata exchange format throughout all steps of the media value chain. Furthermore, technical systems and software applications used in the media production process often have proprietary interfaces for data exchange. In the course of the D-Werft project funded by the German government, metadata exchange through all steps of the media value chain is to be fostered by the application of Linked Data principles. Starting with the idea for a script, metadata from existing systems and applications will be mapped to ontologies to be reused in subsequent production steps. Also for distribution and archival, metadata collected during the production process is a valuable asset to be reused for semantic and exploratory search as well as for intelligent movie recommendation and customized advertising.	From Script Idea to TV Rerun: The Idea of Linked Production Data in the Media Value Chain	NA	2015
Thomas Kurz:Kai Schlegel:Harald Kosch	The amount of audio, video and image data on the web is immensely growing, which leads to data management problems based on the hidden character of multimedia. Therefore the interlinking of semantic concepts and media data with the aim to bridge the gap between the document web and the Web of Data has become a common practice and is known as Linked Media. However, the value of connecting media to its semantic meta data is limited due to lacking access methods specialized for media assets and fragments as well as to the variety of used description models. With SPARQL-MM we extend SPARQL, the standard query language for the Semantic Web with media specific concepts and functions to unify the access to Linked Media. In this paper we describe the motivation for SPARQL-MM, present the State of the Art of Linked Media description formats and Multimedia query languages, and outline the specification and implementation of the SPARQL-MM function set.	Enabling access to Linked Media with SPARQL-MM	NA:NA:NA	2015
Roeland J.F. Ordelman:Maria Eskevich:Robin Aly:Benoit Huet:Gareth Jones	Multimedia hyperlinking is an emerging research topic in the context of digital libraries and (cultural heritage) archives. We have been studying the concept of video-to-video hyperlinking from a video search perspective in the context of the MediaEval evaluation benchmark for several years. Our task considers a use case of exploring large quantities of video content via an automatically created hyperlink structure at the media fragment level. In this paper we report on our findings, examine the features of the definition of video hyperlinking based on results, and discuss lessons learned with respect to evaluation of hyperlinking in real-life use scenarios.	Defining and Evaluating Video Hyperlinking for Navigating Multimedia Archives	NA:NA:NA:NA:NA	2015
Paloma Marín Arraiza:Sven Strobel	Various techniques for video analysis, concept mapping, semantic search and metadata management are part of the current features of the TIB|AV Portal as described in this demo. The segment identification and ontology annotation make the portal a good platform to support the Linked Data and Media. Weaving into a machine-readable metadata format will complete this task.	The TIB|AV Portal as a Future Linked Media Ecosystem	NA:NA	2015
Sergio Fernández:Sebastian Schaffert:Thomas Kurz	With the tremendous increase in multimedia content on the Web and in corporate intranets, discovering hidden meaning in raw multimedia is becoming one of the biggest challenges. Analysing multimedia content is still in its infancy, requires expert knowledge, and the few available products are associated with excessive price tags, while still not delivering sufficient quality for many tasks. This makes it hard, especially for small and medium-size enterprises, to make use of this technology. In addition analysis components typically operate in isolation and do not consider the context (e.g. embedding text) of a media resource. This paper presents how MICO tries to address these problems by providing an open source service platform, that allows to analyse media in context and includes various analysis engines for video, images, audio, text, link structure and metadata.	MICO: Towards Contextual Media Analysis	NA:NA:NA	2015
Thomas Wilmering:Kevin Page:György Fazekas:Simon Dixon:Sean Bechhofer	Computational feature extraction provides one means of gathering structured analytic metadata for large media collections. We demonstrate a suite of tools we have developed that automate the process of feature extraction from audio in the Internet Archive. The system constructs an RDF description of the analysis workflow and results which is then reconciled and combined with Linked Data about the recorded performance. This Linked Data and provenance information provides the bridging information necessary to employ analytic output in the generation of structured metadata for the underlying media files, with all data published within the same description framework.	Automating Annotation of Media with Linked Data Workflows	NA:NA:NA:NA:NA	2015
Dirk Ahlers:Erik Wilde:Bruno Martins	NA	Session details: LocWeb 2015	NA:NA:NA	2015
Daniele Quercia	Mapping apps are the greatest game-changer for encouraging people to explore the city. You take your phone out and you know immediately where to go. However, the app also assumes there are only a handful of directions to the destination. It has the power to make those handful of directions the definitive direction to that destination. A few years ago, my research started to focus on understanding how people psychologically experience the city. I used computer science tools to replicate social science experiments at scale, at web scale [4,5]. I became captivated by the beauty and genius of traditional social science experiments done by Jane Jacobs, Stanley Milgram, Kevin Lynch[1,2,3]. The result of that research has been the creation of new maps, maps where one does not only find the shortest path but also the most enjoyable path [6,9]. We did so by building a new city map weighted for human emotions. On this cartography, one is not only able to see and connect from point A to point B the shortest segments, but one is also able to see the happy path, the beautiful path, the quiet path. In tests, participants found the happy, the beautiful, the quiet paths far more enjoyable than the shortest one, and that just by adding a few minutes to travel time. Participants also recalled how some paths smelled and sounded. So what if we had a mapping tool that would return the most enjoyable routes based not only on aesthetics but also based on smell and sound? That is the research question this talk will start to address [7,8].	Chatty, Happy, and Smelly Maps	NA	2015
Hsiu-Min Chuang:Chia-Hui Chang	With the increased popularity of mobile devices and smart phones, location-based services (LBS) have become a common need in our daily life. Therefore, maintaining the correctness of POI (Points of Interest) data has become an important issue for many location-based services such as Google Maps and Garmin navigation systems. The simplest form of POI contains a location (e.g., represented by an address) and an identifier (e.g., an organization name) that describes the location. As time goes by, the POI relationship of a location and organization pair may change due to the opening, moving, or closing of a business. Thus, effectively identifying outdated or emerging POI relations is an important issue for improving the quality of POI data. In this paper, we examine the possibility of using location-related pages on the Web to verify existing POI relations via weakly labeled data, e.g., the co-occurrence of an organization and an address in Web pages, the published date of such pages, and the pairing diversity of an address or an organization, etc. The preliminary result shows a promising direction for discovering emerging POI and mandates more research for outdated POI.	Verification of POI and Location Pairs via Weakly Labeled Web Data	NA:NA	2015
Ben De Meester:Tom De Nies:Ruben Verborgh:Erik Mannens:Rik Van de Walle	Digital publications can be packaged and viewed via the Open Web Platform using the EPUB 3 format. Meanwhile, the increased amount of mobile clients and the advent of HTML5's Geolocation have opened a whole range of possibilities for digital publications to interact with their readers. However, EPUB 3 files often remain closed silos of information, no longer linked with the rest of the Web. In this paper, we propose a solution to reconnect digital publication with the (Semantic) Web. We will also show how we can use that connection to improve contextualization for a user, specifically via spatial information. We enrich digital publications by connecting the detected concepts to their URIs on, e.g., DBpedia, and by devising an algorithm to approximate the location of any detected concept, we can provide a user with the spatial center of gravity of his reading position. The evaluation of the location approximation algorithm showed a high recall, and the high correlation between estimation error and standard deviation can provide the user with a sense of correctness (or spread) of an approximation. This means relevant locations (and their possible radius) can be shown for a user, based on the content he or she is reading, and based on his or her location. This methodology can be used to reconnect digital publications with the online world, to entice readers, and ultimately, as a novel location-based recommendation technique.	Reconnecting Digital Publications to the Web using their Spatial Information	NA:NA:NA:NA:NA	2015
Gebrekirstos G. Gebremeskel:Arjen P. de Vries	We investigate the role of geographic proximity in news consumption. Using a month-long log of user interactions with news items of ten information portals, we study the relationship between users' geographic locations and the geographic foci of information portals and local news categories. We find that the location of news consumers correlates with the geographical information of the information portals at two levels: the portal and the local news category. At the portal level, traditional mainstream news portals have a more geographically focused readership than special interest portals, such as sports and technology. At a finer level, the mainstream news portals have local news sections that have even more geographically focused readerships.	The Role of Geographic Information in News Consumption	NA:NA	2015
Martin Atzmueller:Alvin Chin:Christoph Trattner	NA	Session details: MSM 2015	NA:NA:NA	2015
Mohammed Abufouda:Katharina A. Zweig	Many complex network systems suffer from noise that disguises the structure of the network and hinders an accurate analysis of these systems. Link assessment is the process of identifying and eliminating the noise from network systems in order to better understand these systems. In this paper, we address the link assessment problem in social networks that may suffer from noisy relationships. We employed a machine learning classifier for assessing the links in the social network of interest using the data from the associated interaction networks around it. The method was tested with two different data sets: each contains the social network of interest, with ground truth, along with the associated interaction networks. The results showed that it is possible to effectively assess the links of a social network using only the structure of a single network of the associated interaction networks and also using the structure of the whole set of the associated interaction networks. The experiment also revealed that the assessment performance using only the structure of the social network of interest is relatively less accurate than using the associated interaction networks. This indicates that link formation in the social network of interest is not only driven by the internal structure of the social network, but also influenced by the external factors provided in the associated interaction networks.	Are We Really Friends?: Link Assessment in Social Networks Using Multiple Associated Interaction Networks	NA:NA	2015
Cody Buntain:Jennifer Golbeck	Twitter can be a rich source of information when one wants to monitor trends related to a given topic. In this paper, we look at how tweets can augment a public health program that studies emerging patterns of illicit drug use. We describe the architecture necessary to collect vast numbers of tweets over time based on a large number of search terms and the challenges that come with finding relevant information in the collected tweets. We then show several examples of early analysis we have done on this data, examining temporal and geospatial trends.	This is your Twitter on drugs: Any questions?	NA:NA	2015
Doina Alexandra Dumitrescu:Simone Santini	Novelty detection algorithms usually employ similarity measures with the previous seen and relevant documents to decide if a document is of user's interest. The problem that arises by using this approach is that the system might recommend redundant documents. Thus, it has become extremely important to be able to distinguish between "redundant" and "novel" information. To address this limitation, we apply a contextual and semantic approach by building the user profile using self-organizing maps that have the advantage to easily follow the changes in the users interests.	Using Context to Get Novel Recommendation in Internet Message Streams	NA:NA	2015
Georgios Katsimpras:Dimitrios Vogiatzis:Georgios Paliouras	The emergence of social media and the enormous growth of social networks have initiated a great amount of research in social influence analysis. In this regard, many approaches take into account only structural information while a few have also incorporated content. In this study we propose a new method to rank users according to their topic-sensitive influence which utilizes a priori information by employing supervised random walks. We explore the use of supervision in a PageRank-like random walk while also exploiting textual information from the available content. We perform a set of experiments on Twitter datasets and evaluate our findings.	Determining Influential Users with Supervised Random Walks	NA:NA:NA	2015
Sadamori Koujaku:Mineichi Kudo:Ichigaku Takigawa:Hideyuki Imai	Detection of anomalous changes in social networks has been studied in various applications such as change detection of social interests and virus infections. Among several kinds of network changes, we concentrate on the structural changes of relatively small stationary communities. Such a change is important because it implies that some crucial changes have happened in a special group, such as dismiss of a board of directors. One difficulty is that we have to do this in a noisy environment. This paper, therefore, proposes an algorithm that finds stationary communities in a noisy environment. Experiments on two real networks showed the advantages of our proposed algorithm.	Community Change Detection in Dynamic Networks in Noisy Environment	NA:NA:NA:NA	2015
Yun-Qian Miao:Ahmed K. Farahat:Mohamed S. Kamel	With the massive growth of social data, a huge attention has been given to the task of detecting key topics in the Twitter stream. In this paper, we propose the use of novelty detection techniques for identifying both emerging and evolving topics in new tweets. In specific, we propose a locally adaptive approach for density-ratio estimation in which the density ratio between new and reference data is used to capture evolving novelties, and at the same time a locally adaptive kernel is employed into the density-ratio objective function to capture emerging novelties based on the local neighborhood structure. In order to address the challenges associated with short text, we adopt an efficient approach for calculating semantic kernels with the proposed density-ratio method. A comparison to different methods shows the superiority of the proposed algorithm.	Locally Adaptive Density Ratio for Detecting Novelty in Twitter Streams	NA:NA:NA	2015
Sepideh Seifzadeh:Ahmed K. Farahat:Mohamed S. Kamel:Fakhri Karray	Short documents are typically represented by very sparse vectors, in the space of terms. In this case, traditional techniques for calculating text similarity results in measures which are very close to zero, since documents even the very similar ones have a very few or mostly no terms in common. In order to alleviate this limitation, the representation of short-text segments should be enriched by incorporating information about correlation between terms. In other words, if two short segments do not have any common words, but terms from the first segment appear frequently with terms from the second segment in other documents, this means that these segments are semantically related, and their similarity measure should be high. Towards achieving this goal, we employ a method for enhancing document clustering using statistical semantics. However, the problem of high computation time arises when calculating correlation between all terms. In this work, we propose the selection of a few terms, and using these terms with the Nystr\"om method to approximate the term-term correlation matrix. The selection of the terms for the Nystr\"om method is performed by randomly sampling terms with probabilities proportional to the lengths of their vectors in the document space. This allows more important terms to have more influence on the approximation of the term-term correlation matrix and accordingly achieves better accuracy.	Short-Text Clustering using Statistical Semantics	NA:NA:NA:NA	2015
Emilio Serrano:Carlos Ángel Iglesias:Mercedes Garijo	Viral marketing, marketing techniques that use pre-existing social networks, has experienced a significant encouragement in the last years. In this scope, Twitter is the most studied social network in viral marketing and the rumor spread is a widely researched problem. This paper contributes with a (1) novel agent-based social simulation model for rumors spread in Twitter. This model relies on the hypothesis that (2) when a user is recovered, this user will not influence his or her neighbors in the social network to recover. To support this hypothesis: (3) two Twitter rumor datasets are studied; (4) a baseline model which does not include the hypothesis is revised, reproduced, and implemented; (5) and a number of experiments are conducted comparing the real data with the two models results.	A Novel Agent-Based Rumor Spreading Model in Twitter	NA:NA:NA	2015
Greg Stoddard	NA	Popularity and Quality in Social News Aggregators: A Study of Reddit and Hacker News	NA	2015
Io Taxidou:Tom De Nies:Ruben Verborgh:Peter M. Fischer:Erik Mannens:Rik Van de Walle	In recent years, research in information diffusion in social media has attracted a lot of attention, since the produced data is fast, massive and viral. Additionally, the provenance of such data is equally important because it helps to judge the relevance and trustworthiness of the information enclosed in the data. However, social media currently provide insufficient mechanisms for provenance, while models of information diffusion use their own concepts and notations, targeted to specific use cases. In this paper, we propose a model for information diffusion and provenance, based on the W3C PROV Data Model. The advantage is that PROV is a Web-native and interoperable format that allows easy publication of provenance data, and minimizes the integration effort among different systems making use of PROV.	Modeling Information Diffusion in Social Media as Provenance with W3C PROV	NA:NA:NA:NA:NA:NA	2015
Tomu Tominaga:Yoshinori Hijikata	In recent years, many researchers have studied the characteristics of Twitter, which is a microblogging service used by a large number of people worldwide. However, to the best of our knowledge, no study has yet been conducted to study the relationship between profile images and user behaviors on Twitter. We assume that the profile images and behaviors of users are influenced by their internal properties, because users consider their profile images as symbolic representations of themselves on Twitter. We empirically categorized profile images into 13 types, and investigated the relationships between each category of profile images and users' behaviors on Twitter.	Study on the Relationship between Profile Images and User Behaviors on Twitter	NA:NA	2015
Suncong Zheng:Hongyun Bao:Guanhua Tian:Yufang Wu:Bo Xu:Hongwei Hao	Existing works on user behavior analysis mainly focus on modeling a single behavior and predicting whether a user will take an action or not. However, users' behaviors do not always happen in isolation, sometimes, different behaviors may happen simultaneously. Therefore, in this paper, we try to analyze the combination of basic behaviors, called behavioral state here, which can describes users' complex behaviors comprehensively. We propose a model, called Personal Timed Hidden Markov Model (PTHMM), to settle the problem by considering time-interval information of users' behaviors and users' personalization. The experimental result on sina-weibo demonstrates the effectiveness of the model. It also shows that users' behavioral state is affected by their historical behaviors, and the influence of historical behaviors declines with the increasing of historical time.	PTHMM: Beyond Single Specific Behavior Prediction	NA:NA:NA:NA:NA:NA	2015
Ben Steichen:Nicola Ferro:David Lewis:Ed H. Chi	NA	Session details: MWA 2015	NA:NA:NA:NA	2015
Lorenzo Albano:Domenico Beneventano:Sonia Bergamaschi	In [Marco2013] a novel approach to Web search result clustering based on Word Sense Induction, i.e. the automatic discovery of word senses from raw text was presented; key to the proposed approach is the idea of, first, automatically inducing senses for the target query and, second, clustering the search results based on their semantic similarity to the word senses induced. In [1] we proposed an innovative Word Sense Induction method based on multilingual data; key to our approach was the idea that a multilingual context representation, where the context of the words is expanded by considering its translations in different languages, may improve the WSI results; the experiments showed a clear performance gain. In this paper we give some preliminary ideas to exploit our multilingual Word Sense Induction method to Web search result clustering.	Multilingual Word Sense Induction to Improve Web Search Result Clustering	NA:NA:NA	2015
Niels Bloom:Mariët Theune:Franciska De Jong	Associative networks are a connectionist language model with the ability to categorize large sets of documents. In this research we combine monolingual associative networks based on Wikipedia to create a larger, multilingual associative network, using the cross-lingual connections between Wikipedia articles. We prove that such multilingual associative networks perform better than monolingual associative networks in tasks related to document categorization by comparing the results of both types of associative network on a multilingual dataset.	Document Categorization using Multilingual Associative Networks based on Wikipedia	NA:NA:NA	2015
Gavin Brelstaff:Francesca Chessa	Great writers help keep a language efficient for discourse of all kinds. In doing so they produce exceptional texts which may defy Statistical Machine Translation by employing uncommon idiom. Such "turns of phrase" can enter into a Nation's collective memory and form the basis from which compassion and conviction are conveyed during important national discourse. Communities that unite across language barriers have no such robust basis for discourse. Here we describe a Multilingual Web prototype application that promotes appreciation of exceptional texts by non-native readers. The application allows dual column original/translation texts (in Open Office format) to be imported into the translator's browser, to be manually aligned for semantic correspondence, to be aligned with an audio reading, and then saved as HTML5 for subsequent presentation to non-native readers. We hope to provide a new way of experiencing exceptional texts (poetry, here) that transmits their significance without incurring extraneous distraction. We motivate, outline and illustrate our application in action.	Exceptional Texts On The Multilingual Web	NA:NA	2015
Khyathi Chandu Raghavi:Manoj Kumar Chinnakotla:Manish Shrivastava	Code-Mixing (CM) is defined as the embedding of linguistic units such as phrases, words, and morphemes of one language into an utterance of another language. CM is a natural phenomenon observed in many multilingual societies. It helps in speeding-up communication and allows wider variety of expression due to which it has become a popular mode of communication in social media forums like Facebook and Twitter. However, current Question Answering (QA) research and systems only support expressing a question in a single language which is an unrealistic and hard proposition especially for certain domains like health and technology. In this paper, we take the first step towards the development of a full-fledged QA system in CM language which is building a Question Classification (QC) system. The QC system analyzes the user question and infers the expected Answer Type (AType). The AType helps in locating and verifying the answer as it imposes certain type-specific constraints. In this paper, we present our initial efforts towards building a full-fledged QA system for CM language. We learn a basic Support Vector Machine (SVM) based QC system for English-Hindi CM questions. Due to the inherent complexities involved in processing CM language and also the unavailability of language processing resources such POS taggers, Chunkers, Parsers, we design our current system using only word-level resources such as language identification, transliteration and lexical translation. To reduce data sparsity and leverage resources available in a resource-rich language, in stead of extracting features directly from the original CM words, we translate them commonly into English and then perform featurization. We created an evaluation dataset for this task and our system achieves an accuracy of 63% and 45% in coarse-grained and fine-grained categories of the question taxanomy. The idea of translating features into English indeed helps in improving accuracy over the unigram baseline.	"Answer ka type kya he?": Learning to Classify Questions in Code-Mixed Language	NA:NA:NA	2015
Ali Hosseinzadeh Vahid:Piyush Arora:Qun Liu:Gareth J.F. Jones	Technical advances and its increasing availability, mean that Machine Translation (MT) is now widely used for the translation of search queries in multilingual search tasks. A number of free-to-use high-quality online MT systems are now available and, although imperfect in their translation behavior, are found to produce good performance in Cross-Language Information Retrieval (CLIR) applications. Users of these MT systems in CLIR tasks generally assume that they all behave similarly in CLIR applications, and the choice of MT system is often made on the basis of convenience. We present a set of experiments which compare the impact of applying two of the best known online systems, Google and Bing translation, for query translation across multiple language pairs and for two very different CLIR tasks. Our experiments show that the MT systems perform differently on average for different tasks and language pairs, but more significantly for different individual queries. We examine the differing translation behavior of these tools and seek to draw conclusions in terms of their suitability for use in different settings.	A Comparative Study of Online Translation Services for Cross Language Information Retrieval	NA:NA:NA:NA	2015
Evangelos Papalexakis:A. Seza Doğruöz	There are more multilingual speakers in the world than monolingual ones. Immigration is one of the key factors to bring speakers of different languages in contact with each other. In order to develop relevant policies and recommendations tailored according to the needs of immigrant communities, it is essential to understand the interactions between the users within and across sub-communities. Using a novel method (tensor analysis), we reveal the social network structure of an online multilingual discussion forum which hosts an immigrant community in the Netherlands. In addition to the network structure, we automatically discover and categorize monolingual and bilingual sub-communities and track their formation, evolution and dissolution over a long period of time.	Understanding Multilingual Social Networks in Online Immigrant Communities	NA:NA	2015
Silvia Rodríguez Vázquez	The Web is an open network accessed by people across countries, languages and cultures, irrespective of their functional diversity. Over the last two decades, interest about web accessibility issues has significantly increased among web professionals, but people with disabilities still encounter significant difficulties when browsing the Internet. In the particular case of blind users, the use of assistive technologies such as screen readers is key to navigate and interact with web content. Although research efforts made until now have led to a better understanding of visually-impaired users' browsing behavior and, hence, the definition of web design best practices for an improved user experience by this population group, the particularities of websites with multiple language versions have been mostly overlooked. This communication paper seeks to shed light on the major challenges faced by visually impaired users when accessing the multilingual web, as well as on why and how the web localization community should contribute to a more accessible web for all.	Exploring Current Accessibility Challenges in the Multilingual Web for Visually-Impaired Users	NA	2015
Gyöngyi Rózsa:Anita Komlodi:Peng Chu	Online searching is a central element of internet users' information behaviors. Searching is usually executed in a user's native language, but searching in English as a foreign language is often necessitated by the lack of content in languages that are underrepresented in Web content. This paper reports results from a study of searching in English as a foreign language and aims at understanding this particular group of users' behaviors. Searchers whose native language is not English may have to resort to queries in English in support of their information needs due to the lack or low quality of the web content in their own language. However, when searching for information in a foreign language, users face a unique set of challenges that are not present for native language searching. We studied this problem through qualitative research methods and report results from focus groups in this paper. The results reported in this paper describe typical problems foreign language searchers face, the differences in information-seeking behavior in English and in the participants' native language, and advice and ideas shared by the focus group participants about how to search effectively and efficiently in English.	Online Searching in English as a Foreign Language	NA:NA:NA	2015
Marko Grobelnik:James Hodson:Stefano Pacifico:Evan Sandhaus	NA	Session details: NewsWWW 2015	NA:NA:NA:NA	2015
Anastassia Fedyk	The importance of the media for individual and market behavior cannot be overstated. For example, a front-page article in the New York Times that mostly reprints information from six months prior can cause a company's stock price to jump by over 300%. To better understand the channels through which the media affects markets and the resulting implications for news production, we study how individuals process information in news. Do readers display a preference for news with a positive slant? Are consumers of news segregated based on the media outlets they favor? Do individuals recognize which news is novel, and which simply reprints old information? While these questions are grounded in fundamental human psychology, they are also inextricably linked to the rapidly changing technology of news production. With over a million stories a day passing through the Bloomberg terminal alone, the volume of data -- both on the content of news and the behavior of readers -- has skyrocketed. As a result, analysis of media production and consumption requires ever more sophisticated techniques for identifying the informational value of news and the behavioral patterns of its modern readers.	Supply and Demand: Propagation and Absorption of News	NA	2015
Fabon Dzogang:Thomas Lansdall-Welfare:Saatviga Sudhahar:Nello Cristianini	We study the task of learning the preferences of online readers of news, based on their past choices. Previous work has shown that it is possible to model this situation as a competition between articles, where the most appealing articles of the day are those selected by the most users. The appeal of an article can be computed from its textual content, and the evaluation function can be learned from training data. In this paper, we show how this task can benefit from an efficient algorithm, based on hashing representations, which enables it to be deployed on high intensity data streams. We demonstrate the effectiveness of this approach on four real world news streams, compare it with standard approaches, and describe a new online demonstration based on this technology.	Scalable Preference Learning from Data Streams	NA:NA:NA:NA	2015
Blaz Fortuna:Pat Moore:Marko Grobelnik	This paper presents an approach for recommending news articles on a large news portal. Focus is given to interpretability of the developed models, analysis of their performance, and deriving understanding of short and long-term user behavior on a news portal.	Interpreting News Recommendation Models	NA:NA:NA	2015
Sen Jia:Thomas Lansdall-Welfare:Nello Cristianini	Analysing the representation of gender in news media has a long history within the fields of journalism, media and communication. Typically this can be performed by measuring how often people of each gender are mentioned within the textual content of news articles. In this paper, we adopt a different approach, classifying the faces in images of news articles into their respective gender. We present a study on $885{,}573$ news articles gathered from the web, covering a period of four months between 19th October 2014 and 19th January 2015 from $882$ news outlets. Findings show that gender bias differs by topic, with Fashion and the Arts showing the least bias. Comparisons of gender bias by outlet suggest that tabloid-style news outlets may be less gender-biased than broadsheet-style ones, supporting previous results from textual content analysis of news articles.	Measuring Gender Bias in News Images	NA:NA:NA	2015
Aljaž Košmerlj:Evgenia Belyaeva:Gregor Leban:Marko Grobelnik:Blaž Fortuna	We present initial results of our effort to build an extensive and complete taxonomy of events described in news articles. By crawling Wikipedia's current events portal we identified nine top-level event types. Using articles referenced by the portal we built a event type classification model for news articles using lexical and semantic features and present a small-scale manual evaluation of its results. Results show that our model can accurately distinguish between event types but its coverage could still be significantly improved.	Towards a Complete Event Type Taxonomy	NA:NA:NA:NA:NA	2015
Will Radford:Daniel Tse:Joel Nothman:Ben Hachey:George Wright:James R. Curran:Will Cannings:Tim O'Keefe:Matthew Honnibal:David Vadas:Candice Loxley	We report on a four year academic research project to build a natural language processing platform in support of a large media company. The Computable News platform processes news stories, producing a layer of structured data that can be used to build rich applications. We describe the underlying platform and the research tasks that we explored building it. The platform supports a wide range of prototype applications designed to support different newsroom functions. We hope that this qualitative review provides some insight into the challenges involved in this type of project.	The Computable News project: Research in the Newsroom	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2015
Neha Gupta:Eunyee Koh:Lihong Li	NA	Session details: OOEW 2015	NA:NA:NA	2015
Scott Clark	We introduce Bayesian Global Optimization as an efficient way to optimize a system's parameters, when evaluating parameters is time-consuming or expensive. The adaptive sequential experimentation techniques described can be used to help tackle a myriad of problems including optimizing a system's click-through or conversion rate via online A/B testing, tuning parameters of a machine learning prediction method or expensive batch job, designing an engineering system or finding the optimal parameters of a real-world physical experiment. We explore different tools available for performing these tasks, including Yelp's MOE and SigOpt. We will present the motivation, implementation, and background of these tools. Applications and examples from industry and best practices for using the techniques will be provided.	Adaptive Sequential Experimentation Techniques for A/B Testing and Model Tuning	NA	2015
Alex Deng	As A/B testing gains wider adoption in the industry, more people begin to realize the limitations of the traditional frequentist null hypothesis statistical testing (NHST). The large number of search results for the query "Bayesian A/B testing" shows just how much the interest in the Bayesian perspective is growing. In recent years there are also voices arguing that Bayesian A/B testing should replace frequentist NHST and is strictly superior in all aspects. Our goal here is to clarify the myth by looking at both advantages and issues of Bayesian methods. In particular, we propose an objective Bayesian A/B testing framework for which we hope to bring the best from Bayesian and frequentist methods together. Unlike traditional methods, this method requires the existence of historical A/B test data to objectively learn a prior. We have successfully applied this method to Bing, using thousands of experiments to establish the priors.	Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments	NA	2015
Ramesh Johari	A/B testing is a hallmark of Internet services: from e-commerce sites to social networks to marketplaces, nearly all online services use randomized experiments as a mechanism to make better business decisions. Such tests are generally analyzed using classical frequentist statistical measures: p-values and confidence intervals. Despite their ubiquity, these reported values are computed under the assumption that the experimenter will not continuously monitor their test--in other words, there should be no repeated "peeking" at the results that affects the decision of whether to continue the test. On the other hand, one of the greatest benefits of advances in information technology, computational power, and visualization is precisely the fact that experimenters can watch experiments in progress, with greater granularity and insight over time than ever before. We ask the question: if users will continuously monitor experiments, then what statistical methodology is appropriate for hypothesis testing, significance, and confidence intervals? We present recent work addressing this question. In particular, building from results in sequential hypothesis testing, we present analogues of classical frequentist statistical measures that are valid even though users are continuously monitoring the results.	Can I Take a Peek?: Continuous Monitoring of Online A/B Tests	NA	2015
Filip Radlinski	Online evaluation allows information retrieval systems to be assessed based on how real users respond to search results presented. Compared with traditional offline evaluation based on manual relevance assessments, online evaluation is particularly attractive in settings where reliable assessments are difficult or too expensive to obtain. However, the successful use of online evaluation requires the right metrics to be used, as real user behaviour is often difficult to interpret. I will present interleaving, a sensitive online evaluation approach that creates paired comparisons for every user query, and compare it with alternative A/B online evaluation approaches. I will also show how interleaving can be parameterized to create a family of evaluation metrics that can be chosen to best match the goals of an evaluation.	Online Search Evaluation with Interleaving	NA	2015
Olivier Chapelle	Click-through rates and conversion rates are two core machine learning problems in online advertising. The evaluation of such systems is often based on traditional supervised learning metrics that ignore how the predictions are used. These predictions are in fact part of bidding systems in online advertising auctions. We present here an empirical evaluation of a metric that is specifically tailored for auctions in online advertising and show that it correlates better than standard metrics with A/B test results.	Offline Evaluation of Response Prediction in Online Advertising Auctions	NA	2015
Alex Deng	As A/B testing gains wider adoption in the industry, more people begin to realize the limitations of the traditional frequentist null hypothesis statistical testing (NHST). The large number of search results for the query ``Bayesian A/B testing'' shows just how much the interest in the Bayesian perspective is growing. In recent years there are also voices arguing that Bayesian A/B testing should replace frequentist NHST and is strictly superior in all aspects. Our goal here is to clarify the myth by looking at both advantages and issues of Bayesian methods. In particular, we propose an objective Bayesian A/B testing framework for which we hope to bring the best from Bayesian and frequentist methods together. Unlike traditional methods, this method requires the existence of historical A/B test data to objectively learn a prior. We have successfully applied this method to Bing, using thousands of experiments to establish the priors.	Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments	NA	2015
Lihong Li:Shunbao Chen:Jim Kleban:Ankur Gupta	Optimizing an interactive system against a predefined online metric is particularly challenging, especially when the metric is computed from user feedback such as clicks and payments. The key challenge is the counterfactual nature: in the case of Web search, any change to a component of the search engine may result in a different search result page for the same query, but we normally cannot infer reliably from search log how users would react to the new result page. Consequently, it appears impossible to accurately estimate online metrics that depend on user feedback, unless the new engine is actually run to serve live users and compared with a baseline in a controlled experiment. This approach, while valid and successful, is unfortunately expensive and time-consuming. In this paper, we propose to address this problem using causal inference techniques, under the contextual-bandit framework. This approach effectively allows one to run potentially many online experiments offline from search log, making it possible to estimate and optimize online metrics quickly and inexpensively. Focusing on an important component in a commercial search engine, we show how these ideas can be instantiated and applied, and obtain very promising results that suggest the wide applicability of these techniques.	Counterfactual Estimation and Optimization of Click Metrics in Search Engines: A Case Study	NA:NA:NA:NA	2015
Tobias Schnabel:Adith Swaminathan:Thorsten Joachims	We address the problem of assessing the quality of a ranking system (e.g., search engine, recommender system, review ranker) given a fixed budget for collecting expert judgments. In particular, we propose a method that selects which items to judge in order to optimize the accuracy of the quality estimate. Our method is not only efficient, but also provides estimates that are unbiased --- unlike common approaches that tend to underestimate performance or that have a bias against new systems that are evaluated re-using previous relevance scores.	Unbiased Ranking Evaluation on a Budget	NA:NA:NA	2015
Adith Swaminathan:Thorsten Joachims	We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem through propensity scoring. Next, we derive generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. Using the CRM principle, we derive a new learning algorithm -- Policy Optimizer for Exponential Models (POEM) -- for structured output prediction. We evaluate POEM on several multi-label classification problems and verify that its empirical performance supports the theory.	Counterfactual Risk Minimization	NA:NA	2015
Pengyuan Wang:Wei Sun:Dawei Yin	We present a causal inference framework for evaluating the impact of advertising treatments. Our framework is computationally efficient by employing a tree structure that specifies the relationship between user characteristics and the corresponding ad treatment. We illustrate the applicability of our proposal on a novel advertising effectiveness study: finding the best ad size on different mobile devices in order to maximize the success rates. The study shows a surprising phenomenon that a larger mobile device does not need a larger ad. In particular, the 300*250 ad size is universally good for all the mobile devices, regardless of the mobile device size.	What Size Should A Mobile Ad Be?	NA:NA:NA	2015
Kalina Bontcheva:Maria Liakata:Rob Procter:Arno Scharl	NA	Session details: RDSM 2015	NA:NA:NA:NA	2015
Darren Scott Appling:Erica J. Briscoe:Clayton J. Hutto	Although a large body of work has previously investigated various cues predicting deceptive communications, especially as demonstrated through written and spoken language (e.g., [30]), little has been done to explore predicting kinds of de- ception. We present novel work to evaluate the use of textual cues to discriminate between deception strategies (such as exaggeration or falsification), concentrating on intention- ally untruthful statements meant to persuade in a social media context. We conduct human subjects experimenta- tion wherein subjects were engaged in a conversational task and then asked to label the kind(s) of deception they employed for each deceptive statement made. We then develop discriminative models to understand the difficulty between choosing between one and several strategies. We evaluate the models using precision and recall for strategy prediction among 4 deception strategies based on the most relevant psycholinguistic, structural, and data-driven cues. Our single strategy model results demonstrate as much as a 58% increase over baseline (random chance) accuracy and we also find that it is more difficult to predict certain kinds of de- ception than others.	Discriminative Models for Predicting Deception Strategies	NA:NA:NA	2015
Jun Ito:Jing Song:Hiroyuki Toda:Yoshimasa Koike:Satoshi Oyama	With the fast development of Social Networking Services (SNS) such as Twitter, which enable users to exchange short messages online, people can get information not only from the traditional news media but also from the masses of SNS users. However, SNS users sometimes propagate spurious or misleading information, so an effective way to automatically assess the credibility of information is required. In this paper, we propose methods to assess information credibility on Twitter, methods that utilize the "tweet topic" and "user topic" features derived from the Latent Dirichlet Allocation (LDA) model. We collected two thousand tweets labeled by seven annotators each, and designed effective features for our classifier on the basis of data analysis results. An experiment we conducted showed a 3% improvement in Area Under Curve (AUC) scores compared with existing methods, leading us to conclude that using topical features is an effective way to assess tweet credibility.	Assessment of Tweet Credibility with LDA Features	NA:NA:NA:NA:NA	2015
Stephen Mayhew:Dan Roth	Trustworthiness is a field of research that seeks to estimate the credibility of information by using knowledge of the source of the information. The most interesting form of this problem is when different pieces of information share sources, and when there is conflicting information from different sources. This model can be naturally represented as a bipartite graph. In order to understand this data well, it is important to have several methods of exploring it. A good visualization can help to understand the problem in a way that no simple statistics can. This paper defines several desiderata for a "good" visualization and presents three different visualization methods for trustworthiness graphs. The first visualization method is simply a naive bipartite layout, which is infeasible in nearly all cases. The second method is a physics-based graph layout that reveals some interesting and important structure of the graph. The third method is an orthogonal approach based on the adjacency matrix representation of a graph, but with many improvements that give valuable insights into the structure of the trustworthiness graph. We present interactive web-based software for the third form of visualization.	Visualization of Trustworthiness Graphs	NA:NA	2015
Richard McCreadie:Craig Macdonald:Iadh Ounis	When a significant event occurs, many social media users leverage platforms such as Twitter to track that event. Moreover, emergency response agencies are increasingly looking to social media as a source of real-time information about such events. However, false information and rumours are often spread during such events, which can influence public opinion and limit the usefulness of social media for emergency management. In this paper, we present an initial study into rumour identification during emergencies using crowdsourcing. In particular, through an analysis of three tweet datasets relating to emergency events from 2014, we propose a taxonomy of tweets relating to rumours. We then perform a crowdsourced labeling experiment to determine whether crowd assessors can identify rumour-related tweets and where such labeling can fail. Our results show that overall, agreement over the tweet labels produced were high (0.7634 Fleiss Kappa), indicating that crowd-based rumour labeling is possible. However, not all tweets are of equal difficulty to assess. Indeed, we show that tweets containing disputed/controversial information tend to be some of the most difficult to identify.	Crowdsourced Rumour Identification During Emergencies	NA:NA:NA	2015
Vlad Sandulescu:Martin Ester	Online reviews have increasingly become a very important resource for consumers when making purchases. Though it is becoming more and more difficult for people to make well-informed buying decisions without being deceived by fake reviews. Prior works on the opinion spam problem mostly considered classifying fake reviews using behavioral user patterns. They focused on prolific users who write more than a couple of reviews, discarding one-time reviewers. The number of singleton reviewers however is expected to be high for many review websites. While behavioral patterns are effective when dealing with elite users, for one-time reviewers, the review text needs to be exploited. In this paper we tackle the problem of detecting fake reviews written by the same person using multiple names, posting each review under a different name. We propose two methods to detect similar reviews and show the results generally outperform the vectorial similarity measures used in prior works. The first method extends the semantic similarity between words to the reviews level. The second method is based on topic modeling and exploits the similarity of the reviews topic distributions using two models: bag-of-words and bag-of-opinion-phrases. The experiments were conducted on reviews from three different datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset (800 reviews).	Detecting Singleton Review Spammers Using Semantic Similarity	NA:NA	2015
Marcella Tambuscio:Giancarlo Ruffo:Alessandro Flammini:Filippo Menczer	spread of misinformation, rumors and hoaxes. The goal of this work is to introduce a simple modeling framework to study the diffusion of hoaxes and in particular how the availability of debunking information may contain their diffusion. As traditionally done in the mathematical modeling of information diffusion processes, we regard hoaxes as viruses: users can become infected if they are exposed to them, and turn into spreaders as a consequence. Upon verification, users can also turn into non-believers and spread the same attitude with a mechanism analogous to that of the hoax-spreaders. Both believers and non-believers, as time passes, can return to a susceptible state. Our model is characterized by four parameters: spreading rate, gullibility, probability to verify a hoax, and that to forget one's current belief. Simulations on homogeneous, heterogeneous, and real networks for a wide range of parameters values reveal a threshold for the fact-checking probability that guarantees the complete removal of the hoax from the network. Via a mean field approximation, we establish that the threshold value does not depend on the spreading rate but only on the gullibility and forgetting probability. Our approach allows to quantitatively gauge the minimal reaction necessary to eradicate a hoax.	Fact-checking Effect on Viral Hoaxes: A Model of Misinformation Spread in Social Networks	NA:NA:NA:NA	2015
Xing Zhou:Juan Cao:Zhiwei Jin:Fei Xie:Yu Su:Dafeng Chu:Xuehui Cao:Junqiang Zhang	In this paper, we propose a novel framework for real-time news certification. Traditional methods detect rumors on message-level and analyze the credibility of one tweet. However, in most occasions, we only remember the keywords of an event and it's hard for us to completely describe an event in a tweet. Based on the keywords of an event, we gather related microblogs through a distributed data acquisition system which solves the real-time processing needs. Then, we build an ensemble model that combine user-based, propagation-based and content-based model. The experiments show that our system can give a response at 35 seconds on average per query which is critical for real-time system. Most importantly, our ensemble model boost the performance. We also offer some important information such as key users, key microblogs and timeline of events for further investigation of an event.Our system is already deployed in the Xihua News Agency for half a year. To the best of our knowledge, this is the first real-time news certification system for verifying social media contents.	Real-Time News Cer tification System on Sina Weibo	NA:NA:NA:NA:NA:NA:NA:NA	2015
Francesco Osborne:Silvio Peroni:Jun Zhao	NA	Session details: SAVE-SD 2015	NA:NA:NA	2015
Paul Groth	Over the past several years, we have seen an explosion in the number of tools and services that enable scholars to improve their personal productivity whether it is socially enabled reference managers or cloud-hosted experimental environments. However, we have yet to see a step-change in the productivity of the system of scholarship as a whole. While there are certainly broader social reasons for this, in this talk I argue that we are just now at a technical position to create radical change in how scholarship is performed. Specifically, I will discuss how recent advances in machine reading, developments in open data and explicit social networks, can be used to create scholarly knowledge graphs. These graphs can connect the underlying intellectual corpus with ongoing discourse allowing the development of algorithms that hypothesize, filter and reflect alongside humans.	Increasing the Productivity of Scholarship: The Case for Knowledge Graphs	NA	2015
Luigi Di Caro:Mario Cataldi:Myriam Lamolle:Claudio Schifanella	In the last decades, many measures and metrics have been proposed with the goal of automatically providing quantitative rather than qualitative indications over researchers' academic productions. However, when evaluating a researcher, most of the commonly-applied measures do not consider one of the key aspect of every research work: the collaborations among researchers and, more specifically, the impact that each co-author has on the scientific production of another. In fact, in an evaluation process, some co-authored works can unconditionally favor researchers working in competitive research environments surrounded by experts able to lead high-quality research projects, where state-of-the-art measures usually fail in trying to distinguish co-authors from their pure publication history. In the light of this, instead of focusing on a pure quantitative/qualitative evaluation of curricula, we propose a novel temporal model for formalizing and estimating the dependence of a researcher on individual collaborations, over time, in surrounding communities. We then implemented and evaluated our model with a set of experiments on real case scenarios and through an extensive user study.	It is not What but Who you Know: A Time-Sensitive Collaboration Impact Measure of Researchers in Surrounding Communities	NA:NA:NA:NA	2015
Angelo Di Iorio:Raffaele Giannella:Francesco Poggi:Fabio Vitali	Bibliographies are fundamental tools for research communities. Besides the obvious uses as connection to previous research, citations are also widely used for evaluation purposes: the productivity of researchers, departments and universities is increasingly measured by counting their citations. Unfortunately, citations counters are just rough indicators: a deeper knowledge of individual citations -- where, when, by whom and why -- improves research evaluation tasks and supports researchers in their daily activity. Yet, such information is mostly hidden within repositories of scholarly papers and is still difficult to find, navigate and make use of. In this paper, we present a novel tool for exploring scientific articles through their citations. The environment is built on top of a rich citation network, encoded as a LOD, and includes a user-friendly interface to access, filter and highlight information about bibliographic data.	Exploring Bibliographies for Research-related Tasks	NA:NA:NA:NA	2015
Anna Lisa Gentile:Maribel Acosta:Luca Costabello:Andrea Giovanni Nuzzolese:Valentina Presutti:Diego Reforgiato Recupero	In this paper we describe Conference Live, a semantic Web application to browse conference data. Conference Live is a Web and mobile application based on conference data from the Semantic Web Dog Food server, which provides facilities to browse papers and authors at a specific conference. Available data for the specific conference is enriched with social features (e.g. integrated Twitter accounts of paper authors), scheduling features (calendar information are attached for paper presentations and social events), the possibility to check and add feedback to each paper and to vote for papers, if the conference includes sessions where participants can vote, as it is popular e.g. for poster sessions. As use case we report on the usage of the application at the Extended Semantic Web Conference (ESWC) in May 2014.	Conference Live: Accessible and Sociable Conference Semantic Data	NA:NA:NA:NA:NA:NA	2015
Patrick Golden:Ryan Shaw	The PeriodO period gazetteer collects definitions of time periods made by archaeologists and other historical scholars. In constructing the gazetteer, we sought to make period definitions parsable and comparable by computers while also retaining the broader scholarly context in which they were conceived. Our approach resulted in a dataset of period definitions and their provenances that resemble what data scientists working in the e-science domain have dubbed "nanopublications." In this paper we describe the origin and goals of nanopublications, provide an overview of the design and implementation of a database of period definitions, and highlight the similarities and differences between the two.	Period Assertion as Nanopublication: The PeriodO Period Gazetteer	NA:NA	2015
Hugo Mougard:Matthieu Riou:Colin de la Higuera:Solen Quiniou:Olivier Aubert	This paper investigates the possibilities offered by the more and more common availability of scientific video material. In particular it investigates how to best study research results by combining recorded talks and their corresponding scientific articles. To do so, it outlines desired properties of an interesting e-research system based on cognitive considerations and considers related issues. This design work is completed by the introduction of two prototypes.	The Paper or the Video: Why Choose?	NA:NA:NA:NA:NA	2015
Bahar Sateli:René Witte	Finding research literature pertaining to a task at hand is one of the essential tasks that scientists face on daily basis. Standard information retrieval techniques allow to quickly obtain a vast number of potentially relevant documents. Unfortunately, the search results then require significant effort for manual inspection, where we would rather select relevant publications based on more fine-grained, semantically rich queries involving a publication's contributions, methods, or application domains. We argue that a novel combination of three distinct methods can significantly advance this vision: (i) Natural Language Processing (NLP) for Rhetorical Entity (RE) detection; (ii) Named Entity (NE) recognition based on the Linked Open Data (LOD) cloud; and (iii) automatic generation of RDF triples for both NEs and REs using semantic web ontologies to interconnect them. Combined in a single workflow, these techniques allow us to automatically construct a knowledge base that facilitates numerous advanced use cases for managing scientific documents.	What's in this paper?: Combining Rhetorical Entities with Linked Open Data for Semantic Literature Querying	NA:NA	2015
Ilaria Tiddi:Mathieu d'Aquin:Enrico Motta	In this paper we exploit knowledge from Linked Data to ease the process of analysing scholarly data. In the last years, many techniques have been presented with the aim of analysing such data and revealing new, unrevealed knowledge, generally presented in the form of ``patterns". However, the discovered patterns often still require human interpretation to be further exploited, which might be a time and energy consuming process. Our idea is that the knowledge shared within Linked Data can actuality help and ease the process of interpreting these patterns. In practice, we show how research communities obtained through standard network analytics techniques can be made more understandable through exploiting the knowledge contained in Linked Data. To this end, we apply our system Dedalo that, by performing a simple Linked Data traversal, is able to automatically label clusters of words, corresponding to topics of the different communities.	Using Linked Data Traversal to Label Academic Communities	NA:NA:NA	2015
Anna Tordai	With the rise of digital publishing and open access it has become increasingly important for publishers to store information about ownership and licensing of published works in a robust way. In a fast moving environment Elsevier, a leading science publisher, recognizes the importance of sound models underlying its data. In this paper, we describe a data model for copyright and licensing used by Elsevier for capturing elements of copyright. We explain some of the rationale behind the model and provide examples of frequently occurring cases in terms of the model.	A Model for Copyright and Licensing: Elsevier's Copyright Model	NA	2015
Theresa Velden:Shiyan Yan:Kan Yu:Carl Lagoze	The increasing online availability of scholarly corpora promises unprecedented opportunities for visualizing and studying scholarly communities. We seek to leverage this with a mixed-method approach that integrates network analysis of features of the online corpora with ethnographic studies of the communities that produce them. In our development of tools and visualizations we seek to support the going back and forth between views of community structures and the perceptions and research trajectories of individual researchers and research groups. We here present results from tracking the temporal evolution of community structures within a research specialty. We explore how the temporal evolution of these maps can be used to provide insights into the historical evolution of a field as well as extract more accurate snapshots of the community structures at a given point in time. We are currently conducting qualitative interviews with experts in this research specialty to assess the validity of the maps.	Mapping The Evolution of Scientific Community Structures in Time	NA:NA:NA:NA	2015
Yan Wu:Srinivasan Venkatramanan:Dah Ming Chiu	Academic publication metadata can be used to analyze the collaboration, productivity and hot topic trends of a research community. Recently, it is shown that authors with uninterrupted and continuous presence (UCP) over a time window, though small in number (about 1%), amass the majority of significant and high-influence academic output. We adopt the UCP metric to retrieve the most active authors in the Computer Science (CS) community over different time windows in the past $50$ years, and use them to analyze collaboration, productivity and topic trends. We show that the UCP authors are representative of the overall population; the community is increasingly moving in the direction of Team Research (as opposed to Soloist or Mentor-mentee research), with increased level and degree of collaboration; and the research topics become increasingly inter-related. By focusing on the UCP authors, we can more easily visualize these trends.	Research Collaboration and Topic Trends in Computer Science: An Analysis Based on UCP Authors	NA:NA:NA	2015
Alessia Bardi:Paolo Manghi	Enhanced Publication Information Systems (EPISs) are information systems devised for the management of enhanced publications (EP), i.e. digital publications enriched with (links to) other research outcomes such as data, processing workflows, software. Today, EPISs are typically realised with a "from scratch" approach that entails non-negligible implementation and maintenance costs. This work argues for a more systemic approach to narrow those costs and presents the notion of Enhanced Publication Management Systems, software frameworks that support the realisation of EPISs by providing developers with EP-oriented tools and functionalities.	Enhanced Publication Management Systems: A Systemic Approach Towards Modern Scientific Communication	NA:NA	2015
Laurens De Vocht:Selver Softic:Anastasia Dimou:Ruben Verborgh:Erik Mannens:Martin Ebner:Rik Van de Walle	The various ways of interacting with social media, web collaboration tools, co-authorship and citation networks for scientific and research purposes remain distinct. In this paper, we propose a solution to align such information. We particularly developed an exploratory visualization of research networks. The result is a scholar centered, multi-perspective view of conferences and people based on their collaborations and online interactions. We measured the relevance and user acceptance of this type of interactive visualization. Preliminary results indicate a high precision both for recognized people and conferences. The majority in a group of test-users responded positively to a set of statements about the acceptance.	Visualizing Collaborations and Online Social Interactions at Scientific Conferences for Scholarly Networking	NA:NA:NA:NA:NA:NA:NA	2015
Fajar J. Ekaputra:Marta Sabou:Estefanía Serral:Stefan Biffl	Complementary to managing bibliographic information as done by digital libraries, the management of concrete research objects (e.g., experimental workflows, design patterns) is a pre-requisite to foster collaboration and reuse of research results. In this paper we describe the case of the Empirical Software Engineering domain, where researchers use systematic literature reviews (SLRs) to conduct and report on literature studies. Given their structured nature, the outputs of such SLR processes are a special and complex type of research object. Since performing SLRs is a time consuming process, it is highly desirable to enable sharing and reuse of the complex knowledge structures produced through SLRs. This would enable, for example, conducting new studies that build on the findings of previous studies. To support collaborative features necessary for multiple research groups to share and reuse each other's work, we hereby propose a solution approach that is inspired by software engineering best-practices and is implemented using Semantic Web technologies.	Collaborative Exchange of Systematic Literature Review Results: The Case of Empirical Software Engineering	NA:NA:NA:NA	2015
Daniel Garijo:Nandana Mihindukulasooriya:Oscar Corcho	In this demo we present LDP4ROs, a prototype implementation that allows creating, browsing and updating Research Objects (ROs) and their contents using typical HTTP operations. This is achieved by aligning the RO model with the W3C Linked Data Platform (LDP).	LDP4ROs: Managing Research Objects with the W3C Linked Data Platform	NA:NA:NA	2015
Theodoros Giannakopoulos:Ioannis Foufoulas:Eleftherios Stamatogiannakis:Harry Dimitropoulos:Natalia Manola:Yannis Ioannidis	Authors of scientific publications and books use images to present a wide spectrum of information. Despite the richness of the visual content of scientific publications the figures are usually not taken into consideration in the context of text mining methodologies towards the automatic indexing and retrieval of scientific corpora. In this work, we present a system for automatic categorization of figures from scientific literature to a set of predefined classes. We have employed a wide range of visual features that achieve high discrimination ability between the adopted classes. A real-world dataset has been compiled and annotated in order to train and evaluate the proposed method using three different classification schemata.	Visual-Based Classification of Figures from Scientific Literature	NA:NA:NA:NA:NA:NA	2015
Tobias Kuhn	As a response to the trends of the increasing importance of computational approaches and the accelerating pace in science, I propose in this position paper to establish the concept of "science bots" that autonomously perform programmed tasks on input data they encounter and immediately publish the results. We can let such bots participate in a reputation system together with human users, meaning that bots and humans get positive or negative feedback by other participants. Positive reputation given to these bots would also shine on their owners, motivating them to contribute to this system, while negative reputation will allow us to filter out low-quality data, which is inevitable in an open and decentralized system.	Science Bots: A Model for the Future of Scientific Computation?	NA	2015
Aziz Mohaisen:Bruno Goncalves:Yanhua Li	NA	Session details: SIMPLEX 2015	NA:NA:NA	2015
Nishanth Sastry	The user generated content revolution has created a glut of multimedia content online -- from Flickr to Facebook, new images are being made available for public consumption everyday. In this talk, we will first explore how, on sites such as Pinterest, users are bringing order to this burgeoning collection by manually curating collections of images in ways that are highly personalised and relevant to their own use. We will then discuss the phenomenon of social bootstrapping, whereby existing mature social networks such as Facebook are helping bootstrap engaged communities of content curators on external sites such as Pinterest. Finally, we will demonstrate how the manual effort involved in curation can be amplified using a unique human-machine collaboration: By treating the curation efforts of a subset of users on Pinterest as a distributed human computation over a low-dimensional approximation of the content corpus, we derive simple yet powerful signals, which, when combined with image-related features drawn from state-of-the-art deep learning techniques, allow us to automatically and accurately populate the personalised curated collections of all other users.	Predicting Pinterest: Organising the World's Images with Human-machine Collaboration	NA	2015
Bruno Ribeiro	A new era of data analytics of online social networks promises tremendous high-impact societal, business, and healthcare applications. As more users join online social networks, the data available for analysis and forecast of human social and collective behavior grows at an incredible pace. The first part of this talk introduces an apparent paradox, where larger online social networks entail more user data but also less analytic and forecasting capabilities [7]. More specifically, the paradox applies to forecasting properties of network processes such as network cascades, showing that in some scenarios unbiased long term forecasting becomes increasingly inaccurate as the network grows but, paradoxically, short term forecasting -- such as the predictions in Cheng et al. [2] and Ribeiro et al. [7] -- improves with network size. We discuss the theoretic foundations of this paradox and its connections with known information theoretic measures such as Shannon capacity. We also discuss the implications of this paradox on the scalability of big data applications and show how information theory tools -- such as Fisher information [3,8] -- can be used to design more accurate and scalable methods for network analytics [6,8,10]. The second part of the talk focuses on how these results impact our ability to perform network analytics when network data is only available through crawlers and the complete network topology is unknown [1,4,5,9].	Challenges of Forecasting and Measuring a Complex Networked World	NA	2015
Yanhua Li:Zhi-Li Zhang	Complex networks are becoming indispensable parts of our lives. The Internet, wireless (cellular) networks, online social networks, and transportation networks are examples of some well-known complex networks around us. These networks generate an immense range of big data: weblogs, social media, the Internet traffic, which have increasingly drawn attentions from the computer science research community to explore and investigate the fundamental properties of, and improve the user experiences on, these complex networks. This work focuses on understanding complex networks based on the graph spectrum, namely, developing and applying spectral graph theories and models for understanding and employing versatile and oblivious network information -- asymmetrical characteristics of the wireless transmission channels, multiplex social relations, e.g., trust and distrust relations, etc -- in solving various application problems, such as estimating transmission cost in wireless networks, Internet traffic engineering, and social influence analysis in social networks.	Understanding Complex Networks Using Graph Spectrum	NA:NA	2015
Golshan Golnari:Yanhua Li:Zhi-Li Zhang	Reachability is crucial to many network operations in various complex networks. More often than not, however, it is not sufficient simply to know whether a source node s can reach a target node t in the network. Additional information associated with reachability such as how long or how many possible ways node s may take to reach node t. In this paper we analyze another piece of important information associated with reachability -- which we call pivotality. Pivotality captures how pivotal a role that a node k or a subset of nodes S may play in the reachability from node s to node t in a given network. We propose two important metrics, the avoidance and transit hitting times, which extend and generalize the classical notion of hitting times. We show these metrics can be computed from the fundamental matrices associated with the appropriately defined random walk transition probability matrices and prove that the classical hitting time from a source to a target can be decomposed into the avoidance and transit hitting times with respect to any third node. Through simulated and real-world network examples, we demonstrate that these metrics provide a powerful ranking tool for the nodes based on their pivotality in the reachability.	Pivotality of Nodes in Reachability Problems Using Avoidance and Transit Hitting Time Metrics	NA:NA:NA	2015
Ah Reum Kang:Juyong Park:Jina Lee:Huy Kang Kim	Among many types of online games, Massively Multiplayer Online Role Playing Games (MMORPGs) provide players with the most realistic gaming experience inspired by the real, offline world. In particular, much stress is put upon socializing and collaboration with others as a condition for one's success, just as in real life. An advantage of studying MMORPGs is that since all actions are recorded, we can observe phenomena that are hard to observe in real life. For instance, we could observe how the all-important collaboration between people come into being, evolve, and eventually die out from the data to gain valuable insights to the group dynamics. In this paper, we analyzed the successes and failures of the online game groups in two different MMORPG, ArcheAge of XLGames, Inc. and Aion of NCsoft, Inc.. We find that there exist factors that influence the dynamics of group growth common to the games regardless of the games' maturity.	Rise and Fall of Online Game Groups: Common Findings on Two Different Games	NA:NA:NA:NA	2015
Akshay Minocha:Navjyoti Singh:Arjit Srivastava	We construct a complex citation network of a subset of Indian Constitutional Articles and the legal judgments that invoke them. We describe, how this dataset is constructed and also introduce the term of dispersion from network science related to social networks, in the context of legal relevance. Our research shows that dispersion is a decisive structural feature to show the importance of relevant legal judgments and landmark decisions. Our method provides similarity information about the document in question, which otherwise remains undetected by standard citation metrics.	Finding Relevant Indian Judgments using Dispersion of Citation Network	NA:NA:NA	2015
Osnat Mokryn:Alexey Reznik	In this paper, we present a hypothesis that power laws are found only in datasets sampled from a static data, in which each and every item has gained its maximal importance and is not in the process of changing it during the sampling period. We motivate our hypothesis by examining languages, and word-ranking distribution as it appears in books, and in the Bible. To demonstrate the validity of our hypothesis, we experiment with the Wikipedia edit collaboration network. We find that the dataset fits a skewed distribution. Next, we identify its dynamic part. We then show that when the modified part is removed from the obtained dataset, the remaining static part exhibits a good fit to a power law distribution.	On Skewed Distributions and Straight Lines: A Case Study on the Wiki Collaboration Network	NA:NA	2015
Matthew Saltz:Arnau Prat-Pérez:David Dominguez-Sal	Community detection has become an extremely active area of research in recent years, with researchers proposing various new metrics and algorithms to address the problem. Recently, the Weighted Community Clustering (WCC) metric was proposed as a novel way to judge the quality of a community partitioning based on the distribution of triangles in the graph, and was demonstrated to yield superior results over other commonly used metrics like modularity. The same authors later presented a parallel algorithm for optimizing WCC on large graphs. In this paper, we propose a new distributed, vertex-centric algorithm for community detection using the WCC metric. Results are presented that demonstrate the algorithm's performance and scalability on up to 32 worker machines and real graphs of up to 1.8 billion edges. The algorithm scales best with the largest graphs, finishing in just over an hour for the largest graph, and to our knowledge, it is the first distributed algorithm for optimizing the WCC metric.	Distributed Community Detection with the WCC Metric	NA:NA:NA	2015
Shou-De Lin:Lun-Wei Lu:Cheng-Te Li:Erik Cambria	NA	Session details: SocialNLP 2015	NA:NA:NA:NA	2015
Nicholas Jing Yuan	In recent years, with the rapid development of positioning technologies, online social networks, sensors and smart devices, large scale human behavioral data are now readily available. The growing availability of such behavioral data provides us unprecedented opportunities to gain more in depth understanding of users in both the physical world and cyber world, especially in online social networks. In this talk, I will introduce our recent research efforts in social and urban mining based on large-scale human behavioral datasets showcased by two projects: 1) LifeSpec: Modeling the spectrum of urban lifestyles based on heterogeneous online social network data. 2) L2P: Inferring demographic attributes from location check-ins.	Mining Social and Urban Big Data	NA	2015
Jinlong Guo:Yujie Lu:Tatsunori Mori:Catherine Blake	This paper presents a new model for the task of contrastive opinion summarization (COS) particularly for controversial issues. Traditional COS methods, which mainly rely on sentence similarity measures are not sufficient for a complex controversial issue. We therefore propose an Expert-Guided Contrastive Opinion Summarization (ECOS) model. Compared to previous methods, our model can (1) integrate expert opinions with ordinary opinions from social media and (2) better align the contrastive arguments under the guidance of expert prior opinion. We create a new data set about a complex social issue with "sufficient" controversy and experimental results on this data show that the proposed model are effective for (1) producing better arguments summary in understanding a controversial issue and (2) generating contrastive sentence pairs.	Expert-Guided Contrastive Opinion Summarization for Controversial Issues	NA:NA:NA:NA	2015
Kamel Nebhi:Kalina Bontcheva:Genevieve Gorrell	The rapid proliferation of microblogs such as Twitter has resulted in a vast quantity of written text becoming available that contains interesting information for NLP tasks. However, the noise level in tweets is so high that standard NLP tools perform poorly. In this pa- per, we present a statistical truecaser for tweets using a 3-gram language model built with truecased newswire texts and tweets. Our truecasing method shows an improvement in named entity recognition and part-of-speech tagging tasks.	ResToRinG CaPitaLiZaTion in #TweeTs	NA:NA:NA	2015
Ervin Tasnádi:Gábor Berend	In this paper, we introduce a supervised machine learning framework for the link prediction problem. The social network we conducted our empirical evaluation on originates from the restaurant review portal, yelp.com. The proposed framework not only uses the structure of the social network to predict non-existing edges in it, but also makes use of further graphs that were constructed based on implicit information provided in the dataset. The implicit information we relied on includes the language use of the members of the social network and their ratings with respect the businesses they reviewed. Here, we also investigate the possibility of building supervised learning models to predict social links without relying on features derived from the structure of the social network itself, but based on such implicit information alone. Our empirical results not only revealed that the features derived from different sources of implicit information can be useful on their own, but also that incorporating them in a unified framework has the potential to improve classification results, as the different sources of implicit information can provide independent and useful views about the connectedness of users.	Supervised Prediction of Social Network Links Using Implicit Sources of Information	NA:NA	2015
Wendy Hall:David de Roure:Nigel Shadbolt:Elena Simperl:Thanassis Tiropanis:Matthew Weber	NA	Session details: SOCM 2015	NA:NA:NA:NA:NA:NA	2015
Marco Brambilla:Stefano Ceri:Andrea Mauri:Riccardo Volonterio	Crowdsourcing applications are becoming widespread; they cover very different scenarios, including opinion mining, multimedia data annotation, localised information gathering, marketing campaigns, expert response gathering, and so on. The quality of the outcome of these applications depends on different design parameters and constraints, and it is very hard to judge about their combined effects without doing some experiments; on the other hand, there are no experiences or guidelines that tell how to conduct experiments, and thus these are often conducted in an ad-hoc manner, typically through adjustments of an initial strategy that may converge to a parameter setting which is quite different from the best possible one. In this paper we propose a comparative, explorative approach for designing crowdsourcing tasks. The method consists of defining a representative set of execution strategies, then execute them on a small dataset, then collect quality measures for each candidate strategy, and finally decide the strategy to be used with the complete dataset.	An Explorative Approach for Crowdsourcing Tasks Design	NA:NA:NA:NA	2015
Vanilson Burégio:Kellyton Brito:Nelson Rosa:Misael Neto:Vinícius Garcia:Silvio Meira	Government initiatives to open data to the public are becoming increasingly popular every day. The vast amount of data made available by government organizations yields interesting opportunities and challenges - both socially and technically. In this paper, we propose a social machine-oriented architecture as a way to extend the power of open data and create the basis to derive government as a social machine (Gov-SM). The proposed Gov-SM combines principles from existing architectural patterns and provides a platform of specialized APIs to enable the creation of several other social-technical systems on top of it. Based on some implementation experiences, we believe that deriving government as a social machine can, in more than one sense, collaborate to fully integrate users, developers and crowd in order to participate in and solve a multitude of governmental issues and policy.	Towards Government as a Social Machine	NA:NA:NA:NA:NA:NA	2015
Markus Luczak-Roesch:Ramine Tinati:Nigel Shadbolt	This paper is an attempt to lay out foundations for a general theory of coincidence in information spaces such as the World Wide Web, expanding on existing work on bursty structures in document streams and information cascades. We elaborate on the hypothesis that every resource that is published in an information space, enters a temporary interaction with another resource once a unique explicit or implicit reference between the two is found. This thought is motivated by Erwin Shroedingers notion of entanglement between quantum systems. We present a generic information cascade model that exploits only the temporal order of information sharing activities, combined with inherent properties of the shared information resources. The approach was applied to data from the world's largest online citizen science platform Zooniverse and we report about findings of this case study.	When Resources Collide: Towards a Theory of Coincidence in Information Spaces	NA:NA:NA	2015
Dave Murray-Rust:Segolene Tarte:Mark Hartswood:Owen Green	In this paper, we concern ourselves with the ways in which humans inhabit social machines: the structures and techniques which allow the enmeshing of multiple life traces within the flow of online interaction. In particular, we explore the distinction between transport and journeying, between networks and meshworks, and the different attitudes and modes of being appropriate to each. By doing this, we hope to capture a part of the sociality of social machines, to build an understanding of the ways in which lived lives relate to digital structures, and the emergence of the communality of shared work. In order to illustrate these ideas, we look at several aspects of existing social machines, and tease apart the qualities which relate to the different modes of being. The distinctions and concepts outlined here provide another element in both the analysis and development of social machines, understanding how people may joyfully and directedly engage with collective activities on the web.	On Wayfaring in Social Machines	NA:NA:NA:NA	2015
Ramine Tinati:Xin Wang:Ian Brown:Thanassis Tiropanis:Wendy Hall	Over the past years, streaming Web services have become popular, with many of the top Web platforms now offering near real-time streams of user and machine activity. In light of this, Web Observatories now are faced with the challenge of being able to process and republish real-time, big data, Web streams, whilst maintaining access control and data consistency. In this paper we describe the architecture used in the Southampton Web Observatory to harvest, process, and serve real-time Web streams.	A Streaming Real-Time Web Observatory Architecture for Monitoring the Health of Social Machines	NA:NA:NA:NA:NA	2015
Max Van Kleek:Daniel A. Smith:Dave Murray-Rust:Amy Guy:Kieron O'Hara:Laura Dragan:Nigel R. Shadbolt	Personal Data Stores are among the many efforts that are currently underway to try to re-decentralise the Web, and to bring more control and data management and storage capability under the control of the user. Few of these architectures, however, have considered the needs of supporting decentralised social software from the user's perspective. In this short paper, we present the results of our design exercise, focusing on two key design needs for building decentralised social machines: that of supporting heterogeneous social apps and multiple, separable user identities. We then present the technical design of a prototype social machine platform, INDX, which realises both of these requirements, and a prototype heterogeneous microblogging application which demonstrates its capabilities.	Social Personal Data Stores: the Nuclei of Decentralised Social Machines	NA:NA:NA:NA:NA:NA:NA	2015
Jeff Vass:Jo E. Munson	This paper sets out an approach to Social Machines (SMs), their description and analysis, based on a development of social constructionist theoretical principles adapted for Web Science. We argue that currently the search for the primitives of SMs, or appropriate units of analysis to describe them, tends to favour either the technology or sociality. We suggest an approach that favours distributed agency whether it is machinic or human or both. We argue that current thinking (e.g. Actor Network Theory) is unsuited to SMs. Instead we describe an alternative which prioritizes a view of socio-technical activity as forming `reflexive project structures'. We show that reflexivity in social systems can be further usefully divided into more fundamental elements (Recognition and Responsivity). This process enables us to capture more of the variation in SMs and to distinguish them from non-Web based socio-technical systems. We illustrate the approach by looking at different kinds of SMs showing how they relate to contemporary social theory.	Revisiting the Three Rs of Social Machines: Reflexivity, Recognition and Responsivity	NA:NA	2015
Carlos Castillo:Fernando Diaz:Jie Yin:Maja Vukovic	NA	Session details: SWDM 2015	NA:NA:NA:NA	2015
Kenneth M. Anderson	Crisis Informatics is a multidisciplinary research area that examines the socio-technical relationships among people, information, and technology during mass emergency events. One area of crisis informatics examines the on-line behaviors of members of the public making use of social media during a crisis event to make sense of it, to report on it, and, in some cases, to coordinate a response to it either locally or from afar. In order to study those behaviors, this social media data has to be systematically captured and stored in a scalable and reliable way for later analysis. Project EPIC is a large U.S. National Science Foundation funded project that has been performing crisis informatics research since Fall 2009 and has been designing and developing a reliable and robust software infrastructure for the storage and analysis of large crisis informatics data sets. Prof. Ken Anderson has led the research and development in this software engineering effort and will discuss the challenges (both technical and social) that Project EPIC faced in developing its software infrastructure, known as EPIC Collect and EPIC Analyze. EPIC Collect has been in 24/7 operation in various forms since Spring 2010 and has collected terabytes of social media data across hundreds of mass emergency events since that time. EPIC Analyze is a data analysis platform for large social media data sets that provides efficient browsing, filtering, and collaborative annotation services. Prof. Anderson will discuss these systems and also present the challenges of collecting and analyzing social media data (with an emphasis on Twitter data) at scale. Project EPIC has designed and evaluated software architectural styles that can be adopted by other research groups to help develop their own capacity to work in this space. Prof. Anderson will conclude the talk with a vision for future work in this area: What's next for crisis informatics software infrastructure?	Towards Next-Generation Software Infrastructure for Crisis Informatics Research	NA	2015
Daniele Quercia	Social media has been increasingly used to manage emergencies (hot management), yet it is still unclear to which extent its use is truly beneficial to manage calm situations (cold management). Take socioeconomic deprivation of cities. Measuring it in an accurate and timely fashion has become a priority for governments around the world. Traditionally, deprivation indexes have been derived from census data, which is however very expensive to obtain, and thus acquired only every few years. In recent years, we have proposed alternative computational methods to automatically extract proxies of deprivation at a fine spatio-temporal level of granularity. More specifically, we have proposed new ways of: a) mining deprivation at a fine level of spatio-temporal granularity~\cite{Venerandi15}; b) profiling the functional and temporal uses of cities~\cite{ruiz15taxonomy}; and c) determining which streets are safe from crime and which are walkable~\cite{Quercia2015digital}. All this only requires access to freely available user-generated content (on, e.g., Foursquare, Open Street Map, Flickr), and, as such, is complementary to the use of expensive proprietary data and outdated governmental data.	Social Media for Cold Management	NA	2015
Seigo Baba:Fujio Toriumi:Takeshi Sakaki:Kosuke Shinoda:Satoshi Kurihara:Kazuhiro Kazama:Itsuki Noda	During a disaster, appropriate information must be collected. For example, victims and survivors require information about shelter locations and dangerous points or advice about protecting themselves. Rescuers need information about the details of volunteer activities and supplies, especially potential shortages. However, collecting such localized information is difficult from such mass media as TV and newspapers because they generally focus on information aimed at the general public. On the other hand, social media can attract more attention than mass media under these circumstances since they can provide such localized information. In this paper, we focus on Twitter, one of the most influential social media, as a source of local information. By assuming that users who retweet the same tweet are interested in the same topic, we can classify tweets that are required by users with similar interests based on retweets. Thus, we propose a novel tweet classification method that focuses on retweets without text mining. We linked tweets based on retweets to make a retweet network that connects similar tweets and extracted clusters that contain similar tweets from the constructed network by our clustering method. We also subjectively verified the validity of our proposed classification method. Our experiment verified that the ratio of the clusters whose tweets are mutually similar in the cluster to all clusters is very high and the similarities in each cluster are obvious. Finally, we calculated the linguistic similarities of the results to clarify our proposed method's features. Our method classified topic-similar tweets, even if they are not linguistically similar.	Classification Method for Shared Information on Twitter Without Text Data	NA:NA:NA:NA:NA:NA:NA	2015
Maria Grazia Busa:Maria Teresa Musacchio:Shane Finan:Cilian Fennell	Social media provides a digital space -- a meeting place, for different people, often representing one or more groups in a society. The use of this space during a disaster, especially where information needs are high and the availability of factually accurate and ethically sourced data is scarce, has increased substantially over the last 5-10 years. This paper attempts to address communication in social media and trust between the public and figures of authority during a natural disaster in order to suggest communication strategies that can enhance or reinforce trust between these bodies before, during and after a natural disaster.	Trust-building through Social Media Communications in Disaster Management	NA:NA:NA:NA	2015
Davide Buscaldi:Irazú Hernandez-Farias	People use social networks for different communication purposes, for example to share their opinion on ongoing events. One way to exploit this common knowledge is by using Sentiment Analysis and Natural Language Processing in order to extract useful information. In this paper we present a SA approach applied to a set of tweets related to a recent natural disaster in Italy; our goal is to identify tweets that may provide useful information from a disaster management perspective.	Sentiment Analysis on Microblogs for Natural Disasters Management: a Study on the 2014 Genoa Floodings	NA:NA	2015
Alfredo Cobo:Denis Parra:Jaime Navón	During recent years the online social networks (in particular Twitter) have become an important alternative information channel to traditional media during natural disasters, but the amount and diversity of messages poses the challenge of information overload to end users. The goal of our research is to develop an automatic classifier of tweets to feed a mobile application that reduces the difficulties that citizens face to get relevant information during natural disasters. In this paper, we present in detail the process to build a classifier that filters tweets relevant and non-relevant to an earthquake. By using a dataset from the Chilean earthquake of 2010, we first build and validate a ground truth, and then we contribute by presenting in detail the effect of class imbalance and dimensionality reduction over 5 classifiers. We show how the performance of these models is affected by these variables, providing important considerations at the moment of building these systems.	Identifying Relevant Messages in a Twitter-based Citizen Channel for Natural Disaster Situations	NA:NA:NA	2015
Stefano Cresci:Maurizio Tesconi:Andrea Cimino:Felice Dell'Orletta	This work focuses on the analysis of Italian social media messages for disaster management and aims at the detection of messages carrying critical information for the damage assessment task. A main novelty of this study consists in the focus on out-domain and cross-event damage detection, and on the investigation of the most relevant tweet-derived features for these tasks. We devised different experiments by resorting to a wide set of linguistic features qualifying the lexical and grammatical structure of a text as well as ad-hoc features specifically implemented for this task. We investigated the most effective features that allow to achieve the best results. A further result of this study is the construction of the first manually annotated Italian corpus of social media messages for damage assessment.	A Linguistically-driven Approach to Cross-Event Damage Assessment of Natural Disasters from Social Media Messages	NA:NA:NA:NA	2015
Nathan O. Hodas:Greg Ver Steeg:Joshua Harrison:Satish Chikkagoudar:Eric Bell:Courtney D. Corley	People around the world use social media platforms such as Twitter to express their opinion and share activities about various aspects of daily life. In the same way social media changes communication in daily life, it also is transforming the way individuals communicate during disasters and emergencies. Because emergency officials have come to rely on social media to communicate alerts and updates, they must learn how users communicate disaster related content on social media. We used a novel information-theoretic unsupervised learning tool, CorEx, to extract and characterize highly relevant content used by the public on Twitter during known emergencies, such as fires, explosions, and hurricanes. Using the resulting analysis, authorities may be able to score social media content and prioritize their attention toward those messages most likely to be related to the disaster.	Disentangling the Lexicons of Disaster Response in Twitter	NA:NA:NA:NA:NA:NA	2015
Muhammad Imran:Carlos Castillo	While categorizing any type of user-generated content online is a challenging problem, categorizing social media messages during a crisis situation adds an additional layer of complexity, due to the volume and variability of information, and to the fact that these messages must be classified as soon as they arrive. Current approaches involve the use of automatic classification, human classification, or a mixture of both. In these types of approaches, there are several reasons to keep the number of information categories small and updated, which we examine in this article. This means at the onset of a crisis an expert must select a handful of information categories into which information will be categorized. The next step, as the crisis unfolds, is to dynamically change the initial set as new information is posted online. In this paper, we propose an effective way to dynamically extract emerging, potentially interesting, new categories from social media data.	Towards a Data-driven Approach to Identify Crisis-Related Topics in Social Media Streams	NA:NA	2015
Yafeng Lu:Xia Hu:Feng Wang:Shamanth Kumar:Huan Liu:Ross Maciejewski	Recently, social media, such as Twitter, has been successfully used as a proxy to gauge the impacts of disasters in real time. However, most previous analyses of social media during disaster response focus on the magnitude and location of social media discussion. In this work, we explore the impact that disasters have on the underlying sentiment of social media streams. During disasters, people may assume negative sentiments discussing lives lost and property damage, other people may assume encouraging responses to inspire and spread hope. Our goal is to explore the underlying trends in positive and negative sentiment with respect to disasters and geographically related sentiment. In this paper, we propose a novel visual analytics framework for sentiment visualization of geo-located Twitter data. The proposed framework consists of two components, sentiment modeling and geographic visualization. In particular, we provide an entropy-based metric to model sentiment contained in social media data. The extracted sentiment is further integrated into a visualization framework to explore the uncertainty of public opinion. We explored Ebola Twitter dataset to show how visual analytics techniques and sentiment modeling can reveal interesting patterns in disaster scenarios.	Visualizing Social Media Sentiment in Disaster Scenarios	NA:NA:NA:NA:NA:NA	2015
Richard McCreadie:Karolin Kappler:Andreas Kaltenbrunner:Magdalini Kardara:Craig Macdonald:John Soldatos:Iadh Ounis	Social media statistics during recent disasters (e.g. the 20 million tweets relating to 'Sandy' storm and the sharing of related photos in Instagram at a rate of 10/sec) suggest that the understanding and management of real-world events by civil protection and law enforcement agencies could benefit from the effective blending of social media information into their resilience processes. In this paper, we argue that despite the widespread use of social media in various domains (e.g. marketing/branding/finance), there is still no easy, standardized and effective way to leverage different social media streams -- also referred to as social sensors -- in security/emergency management applications. We also describe the EU FP7 project SUPER (Social sensors for secUrity assessments and Proactive EmeRgencies management), started in 2014, which aims to tackle this technology gap.	SUPER: Towards the use of Social Sensors for Security Assessments and Proactive Management of Emergencies	NA:NA:NA:NA:NA:NA:NA	2015
Sophie Parsons:Peter M. Atkinson:Elena Simperl:Mark Weal	Social Networks such as Twitter are often used for disseminating and collecting information during natural disasters. The potential for its use in Disaster Management has been acknowledged. However, more nuanced understanding of the communications that take place on social networks are required to more effectively integrate this information into the processes within disaster management. The type and value of information shared should be assessed, determining the benefits and issues, with credibility and reliability as known concerns. Mapping the tweets in relation to the modelled stages of a disaster can be a useful evaluation for determining the benefits/drawbacks of using data from social networks, such as Twitter, in disaster management. A thematic analysis of tweets' content, language and tone during the UK Storms and Floods 2013/14 was conducted. Manual scripting was used to determine the official sequence of events, and classify the stages of the disaster into the phases of the Disaster Management Lifecycle, to produce a timeline. Twenty-five topics discussed on Twitter emerged, and three key types of tweets, based on the language and tone, were identified. The timeline represents the events of the disaster, according to the Met Office reports, classed into B. Faulkner's Disaster Management Lifecycle framework. Context is provided when observing the analysed tweets against the timeline. This illustrates a potential basis and benefit for mapping tweets into the Disaster Management Lifecycle phases. Comparing the number of tweets submitted in each month with the timeline, suggests users tweet more as an event heightens and persists. Furthermore, users generally express greater emotion and urgency in their tweets. This paper concludes that the thematic analysis of content on social networks, such as Twitter, can be useful in gaining additional perspectives for disaster management. It demonstrates that mapping tweets into the phases of a Disaster Management Lifecycle model can have benefits in the recovery phase, not just in the response phase, to potentially improve future policies and activities.	Thematically Analysing Social Network Content During Disasters Through the Lens of the Disaster Management Lifecycle	NA:NA:NA:NA	2015
Soudip Roy Chowdhury:Hemant Purohit:Muhammad Imran	Existing literature demonstrates the usefulness of system-mediated algorithms, such as supervised machine learning for detecting classes of messages in the social-data stream (e.g., topically relevant vs. irrelevant). The classification accuracies of these algorithms largely depend upon the size of labeled samples that are provided during the learning phase. Other factors such as class distribution, term distribution among the training set also play an important role on classifier's accuracy. However, due to several reasons (money / time constraints, limited number of skilled labelers etc.), a large sample of labeled messages is often not available immediately for learning an efficient classification model. Consequently, classifier trained on a poor model often mis-classifies data and hence, the applicability of such learning techniques (especially for the online setting) during ongoing crisis response remains limited. In this paper, we propose a post-classification processing step leveraging upon two additional content features-stable hashtag association and stable named entity association, to improve the classification accuracy for a classifier in realistic settings. We have tested our algorithms on two crisis datasets from Twitter (Hurricane Sandy 2012 and Queensland Floods 2013), and compared our results against the results produced by a "best-in-class'' baseline online classifier. By showing the consistent better quality results than the baseline algorithm i.e., by correctly classifying the misclassified data points from the prior step (false negative and false positive to true positive and true negative classes, respectively), we demonstrate the applicability of our approach in practice.	D-Sieve: A Novel Data Processing Engine for Efficient Handling of Crises-Related Social Messages	NA:NA:NA	2015
Antonia Saravanou:George Valkanas:Dimitrios Gunopulos:Gennady Andrienko	Twitter is one of the most prominent social media platforms nowadays. A primary reason that has brought the medium at the spotlight of academic attention is its real-time nature, with people constantly uploading information regarding their surroundings. This trait, coupled with the service's data access policy for researchers and developers, has allowed the community to explore Twitter's potential as a news reporting tool. Finding out promptly about newsworthy events can prove extremely useful in crisis management situations. In this paper, we explore the use of Twitter as a mechanism used in disaster relief, and consequently in public safety. In particular, we perform a case study on the floods that occurred in the United Kingdom during January 2014, and how these were reflected on Twitter, according to tweets (i.e., posts) submitted by the users. We present a systematic algorithmic analysis of tweets collected with respect to our use case scenario, supplemented by visual analytic tools. Our objective is to identify meaningful and effective ways to take advantage of the wealth of Twitter data in crisis management, and we report on the findings of our analysis.	Twitter Floods when it Rains: A Case Study of the UK Floods in early 2014	NA:NA:NA:NA	2015
Leif Romeritch Syliongka:Nathaniel Oco:Alron Jan Lam:Cheryll Ruth Soriano:Ma. Divina Gracia Roldan:Francisco Magno:Charibeth Cheng	In this paper, we present a framework that combines automatic and manual approaches to discover themes in disaster-related tweets. As case study, we decided to focus on tweets related to typhoon Haiyan, which caused billions of dollars in damages. We collected tweets from November 2013 to March 2014 and used the local typhoon name "Yolanda" as the filter. Data association was used to expand the tweet set and k-means clustering was then applied. Clusters with high number of instances were subjected to open coding for labeling. The Silhouette indices ranged from 0.27 to 0.50. Analyses reveal that the use of automated Natural Language Processing (NLP) approach has the potential to deal with huge volumes of tweets by clustering frequently occurring words and phrases. This complements the manual approach to surface themes from a more manageable set of tweet pool, allowing for a more nuanced analysis of tweets from a human expert. As application, the themes identified during open coding were used as labels to train a classifier system. Future work could explore on using topic models and focusing on specific content or issues, such as natural calamities and citizen's participation in addressing these.	Combining Automatic and Manual Approaches: Towards a Framework for Discovering Themes in Disaster-related Tweets	NA:NA:NA:NA:NA:NA:NA	2015
Irina Temnikova:Sarah Vieweg:Carlos Castillo	The readability of text documents has been studied from a linguistic perspective long before people began to regularly communicate via Internet technologies. Typically, such studies look at books or articles containing many paragraphs and pages. However, the readability of short messages comprising a few sentences, common on today's social networking sites and microblogging services, has received less attention from researchers working on "readability". Emergency management specialists, crisis response practitioners, and scholars have long recognized that clear communication is essential during crises. To the best of our knowledge, the work we present here is the first to study the readability of crisis communications posted on Twitter-by governments, non-governmental organizations, and mainstream media. The data we analyze is comprised of hundreds of tweets posted during 15 different crises in English-speaking countries, which happened between 2012 and 2013. We describe factors which negatively affect comprehension, and consider how understanding can be improved. Based on our analysis and observations, we conclude with several recommendations for how to write brief crisis messages on social media that are clear and easy to understand.	The Case for Readability of Crisis Communications in Social Media	NA:NA:NA	2015
NA	NA	Session details: TargetAd 2015	NA	2015
Ricardo Baeza-Yates:Nemanja Djuric:Mihajlo Grbovic:Vladan Radosavljevic:Fabrizio Silvestri	Semantic embeddings of words (or objects in general) into a vector space have proven to be a powerful tool in many applications. In this talk we are going to show one possible application of semantic embeddings to sponsored search. Sponsored search represents the major source of revenue for web search engines and it is based on the following mechanism: each advertiser maintains a list of keywords they deem of interest with regards to their business. According to this targeting model, when a query is issued, all advertisers with a matching keyword are entered into an auction according to the amount they bid for the query, and the winner gets to show their ad, usually paying the next largest bid (this is called second price). The main challenges is that a query may not match many keywords, resulting in lower auction value, lower ad quality, and lost revenue for both, advertisers and publishers. We address them by applying semantic embeddings to this problem by learning how to project queries and ads in a common embedding, thus sharing the same feature space. The major novelty of the techniques we show is that learning is done by jointly modeling their content (words in queries and ad metadata), as well as their context within a search session. This model has several advantages and can be applied to at least three tasks. First, it can be used to generate query rewrites with a specific bias towards rewrites able to match relevant advertising. Second, it can be used also to retrieve for a given a query a set of relevant ads to be sent to the auction phase. Third, given an ad we are able to retrieve all the queries for which that ad can be considered relevant. The major advantage of learning both content and context embeddings is in the fact that a context-based model may suffer from coverage issue: if a query or an ad does not appear in the training set it cannot be treated by the model; content-based embeddings instead can be used to also build models capturing similarities between content, e.g. for a query not appearing in the model built we may capture some of its sub-queries by using content vectors. Another very interesting characteristic of this method is that all the tasks mentioned above are basically solved by means of a simple K-nearest neighbor search over the set of vectors in the embedding. The method has been trained up to 12 billion sessions, one of the largest corpora reported so far. We report offline and online experimental results, as well as post-deployment metrics. The results show that this approach significantly outperforms existing state-of-the-art, substantially improving a number of key business metrics.	Large-scale Contextual Query-to-Ad Matching and Retrieval System for Sponsored Search: (Abstract)	NA:NA:NA:NA:NA	2015
Yue Lu:Sandeep Pandey	Online advertising is a multi-billion dollar industry and it also serves as the major revenue source for Twitter Inc. In this talk, we present the ads selection pipeline at Twitter, using Promoted Tweets in Home Timelines as an example. The pipeline starts from targeting, where we model Twitter users' attributes offline, e.g. user gender, age, interest etc, so that we can match them with advertisers' specified audience criteria. The second critical component is user engagement rate prediction, where we employ a large-scale online learning system to do real-time training and prediction with rich features. Lastly, we run a second price auction based on the predictions, advertisers' bids and some other optimization parameters. We will present a series of case studies drawn from recent experiments in the setting of the deployed system used at Twitter.	Ads Selection At Twitter	NA:NA	2015
Michal Aharon:Amit Kagian:Yohay Kaplan:Raz Nissim:Oren Somekh	Modern ad serving systems can benefit when allowed to accumulate user information and use it as part of the serving algorithm. However, this often does not coincide with how the web is used. Many domains will see users for only brief interactions, as users enter a domain through a search result or social media link and then leave. Having access to little or no user information and no ability to assemble a user profile over a prolonged period of use, we would still like to leverage the information we have to the best of our ability. In this paper we attempt several methods of improving ad serving for occasional users, including leveraging user information that is still available, content analysis of the page, information about the page's content generators and historical breakdown of visits to the page. We compare and combine these methods in a framework of a collaborative filtering algorithm, test them on real data collected from Yahoo Answers, and achieve significant improvements over baseline algorithms.	Serving Ads to "Yahoo Answers" Occasional Visitors	NA:NA:NA:NA:NA	2015
Rob Hall:Josh Attenberg	Personalization has become a predominant theme in online advertising; the internet allows advertisers to target only those users with the greatest chances of engagement, maximizing the probability of success and user happiness. However, a na{\"i}ve approach to matching users with their most suitable content scales proportionally to the product of the cardinalities of the user and content sets. For advertisers with large portfolios, this quickly becomes intractable. In this work, we address this more general {\em top-$k$ personalization} problem, giving a scalable method to produce recommendations based on personalization models where the affinity between a user and an item is captured by an inner product (i.e., most matrix factorization models). We first transform the problem into finding the $k$-nearest neighbors among the items for each user, then approximate the solution via a method which is particularly suited for use on a map-reduce cluster. We empirically show that our method is between 1 and 2 orders of magnitude faster than previous work, while maintaining excellent approximation quality. Additionally, we provide an open-source implementation of our proposed method, this implementation is used in production at Etsy for a number of large scale personalization systems, and is the same code as used in the experiments below.	Fast and Accurate Maximum Inner Product Recommendations on Map-Reduce	NA:NA	2015
Diane Hu:Tristan Schneiter	The Activity Feed is Etsy's take on the ubiquitous "web feed" - a continuous stream of aggregated content, personalized for each user. These streams have become the de facto means of serving advertisements in the context of social media. Any visitor to Facebook or Twitter has seen advertisements placed on their web feed. For Etsy, an online marketplace for handmade and vintage goods with over 29 million unique items, the AF makes the marketplace feel a bit smaller for users. It enables discovery of relevant content, including activities from their social graph, recommended shops and items, and new listings from favorite shops. At the same time, Etsy's AF provides a platform for presenting users with targeted content, as well as advertisements, served alongside relevant and timely content. One of the biggest challenges for building such a feed is providing an engaging experience for all users across Etsy. Some users are first-time visitors who may find Etsy to be overwhelming. Others are long-time power users who already know what they're looking for and how to find it. In this work, we describe solutions to the challenges encountered while delivering targeted content to our tens of million of users. We also cover our means of adapting to each user's actions, evolving our targeted content offerings as the user's familiarity with Etsy grows. Finally, we discuss the impact of our system through extensive experimentation on live traffic, and show how these improvements have led to increased user engagement.	Targeted Content for a Real-Time Activity Feed: For First Time Visitors to Power Users	NA:NA	2015
Tolga Könik:Rajyashree Mukherjee:Jayasimha Katukuri	We present a new algorithm for recommending alternatives to a given item in an e-commerce setting. Our algorithm is an incremental improvement over an earlier system, which recommends similar items by first assigning the input item to clusters and then selecting best quality items within those clusters. The original algorithm does not consider the recent context and our new algorithm improves the earlier system by personalizing the recommendations to user intentions. The system measures user intention using the recent queries, which are used to determine the level of abstraction in similarity and relative importance of similarity dimensions. We show that user engagement increases when recommended item titles share more terms with most recent queries. Moreover, the new algorithm increases query coverage without sacrificing input item similarity and item quality.	Subjective Similarity: Personalizing Alternative Item Recommendations	NA:NA:NA	2015
Michal Laclavik:Marek Ciglan:Sam Steingold:Martin Seleng:Alex Dorman:Stefan Dlugolinsky	State of the art query categorization methods usually exploit web search services to retrieve the best matching web documents and map them to a given taxonomy of categories. This is effective but impractical when one does not own a web corpus and has to use a 3rd party web search engine API. The problem lies in performance and in financial costs. In this paper, we present a novel, fast and scalable approach to categorization of search queries based on a limited intermediate corpus: we use Wikipedia as the knowledge base. The presented solution relies on two steps: first a query is mapped to the relevant Wikipedia pages; second, the retrieved documents are categorized into a given taxonomy. We approach the first challenge as an entity search problem and present a new document categorization approach for the second step. On a standard data set, our approach achieves results comparable to the state-of-the-art approaches while maintaining high performance and scalability.	Search Query Categorization at Scale	NA:NA:NA:NA:NA:NA	2015
Chun Lu:Milan Stankovic:Philippe Laublet	The e-tourism is today an important field of the e-commerce. One specificity of this field is that consumers spend much time comparing many options on multiple websites before purchasing. It's easy for consumers to forget the viewed offers or websites. The Behavioral Retargeting (BR) is a widely used technique for online advertising. It leverages consumers' actions on advertisers' websites and displays relevant ads on publishers' websites. In this paper, we're interested in the relevance of the displayed ads in the e-tourism field. We present MERLOT 1, a Semantic-based travel destination recommender system that can be deployed to improve the relevance of BR in the e-tourism field. We conducted a preliminary experiment with the real data of a French travel agency. The results of 33 participants showed very promising results with regards to the baseline according to all used metrics. By this paper, we wish to provide a novel viewpoint to address the BR relevance problem, different from the dominating machine learning approaches.	Leveraging Semantic Web technologies for more relevant E-tourism Behavioral Retargeting	NA:NA:NA	2015
Katie O'Donnell:Henriette Cramer	Advertising is key to the business model of many online services. Personalization aims to make ads more relevant for users and more effective for advertisers. However, relatively few studies into user attitudes towards personalized ads are available. We present a San Francisco Bay Area survey (N=296) and in-depth interviews (N=24) with teens and adults. People are divided and often either (strongly) agreed or disagreed about utility or invasiveness of personalized ads and associated data collection. Mobile ads were reported to be less relevant than those on desktop. Participants explained ad personalization based on their personal previous behaviors and guesses about demographic targeting. We describe both metrics improvements as well as opportunities for improving online advertising by focusing on positive ad interactions reported by our participants, such as personalization focused not just on product categories but specific brands and styles, awareness of life events, and situations in which ads were useful or even inspirational.	People's Perceptions of Personalized Ads	NA:NA	2015
Thomas Steiner	In this paper, we present our ongoing research on an ads quality testing tool that we call AdAlyze Redux. This tool allows advertisers to get individual best practice recommendations based on an expandable set of textual ads features, tailored to exactly the ads in an advertiser's set of accounts. This lets them optimize their ad copies against the common online advertising key performance indicators clickthrough rate and, if available, conversion rate. We choose the Web as the tool's platform and automatically generate the analyses as platform-independent HTML5 slides and full reports.	AdAlyze Redux: Post-Click and Post-Conversion Text Feature Attribution for Sponsored Search Ads	NA	2015
Georgios Theocharous:Philip S. Thomas:Mohammad Ghavamzadeh	The main objective in the ad recommendation problem is to find a strategy that, for each visitor of the website, selects the ad that has the highest probability of being clicked. This strategy could be computed using supervised learning or contextual bandit algorithms, which treat two visits of the same user as two separate independent visitors, and thus, optimize greedily for a single step into the future. Another approach would be to use reinforcement learning (RL) methods, which differentiate between two visits of the same user and two different visitors, and thus, optimizes for multiple steps into the future or the life-time value (LTV) of a customer. While greedy methods have been well-studied, the LTV approach is still in its infancy, mainly due to two fundamental challenges: how to compute a good LTV strategy and how to evaluate a solution using historical data to ensure its "safety" before deployment. In this paper, we tackle both of these challenges by proposing to use a family of off-policy evaluation techniques with statistical guarantees about the performance of a new strategy. We apply these methods to a real ad recommendation problem, both for evaluating the final performance and for optimizing the parameters of the RL algorithm. Our results show that our LTV optimization algorithm equipped with these off-policy evaluation techniques outperforms the greedy approaches. They also give fundamental insights on the difference between the click through rate (CTR) and LTV metrics for performance evaluation in the ad recommendation problem.	Ad Recommendation Systems for Life-Time Value Optimization	NA:NA:NA	2015
Marc Spaniol:Ricardo Baeza-Yates:Julien Masanès	NA	Session details: TempWeb 2015	NA:NA:NA	2015
Paolo Boldi	Given a large complex network, which of its nodes are more central? This question emerged in many contexts (e.g., sociology, psychology and computer science), and gave rise to a large range of proposed centrality measures. Providing a sufficiently general and mathematically sound classification of these measures is challenging: on one hand, it requires that one can suggest some simple, basic properties that a centrality measure should exhibit; on the other hand, it calls for innovative algorithms that allow an efficient computation of these measures on large real networks. HyperBall is a recently proposed tool that accesses the graph in a semi-streaming fashion and is at the same time able to compute the distance distribution and to approximate all geometric (i.e., distance-based) centralities. It uses a very small amount of core memory, thanks to the application of HyperLogLog counters, and exhibits high, guaranteed accuracy.	Large-scale Network Analytics: Diffusion-based Computation of Distances and Geometric Centralities	NA	2015
Abdalghani Abujabal:Klaus Berberich	We address the problem of identifying important events in the past, present, and future from semantically-annotated large-scale document collections. Semantic annotations that we consider are named entities (e.g., persons, locations, organizations) and temporal expressions (e.g., during the 1990s). More specifically, for a given time period of interest, our objective is to identify, rank, and describe important events that happened. Our approach P2F Miner makes use of frequent itemset mining to identify events and group sentences related to them. It uses an information-theoretic measure to rank identified events. For each of them, it selects a representative sentence as a description. Experiments on ClueWeb09 using events listed in Wikipedia year articles as ground truth show that our approach is effective and outperforms a baseline based on statistical language models.	Important Events in the Past, Present, and Future	NA:NA	2015
Sarah Chasins:Phitchaya Mangpo Phothilimthana	As dynamic, complex, and non-deterministic webpages proliferate, running controlled web experiments on live webpages is becoming increasingly difficult. To compare algorithms that take webpages as inputs, an experimenter must worry about ever-changing webpages, and also about scalability. Because webpage contents are constantly changing, experimenters must intervene to hold webpages constant, in order to guarantee a fair comparison between algorithms. Because webpages are increasingly customized and diverse, experimenters must test web algorithms over thousands of webpages, and thus need to implement their experiments efficiently. Unfortunately, no existing testing frameworks have been designed for this type of experiment. We introduce Dicer, a framework for running large-scale controlled experiments on live webpages. Dicer's programming model allows experimenters to easily 1) control when to enforce a same-page guarantee and 2) parallelize test execution. The same-page guarantee ensures that all loads of a given URL produce the same response. The framework utilizes a specialized caching proxy server to enforce this guarantee. We evaluate tool on a dataset of 1,000 real webpages, and find it upholds the same-page guarantee with little overhead.	Dicer: A Framework for Controlled, Large-Scale Web Experiments	NA:NA	2015
Iftah Gamzu:Zohar Karnin:Yoelle Maarek:David Wajc	The majority of Web email is known to be generated by machines even when one excludes spam. Many machine-generated email messages such as invoices or travel itineraries are critical to users. Recent research studies establish that causality relations between certain types of machine-generated email messages exist and can be mined. These relations exhibit a link between a given message to a past message that gave rise to its creation. For example, a shipment notification message can often be linked to a past online purchase message. Instead of studying how an incoming message can be linked to the past, we propose here to focus on predicting future email arrival as implied by causality relations. Such a prediction method has several potential applications, ranging from improved ad targeting in up sell scenarios to reducing false positives in spam detection. We introduce a novel approach for predicting which types of machine-generated email messages, represented by so-called "email templates", a user should receive in future time windows. Our prediction approach relies on (1) statistically inferring causality relations between email templates, (2) building a generative model that explains the inbox of each user using those causality relations, and (3) combining those results to predict which email templates are likely to appear in future time frames. We present preliminary experimental results and some data insights obtained by analyzing several million inboxes of Yahoo Mail users, who voluntarily opted-in for such research.	You Will Get Mail!Predicting the Arrival of Future Email	NA:NA:NA:NA	2015
Toni Gruetze:Gary Yao:Ralf Krestel	Social networking services, such as Facebook, Google+, and Twitter are commonly used to share relevant Web documents with a peer group. By sharing a document with her peers, a user recommends the content for others and annotates it with a short description text. This short description yield many chances for text summarization and categorization. Because today's social networking platforms are real-time media, the sharing behaviour is subject to many temporal effects, i.e., current events, breaking news, and trending topics. In this paper, we focus on time-dependent hashtag usage of the Twitter community to annotate shared Web-text documents. We introduce a framework for time-dependent hashtag recommendation models and introduce two content-based models. Finally, we evaluate the introduced models with respect to recommendation quality based on a Twitter-dataset consisting of links to Web documents that were aligned with hashtags.	Learning Temporal Tagging Behaviour	NA:NA:NA	2015
Nattiya Kanhabua:Tu Ngoc Nguyen:Wolfgang Nejdl	In many cases, a user turns to search engines to find information about real-world situations, namely, political elections, sport competitions, or natural disasters. Such temporal querying behavior can be observed through a significant number of event-related queries generated in web search. In this paper, we study the task of detecting event-related queries, which is the first step for understanding temporal query intent and enabling different temporal search applications, e.g., time-aware query auto-completion, temporal ranking, and result diversification. We propose a two-step approach to detecting events from query logs. We first identify a set of event candidates by considering both implicit and explicit temporal information needs. The next step further classifies the candidates into two main categories, namely, event or non-event. In more detail, we leverage different machine learning techniques for query classification, which are trained using the feature set composed of time series features from signal processing, along with features derived from click-through information, and standard statistical features. In order to evaluate our proposed approach, we conduct an experiment using two real-world query logs with manually annotated relevance assessments for 837 events. To this end, we provide a large set of event-related queries made available for fostering research on this challenging task.	Learning to Detect Event-Related Queries for Web Search	NA:NA:NA	2015
Tomasz Kusmierczyk:Christoph Trattner:Kjetil Nørvåg	Since innovation plays an important role in the context of food, as evident in how successful chefs, restaurants or cuisines in general evolve over time, we were interested in exploring this dimension from a more virtual perspective. In particular, the paper presents results of a study that was conducted in the context of a large-scale German online food community forum to explore another important dimension of online food recipe production, namely known as online food innovation. The study shows interesting findings and temporal patterns in terms of how online food recipe innovation takes place.	Temporal Patterns in Online Food Innovation	NA:NA:NA	2015
Jimmy Lin	Warcbase is an open-source platform for storing, managing, and analyzing web archives using modern "big data" infrastructure on commodity clusters---specifically, HBase for storage and Hadoop for data analytics. This paper describes an effort to scale "down" Warcbase onto a Raspberry Pi, an inexpensive single-board computer about the size of a deck of playing cards. Apart from an interesting technology demonstration, such a design presents new opportunities for personal web archiving, in enabling a low-cost, low-power, portable device that is able to continuously capture a user's web browsing history---not only the URLs of the pages that a user has visited, but the contents of those pages---and allowing the user to revisit any previously-encountered page, as it appeared at that time. Experiments show that data ingestion throughput and temporal browsing latency are adequate with existing hardware, which means that such capabilities are already feasible today.	Scaling Down Distributed Infrastructure on Wimpy Machines for Personal Web Archiving	NA	2015
Tu Ngoc Nguyen:Nattiya Kanhabua:Wolfgang Nejdl:Claudia Niederée	With the reflection of nearly all types of social cultural, societal and everyday processes of our lives in the web, web archives from organizations such as the Internet Archive have the potential of becoming huge gold-mines for temporal content analytics of many kinds (e.g., on politics, social issues, economics or media). First hand evidences for such processes are of great benefit for expert users such as journalists, economists, historians, etc. However, searching in this unique longitudinal collection of huge redundancy (pages of near-identical content are crawled all over again) is completely different from searching over the web. In this work, we present our first study of mining the temporal dynamics of subtopics by leveraging the value of anchor text along the time dimension of the enormous web archives. This task is especially useful for one important ranking problem in the web archive context, the time-aware search result diversification. Due to the time uncertainty (the lagging nature and unpredicted behavior of the crawlers), identifying the trending periods for such temporal subtopics relying solely on the timestamp annotations of the web archive (i.e., crawling times) is extremely difficult. We introduce a brute-force approach to detect a time-reliable sub-collection and propose a method to leverage them for relevant time mining of subtopics. This is empirically found effective in solving the problem.	Mining Relevant Time for Query Subtopics in Web Archives	NA:NA:NA:NA	2015
Nataliia Rümmele:Ryutaro Ichise:Hannes Werthner	In the link prediction problem, formulated as a binary classification problem, we want to classify each pair of disconnected nodes in the network whether they will be connected by a link in the future. We study link formation in social networks with two types of links over several time periods. To solve the link prediction problem, we follow the approach of counting 3-node graphlets and suggest three extensions to the original method. By performing experiments on two real-world social networks we show that the new methods have a predictive power, however, network evolution cannot be explained by one specific feature at all time points. We also observe that some network properties can point at features which are more effective for temporal link prediction.	Exploring Supervised Methods for Temporal Link Prediction in Heterogeneous Social Networks	NA:NA:NA	2015
Mohsen Shahriari:Sebastian Krott:Ralf Klamma	In this paper we propose a new two-phase algorithm for overlapping community detection (OCD) in social networks. In the first phase, called disassortative degree mixing, we identify nodes with high degrees through a random walk process on the row-normalized disassortative matrix representation of the network. In the second phase, we calculate how closely each node of the network is bound to the leaders via a cascading process called network coordination game. We implemented the algorithm and four additional ones as a Web service on a federated peer-to-peer infrastructure. Comparative test results for small and big real world networks demonstrated the correct identification of leaders, high precision and good time complexity. The Web service is available as open source software.	Disassortative Degree Mixing and Information Diffusion for Overlapping Community Detection in Social Networks (DMID)	NA:NA:NA	2015
Andreas Spitz:Jannik Strötgen:Thomas Bögel:Michael Gertz	Approaches in support of the extraction and exploration of temporal information in documents provide an important ingredient in many of today's frameworks for text analysis. Methods range from basic techniques, primarily the extraction of temporal expressions and events from documents, to more sophisticated approaches such as ranking of documents with respect to their temporal relevance to some query term or the construction of timelines. Almost all of these approaches operate on the document level, that is, for a collection of documents a timeline is extracted or a ranked list of documents is returned for a temporal query term. In this paper, we present an approach to characterize individual dates, which can be of different granularities, and terms. Given a query date, a ranked list of terms is determined that are highly relevant for that date and best summarize the date. Analogously, for a query term, a ranked list of dates is determined that best characterize the term. Focusing on just dates and single terms as they occur in documents provides a fine-grained query and exploration method for document collections. Our approach is based on a weighted bipartite graph representing the co-occurrences of time expressions and terms in a collection of documents. We present different measures to obtain a ranked list of dates and terms for a query term and date, respectively. Our experiments and evaluation using Wikipedia as a document collection show that our approach provides an effective means in support of date and temporal term summarization.	Terms in Time and Times in Context: A Graph-based Term-Time Ranking Model	NA:NA:NA:NA	2015
Huajun Chen:Jeff Z. Pan	NA	Session details: WDS4SC 2015	NA:NA	2015
George Asimakopoulos:Sotiris Christodoulou:Andreas Gizas:Vassilios Triantafillou:Giannis Tzimas:John Gialelis:Artemios Voyiatzis:Dimitris Karadimas:Andreas Papalambrou	Dynacargo is an ongoing research project that introduces a breakthrough approach for cargo management systems, as it places the hauled cargos in the center of a haulage information management system, instead of the vehicle. Dynacargo attempts to manage both distribution and collection processes, providing an integrated approach. This paper presents the Dynacargo architectural modules and interrelations between them, as well as the research issues and development progress of some selected modules. In the context of Dynacargo project, a set of durable, low cost RFID tags are placed on waste bins in order to produce crucial data that is fed via diverse communication channels into the cargo management system. Besides feeding the management system with raw data from waste bins, data mining techniques are used on archival data, in order to predict current waste bins fill status. Moreover easy-to-use mobile and web applications will be developed to encourage citizens to participate and become active information producers and consumers. Dynacargo project overall aim is to develop a near real-time monitoring system that monitors and transmits waste bins' fill level, in order to dynamically manage the waste collection more efficiently by minimizing distances covered by refuse vehicles, relying on efficient routing algorithms.	Architecture and Implementation Issues, Towards a Dynamic Waste Collection Management System	NA:NA:NA:NA:NA:NA:NA:NA:NA	2015
Daniel Castellani Ribeiro:Huy T. Vo:Juliana Freire:Cláudio T. Silva	Large volumes of urban data are being made available through a variety of open portals. Besides promoting transparency, these data can bring benefits to government, science, citizens and industry. It is no longer a fantasy to ask "if you could know anything about a city, what do you want to know" and to ponder what could be done with that information. However, the great number and variety of datasets creates a new challenge: how to find relevant datasets. While existing portals provide search interfaces, these are often limited to keyword searches over the limited metadata associated each dataset, for example, attribute names and textual description. In this paper, we present a new tool, UrbanProfiler, that automatically extracts detailed information from datasets. This information includes attribute types, value distributions, and geographical information, which can be used to support complex search queries as well as visualizations that help users explore and obtain insight into the contents of a data collection. Besides describing the tool and its implementation, we present case studies that illustrate how the tool was used to explore a large open urban data repository.	An Urban Data Profiler	NA:NA:NA:NA	2015
Sergio Consoli:Misael Mongiovic:Andrea G. Nuzzolese:Silvio Peroni:Valentina Presutti:Diego Reforgiato Recupero:Daria Spampinato	Data management is crucial in modern smart cities. A good data model for smart cities has to be able to describe and integrate data from multiple domains, such as geographic information, public transportation, road maintenance, waste collection, and urban faults management. We describe our approach for creating a semantic platform for the Municipality of Catania, one of the main cities in Southern Italy. The ultimate goal is to boost the metropolis towards the route of a modern smart city and improve urban life. Our platform exhibits a consistent, minimal and comprehensive semantic data model for the city based on the Linked Open Data paradigm. Both the model and the data are publically accessible thorough dedicated user-friendly services, which allow citizens to observe and interact with the work of the public administration. Our platform also enables interested businesses and programmers to develop front-end services on the top of it. We describe the methodology used to extract data from sources, enrich them, building an ontology that describes them and publish them under the Linked Open Data paradigm. We include in our description employed tools and technologies. Our methodology is based on the standards of the W3C, on good practices of ontology design, on the guidelines issued by the Agency for Digital Italy and the Italian Index of Public Administration, as well as on the in-depth experience of the researchers in this field.	A Smart City Data Model based on Semantics Best Practice and Principles	NA:NA:NA:NA:NA:NA:NA	2015
Cataldo Musto:Giovanni Semeraro:Marco de Gemmis:Pasquale Lops	This paper presents a domain-agnostic framework for intelligent processing of textual streams coming from social networks. The framework implements a pipeline of techniques for semantic representation, sentiment analysis, automatic content classification, and provides an analytics console to get some findings from the extracted data. The effectiveness of the platform has already been proved by deploying it in two smart cities-related scenarios: in the first it was exploited to monitor the recovering state of the social capital of L'Aquila's city after the dreadful earthquake of April 2009, while in the latter a semantic analysis of the content posted on social networks was performed to build a map of the most at-risk areas of the Italian territory. In both scenarios, the outcomes resulting from the analysis confirmed the insight that the adoption of methodologies for intelligent and semantic analysis of textual content can provide interesting findings useful to improve the understanding of very complex phenomena.	Developing Smart Cities Services through Semantic Analysis of Social Streams	NA:NA:NA:NA	2015
Gloria Re Calegari:Irene Celino	Urban information abound today in open Web sources as well as in enterprise datasets. However, the maintenance and update of this wealth of information about cities comes at different costs: some datasets are automatically produced, while other sources require expensive workflows including human intervention. Regression techniques can be employed to predict a costly dataset from a set of cheaper information sources. In this paper we present our early experiments in predicting land use and demographics from heterogeneous open and enterprise datasets referring to the city of Milano. The results are encouraging, thus demonstrating that a data science approach leveraging diverse data can be actually worth for a smarter urban planning support.	Smart Urban Planning Support through Web Data Science on Open and Enterprise Data	NA:NA	2015
Jacqueline Bourdeau:Bebo White:Irwin King	NA	Session details: WebET 2015	NA:NA:NA	2015
Jacqueline Bourdeau:Thomas Forissier:Yves Mazabraud:Roger Nkambou	NA	Web-Based Context-Aware Science Learning	NA:NA:NA:NA	2015
Manolis Mavrikis:Zheng Zhu:Sergio Gutierrez-Santos:Alexandra Poulovassilis	Log files from adaptive Exploratory Learning Environments can contain prohibitively large quantities of data for visualisation and analysis. Moreover, it is hard to know in advance what data is required for analytical purposes. Using a microworld for secondary algebra as a case study, we discuss how students' interaction data can be transformed into a data warehouse in order to allow its visualisation and exploration using online analytical processing (OLAP) tools. We also present some additional, more targeted, visualisations of the students' interaction data. We demonstrate the possibilities that these visualisations provide for exploratory data analysis, enabling confirmation or contradiction of expectations that pedagogical experts may have about the system and ultimately providing both empirical evidence and insights for its further development.	Visualisation and Analysis of Students' Interaction Data in Exploratory earning Environments	NA:NA:NA:NA	2015
Andrea Zielinski:Jürgen Bock	Personalized learning pathways have been advocated by didactic experts to overcome the problem of disorientation and information overload in technology enhanced learning (TEL). They are not only relevant for providing user-adaptive navigational support, but can also be used for composing learning objects into new personalized courses (sequencing and assembly). In this paper we investigate, how Semantic Web technologies can effectively support these tasks, based on a proper representation of learning objects and courses according to didactic requirements. We claim that both eLearning tasks, adaptive navigation and course assembly, call for a representational model that can capture the syntax and semantics of learning pathways adequately. In particular: (1) a new type of navigation that takes into account ordering information and the hierarchical structure of an eLearning course complemented with adaptive constraints; (2) closely tied to it, a semantic layer to guarantee interoperability and validation of the correctness of the learning pathway descriptions. We investigate to what extend Semantic Web Languages like RDF/S and OWL are expressive enough to handle different aspects of learning pathways. While both share a structural similarity with DAGs, only OWL ontologies - formally underpinned by description logics (DLs) - are expressive enough to validate the correctness of the data and infer semantically related learning resources on the pathway. For tasks that are more related to the syntax of learning pathways, in particular navigation similar to a guided tour, we test the time efficiency on various synthetic OWL ontologies using the HermiT reasoner. Experimental results show that the course structure and the density of the knowledge graph impact on the performance. We claim that in a dynamically changing environment, where the computation of reachability of a vertex is computed on demand at run-time, OWL-based reasoning does not scale up well. Using a real-world case study from the eLearning domain, we compare an OWL 2 DL implementation with an equivalent graph algorithm implementation with respect to time efficiency.	A Case Study on the Use of Semantic Web Technologies for Learner Guidance	NA:NA	2015
Radoslaw Nielek:Adam Wierzbicki:Adam Jatowt:Katsumi Tanaka	NA	Session details: WebQuality 2015	NA:NA:NA:NA	2015
Cinzia Cappiello	n today's information era, every day more and more information is generated and people, on the one hand, have advantages due the increasing support in decision processes and, on the other hand, are experiencing difficulties in the selection of the right data to use. That is, users may leverage on more data but at the same time they may not be able to fully value such data since they lack the necessary knowledge about their provenance and quality. The data quality research area provides quality assessment and improvement methods that can be a valuable support for users that have to deal with the complexity of Web content. In fact, such methods help users to identify the suitability of information for their purposes. Most of the methods and techniques proposed, however, address issues for structured data and/or for defined contexts. Clearly, they cannot be easily used on the Web, where data come from heterogeneous sources and the context of use is most of the times unknown. In this keynote, the need for new assessment techniques is highlighted together with the importance of tracking data provenance as well as the reputation and trustworthiness of the sources. In fact, it is well known that the increase of data volume often corresponds to an increase of value, but to maximize such value the data sources to be used have to carefully analyzed, selected and integrated depending on the specific context of use. The talk discusses the data quality dimensions necessary to analyze different Web data sources and provides a set of illustrative examples that show how to maximize the quality of gathered information.	On the Role of Data Quality in Improving Web Information Value	NA	2015
Vlad Bulakh:Minaxi Gupta	We study carding shops that sell stolen credit and debit card information online. By bypassing the anti-scrapping mechanisms they use, we find that the prices of cards depend heavily on factors such as the issuing bank, country of origin, and whether the card can be used in brick-and-mortar stores or not. Almost 70% of cards sold by these outfits are priced at or below the cost banks incur in re-issuing them. Ironically, this makes buying their own cards more economical for the banks than re-issuing. We also find that the monthly revenues for the carding shops we study are high enough to justify the risk fraudsters take. Further, inventory at carding outfits seems to follow data breaches and the impact of delayed deployment of the smart chip technology is evident in the disproportionate share the U.S. commands in the underground card fraud economy.	Characterizing Credit Card Black Markets on the Web	NA:NA	2015
Balint Daroczy:David Siklois:Robert Palovics:Andras A. Benczur	We compare machine learning methods to predict quality aspects of the C3 dataset collected as a part of the Reconcile project. We give methods for automatically assessing the credibility, presentation, knowledge, intention and completeness by extending the attributes in the C3 dataset by the page textual content. We use Gradient Boosted Trees and recommender methods over the evaluator, site, evaluation triplets and their metadata and combine with text classifiers. In our experiments best results can be reached by the theoretically justified normalized SVM kernel. The normalization can be derived by using the Fisher information matrix of the text content. As the main contribution, we describe the theory of the Fisher matrix and show that SVM may be particularly suitable for difficult text classification tasks.	Text Classification Kernels for Quality Prediction over the C3 Data Set	NA:NA:NA:NA	2015
Filippo Geraci	Spam websites are domains whose owners are not interested in using them as gates for their activities but they are parked to be sold in the secondary market of web domains. To transform the costs of the annual registration fees in an opportunity of revenues, spam websites most often host a large amount of ads in the hope that someone who lands on the site by chance clicks on some ads. Since parking has become a widespread activity, a large number of specialized companies have come out and made parking a straightforward task that simply requires to set the domain's name servers appropriately. Although parking is a legal activity, spam websites have a deep negative impact on the information quality of the web and can significantly deteriorate the performances of most web mining tools. For example these websites can influence search engines results or introduce an extra burden for crawling systems. In addition, spam websites represent a cost for ad bidders that are obliged to pay for impressions or clicks that have a negligible probability to produce revenues. In this paper, we experimentally show that spam websites hosted by the same service provider tend to have similar look-and-feel. Exploiting this structural similarity we face the problem of the automatic identification of spam websites. In addition, we use the outcome of the classification for compiling the list of the name servers used by spam websites so that they can be discarded before the first connection just after the first DNS query. A dump of our dataset (including web pages and meta information) and the corresponding manual classification is freely available upon request.	Identification of Web Spam through Clustering of Website Structures	NA	2015
Lei Li:Daqing He:Wei Jeng:Spencer Goodwin:Chengzhi Zhang	Despite various studies on examining and predicting answer quality on generic social Q&A sites such as Yahoo! Answers, little is known about why answers on academic Q&A sites are voted on by scholars who follow the discussion threads to be high quality answers. Using 1021 answers obtained from the Q&A part of an academic social network site ResearchGate (RG), we firstly explored whether various web-captured features and human-coded features can be the critical factors that influence the peer-judged answer quality. Then using the identified critical features, we constructed three classification models to predict the peer-judged rating. Our results identify four main findings. Firstly, responders' authority, shorter response time and greater answer length are the critical features that positively associate with the peer-judged answer quality. Secondly, answers containing social elements are very likely to harm the peer-judged answer quality. Thirdly, an optimized SVM algorithm has an overwhelming advantage over other models in terms of accuracy. Finally, the prediction based on web-captured features had better performance when comparing to prediction on human-coded features. We hope that these interesting insights on ResearchGate's answer quality can help the further design of academic Q&A sites.	Answer Quality Characteristics and Prediction on an Academic Q&A Site: A Case Study on ResearchGate	NA:NA:NA:NA:NA	2015
Pierre Maret:Rajendra Akerkar:Laurent Vercouter	NA	Session details: WIC 2015	NA:NA:NA	2015
Peter Mika	Ten years have passed since the appearance of the first major online social networks as well as the standardization of Semantic Web technology. In this presentation, we will look at whether Semantic Web technologies have fulfilled their promise in improving the online networking experience, and conversely, the extent to which online networking helped improve the Semantic Web itself.	Social Networks and the Semantic Web: A Retrospective of the Past 10 Years	NA	2015
Evangelos Chatzicharalampous:Zigkolis Christos:Athena Vakali	Since the first introduced Collaborative Filtering Recommenders (CFR) there have been many attempts to improve their performance by enhancing the prediction accuracy. Even though rating prediction is the prevailing paradigm in CFR, there are other issues which have gained significant attention with respect to the content and its variety. Coverage, which constitutes the degree to which recommendations cover the set of available items, is an important factor along with diversity of the items proposed to an individual, often measured by an average dissimilarity between all pairs of recommended items. In this paper, we argue that coverage and diversity cannot be effectively addressed by conventional CFR with pure similarity-based neighborhood creation processes, especially in sparse datasets. Motivated by the need for including wider content characteristics, we propose a novel neighbor selection technique which emphasizes on variety in preferences (to cover polyphony in selection). Our approach consists of a new metric, named "Exploriometer", which acts as a personality trait for users based on their rating behavior. We favor users who are explorers in order to increase polyphony, and subsequently coverage and diversity; but we still select similar users when we create neighborhoods as a solid basis in order to keep accuracy levels high. The proposed approach has been experimented by two real-world datasets (MovieLens and Yahoo! Music ) with coverage, diversity and accuracy aware recommendations extracted by both traditional CFR and CFR enhanced with our neighborhood creation process. We also introduce a new metric, inspired by the Pearson Correlation Coefficient, to estimate the diversity of recommended items. The derived results demonstrate that our neighbor selection technique can enhance coverage and diversity of the recommendations, especially on sparse datasets.	Exploriometer: Leveraging Personality Traits for Coverage and Diversity Aware Recommendations	NA:NA:NA	2015
Pierre Maret:Rajendra Akerkar:Laurent Vercouter	The World Wide Web (WWW) provides precious means for communication, which goes far beyond the traditional communication media. Web-based communities have become imperative spaces for individuals to seek and share expertise. Networks in these communities usually differ in their topology from other networks such as the World Wide Web. In this paper, we explore some research issues of web intelligence and communities. We will also introduce the WI&C'15 workshop's goal and structure.	Web Intelligence and Communities	NA:NA:NA	2015
Romain Noël:Nicolas Malandain:Alexandre Pauchet:Laurent Vercouter:Bruno Grilheres:Stephan Brunessaux	The discovery of new sources of information on a given topic is a prominent problem for Experts in Intelligence Analysis (EIA) who cope with the search of pages on specific and sensitive topics. Their information needs are difficult to express with queries and pages with sensitive content are difficult to find with traditional search engines as they are usually poorly indexed. We propose a double vector to model EIA's information needs, composed of DBpedia resources and keywords, both extracted from Web pages provided by the user. We also introduce a new similarity measure that is used in a Web source discovery system called DOWSER. DOWSER aims at providing users with new sources of information related to their needs without considering the popularity of a page. A series of experiments provides an empirical evaluation of the whole system.	A Bi-Dimensional User Profile to Discover Unpopular Web Sources	NA:NA:NA:NA:NA:NA	2015
Phuong Nguyen:Paolo Tomeo:Tommaso Di Noia:Eugenio Di Sciascio	The Web of Data is the natural evolution of the World Wide Web from a set of interlinked documents to a set of interlinked entities. It is a graph of information resources interconnected by semantic relations, thereby yielding the name Linked Data. The proliferation of Linked Data is for sure an opportunity to create a new family of data-intensive applications such as recommender systems. In particular, since content-based recommender systems base on the notion of similarity between items, the selection of the right graph-based similarity metric is of paramount importance to build an effective recommendation engine. In this paper, we review two existing metrics, SimRank and PageRank, and investigate their suitability and performance for computing similarity between resources in RDF graphs and investigate their usage to feed a content-based recommender system. Finally, we conduct experimental evaluations on a dataset for musical artists and bands recommendations thus comparing our results with two other content-based baselines measuring their performance with precision and recall, catalog coverage, items distribution and novelty metrics.	An evaluation of SimRank and Personalized PageRank to build a recommender system for the Web of Data	NA:NA:NA:NA	2015
Ruben Verborgh:Thomas Steiner:Carlos Pedrinaci	NA	Session details: WSREST 2015	NA:NA:NA	2015
Hermano Albuquerque Lira:Jose Renato Villela Dantas:Bruno de Azevedo Muniz:Tadeu Matos Nunes:Pedro Porfirio Muniz Farias	In the Web, linked data is growing rapidly due to its potential to facilitate data retrieval and data integration. At the same time, relational database systems store a vast amount of data published on the Web. Current linked data in the Web is mainly read only. It allows for integration, navigation, and consultations in large structured datasets, but it still lacks a general concept for reading and writing. This paper proposes a specification, entitled Semantic Data Services (SDS), for RESTful Web services that provide data access. To provide linked data read-write capacity, SDS proposes a mechanism for integrity checking that is analogous to that used in relational databases. SDS implements the Semantic Restful Interface (SERIN) specification. SERIN uses annotations on classes in an ontology, describing the semantic web services that are available to manipulate data. This work extends SERIN specification adding annotations to allow the adoption of data access integrity constraints.	An Approach to Support Data Integrity for Web Services Using Semantic RESTful Interfaces	NA:NA:NA:NA:NA	2015
Junjie Feng:Aaron Harwood	Web browsers are de facto clients for an ever-increasing range of web applications. At the same time, web users are accessing these applications from a wide range of devices. This paper presents a solution for runtime browser session migration and management, called BrowserCloud, which allows a user to securely manage multiple browsers from a personal or third-party Cloud service, migrate snapshots of active browser sessions between browsers over different devices, using a robust security module. The design of BrowserCloud is based on browser extensions/plugins that can preserve and restore browser session state, and a PHP server that stores browser sessions securely. We have tested our implementation over a range of increasingly complex web applications, including WebRTC and HTML5 video. To the best of our knowledge, our implementation is the most robust and secure approach to runtime browser session management to date.	BrowserCloud: A Personal Cloud for Browser Session Migration and Management	NA:NA	2015
Tobias Fertig:Peter Braun	In contrast to the increasing popularity of REpresentational State Transfer (REST), systematic testing of RESTful Application Programming Interfaces (API) has not attracted much attention so far. This paper describes different aspects of automated testing of RESTful APIs. Later, we focus on functional and security tests, for which we apply a technique called model-based software development. Based on an abstract model of the RESTful API that comprises resources, states and transitions a software generator not only creates the source code of the RESTful API but also creates a large number of test cases that can be immediately used to test the implementation. This paper describes the process of developing a software generator for test cases using state-of-the-art tools and provides an example to show the feasibility of our approach.	Model-driven Testing of RESTful APIs	NA:NA	2015
Felix Leif Keppmann:Maria Maleshkova:Andreas Harth	Networked applications continuously move towards service-based and modular solutions. At the same time, web technologies, proven to be modular and distributed, are applied to these application areas. However, web technologies have to be adapted to the new characteristics of the involved systems -- no explicit client and server roles, use of heterogeneous devices, or high frequency and low latency data communication. To this end, we present an approach for describing distributed applications in terms of graphs of communicating nodes. In particular, we develop a formal model for capturing the communication between nodes, by including dynamic and static data producing devices, data consuming client applications, as well as devices that can serve as data produces and consumers at the same time. In our model, we characterise nodes by their frequencies of data exchange. We complement our model with a decision algorithm for determining the pull/push communication direction to optimise the amount of redundantly transferred data (i.e., data that is pushed but cannot be processed or data that is pulled but is not yet updated). The presented work lays the foundation for creating distributed applications which can automatically optimise data exchange.	Towards Optimising the Data Flow in Distributed Applications	NA:NA:NA	2015
Ronnie Mitra	Well-designed Web APIs must provide high levels of usability and must "get it right" on the first release. One strategy for accomplishing this feat is to identify usability issues early in the design process before a public release. Sketching is a useful way of improving the user experience early in the design phase. Designers can create many sketches and learn from them. The Rapido tool is designed to automate the Web API sketching process and help designers improve usability in an iterative fashion.	Rapido: A Sketching Tool for Web API Designers	NA	2015
Michael Petychakis:Fenareti Lampathaki:Dimitrios Askounis	During the past years, the data deluge that prevails in the World Wide Web has been accompanied by a number of APIs that expose business logic. In this paper, we discuss a novel approach to enrich existing API standards definitions with business rules. Taking advantage of the REST principles, we aim at enabling the creation of generic clients that can dynamically navigate through semantically enriched web affordances with the help of Hydra-based Hypermedia API descriptions, which encapsulate the finite state machine of possible actions into SWRL rules.	Adding Rules on Existing Hypermedia APIs	NA:NA:NA	2015
Ricardo Baeza-Yates:Meeyoung Cha	NA	Session details: Tutorials	NA:NA	2015
Hao Ma:Yan Ke	Entities and the Knowledge about the entities have become indispensable building blocks in modern search engines. This tutorial aims to present the current state of research in the emerging semantic search and recommendation field by studying how to help users effectively explore the knowledge base as well as answer their information needs. Many live applications and systems will be demonstrated throughout this tutorial. After the completion of the tutorial, the audience will have an introduction and overview of what is the emerging topics of entity recommendation and understanding. The audience will learn and be able to understand some current research work as well as industry practices using computational intelligence techniques in entity recommendation and understanding. One of the major goals of this tutorial is to help audience identify a few research directions that could have big impact in the near future.	An Introduction to Entity Recommendation and Understanding	NA:NA	2015
Antoine Bordes:Evgeniy Gabrilovich	Recent years have witnessed a proliferation of large-scale knowledge graphs, such as Freebase, Google's Knowledge Graph, YAGO, Facebook's Entity Graph, and Microsoft's Satori. Whereas there is a large body of research on mining homogeneous graphs, this new generation of information networks are highly heterogeneous, with thousands of entity and relation types and billions of instances of vertices and edges. In this tutorial, we will present the state of the art in constructing, mining, and growing knowledge graphs. The purpose of the tutorial is to equip newcomers to this exciting field with an understanding of the basic concepts, tools and methodologies, available datasets, and open research challenges. A publicly available knowledge base (Freebase) will be used throughout the tutorial to exemplify the different techniques.	Constructing and Mining Web-Scale Knowledge Graphs: WWW 2015 Tutorial	NA:NA	2015
Kyomin Jung:Byoung-Tak Zhang:Prasenjit Mitra	Deep learning is a machine learning technology that automatically extracts higher-level representations from raw data by stacking multiple layers of neuron-like units. The stacking allows for extracting representations of increasingly-complex features without time-consuming, offline feature engineering. Recent success of deep learning has shown that it outperforms state-of-the-art systems in image processing, voice recognition, web search, recommendation systems, etc [1]. A lot of industrial-scale big data processing systems including IBM Watson's Jeopardy Contest 2011, Google Now, Facebook's face recognition system, and the voice recognition systems by Google and Microsoft use deep learning [2][3][6]. Deep learning has a huge potential to improve the intelligence of the web and the web service systems by efficiently and effectively mining big data on the Web[4][5]. This tutorial provides the basics of deep learning as well as its key applications. We give the motivation and underlying ideas of deep learning and describe the architectures and learning algorithms for various deep learning models. We also cover applications of deep learning for image and video processing, natural language and text data analysis, social data analytics, and wearable IoT sensor data with an emphasis in the domain of Web systems. We will deliver the key insight and understanding of these techniques, using graphical illustrations and examples that could be important in analyzing a large amount of Web data. The tutorial is prepared to attract general audience at the WWW Conference, who are interested in machine learning and big data analysis for Web data. The tutorial consists of five parts. The first part presents the basics of neural networks, and their structures. Then we explain the training algorithm via backpropagation, which is a common method of training artificial neural networks including deep neural networks. We will emphasize how each of these concepts can be used in various Web data analysis. In the second part of the tutorial, we describe the learning algorithms for deep neural networks and related ideas, such as contrastive divergence, wake-sleep algorithms, and Monte Carlo simulation. We then describe various kinds of deep architectures, including stacked autoencoders, deep belief networks [7], convolutional neural networks [8], and deep hypernetworks [9]. In the third part, we present more details of the recursive neural networks, which can learn structured tree outputs as well as vector representations for phrases and sentences. We first show how training the recursive neural network can be achieved by a modified version of the back-propagation algorithm introduced before. These modifications allow the algorithm to work on tree structures. Then we will present its applications to sentence analysis including POS tagging, and sentiment analysis. The fourth part discusses the neural networks used to generate word embeddings, such as Word2Vec [10], DSSM for deep semantic similarity [11], and object detection in images [12], such as GoogLeNet, and AlexNet. We will explain in detail the applications of these deep learning techniques in the analysis of various social network data. By this point, the audience should have a clear understanding of how to build a deep learning system for word, sentence and document level tasks. The fifth part of the tutorial will cover other application examples of deep learning. These include object segmentation and action recognition from videos [9], web data analytics, and wearable/IoT sensor data modeling for smart services.	Deep Learning for the Web	NA:NA:NA	2015
Manuel Gomez-Rodriguez:Le Song	In recent years, there has been an increasing effort on developing realistic models, and learning and inference algorithms to understand, predict, and influence diffusion over networks. This has been in part due to the increasing availability and granularity of large-scale diffusion data, which, in principle, allows for understanding and modeling not only macroscopic diffusion but also microscopic (node-level) diffusion. To this aim, a bottom-up approach has been typically considered, which starts by considering how particular ideas, pieces of information, products, or, more generally, contagions spread locally from node to node apparently at random to later produce global, macroscopic patterns at a network level. However, this bottom-up approach also raises significant modeling, algorithmic and computational challenges which require leveraging methods from machine learning, probabilistic modeling, event history analysis and graph theory, as well as the nascent field of network science. In this tutorial, we will present several diffusion models designed for fine-grained large-scale diffusion data, present some canonical research problem in the context of diffusion, and introduce state-of-the-art algorithms to solve some of these problems, in particular, network estimation, influence estimation and influence control.	Diffusion in Social and Information Metworks: Research Problems; Probabilistic Models & Machine Learning Methods	NA:NA	2015
Rodrygo L.T. Santos:Pablo Castells:Ismail Sengor Altingovde:Fazli Can	This tutorial aims to provide a unifying account of current research on diversity and novelty in different web information systems. In particular, the tutorial will cover the motivations, as well as the most established approaches for producing and evaluating diverse results in search engines, recommender systems, and data streams, all within the context of the World Wide Web. By contrasting the state-of-the-art in these multiple domains, this tutorial aims to derive a common understanding of the diversification problem and the existing solutions, their commonalities and differences, as a means to foster new research directions.	Diversity and Novelty on the Web: Search, Recommendation, and Data Streaming Aspects	NA:NA:NA:NA	2015
Sihem Amer-Yahia:Senjuti Basu Roy	Forming and exploring complex objects is at the heart of a variety of emerging web applications. Historically, existing work on complex objects has been developed in two separate areas: composite item retrieval and team formation. At the same time, emerging applications that harness the wisdom of crowd workers, such as, document editing by workers, sentence translation by fans (or fan-subbing), innovative design, citizen science or journalism, represent complex crowdsourcing, in which an object may represent a complex task formed by a set of sub-tasks or a team of workers who work together to solve the task. The goal of this tutorial is to bridge the gap between composite item retrieval and team formation and define new research directions for complex crowdsourcing applications	From Complex Object Exploration to Complex Crowdsourcing.	NA:NA	2015
Cheng-Te Li:Hsun-Ping Hsieh	With the maturity of wireless communication techniques, GPS-equipped mobile devices become ubiquitous, and location-acquisition technologies and services are flourishing. These location applications as well as mobile devices, developed and combined with the social networking services, foster the emergence of geo-social media, a novel type of user-generated geo-social data, such as data from Facebook, Twitter, and Foursquare. In geo-social media, social connections and geo-location information of users are the essential elements, which keep track of their user interactions and their spatial-temporal activities. While social interactions are depicted by online network structures, and geographical activities are usually represented as check-in records. Due to the pervasive mobility of users, a huge amount of user-generated geo-social data is rapidly generated. Such big geo-social data not only collectively represents diverse kinds of real-world human activities, but also serves as a handy resource for various geo-social applications. In this tutorial, we aim to present the recent advances on geo-social media analytics in a systematic manner, consisting of five parts: (a) properties of geo-social networks, which unveil the relationships between human mobility and social structures; (b) geo-social link prediction, using geographical, mobility, activity features with various inference models; (c) location recommendation, leveraging personal, social, contextual, geographical, and content information; (d) geo-social influence propagation and maximization; and (e) connecting online and offline social networks for revisiting conventional SNA wisdom and developing applications that bridge virtual and physical social worlds. We also highlight the unsolved problems for each of the aforementioned topics and future directions of geo-social media analytics. We believe this tutorial can benefit the research communities of mobile web, data mining, information retrieval, social network analysis, recommender system, and marketing and advertisement.	Geo-Social Media Analytics	NA:NA	2015
Johannes Hoffart:Nicoleta Preda:Fabian M. Suchanek:Gerhard Weikum	NA	Knowledge Bases for Web Content Analytics	NA:NA:NA:NA	2015
Rok Sosic:Jure Leskovec	Many techniques for the modeling, analysis and optimization of Web related datasets are based on studies of large scale networks, where a network can contain hundreds of millions of nodes and billions of edges. Network analysis tools must provide not only extensive functionality, but also high performance in processing these large networks. The tutorial will present Stanford Network Analysis Platform (SNAP), a general purpose, high performance system for analysis and manipulation of large networks. SNAP is being used widely in studies of the Web datasets. SNAP consists of open source software, which provides a rich set of functions for performing network analytics, and a popular repository of publicly available real world network datasets. SNAP software APIs are available in Python and C++. The tutorial will cover all aspects of SNAP, including APIs and datasets. The tutorial will include a hands-on component, where the participants will have the opportunity to use SNAP on their computers.	Large Scale Network Analytics with SNAP: Tutorial at the World Wide Web 2015 Conference	NA:NA	2015
Dongwon Lee:Huan Liu	This tutorial covers the state-of-the-art developments in LIKE and recommendation in social media. It is designed for graduate students, practitioners, or IT managers with general understanding on WWW and social media. No prerequisite is expected.	LIKE and Recommendation in Social Media	NA:NA	2015
Spiros Papadimitriou:Tina Eliassi-Rad	The fairly recent explosion in the availability of reasonably fast wireless and mobile data networks has spurred demand for more capable mobile computing devices. Conversely, the emergence of new devices increases demand for better networks, creating a virtuous cycle. The current concept of a smartphone as an always-connected computing device with multiple sensing modalities was brought into the mainstream by the Apple iPhone just a few years ago. Such devices are now seeing an explosive growth. Additionally, for many people in the world, such devices will be the first computers they use. Furthermore, small, cheap, always-connected devices (standalone or peripheral) with additional sensing capabilities are very recently emerging, further blurring the lines between the Web, mobile applications (a.k.a.~apps), and the real world. All of this opens up countless possibilities for data collection and analysis, for a broad range of applications. In this tutorial, we survey the state-of-the-art in terms of mining mobility data across different application areas such as ads, geo-social, privacy and security. Our tutorial consists of three parts. (1) We summarize the possibilities and challenges in the collection of data from various sensing modalities. (2) We cover cross-cutting challenges such as real-time analysis and security; and we outline cross-cutting algorithms for mobile data mining such as network inference and streaming algorithms. (3) We focus on how all of this can be usefully applied to broad classes of applications, notably mobile and location-based social, mobile advertising and search, mobile Web, and privacy and security. We conclude by showcasing the opportunities for new data collection techniques and new data mining methods to meet the challenges and applications that are unique to the mobile arena (e.g., leveraging emerging embedded computing and sensing technologies to collect a large variety and volume of new kinds of "big data").	Mining Mobility Data	NA:NA	2015
Eytan Bakshy:Sean J. Taylor	Experiments are the gold standard for establishing causal relationships. While Web-based experiments ("A/B tests") have routinely been used to assess alternative ranking models or user interface designs, they have become increasingly popular for answering important questions in the social sciences. This tutorial teaches attendees how to design, plan, implement, and analyze online experiments. First, we review basic concepts in causal inference and motivate the need for experiments. Then we will discuss basic statistical tools to help plan experiments: exploratory analysis, power calculations, and the use of simulation in R. We then discuss statistical methods to estimate causal quantities of interest and construct appropriate confidence intervals. We then discuss how to design and implement online experiments using PlanOut, an open-source toolkit for advanced online experimentation used at Facebook. We will show how to implement a variety of experiments, including basic A/B tests, within-subjects designs, as well as more sophisticated experiments. We demonstrate how experimental designs from social computing literature can be implemented, and then collaboratively plan and implement an experiment together. We then discuss issues with logging and common errors in the deployment and analysis of experiments. Finally, we will conclude the tutorial with a discussion of strategies and scalable methods for analyzing online experiments, including working with weighted data, and data with single and multi-way dependence. Throughout the tutorial, attendees will be given code examples and participate in the planning, implementation, and analysis of a Web application using Python, PlanOut, and R.	Online Experiments for Computational Social Science	NA:NA	2015
Deepak Ajwani:Marcel Karnstedt:Alessandra Sala	Analyzing and processing large graphs is of fundamental importance for an ever-growing number of applications. Significant advancements in the last few years at both, systems and algorithmic side, let graph processing become increasingly scalable and efficient. Often, these advances are still not well-known and well-understood outside the systems and algorithms communities. In particular, there is very little understanding of the various trade-offs involved in the usage of particular combinations of algorithms, data structures, and systems. This tutorial will have a particular focus on this aspect, imparting theoretical knowledge intertwined with hands-on experience. Since there is no clearly winning system/algorithm combination that performs best on all the different metrics, it is of utmost importance to understand the pros and cons of the various alternatives. The tutorial will enable application developers in industry and academics, students as well as researchers to make corresponding decisions in an informed way. The participants do neither require any particular a-priori knowledge apart from a basic understanding of core computer science concepts, nor any special equipment apart from their laptop. After a general introduction, we will describe the critical dimensions that need to be tackled together to effectively and efficiently overcome problems in large graph processing: data representation, data storage, acceleration via multi-core programming, and horizontally scalable graph-processing infrastructures. Thereafter, we will provide an overview of existing graph-processing systems and graph databases. This will be followed by hands-on experiences with popular representatives of such systems. Finally, we will provide a detailed description of algorithms used in these systems for fundamental problems like shortest paths and Pagerank, how they are implemented, and how this affects the overall performance. We will also cover basic data structures such as distance oracles that can be built on these systems to efficiently answer distance queries for real-world graphs.	Processing Large Graphs: Representations, Storage, Systems and Algorithms	NA:NA:NA	2015
Konstantinos Pelechrinis:Daniele Quercia	Based on a recent report from the United Nations, more than 50% of the world's population currently lives in cities. This percentage is projected to increase to 70% by the year 2050 [1]. As massive amounts of people move to urban areas there is a need for cities to be run more efficiently, while at the same time improving the quality of life of their dwellers. Nevertheless, the exact same force that sets the above requirement, i.e., the proliferation of urbanization levels, makes this task much harder and challenging, especially in megacities. Despite the aforementioned conflicting dynamics, many city management operations can be facilitated by appropriate exploitation of the unprecedented amount of data that can be made available to authorities from a variety of sources. In the era of big data and ubiquitous and pervasive mobile computing, different types of sensors such as parking meters, weather sensors, traffic sensors, pipe sensors, public transportation ticket readers and even human sensors (e.g., through web technologies, social media or cell phone usage data) can assist in these efforts. Furthermore, civic applications can exploit web and mobile technologies to deliver a livable, sustainable and resilient environment to the citizens. Harnessing these information streams and technologies presents many challenges that are in the epicenter of this tutorial. In this tutorial we will present the current practices and methods in the emerging field of urban informatics as well as the open challenges. The topics to be covered in this tutorial are structured in three sessions: (i) introduction to urban studies and urban informatics, (ii) civic data and technologies for urban sensing and (iii) analytical techniques used for urban data analysis. Finally, we will also provide concrete examples of urban informatics applications.	Urban Informatics and the Web	NA:NA	2015
Sören Auer:Tim Berners-Lee:Christian Bizer:Tom Heath	This paper presents a brief summary of the eight workshop on Linked Data on the Web. The LDOW 2013 workshop is held in conjunction with the World Wide Web conference 2013. The focus is on data publishing, integration and consumption using RDF and other semantic representation formalisms and technologies.	LDOW 2013: The 8th Workshop on Linked Data on the Web	NA:NA:NA:NA	2015
A.P.J. Abdul Kalam	NA	How can scientists help to spread the web to all sections of the society	NA	2018
Sir Tim Berners-Lee	How can we best design Web technology to support the features we would like of our society such as: openness, justice, transparency, accountability, participation, innovation, science and democracy?	Designing the web for an open society	NA	2018
Christos H. Papadimitriou	The advent of the Internet brought parallel paradigm shifts to both Economics and Computer Science. Computer scientists realized that large-scale performing systems can emerge from the interaction of selfish agents and that incentives are a quintessential part of a good system design. And economists saw that the default platforms of economic transactions are computational and interconnected. Algorithmic Game Theory is a subdiscipline that emerged from this turmoil, revisiting some of the most important problems in Economics and Game Theory from a computational and network perspective. This talk will survey some of the major themes, results and challenges in this field.	Games, algorithms, and the Internet	NA	2018
Yelong Shen:Jun Yan:Shuicheng Yan:Lei Ji:Ning Liu:Zheng Chen	Understanding user intent from her sequential search behaviors, i.e. predicting the intent of each user query in a search session, is crucial for modern Web search engines. However, due to the huge number of user behavior variables and coarse level intent labels defined by human editors, it is very difficult to directly model user behavioral dynamics or user intent dynamics in user search sessions. In this paper, we propose a novel Sparse Hidden-Dynamic Conditional Random Fields (SHDCRF) model for user intent learning from their search sessions. Through incorporating the proposed hidden state variables, SHDCRF aims to learn a substructure, i.e. a set of related hidden variables, for each intent label and they are used to model the intermediate dynamics between user intent labels and user behavioral variables. In addition, SHDCRF learns a sparse relation between the hidden variables and intent labels to make the hidden state variables explainable. Extensive experiment results, on real user search sessions from a popular commercial search engine show that the proposed SHDCRF model significantly outperforms in terms of intent prediction results that those classical solutions such as Support Vector Machine (SVM), Conditional Random Field (CRF) and Latnet-Dynamic Conditional Random Field (LDCRF).	Sparse hidden-dynamics conditional random fields for user intent understanding	NA:NA:NA:NA:NA:NA	2018
Botao Hu:Yuchen Zhang:Weizhu Chen:Gang Wang:Qiang Yang	Modeling a user's click-through behavior in click logs is a challenging task due to the well-known position bias problem. Recent advances in click models have adopted the examination hypothesis which distinguishes document relevance from position bias. In this paper, we revisit the examination hypothesis and observe that user clicks cannot be completely explained by relevance and position bias. Specifically, users with different search intents may submit the same query to the search engine but expect different search results. Thus, there might be a bias between user search intent and the query formulated by the user, which can lead to the diversity in user clicks. This bias has not been considered in previous works such as UBM, DBN and CCM. In this paper, we propose a new intent hypothesis as a complement to the examination hypothesis. This hypothesis is used to characterize the bias between the user search intent and the query in each search session. This hypothesis is very general and can be applied to most of the existing click models to improve their capacities in learning unbiased relevance. Experimental results demonstrate that after adopting the intent hypothesis, click models can better interpret user clicks and achieve a significant NDCG improvement.	Characterizing search intent diversity into click models	NA:NA:NA:NA:NA	2018
Lydia B. Chilton:Jaime Teevan	Web search engines have historically focused on connecting people with information resources. For example, if a person wanted to know when their flight to Hyderabad was leaving, a search engine might connect them with the airline where they could find flight status information. However, search engines have recently begun to try to meet people's search needs directly, providing, for example, flight status information in response to queries that include an airline and a flight number. In this paper, we use large scale query log analysis to explore the challenges a search engine faces when trying to meet an information need directly in the search result page. We look at how people's interaction behavior changes when inline content is returned, finding that such content can cannibalize clicks from the algorithmic results. We see that in the absence of interaction behavior, an individual's repeat search behavior can be useful in understanding the content's value. We also discuss some of the ways user behavior can be used to provide insight into when inline answers might better trigger and what types of additional information might be included in the results.	Addressing people's information needs directly in a web search result page	NA:NA	2018
Xiaofei Zhu:Jiafeng Guo:Xueqi Cheng:Pan Du:Hua-Wei Shen	Query recommendation has been considered as an effective way to help search users in their information seeking activities. Traditional approaches mainly focused on recommending alternative queries with close search intent to the original query. However, to only take relevance into account may generate redundant recommendations to users. It is better to provide diverse as well as relevant query recommendations, so that we can cover multiple potential search intents of users and minimize the risk that users will not be satisfied. Besides, previous query recommendation approaches mostly relied on measuring the relevance or similarity between queries in the Euclidean space. However, there is no convincing evidence that the query space is Euclidean. It is more natural and reasonable to assume that the query space is a manifold. In this paper, therefore, we aim to recommend diverse and relevant queries based on the intrinsic query manifold. We propose a unified model, named manifold ranking with stop points, for query recommendation. By turning ranked queries into stop points on the query manifold, our approach can generate query recommendations by simultaneously considering both diversity and relevance in a unified way. Empirical experimental results on a large scale query log of a commercial search engine show that our approach can effectively generate highly diverse as well as closely related query recommendations.	A unified framework for recommending diverse and relevant queries	NA:NA:NA:NA:NA	2018
Idan Szpektor:Aristides Gionis:Yoelle Maarek	The ability to aggregate huge volumes of queries over a large population of users allows search engines to build precise models for a variety of query-assistance features such as query recommendation, correction, etc. Yet, no matter how much data is aggregated, the long-tail distribution implies that a large fraction of queries are rare. As a result, most query assistance services perform poorly or are not even triggered on long-tail queries. We propose a method to extend the reach of query assistance techniques (and in particular query recommendation) to long-tail queries by reasoning about rules between query templates rather than individual query transitions, as currently done in query-flow graph models. As a simple example, if we recognize that 'Montezuma' is a city in the rare query "Montezuma surf" and if the rule 'city surf → beach has been observed, we are able to offer "Montezuma beach" as a recommendation, even if the two queries were never observed in a same session. We conducted experiments to validate our hypothesis, first via traditional small-scale editorial assessments but more interestingly via a novel automated large scale evaluation methodology. Our experiments show that general coverage can be relatively increased by 24% using templates without penalizing quality. Furthermore, for 36% of the 95M queries in our query flow graph, which have no out edges and thus could not be served recommendations, we can now offer at least one recommendation in 98% of the cases.	Improving recommendation for long-tail queries via templates	NA:NA:NA	2018
Yuanhua Lv:Taesup Moon:Pranam Kolari:Zhaohui Zheng:Xuanhui Wang:Yi Chang	With the explosive growth of online news readership, recommending interesting news articles to users has become extremely important. While existing Web services such as Yahoo! and Digg attract users' initial clicks by leveraging various kinds of signals, how to engage such users algorithmically after their initial visit is largely under-explored. In this paper, we study the problem of post-click news recommendation. Given that a user has perused a current news article, our idea is to automatically identify "related" news articles which the user would like to read afterwards. Specifically, we propose to characterize relatedness between news articles across four aspects: relevance, novelty, connection clarity, and transition smoothness. Motivated by this understanding, we define a set of features to capture each of these aspects and put forward a learning approach to model relatedness. In order to quantitatively evaluate our proposed measures and learn a unified relatedness function, we construct a large test collection based on a four-month commercial news corpus with editorial judgments. The experimental results show that the proposed heuristics can indeed capture relatedness, and that the learned unified relatedness function works quite effectively.	Learning to model relatedness for news recommendation	NA:NA:NA:NA:NA:NA	2018
Ashok Kumar Ponnuswami:Kumaresh Pattabiraman:Desmond Brand:Tapas Kanungo	Modern day federated search engines aggregate heterogeneous types of results from multiple vertical search engines and compose a single search engine result page (SERP). The search engine aggregates the results and produces one ranked list, constraining the vertical results to specific slots on the SERP. The usual way to compare two ranking algorithms is to first fix their operating points (internal thresholds), and then run an online experiment that lasts multiple weeks. Online user engagement metrics are then compared to decide which algorithm is better. However, this method does not characterize and compare the behavior over the entire span of operating points. Furthermore, this time-consuming approach is not practical if we have to conduct the experiment over numerous operating points. In this paper we propose a method of characterizing the performance of models that allows us to predict answers to "what if" questions about online user engagement using click-logs over the entire span of feasible operating points. We audition verticals at various slots on the SERP and generate click-logs. This log is then used to create operating curves between variables of interest (for example between result quality and click-through). The operating point for the system then can be chosen to achieve a specific trade-off between the variables. We apply this methodology to predict i) the online performance of two different models, ii) the impact of changing internal quality thresholds on clickthrough, iii) the behavior of introducing a new feature, iv) which machine learning loss function will give better online engagement, v) the impact of sampling distribution of head and tail queries in the training process. The results are reported on a well-known federated search engine. We validate the predictions with online experiments.	Model characterization curves for federated search using click-logs: predicting user engagement metrics for the span of feasible operating points	NA:NA:NA:NA	2018
Jangwon Seo:Fernando Diaz:Evgeniy Gabrilovich:Vanja Josifovski:Bo Pang	Proactive link suggestion leads to improved user experience by allowing users to reach relevant information with fewer clicks, fewer pages to read, or simply faster because the right pages are prefetched just in time. In this paper we tackle two new scenarios for link suggestion, which were not covered in prior work owing to scarcity of historical browsing data. In the web search scenario, we propose a method for generating quick links - additional entry points into Web sites, which are shown for top search results for navigational queries - for tail sites, for which little browsing statistics is available. Beyond Web search, we also propose a method for link suggestion in general web browsing, effectively anticipating the next link to be followed by the user. Our approach performs clustering of Web sites in order to aggregate information across multiple sites, and enables relevant link suggestion for virtually any site, including tail sites and brand new sites for which little historical data is available. Empirical evaluation confirms the validity of our method using editorially labeled data as well as real-life search and browsing data from a major US search engine.	Generalized link suggestions via web site clustering	NA:NA:NA:NA:NA	2018
Wei Hu:Jianfeng Chen:Yuzhong Qu	An object on the Semantic Web is likely to be denoted with multiple URIs by different parties. Object coreference resolution is to identify "equivalent" URIs that denote the same object. Driven by the Linking Open Data (LOD) initiative, millions of URIs have been explicitly linked with owl:sameAs statements, but potentially coreferent ones are still considerable. Existing approaches address the problem mainly from two directions: one is based upon equivalence inference mandated by OWL semantics, which finds semantically coreferent URIs but probably omits many potential ones; the other is via similarity computation between property-value pairs, which is not always accurate enough. In this paper, we propose a self-training approach for object coreference resolution on the Semantic Web, which leverages the two classes of approaches to bridge the gap between semantically coreferent URIs and potential candidates. For an object URI, we firstly establish a kernel that consists of semantically coreferent URIs based on owl:sameAs, (inverse) functional properties and (max-)cardinalities, and then extend such kernel iteratively in terms of discriminative property-value pairs in the descriptions of URIs. In particular, the discriminability is learnt with a statistical measurement, which not only exploits key characteristics for representing an object, but also takes into account the matchability between properties from pragmatics. In addition, frequent property combinations are mined to improve the accuracy of the resolution. We implement a scalable system and demonstrate that our approach achieves good precision and recall for resolving object coreference, on both benchmark and large-scale datasets.	A self-training approach for resolving object coreference on the semantic web	NA:NA:NA	2018
Matthias Hagen:Martin Potthast:Benno Stein:Christof Bräutigam	We address the problem of query segmentation: given a keyword query, the task is to group the keywords into phrases, if possible. Previous approaches to the problem achieve reasonable segmentation performance but are tested only against a small corpus of manually segmented queries. In addition, many of the previous approaches are fairly intricate as they use expensive features and are difficult to be reimplemented. The main contribution of this paper is a new method for query segmentation that is easy to implement, fast, and that comes with a segmentation accuracy comparable to current state-of-the-art techniques. Our method uses only raw web n-gram frequencies and Wikipedia titles that are stored in a hash table. At the same time, we introduce a new evaluation corpus for query segmentation. With about 50,000 human-annotated queries, it is two orders of magnitude larger than the corpus being used up to now.	Query segmentation revisited	NA:NA:NA:NA	2018
Ziv Bar-Yossef:Naama Kraus	Query auto completion is known to provide poor predictions of the user's query when her input prefix is very short (e.g., one or two characters). In this paper we show that context, such as the user's recent queries, can be used to improve the prediction quality considerably even for such short prefixes. We propose a context-sensitive query auto completion algorithm, NearestCompletion, which outputs the completions of the user's input that are most similar to the context queries. To measure similarity, we represent queries and contexts as high-dimensional term-weighted vectors and resort to cosine similarity. The mapping from queries to vectors is done through a new query expansion technique that we introduce, which expands a query by traversing the query recommendation tree rooted at the query. In order to evaluate our approach, we performed extensive experimentation over the public AOL query log. We demonstrate that when the recent user's queries are relevant to the current query she is typing, then after typing a single character, NearestCompletion's MRR is 48% higher relative to the MRR of the standard MostPopularCompletion algorithm on average. When the context is irrelevant, however, NearestCompletion's MRR is essentially zero. To mitigate this problem, we propose HybridCompletion, which is a hybrid of NearestCompletion with MostPopularCompletion. HybridCompletion is shown to dominate both NearestCompletion and MostPopularCompletion, achieving a total improvement of 31.5% in MRR relative to MostPopularCompletion on average.	Context-sensitive query auto-completion	NA:NA	2018
Huizhong Duan:Bo-June (Paul) Hsu	In this paper, we study the problem of online spelling correction for query completions. Misspelling is a common phenomenon among search engines queries. In order to help users effectively express their information needs, mechanisms for automatically correcting misspelled queries are required. Online spelling correction aims to provide spell corrected completion suggestions as a query is incrementally entered. As latency is crucial to the utility of the suggestions, such an algorithm needs to be not only accurate, but also efficient. To tackle this problem, we propose and study a generative model for input queries, based on a noisy channel transformation of the intended queries. Utilizing spelling correction pairs, we train a Markov n-gram transformation model that captures user spelling behavior in an unsupervised fashion. To find the top spell-corrected completion suggestions in real-time, we adapt the A* search algorithm with various pruning heuristics to dynamically expand the search space efficiently. Evaluation of the proposed methods demonstrates a substantial increase in the effectiveness of online spelling correction over existing techniques.	Online spelling correction for query completion	NA:NA	2018
Paul Dütting:Monika Henzinger:Ingmar Weber	Auctions are widely used on the Web. Applications range from internet advertising to platforms such as eBay. In most of these applications the auctions in use are single/multi-item auctions with unit demand. The main drawback of standard mechanisms for this type of auctions, such as VCG and GSP, is the limited expressiveness that they offer to the bidders. The General Auction Mechanism (GAM) of [1] is taking a first step towards addressing the problem of limited expressiveness by computing a bidder optimal, envy free outcome for linear utility functions with identical slopes and a single discontinuity per bidder-item pair. We show that in many practical situations this does not suffice to adequately model the preferences of the bidders, and we overcome this problem by presenting the first mechanism for piece-wise linear utility functions with non-identical slopes and multiple discontinuities. Our mechanism runs in polynomial time. Like GAM it is incentive compatible for inputs that fulfill a certain non-degeneracy requirement, but our requirement is more general than the requirement of GAM. For discontinuous utility functions that are non-degenerate as well as for continuous utility functions the outcome of our mechanism is a competitive equilibrium. We also show how our mechanism can be used to compute approximately bidder optimal, envy free outcomes for a general class of continuous utility functions via piece-wise linear approximation. Finally, we prove hardness results for even more expressive settings.	An expressive mechanism for auctions on the web	NA:NA:NA	2018
Arpita Ghosh:Preston McAfee	We model the economics of incentivizing high-quality user generated content (UGC), motivated by settings such as online review forums, question-answer sites, and comments on news articles and blogs. We provide a game-theoretic model within which to study the problem of incentivizing high quality UGC, in which contributors are strategic and motivated by exposure. Our model has the feature that both the quality of contributions as well as the extent of participation is determined endogenously in a free-entry Nash equilibrium. The model predicts, as observed in practice, that if exposure is independent of quality, there will be a flood of low quality contributions in equilibrium. An ideal mechanism in this context would elicit both high quality and high participation in equilibrium, with near-optimal quality as the available attention diverges, and should be easily implementable in practice. We consider a very simple elimination mechanism, which subjects each contribution to rating by some number A of viewers, and eliminates any contributions that are not uniformly rated positively. We construct and analyze free-entry Nash equilibria for this mechanism, and show that A can be chosen to achieve quality that tends to optimal, along with diverging participation, as the number of viewers diverges.	Incentivizing high-quality user-generated content	NA:NA	2018
L. Elisa Celis:Gregory Lewis:Markus M. Mobius:Hamid Nazerzadeh	We present a simple auction mechanism which extends the second-price auction with reserve and is truthful in expectation. This mechanism is particularly effective in private value environments where the distribution of valuations are irregular. Bidders can "buy-it-now", or alternatively "take-a-chance" where the top d bidders are equally likely to win. The randomized take-a-chance allocation incentivizes high valuation bidders to buy-it-now. We show that for a large class of valuations, this mechanism achieves similar allocations and revenues as Myerson's optimal mechanism, and outperforms the second-price auction with reserve. In addition, we present an evaluation of bid data from Microsoft's AdECN platform. We find the valuations are irregular, and counterfactual experiments suggest our BIN-TAC mechanism would improve revenue by 11% relative to an optimal second-price mechanism with reserve.	Buy-it-now or take-a-chance: a simple sequential screening mechanism	NA:NA:NA:NA	2018
Randall A. Lewis:Justin M. Rao:David H. Reiley	Measuring the causal effects of online advertising (adfx) on user behavior is important to the health of the WWW publishing industry. In this paper, using three controlled experiments, we show that observational data frequently lead to incorrect estimates of adfx. The reason, which we label "activity bias," comes from the surprising amount of time-based correlation between the myriad activities that users undertake online. In Experiment 1, users who are exposed to an ad on a given day are much more likely to engage in brand-relevant search queries as compared to their recent history for reasons that had nothing do with the advertisement. In Experiment 2, we show that activity bias occurs for page views across diverse websites. In Experiment 3, we track account sign-ups at a competitor's (of the advertiser) website and find that many more people sign-up on the day they saw an advertisement than on other days, but that the true "competitive effect" was minimal. In all three experiments, exposure to a campaign signals doing "more of everything" in given period of time, making it difficult to find a suitable "matched control" using prior behavior. In such cases, the "match" is fundamentally different from the exposed group, and we show how and why observational methods lead to a massive overestimate of adfx in such circumstances.	Here, there, and everywhere: correlated online behaviors can lead to overestimates of the effects of advertising	NA:NA:NA	2018
Michael Grabchak:Narayan Bhamidipati:Rushi Bhatt:Dinesh Garg	Stochastic knapsack problems deal with selecting items with potentially random sizes and rewards so as to maximize the total reward while satisfying certain capacity constraints. A novel variant of this problem, where items are worthless unless collected in bundles, is introduced here. This setup is similar to the Groupon model, where a deal is off unless a minimum number of users sign up for it. Since the optimal algorithm to solve this problem is not practical, several adaptive greedy approaches with reasonable time and memory requirements are studied in detail - theoretically, as well as, experimentally. Worst case performance guarantees are provided for some of these greedy algorithms, while results of experimental evaluation demonstrate that they are much closer to optimal than what the theoretical bounds suggest. Applications include optimizing for online advertising pricing models where advertisers pay only when certain goals, in terms of clicks or conversions, are met. We perform extensive experiments for the situation where there are between two and five ads. For typical ad conversion rates, the greedy policy of selecting items having the highest individual expected reward obtains a value within 5% of optimal over 95% of the time for a wide selection of parameters.	Adaptive policies for selecting groupon style chunked reward ads in a stochastic knapsack framework	NA:NA:NA:NA	2018
Danilo Ardagna:Barbara Panicucci:Mauro Passacantando	Cloud computing is an emerging paradigm which allows the on-demand delivering of software, hardware, and data as services. As cloud-based services are more numerous and dynamic, the development of efficient service provisioning policies become increasingly challenging. Game theoretic approaches have shown to gain a thorough analytical understanding of the service provisioning problem. In this paper we take the perspective of Software as a Service (SaaS) providers which host their applications at an Infrastructure as a Service (IaaS) provider. Each SaaS needs to comply with quality of service requirements, specified in Service Level Agreement (SLA) contracts with the end-users, which determine the revenues and penalties on the basis of the achieved performance level. SaaS providers want to maximize their revenues from SLAs, while minimizing the cost of use of resources supplied by the IaaS provider. Moreover, SaaS providers compete and bid for the use of infrastructural resources. On the other hand, the IaaS wants to maximize the revenues obtained providing virtualized resources. In this paper we model the service provisioning problem as a Generalized Nash game, and we propose an efficient algorithm for the run time management and allocation of IaaS resources to competing SaaSs.	A game theoretic formulation of the service provisioning problem in cloud systems	NA:NA:NA	2018
Junjie Zhang:Christian Seifert:Jack W. Stokes:Wenke Lee	A drive-by download attack occurs when a user visits a webpage which attempts to automatically download malware without the user's consent. Attackers sometimes use a malware distribution network (MDN) to manage a large number of malicious webpages, exploits, and malware executables. In this paper, we provide a new method to determine these MDNs from the secondary URLs and redirect chains recorded by a high-interaction client honeypot. In addition, we propose a novel drive-by download detection method. Instead of depending on the malicious content used by previous methods, our algorithm first identifies and then leverages the URLs of the MDN's central servers, where a central server is a common server shared by a large percentage of the drive-by download attacks in the same MDN. A set of regular expression-based signatures are then generated based on the URLs of each central server. This method allows additional malicious webpages to be identified which launched but failed to execute a successful drive-by download attack. The new drive-by detection system named ARROW has been implemented, and we provide a large-scale evaluation on the output of a production drive-by detection system. The experimental results demonstrate the effectiveness of our method, where the detection coverage has been boosted by 96% with an extremely low false positive rate.	ARROW: GenerAting SignatuRes to Detect DRive-By DOWnloads	NA:NA:NA:NA	2018
Davide Canali:Marco Cova:Giovanni Vigna:Christopher Kruegel	Malicious web pages that host drive-by-download exploits have become a popular means for compromising hosts on the Internet and, subsequently, for creating large-scale botnets. In a drive-by-download exploit, an attacker embeds a malicious script (typically written in JavaScript) into a web page. When a victim visits this page, the script is executed and attempts to compromise the browser or one of its plugins. To detect drive-by-download exploits, researchers have developed a number of systems that analyze web pages for the presence of malicious code. Most of these systems use dynamic analysis. That is, they run the scripts associated with a web page either directly in a real browser (running in a virtualized environment) or in an emulated browser, and they monitor the scripts' executions for malicious activity. While the tools are quite precise, the analysis process is costly, often requiring in the order of tens of seconds for a single page. Therefore, performing this analysis on a large set of web pages containing hundreds of millions of samples can be prohibitive. One approach to reduce the resources required for performing large-scale analysis of malicious web pages is to develop a fast and reliable filter that can quickly discard pages that are benign, forwarding to the costly analysis tools only the pages that are likely to contain malicious code. In this paper, we describe the design and implementation of such a filter. Our filter, called Prophiler, uses static analysis techniques to quickly examine a web page for malicious content. This analysis takes into account features derived from the HTML contents of a page, from the associated JavaScript code, and from the corresponding URL. We automatically derive detection models that use these features using machine-learning techniques applied to labeled datasets. To demonstrate the effectiveness and efficiency of Prophiler, we crawled and collected millions of pages, which we analyzed for malicious behavior. Our results show that our filter is able to reduce the load on a more costly dynamic analysis tools by more than 85%, with a negligible amount of missed malicious pages.	Prophiler: a fast filter for the large-scale detection of malicious web pages	NA:NA:NA:NA	2018
John P. John:Fang Yu:Yinglian Xie:Arvind Krishnamurthy:Martín Abadi	Many malicious activities on the Web today make use of compromised Web servers, because these servers often have high pageranks and provide free resources. Attackers are therefore constantly searching for vulnerable servers. In this work, we aim to understand how attackers find, compromise, and misuse vulnerable servers. Specifically, we present heat-seeking honeypots that actively attract attackers, dynamically generate and deploy honeypot pages, then analyze logs to identify attack patterns. Over a period of three months, our deployed honeypots, despite their obscure location on a university network, attracted more than 44,000 attacker visits from close to 6,000 distinct IP addresses. By analyzing these visits, we characterize attacker behavior and develop simple techniques to identify attack traffic. Applying these techniques to more than 100 regular Web servers as an example, we identified malicious queries in almost all of their logs.	Heat-seeking honeypots: design and experience	NA:NA:NA:NA:NA	2018
Xiaoxin Yin:Wenzhao Tan	Accessing online information from various data sources has become a necessary part of our everyday life. Unfortunately such information is not always trustworthy, as different sources are of very different qualities and often provide inaccurate and conflicting information. Existing approaches attack this problem using unsupervised learning methods, and try to infer the confidence of the data value and trustworthiness of each source from each other by assuming values provided by more sources are more accurate. However, because false values can be widespread through copying among different sources and out-of-date data often overwhelm up-to-date data, such bootstrapping methods are often ineffective. In this paper we propose a semi-supervised approach that finds true values with the help of ground truth data. Such ground truth data, even in very small amount, can greatly help us identify trustworthy data sources. Unlike existing studies that only provide iterative algorithms, we derive the optimal solution to our problem and provide an iterative algorithm that converges to it. Experiments show our method achieves higher accuracy than existing approaches, and it can be applied on very huge data sets when implemented with MapReduce.	Semi-supervised truth discovery	NA:NA	2018
Raju Balakrishnan:Subbarao Kambhampati	One immediate challenge in searching the deep web databases is source selection - i.e. selecting the most relevant web databases for answering a given query. The existing database selection methods (both text and relational) assess the source quality based on the query-similarity-based relevance assessment. When applied to the deep web these methods have two deficiencies. First is that the methods are agnostic to the correctness (trustworthiness) of the sources. Secondly, the query based relevance does not consider the importance of the results. These two considerations are essential for the open collections like the deep web. Since a number of sources provide answers to any query, we conjuncture that the agreements between these answers are likely to be helpful in assessing the importance and the trustworthiness of the sources. We compute the agreement between the sources as the agreement of the answers returned. While computing the agreement, we also measure and compensate for possible collusion between the sources. This adjusted agreement is modeled as a graph with sources at the vertices. On this agreement graph, a quality score of a source that we call SourceRank, is calculated as the stationary visit probability of a random walk. We evaluate SourceRank in multiple domains, including sources in Google Base, with sizes up to 675 sources. We demonstrate that the SourceRank tracks source corruption. Further, our relevance evaluations show that SourceRank improves precision by 22-60% over the Google Base and the other baseline methods. SourceRank has been implemented in a system called Factal.	SourceRank: relevance and trust assessment for deep web sources based on inter-source agreement	NA:NA	2018
Michael J. Welch:Junghoo Cho:Christopher Olston	Ambiguous queries constitute a significant fraction of search instances and pose real challenges to web search engines. With current approaches the top results for these queries tend to be homogeneous, making it difficult for users interested in less popular aspects to find relevant documents. While existing research in search diversification offers several solutions for introducing variety into the results, the majority of such work is predicated, implicitly or otherwise, on the assumption that a single relevant document will fulfill a user's information need, making them inadequate for many informational queries. In this paper we present a search-diversification algorithm particularly suitable for informational queries by explicitly modeling that the user may need more than one page to satisfy their need. This modeling enables our algorithm to make a well-informed tradeoff between a user's desire for multiple relevant documents, probabilistic information about an average user's interest in the subtopics of a multifaceted query, and uncertainty in classifying documents into those subtopics. We evaluate the effectiveness of our algorithm against commercial search engine results and other modern ranking strategies, demonstrating notable improvement in multiple document scenarios.	Search result diversity for informational queries	NA:NA:NA	2018
Zhijun Yin:Liangliang Cao:Jiawei Han:Chengxiang Zhai:Thomas Huang	This paper studies the problem of discovering and comparing geographical topics from GPS-associated documents. GPS-associated documents become popular with the pervasiveness of location-acquisition technologies. For example, in Flickr, the geo-tagged photos are associated with tags and GPS locations. In Twitter, the locations of the tweets can be identified by the GPS locations from smart phones. Many interesting concepts, including cultures, scenes, and product sales, correspond to specialized geographical distributions. In this paper, we are interested in two questions: (1) how to discover different topics of interests that are coherent in geographical regions? (2) how to compare several topics across different geographical locations? To answer these questions, this paper proposes and compares three ways of modeling geographical topics: location-driven model, text-driven model, and a novel joint model called LGTA (Latent Geographical Topic Analysis) that combines location and text. To make a fair comparison, we collect several representative datasets from Flickr website including Landscape, Activity, Manhattan, National park, Festival, Car, and Food. The results show that the first two methods work in some datasets but fail in others. LGTA works well in all these datasets at not only finding regions of interests but also providing effective comparisons of the topics across different locations. The results confirm our hypothesis that the geographical distributions can help modeling topics, while topics provide important cues to group different geographical regions.	Geographical topic discovery and comparison	NA:NA:NA:NA:NA	2018
Yookyung Jo:John E. Hopcroft:Carl Lagoze	In this paper we study how to discover the evolution of topics over time in a time-stamped document collection. Our approach is uniquely designed to capture the rich topology of topic evolution inherent in the corpus. Instead of characterizing the evolving topics at fixed time points, we conceptually define a topic as a quantized unit of evolutionary change in content and discover topics with the time of their appearance in the corpus. Discovered topics are then connected to form a topic evolution graph using a measure derived from the underlying document network. Our approach allows inhomogeneous distribution of topics over time and does not impose any topological restriction in topic evolution graphs. We evaluate our algorithm on the ACM corpus. The topic evolution graphs obtained from the ACM corpus provide an effective and concrete summary of the corpus with remarkably rich topology that are congruent to our background knowledge. In a finer resolution, the graphs reveal concrete information about the corpus that were previously unknown to us, suggesting the utility of our approach as a navigational tool for the corpus.	The web of topics: discovering the topology of topic evolution in a corpus	NA:NA:NA	2018
Amr Ahmed:Qirong Ho:Jacob Eisenstein:Eric Xing:Alexander J. Smola:Choon Hui Teo	News clustering, categorization and analysis are key components of any news portal. They require algorithms capable of dealing with dynamic data to cluster, interpret and to temporally aggregate news articles. These three tasks are often solved separately. In this paper we present a unified framework to group incoming news articles into temporary but tightly-focused storylines, to identify prevalent topics and key entities within these stories, and to reveal the temporal structure of stories as they evolve. We achieve this by building a hybrid clustering and topic model. To deal with the available wealth of data we build an efficient parallel inference algorithm by sequential Monte Carlo estimation. Time and memory costs are nearly constant in the length of the history, and the approach scales to hundreds of thousands of documents. We demonstrate the efficiency and accuracy on the publicly available TDT dataset and data of a major internet news site.	Unified analysis of streaming news	NA:NA:NA:NA:NA:NA	2018
Vidit Jain:Manik Varma	Our objective is to improve the performance of keyword based image search engines by re-ranking their original results. To this end, we address three limitations of existing search engines in this paper. First, there is no straight-forward, fully automated way of going from textual queries to visual features. Image search engines therefore primarily rely on static and textual features for ranking. Visual features are mainly used for secondary tasks such as finding similar images. Second, image rankers are trained on query-image pairs labeled with relevance judgments determined by human experts. Such labels are well known to be noisy due to various factors including ambiguous queries, unknown user intent and subjectivity in human judgments. This leads to learning a sub-optimal ranker. Finally, a static ranker is typically built to handle disparate user queries. The ranker is therefore unable to adapt its parameters to suit the query at hand which again leads to sub-optimal results. We demonstrate that all of these problems can be mitigated by employing a re-ranking algorithm that leverages aggregate user click data. We hypothesize that images clicked in response to a query are mostly relevant to the query. We therefore re-rank the original search results so as to promote images that are likely to be clicked to the top of the ranked list. Our re-ranking algorithm employs Gaussian Process regression to predict the normalized click count for each image, and combines it with the original ranking score. Our approach is shown to significantly boost the performance of the Bing image search engine on a wide range of tail queries.	Learning to re-rank: query-dependent image re-ranking using click data	NA:NA	2018
Liangda Li:Ke Zhou:Gui-Rong Xue:Hongyuan Zha:Yong Yu	It is well-known that textual information such as video transcripts and video reviews can significantly enhance the performance of video summarization algorithms. Unfortunately, many videos on the Web such as those from the popular video sharing site YouTube do not have useful textual information. The goal of this paper is to propose a transfer learning framework for video summarization: in the training process both the video features and textual features are exploited to train a summarization algorithm while for summarizing a new video only its video features are utilized. The basic idea is to explore the transferability between videos and their corresponding textual information. Based on the assumption that video features and textual features are highly correlated with each other, we can transfer textual information into knowledge on summarization using video information only. In particular, we formulate the video summarization problem as that of learning a mapping from a set of shots of a video to a subset of the shots using the general framework of SVM-based structured learning. Textual information is transferred by encoding them into a set of constraints used in the structured learning process which tend to provide a more detailed and accurate characterization of the different subsets of shots. Experimental results show significant performance improvement of our approach and demonstrate the utility of textual information for enhancing video summarization.	Video summarization via transferrable structured learning	NA:NA:NA:NA:NA	2018
Guo-Jun Qi:Charu Aggarwal:Thomas Huang	In this paper, we study the problem of transfer learning from text to images in the context of network data in which link based bridges are available to transfer the knowledge between the different domains. The problem of classification of image data is often much more challenging than text data because of the following two reasons: (a) Labeled text data is very widely available for classification purposes. On the other hand, this is often not the case for image data, in which a lot of images are available from many sources, but many of them are often not labeled. (b) The image features are not directly related to semantic concepts inherent in class labels. On the other hand, since text data tends to have natural semantic interpretability (because of their human origins), they are often more directly related to class labels. Therefore, the relationships between the images and text features also provide additional hints for the classification process in terms of the image feature transformations which provide the most effective results.   The semantic challenges of image features are glaringly evident, when we attempt to recognize complex abstract concepts, and the visual features often fail to discriminate such concepts. However, the copious availability of bridging relationships between text and images in the context of web and social network data can be used in order to design for effective classifiers for image data. One of our goals in this paper is to develop a mathematical model for the functional relationships between text and image features, so as indirectly transfer semantic knowledge through feature transformations. This feature transformation is accomplished by mapping instances from different domains into a common space of unspecific topics. This is used as a bridge to semantically connect the two heterogeneous spaces. This is also helpful for the cases where little image data is available for the classification process. We evaluate our knowledge transfer techniques on an image classification task with labeled text corpora and show the effectiveness with respect to competing algorithms.	Towards semantic knowledge propagation from text corpus to web images	NA:NA:NA	2018
Ghassan O. Karame:Aurélien Francillon:Srdjan Čapkun	Currently, several online businesses deem that advertising revenues alone are not sufficient to generate profits and are therefore set to charge for online content. In this paper, we explore a complement to the current advertisement model; more specifically, we propose a micropayment model for non-specialized commodity web-services based on microcomputations. In our model, a user that wishes to access online content offered by a website does not need to register or pay to access the website; instead, he will accept to run microcomputations on behalf of the website in exchange for access to the content. These microcomputations can, for example, support ongoing computing projects that have clear social benefits (e.g., projects relating to HIV, dengue, cancer, etc.) or can contribute towards commercial computing projects. We argue that this micropayment model is economically and technically viable and that it can be integrated in existing distributed computing frameworks (e.g., the BOINC platform). We implement a preliminary prototype of a system based on our model through which we evaluate its performance and usability. Finally, we analyze the security and privacy of our proposal and we show that it ensures payment for the content while preserving the privacy of users.	Pay as you browse: microcomputations as micropayments in web-based services	NA:NA:NA	2018
Sayan Bhattacharya:Sreenivas Gollapudi:Kamesh Munagala	In commerce search, the set of products returned by a search engine often forms the basis for all user interactions leading up to a potential transaction on the web. Such a set of products is known as the consideration set. In this study, we consider the problem of generating consideration set of products in commerce search so as to maximize user satisfaction. One of the key features of commerce search that we exploit in our study is the association of a set of important attributes with the products and a set of specified attributes with the user queries. Those important attributes not used in the query are treated as unspecified. The attribute space admits a natural definition of user satisfaction via user preferences on the attributes and their values, viz. require that the surfaced products be close to the specified attribute values in the query, and diverse with respect to the unspecified attributes. We model this as a general Max-Sum Dispersion problem wherein we are given a set of n nodes in a metric space and the objective is to select a subset of nodes with total cost at most a given budget, and maximize the sum of the pairwise distances between the selected nodes. In our setting, each node denotes a product, the cost of a node being inversely proportional to its relevance with respect to specified attributes. The distance between two nodes quantifies the diversity with respect to the unspecified attributes. The problem is NP-hard and a 2-approximation was previously known only when all the nodes have unit cost. In our setting, we do not make any assumptions on the cost. We label this problem as the General Max-Sum Dispersion problem. We give the first constant factor approximation algorithm for this problem, achieving an approximation ratio of 2. Further, we perform extensive empirical analysis on real-world data to show the effectiveness of our algorithm.	Consideration set generation in commerce search	NA:NA:NA	2018
Beibei Li:Anindya Ghose:Panagiotis G. Ipeirotis	With the growing pervasiveness of the Internet, online search for products and services is constantly increasing. Most product search engines are based on adaptations of theoretical models devised for information retrieval. However, the decision mechanism that underlies the process of buying a product is different than the process of locating relevant documents or objects. We propose a theory model for product search based on expected utility theory from economics. Specifically, we propose a ranking technique in which we rank highest the products that generate the highest surplus, after the purchase. In a sense, the top ranked products are the "best value for money" for a specific user. Our approach builds on research on "demand estimation" from economics and presents a solid theoretical foundation on which further research can build on. We build algorithms that take into account consumer demographics, heterogeneity of consumer preferences, and also account for the varying price of the products. We show how to achieve this without knowing the demographics or purchasing histories of individual consumers but by using aggregate demand data. We evaluate our work, by applying the techniques on hotel search. Our extensive user studies, using more than 15,000 user-provided ranking comparisons, demonstrate an overwhelming preference for the rankings generated by our techniques, compared to a large number of existing strong state-of-the-art baselines.	Towards a theory model for product search	NA:NA:NA	2018
Kira Radinsky:Eugene Agichtein:Evgeniy Gabrilovich:Shaul Markovitch	Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspaper archive spanning many years. Two words such as "war" and "peace" might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this paper, we propose a new semantic relatedness model, Temporal Semantic Analysis (TSA), which captures this temporal information. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of semantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks.	A word at a time: computing word relatedness using temporal semantic analysis	NA:NA:NA:NA	2018
Yue Lu:Malu Castellanos:Umeshwar Dayal:ChengXiang Zhai	The explosion of Web opinion data has made essential the need for automatic tools to analyze and understand people's sentiments toward different topics. In most sentiment analysis applications, the sentiment lexicon plays a central role. However, it is well known that there is no universally optimal sentiment lexicon since the polarity of words is sensitive to the topic domain. Even worse, in the same domain the same word may indicate different polarities with respect to different aspects. For example, in a laptop review, "large" is negative for the battery aspect while being positive for the screen aspect. In this paper, we focus on the problem of learning a sentiment lexicon that is not only domain specific but also dependent on the aspect in context given an unlabeled opinionated text collection. We propose a novel optimization framework that provides a unified and principled way to combine different sources of information for learning such a context-dependent sentiment lexicon. Experiments on two data sets (hotel reviews and customer feedback surveys on printers) show that our approach can not only identify new sentiment words specific to the given domain but also determine the different polarities of a word depending on the aspect in context. In further quantitative evaluation, our method is proved to be effective in constructing a high quality lexicon by comparing with a human annotated gold standard. In addition, using the learned context-dependent sentiment lexicon improved the accuracy in an aspect-level sentiment classification task.	Automatic construction of a context-aware sentiment lexicon: an optimization approach	NA:NA:NA:NA	2018
Kuansan Wang:Christopher Thrasher:Bo-June Paul Hsu	This paper uses the URL word breaking task as an example to elaborate what we identify as crucial in designing statistical natural language processing (NLP) algorithms for Web scale applications: (1) rudimentary multilingual capabilities to cope with the global nature of the Web, (2) multi-style modeling to handle diverse language styles seen in the Web contents, (3) fast adaptation to keep pace with the dynamic changes of the Web, (4) minimal heuristic assumptions for generalizability and robustness, and (5) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost. We first show that the state-of-the-art word breaking techniques can be unified and generalized under the Bayesian minimum risk (BMR) framework that, using a Web scale N-gram, can meet the first three requirements. We discuss how the existing techniques can be viewed as introducing additional assumptions to the basic BMR framework, and describe a generic yet efficient implementation called word synchronous beam search. Testing the framework and its implementation on a series of large scale experiments reveals the following. First, the language style used to build the model plays a critical role in the word breaking task, and the most suitable for the URL word breaking task appears to be that of the document title where the best performance is obtained. Models created from other language styles, such as from document body, anchor text, and even queries, exhibit varying degrees of mismatch. Although all styles benefit from increasing modeling power which, in our experiments, corresponds to the use of a higher order N-gram, the gain is most recognizable for the title model. The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models, but are less effective and, in many cases, lead to poorer performance than the matched model with minimal assumptions. For the matched model based on document titles, an accuracy rate of 97.18% can already be achieved using simple trigram without any heuristics.	Web scale NLP: a case study on url word breaking	NA:NA:NA	2018
Krysta M. Svore:Maksims N. Volkovs:Christopher J.C. Burges	We investigate the problem of learning to rank with document retrieval from the perspective of learning for multiple objective functions. We present solutions to two open problems in learning to rank: first, we show how multiple measures can be combined into a single graded measure that can be learned. This solves the problem of learning from a 'scorecard' of measures by making such scorecards comparable, and we show results where a standard web relevance measure (NDCG) is used for the top-tier measure, and a relevance measure derived from click data is used for the second-tier measure; the second-tier measure is shown to significantly improve while leaving the top-tier measure largely unchanged. Second, we note that the learning-to-rank problem can itself be viewed as changing as the ranking model learns: for example, early in learning, adjusting the rank of all documents can be advantageous, but later during training, it becomes more desirable to concentrate on correcting the top few documents for each query. We show how an analysis of these problems leads to an improved, iteration-dependent cost function that interpolates between a cost function that is more appropriate for early learning, with one that is more appropriate for late-stage learning. The approach results in a significant improvement in accuracy with the same size models. We investigate these ideas using LambdaMART, a state-of-the-art ranking algorithm.	Learning to rank with multiple objective functions	NA:NA:NA	2018
Maryam Karimzadehgan:Wei Li:Ruofei Zhang:Jianchang Mao	This paper is concerned with the problem of learning a model to rank objects (Web pages, ads and etc.). We propose a framework where the ranking model is both optimized and evaluated using the same information retrieval measures such as Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP). The main difficulty in direct optimization of NDCG and MAP is that these measures depend on the rank of objects and are not differentiable. Most learning-to-rank methods that attempt to optimize NDCG or MAP approximate such measures so that they can be differentiable. In this paper, we propose a simple yet effective stochastic optimization algorithm to directly minimize any loss function, which can be defined on NDCG or MAP for the learning-to-rank problem. The algorithm employs Simulated Annealing along with Simplex method for its parameter search and finds the global optimal parameters. Experiment results using NDCG-Annealing algorithm, an instance of the proposed algorithm, on LETOR benchmark data sets show that the proposed algorithm is both effective and stable when compared to the baselines provided in LETOR 3.0. In addition, we applied the algorithm for ranking ads in contextual advertising. Our method has shown to significantly improve relevance in offline evaluation and business metrics in online tests in a real large-scale advertising serving system. To scale our computations, we parallelize the algorithm in a MapReduce framework running on Hadoop.	A stochastic learning-to-rank algorithm and its application to contextual advertising	NA:NA:NA:NA	2018
Stephen Tyree:Kilian Q. Weinberger:Kunal Agrawal:Jennifer Paykin	Gradient Boosted Regression Trees (GBRT) are the current state-of-the-art learning paradigm for machine learned web-search ranking - a domain notorious for very large data sets. In this paper, we propose a novel method for parallelizing the training of GBRT. Our technique parallelizes the construction of the individual regression trees and operates using the master-worker paradigm as follows. The data are partitioned among the workers. At each iteration, the worker summarizes its data-partition using histograms. The master processor uses these to build one layer of a regression tree, and then sends this layer to the workers, allowing the workers to build histograms for the next layer. Our algorithm carefully orchestrates overlap between communication and computation to achieve good performance. Since this approach is based on data partitioning, and requires a small amount of communication, it generalizes to distributed and shared memory machines, as well as clouds. We present experimental results on both shared memory machines and clusters for two large scale web search ranking data sets. We demonstrate that the loss in accuracy induced due to the histogram approximation in the regression tree creation can be compensated for through slightly deeper trees. As a result, we see no significant loss in accuracy on the Yahoo data sets and a very small reduction in accuracy for the Microsoft LETOR data. In addition, on shared memory machines, we obtain almost perfect linear speed-up with up to about 48 cores on the large data sets. On distributed memory machines, we get a speedup of 25 with 32 processors. Due to data partitioning our approach can scale to even larger data sets, on which one can reasonably expect even higher speedups.	Parallel boosted regression trees for web search ranking	NA:NA:NA:NA	2018
Umut Ozertem:Rosie Jones:Benoit Dumoulin	We provide a novel method of evaluating search results, which allows us to combine existing editorial judgments with the relevance estimates generated by click-based user browsing models. There are evaluation methods in the literature that use clicks and editorial judgments together, but our approach is novel in the sense that it allows us to predict the impact of unseen search models without online tests to collect clicks and without requesting new editorial data, since we are only re-using existing editorial data, and clicks observed for previous result set configurations. Since the user browsing model and the pre-existing editorial data cannot provide relevance estimates for all documents for the selected set of queries, one important challenge is to obtain this performance estimation where there are a lot of ranked documents with missing relevance values. We introduce a query and rank based smoothing to overcome this problem. We show that a hybrid of these smoothing techniques performs better than both query and position based smoothing, and despite the high percentage of missing judgments, the resulting method is significantly correlated (0.74) with DCG values evaluated using fully judged datasets, and approaches inter-annotator agreement. We show that previously published techniques, applicable to frequent queries, degrade when applied to a random sample of queries, with a correlation of only 0.29. While our experiments focus on evaluation using DCG, our method is also applicable to other commonly used metrics.	Evaluating new search engine configurations with pre-existing judgments and clicks	NA:NA:NA	2018
Azin Ashkan:Charles L.A. Clarke	The Maximum Entropy Method provides one technique for validating search engine effectiveness measures. Under this method, the value of an effectiveness measure is used as a constraint to estimate the most likely distribution of relevant documents under a maximum entropy assumption. This inferred distribution may then be compared to the actual distribution to quantify the "informativeness" of the measure. The inferred distribution may also be used to estimate values for other effectiveness measures. Previous work focused on traditional effectiveness measures, such as average precision. In this paper, we extend the Maximum Entropy Method to the newer cascade and intent-aware effectiveness measures by considering the dependency of the documents ranked in a results list. These measures are intended to reflect the novelty and diversity of search results in addition to the traditional relevance. Our results indicate that intent-aware measures based on the cascade model are informative in terms of both inferring actual distribution and predicting the values of other retrieval measures.	On the informativeness of cascade and intent-aware effectiveness measures	NA:NA	2018
Denis Helic:Markus Strohmaier:Christoph Trattner:Markus Muhr:Kristina Lerman	Recently, a number of algorithms have been proposed to obtain hierarchical structures - so-called folksonomies - from social tagging data. Work on these algorithms is in part driven by a belief that folksonomies are useful for tasks such as: (a) Navigating social tagging systems and (b) Acquiring semantic relationships between tags. While the promises and pitfalls of the latter have been studied to some extent, we know very little about the extent to which folksonomies are pragmatically useful for navigating social tagging systems. This paper sets out to address this gap by presenting and applying a pragmatic framework for evaluating folksonomies. We model exploratory navigation of a tagging system as decentralized search on a network of tags. Evaluation is based on the fact that the performance of a decentralized search algorithm depends on the quality of the background knowledge used. The key idea of our approach is to use hierarchical structures learned by folksonomy algorithm as background knowledge for decentralized search. Utilizing decentralized search on tag networks in combination with different folksonomies as hierarchical background knowledge allows us to evaluate navigational tasks in social tagging systems. Our experiments with four state-of-the-art folksonomy algorithms on five different social tagging datasets reveal that existing folksonomy algorithms exhibit significant, previously undiscovered, differences with regard to their utility for navigation. Our results are relevant for engineers aiming to improve navigability of social tagging systems and for scientists aiming to evaluate different folksonomy algorithms from a pragmatic perspective.	Pragmatic evaluation of folksonomies	NA:NA:NA:NA:NA	2018
Yeye He:Dong Xin	In this paper, we study the problem of expanding a set of given seed entities into a more complete set by discovering other entities that also belong to the same concept set. A typical example is to use "Canon" and "Nikon" as seed entities, and derive other entities (e.g., "Olympus") in the same concept set of camera brands. In order to discover such relevant entities, we exploit several web data sources, including lists extracted from web pages and user queries from a web search engine. While these web data are highly diverse with rich information that usually cover a wide range of the domains of interest, they tend to be very noisy. We observe that previously proposed random walk based approaches do not perform very well on these noisy data sources. Accordingly, we propose a new general framework based on iterative similarity aggregation, and present detailed experimental results to show that, when using general-purpose web data for set expansion, our approach outperforms previous techniques in terms of both precision and recall.	SEISA: set expansion by iterative similarity aggregation	NA:NA	2018
Lorenzo Blanco:Nilesh Dalvi:Ashwin Machanavajjhala	In this paper, we present a highly scalable algorithm for structurally clustering webpages for extraction. We show that, using only the URLs of the webpages and simple content features, it is possible to cluster webpages effectively and efficiently. At the heart of our techniques is a principled framework, based on the principles of information theory, that allows us to effectively leverage the URLs, and combine them with content and structural properties. Using an extensive evaluation over several large full websites, we demonstrate the effectiveness of our techniques, at a scale unattainable by previous techniques.	Highly efficient algorithms for structural clustering of large websites	NA:NA:NA	2018
Anton Bakalov:Ariel Fuxman:Partha Pratim Talukdar:Soumen Chakrabarti	Search engines today offer a rich user experience, no longer restricted to "ten blue links". For example, the query "Canon EOS Digital Camera" returns a photo of the digital camera, and a list of suitable merchants and prices. Similar results are offered in other domains like food, entertainment, travel, etc. All these experiences are fueled by the availability of structured data about the entities of interest. To obtain this structured data, it is necessary to solve the following problem: given a category of entities with its schema, and a set of Web pages that mention and describe entities belonging to the category, build a structured representation for the entity under the given schema. Specifically, collect structured numerical or discrete attributes of the entities. Most previous approaches regarded this as an information extraction problem on individual documents, and made no special use of numerical attributes. In contrast, we present an end-to-end framework which leverages signals not only from the Web page context, but also from a collective analysis of all the pages corresponding to an entity, and from constraints related to the actual values within the domain. Our current implementation uses a general and flexible Integer Linear Program (ILP) to integrate all these signals into holistic decisions over all attributes. There is one ILP per entity and it is small enough to be solved in under 38 milliseconds in our experiments. We apply the new framework to a setting of significant practical importance: catalog expansion for Commerce search engines, using data from Bing Shopping. Finally, we present experiments that validate the effectiveness of the framework and its superiority to local extraction.	SCAD: collective discovery of attribute values	NA:NA:NA:NA	2018
Salvatore Scellato:Cecilia Mascolo:Mirco Musolesi:Jon Crowcroft	Providers such as YouTube offer easy access to multimedia content to millions, generating high bandwidth and storage demand on the Content Delivery Networks they rely upon. More and more, the diffusion of this content happens on online social networks such as Facebook and Twitter, where social cascades can be observed when users increasingly repost links they have received from others. In this paper we describe how geographic information extracted from social cascades can be exploited to improve caching of multimedia files in a Content Delivery Network. We take advantage of the fact that social cascades can propagate in a geographically limited area to discern whether an item is spreading locally or globally. This informs cache replacement policies, which utilize this information to ensure that content relevant to a cascade is kept close to the users who may be interested in it. We validate our approach by using a novel dataset which combines social interaction data with geographic information: we track social cascades of YouTube links over Twitter and build a proof-of-concept geographic model of a realistic distributed Content Delivery Network. Our performance evaluation shows that we are able to improve cache hits with respect to cache policies without geographic and social information.	Track globally, deliver locally: improving content delivery networks by tracking geographic social cascades	NA:NA:NA:NA	2018
Sipat Triukose:Zhihua Wen:Michael Rabinovich	Content delivery networks (CDNs) have become a crucial part of the modern Web infrastructure. This paper studies the performance of the leading content delivery provider - Akamai. It measures the performance of the current Akamai platform and considers a key architectural question faced by both CDN designers and their prospective customers: whether the co-location approach to CDN platforms adopted by Akamai, which tries to deploy servers in numerous Internet locations, brings inherent performance benefits over a more consolidated data center approach pursued by other influential CDNs such as Limelight. We believe the methodology we developed for this study will be useful for other researchers in the CDN arena.	Measuring a commercial content delivery network	NA:NA:NA	2018
Paul Heymann:Hector Garcia-Molina	We present "Turkalytics," a novel analytics tool for human computation systems. Turkalytics processes and reports logging events from workers in real-time and has been shown to scale to over one hundred thousand logging events per day. We present a state model for worker interaction that covers the Mechanical Turk (the SCRAP model) and a data model that demonstrates the diversity of data collected by Turkalytics. We show that Turkalytics is effective at data collection, in spite of it being unobtrusive. Lastly, we describe worker locations, browser environments, activity information, and other examples of data collected by our tool.	Turkalytics: analytics for human computation	NA:NA	2018
Gal Lavee:Ronny Lempel:Edo Liberty:Oren Somekh	Modern search engines are expected to make documents searchable shortly after they appear on the ever changing Web. To satisfy this requirement, the Web is frequently crawled. Due to the sheer size of their indexes, search engines distribute the crawled documents among thousands of servers in a scheme called local index-partitioning, such that each server indexes only several million pages. To ensure documents from the same host (e.g., www.nytimes.com) are distributed uniformly over the servers, for load balancing purposes, random routing of documents to servers is common. To expedite the time documents become searchable after being crawled, documents may be simply appended to the existing index partitions. However, indexing by merely appending documents, results in larger index sizes since document reordering for index compactness is no longer performed. This, in turn, degrades search query processing performance which depends heavily on index sizes. A possible way to balance quick document indexing with efficient query processing, is to deploy online document routing strategies that are designed to reduce index sizes. This work considers the effects of several online document routing strategies on the aggregated partitioned index size. We show that there exists a tradeoff between the compression of a partitioned index and the distribution of documents from the same host across the index partitions (i.e., host distribution). We suggest and evaluate several online routing strategies with regard to their compression, host distribution, and complexity. In particular, we present a term based routing algorithm which is shown analytically to provide better compression results than the industry standard random routing scheme. In addition, our algorithm demonstrates comparable compression performance and host distribution while having much better running time complexity than other document routing heuristics. Our findings are validated by experimental evaluation performed on a large benchmark collection of Web pages.	Inverted index compression via online document routing	NA:NA:NA:NA	2018
Andrei Broder:Shirshanka Das:Marcus Fontoura:Bhaskar Ghosh:Vanja Josifovski:Jayavel Shanmugasundaram:Sergei Vassilvitskii	We introduce the problem of evaluating graph constraints in content-based publish/subscribe (pub/sub) systems. This problem formulation extends traditional content-based pub/sub systems in the following manner: publishers and subscribers are connected via a (logical) directed graph G with node and edge constraints, which limits the set of valid paths between them. Such graph constraints can be used to model a Web advertising exchange (where there may be restrictions on how advertising networks can connect advertisers and publishers) and content delivery problems in social networks (where there may be restrictions on how information can be shared via the social graph). In this context, we develop efficient algorithms for evaluating graph constraints over arbitrary directed graphs G. We also present experimental results that demonstrate the effectiveness and scalability of the proposed algorithms using a realistic dataset from Yahoo!'s Web advertising exchange.	Efficiently evaluating graph constraints in content-based publish/subscribe	NA:NA:NA:NA:NA:NA:NA	2018
Xiaoxin Yin:Wenzhao Tan:Chao Liu	Recently answers for fact lookup queries have appeared on major search engines. For example, for the query Barack Obama date of birth Google directly shows "4 August 1961" above its regular results. In this paper, we describe FACTO, an end-to-end system for answering fact lookup queries for web search. FACTO extracts structured data from tables on the web, aggregates and cleans such data and stores them in a database. Given a web search query, FACTO will decide if it asks for facts in this database, and provides the most confident answer when possible. FACTO achieves higher precision and comparable coverage comparing with the fact lookup engines by Google and Ask.com, although FACTO is developed by a very small team. We present the challenges and our solutions in developing every component of FACTO. Some solutions are based on existing technologies, and many others are novel approaches proposed by us.	FACTO: a fact lookup engine based on web tables	NA:NA:NA	2018
Brendan Meeder:Brian Karrer:Amin Sayedi:R. Ravi:Christian Borgs:Jennifer Chayes	Understanding a network's temporal evolution appears to require multiple observations of the graph over time. These often expensive repeated crawls are only able to answer questions about what happened from observation to observation, and not what happened before or between network snapshots. Contrary to this picture, we propose a method for Twitter's social network that takes a single static snapshot of network edges and user account creation times to accurately infer when these edges were formed. This method can be exact in theory, and we demonstrate empirically for a large subset of Twitter relationships that it is accurate to within a few hours in practice. We study users who have a very large number of edges or who are recommended by Twitter. We examine the graph formed by these nearly 1,800 Twitter celebrities and their 862 million edges in detail, showing that a single static snapshot can give novel insights about Twitter's evolution. We conclude from this analysis that real-world events and changes to Twitter's interface for recommending users strongly influence network growth.	We know who you followed last summer: inferring social link creation times in twitter	NA:NA:NA:NA:NA:NA	2018
Mohsen Jamali:Gholamreza Haffari:Martin Ester	A social rating network (SRN) is a social network in which edges represent social relationships and users (nodes) express ratings on some of the given items. Such networks play an increasingly important role in reviewing websites such as Epinions.com or online sharing websites like Flickr.com. In this paper, we first observe and analyze the temporal behavior of users in a social rating network, who express ratings and create social relations. Then, we model the temporal dynamics of an SRN based on our observations, using the bidirectional effects of ratings and social relations. While existing models for other types of social networks have captured some of the effects, our model is the first one to represent all four effects, i.e. social relations-on-ratings (social influence), social relations-on-social relations (transitivity), ratings-on-social relations (selection), and ratings-on-ratings (correlational influence). Existing works consider these effects as static and constant throughout the evolution of an SRN, however our observations reveal that these effects are actually dynamic. We propose a probabilistic generative model for SRNs, which models the strength and dynamics of each effect throughout the network evolution. This model can serve for the prediction of future links, ratings or community structures. Due to the sensitive nature of SRNs, another motivation for our work is the generation of synthetic SRN data sets for research purposes. Our experimental studies on two real life datasets (Epinions and Flickr) demonstrate that the proposed model produces social rating networks that agree with real world data on a comprehensive set of evaluation criteria.	Modeling the temporal dynamics of social rating networks using bidirectional effects of social relations and rating patterns	NA:NA:NA	2018
Shuang-Hong Yang:Bo Long:Alex Smola:Narayanan Sadagopan:Zhaohui Zheng:Hongyuan Zha	Targeting interest to match a user with services (e.g. news, products, games, advertisements) and predicting friendship to build connections among users are two fundamental tasks for social network systems. In this paper, we show that the information contained in interest networks (i.e. user-service interactions) and friendship networks (i.e. user-user connections) is highly correlated and mutually helpful. We propose a framework that exploits homophily to establish an integrated network linking a user to interested services and connecting different users with common interests, upon which both friendship and interests could be efficiently propagated. The proposed friendship-interest propagation (FIP) framework devises a factor-based random walk model to explain friendship connections, and simultaneously it uses a coupled latent factor model to uncover interest interactions. We discuss the flexibility of the framework in the choices of loss objectives and regularization penalties and benchmark different variants on the Yahoo! Pulse social networking system. Experiments demonstrate that by coupling friendship with interest, FIP achieves much higher performance on both interest targeting and friendship prediction than systems using only one source of information.	Like like alike: joint friendship and interest propagation in social networks	NA:NA:NA:NA:NA:NA	2018
Simla Ceyhan:Xiaolin Shi:Jure Leskovec	Online peer-to-peer (P2P) lending services are a new type of social platform that enables individuals borrow and lend money directly from one to another. In this paper, we study the dynamics of bidding behavior in a P2P loan auction website, Prosper.com. We investigate the change of various attributes of loan requesting listings over time, such as the interest rate and the number of bids. We observe that there is herding behavior during bidding, and for most of the listings, the numbers of bids they receive reach spikes at very similar time points. We explain these phenomena by showing that there are economic and social factors that lenders take into account when deciding to bid on a listing. We also observe that the profits the lenders make are tied with their bidding preferences. Finally, we build a model based on the temporal progression of the bidding, that reliably predicts the success of a loan request listing, as well as whether a loan will be paid back or not.	Dynamics of bidding in a P2P lending service: effects of herding and predicting loan success	NA:NA:NA	2018
Mangesh Gupte:Pravin Shankar:Jing Li:S. Muthukrishnan:Liviu Iftode	Social hierarchy and stratification among humans is a well studied concept in sociology. The popularity of online social networks presents an opportunity to study social hierarchy for different types of networks and at different scales. We adopt the premise that people form connections in a social network based on their perceived social hierarchy; as a result, the edge directions in directed social networks can be leveraged to infer hierarchy. In this paper, we define a measure of hierarchy in a directed online social network, and present an efficient algorithm to compute this measure. We validate our measure using ground truth including Wikipedia notability score. We use this measure to study hierarchy in several directed online social networks including Twitter, Delicious, YouTube, Flickr, LiveJournal, and curated lists of several categories of people based on different occupations, and different organizations. Our experiments on different online social networks show how hierarchy emerges as we increase the size of the network. This is in contrast to random graphs, where the hierarchy decreases as the network size increases. Further, we show that the degree of stratification in a network increases very slowly as we increase the size of the graph.	Finding hierarchy in directed online social networks	NA:NA:NA:NA:NA	2018
Abhinav Mishra:Arnab Bhattacharya	Many real-life graphs such as social networks and peer-to-peer networks capture the relationships among the nodes by using trust scores to label the edges. Important usage of such networks includes trust prediction, finding the most reliable or trusted node in a local subgraph, etc. For many of these applications, it is crucial to assess the prestige and bias of a node. The bias of a node denotes its propensity to trust/mistrust its neighbours and is closely related to truthfulness. If a node trusts all its neighbours, its recommendation of another node as trustworthy is less reliable. It is based on the idea that the recommendation of a highly biased node should weigh less. In this paper, we propose an algorithm to compute the bias and prestige of nodes in networks where the edge weight denotes the trust score. Unlike most other graph-based algorithms, our method works even when the edge weights are not necessarily positive. The algorithm is iterative and runs in O(km) time where k is the number of iterations and m is the total number of edges in the network. The algorithm exhibits several other desirable properties. It converges to a unique value very quickly. Also, the error in bias and prestige values at any particular iteration is bounded. Further, experiments show that our model conforms well to social theories such as the balance theory (enemy of a friend is an enemy, etc.).	Finding the bias and prestige of nodes in networks based on trust scores	NA:NA	2018
Wei Dong:Charikar Moses:Kai Li	K-Nearest Neighbor Graph (K-NNG) construction is an important operation with many web related applications, including collaborative filtering, similarity search, and many others in data mining and machine learning. Existing methods for K-NNG construction either do not scale, or are specific to certain similarity measures. We present NN-Descent, a simple yet efficient algorithm for approximate K-NNG construction with arbitrary similarity measures. Our method is based on local search, has minimal space overhead and does not rely on any shared global index. Hence, it is especially suitable for large-scale applications where data structures need to be distributed over the network. We have shown with a variety of datasets and similarity measures that the proposed method typically converges to above 90% recall with each point comparing only to several percent of the whole dataset on average.	Efficient k-nearest neighbor graph construction for generic similarity measures	NA:NA:NA	2018
Paolo Boldi:Marco Rosa:Massimo Santini:Sebastiano Vigna	We continue the line of research on graph compression started with WebGraph, but we move our focus to the compression of social networks in a proper sense (e.g., LiveJournal): the approaches that have been used for a long time to compress web graphs rely on a specific ordering of the nodes (lexicographical URL ordering) whose extension to general social networks is not trivial. In this paper, we propose a solution that mixes clusterings and orders, and devise a new algorithm, called Layered Label Propagation, that builds on previous work on scalable clustering and can be used to reorder very large graphs (billions of nodes). Our implementation uses task decomposition to perform aggressively on multi-core architecture, making it possible to reorder graphs of more than 600 millions nodes in a few hours. Experiments performed on a wide array of web graphs and social networks show that combining the order produced by the proposed algorithm with the WebGraph compression framework provides a major increase in compression with respect to all currently known techniques, both on web graphs and on social networks. These improvements make it possible to analyse in main memory significantly larger graphs.	Layered label propagation: a multiresolution coordinate-free ordering for compressing social networks	NA:NA:NA:NA	2018
Liran Katzir:Edo Liberty:Oren Somekh	Online social networks have become very popular in recent years and their number of users is already measured in many hundreds of millions. For various commercial and sociological purposes, an independent estimate of their sizes is important. In this work, algorithms for estimating the number of users in such networks are considered. The proposed schemes are also applicable for estimating the sizes of networks' sub-populations. The suggested algorithms interact with the social networks via their public APIs only, and rely on no other external information. Due to obvious traffic and privacy concerns, the number of such interactions is severely limited. We therefore focus on minimizing the number of API interactions needed for producing good size estimates. We adopt the abstraction of social networks as undirected graphs and use random node sampling. By counting the number of collisions or non-unique nodes in the sample, we produce a size estimate. Then, we show analytically that the estimate error vanishes with high probability for smaller number of samples than those required by prior-art algorithms. Moreover, although our algorithms are provably correct for any graph, they excel when applied to social network-like graphs. The proposed algorithms were evaluated on synthetic as well real social networks such as Facebook, IMDB, and DBLP. Our experiments corroborated the theoretical results, and demonstrated the effectiveness of the algorithms.	Estimating sizes of social networks via biased sampling	NA:NA:NA	2018
Siddharth Suri:Sergei Vassilvitskii	The clustering coefficient of a node in a social network is a fundamental measure that quantifies how tightly-knit the community is around the node. Its computation can be reduced to counting the number of triangles incident on the particular node in the network. In case the graph is too big to fit into memory, this is a non-trivial task, and previous researchers showed how to estimate the clustering coefficient in this scenario. A different avenue of research is to to perform the computation in parallel, spreading it across many machines. In recent years MapReduce has emerged as a de facto programming paradigm for parallel computation on massive data sets. The main focus of this work is to give MapReduce algorithms for counting triangles which we use to compute clustering coefficients. Our contributions are twofold. First, we describe a sequential triangle counting algorithm and show how to adapt it to the MapReduce setting. This algorithm achieves a factor of 10-100 speed up over the naive approach. Second, we present a new algorithm designed specifically for the MapReduce framework. A key feature of this approach is that it allows for a smooth tradeoff between the memory available on each individual machine and the total memory available to the algorithm, while keeping the total work done constant. Moreover, this algorithm can use any triangle counting algorithm as a black box and distribute the computation across many machines. We validate our algorithms on real world datasets comprising of millions of nodes and over a billion edges. Our results show both algorithms effectively deal with skew in the degree distribution and lead to dramatic speed ups over the naive implementation.	Counting triangles and the curse of the last reducer	NA:NA	2018
Lars Backstrom:Jon Kleinberg	Bucket testing, also known as A/B testing, is a practice that is widely used by on-line sites with large audiences: in a simple version of the methodology, one evaluates a new feature on the site by exposing it to a very small fraction of the total user population and measuring its effect on this exposed group. For traditional uses of this technique, uniform independent sampling of the population is often enough to produce an exposed group that can serve as a statistical proxy for the full population. In on-line social network applications, however, one often wishes to perform a more complex test: evaluating a new social feature that will only produce an effect if a user and some number of his or her friends are exposed to it. In this case, independent uniform draws from the population will be unlikely to produce groups that contains users together with their friends, and so the construction of the sample must take the network structure into account. This leads quickly to challenging combinatorial problems, since there is an inherent tension between producing enough correlation to select users and their friends, but also enough uniformity and independence that the selected group is a reasonable sample of the full population. Here we develop an algorithmic framework for bucket testing in a network that addresses these challenges. First we describe a novel walk-based sampling method for producing samples of nodes that are internally well-connected but also approximately uniform over the population. Then we show how a collection of multiple independent subgraphs constructed this way can yield reasonable samples for testing. We demonstrate the effectiveness of our algorithms through computational experiments on large portions of the Facebook network.	Network bucket testing	NA:NA	2018
Paolo Boldi:Marco Rosa:Sebastiano Vigna	The neighbourhood function NG(t) of a graph G gives, for each t ∈ N, the number of pairs of nodes x, y such that y is reachable from x in less that t hops. The neighbourhood function provides a wealth of information about the graph [10] (e.g., it easily allows one to compute its diameter), but it is very expensive to compute it exactly. Recently, the ANF algorithm [10] (approximate neighbourhood function) has been proposed with the purpose of approximating NG(t) on large graphs. We describe a breakthrough improvement over ANF in terms of speed and scalability. Our algorithm, called HyperANF, uses the new HyperLogLog counters [5] and combines them efficiently through broadword programming [8]; our implementation uses talk decomposition to exploit multi-core parallelism. With HyperANF, for the first time we can compute in a few hours the neighbourhood function of graphs with billions of nodes with a small error and good confidence using a standard workstation. Then, we turn to the study of the distribution of the distances between reachable nodes (that can be efficiently approximated by means of HyperANF), and discover the surprising fact that its index of dispersion provides a clear-cut characterisation of proper social networks vs. web graphs. We thus propose the spid (Shortest-Paths Index of Dispersion) of a graph as a new, informative statistics that is able to discriminate between the above two types of graphs. We believe this is the first proposal of a significant new non-local structural index for complex networks whose computation is highly scalable.	HyperANF: approximating the neighbourhood function of very large graphs on a budget	NA:NA:NA	2018
Darko Anicic:Paul Fodor:Sebastian Rudolph:Nenad Stojanovic	Streams of events appear increasingly today in various Web applications such as blogs, feeds, sensor data streams, geospatial information, on-line financial data, etc. Event Processing (EP) is concerned with timely detection of compound events within streams of simple events. State-of-the-art EP provides on-the-fly analysis of event streams, but cannot combine streams with background knowledge and cannot perform reasoning tasks. On the other hand, semantic tools can effectively handle background knowledge and perform reasoning thereon, but cannot deal with rapidly changing data provided by event streams. To bridge the gap, we propose Event Processing SPARQL (EP-SPARQL) as a new language for complex events and Stream Reasoning. We provide syntax and formal semantics of the language and devise an effective execution model for the proposed formalism. The execution model is grounded on logic programming, and features effective event processing and inferencing capabilities over temporal and static knowledge. We provide an open-source prototype implementation and present a set of tests to show the usefulness and effectiveness of our approach.	EP-SPARQL: a unified language for event processing and stream reasoning	NA:NA:NA:NA	2018
Markus Krötzsch:Frederick Maier:Adila Krisnadhi:Pascal Hitzler	We propose a description-logic style extension of OWL 2 with nominal schemas which can be used like "variable nominal classes" within axioms. This feature allows ontology languages to express arbitrary DL-safe rules (as expressible in SWRL or RIF) in their native syntax. We show that adding nominal schemas to OWL 2 does not increase the worst-case reasoning complexity, and we identify a novel tractable language SROELV3(∩, x) that is versatile enough to capture the lightweight languages OWL EL and OWL RL.	A better uncle for OWL: nominal schemas for integrating rules and ontologies	NA:NA:NA:NA	2018
Wangchao Le:Songyun Duan:Anastasios Kementsietsidis:Feifei Li:Min Wang	The problem of answering SPARQL queries over virtual SPARQL views is commonly encountered in a number of settings, including while enforcing security policies to access RDF data, or when integrating RDF data from disparate sources. We approach this problem by rewriting SPARQL queries over the views to equivalent queries over the underlying RDF data, thus avoiding the costs entailed by view materialization and maintenance. We show that SPARQL query rewriting combines the most challenging aspects of rewriting for the relational and XML cases: like the relational case, SPARQL query rewriting requires synthesizing multiple views; like the XML case, the size of the rewritten query is exponential to the size of the query and the views. In this paper, we present the first native query rewriting algorithm for SPARQL. For an input SPARQL query over a set of virtual SPARQL views, the rewritten query resembles a union of conjunctive queries and can be of exponential size. We propose optimizations over the basic rewriting algorithm to (i) minimize each conjunctive query in the union; (ii) eliminate conjunctive queries with empty results from evaluation; and (iii) efficiently prune out big portions of the search space of empty rewritings. The experiments, performed on two RDF stores, show that our algorithms are scalable and independent of the underlying RDF stores. Furthermore, our optimizations have order of magnitude improvements over the basic rewriting algorithm in both the rewriting size and evaluation time.	Rewriting queries on SPARQL views	NA:NA:NA:NA:NA	2018
Ceren Budak:Divyakant Agrawal:Amr El Abbadi	In this work, we study the notion of competing campaigns in a social network and address the problem of influence limitation where a "bad" campaign starts propagating from a certain node in the network and use the notion of limiting campaigns to counteract the effect of misinformation. The problem can be summarized as identifying a subset of individuals that need to be convinced to adopt the competing (or "good") campaign so as to minimize the number of people that adopt the "bad" campaign at the end of both propagation processes. We show that this optimization problem is NP-hard and provide approximation guarantees for a greedy solution for various definitions of this problem by proving that they are submodular. We experimentally compare the performance of the greedy method to various heuristics. The experiments reveal that in most cases inexpensive heuristics such as degree centrality compare well with the greedy approach. We also study the influence limitation problem in the presence of missing data where the current states of nodes in the network are only known with a certain probability and show that prediction in this setting is a supermodular problem. We propose a prediction algorithm that is based on generating random spanning trees and evaluate the performance of this approach. The experiments reveal that using the prediction algorithm, we are able to tolerate about 90% missing data before the performance of the algorithm starts degrading and even with large amounts of missing data the performance degrades only to 75% of the performance that would be achieved with complete data.	Limiting the spread of misinformation in social networks	NA:NA:NA	2018
Carlos Castillo:Marcelo Mendoza:Barbara Poblete	We analyze the information credibility of news propagated through Twitter, a popular microblogging service. Previous research has shown that most of the messages posted on Twitter are truthful, but the service is also used to spread misinformation and false rumors, often unintentionally. On this paper we focus on automatic methods for assessing the credibility of a given set of tweets. Specifically, we analyze microblog postings related to "trending" topics, and classify them as credible or not credible, based on features extracted from them. We use features from users' posting and re-posting ("re-tweeting") behavior, from the text of the posts, and from citations to external sources. We evaluate our methods using a significant number of human assessments about the credibility of items on a recent sample of Twitter postings. Our results shows that there are measurable differences in the way messages propagate, that can be used to classify them automatically as credible or not credible, with precision and recall in the range of 70% to 80%.	Information credibility on twitter	NA:NA:NA	2018
Xinyu Xing:Yu-Li Liang:Hanqiang Cheng:Jianxun Dang:Sui Huang:Richard Han:Xue Liu:Qin Lv:Shivakant Mishra	Online video chat services such as Chatroulette, Omegle, and vChatter that randomly match pairs of users in video chat sessions are fast becoming very popular, with over a million users per month in the case of Chatroulette. A key problem encountered in such systems is the presence of flashers and obscene content. This problem is especially acute given the presence of underage minors in such systems. This paper presents SafeVchat, a novel solution to the problem of flasher detection that employs an array of image detection algorithms. A key contribution of the paper concerns how the results of the individual detectors are fused together into an overall decision classifying the user as misbehaving or not, based on Dempster-Shafer Theory. The paper introduces a novel, motion-based skin detection method that achieves significantly higher recall and better precision. The proposed methods have been evaluated over real-world data and image traces obtained from Chatroulette.com.	SafeVchat: detecting obscene content and misbehaving users in online video chat services	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Daniel M. Romero:Brendan Meeder:Jon Kleinberg	There is a widespread intuitive sense that different kinds of information spread differently on-line, but it has been difficult to evaluate this question quantitatively since it requires a setting where many different kinds of information spread in a shared environment. Here we study this issue on Twitter, analyzing the ways in which tokens known as hashtags spread on a network defined by the interactions among Twitter users. We find significant variation in the ways that widely-used hashtags on different topics spread. Our results show that this variation is not attributable simply to differences in "stickiness," the probability of adoption based on one or more exposures, but also to a quantity that could be viewed as a kind of "persistence" - the relative extent to which repeated exposures to a hashtag continue to have significant marginal effects. We find that hashtags on politically controversial topics are particularly persistent, with repeated exposures continuing to have unusually large marginal effects on adoption; this provides, to our knowledge, the first large-scale validation of the "complex contagion" principle from sociology, which posits that repeated exposures to an idea are particularly crucial when the idea is in some way controversial or contentious. Among other findings, we discover that hashtags representing the natural analogues of Twitter idioms and neologisms are particularly non-persistent, with the effect of multiple exposures decaying rapidly relative to the first exposure. We also study the subgraph structure of the initial adopters for different widely-adopted hashtags, again finding structural differences across topics. We develop simulation-based and generative models to analyze how the adoption dynamics interact with the network structure of the early adopters on which a hashtag spreads.	Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter	NA:NA:NA	2018
Shaomei Wu:Jake M. Hofman:Winter A. Mason:Duncan J. Watts	We study several longstanding questions in media communications research, in the context of the microblogging service Twitter, regarding the production, flow, and consumption of information. To do so, we exploit a recently introduced feature of Twitter known as "lists" to distinguish between elite users - by which we mean celebrities, bloggers, and representatives of media outlets and other formal organizations - and ordinary users. Based on this classification, we find a striking concentration of attention on Twitter, in that roughly 50% of URLs consumed are generated by just 20K elite users, where the media produces the most information, but celebrities are the most followed. We also find significant homophily within categories: celebrities listen to celebrities, while bloggers listen to bloggers etc; however, bloggers in general rebroadcast more information than the other categories. Next we re-examine the classical "two-step flow" theory of communications, finding considerable support for it on Twitter. Third, we find that URLs broadcast by different categories of users or containing different types of content exhibit systematically different lifespans. And finally, we examine the attention paid by the different user categories to different news topics.	Who says what to whom on twitter	NA:NA:NA:NA	2018
Demetris Antoniades:Iasonas Polakis:Georgios Kontaxis:Elias Athanasopoulos:Sotiris Ioannidis:Evangelos P. Markatos:Thomas Karagiannis	Short URLs have become ubiquitous. Especially popular within social networking services, short URLs have seen a significant increase in their usage over the past years, mostly due to Twitter's restriction of message length to 140 characters. In this paper, we provide a first characterization on the usage of short URLs. Specifically, our goal is to examine the content short URLs point to, how they are published, their popularity and activity over time, as well as their potential impact on the performance of the web. Our study is based on traces of short URLs as seen from two different perspectives: i) collected through a large-scale crawl of URL shortening services, and ii) collected by crawling Twitter messages. The former provides a general characterization on the usage of short URLs, while the latter provides a more focused view on how certain communities use shortening services. Our analysis highlights that domain and website popularity, as seen from short URLs, significantly differs from the distributions provided by well publicised services such as Alexa. The set of most popular websites pointed to by short URLs appears stable over time, despite the fact that short URLs have a limited high popularity lifetime. Surprisingly short URLs are not ephemeral, as a significant fraction, roughly 50%, appears active for more than three months. Overall, our study emphasizes the fact that short URLs reflect an "alternative" web and, hence, provide an additional view on web usage and content consumption complementing traditional measurement sources. Furthermore, our study reveals the need for alternative shortening architectures that will eliminate the non-negligible performance penalty imposed by today's shortening services.	we.b: the web of short urls	NA:NA:NA:NA:NA:NA:NA	2018
Silvio Lattanzi:Alessandro Panconesi:D. Sivakumar	We demonstrate how a recent model of social networks ("Affiliation Networks", [21]) offers powerful cues in local routing within social networks, a theme made famous by sociologist Milgram's "six degrees of separation" experiments. This model posits the existence of an "interest space" that underlies a social network; we prove that in networks produced by this model, not only do short paths exist among all pairs of nodes but natural local routing algorithms can discover them effectively. Specifically, we show that local routing can discover paths of length O(log2 n) to targets chosen uniformly at random, and paths of length O(1) to targets chosen with probability proportional to their degrees. Experiments on the co-authorship graph derived from DBLP data confirm our theoretical results, and shed light into the power of one step of lookahead in routing algorithms for social networks.	Milgram-routing in social networks	NA:NA:NA	2018
Dashun Wang:Zhen Wen:Hanghang Tong:Ching-Yung Lin:Chaoming Song:Albert-László Barabási	Information spreading processes are central to human interactions. Despite recent studies in online domains, little is known about factors that could affect the dissemination of a single piece of information. In this paper, we address this challenge by combining two related but distinct datasets, collected from a large scale privacy-preserving distributed social sensor system. We find that the social and organizational context significantly impacts to whom and how fast people forward information. Yet the structures within spreading processes can be well captured by a simple stochastic branching model, indicating surprising independence of context. Our results build the foundation of future predictive models of information flow and provide significant insights towards design of communication platforms.	Information spreading in context	NA:NA:NA:NA:NA:NA	2018
Cristian Danescu-Niculescu-Mizil:Michael Gamon:Susan Dumais	The psycholinguistic theory of communication accommodation accounts for the general observation that participants in conversations tend to converge to one another's communicative behavior: they coordinate in a variety of dimensions including choice of words, syntax, utterance length, pitch and gestures. In its almost forty years of existence, this theory has been empirically supported exclusively through small-scale or controlled laboratory studies. Here we address this phenomenon in the context of Twitter conversations. Undoubtedly, this setting is unlike any other in which accommodation was observed and, thus, challenging to the theory. Its novelty comes not only from its size, but also from the non real-time nature of conversations, from the 140 character length restriction, from the wide variety of social relation types, and from a design that was initially not geared towards conversation at all. Given such constraints, it is not clear a priori whether accommodation is robust enough to occur given the constraints of this new environment. To investigate this, we develop a probabilistic framework that can model accommodation and measure its effects. We apply it to a large Twitter conversational dataset specifically developed for this task. This is the first time the hypothesis of linguistic style accommodation has been examined (and verified) in a large scale, real world setting. Furthermore, when investigating concepts such as stylistic influence and symmetry of accommodation, we discover a complexity of the phenomenon which was never observed before. We also explore the potential relation between stylistic influence and network features commonly associated with social status.	Mark my words!: linguistic style accommodation in social media	NA:NA:NA	2018
Matthew Richardson:Ryen W. White	Synchronous social Q&A systems exist on the Web and in the enterprise to connect people with questions to people with answers in real-time. In such systems, askers' desire for quick answers is in tension with costs associated with interrupting numerous candidate answerers per question. Supporting users of synchronous social Q&A systems at various points in the question lifecycle (from conception to answer) helps askers make informed decisions about the likelihood of question success and helps answerers face fewer interruptions. For example, predicting that a question will not be well answered may lead the asker to rephrase or retract the question. Similarly, predicting that an answer is not forthcoming during the dialog can prompt system behaviors such as finding other answerers to join the conversation. As another example, predictions of asker satisfaction can be assigned to completed conversations and used for later retrieval. In this paper, we use data from an instant-messaging-based synchronous social Q&A service deployed to an online community of over two thousand users to study the prediction of: (i) whether a question will be answered, (ii) the number of candidate answerers that the question will be sent to, and (iii) whether the asker will be satisfied by the answer received. Predictions are made at many points of the question lifecycle (e.g., when the question is entered, when the answerer is located, halfway through the asker-answerer dialog, etc.). The findings from our study show that we can learn capable models for these tasks using a broad range of features derived from user profiles, system interactions, question setting, and the dialog between asker and answerer. Our research can lead to more sophisticated and more useful real-time Q&A support.	Supporting synchronous social q&a throughout the question lifecycle	NA:NA	2018
Mikhil Masli:Werner Geyer:Casey Dugan:Beth Brownholtz	Existing enterprise calendaring systems have suffered from problems like rigidity, lack of transparency, and poor integration with social networks. We present the system design and rationale for a novel social coordination mechanism, called "Suggestions," that addresses these issues. Our system integrates ideas drawn from designs of lightweight polling systems and one's social network into an open calendar tool, providing a space for users to coordinate, socialize around, or negotiate the "what" and the "when" of their events. Suggestions was released inside a large enterprise setting, where initial interviews revealed users' thoughts on transparent scheduling, reaching wider audiences and task appropriateness, and suggested ways to improve our design.	The design and usage of tentative events for time-based social coordination in the enterprise	NA:NA:NA:NA	2018
Tom Yeh:Brandyn White:Jose San Pedro:Boriz Katz:Larry S. Davis	The multimedia information retrieval community has dedicated extensive research effort to the problem of content-based image retrieval (CBIR). However, these systems find their main limitation in the difficulty of creating pictorial queries. As a result, few systems offer the option of querying by visual examples, and rely on automatic concept detection and tagging techniques to provide support for searching visual content using textual queries. This paper proposes and studies a practical multimodal web search scenario, where CBIR fits intuitively to improve the retrieval of rich information queries. Many online articles contain useful know-how knowledge about computer applications. These articles tend to be richly illustrated by screenshots. We present a system to search for such software know-how articles that leverages the visual correspondences between screenshots. Users can naturally create pictorial queries simply by taking a screenshot of the application to retrieve a list of articles containing a matching screenshot. We build a prototype comprising 150k articles that are classified into walkthrough, book, gallery, and general categories, and provide a comprehensive evaluation of this system, focusing on technical (accuracy of CBIR techniques) and usability (perceived system usefulness) aspects. We also consider the study of added value features of such a visual-supported search, including the ability to perform cross-lingual queries. We find that the system is able to retrieve matching screenshots for a wide variety of programs, across language boundaries, and provide subjectively more useful results than keyword-based web and image search engines.	A case for query by image and text content: searching computer help using screenshots and keywords	NA:NA:NA:NA:NA	2018
Young Yoon:Chunyang Ye:Hans-Arno Jacobsen	In service-oriented architectures (SOA), independently developed Web services can be dynamically composed. However, the composition is prone to producing semantically conflicting interactions among the services. For example, in an interdepartmental business collaboration through Web services, the decision by the marketing department to clear out the inventory might be inconsistent with the decision by the operations department to increase production. Resolving semantic conflicts is challenging especially when services are loosely coupled and their interactions are not carefully governed. To address this problem, we propose a novel distributed service choreography framework. We deploy safety constraints to prevent conflicting behavior and enforce reliable and efficient service interactions via federated publish/subscribe messaging, along with strategic placement of distributed choreography agents and coordinators to minimize runtime overhead. Experimental results show that our framework prevents semantic conflicts with negligible overhead and scales better than a centralized approach by up to 60%.	A distributed framework for reliable and efficient service choreographies	NA:NA:NA	2018
Samik Basu:Tevfik Bultan	Choreography analysis has been a crucial problem in service oriented computing. Interactions among services involve message exchanges across organizational boundaries in a distributed computing environment, and in order to build such systems in a reliable manner, it is necessary to develop techniques for analyzing such interactions. Choreography conformance involves verifying that a set of services behave according to a given choreography specification that characterizes their interactions. Unfortunately this is an undecidable problem when services interact with asynchronous communication. In this paper we present techniques that identify if the interaction behavior for a set of services remain the same when asynchronous communication is replaced with synchronous communication. This is called the synchronizability problem and determining the synchronizability of a set of services has been an open problem for several years. We solve this problem in this paper. Our results can be used to identify synchronizable services for which choreography conformance can be checked efficiently. Our results on synchronizability are applicable to any software infrastructure that supports message-based interactions.	Choreography conformance via synchronizability	NA:NA	2018
Gregory Grefenstette:Ricardo Baeza-Yates	The industrial track covered two full days divided in four sessions for the 11 papers accepted. Each session finished with a panel. The first session was about Big Data where research from Amazon, Ask, Netflix and Yahoo!, together with researchers from UCSB and Cal Poly was presented. The corresponding panel was on the industrial applications of open and big data moderated by François Bourdoncle from Dassault Systàmes. The second session was on eCommerce, Mobile and Patents where articles from eBay, Samsung, and Yahoo!, together with Oxford University was exposed. The panel of this session was on protecting innovation by using patents and/or copyright moderated by Eva Hopper of the European Patent Office. The third session was on Users and Linked Data with articles from IBM and Yahoo!. The subsequent panel was about Link Enterprise Data moderated by Christian Fauré from Cap Gemini. The fourth and final session was about Forums, Cloud, University, including work from Microsoft, UST (China) and the U. of Waterloo. The final panel was on Industry-University Collaboration moderated by Ricardo Baeza-Yates from Yahoo! Research. Considering the importance, novelty and diversity of the topics covered, we hope that you enjoyed this track.	Session details: Industry track presentations	NA:NA	2018
Mohamed Aly:Andrew Hatch:Vanja Josifovski:Vijay K. Narayanan	We present the experiences from building a web-scale user modeling platform for optimizing display advertising targeting at Yahoo!. The platform described in this paper allows for per-campaign maximization of conversions representing purchase activities or transactions. Conversions directly translate to advertiser's revenue, and thus provide the most relevant metrics of return on advertising investment. We focus on two major challenges: how to efficiently process histories of billions of users on a daily basis, and how to build per-campaign conversion models given the extremely low conversion rates (compared to click rates in a traditional setting). We first present mechanisms for building web-scale user profiles in a daily incremental fashion. Second, we show how to reduce the latency through in-memory processing of billions of user records. Finally, we discuss a technique for scaling the number of handled campaigns/models by introducing an efficient labeling technique that allows for sharing negative training examples across multiple campaigns.	Web-scale user modeling for targeting	NA:NA:NA:NA	2018
Eriq Augustine:Cailin Cushing:Alex Dekhtyar:Kevin McEntee:Kimberly Paterson:Matt Tognetti	Over the past couple of years, Netflix has significantly expanded its online streaming offerings, which now encompass multiple delivery platforms and thousands of titles available for instant view. This paper documents the design and development of an outage detection system for the online services provided by Netflix. Unlike other internal quality-control measures used at Netflix, this system uses only publicly available information: the tweets, or Twitter posts, that mention the word "Netflix," and has been developed and deployed externally, on servers independent of the Netflix infrastructure. This paper discussed the system and provides assessment of the accuracy of its real-time detection and alert mechanisms.	Outage detection via real-time social stream analysis: leveraging the power of online complaints	NA:NA:NA:NA:NA:NA	2018
Xiubo Geng:Xin Fan:Jiang Bian:Xin Li:Zhaohui Zheng	E-commerce has emerged as a popular channel for Web users to conduct transaction over Internet. In e-commerce services, users usually prefer to discover information via querying over category browsing, since the hierarchical structure supported by category browsing can provide them a more effective and efficient way to find their interested properties. However, in many emerging e-commerce services, well-defined hierarchical structures are not always available; moreover, in some other e-commerce services, the pre-defined hierarchical structures are too coarse and less intuitive to distinguish properties according to users interests. This will lead to very bad user experience. In this paper, to address these problems, we propose a hierarchical clustering method to build the query taxonomy based on users' exploration behavior automatically, and further propose an intuitive and light-weight approach to construct browsing list for each cluster to help users discover interested items. The advantage of our approach is four folded. First, we build a hierarchical taxonomy automatically, which saves tedious human effort. Second, we provide a fine-grained structure, which can help user reach their interested items efficiently. Third, our hierarchical structure is derived from users' interaction logs, and thus is intuitive to users. Fourth, given the hierarchical structures, for each cluster, we present both frequently clicked items and retrieved results of queries in the category, which provides more intuitive items to users. We evaluate our work by applying it to the exploration task of a real-world e-commerce service, i.e. online shop for smart mobile phone's apps. Experimental results show that our clustering algorithm is efficient and effective to assist users to discover their interested properties, and further comparisons illustrate that the hierarchical topic browsing performs much better than existing category browsing approach (i.e. Android Market mobile apps category) in terms of information exploration.	Optimizing user exploring experience in emerging e-commerce products	NA:NA:NA:NA:NA	2018
Jingtian Jiang:Nenghai Yu:Chin-Yew Lin	In this paper, we present FoCUS (Forum Crawler Under Supervision), a supervised web-scale forum crawler. The goal of FoCUS is to only trawl relevant forum content from the web with minimal overhead. Forum threads contain information content that is the target of forum crawlers. Although forums have different layouts or styles and are powered by different forum software packages, they always have similar implicit navigation paths connected by specific URL types to lead users from entry pages to thread pages. Based on this observation, we reduce the web forum crawling problem to a URL type recognition problem and show how to learn accurate and effective regular expression patterns of implicit navigation paths from an automatically created training set using aggregated results from weak page type classifiers. Robust page type classifiers can be trained from as few as 5 annotated forums and applied to a large set of unseen forums. Our test results show that FoCUS achieved over 98% effectiveness and 97% coverage on a large set of test forums powered by over 150 different forum software packages.	FoCUS: learning to crawl web forums	NA:NA:NA	2018
Shahab Kamali:Johnson Apacible:Yasaman Hosseinkashi	Conventional search engines such as Bing and Google provide a user with a short answer to some queries as well as a ranked list of documents, in order to better meet her information needs. In this paper we study a class of such queries that we call math. Calculations (e.g. "12% of 24$ ", "square root of 120"), unit conversions (e.g. "convert 10 meter to feet"), and symbolic computations (e.g. "plot x^2+x+1") are examples of math queries. Among the queries that should be answered, math queries are special because of the infinite combinations of numbers and symbols, and rather few keywords that form them. Answering math queries must be done through real time computations rather than keyword searches or database look ups. The lack of a formal definition for the entire range of math queries makes it hard to automatically identify them all. We propose a novel approach for recognizing and classifying math queries using large scale search logs, and investigate its accuracy through empirical experiments and statistical analysis. It allows us to discover classes of math queries even if we do not know their structures in advance. It also helps to identify queries that are not math even though they might look like math queries. We also evaluate the usefulness of math answers based on the implicit feedback from users. Traditional approaches for evaluating the quality of search results mostly rely on the click information and interpret a click on a link as a sign of satisfaction. Answers to math queries do not contain links, therefore such metrics are not applicable to them. In this paper we describe two evaluation metrics that can be applied for math queries, and present the results on a large collection of math queries taken from Bing's search logs.	Answering math queries with search engines	NA:NA:NA	2018
Ronny Lempel:Ronen Barenboim:Edward Bortnikov:Nadav Golbandi:Amit Kagian:Liran Katzir:Hayim Makabee:Scott Roy:Oren Somekh	The process of creating modern Web media experiences is challenged by the need to adapt the content and presentation choices to dynamic real-time fluctuations of user interest across multiple audiences. We introduce FAME -- a Framework for Agile Media Experiences -- which addresses this scalability problem. FAME allows media creators to define abstract page models that are subsequently transformed into real experiences through algorithmic experimentation. FAME's page models are hierarchically composed of simple building blocks, mirroring the structure of most Web pages. They are resolved into concrete page instances by pluggable algorithms which optimize the pages for specific business goals. Our framework allows retrieving dynamic content from multiple sources, defining the experimentation's degrees of freedom, and constraining the algorithmic choices. It offers an effective separation of concerns in the media creation process, enabling multiple stakeholders with profoundly different skills to apply their crafts and perform their duties independently, composing and reusing each other's work in modular ways.	Hierarchical composable optimization of web pages	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Boris Motik:Ian Horrocks:Su Myeon Kim	To make mobile device applications more intelligent, one can combine the information obtained via device sensors with background knowledge in order to deduce the user's current context, and then use this context to adapt the application's behaviour to the user's needs. In this paper we describe Delta-Reasoner, a key component of the Intelligent Mobile Platform (IMP), which was designed to support context-aware applications running on mobile devices. Context-aware applications and the mobile platform impose unusual requirements on the reasoner, which we have met by incorporating advanced features such as incremental reasoning and continuous query evaluation into our reasoner. Although we have so far been able to conduct only a very preliminary performance evaluation, our results are very encouraging: our reasoner exhibits sub-second response time on ontologies whose size significantly exceeds the size of the ontologies used in the IMP.	Delta-reasoner: a semantic web reasoner for an intelligent mobile platform	NA:NA:NA	2018
Gyanit Singh:Nish Parikh:Neel Sundaresan	In e-commerce applications product descriptions are often concise. E-Commerce search engines often have to deal with queries that cannot be easily matched to product inventory resulting in zero recall or null query situations. Null queries arise from differences in buyer and seller vocabulary or from the transient nature of products. In this paper, we describe a system that rewrites null e-commerce queries to find matching products as close to the original query as possible. The system uses query relaxation to rewrite null queries in order to match products. Using eBay as an example of a dynamic marketplace, we show how using temporal feedback that respects product category structure using the repository of expired products, we improve the quality of recommended results. The system is scalable and can be run in a high volume setting. We show through our experiments that high quality product recommendations for more than 25% of null queries are achievable.	Rewriting null e-commerce queries to recommend products	NA:NA:NA	2018
Sivan Yogev:Haggai Roitman:David Carmel:Naama Zwerdling	In this paper we describe a novel approach for exploratory search over rich entity-relationship data that utilizes a unique combination of expressive, yet intuitive, query language, faceted search, and graph navigation. We describe an extended faceted search solution which allows to index, search, and browse rich entity-relationship data. We report experimental results of an evaluation study, using a benchmark of several of entity-relationship datasets, demonstrating that our exploratory approach is both effective and efficient compared to other existing approaches.	Towards expressive exploratory search over entity-relationship data	NA:NA:NA:NA	2018
Xiaoqing Zheng:Yiling Gu:Yinsheng Li	Most of today's web content is designed for human consumption, which makes it difficult for software tools to access them readily. Even web content that is automatically generated from back-end databases is usually presented without the original structural information. In this paper, we present an automated information extraction algorithm that can extract the relevant attribute-value pairs from product descriptions across different sites. A notion, called structural-semantic entropy, is used to locate the data of interest on web pages, which measures the density of occurrence of relevant information on the DOM tree representation of web pages. Our approach is less labor-intensive and insensitive to changes in web-page format. Experimental results on a large number of real-life web page collections are encouraging and confirm the feasibility of the approach, which has been successfully applied to detect false drug advertisements on the web due to its capacity in associating the attributes of records with their respective values.	Data extraction from web pages based on structural-semantic entropy	NA:NA:NA	2018
Shanzhong Zhu:Alexandra Potapova:Maha Alabduljalil:Xin Liu:Tao Yang	Removing redundant content is an important data processing operation in search engines and other web applications. An offline approach can be important for reducing the engine's cost, but it is challenging to scale such an approach for a large data set which is updated continuously. This paper discusses our experience in developing a scalable approach with parallel clustering that detects and removes near duplicates incrementally when processing billions of web pages. It presents a multidimensional mapping to balance the load among multiple machines. It further describes several approximation techniques to efficiently manage distributed duplicate groups with transitive relationship. The experimental results evaluate the efficiency and accuracy of the incremental clustering, assess the effectiveness of the multidimensional mapping, and demonstrate the impact on online cost reduction and search quality.	Clustering and load balancing optimization for redundant content removal	NA:NA:NA:NA:NA	2018
Sylvie Calabretto:Jalal U. Mahmud:Gabriella Pasi:Pierre-Edouard Portier	We are very satisfied with the variety and the quality of the contributions selected for the WWW2012 PhD Symposium. This is due mainly to the work of the members of the Program Committee, who we thank for successfully completing such a difficult task. Indeed, the criteria taken into account while reviewing a PhD Symposium paper are uncommon ones. Firstly, all the retained papers are introducing innovative scientific ideas. Moreover, they are presenting work in progress, sometimes difficult to be evaluated, and well suited for receiving feedback from experienced researchers as well as from other PhD students working in related research areas. The PhD Symposium is organized into six sessions split between Wednesday 4/18 and Friday 4/20. The first session, on Wednesday afternoon, will have an "annotation" flavor with three works introducing innovative ways of using user generated annotations. The second session, also on Wednesday afternoon, will host papers dealing with Information Retrieval issues in the context of the Web. The third one, on Thursday afternoon, is centered on works related to Social Networks approach applied to the Web. The fourth session, the same afternoon, includes papers aiming at extracting knowledge from the big Web graph. On the last day, the fifth session will be dedicated to works related to the Semantic Web. Finally, the last session, on Friday afternoon, will give us the opportunity of discovering works about innovative Web applications. We hope you enjoy the Symposium!	Session details: PhD Symposium	NA:NA:NA:NA	2018
Giovanni Bartolomeo:Stefano Salsano	Entities have been deserved special attention in the latest years, however their identification is still troublesome. Existing approaches exploit ad hoc services or centralized architectures. In this paper we present a novel approach to recognize naturally emerging entity identifiers built on top of Linked Data concepts and protocols.	From linked data to linked entities: a migration path	NA:NA	2018
Maral Dadvar:Franciska de Jong	As a result of the invention of social networks friendships, relationships and social communications have all gone to a new level with new definitions. One may have hundreds of friends without even seeing their faces. Meanwhile, alongside this transition there is increasing evidence that online social applications have been used by children and adolescents for bullying. State-of-the-art studies in cyberbullying detection have mainly focused on the content of the conversations while largely ignoring the users involved in cyberbullying. We propose that incorporation of the users' information, their characteristics, and post-harassing behaviour, for instance, posting a new status in another social network as a reaction to their bullying experience, will improve the accuracy of cyberbullying detection. Cross-system analyses of the users' behaviour - monitoring their reactions in different online environments - can facilitate this process and provide information that could lead to more accurate detection of cyberbullying.	Cyberbullying detection: a step toward a safer internet yard	NA:NA	2018
Muhammad Faheem	The steady growth of the World Wide Web raises challenges regarding the preservation of meaningful Web data. Tools used currently by Web archivists blindly crawl and store Web pages found while crawling, disregarding the kind of Web site currently accessed (which leads to suboptimal crawling strategies) and whatever structured content is contained in Web pages (which results in page-level archives whose content is hard to exploit). We focus in this PhD work on the crawling and archiving of publicly accessible Web applications, especially those of the social Web. A Web application is any application that uses Web standards such as HTML and HTTP to publish information on the Web, accessible by Web browsers. Examples include Web forums, social networks, geolocation services, etc. We claim that the best strategy to crawl these applications is to make the Web crawler aware of the kind of application currently processed, allowing it to refine the list of URLs to process, and to annotate the archive with information about the structure of crawled content. We add adaptive characteristics to an archival Web crawler: being able to identify when a Web page belongs to a given Web application and applying the appropriate crawling and content extraction methodology.	Intelligent crawling of web applications for web archiving	NA	2018
Javier D. Fernández	The Web of Data is increasingly producing large RDF datasets from diverse fields of knowledge, pushing the Web to a data-to-data cloud. However, traditional RDF representations were inspired by a document-centric view, which results in verbose/redundant data, costly to exchange and post-process. This article discusses an ongoing doctoral thesis addressing efficient formats for publication, exchange and consumption of RDF on a large scale. First, a binary serialization format for RDF, called HDT, is proposed. Then, we focus on compressed rich-functional structures which take part of efficient HDT representation as well as most applications performing on huge RDF datasets.	Binary RDF for scalable publishing, exchanging and consumption in the web of data	NA	2018
Riste Gligorov	In recent years, crowdsourcing has gained attention as an alternative method for collecting video annotations. An example is the internet video labeling game Waisda? launched by the Netherlands Institute for Sound and Vision. The goal of this PhD research is to investigate the value of the user tags collected with this video labeling game. To this end, we address the following four issues. First, we perform a comparative analysis between user-generated tags and professional annotations in terms of what aspects of videos they describe. Second, we measure how well user tags are suited for fragment retrieval and compare it with fragment search based on other sources like transcripts and professional annotations. Third, as previous research suggested that user tags predominately refer to objects and rarely describe scenes, we will study whether user tags can be successfully exploited to generate scene-level descriptions. Finally, we investigate how tag quality can be characterized and potential methods to improve it.	User-generated metadata in audio-visual collections	NA	2018
Simon Jonassen	In theory, term-wise partitioned indexes may provide higher throughput than document-wise partitioned. In practice, term-wise partitioning shows lacking scalability with increasing collection size and intra-query parallelism, which leads to long query latency and poor performance at low query loads. In our work, we have developed several techniques to deal with these problems. Our current results show a significant improvement over the state-of-the-art approach on a small distributed IR system, and our next objective is to evaluate the scalability of the improved approach on a large system. In this paper, we describe the relation between our work and the problem of scalability, summarize the results, limitations and challenges of our current work, and outline directions for further research.	Scalable search platform: improving pipelined query processing for distributed full-text retrieval	NA	2018
Somayeh Khatiban	The term online reputation addresses trust relationships amongst agents in dynamic open systems. These can appear as ratings, recommendations, referrals and feedback. Several reputation models and rating aggregation algorithms have been proposed. However, finding a trusted entity on the web is still an issue as all reputation systems work individually. The aim of this project is to introduce a global reputation system that aggregates people's opinions from different resources (e.g. e-commerce websites, and review) with the help federated search techniques. A sentiment analysis approach is subsequently used to extract high quality opinions and inform how to increase trust in the search result.	Building reputation and trust using federated search and opinion mining	NA	2018
Vikash Kumar	Rule based information processing has traditionally been vital in many aspects of business, process manufacturing and information science. The need for rules gets even more magnified when limitations of ontology development in OWL are taken into account. In conjunction, the potent combination of ontology and rule based applications could be the future of information processing and knowledge representation on the web. However, semantic rules tend to be very dependent on multitudes of parameters and context data making it less flexible for use in applications where users could benefit from each other by socially sharing intelligence in the form of policies. This work aims to address this issue arising in rule based semantic applications in the use cases of smart home communities and privacy aware m-commerce setting for mobile users. In this paper, we propose a semantic policy sharing and adaptation infrastructure that enables a semantic rule created in one set of environmental, physical and contextual settings to be adapted for use in a situation when those settings/parameters/context variables change. The focus will mainly be on behavioural policies in the smart home use case and privacy enforcing and data filtering policies in the m-commerce scenario. Finally, we look into the possibility of making this solution application independent so that the benefits of such a policy adaptation infrastructure could be exploited in other application settings as well.	A semantic policy sharing and adaptation infrastructure for pervasive communities	NA	2018
Sangkeun Lee	As the volume of information on the Web is explosively growing, recommender systems have become essential tools for helping users to find what they need or prefer. Most existing systems are two-dimensional in that they only exploit User and Item dimensions and perform a typical form of recommendation 'Recommending Item to User'. Yet, in many applications, the capabilities of dealing with multidimensional information and of adapting to various forms of recommendation requests are very important. In this paper, we take a graph-based approach to accomplishing such requirements in recommender systems and present a generic graph-based multidimensional recommendation framework. Based on the framework, we propose two homogeneous graph-based and one heterogeneous graph-based multidimensional recommendation methods. We expect our approach will be useful for increasing recommendation performance and enabling flexibility of recommender systems so that they can incorporate various user intentions into their recommendation process. We present our research result that we have reached and discuss remaining challenges and future work.	A generic graph-based multidimensional recommendation framework and its implementations	NA	2018
Elaheh Momeni	Many social media portals are featuring annotation functionality in order to integrate the end users' knowledge with existing digital curation processes. This facilitates extending existing metadata about digital resources. However, due to various levels of annotators' expertise, the quality of annotations can vary from excellent to vague. The evaluation and moderation of annotations (be they troll, vague, or helpful) have not been sufficiently analyzed automatically. Available approaches mostly attempt to solve the problem by using distributed moderation systems, which are influenced by factors affecting accuracy (such as imbalance voting). Despite this, we hypothesize that analyzing and exploiting both content and context dimensions of annotations may assist the automatic moderation process. In this research, we focus on leveraging the context and content features of social web annotations for semi-automatic semantic moderation. This paper describes the vision of our research, proposes an approach for semi-automatic semantic moderation, introduces an ongoing effort from which we collect data that can serve as a basis for evaluating our assumption, and report on lessons learned so far.	Semi-automatic semantic moderation of web annotations	NA	2018
Nataliia Pobiedina	The proposed PhD work approaches the problem of information flow and change on the Web. To model temporal dynamics both of the Web structure and its content, the author proposes to apply the framework of stochastic graph transformation systems. This framework is currently widely used in software engineering and model checking. A quantitative and qualitative evaluation of the framework will be performed during a case study of the short-term temporal behavior of economics news on selected English news websites and blogs over selected time period.	Modeling the flow and change of information on the web	NA	2018
Massimiliano Ruocco	Media sharing applications such as Panoramio and Flickr contain a huge amount of pictures that need to be organized to facilitate browsing and retrieval. Such pictures are often surrounded by a set of metadata or image tags, constituting the image context. With the advent of the paradigm of Web 2.0 especially the past five years, the concept of image context has further evolved, allowing users to tag their own and other people's pictures. Focusing on tagging, we distinguish between static and dynamic features. The set of static features include textual and visual features, as well as the contextual information. Further, we may identify other features belonging to the social context as a result of the usage within the media sharing applications. Due to their dynamic nature, we call these the dynamic set of features. In this work, we assume that every media uploaded contains both static and dynamic features. In addition, a user may be linked with other users with whom he/she shares common interests. This has resulted in a new series of challenges within the research field of semantic understanding. One of the main goals of this work is to address these challenges.	Context-aware image semantic extraction in the social web	NA	2018
Oshani Wasana Seneviratne	Given the ubiquity of data on the web, and the lack of usage restriction enforcement mechanisms, stories of personal, creative and other kinds of data misuses are on the rise. There should be both sociological and technological mechanisms that facilitate accountability on the web that would prevent such data misuses. Sociological mechanisms appeal to the data consumer's self-interest in adhering to the data provider's desires. This involves a system of rewards such as recognition and financial incentives, and deterrents such as prohibitions by laws for any violations and social pressure. Bur there is no well-defined technological mechanism for the discovery of accountability or the lack of it on the web. As part of my PhD thesis I propose a solution to this problem by designing a web protocol called HTTPA (Accountable HTTP). This protocol will enable data consumers and data producers to agree to specific usage restrictions, preserve the provenance of data transferred from a web server to a client and back to another web server, and more importantly provide a mechanism to derive an `audit trail' for the data reuse with the help of a trusted intermediary called a `Provenance Tracker Network'.	Augmenting the web with accountability	NA	2018
Cheng Wang	Web extraction is the task of turning unstructured HTML into knowledge. Computers are able to generate annotations of unstructured HTML, but it is more important to turn those annotations into structured knowledge. Unfortunately, the current systems extracting knowledge from result pages lack accuracy. In this proposal, we present AMBER, a system fully automated turning annotations to structured knowledge from any result page of a given domain. AMBER observes basic domain attributes on a page and leverages repeated occurrences of similar attributes to group related attributes into records. This contrasts to previous approaches that analyze the repeated structure only of the HTML, as no domain knowledge is available. Our multi-domain experimental evaluation on hundreds of sites demonstrates that AMBER achieves accuracy (>98%) comparable to skilled human annotator.	AMBER: turning annotations into knowledge	NA	2018
Wei Wang	To relieve "News Information Overload", in this paper, we propose a novel approach of 5W1H (who, what, whom, when, where, how) event semantic elements extraction for Chinese news event knowledge base construction. The approach comprises a key event identification step, an event semantic elements extraction step and an event ontology population step. We first use a machine learning method to identify the key events from Chinese news stories. Then we extract event 5W1H elements by employing the combination of SRL, NER technique and rule-based method. At last we populate the extracted facts of news events to NOEM, an event ontology designed specifically for modeling semantic elements and relations of events. Our experiments on real online news data sets show the reasonability and feasibility of our approach.	Chinese news event 5W1H semantic elements extraction for event ontology population	NA	2018
Yulian Yang	As scalability and flexibility have become the critical concerns in information management systems, self-organizing networks attract attentions from both research and industrial communities. This work proposes a semi-structured semantic overlay for information retrieval in large-scale self-organizing networks. With the autonomy to their own resources, the nodes are organized into a semantic overlay hosting topically discriminative communities. For information retrieval within a community, unstructured routing approach is employed for the sake of flexibility; While for joining new nodes and routing queries to a distant community, a structured mechanism is designed to save the traffic and time cost. Different from the semantic overlay in the literature, our proposal has three contributions: 1. we design topic-based indexing to form and maintain the semantic overlay, to guarantee both scalability and efficiency; 2. We introduce unstructured routing approach within the community, to allow flexible node joining and leaving; 3. We take advantage of the interaction among nodes to capture the overlay changes and make corresponding adaption in topic-based indexing.	Semi-structured semantic overlay for information retrieval in self-organizing networks	NA	2018
Julien Masanes:Ricardo Baeza-Yates:Wolfgang Nejdl:Marko Grobelnik:Manuel Tomas Carrasco Benitez	Taking the opportunity of having WWW in Europe this year, we are excited to invite you to a special track dedicated to web research in the making, looking at a selection the most promising projects funded by the European Commisision in this domain. This is a also an opportunity to discuss openly on future topics for research, with representatives of the European Commission exposing their goals for the next framework programme. The session will be specially looking into the following domains: Data management, Search, Social computing, Infrastructure for Web Science, Language technology and Mobile. We particularly invite researchers of other continents to come and share their views on the approaches and directions taken in these projects.	Session details: European track presentations	NA:NA:NA:NA:NA	2018
Serge Abiteboul:Pierre Senellart:Victor Vianu	The Webdam ERC grant is a five-year project that started in December 2008. The goal is to develop a formal model for Web data management that would open new horizons for the development of the Web in a well-principled way, enhancing its functionality, performance, and reliability. Specifically, the goal is to develop a universally accepted formal framework for describing complex and flexible interacting Web applications featuring notably data exchange, sharing, integration, querying, and updating. We also propose to develop formal foundations that will enable peers to concurrently reason about global data management activities, cooperate in solving specific tasks, and support services with desired quality of service. Although the proposal addresses fundamental issues, its goal is to serve as the basis for future software development for Web data management.	The ERC webdam on foundations of web data management	NA:NA:NA	2018
Shadi Abou-Zahra	The W3C web accessibility standards have now existed for over a decade yet implementation of accessible websites, software, and web technologies is lagging behind this development. This fact is largely due to lack of knowledge and expertise among developers and due to fragmentation of web accessibility approaches. It is an opportune time to develop authoritative practical guidance and harmonized approaches, and to research potential challenges and opportunities in future technologies in a collaborative setting. The EC-funded WAI-ACT project addresses these needs through use of an open cooperation framework that builds on and extends the existing mechanisms of the W3C Web Accessibility Initiative (WAI). This paper presents the WAI-ACT project and how it will drive accessibility implementation in advanced web technologies.	WAI-ACT: web accessibility now	NA	2018
Pierre Andrews:Francesco De Natale:Sven Buschbeck:Anthony Jameson:Kerstin Bischoff:Claudiu S. Firan:Claudia Niederée:Vasileios Mezaris:Spiros Nikolopoulos:Vanessa Murdock:Adam Rae	The idea of the European project GLOCAL is to use events as the central concept for search, organization and combination of multimedia content from various sources. For this purpose methods for event detection and event matching as well as media analysis are developed. Considered events range from private, over local, to global events.	GLOCAL: event-based retrieval of networked media	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Marco Brambilla:Piero Fraternali:Carmen Karina Vaca Ruiz	Social BPM fuses business process management practices with social networking applications, with the aim of enhancing the enterprise performance by means of a controlled participation of external stakeholders to process design and enactment. This project-centered demonstration paper proposes a model-driven approach to participatory and social enactment of business processes. The approach consists of defining a specific notation for describing Social BPM behaviors (defined as a BPMN 2.0 extension), a methodology, and a technical framework that allows enterprises to implement of social processes as Web applications integrated with public or private Web social networks. The presented work is performed within the BPM4People SME Capacities project	Combining social web and BPM for improving enterprise performances: the BPM4People approach to social BPM	NA:NA:NA	2018
David Carmel:Naama Zwerdling:Sivan Yogev	In this paper we describe an entity oriented search and exploration system that we are developing for the EU Cultura project.	Entity oriented search and exploration for cultural heritage collections: the EU cultura project	NA:NA:NA	2018
Milen Chechev:Meritxell Gonzàlez:Lluís Màrquez:Cristina España-Bonet	This paper describes the patents retrieval prototype developed within the MOLTO project. The prototype aims to provide a multilingual natural language interface for querying the content of patent documents. The developed system is focused on the biomedical and pharmaceutical domain and includes the translation of the patent claims and abstracts into English, French and German. Aiming at the best retrieval results of the patent information and text content, patent documents are preprocessed and semantically annotated. Then, the annotations are stored and indexed in an OWLIM semantic repository, which contains a patent specific ontology and others from different domains. The prototype, accessible online at http://molto-patents.ontotext.com, presents a multilingual natural language interface to query the retrieval system. In MOLTO, the multilingualism of the queries is addressed by means of the GF Tool, which provides an easy way to build and maintain controlled language grammars for interlingual translation in limited domains. The abstract representation obtained from the GF is used to retrieve both the matched RDF instances and the list of patents semantically related to the user's search criteria. The online interface allows to browse the retrieved patents and shows on the text the semantic annotations that explain the reason why any particular patent has matched the user's criteria.	The patents retrieval prototype in the MOLTO project	NA:NA:NA:NA	2018
Olexiy Chudnovskyy:Tobias Nestler:Martin Gaedke:Florian Daniel:José Ignacio Fernández-Villamor:Vadim Chepegin:José Angel Fornas:Scott Wilson:Christoph Kögler:Heng Chang	With the success of Web 2.0 we are witnessing a growing number of services and APIs exposed by Telecom, IT and content providers. Targeting the Web community and, in particular, Web application developers, service providers expose capabilities of their infrastructures and applications in order to open new markets and to reach new customer groups. However, due to the complexity of the underlying technologies, the last step, i.e., the consumption and integration of the offered services, is a non-trivial and time-consuming task that is still a prerogative of expert developers. Although many approaches to lower the entry barriers for end users exist, little success has been achieved so far. In this paper, we introduce the OMELETTE project and show how it addresses end-user-oriented telco mashup development. We present the goals of the project, describe its contributions, summarize current results, and describe current and future work.	End-user-oriented telco mashups: the OMELETTE approach	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Dana Dannélls:Mariana Damova:Ramona Enache:Milen Chechev	In this paper we report on our ongoing work in the EU project Multilingual Online Translation (MOLTO), supported by the European Union Seventh Framework Programme under grant agreement FP7-ICT-247914. More specifically, we present work workpackage 8 (WP8): Case Study: Cultural Heritage. The objective of the work is to build an ontology-based multilingual application for museum information on the Web. Our approach relies on the innovative idea of Reason-able View of the Web of linked data applied to the domain of cultural heritage. We have been developing a Web application that uses Semantic Web ontologies for generating coherent multilingual natural language descriptions about museum objects. We have been experimenting with museum data to test our approach and find that it performs well for the examined languages.	Multilingual online generation from semantic web ontologies	NA:NA:NA:NA	2018
Kerstin Denecke:Peter Dolog:Pavel Smrz	Disease surveillance systems exist to offer an easily accessible "epidemiological snapshot" on up-to-date summary statistics for numerous infectious diseases. However, these indicator-based systems represent only part of the solution. Experiences show that they fail when confronted with agents that are new emerging like the agents causing the lung disease SARS in 2002. Further, due to slow reporting mechanisms, the time until health threats become visible to public health officials can be long. The M-Eco project provides an event-based approach to the early detection of emerging health threats. The developed technologies exploit content from social media and multimedia data as input and analyze it by sophisticated event-detection techniques to identify potential threats. Alerts for public health threats are provided to the user in a personalized way.	Making use of social media data in public health	NA:NA:NA	2018
Sotiris Diplaris:Symeon Papadopoulos:Ioannis Kompatsiaris:Ayse Goker:Andrew Macfarlane:Jochen Spangenberg:Hakim Hacid:Linas Maknavicius:Matthias Klusch	SocialSensor will develop a new framework for enabling real-time multimedia indexing and search in the Social Web. The project moves beyond conventional text-based indexing and retrieval models by mining and aggregating user inputs and content over multiple social networking sites. Social Indexing will incorporate information about the structure and activity of the users social network directly into the multimedia analysis and search process. Furthermore, it will enhance the multimedia consumption experience by developing novel user-centric media visualization and browsing paradigms. For example, SocialSensor will analyse the dynamic and massive user contributions in order to extract unbiased trending topics and events and will use social connections for improved recommendations. To achieve its objectives, SocialSensor introduces the concept of Dynamic Social COntainers (DySCOs), a new layer of online multimedia content organisation with particular emphasis on the real-time, social and contextual nature of content and information consumption. Through the proposed DySCOs-centered media search, SocialSensor will integrate social content mining, search and intelligent presentation in a personalized, context and network-aware way, based on aggregation and indexing of both UGC and multimedia Web content.	SocialSensor: sensing user generated input for improved media discovery and experience	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
David Filip:Dave Lewis:Felix Sasaki	We report on the MultilingualWeb initiative, a collaboration between the W3C Internationalization Activity and the European Commission, realized as a series of EC-funded projects. We review the outcomes of "MultilingualWeb", which conducted 4 workshops analyzing "gaps" within Web standardization that currently hinder multilinguality. Gap analysis led to formation of "MultilingualWeb-LT" - project and W3C Working Group with cross industry representation that will address priority issues via standardization of interoperability metadata.	The multilingual web: report on multilingualweb initiative	NA:NA:NA	2018
Marie-Claire Forgue:Dominique Hazaël-Massieux	The popularity of mobile applications is very high and still growing rapidly. These applications allow their users to stay connected with a large number of service providers in seamless fashion, both for leisure and productivity. But service prThe popularity of mobile applications is very high and still growing rapidly. These applications allow their users to stay connected with a large number of service providers in seamless fashion, both for leisure and productivity. But service providers suffer from the high fragmentation of mobile development platforms that force them to develop, maintain and deploy their applications in a large number of versions and formats. The Mobile Web Applications (MobiWebApp [1]) EU project aims to build on Europe's strength in mobile technologies to enable European research and industry to strengthen its position in Web technologies to be active and visible on the mobile applications market.	Mobile web applications: bringing mobile apps and web together	NA:NA	2018
Piero Fraternali:Marco Tagliasacchi:Davide Martinenghi:Alessandro Bozzon:Ilio Catallo:Eleonora Ciceri:Francesco Nucci:Vincenzo Croce:Ismail Sengor Altingovde:Wolf Siberski:Fausto Giunchiglia:Wolfgang Nejdl:Martha Larson:Ebroul Izquierdo:Petros Daras:Otto Chrons:Ralph Traphoener:Bjoern Decker:John Lomas:Patrick Aichroth:Jasminko Novak:Ghislain Sillaume:F. Sanchez Figueroa:Carolina Salas-Parra	The Cubrik Project is an Integrated Project of the 7th Framework Programme that aims at contributing to the multimedia search domain by opening the architecture of multimedia search engines to the integration of open source and third party content annotation and query processing components, and by exploiting the contribution of humans and communities in all the phases of multimedia search, from content processing to query processing and relevance feedback processing. The CUBRIK presentation will showcase the architectural concept and scientific background of the project and demonstrate an initial scenario of human-enhanced content and query processing pipeline.	The CUBRIK project: human-enhanced time-aware multimedia search	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Christian Fuhrhop:John Lyle:Shamal Faily	This poster paper describes the webinos project and presents the architecture and security features developed in webinos. It highlights the main objectives and concepts of the project and describes the architecture derived to achive the objectives.	The webinos project	NA:NA:NA	2018
Tim Furche:Georg Gottlob:Giovanni Grasso:Omer Gunes:Xiaoanan Guo:Andrey Kravchenko:Giorgio Orsi:Christian Schallhart:Andrew Sellers:Cheng Wang	Search engines are the sinews of the web. These sinews have become strained, however: Where the web's function once was a mix of library and yellow pages, it has become the central marketplace for information of almost any kind. We search more and more for objects with specific characteristics, a car with a certain mileage, an affordable apartment close to a good school, or the latest accessory for our phones. Search engines all too often fail to provide reasonable answers, making us sift through dozens of websites with thousands of offers--never to be sure a better offer isn't just around the corner. What search engines are missing is understanding of the objects and their attributes published on websites. Automatically identifying and extracting these objects is akin to alchemy: transforming unstructured web information into highly structured data with near perfect accuracy. With DIADEM we present a formula for this transformation, but at a price: DIADEM identifies and extracts data from a website with high accuracy. The price is that for this task we need to provide DIADEM with extensive knowledge about the ontology and phenomenology of the domain, i.e., about entities (and relations) and about the representation of these entities in the textual, structural, and visual language of a website of this domain. In this demonstration, we demonstrate with a first prototype of DIADEM that, in contrast to alchemists, DIADEM has developed a viable formula.	DIADEM: domain-centric, intelligent, automated data extraction methodology	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
George Papadakis:Konstantinos Tserpes:Emmanuel Sardis:Magdalini Kardara:Athanasios Papaoikonomou:Fotis Aisopos	Social Network (SN) environments are the ideal future service marketplaces. It is well known and documented that SN users are increasing at a tremendous pace. Taking advantage of these social dynamics as well as the vast volumes, of amateur content generated every second, is a major step towards creating a potentially huge market of services. In this paper, we describe the external web services that SocIoS project is researching and developing, and will support with the Social Media community. Aiming to support the end users of SNs, to enhance their transactions with more automated ways, and with the advantage for better production and performance in their workflows over SNs inputs and content, this work presents the main architecture, functionality, and benefits per external service. Finally, introduces the end user, into the new era of SNs with business applicability and better social transactions over SNs content.	Social media meta-API: leveraging the content of social networks	NA:NA:NA:NA:NA:NA	2018
Thomas Risse:Wim Peters	The ARCOMEM project is about memory institutions like archives, museums and libraries in the age of the Social Web. Social media are becoming more and more pervasive in all areas of life. ARCOMEM's aim is to help to transform archives into collective memories that are more tightly integrated with their community of users and to exploit Web 2.0 and the wisdom of crowds to make Web archiving a more selective and meaning-based process. ARCOMEM (FP7-IST-270239) is an Integrating Project in the FP7 program of the European Commission, which involves twelve partners from academia, industry and public sector. The project will run from January 1, 2011 to December 31, 2013.	ARCOMEM: from collect-all ARchives to COmmunity MEMories	NA:NA	2018
Evangelos Sakkopoulos:Tomas Mildorf:Karel Charvat:Inga Berzina:Kai-Uwe Krause	Plan4All project contributes on the harmonization of spatial data and related metadata in order to make them available through Web across a linked data platform. A prototype of a Web search European spatial data portal is already available at http://www.plan4all.eu. The key aim is to provide a methodology and present best practices towards the standardization of spatial data according to the INSPIRE principles and provide results that would be a reference material for linking data and data specification from the spatial planning point of view. The results include methodology and implementation of multilingual search for data and common portrayal rules for content providers. These are critical services for sharing and understanding spatial data across Europe. Plan4All paradigm shows that a clear applicable methodology for harmonization of spatial data on all different topics of interest can be achieve efficiently. Plan4All shows that it is possible to build Pan European Web access, to link spatial data and to utilize multilingual metadata providing a roadmap for linked spatial data across and hopefully beyond Europe. The proposed demonstration based on Plan4All experience aims to show experience, best practices and methods to achieve data harmonization and provision of linked spatial data on the Web.	Plan4All GeoPortal: web of spatial data	NA:NA:NA:NA:NA	2018
John Soldatos:Moez Draief:Craig Macdonald:Iadh Ounis	This paper presents work in progress within the FP7 EU-funded project SMART to develop a multimedia search engine over content and information stemming from the physical world, as derived through visual, acoustic and other sensors. Among the unique features of the search engine is its ability to respond to social queries, through integrating social networks with sensor networks. Motivated by this innovation, the paper presents and discusses the state-of-the-art in participatory sensing and other technologies blending social and sensor networks.	Multimedia search over integrated social and sensor networks	NA:NA:NA:NA	2018
Marc Spaniol:Gerhard Weikum	Web-preservation organization like the Internet Archive not only capture the history of born-digital content but also reflect the zeitgeist of different time periods over more than a decade. This longitudinal data is a potential gold mine for researchers like sociologists, politologists, media and market analysts, or experts on intellectual property. The LAWA project (Longitudinal Analytics of Web Archive data) is developing an Internet-based experimental testbed for large-scale data analytics on Web archive collections. Its emphasis is on scalable methods for this specific kind of big-data analytics, and software tools for aggregating, querying, mining, and analyzing Web contents over long epochs. In this paper, we highlight our research on {\em entity-level analytics} in Web archive data, which lifts Web analytics from plain text to the entity-level by detecting named entities, resolving ambiguous names, extracting temporal facts and visualizing entities over extended time periods. Our results provide key assets for tracking named entities in the evolving Web, news, and social media.	Tracking entities in web archives: the LAWA project	NA:NA	2018
Thomas Steiner:Lorenzo Sutton:Sabine Spiller:Marilena Lazzaro:Francesco Nucci:Vincenzo Croce:Alberto Massari:Antonio Camurri:Anne Verroust-Blondet:Laurent Joyeux:Jonas Etzold:Paul Grimm:Athanasios Mademlis:Sotiris Malassiotis:Petros Daras:Apostolos Axenopoulos:Dimitrios Tzovaras	In this paper, we report on work around the I-SEARCH EU (FP7 ICT STREP) project whose objective is the development of a multimodal search engine. We present the project's objectives, and detail the achieved results, amongst which a Rich Unified Content Description format.	I-SEARCH: a multimodal search engine based on rich unified content description (RUCoD)	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Andrejs Vasiljevs:Raivis Skadins:Indra Samite	This paper presents European Union co-funded projects to advance the development and use of machine translation (MT) that will benefit from the possibilities provided by the Web. Current mass-market and online MT systems are of a general nature and perform poorly for smaller languages and domain specific texts. The ICT-PSP Programme project LetsMT! develops a user-driven machine translation "factory in the cloud" enabling web users to get customized MT that better fits their needs. Harnessing the huge potential of the web together with open statistical machine translation (SMT) technologies LetsMT! has created an innovative online collaborative platform for data sharing and building MT. Users can upload their parallel corpora to an online repository and generate user-tailored SMT systems based on user selected data. FP7 Programme project ACCURAT researches new methods for accumulating more data from the Web to improve the quality of data-driven machine translation systems. ACCURAT has created techniques and tools to use comparable corpora such as news feeds and multinational web pages. Although the majority of these texts are not direct translations, they share a lot of common paragraphs, sentences, phrases, terms and named entities in different languages which are useful for machine translation.	Enabling users to create their own web-based machine translation engine	NA:NA:NA	2018
Stuart N. Wrigley:Raúl García-Castro:Lyndon Nixon	This paper describes the main goals and outcomes of the EU-funded Framework 7 project entitled Semantic Evaluation at Large Scale (SEALS). The growth and success of the Semantic Web is built upon a wide range of Semantic technologies from ontology engineering tools through to semantic web service discovery and semantic search. The evaluation of such technologies - and, indeed, assessments of their mutual compatibility - is critical for their sustained improvement and adoption. The SEALS project is creating an open and sustainable platform on which all aspects of an evaluation can be hosted and executed and has been designed to accommodate most technology types. It is envisaged that the platform will become the de facto repository of test datasets and will allow anyone to organise, execute and store the results of technology evaluations free of charge and without corporate bias. The demonstration will show how individual tools can be prepared for evaluation, uploaded to the platform, evaluated according to some criteria and the subsequent results viewed. In addition, the demonstration will show the flexibility and power of the SEALS Platform for evaluation organisers by highlighting some of the key technologies used.	Semantic evaluation at large scale (SEALS)	NA:NA:NA	2018
Daniel Muller:Antoine Isaac:Bebo White	The goal of the WWW2012 Demo Track is to allow researchers and practitioners to demonstrate new Web-based systems illustrating the diversity, power, and benefit of the technology. In our CFP we encouraged submitters to search for "the WOW! effect" -- something in their demonstration that would cause attendees to think to themselves and say to others, "this is really amazing! This is what the Web is for!" The submitters met our challenge and provided a collection of demonstrations that have something for everyone. Our Program Committee had the difficult task of choosing the few that we were able to fit into the program. Out of 79 submissions, we were able to accept 14 full demonstrations and 22 short demonstrations. The demonstrations have been organized into five logical sub-tracks: Web Culture and Media Social Networks and Personalized Access to Information New Paradigms for Searching and Finding Helping Developers to Tame the Web Government and the People on the Web We are also fortunate to have 3 exceptional invited demonstrations. Piotr Adamczyk of the Google Art Project Team will demonstrate how Google has collaborated with some of the great art museums of the world to provide access to hundreds of artworks at incredible zoom levels. Such detail to art masterpieces has never before been possible except to museum curators. Curtis Wong, manager of Next Media Research for Microsoft, will reprise his immensely popular TED talk on the WorldWide Telescope. This powerful Webbased tool for exploring the universe compiles images from telescopes and satellites to build a comprehensive, interactive view of the cosmos. Our third invited demonstration is from the office of the Conseil Général de Saône-et-Loire in France. The objectives of their Opendata71.fr project is to transform the relationship between citizens and local government by increasing the understanding of the missions, actions, and roles both of the administration and the elected representatives. We would like to invite WWW2012 registrants to attend as many sessions in the Demo Track as your schedule allows. Many of the demo presenters have also consented to repeat their presentations and host faceto- face discussions in an area specifically designated in the exhibition hall. In a joint experiment with the IW3C2, some of the demonstrations will even be available in the virtual world Second Life.	Session details: Demonstrations	NA:NA:NA	2018
Fabian Abel:Claudia Hauff:Geert-Jan Houben:Richard Stronkman:Ke Tao	In this paper, we present Twitcident, a framework and Web-based system for filtering, searching and analyzing information about real-world incidents or crises. Twitcident connects to emergency broadcasting services and automatically starts tracking and filtering information from Social Web streams (Twitter) when a new incident occurs. It enriches the semantics of streamed Twitter messages to profile incidents and to continuously improve and adapt the information filtering to the current temporal context. Faceted search and analytical tools allow users to retrieve particular information fragments and overview and analyze the current situation as reported on the Social Web. Demo: http://wis.ewi.tudelft.nl/twitcident/	Twitcident: fighting fire with information from social web streams	NA:NA:NA:NA:NA	2018
Maurizio Atzori:Carlo Zaniolo	A novel method is demonstrated that allows semantic and well-structured knowledge bases (such as DBpedia) to be easily queried directly from Wikipedia's pages. Using Swipe, naive users with no knowledge of RDF triples and SPARQL can easily query DBpedia with powerful questions such as: "Who are the U.S. presidents who took office when they were 55-year old or younger, during the last 60 years", or "Find the town in California with less than 10 thousand people". This is accomplished by a novel Search by Example (SBE) approach where a user can enter the query conditions directly on the Infobox of a Wikipedia page. In fact, Swipe activates various fields of Wikipedia to allow users to enter query conditions, and then uses these conditions to generate equivalent SPARQL queries and execute them on DBpedia. Finally, Swipe returns the query results in a form that is conducive to query refinements and further explorations. Swipe's SBE approach makes semi-structured documents queryable in an intuitive and user-friendly way and, through Wikipedia, delivers the benefits of querying and exploring large knowledge bases to all Web users.	SWiPE: searching wikipedia by example	NA:NA	2018
Michael Benedikt:Tim Furche:Andreas Savvides:Pierre Senellart	An important feature of web search interfaces are the restrictions enforced on input values - those reflecting either the semantics of the data or requirements specific to the interface. Both integrity constraints and "access restrictions" can be of great use to web exploration tools. We demonstrate here a novel technique for discovering constraints that requires no form submissions whatsoever. We work via statically analyzing the JavaScript client-side code used to enforce the constraints, when such code is available. We combine custom recognizers for JavaScript functions relevant to constraint checking with a generic program analysis layer. Integrated with a web browser, our system shows the constraints detected on accessed web forms, and allows a user to see the corresponding JavaScript code fragment.	ProFoUnd: program-analysis-based form understanding	NA:NA:NA:NA	2018
Marco Bertini:Alberto Del Bimbo:Andrea Ferracani:Daniele Pezzatini	This paper presents a system for the social annotation and discovery of videos based on social networks and social knowledge. The system, developed as a web application, allows users to comment and annotate, manually and automatically, video frames and scenes enriching their content with tags, references to Facebook users and pages and Wikipedia resources. These annotations are used to semantically model the interests and the folksonomy of each user and resource in the network, and to suggest to users new resources, Facebook friends and videos whose content is related to their interests. A screencast showing an example of these functionalities is publicly available at:http://vimeo.com/miccunifi/facetube	A social network for video annotation and discovery based on semantic profiling	NA:NA:NA:NA	2018
Christoph Böhm:Markus Freitag:Arvid Heise:Claudia Lehmann:Andrina Mascher:Felix Naumann:Vuk Ercegovac:Mauricio Hernandez:Peter Haase:Michael Schmidt	Many government organizations publish a variety of data on the web to enable transparency, foster applications, and to satisfy legal obligations. Data content, format, structure, and quality vary widely, even in cases where the data is published using the wide-spread linked data principles. Yet within this data and their integration lies much value: We demonstrate GovWILD, a web-based prototype that integrates and cleanses Open Government Data at a large scale. Apart from the web-based interface that presents a use case of the created dataset at govwild.org, we provide all integrated data as a download. This data can be used to answer questions about politicians, companies, and government funding.	GovWILD: integrating open government data for transparency	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Elena Demidova:Xuan Zhou:Wolfgang Nejdl	Freebase is a large-scale open-world database where users collaboratively create and structure content over an open platform. Keyword queries over Freebase are notoriously ambiguous due to the size and the complexity of the dataset. To this end, novel techniques are required to enable naive users to express their informational needs and retrieve the desired data. FreeQ offers users an interactive interface for incremental query construction over a large-scale dataset, so that the users can find desired information quickly and accurately.	FreeQ: an interactive query interface for freebase	NA:NA:NA	2018
Yerach Doytsher:Ben Galon:Yaron Kanza	navigation systems, allow users to record their location history. The location history data can be analyzed to generate life patterns|patterns that associate people to places they frequently visit. Accordingly, an SSN is a graph that consists of (1) a social network, (2) a spatial network, and (3) life patterns that connect the users of the social network to locations, i.e., to geographical entities in the spatial network. In this paper we present a system that stores SNN in a graph-based database management system and provides a novel query language, namely SSNQL, for querying the integrated data. The system includes a Web-based graphical user interface that allows presenting the social network, presenting the spatial network and posing SSNQL queries over the integrated data. The user interface also depicts the structure of queries for the purpose of debugging and optimization. Our demonstration presents the management of the integrated data as an SSN and it illustrates the query evaluation process in SSNQL.	Querying socio-spatial networks on the world-wide web	NA:NA:NA	2018
Pavlos Fafalios:Ioannis Kitsos:Yannis Tzitzikas	The last years there is an increasing interest on providing the top search results while the user types a query letter by letter. In this paper we present and demonstrate a family of instant search applications which apart from showing instantly only the top search results, they can show various other kinds of precomputed aggregated information. This paradigm is more helpful for the end user (in comparison to the classic search-as-you-type), since it can combine autocompletion, search-as-you-type, results clustering, faceted search, entity mining, etc. Furthermore, apart from being helpful for the end user, it is also beneficial for the server's side. However, the instant provision of such services for large number of queries, big amounts of precomputed information, and large number of concurrent users is challenging. We demonstrate how this can be achieved using very modest hardware. Our approach relies on (a) a partitioned trie-based index that exploits the available main memory and disk, and (b) dedicated caching techniques. We report performance results over a server running on a modest personal computer (with 3 GB main memory) that provides instant services for millions of distinct queries and terabytes of precomputed information. Furthermore these services are tolerant to user typos and the word order.	Scalable, flexible and generic instant overview search	NA:NA:NA	2018
Roi Friedman:Itsik Hefez:Yaron Kanza:Roy Levin:Eliyahu Safra:Yehoshua Sagiv	Many smartphones, nowadays, use GPS to detect the location of the user, and can use the Internet to interact with remote location-based services. These two capabilities support online navigation that incorporates search. In this demo we presents WISER---a system for Web-based Interactive Search en Route. In the system, users perform route search by providing (1) a target location, and (2) search terms that specify types of geographic entities to be visited. The task is to find a route that minimizes the travel distance from the initial location of the user to the target, via entities of the specified types. However, planning a route under conditions of uncertainty requires the system to take into account the possibility that some visited entities will not satisfy the search requirements, so that the route may need to go via several entities of the same type. In an interactive search, the user provides feedback regarding her satisfaction with entities she visits during the travel, and the system changes the route, in real time, accordingly. The goal is to use the interaction for computing a route that is more effective than a route that is computed in a non-interactive fashion.	WISER: a web-based interactive route search system for smartphones	NA:NA:NA:NA:NA:NA	2018
Tim Furche:Giovanni Grasso:Giorgio Orsi:Christian Schallhart:Cheng Wang	Wrapper induction faces a dilemma: To reach web scale, it requires automatically generated examples, but to produce accurate results, these examples must have the quality of human annotations. We resolve this conflict with AMBER, a system for fully automated data extraction from result pages. In contrast to previous approaches, AMBER employs domain specific gazetteers to discern basic domain attributes on a page, and leverages repeated occurrences of similar attributes to group related attributes into records rather than relying on the noisy structure of the DOM. With this approach AMBER is able to identify records and their attributes with almost perfect accuracy (>98%) on a large sample of websites. To make such an approach feasible at scale, AMBER automatically learns domain gazetteers from a small seed set. In this demonstration, we show how AMBER uses the repeated structure of records on deep web result pages to learn such gazetteers. This is only possible with a highly accurate extraction system. Depending on its parametrization, this learning process runs either fully automatically or with human interaction. We show how AMBER bootstraps a gazetteer for UK locations in 4 iterations: From a small seed sample we achieve 94.4% accuracy in recognizing UK locations in the $4th$ iteration.	Automatically learning gazetteers from the deep web	NA:NA:NA:NA:NA	2018
Kavita Ganesan:ChengXiang Zhai	Traditional web search engines enable users to find documents based on topics. However, in finding entities such as restaurants, hotels and products, traditional search engines fail to suffice as users are often interested in finding entities based on structured attributes such as price and brand and unstructured information such as opinions of other web users. In this paper, we showcase a preference driven search system, that enables users to find entities of interest based on a set of structured preferences as well as unstructured opinion preferences. We demonstrate our system in the context of hotel search.	FindiLike: preference driven entity search	NA:NA	2018
Sedat Gokalp:Hasan Davulcu	US Senate is the venue of political debates where the federal bills are formed and voted. Senators show their support/opposition along the bills with their votes. This information makes it possible to extract the polarity of the senators. We use signed bipartite graphs for modeling debates, and we propose an algorithm for partitioning both the senators, and the bills comprising the debate into binary opposing camps. Simultaneously, our algorithm scales both the senators and the bills on a univariate scale. Using this scale, a researcher can identify moderate and partisan senators within each camp, and polarizing vs. unifying bills. We applied our algorithm on all the terms of the US Senate to the date for longitudinal analysis and developed a web based interactive user interface www.PartisanScale.com to visualize the analysis.	Partisan scale	NA:NA	2018
Xiaonan Guo:Jochen Kranzdorf:Tim Furche:Giovanni Grasso:Giorgio Orsi:Christian Schallhart	Web forms are the interfaces of the deep web. Though modern web browsers provide facilities to assist in form filling, this assistance is limited to prior form fillings or keyword matching. Automatic form understanding enables a broad range of applications, including crawlers, meta-search engines, and usability and accessibility support for enhanced web browsing. In this demonstration, we use a novel form understanding approach, OPAL, to assist in form filling even for complex, previously unknown forms. OPAL associates form labels to fields by analyzing structural properties in the HTML encoding and visual features of the page rendering. OPAL interprets this labeling and classifies the fields according to a given domain ontology. The combination of these two properties, allows OPAL to deal effectively with many forms outside of the grasp of existing form filling techniques. In the UK real estate domain, OPAL achieves >99% accuracy in form understanding.	OPAL: a passe-partout for web forms	NA:NA:NA:NA:NA:NA	2018
Mihály Héder:Pablo N. Mendes	We describe a tool kit to support a knowledge-enhancement cycle on the Web. In the first step, structured data which is extracted from Wikipedia is used to construct automatic content enhancement engines. Those engines can be used to interconnect knowledge in structured and unstructured information sources on the Web, including Wikipedia itself. Sztakipedia-toolbar is a MediaWiki user script which brings DBpedia Spotlight and other kinds of machine intelligence into the Wiki editor interface to provide enhancement suggestions to the user. The suggestions offered by the tool focus on complementing knowledge and increasing the availability of structured data on Wikipedia. This will, in turn, increase the available information for the content enhancement engines themselves, completing a virtuous cycle of knowledge enhancement.	Round-trip semantics with sztakipedia and DBpedia spotlight	NA:NA	2018
Muhammad Imran:Felix Kling:Stefano Soi:Florian Daniel:Fabio Casati:Maurizio Marchese	In this demonstration, we present ResEval Mash, a mashup platform for research evaluation, i.e., for the assessment of the productivity or quality of researchers, teams, institutions, journals, and the like - a topic most of us are acquainted with. The platform is specifically tailored to the need of sourcing data about scientific publications and researchers from the Web, aggregating them, computing metrics (also complex and ad-hoc ones), and visualizing them. ResEval Mash is a hosted mashup platform with a client-side editor and runtime engine, both running inside a common web browser. It supports the processing of also large amounts of data, a feature that is achieved via the sensible distribution of the respective computation steps over client and server. Our preliminary user study shows that ResEval Mash indeed has the power to enable domain experts to develop own mashups (research evaluation metrics); other mashup platforms rather support skilled developers. The reason for this success is ResEval Mash's domain-specificity.	ResEval mash: a mashup tool for advanced research evaluation	NA:NA:NA:NA:NA:NA	2018
Parag Mulendra Joshi:Claudio Bartolini:Sven Graupner	In this paper, we describe [email protected], a system designed for effortless and instantaneous sharing of enterprise knowledge through routine email communications and powerful harvesting of such enterprise information using text analytics techniques. [email protected] is a system that enables dynamic, non-intrusive and effortless sharing of information within an enterprise and automatically harvests knowledge from such daily interactions. It also allows enterprise knowledge workers to easily subscribe to new information. It enables self organization of information in conversations while it carefully avoids requiring users to substantially change their usual work-flow of exchanging emails. Incorporating this system in an enterprise improves productivity by: "discovery of connections between employees with converging interests and expertise, similar to social networks naturally leading to formation of interest groups, avoiding the problem of information lost in mountains of emails," creating expert profiles by mapping areas of expertise or interests to organizational map. Harvested information includes folksonomy appropriate to an organization, tagged and organized conversations and expertise map.	[email protected]: intuitive and effortless categorization and sharing of email conversations	NA:NA:NA	2018
Jochen Kranzdorf:Andrew Sellers:Giovanni Grasso:Christian Schallhart:Tim Furche	Good examples are hard to find, particularly in wrapper induction: Picking even one wrong example can spell disaster by yielding overgeneralized or overspecialized wrappers. Such wrappers extract data with low precision or recall, unless adjusted by human experts at significant cost. Visual OXPath is an open-source, visual wrapper induction system that requires minimal examples and eases wrapper refinement: Often it derives the intended wrapper from a single example through sophisticated heuristics that determine the best set of similar examples. To ease wrapper refinement, it offers a list of wrappers ranked by example similarity and robustness. Visual OXPath offers extensive visual feedback for this refinement which can be performed without any knowledge of the underlying wrapper language. Where further refinement by a human wrapper is needed, Visual OXPath profits from being based on OXPath, a declarative wrapper language that extends XPath with a thin layer of features necessary for extraction and page navigation.	Visual oXPath: robust wrapping by example	NA:NA:NA:NA:NA	2018
Thomas Kurz:Sebastian Schaffert:Georg Güntner:Manuel Fernandez	The Linked Data movement with the aims of publishing and interconnecting machine readable data has originated in the last decade. Although the set of (open) data sources is rapidly growing, the integration of multimedia in this Web of Data is still at a very early stage. This paper describes, how arbitrary video content and metadata can be processed to identify meaningful linking partners for video fragments - and thus create a web of linked media. The video test-set for our demonstrator is part of the Red Bull Content Pool and confined to the Cliff Diving domain. The candidate set of possible link targets is a combination of a Red Bull thesaurus, information about divers from www.redbull.com and concepts from DBPedia. The demo includes both a semantic search on videos and video fragments and a player for videos with semantic enhancements.	Adding wings to red bull media: search and display semantically enhanced video fragments	NA:NA:NA:NA	2018
Daniel Lacroix:Yves-Armel Martin	Kjing is a web app that allow to rapidly set a multiscreen multi-device environment and to interact and distribute content in realtime. It can be used for museographic, educational or conferencing purpose.	Kjing: (mix the knowledge)	NA:NA	2018
Luis A. Leiva:Roberto Vivó	Processing web interaction data is known to be cumbersome and time-consuming. State-of-the-art web tracking systems usually allow replaying user interactions in the form of mouse tracks, a video-like visualization scheme, to engage practitioners in the analysis process. However, traditional online video inspection has not explored the full capabilities of hypermedia and interactive techniques. In this paper, we introduce a web-based tracking tool that generates interactive visualizations from users' activity. The system unobtrusively collects browser events derived from normal usage, offering a unified framework to inspect interaction data in several ways. We compare our approach to related work in the research community as well as in commercial systems, and describe how ours fits in a real-world scenario. This research shows that there is a wide range of applications where the proposed tool can assist the WWW community.	Interactive hypervideo visualization for browsing behavior analysis	NA:NA	2018
Yabing Liu:Bimal Viswanath:Mainack Mondal:Krishna P. Gummadi:Alan Mislove	Online social networks like Facebook allow users to connect, communicate, and share content. The popularity of these services has lead to an information overload for their users; the task of simply keeping track of different interactions has become daunting. To reduce this burden, sites like Facebook allows the user to group friends into specific lists, known as friendlists, aggregating the interactions and content from all friends in each friendlist. While this approach greatly reduces the burden on the user, it still forces the user to create and populate the friendlists themselves and, worse, makes the user responsible for maintaining the membership of their friendlists over time. We show that friendlists often have a strong correspondence to the structure of the social network, implying that friendlists may be automatically inferred by leveraging the social network structure. We present a demonstration of Friendlist Manager, a Facebook application that proposes friendlists to the user based on the structure of their local social network, allows the user to tweak the proposed friendlists, and then automatically creates the friendlists for the user.	Simplifying friendlist management	NA:NA:NA:NA:NA	2018
Maurizio Montagnuolo:Alberto Messina	This paper presents a novel framework for multimodal news data aggregation, retrieval and browsing. News aggregations are contextualised within automatically extracted information such as entities (i.e. persons, places and organisations), temporal span, categorical topics, social networks popularity and audience scores. Further resources coming from professional repositories, and related to the aggregation topics, can be accessed as well. The system is accessible through a Web interface supporting interactive navigation and exploration of large-scale collections of news stories at the topic and context levels. Users can select news topics and sub-topics interactively, building their personal paths towards worldwide events, main characters, dates and contents.	The RaiNewsbook: browsing worldwide multimodal news stories by facts, entities and dates	NA:NA	2018
Roberto Navigli:Simone Paolo Ponzetto	Knowledge on word meanings and their relations across languages is vital for enabling semantic information technologies: in fact, the ever increasingly multilingual nature of the Web now calls for the development of methods that are both robust and widely applicable for processing textual information in a multitude of languages. In our research, we approach this ambitious task by means of BabelNet, a wide-coverage multilingual lexical knowledge base. In this paper we present an Application Programming Interface and a Graphical User Interface which, respectively, allow programmatic access and visual exploration of BabelNet. Our contribution is to provide the research community with easy-to-use tools for performing multilingual lexical semantic analysis, thereby fostering further research in this direction.	BabelNetXplorer: a platform for multilingual lexical knowledge base access and exploration	NA:NA	2018
Nikolaos Papailiou:Ioannis Konstantinou:Dimitrios Tsoumakos:Nectarios Koziris	In this work we present H2RDF, a fully distributed RDF store that combines the MapReduce processing framework with a NoSQL distributed data store. Our system features two unique characteristics that enable efficient processing of both simple and multi-join SPARQL queries on virtually unlimited number of triples: Join algorithms that execute joins according to query selectivity to reduce processing; and adaptive choice among centralized and distributed (MapReduce-based) join execution for fast query responses. Our system efficiently answers both simple joins and complex multivariate queries and easily scales to 3 billion triples using a small cluster of 9 worker nodes. H2RDF outperforms state-of-the-art distributed solutions in multi-join and nonselective queries while achieving comparable performance to centralized solutions in selective queries. In this demonstration we showcase the system's functionality through an interactive GUI. Users will be able to execute predefined or custom-made SPARQL queries on datasets of different sizes, using different join algorithms. Moreover, they can repeat all queries utilizing a different number of cluster resources. Using real-time cluster monitoring and detailed statistics, participants will be able to understand the advantages of different execution schemes versus the input data as well as the scalability properties of H2RDF over both the data size and the available worker resources.	H2RDF: adaptive query processing on RDF data in the cloud.	NA:NA:NA:NA	2018
Antonio Pintus:Davide Carboni:Andrea Piras	The Web of Things is a scenario where potentially billions of connected smart objects communicate using the Web protocols, HTTP in primis. A Web of Things envisioning and design has raised several research issues, from protocols adoption and communication models to architectural styles and social aspects facing. In this demo we present the prototype of a scalable architecture for a large scale social Web of Things for smart objects and services, named Paraimpu. It is a Web-based platform which allows to add, use, share and inter-connect real HTTP-enabled smart objects and "virtual" things like services on the Web and social networks. Paraimpu defines and uses few strong abstractions, in order to allow mash-ups of heterogeneous things introducing powerful rules for data adaptation. Adding and inter-connecting objects is supported through user friendly models and features.	Paraimpu: a platform for a social web of things	NA:NA:NA	2018
Yves Raimond:Chris Lowis:Roderick Hodgson:Jonathan Tweed	The BBC is currently tagging programmes manually, using DBpedia as a source of tag identifiers, and a list of suggested tags extracted from the programme synopsis. These tags are then used to help navigation and topic-based search of programmes on the BBC website. However, given the very large number of programmes available in the archive, most of them having very little metadata attached to them, we need a way to automatically assign tags to programmes. We describe a framework to do so, using speech recognition, text processing and concept tagging techniques. We describe how this framework was successfully applied to a very large BBC radio archive. We demonstrate an application using automatically extracted tags to aid discovery of archive content.	Automated semantic tagging of speech audio	NA:NA:NA:NA	2018
Soudip Roy Chowdhury:Carlos Rodríguez:Florian Daniel:Fabio Casati	In this demonstration, we describe Baya, an extension of Yahoo! Pipes that guides and speeds up development by interactively recommending composition knowledge harvested from a repository of existing pipes. Composition knowledge is delivered in the form of reusable mashup patterns, which are retrieved and ranked on the fly while the developer models his own pipe (the mashup) and that are automatically weaved into his pipe model upon selection. Baya mines candidate patterns from pipe models available online and thereby leverages on the knowledge of the crowd, i.e., of other developers. Baya is an extension for the Firefox browser that seamlessly integrates with Pipes. It enhances Pipes with a powerful new feature for both expert developers and beginners, speeding up the former and enabling the latter. The discovery of composition knowledge is provided as a service and can easily be extended toward other modeling environments.	Baya: assisted mashup development as a service	NA:NA:NA:NA	2018
Eric Rozell:Peter Fox:Jin Zheng:Jim Hendler	This demo paper will discuss a search interface framework designed as part of the Semantic eScience Framework project at the Tetherless World Constellation. The search interface framework, S2S, was designed to facilitate the construction of interactive user interfaces for data catalogs. We use Semantic Web technologies, including an OWL ontology for describing the semantics of data services, as well as the semantics of user interface components. We have applied S2S in three different scenarios: (1) the development of a faceted browse interface integrated with an interactive mapping and visualization tool for biological and chemical oceanographic data, (2) the development of a faceted browser for more than 700,000 open government datasets in over 100 catalogs worldwide, and (3) the development of a user interface for a virtual observatory in the field of solar-terrestrial physics. Throughout this paper, we discuss the architecture of the S2S framework, focusing on its extensibility and reusability, and also review the application scenarios.	S2S architecture and faceted browsing applications	NA:NA:NA:NA	2018
Henry Story:Romain Blin:Julien Subercaze:Christophe Gravier:Pierre Maret	This demonstration presents the process of transforming a Web 2.0 centralized social network into a Web 3.0, distributed, and secured Social application, and what was learnt in this process. The initial Web 2.0 Social Network application was written by a group of students over a period of 4 months in the spring of 2011. It had all the bells and whistles of the well known Social Networks: walls to post on, circles of friends, etc. The students were very enthusiastic in building their social network, but the chances of it growing into a large community were close to non-existent unless a way could be found to tie it into a bigger social network. This is where linked data protected by the Web Access Control Ontology and WebID authentication could come to the rescue. The paper describes this transformation process, and we will demonstrate the full software version at the conference.	Turning a Web 2.0 social network into a Web 3.0, distributed, and secured social web application	NA:NA:NA:NA:NA	2018
Fabian M. Suchanek:David Gross-Amblard	In this paper, we study how artificial facts can be added to an RDFS ontology. Artificial facts are an easy way of proving the ownership of an ontology: If another ontology contains the artificial fact, it has probably been taken from the original ontology. We show how the ownership of an ontology can be established with provably tight probability bounds, even if only parts of the ontology are being re-used. We explain how artificial facts can be generated in an inconspicuous and minimally disruptive way. Our demo allows users to generate artificial facts and to guess which facts were generated.	Adding fake facts to ontologies	NA:NA	2018
Ba Quan Truong:Aixin Sun:Sourav S. Bhowmick	Tag-based social image search enables users to formulate queries using keywords. However, as queries are usually very short and users have very different interpretations of a particular tag in annotating and searching images, the returned images to a tag query usually contain a collection of images related to multiple concepts. We demonstrate Casis, a system for concept-aware social image search. Casis detects tag concepts based on the collective knowledge embedded in social tagging from the initial results to a query. A tag concept is a set of tags highly associated with each other and collectively conveys a semantic meaning. Images to a query are then organized by tag concepts. Casis provides intuitive and interactive browsing of search results through a tag concept graph, which visualizes the tags defining each tag concept and their relationships within and across concepts. Supporting multiple retrieval methods and multiple concept detection algorithms, Casis offers superior social image search experiences by choosing the most suitable retrieval methods and concept-aware image organizations.	CASIS: a system for concept-aware social image search	NA:NA:NA	2018
Tony Veale:Yanfen Hao	Models of sentiment analysis in text require an understanding of what kinds of sentiment-bearing language are generally used to describe specific topics. Thus, fine-grained sentiment analysis requires both a topic lexicon and a sentiment lexicon, and an affective mapping between both. For instance, when one speaks disparagingly about a city (like London, say), what aspects of city does one generally focus on, and what words are used to disparage those aspects? As when we talk about the weather, our language obeys certain familiar patterns - what we might call clichés and stereotypes - when we talk about familiar topics. In this paper we describe the construction of an affective stereotype lexicon, that is, a lexicon of stereotypes and their most salient affective qualities. We show, via a demonstration system called MOODfinger, how this lexicon can be used to underpin the processes of affective query expansion and summarization in a system for retrieving and organizing news content from the Web. Though we adopt a simple bipolar +/- view of sentiment, we show how this stereotype lexicon allows users to coin their own nuanced moods on demand.	In the mood for affective search with web stereotypes	NA:NA	2018
Luca Vignaroli:Roberto Del Pero:Fulvio Negro	The way we watch television is changing with the introduction of attractive Web activities that move users away from TV to other media. The integration of the cultures of TV and Web is still an open issue. How can we make TV more open? How can we enable a possible collaboration of these two different worlds? TV-Web convergence is much more than placing a Web browser into a TV set or putting TV content into a Web media player. The NoTube project, funded by the European Community, is demonstrating how an open and general set of tools adaptable to a number of possible scenarios and allowing a designer to implement the targeted final service with ease can be introduced. A prototype based on the NoTube model in which the Smartphone is used as secondary screen is presented. The video demonstration [11] is available at http://youtu.be/dMM7MH9CZY8.	Personalized newscasts and social networks: a prototype built over a flexible integration model	NA:NA:NA	2018
Hao Wu:Hui Fang:Steven J. Stanhope	Drugs can treat human diseases through chemical interactions between the ingredients and intended targets in the human body. However, the ingredients could unexpectedly interact with off-targets, which may cause adverse drug side effects. Notifying patients and physicians of potential drug effects is an important step in improving healthcare quality and delivery. With the increasing popularity of Web 2.0 applications, more and more patients start discussing drug side effects in many online sources. In this paper, we describe our efforts on building UDWarning, a novel early warning system for unrecognized drug side effects discovery based on the text information gathered from the Internet. The system can automatically build a knowledge base for drug side effects by integrating the information related to drug side effects from different sources. It can also monitor the online information about drugs and discover possible unrecognized drug side effects. Our demonstration will show that the system has the potentials to expedite the discovery process of unrecognized drug side effects and to improve the quality of healthcare.	An early warning system for unrecognized drug side effects discovery	NA:NA:NA	2018
Jian Wu:Liang Chen:Yanan Xie:Zibin Zheng	With the increase of web services and user demand's diversity, effective web service discovery is becoming a big challenge. Clustering web services would greatly boost the ability of web service search engine to retrieve relevant ones. In this paper, we propose a web service search engine Titan which contains 15,969 web services crawled from the Internet. In Titan, two main technologies, i.e., web service clustering and tag recommendation, are employed to improve the effectiveness of web service discovery. Specifically, both WSDL (Web Service Description Language) documents and tags of web services are utilized for clustering, while tag recommendation is adopted to handle some inherent problems of tagging data, e.g., uneven tag distribution and noise tags.	Titan: a system for effective web service discovery	NA:NA:NA:NA	2018
Mohamed Yahya:Klaus Berberich:Shady Elbassuoni:Maya Ramanath:Volker Tresp:Gerhard Weikum	We present DEANNA, a framework for natural language question answering over structured knowledge bases. Given a natural language question, DEANNA translates questions into a structured SPARQL query that can be evaluated over knowledge bases such as Yago, Dbpedia, Freebase, or other Linked Data sources. DEANNA analyzes questions and maps verbal phrases to relations and noun phrases to either individual entities or semantic classes. Importantly, it judiciously generates variables for target entities or classes to express joins between multiple triple patterns. We leverage the semantic type system for entities and use constraints in jointly mapping the constituents of the question to relations, classes, and entities. We demonstrate the capabilities and interface of DEANNA, which allows advanced users to influence the translation process and to see how the different components interact to produce the final result.	Deep answers for naturally asked questions on the web of data	NA:NA:NA:NA:NA:NA	2018
Lionel Médini:Yi Chang	We are very pleased to welcome the 100 posters of this WWW 2012 poster track. We wish to thank all Program Committee members for the quality (and the quantity) of their work. And naturally, we also congratulate authors of the selected contributions that reflect the variety of the WWW series conferences: all posters present high quality work in various domains, poster authors come from various countries and represent both academic and industrial research teams. The poster track provides an opportunity for authors and conference attendees to interact and exchange about innovative theoretical and technical work in progress around, about and on the Web. In addition to being displayed during three whole days from 4/18 to 4/20, WWW'2012 posters will also be shortly presented on Thursday 4/19 PM. Seven "spotlight events" are organized to give conference attendees a 5-minute glimpse on each poster content, in the following areas: 1 - Web search / Web mining: WWW contains rich data, yet how to effectively use such rich data is challenging. Web search is mainly about searching information with a proper query, while web mining has a boarder scope of knowledge exaction from the Web. 2 - Natural Language Processing / Information Search and Retrieval: NLP and IR are two major foundations of Web search. With more and more applications on the Web, the boundary between NLP and IR is getting smaller. How to efficiently and effectively handle large scale online data is a challenge for NLP and IR communities. 3 - Web engineering / Performance / Security / Privacy: this session copes with the everyday challenges of Web development: how to improve contents and applications design processes, how to serve them efficiently and securely to users and how to face privacy issues. 4 - Recommender Systems / Semantic Web: Recommender Systems allow Web communities access the best of users' opinions. Semantic Web and Linked Data rely on communities' data to harvest and process complex information. This session presents posters that cope with graph algorithms and semantic/social data. 5 - Social Web / Human Factors / Accessibility: this session deals with human-oriented topics. The presented posters mainly concern analysis and prediction of social behavior and influence, as well as of individual behavior, affective computing and Web accessibility. 6 - Microblogging / Social media / Catch up TV: social media and microblogging provide rich online information with a different perspective from traditional static Web page data. How to effectively search, mine and utilize such valuable information is a very stimulating research area. 7 - Monetization: posters in this session explore efficient ways to spread ads on the Web or analyze the efficiency of existing algorithms on actual e-commerce activity. This year's edition aims at "augmenting posters" in several ways: poster views are also available in virtual worlds, and attendees can access poster metadata and additional information and even chat with authors using their smartphones and tabs. Numerous efforts have been made to encourage discussions about posters for the largest audience, inside and outside the conference. So come, see and participate!	Session details: Poster presentations	NA:NA	2018
Rakesh Agrawal:Ariel Fuxman:Anitha Kannan:John Shafer:Partha Pratim Talukdar	Postulate two independently created data sources. The first contains text documents, each discussing one or a small number of objects. The second is a collection of structured records, each containing information about the characteristics of some objects. We present techniques for associating structured records to corresponding text documents and empirical results supporting the proposed techniques.	Associating structured records to text documents	NA:NA:NA:NA:NA	2018
Fotis Aisopos:George Papadakis:Konstantinos Tserpes:Theodora Varvarigou	Microblog content poses serious challenges to the applicability of sentiment analysis, due to its inherent characteristics. We introduce a novel method relying on content-based and context-based features, guaranteeing high effectiveness and robustness in the settings we are considering. The evaluation of our methods over a large Twitter data set indicates significant improvements over the traditional techniques.	Textual and contextual patterns for sentiment analysis over microblogs	NA:NA:NA:NA	2018
H. Asthana:Ingemar J. Cox	We describe a framework for a micro-blogging social network implemented in an unstructured peer-to-peer network. A micro-blogging social network must provide capabilities for users to (i) publish, (ii) follow and (iii) search. Our retrieval mechanism is based on a probably approximately correct (PAC) search architecture in which a query is sent to a fixed number of nodes in the network. In PAC, the probability of attaining a particular accuracy is a function of the number of nodes queried (fixed) and the replication rate of documents (micro-blog). Publishing a micro-blog then becomes a matter of replicating the micro-blog to the required number of random nodes without any central coordination. To solve this, we use techniques from the field of rumour spreading (gossip protocols) to propagate new documents. Our document spreading algorithm is designed such that a document has a very high probability of being copied to only the required number of nodes. Results from simulations performed on networks of 10,000, 100,000 and 500,000 nodes verify our mathematical models. The framework is also applicable for indexing dynamic web pages in a distributed search engine or for a system which indexes newly created BitTorrents in a decentralized environment.	PAC'nPost: a framework for a micro-blogging social network in an unstructured P2P network	NA:NA	2018
Javad Azimi:Ruofei Zhang:Yang Zhou:Vidhya Navalpakkam:Jianchang Mao:Xiaoli Fern	Display advertising has been a significant source of revenue for publishers and ad networks in the online advertising ecosystem. One of the main goals in display advertising is to maximize user response rate for advertising campaigns, such as click through rates (CTR) or conversion rates. Although %in the online advertising industry we believe that the visual appearance of ads (creatives) matters for propensity of user response, there is no published work so far to address this topic via a systematic data-driven approach. In this paper we quantitatively study the relationship between the visual appearance and performance of creatives using large scale data in the world's largest display ads exchange system, RightMedia. We designed a set of 43 visual features, some of which are novel and some are inspired by related work. We extracted these features from real creatives served on RightMedia. Then, we present recommendations of visual features that have the most important impact on CTR to the professional designers in order to optimize their creative design. We believe that the findings presented in this paper will be very useful for the online advertising industry in designing high-performance creatives. We have also designed and conducted an experiment to evaluate the effectiveness of visual features by themselves for CTR prediction.	The impact of visual appearance on user response in online display advertising	NA:NA:NA:NA:NA:NA	2018
Joel Barajas:Ram Akella:Marius Holtan:Jaimie Kwon:Aaron Flores:Victor Andrei	We develop a descriptive method to estimate the impact of ad impressions on commercial actions dynamically without tracking cookies. We analyze 2,885 campaigns for 1,251 products from the Advertising.com ad network. We compare our method with A/B testing for 2 campaigns, and with a public synthetic dataset.	Impact of ad impressions on dynamic commercial actions: value attribution in marketing campaigns	NA:NA:NA:NA:NA:NA	2018
Thomas Beauvisage:Jean-Samuel Beuscart	This paper studies the demand for TV contents on online catch up platforms, in order to assess how catch up TV offers transform TV consumption. We build upon empirical data on French TV consumption in June 2011: a daily monitoring of online audience on web catch up platforms, and live audience ratings of traditional broadcast TV. We provide three main results: 1) online consumption is more concentrated than off-line audience, contradicting the hypothesis of a long tail effect of catch up TV; 2) the temporality of replay TV consumption on the web is very close to the live broadcasting of the programs, thus softening rather than breaking the synchrony of traditional TV; 3) detailed data on online consumption of news reveals two patterns of consumption ("alternative TV ritual" vs. "à la carte").	Audience dynamics of online catch up TV	NA:NA	2018
José Bento:Stratis Ioannidis:S. Muthukrishnan:Jinyun Yan	We study recommendations for persistent groups that repeatedly engage in a joint activity. We approach this as a multi-arm bandit problem. We design a recommendation policy and show it has logarithmic regret. Our analysis also shows that regret depends linearly on d, the size of the underlying persistent group. We evaluate our policy on movie recommendations over the MovieLens and MoviePilot datasets.	Group recommendations via multi-armed bandits	NA:NA:NA:NA	2018
Marco Brambilla:Sofia Ceppi:Nicola Gatti:Enrico H. Gerding	Federated search engines combine search results from two or more (general--purpose or domain--specific) content providers. They enable complex searches (e.g., complete vacation planning) or more reliable results by allowing users to receive high quality results from a variety of sources. We propose a new revenue sharing mechanism for federated search engines, considering different actors involved in the search results generation (i.e., content providers, advertising providers, hybrid content+advertising providers, and content integrators). We extend the existing sponsored search auctions by supporting heterogeneous participants and redistribution of monetary values to the different actors, while maintaining flexibility in the payment scheme.	A revenue sharing mechanism for federated search and advertising	NA:NA:NA:NA	2018
Matthias Broecheler:Andrea Pugliese:V.S. Subrahmanian	The Social Semantic Web (SSW) refers to the mix of RDF data in web content, and social network data associated with those who posted that content. Applications to monitor the SSW are becoming increasingly popular. For instance, marketers want to look for semantic patterns relating to the content of tweets and Facebook posts relating to their products. Such applications allow multiple users to specify patterns of interest, and monitor them in real-time as new data gets added to the web or to a social network. In this paper, we develop the concept of SSW view servers in which all of these types of applications can be simultaneously monitored from such servers. The patterns of interest are views. We show that a given set of views can be compiled in multiple possible ways to take advantage of common substructures, and define the concept of an optimal merge. We develop a very fast MultiView algorithm that scalably and efficiently maintains multiple subgraph views. We show that our algorithm is correct, study its complexity, and experimentally demonstrate that our algorithm can scalably handle updates to hundreds of views on real-world SSW databases with up to 540M edges.	Efficient multi-view maintenance in the social semantic web	NA:NA:NA	2018
Liangliang Cao:John R. Smith:Zhen Wen:Zhijun Yin:Xin Jin:Jiawei Han	This paper describes a system to estimate geographical locations for beach photos. We develop an iterative method that not only trains visual classifiers but also discovers geographical clusters for beach regions. The results show that it is possible to recognize different beaches using visual information with reasonable accuracy, and our system works 27 times better than random guess for the geographical localization task.	BlueFinder: estimate where a beach photo was taken	NA:NA:NA:NA:NA:NA	2018
Xuezhi Cao:Kailong Chen:Rui Long:Guoqing Zheng:Yong Yu	Microblogging websites such as Twitter and Chinese Sina Weibo contain large amounts of microblogs posted by users. Many of these microblogs are highly sensitive to the important real-world events and correlated to the news events. Thus, microblogs from these websites can be collected as comments for the news to reveal the opinions and attitude towards the news event among large number of users. In this paper, we present a framework to automatically collect relevant microblogs from microblogging websites to generate comments for popular news on news websites.	News comments generation via mining microblogs	NA:NA:NA:NA:NA	2018
Cinzia Cappiello:Maristella Matera:Matteo Picozzi:Alessandro Caio:Mariano Tomas Guevara	The adoption of adequate tools, oriented towards the End User Development (EUD), can promote mobile mashups as "democratic" tools, able to accommodate the long tail of users' specific needs. We introduce MobiMash, a novel approach and a platform for the construction of mobile mashups, characterized by a lightweight composition paradigm, mainly guided by the notion of visual templates. The composition paradigm generates an application schema that is based on a domain specific language addressing dimensions for data integration and service orchestration, and that guides at run- time the dynamic instantiation of the final mobile app.	MobiMash: end user development for mobile mashups	NA:NA:NA:NA:NA	2018
Gorrell P. Cheek:Mohamed Shehab	We introduce a privacy management approach that leverages users' memory and opinion of their friends to set policies for other similar friends. We refer to this new approach as Same-As Privacy Management. To demonstrate the effectiveness of our privacy management improvements, we implemented a prototype Facebook application and conducted an extensive user study. We demonstrated considerable reductions in policy authoring time using Same-As Privacy Management over traditional group based privacy management approaches. Finally, we presented user perceptions, which were very encouraging.	Privacy management for online social networks	NA:NA	2018
Ye Chen:Pavel Berkhin:Jie Li:Sharon Wan:Tak W. Yan	We study the problem of estimating the value of a contextual ad impression, and based upon which an ad network bids on an exchange. The ad impression opportunity would materialize into revenue only if the ad network wins the impression and a user clicks on the ads, both as a rare event especially in an open exchange for contextual ads. Given a low revenue expectation and the elusive nature of predicting weak-signal click-through rates, the computational cost incurred by bid estimation shall be cautiously justified. We developed and deployed a novel impression valuation model, which is expected to reduce the computational cost by 95% and hence more than double the profit. Our approach is highly economized through a fast implementation of kNN regression that primarily leverages low-dimensional sell-side data (user and publisher). We also address the cold-start problem or the exploration vs. exploitation requirement by Bayesian smoothing using a beta prior, and adapt to the temporal dynamics using an autoregressive model.	Fast and cost-efficient bid estimation for contextual ads	NA:NA:NA:NA:NA	2018
Ye Chen:Mitali Gupta:Tak W. Yan	We describe a fast query evaluation method for ad document retrieval in online advertising, based upon the classic WAND algorithm. The key idea is to localize per-topic term upper bounds into homogeneous ad groups. Our approach is not only theoretically motivated by a topical mixture model; but empirically justified by the characteristics of the ad domain, that is, short and semantically focused documents with natural hierarchy. We report experimental results using artificial and real-world query-ad retrieval data, and show that the tighter-bound WAND outperforms the traditional approach by 35.4% reduction in number of full evaluations.	Fast query evaluation for ad retrieval	NA:NA:NA	2018
Jaehoon Choi:Donghyeon Kim:Seongsoon Kim:Junkyu Lee:Sangrak Lim:Sunwon Lee:Jaewoo Kang	Search engines have become an important decision making tool today. Decision making queries are often subjective, such as 'best sedan for family use,' 'best action movies in 2010,' to name a few. Unfortunately, such queries cannot be answered properly by conventional search systems. In order to address this problem, we introduce Consento, a consensus search engine designed to answer subjective queries. Consento performs subdocument-level indexing to more precisely capture semantics from user opinions. We also introduce a new ranking method, or ConsensusRank that counts in online comments referring to an entity as a weighted vote to the entity. We validated the framework with an empirical study using the data on movie reviews.	CONSENTO: a consensus search engine for answering subjective queries	NA:NA:NA:NA:NA:NA:NA	2018
Aleksandr Chuklin:Pavel Serdyukov	It is often considered that high abandonment rate corresponds to poor IR system performance. However several studies suggested that there are so called good abandonments, i.e. situations when search engine result page (SERP) contains enough details to satisfy the user information need without necessity to click on search results. In those papers only editorial metrics of SERP were used, and one cannot be sure that situations marked as good abandonments by assessors actually imply user satisfaction. In present work we propose some real-world evidences for good abandonments by calculating correlation between editorial and click metrics.	Good abandonments in factoid queries	NA:NA	2018
Aleksandr Chuklin:Pavel Serdyukov	Abandonment rate is one of the most broadly used online user satisfaction metrics. In this paper we discuss the notion of potential good abandonment, i.e. queries that may potentially result in user satisfaction without the need to click on search results (if search engine result page contains enough details to satisfy the user information need). We show, that we can train a classifier which is able to distinguish between potential good and bad abandonments with rather good results compared to our baseline. As a case study we show how to apply these ideas to IR evaluation and introduce a new metric for A/B-testing -- Bad Abandonment Rate.	Potential good abandonment prediction	NA:NA	2018
Luca Costabello:Serena Villata:Nicolas Delaforge:Fabien Gandon	We present and evaluate a context-aware access control framework for SPARQL endpoints queried from mobile.	Ubiquitous access control for SPARQL endpoints: lessons learned and future challenges	NA:NA:NA:NA	2018
Ovidiu Dan:Pavel Dmitriev:Ryen W. White	Search engines record a large amount of metadata each time a user issues a query. While efficiently mining this data can be challeng-ing, the results can be useful in multiple ways, including monitoring search engine performance, improving search relevance, prioritizing research, and optimizing day-to-day operations. In this poster, we describe an approach for mining query log data for actionable insights - specific query segments (sets of queries) that require attention, and actions that need to be taken to improve the segments. Starting with a set of important metrics, we identify query segments that are "interesting" with respect to these metrics using a distributed frequent itemset mining algorithm.	Mining for insights in the search engine query stream	NA:NA:NA	2018
Florian Daniel:Muhammad Imran:Felix Kling:Stefano Soi:Fabio Casati:Maurizio Marchese	The recent emergence of mashup tools has refueled research on end user development, i.e., on enabling end users without programming skills to compose own applications. Yet, similar to what happened with analogous promises in web service composition and business process management, research has mostly focused on technology and, as a consequence, has failed its objective. Plain technology (e.g., SOAP/WSDL web services) or simple modeling languages (e.g., Yahoo! Pipes) don't convey enough meaning to non-programmers. We propose a domain-specific approach to mashups that "speaks the language of the user", i.e., that is aware of the terminology, concepts, rules, and conventions (the domain) the user is comfortable with. We show what developing a domain-specific mashup tool means, which role the mashup meta-model and the domain model play and how these can be merged into a domain-specific mashup meta-model. We apply the approach implementing a mashup tool for the research evaluation domain. Our user study confirms that domain-specific mashup tools indeed lower the entry barrier to mashup development.	Developing domain-specific mashup tools for end users	NA:NA:NA:NA:NA:NA	2018
Florian Daniel:Carlos Rodriguez:Soudip Roy Chowdhury:Hamid R. Motahari Nezhad:Fabio Casati	Despite the emergence of mashup tools like Yahoo! Pipes or JackBe Presto Wires, developing mashups is still non-trivial and requires intimate knowledge about the functionality of web APIs and services, their interfaces, parameter settings, data mappings, and so on. We aim to assist the mashup process and to turn it into an interactive co-creation process, in which one part of the solution comes from the developer and the other part from reusable composition knowledge that has proven successful in the past. We harvest composition knowledge from a repository of existing mashup models by mining a set of reusable composition patterns, which we then use to interactively provide composition recommendations to developers while they model their own mashup. Upon acceptance of a recommendation, the purposeful design of the respective pattern types allows us to automatically weave the chosen pattern into a partial mashup model, in practice performing a set of modeling actions on behalf of the developer. The experimental evaluation of our prototype implementation demonstrates that it is indeed possible to harvest meaningful, reusable knowledge from existing mashups, and that even complex recommendations can be efficiently queried and weaved also inside the client browser.	Discovery and reuse of composition knowledge for assisted mashup development	NA:NA:NA:NA:NA	2018
Ernesto Diaz-Aviles:Avaré Stewart:Edward Velasco:Kerstin Denecke:Wolfgang Nejdl	In the presence of sudden outbreaks, how can social media streams be used to strengthen surveillance capabilities? In May 2011, Germany reported one of the largest described outbreaks of Enterohemorrhagic Escherichia coli (EHEC). By end of June, 47 persons had died. After the detection of the outbreak, authorities investigating the cause and the impact in the population were interested in the analysis of micro-blog data related to the event. Since Thousands of tweets related to this outbreak were produced every day, this task was overwhelming for experts participating in the investigation. In this work, we propose a Personalized Tweet Ranking algorithm for Epidemic Intelligence (PTR4EI), that provides users a personalized, short list of tweets based on the user's context. PTR4EI is based on a learning to rank framework and exploits as features, complementary context information extracted from the social hash-tagging behavior in Twitter. Our experimental evaluation on a dataset, collected in real-time during the EHEC outbreak, shows the superior ranking performance of PTR4EI. We believe our work can serve as a building block for an open early warning system based on Twitter, helping to realize the vision of Epidemic Intelligence for the Crowd, by the Crowd.	Towards personalized learning to rank for epidemic intelligence based on social media streams	NA:NA:NA:NA:NA	2018
Vadim Eisenberg:Yaron Kanza	D2RQ is a popular RDB-to-RDF mapping platform that supports mapping relational databases to RDF and posing SPARQL queries to these relational databases. However, D2RQ merely provides a read-only RDF view on relational databases. Thus, we introduce D2RQ/Update---an extension of D2RQ to enable executing SPARQL/Update statements on the mapped data, and to facilitate the creation of a read-write Semantic Web.	D2RQ/update: updating relational data via virtual RDF	NA:NA	2018
Wei Feng:Jianyong Wang	A social tagging system provides users an effective way to collaboratively annotate and organize items with their own tags. A social tagging system contains heterogenous information like users' tagging behaviors, social networks, tag semantics and item profiles. All the heterogenous information helps alleviate the cold start problem due to data sparsity. In this paper, we model a social tagging system as a multi-type graph and propose a graph-based ranking algorithm called HeterRank for tag recommendation. Experimental results on three publicly available datasets, i.e., CiteULike, Last.fm and Delicious prove the effectiveness of HeterRank for tag recommendation with heterogenous information.	HeterRank: addressing information heterogeneity for personalized recommendation in social tagging systems	NA:NA	2018
Ankur Gandhe:Dinesh Raghu:Rose Catherine	Answer extraction from discussion boards is an extensively studied problem. Most of the existing work is focused on supervised methods for extracting answers using similarity features and forum-specific features. Although this works well for the domain or forum data that it has been trained on, it is difficult to use the same models for a domain where the vocabulary is different and some forum specific features may not be available. In this poster, we report initial results of a domain adaptive answer extractor that performs the extraction in two steps: a) an answer recognizer identifies the sentences in a post which are likely to be answers, and b) a domain relevance module determines the domain significance of the identified answer. We use domain independent methodology that can be easily adapted to any given domain with minimum effort.	Domain adaptive answer extraction for discussion boards	NA:NA:NA	2018
Kahina Gani:Hakim Hacid:Ryan Skraba	In this paper we discuss a piece of work which intends to provide some insights regarding the resolution of the hard problem of multiple identities detection. Based on hypothesis that each person is unique and identifiable whether in its writing style or social behavior, we propose a Framework relying on machine learning models and a deep analysis of social interactions, towards such detection.	Towards multiple identity detection in social networks	NA:NA:NA	2018
Heng Gao:Qiudan Li:Hongyun Bao:Shuangyong Song	In micro-blogging, people talk about their daily life and change minds freely, thus by mining people's interest in micro-blogging, we will easily perceive the pulse of society. In this paper, we catch what people are caring about in their daily life by discovering meaningful communities based on probabilistic factor model (PFM). The proposed solution identifies people's interest from their friendship and content information. Therefore, it reveals the behaviors of people in micro-blogging naturally. Experimental results verify the effectiveness of the proposed model and show people's social life vividly.	How shall we catch people's concerns in micro-blogging?	NA:NA:NA:NA	2018
Sheng Gao:Ludovic Denoyer:Patrick Gallinari	In this paper we address the problem of link prediction in networked data, which appears in many applications such as social network analysis or recommender systems. Previous studies either consider latent feature based models but disregarding local structure in the network, or focus exclusively on capturing local structure of objects based on latent blockmodels without coupling with latent characteristics of objects. To combine the benefits of previous work, we propose a novel model that can incorporate the effects of latent features of objects and local structure in the network simultaneously. To achieve this, we model the relation graph as a function of both latent feature factors and latent cluster memberships of objects to collectively discover globally predictive intrinsic properties of objects and capture latent block structure in the network to improve prediction performance. Extensive experiments on several real world datasets suggest that our proposed model outperforms the other state of the art approaches for link prediction.	Link prediction via latent factor BlockModel	NA:NA:NA	2018
Giovanni Gardelli:Ingmar Weber	We use Yahoo! Toolbar data to gain insights into why people use Q&A sites. We look at questions asked on Yahoo! Answers and analyze both the pre-question behavior of users as well as their general online behavior. Our results indicate that there is a one-dimensional spectrum of users ranging from "social users" to "informational users" and that web search and Q&A sites complement each other, rather than compete. Concerning the pre-question behavior, users who first issue a question-related query are more likely to issue informational questions, rather than conversational ones, and such questions are less likely to attract an answer. Finally, we only find weak evidence for topical congruence between a user's questions and his web queries.	Using toolbar data to understand Yahoo!: answers usage	NA:NA	2018
Wolfgang Gassler:Eva Zangerle:Martin Bürgler:Günther Specht	Current mass-collaboration platforms use tags to annotate and categorize resources enabling eective search capabilities. However, as tags are freely chosen keywords, the resulting tag vocabulary is very heterogeneous. Another shortcoming of simple tags is that they do not allow for a specification of context to create meaningful metadata. In this paper we present the SnoopyTagging approach which supports the user in the process of creating contextualized tags while at the same time decreasing the heterogeneity of the tag vocabulary by facilitating intelligent self-learning recommendation algorithms.	SnoopyTagging: recommending contextualized tags to increase the quality and quantity of meta-information	NA:NA:NA:NA	2018
Andreas Gizas:Sotiris Christodoulou:Theodore Papatheodorou	For web programmers, it is important to choose the proper JavaScript framework that not only serves their current web project needs, but also provides code of high quality and good performance. The scope of this work is to provide a thorough quality and performance evaluation of the most popular JavaScript frameworks, taking into account well established software quality factors and performance tests. The major outcome is that we highlight the pros and cons of JavaScript frameworks in various areas of interest and signify which and where are the problematical points of their code, that probably need to be improved in the next versions.	Comparative evaluation of javascript frameworks	NA:NA:NA	2018
François Goasdoué:Ioana Manolescu:Alexandra Roatiş	We introduce the database fragment of RDF, which extends the popular Description Logic fragment, in particular with support for incomplete information. We then provide novel sound and complete saturation- and reformulation-based techniques for answering the Basic Graph Pattern queries of SPARQL in this fragment. Notably, we extend the state of the art on pushing RDF query processing within robust / efficient relational database management systems. Finally, we experimentally compare our query answering techniques using well-established datasets.	Getting more RDF support from relational databases	NA:NA:NA	2018
Liqiang Guo:Xiaojun Wan	Opinion retrieval is the task of finding documents that express an opinion about a given query. A key challenge in opinion retrieval is to capture the query-related opinion score of a document. Existing methods rely mainly on the proximity information between the opinion terms and the query terms to address the key challenge. In this study, we propose to incorporate the syntactic and semantic information of terms into a probabilistic language model in order to capture the query-related opinion score more accurately.	S2ORM: exploiting syntactic and semantic information for opinion retrieval	NA:NA	2018
Marian Harbach:Sascha Fahl:Thomas Muders:Matthew Smith	Current online social networking (OSN) sites pose severe risks to their users' privacy. Facebook in particular is capturing more and more of a user's past activities, sometimes starting from the day of birth. Instead of transiently passing on information between friends, a user's data is stored persistently and therefore subject to the risk of undesired disclosure. Traditionally, a regular user of a social network has little awareness of her privacy needs in the Web or is not ready to invest a considerable effort in securing her online activities. Furthermore, the centralised nature of proprietary social networking platforms simply does not cater for end-to-end privacy protection mechanisms. In this paper, we present a non-disruptive and lightweight integration of a confidentiality mechanism into OSNs. Additionally, direct integration of visual security indicators into the OSN UI raise the awareness for (un)protected content and thus their own privacy. We present a fully-working prototype for Facebook and an initial usability study, showing that, on average, untrained users can be ready to use the service in three minutes.	All our messages are belong to us: usable confidentiality in social networks	NA:NA:NA:NA	2018
Olaf Hartig:Tom Heath	The emergence of a Web of Data enables new forms of application that require expressive query access, for which mature, Web-scale information retrieval techniques may not be suited. Rather than attempting to deliver expressive query capabilities at Web-scale, this paper proposes the use of smaller, pre-populated data caches whose contents are personalized to the needs of an individual user. We present an approach to a priori population of such caches with Linked Data harvested from the Web, seeded by a simple context model for each user, which is progressively enriched by executing a series of enrichment rules over Linked Data from the Web. Such caches can act as personal data stores supporting a range of different applications. A comprehensive user evaluation demonstrates that our approach can accurately predict the relevance of attributes added to the context model and the execution probability of queries based on these attributes, thereby optimizing the cache population process.	Populating personal linked data caches using context models	NA:NA	2018
Qiang He:Jun Han:Yun Yang:Jean-Guy Schneider:Hai Jin:Steve Versteeg	The critical path of a composite Web application operating in volatile environments, i.e., the execution path in the service composition with the maximum execution time, should be prioritised in cost-effective monitoring as it determines the response time of the Web application. In volatile operating environments, the critical path of a Web application is probabilistic. As such, it is important to estimate the criticalities of the execution paths, i.e., the probabilities that they are critical, to decide which parts of the system to monitor. We propose a novel approach to the identification of Probabilistic Critical Path for Service-based Web Applications (PCP-SWA), which calculates the criticalities of different execution paths in the context of service composition. We evaluate PCP-SWA experimentally using an example Web application. Compared to random monitoring, PCP-SWA based monitoring is 55.67% more cost-effective on average.	Probabilistic critical path identification for cost-effective monitoring of service-based web applications	NA:NA:NA:NA:NA:NA	2018
Inma Hernández:Carlos R. Rivero:David Ruiz:Rafael Corchuelo	Most web page classifiers use features from the page content, which means that it has to be downloaded to be classified. We propose a technique to cluster web pages by means of their URL exclusively. In contrast to other proposals, we analyze features that are outside the page, hence, we do not need to download a page to classify it. Also, it is non-supervised, requiring little intervention from the user. Furthermore, we do not need to crawl extensively a site to build a classifier for that site, but only a small subset of pages. We have performed an experiment over 21 highly visited websites to evaluate the performance of our classifier, obtaining good precision and recall results.	A statistical approach to URL-based web page clustering	NA:NA:NA:NA	2018
Hsun-Ping Hsieh:Cheng-Te Li:Shou-De Lin	In current social networking service (SNS) such as Facebook, there are diverse kinds of interactions between entity types. One commonly-used activity of SNS users is to track and observe the representative social and temporal behaviors of other individuals. This inspires us to propose a new problem of Temporal Social Behavior Search (TSBS) from social interactions in an information network: given a structural query with associated temporal labels, how to find the subgraph instances satisfying the query structure and temporal requirements? In TSBS, a query can be (a) a topological structure, (b) the partially-assigned individuals on nodes, and/or (c) the temporal sequential labels on edges. The TSBS method consists of two parts: offline mining and online matching. to the former mines the temporal subgraph patterns for retrieving representative structures that match the query. Then based on the given query, we perform the online structural matching on the mined patterns and return the top-k resulting subgraphs. Experiments on academic datasets demonstrate the effectiveness of TSBS.	Frequent temporal social behavior search in information networks	NA:NA:NA	2018
Hsun-Ping Hsieh:Cheng-Te Li:Shou-De Lin	With location-based services, such as Foursquare and Gowalla, users can easily perform check-in actions anywhere and anytime. Such check-in data not only enables personal geospatial journeys but also serves as a fine-grained source for trip planning. In this work, we aim to collectively recommend trip routes by leveraging a large-scaled check-in data through mining the moving behaviors of users. A novel recommendation system, TripRec, is proposed to allow users to pecify starting/end and must-go locations. It further provides the flexibility to satisfy certain time constraint (i.e., the expected duration of the trip). By considering a sequence of check-in points as a route, we mine the frequent sequences with some ranking mechanism to achieve the goal. Our TripRec targets at travelers who are unfamiliar to the objective area/city and have time constraints in the trip.	TripRec: recommending trip routes from large scale check-in data	NA:NA:NA	2018
Xia Hu:Huan Liu	Email usage is pervasive among people from different backgrounds, and email corpus can be an important data source to study intricate social structures. Social status and role analysis on a personal email network can help reveal hidden information. The availability of Sarah Palin's email corpus presents a great opportunity to study social statuses and social roles in an email network. However, the email corpus does not readily lend itself to social network analysis due to problems such as noisy email data, scale in size, and temporal constraints. In this paper, we report an initial investigation of social status and role analysis on Sarah Palin's email corpus. In particular, we conduct a preliminary study on Palin's social statuses and roles. To the best of our knowledge, this work is the first exploration of Sarah Palin's email corpus recently released by the state of Alaska.	Social status and role analysis of palin's email network	NA:NA	2018
Anushia Inthiran:Saadat M. Alhashmi:Pervaiz K. Ahmed	In this paper, we analyze medical searching behavior performed by a typical medical searcher. We broadly classify a typical medical searcher as: non-medical professionals or medical professionals. We use behavioral signals to study how task difficulty affects medical searching behavior. Using simulated scenarios, we gathered data from an exploratory survey of 180 search sessions performed by 60 participants. Our research study provides a deep understanding of how task difficulty affects medical search behavior. Non-medical professionals and medical professionals demonstrate similar search behavior when searching on an easy task. Longer queries, more time and more incomplete search sessions are observed for an easy task. However, they demonstrate different results evaluation behavior based on task difficulty.	The affects of task difficulty on medical searches	NA:NA:NA	2018
Jagadeesh Jagarlamudi:Paul N. Bennett:Krysta M. Svore	In this paper we address the problem of improving accuracy of web search in a smaller, data-limited search market (search language) using behavioral data from a larger, data-rich market (assist language). Specifically, we use interlingual classification to infer the search language query's intent using the assist language click-through data. We use these improved estimates of query intent, along with the query intent based on the search language data, to compute features that encode the similarity between a search result (URL) and the query. These features are subsequently fed into the ranking model to improve the relevance ranking of the documents. Our experimental results on German and French languages show the effectiveness of using assist language behavioral data especially, when the search language queries have small click-through data.	Leveraging interlingual classification to improve web search	NA:NA:NA	2018
Jagadeesh Jagarlamudi:Jianfeng Gao	Statistical translation models and latent semantic analysis (LSA) are two effective approaches to exploit click-through data for web search ranking. This paper presents two document ranking models that combine both approaches by explicitly modeling word-pairs. The first model, called PairModel, is a monolingual ranking model based on word pairs that are derived from click-through data. It maps queries and documents into a concept space spanned by these word pairs. The second model, called Bilingual Paired Topic Model (BPTM), uses bilingual word pairs and jointly models a bilingual query-document collection. This model maps queries and documents in multiple languages into a lower dimensional semantic subspace. Experimental results on web search task show that they significantly outperform the state-of-the-art baseline models, and the best result is obtained by interpolating PairModel and BPTM.	Modeling click-through based word-pairs for web search	NA:NA	2018
Yushi Jing:Henry Rowley:Jingbin Wang:David Tsai:Chuck Rosenberg:Michele Covell	Web image retrieval systems, such as Google or Bing image search, present search results as a relevance-ordered list. Although alternative browsing models (e.g. results as clusters or hierarchies) have been proposed in the past, it remains to be seen whether such models can be applied to large-scale image search. This work presents Google Image Swirl, a large-scale, publicly available, hierarchical image browsing system by automatically group the search results based on visual and semantic similarity. This paper describes methods used to build such system and shares the findings from 2-years worth of user feedback and usage statistics.	Google image swirl: a large-scale content-based image visualization system	NA:NA:NA:NA:NA:NA	2018
Noriaki Kawamae	Our proposal, identifying sentiment over N-gram (ISN) focuses on both word order and phrases, and the interdependency between specific rating and corresponding sentiment in a text to detect subjective information.	Identifying sentiments over N-gram	NA	2018
Vaibhav V. Khadilkar:Murat Kantarcioglu:Bhavani Thuraisingham	The focus of online social media providers today has shifted from "content generation" towards finding effective methodologies for "content storage, retrieval and analysis" in the presence of evolving networks. Towards this end, in this paper we present StormRider, a framework that uses existing cloud computing and semantic web technologies to provide application programmers with automated support for these tasks, thereby allowing a richer assortment of use cases to be implemented on the underlying evolving social networks.	StormRider: harnessing "storm" for social networks	NA:NA:NA	2018
Marios Kokkodis	On-line marketplaces have been growing in importance over the last few years. In such environments, reviews consist the main reputation mechanism for the available products. Hence, presenting high quality reviews is crucial in achieving a high level of customer satisfaction. Towards this direction, in this work, we introduce a new dimension of review quality, the reviewer's "trustfulness". We assume that voluntary information provided by Amazon reviewers, regarding whether they are the actual buyers of the product, signals the reliability of a review. Based on this information, we characterize a reviewer as trustworthy (positive instance) or of unknown "trustfulness" (unlabeled instance). Then, we build models that exploit reviewers' profile information and on-line behavior to rank them according to the probability of being trustworthy. Our results are very promising, since they provide evidence that our predictive models separate positive from unlabeled instances with very high accuracies.	Learning from positive and unlabeled amazon reviews: towards identifying trustworthy reviewers	NA	2018
Ralf Krestel:Alex Wall:Wolfgang Nejdl	The Web is a very democratic medium of communication allowing everyone to express his or her opinion about any type of topic. This multitude of voices makes it more and more important to detect bias and help Internet users understand the background of information sources. Political bias of Web sites, articles, or blog posts is hard to identify straightaway. Manual content analysis conducted by experts is the standard way in political and social science to detect this bias. In this paper we present an automated approach relying on methods from information retrieval and corpus statistics to identify biased vocabulary use. As an example, we analyzed 15 years of parliamentary speeches of the German Bundestag and we investigated whether there is bias towards a political party in major national online newspapers and magazines. The results show that bias exists with respect to vocabulary use and it coincides with human judgement.	Treehugger or petrolhead?: identifying bias by comparing online news articles with political speeches	NA:NA:NA	2018
Kyriakos Kritikos:Dimitris Plexousakis	The Internet is moving fast to a new era where million of services and things will be available. In this way, as there will be many functionally-equivalent services for a specific user task, the service non-functional aspect should be considered for filtering and choosing the appropriate services. The related approaches in service discovery mainly concentrate on exploiting constraint solving techniques for inferring if the user non-functional requirements are satisfied by the service nonfunctional capabilities. However, as the matchmaking time is proportional to the number of non-functional service descriptions, these approaches fail to fulfill the user request in a timely manner. To this end, two alternative techniques for improving the non-functional service matchmaking time have been developed. The first one is generic as it can handle non-functional service specifications containing n-ary constraints, while the second is only applicable to unary-constrained specifications. Both techniques were experimentally evaluated. The preliminary evaluation results show that the service matchmaking time is significantly improved without compromising matchmaking accuracy.	Towards optimizing the non-functional service matchmaking time	NA:NA	2018
Andrey Kustarev:Yury Ustinovsky:Pavel Serduykov	Most of major search engines develop different types of personalisation of search results. Personalisation includes deriving user's long-term preferences, query disambiguation etc. User sessions provide very powerful tool commonly used for these problems. In this paper we focus on personalisation based on context-aware reranking. We implement a machine learning framework to approach this problem and study importance of different types of features. We stress that features concerning temporal and context relatedness of queries along with features relied on user's actions are most important and play crucial role for this type of personalisation.	Measuring usefulness of context for context-aware ranking	NA:NA:NA	2018
Himabindu Lakkaraju:Hyung-Il Ahn	In recent times, microblogging sites like Facebook and Twitter have gained a lot of popularity. Millions of users world wide have been using these sites to post content that interests them and also to voice their opinions on several current events. In this paper, we present a novel non-parametric probabilistic model - Temporally driven Theme Event Model (TEM) for analyzing the content on microblogs. We also describe an online inference procedure for this model that enables its usage on large scale data. Experimentation carried out on real world data extracted from Facebook and Twitter demonstrates the efficacy of the proposed approach.	TEM: a novel perspective to modeling content onmicroblogs	NA:NA	2018
Kristina Lerman:Suradej Intagorn:Jeon-Hyung Kang:Rumi Ghosh	The structure of a social network contains information useful for predicting its evolution. We show that structural information also helps predict activity. People who are "close" in some sense in a social network are more likely to perform similar actions than more distant people. We use network proximity to capture the degree to which people are "close" to each other. In addition to standard proximity metrics used in the link prediction task, such as neighborhood overlap, we introduce new metrics that model different types of interactions that take place between people. We study this claim empirically using data about URL forwarding activity on the social media sites Digg and Twitter. We show that structural proximity of two users in the follower graph is related to similarity of their activity, i.e., how many URLs they both forward. We also show that given friends' activity, knowing their proximity to the user can help better predict which URLs the user will forward. We compare the performance of different proximity metrics on the activity prediction task and find that metrics that take into account the attention-limited nature of interactions in social media lead to substantially better predictions.	Using proximity to predict activity in social networks	NA:NA:NA:NA	2018
Cheng-Te Li:Hsun-Ping Hsieh:Shou-De Lin:Man-Kwan Shan	In a dynamic social network, nodes can be removed from the network for some reasons, and consequently affect the behaviors of the network. In this paper, we tackle the challenge of finding a successor node for each removed seed node to maintain the influence spread in the network. Given a social network and a set of seed nodes for influence maximization, who are the best successors to be transferred the jobs of initial influence propagation if some seeds are removed from the network. To tackle this problem, we present and discuss five neighborhood-based selection heuristics, including degree, degree discount, overlapping, community bridge, and community degree. Experiments on DBLP co-authorship network show the effectiveness of devised heuristics.	Finding influential seed successors in social networks	NA:NA:NA:NA	2018
Cheng-Te Li:Shou-De Lin:Man-Kwan Shan	Influence propagation and maximization is a well-studied problem in social network mining. However, most of the previous works focus only on homogeneous social networks where nodes and links are of single type. This work aims at defining information propagation for heterogeneous social networks (containing multiple types of nodes and links). We propose to consider the individual behaviors of persons to model the influence propagation. Person nodes possess different influence probabilities to activate their friends according to their interaction behaviors. The proposed model consists of two stages. First, based on the heterogeneous social network, we create a human-based influence graph where nodes are of human-type and links carry weights that represent how special the target node is to the source node. Second, we propose two entropy-based heuristics to identify the disseminators in the influence graph to maximize the influence spread. Experimental results show promising results for the proposed method.	Influence propagation and maximization for heterogeneous social networks	NA:NA:NA	2018
Cheng-Te Li:Man-Kwan Shan:Shou-De Lin	This paper aims to combine the viral marketing with the idea of direct selling to for influence maximization in a social network. In direct selling, producers can sell the products directly to the consumers without having to go through a cascade of wholesalers. Through direct selling, it is possible to sell the products in a more efficient and economic manner. Motivated by this idea, we propose a target-selecting independent cascade (TIC) model, in which during influence propagation each active node can give up to attempt to influence some neighboring nodes, named victims, who could be hard to affect, and try to activate some of its friends of friends, termed destinations, who could have higher potential to increase the influence spread. Thus, the next question to ask is that given a social network and a set of seeds for influence propagation under TIC model, how to select targets (i.e., victims and destinations) for the attempts of activation during the propagation to boost of influence spread. We propose and evaluate three heuristics for the target selection. Experiments show that selecting targets based on influence probability between nodes have the highest boost of influence spread.	Dynamic selection of activation targets to boost the influence spread in social networks	NA:NA:NA	2018
Cheng-Te Li:Man-Kwan Shan:Shou-De Lin	This paper solves a region-based subgraph discovery problem. We are given a social network and some sample nodes which is supposed to belong to a specific region, and the goal is to obtain a subgraph that contains the sampled nodes with other nodes in the same region. Such regional subgraph discovery can benefit region-based applications, including scholar search, friend suggestion, and viral marketing. To deal with this problem, we assume there is a hidden backbone connecting the query nodes directly or indirectly in their region. The idea is that individuals belonging to the same region tend to share similar interests and cultures. By modeling such fact on edge weights, we search the graph to extract the regional backbone with respect to the query nodes. Then we can expand the backbone to derive the regional network. Experiments on a DBLP co-authorship network show the proposed method can effectively discover the regional subgraph with high precision scores.	Regional subgraph discovery in social networks	NA:NA:NA	2018
Ping Li:Anshumali Shrivastava:Christian A. Konig	Minwise hashing is a standard technique for efficient set similarity estimation in the context of search. The recent work of b-bit minwise hashing provided a substantial improvement by storing only the lowest b bits of each hashed value. Both minwise hashing and b-bit minwise hashing require an expensive preprocessing step for applying k (e.g., k=500) permutations on the entire data in order to compute k minimal values as the hashed data. In this paper, we developed a parallelization scheme using GPUs, which reduced the processing time by a factor of 20-80. Reducing the preprocessing time is highly beneficial in practice, for example, for duplicate web page detection (where minwise hashing is a major step in the crawling pipeline) or for increasing the testing speed of online classifiers (when the test data are not preprocessed).	GPU-based minwise hashing: GPU-based minwise hashing	NA:NA:NA	2018
Yanen Li:Huizhong Duan:ChengXiang Zhai	Query spelling correction is an important component of modern search engines that can help users to express an information need more accurately and thus improve search quality. In this work we proposed and implemented an end-to-end speller correction system, namely CloudSpeller. The CloudSpeller system uses a Hidden Markov Model to effectively model major types of spelling errors in a unified framework, in which we integrate a large-scale lexicon constructed using Wikipedia, an error model trained from high confidence correction pairs, and the Microsoft Web N-gram service. Our system achieves excellent performance on two search query spelling correction datasets, reaching 0.960 and 0.937 F1 scores on the TREC dataset and the MSN dataset respectively.	CloudSpeller: query spelling correction by using a unified hidden markov model with web-scale resources	NA:NA:NA	2018
Yuming Lin:Jingwei Zhang:Xiaoling Wang:Aoying Zhou	In the bag of words framework, documents are often converted into vectors according to predefined features together with weighting mechanisms. Since each feature presentation has its character, it is difficult to determine which one should be chosen for a specific domain, especially for the users who are not familiar with the domain. This paper explores the integration of various feature presentations to improve the classification accuracy. A general two phases framework is proposed. In the first phase, we train multiple base classifiers with various vector spaces and use these classifiers to predict the class of testing samples respectively. In the second phase, the previous predicted results are integrated into the ultimate class via stacking with SVM. The experimental results demonstrate the effectiveness of our method.	Sentiment classification via integrating multiple feature presentations	NA:NA:NA:NA	2018
Yury Logachev:Lidia Grauer:Pavel Serdyukov	There are several popular IR metrics based on an underlying user model. Most of them are parameterized. Usually parameters of these metrics are chosen on the basis of general considerations and not validated by experiments with real users. Particularly, the parameters of the Expected Reciprocal Rank measure are the normalized parameters of the DCG metric, and the latter are chosen in an ad-hoc manner. We suggest two approaches for adjusting parameters of the ERR model by analyzing real users behaviour: one based on a controlled experiment and another relying on search log analysis. We show that our approaches generate parameters that are largely different from the commonly used parameters of the ERR model.	Tuning parameters of the expected reciprocal rank	NA:NA:NA	2018
Juan Antonio Lossio Ventura:Hakim Hacid:Arnaud Ansiaux:Maria Laura Maag	We propose a socio-semantic approach for building conversations from social interactions following three steps: (i) content linkage, (ii) participants (users) linkage, and (iii) temporal linkage. Preliminary evaluations on a Twitter dataset show promising and interesting results.	Conversations reconstruction in the social web	NA:NA:NA:NA	2018
Houari Mahfoud:Abdessamad Imine	Most state-of-the art approaches for securing XML documents allow users to access data only through authorized views defined by annotating an XML grammar (e.g. DTD) with a collection of XPath expressions. To prevent improperdisclosure of confidential information, user queries posed on these views need to be rewritten into equivalent queries on the underlying documents, which enables us to avoid the overhead of view materialization and maintenance. A major concern here is that XPath query rewriting for recursive XML views is still an open problem. To overcome this problem, some authors have proposed rewriting approaches based on the non-standard language, "Regular XPath", which is more expressive than XPath and makes rewriting possible under recursion. However, query rewriting under Regular XPath can be of exponential size as it relies on automaton model. Most importantly, Regular XPath remains a theoretical achievement. Indeed, it is not commonly used in practice as translation and evaluation tools are not available. In this work, we show that query rewriting is always possible for recursive XML views using only the expressive power of the standard XPath. We propose a general approach for securely querying of XML data under arbitrary security views (recursive or not) and for a significant fragment of XPath. We provide a linear rewriting algorithm that is efficient and scales well.	Secure querying of recursive XML views: a standard xpath-based technique	NA:NA	2018
Abdul Majid:Ling Chen:Gencai Chen:Hamid Turab Mirza:Ibrar Hussain	We propose a context and preference aware travel guide that suggests significant tourist destinations to users based on their preferences and current surrounding context using contextualized user-generated contents from the social media repository, i.e., Flickr.	GoThere: travel suggestions using geotagged photos	NA:NA:NA:NA:NA	2018
Debnath Mukherjee:Snehasis Banerjee:Prateep Misra	In the existing ride sharing scenario, the ride taker has to cope with uncertainties since the ride giver may be delayed or may not show up due to some exigencies. A solution to this problem is discussed in this paper. The solution framework is based on gathering information from multiple streams such as traffic status on the ride giver's routes and the ride giver's GPS coordinates. Also, it maintains a list of alternative ride givers so as to almost guarantee a ride for the ride taker. This solution uses a SPARQL-based continuous query framework that is capable of sensing fast-changing real-time situation. It also has reasoning capabilities for handling ride taker's preferences. The paper introduces the concept of user-managed windows that is shown to be required for this solution. Finally we show that the performance of the application is enhanced by designing the application with short incremental queries.	Ad-hoc ride sharing application using continuous SPARQL queries	NA:NA:NA	2018
Xia Ning:George Karypis	This paper focuses on developing effective algorithms that utilize side information for top-N recommender systems. A set of Sparse Linear Methods with Side information (SSLIM) is proposed, that utilize a regularized optimization process to learn a sparse item-to-item coefficient matrix based on historical user-item purchase profiles and side information associated with the items. This coefficient matrix is used within an item-based recommendation framework to generate a size-N ranked list of items for a user. Our experimental results demonstrate that SSLIM outperforms other methods in effectively utilizing side information and achieving performance improvement.	Sparse linear methods with side information for Top-N recommendations	NA:NA	2018
Sylvester Olubolu Orimaye:Saadat M. Alhashmi:Siew Eu-gene	Nollywood is the second largest movie industry in the world in terms of annual movie production. A dominant number of the movies are in Yoruba language spoken by over 20 million people across the globe. The number of Yoruba language movies uploaded to YouTube and their corresponding comments is growing exponentially. However, YouTube comments made by native speakers on Yoruba movies combine English language, Yoruba language, and other commonly used "pidgin" Yoruba language words. Since Yoruba is still a resource constrained language, existing sentiment or subjectivity analysis algorithms have poor performances on YouTube comments made on Yoruba language movies. This is because of the constrained language ambiguities. In this work, we present an automatic sentiment analysis algorithm for YouTube comments on Yoruba language movies. The algorithm uses SentiWordNet thesaurus and a lexicon of commonly used Yoruba language sentiment words and phrases. In terms of precision-recall, the algorithm performs more than a state-of-the-art sentiment analysis technique by up to 20%.	Sentiment analysis amidst ambiguities in youtube comments on yoruba language (nollywood) movies	NA:NA:NA	2018
Thomas Paul:Martin Stopczynski:Daniel Puscher:Melanie Volkamer:Thorsten Strufe	The ever increasing popularity of Facebook and other Online Social Networks has left a wealth of personal and private data on the web, aggregated and readily accessible for broad and automatic retrieval. Protection from both undesired recipients and harvesting by crawlers is implemented by access control, manually configured by the user and owner of the data. Several studies demonstrate that default settings cause an unnoticed over-sharing and that users have trouble understanding and configuring adequate privacy settings. We developed an improved interface for privacy settings in Facebook by mainly applying color coding for different groups, providing easy access to the privacy settings, and applying the principle of common practices. Using a lab study, we show that the new approach increases the usability significantly.	C4PS: colors for privacy settings	NA:NA:NA:NA:NA	2018
Santosh Raju:Raghavendra Udupa	Extracting advertising keywords from web-pages is important in keyword-based online advertising. Previous works have attempted to extract advertising keywords from the whole content of a web-page. However, in some scenarios, it is necessary to extract keywords from just the URL string itself. In this work, we propose an algorithm for extracting advertising keywords from the URL string alone. Our algorithm has applications in contextual and paid search advertising. We evaluate the effectiveness of our algorithm on publisher URLs and show that it produces very good quality keywords that are comparable with keywords produced by page based extractors.	Extracting advertising keywords from URL strings	NA:NA	2018
Christine F. Reilly:Yueh-Hsuan Chiang:Jeffrey F. Naughton	Information extraction (IE) programs for the web consume and produce a lot of data. In order to better understand the program output, the developer and user often desire to know the details of how the output was created. Provenance can be used to learn about the creation of the output. We collect fine-grained provenance by leveraging ongoing work in the IE community to write IE programs in a logic programming language. The logic programming language exposes the semantics of the program, allowing us to gather fine-grained provenance during program execution. We discuss a case study using a web-based community information management system, then present results regarding the performance of queries over the provenance data gathered by our logic program interpreter. Our findings show that it is possible to gather useful fine-grained provenance during the execution of a logic based web information extraction program. Additionally, queries over this provenance information can be performed in a reasonable amount of time.	Instrumenting a logic programming language to gather provenance from an information extraction application	NA:NA:NA	2018
Luz Rello:Ricardo Baeza-Yates	We show that a recently introduced lexical quality measure is also valid to measure textual Web accessibility. Our measure estimates the lexical quality of a site based in the occurrence in English Web pages of a large set of words with errors. We first compute the correlation of our measure with Web popularity measures to show that gives independent information. Second, we carry out a user study using eye tracking to prove that the degree of lexical quality of a text is related to the degree of understandability of a text, one of the factors behind Web accessibility.	Lexical quality as a proxy for web text understandability	NA:NA	2018
Christian Sengstock:Michael Gertz	In this paper we propose a simple and flexible framework to index context-annotated documents, e.g., documents with timestamps or georeferences, by contextual topics. A contextual topic is a distribution over document features with a particular meaning in the context domain, such as a repetitive event or a geographic phenomenon. Such a framework supports document clustering, labeling, and search, with respect to contextual knowledge contained in the document collection. To realize the framework, we introduce an approach to project documents into a context-feature space. Then, dimensionality reduction is used to extract contextual topics in this context-feature space. The topics can then be projected back onto the documents. We demonstrate the utility of our approach with a case study on georeferenced Wikipedia articles.	Latent contextual indexing of annotated documents	NA:NA	2018
Wei Shen:Jianyong Wang:Ping Luo:Min Wang	Automatically populating ontology with named entities extracted from the unstructured text has become a key issue for Semantic Web. This issue naturally consists of two subtasks: (1) for the entity mention whose mapping entity does not exist in the ontology, attach it to the right category in the ontology (i.e., fine-grained named entity classification), and (2) for the entity mention whose mapping entity is contained in the ontology, link it with its mapping real world entity in the ontology (i.e., entity linking). Previous studies only focus on one of the two subtasks. This paper proposes APOLLO, a general weakly supervised frAmework for POpuLating ontoLOgy with named entities. APOLLO leverages the rich semantic knowledge embedded in the Wikipedia to resolve this task via random walks on graphs. An experimental study has been conducted to show the effectiveness of APOLLO.	APOLLO: a general framework for populating ontology with named entities via random walks on graphs	NA:NA:NA:NA	2018
Xin Shuai:Ying Ding:Jerome Busemeyer	Most studies on social influence have focused on direct influence, while another interesting question can be raised as whether indirect influence exists between two users who're not directly connected in the network and what affects such influence. In addition, the theory of complex contagion tells us that more spreaders will enhance the indirect influence between two users. Our observation of intensity of indirect influence, propagated by n parallel spreaders and quantified by retweeting probability on Twitter, shows that complex contagion is validated globally but is violated locally. In other words, the retweeting probability increases non-monotonically with some local drops.	Multiple spreaders affect the indirect influence on twitter	NA:NA:NA	2018
Amit Singh	Bridging the lexical gap between the user's question and the question-answer pairs in Q&A archives has been a major challenge for Q&A retrieval. State-of-the-art approaches address this issue by implicitly expanding the queries with additional words using statistical translation models. In this work we extend the lexical word based translation model to incorporate semantic concepts. We explore strategies to learn the translation probabilities between words and the concepts using the Q&A archives and Wikipedia. Experiments conducted on a large scale real data from Yahoo Answers! show that the proposed techniques are promising and need further investigation.	Entity based translation language model	NA	2018
Koushik Sinha:Geetha Manjunath:Raveesh R. Sharma:Viswanath Gangavaram:Pooja A:Deepak R. Murugaian	Voice interfaces to browsers and mobile applications are becoming popular as typing with touch screens is cumbersome. The main issue of practical speech based interfaces is how to overcome speech recognition errors. This problem is more severe when the users are non-native speakers of English due to differences in pronunciations. In this paper, we describe a novel, intelligent speech interface design approach for IR tasks that is significantly robust to accent variations. Our solution uses phonemic similarity based word spreading and semantic information based filtering to boost the accuracy of any ASR. We evaluated our solution with Google Voice as the ASR for a web question-answering system developed in-house and the results are very encouraging.	Enabling accent resilient speech based information retrieval	NA:NA:NA:NA:NA:NA	2018
Marc Sloan:Jun Wang	The dynamic nature of document relevance is largely ignored by traditional Information Retrieval (IR) models, which assume that scores (relevance) for documents given an information need are static. In this paper, we formulate a general Dynamical Information Retrieval problem, where we consider retrieval as a stochastic, controllable process. The ranking action continuously controls the retrieval system's dynamics and an optimal ranking policy is found that maximizes the overall users' satisfaction during each period. Through deriving the posterior probability of the documents evolving relevancy from user clicks, we can provide a plug-in framework for incorporating a number of click models, which can be combined with Multi-Armed Bandit theory and Portfolio Theory of IR to create a dynamic ranking rule that takes rank bias and click dependency into account. We verify the versatility of our algorithms in a number of experiments and demonstrate improved performance over strong baselines and as a result significant performance gains have been achieved.	Dynamical information retrieval modelling: a portfolio-armed bandit machine approach	NA:NA	2018
Shuangyong Song:Qiudan Li:Hongyun Bao	Over the last few years, Twitter is increasingly becoming an important source of up-to-date topics about what is happening in the world. In this paper, we propose a dynamic topic association detection model to discover relations between Twitter topics, by which users can gain insights into richer information about topics of interest. The proposed model utilizes a time constrained method to extract event-based spatio-temporal topic association, and constructs a dynamic temporal map to represent the obtained result. Experimental results show the improvement of the proposed model compared to static spatio-temporal method and co-occurrence method.	Detecting dynamic association among twitter topics	NA:NA:NA	2018
Sucheta Soundarajan:John Hopcroft	Because network data is often incomplete, researchers consider the link prediction problem, which asks which non-existent edges in an incomplete network are most likely to exist in the complete network. Classical approaches compute the 'similarity' of two nodes, and conclude that highly similar nodes are most likely to be connected in the complete network. Here, we consider several such similarity-based measures, but supplement the similarity calculations with community information. We show that for many networks, the inclusion of community information improves the accuracy of similarity-based link prediction methods.	Using community information to improve the precision of link prediction methods	NA:NA	2018
Vlad Stirbu:David Murphy:Yu You	Augmented reality applications are gaining popularity due to increased capabilities of modern mobile devices. However, existing applications are tightly integrated with backend services that expose content using proprietary interfaces. We demonstrate an architecture that allows visualization of web content in augmented and mirror world applications, based on open web protocols and formats. We describe two clients, one for creating virtual artifacts, web resources that bind together web content with location and a 3D model, and one that visualizes the virtual artifacts in the mirror world.	Open and decentralized platform for visualizing web mash-ups in augmented and mirror worlds	NA:NA:NA	2018
Alisa Strizhevskaya:Alexey Baytin:Irina Galinskaya:Pavel Serdyukov	In this work we are studying actualization techniques for building an up-to-date query suggestions model using query logs. The performance of the proposed actualization algorithms was estimated by real query flow of the Yandex search engine.	Actualization of query suggestions using query logs	NA:NA:NA:NA	2018
Xu Sun:Anshumali Shrivastava:Ping Li	This paper explores the use of online multi-task learning for search query spelling correction, by effectively transferring information from different and biased training datasets for improving spelling correction across datasets. Experiments were conducted on three query spelling correction datasets, including the well-known TREC benchmark data. Our experimental results demonstrate that the proposed method considerably outperforms existing baseline systems in terms of accuracy. Importantly, the proposed method is about one-order of magnitude faster than baseline systems in terms of training speed. In contrast to existing methods which typically require more than (e.g.,) 50 training passes, our algorithm can very closely approach the empirical optimum in around five passes.	Query spelling correction using multi-task learning	NA:NA:NA	2018
Yuchen Tian:Yiqun Liu:Danqing Xu:Ting Yao:Min Zhang:Shaoping Ma	We consider the problem of predicting monthly auto sales in mainland China. First, we design an algorithm using click-through and query reformulation information to cluster related queries and count their frequencies on monthly-basis. By introducing Exponentially Weighted Moving Averages (EWMA) model, we measure the seasonal impact on the sales trend. Two features are combined using linear regression. The experiment shows that our model is effective with high accuracy and outperforms conventional forecasting models.1	Incorporating seasonal time series analysis with search behavior information in sales forecasting	NA:NA:NA:NA:NA:NA	2018
Vincent Toubiana:Vincent Verdot:Benoit Christophe:Mathieu Boussard	Although they are used to expose pictures on the Web, users may not want to have a link between their identity and pictures without being able to modify them or control who accesses them. Photo tagging --- and more broadly face-recognition algorithms --- often escapes to the users' control and creates links between private situations and their public profile. To address this issue, we designed a geo-location aided system to let users declare their tagging preferences directly when the picture is taken. We present Photo-Tagging Preference Enforcement (Photo-TaPE) a system enforcing users tagging preferences without revealing their identity. By improving face-recognition efficiency, Photo-TaPE can guarantee the user tagging preferences in 67% of the cases and significantly reduces the processing time of face-recognition algorithms.	Photo-TaPE: user privacy preferences in photo tagging	NA:NA:NA:NA	2018
Ionut Trestian:Kévin Huguenin:Ling Su:Aleksandar Kuzmanovic	The recent availability of human mobility traces has driven a new wave of research on human movement with straightforward applications in wireless/cellular network. In this paper we revisit the human mobility problem with new assumptions. We believe that human movement is not independent of the surrounding locations, i.e. the points of interest that they visit; most of the time people travel with specific goals in mind, visit specific points of interest, and frequently revisit favorite places. Using GPS mobility traces of a large number of users located across two distinct geographical locations we study the correlation between people's trajectories and the differently spread points of interest nearby.	Understanding human movement semantics: a point of interest based approach	NA:NA:NA:NA	2018
Oren Tsur:Adi Littman:Ari Rappoport	The growing popularity of microblogging backed by services like Twitter, Facebook, Google+ and LinkedIn, raises the challenge of clustering short and extremely sparse documents. In this work we propose SMSC -- a scalable, accurate and efficient multi stage clustering algorithm. Our algorithm leverages users practice of adding tags to some messages by bootstrapping over virtual non sparse documents. We experiment on a large corpus of tweets from Twitter, and evaluate results against a gold-standard classification validated by seven clustering evaluation measures (information theoretic, paired and greedy). Results show that the algorithm presented is both accurate and efficient, significantly outperforming other algorithms. Under reasonable practical assumptions, our algorithm scales up sublinearly in time.	Scalable multi stage clustering of tagged micro-messages	NA:NA:NA	2018
Tony Veale	Affect lexica are useful for sentiment analysis because they map words (or senses) onto sentiment ratings. However, few lexica explain their ratings, or provide sufficient feature richness to allow a selective "spin"" to be placed on a word in context. Since an affect lexicon aims to capture the affect of a word or sense in its most stereotypical usage, it should be grounded in explicit stereotype representations of each word's most salient properties and behaviors. We show here how to acquire a large stereotype lexicon from Web content, and further show how to determine sentiment ratings for each entry in the lexicon, both at the level of properties and behaviors and at the level of stereotypes. Finally, we show how the properties of a stereotype can be segregated on demand, to place a positive or negative spin on a word in context.	Seeing the best and worst of everything on the web with a two-level, feature-rich affect lexicon	NA	2018
Alexey Volkov:Pavel Serdyukov	The paper presents a novel approach to finding regional scopes (geotagging) of websites. It relies on a single binary classification model per region type to perform the multi-class classification and uses a variety of features of different nature that have not been yet used together for machine-learning based regional classification of websites. The evaluation demonstrates the advantage of our "one model per region type" method versus the traditional "one model per region" approach.	Unified classification model for geotagging websites	NA:NA	2018
Jun Wang:Bowei Chen	Many online advertising slots are sold through bidding mechanisms by publishers and search engines. Highly affected by the dual force of supply and demand, the prices of advertising slots vary significantly over time. This then influences the businesses whose major revenues are driven by online advertising, particularly for publishers and search engines. To address the problem, we propose to sell the future advertising slots via option contracts (also called ad options). The ad option can give its buyer the right to buy the future advertising slots at a prefixed price. The pricing model of ad options is developed in order to reduce the volatility of the income of publishers or search engines. Our experimental results confirm the validity of ad options and the embedded risk management mechanisms.	Selling futures online advertising slots via option contracts	NA:NA	2018
Xuanhui Wang:Jian Bian:Yi Chang:Belle Tseng	Most of previous work on news relatedness focuses on news article texts. In this paper, we study the benefit of user-generated comments on modeling news relatedness. Comments contain rich text information which is provided by commenters and rated by readers with thumb-up or thumb-down, but the quality of individual comments varies widely. We compare different ways of capturing relatedness by leveraging both text and user interaction information in comments. Our evaluation based on an editorial data set demonstrates that the text information in comments is very effective to model relatedness while community rating is quite predictive of the comment quality.	Model news relatedness through user comments	NA:NA:NA:NA	2018
Robert West:Ingmar Weber:Carlos Castillo	Who edits Wikipedia? We attempt to shed light on this question by using aggregated log data from Yahoo!'s browser toolbar in order to analyze Wikipedians' editing behavior in the context of their online lives beyond Wikipedia. We broadly characterize editors by investigating how their online behavior differs from that of other users; e.g., we find that Wikipedia editors search more, read more news, play more games, and, perhaps surprisingly, are more immersed in pop culture. Then we inspect how editors' general interests relate to the articles to which they contribute; e.g., we confirm the intuition that editors show more expertise in their active domains than average users. Our results are relevant as they illuminate novel aspects of what has become many Web users' prevalent source of information and can help in recruiting new editors.	A data-driven sketch of Wikipedia editors	NA:NA:NA	2018
Xian Wu:Wei Fan:Meilun Sheng:Li Zhang:Xiaoxiao Shi:Zhong Su:Yong Yu	State-of-the-art knowledge representation in semantic web employs a triple format (subject-relation-object). The limitation is that it can only represent static information, but cannot easily encode revisions of semantic web and knowledge evolution. In reality, knowledge does not stay still but evolves over time. In this paper, we first introduce the concept of "quintuple representation" by adding two new fields, state and time, where state has two values, either in or out, to denote that the referred knowledge takes effective or becomes expired at the given time. We then discuss a two-step statistical framework to mine knowledge evolution into the proposed quintuple representation. Utilizing extracted quintuple properly, it not only can reveal knowledge changing history but also detect expired information. We evaluate the proposed framework on Wikipedia revisions, as well as, common web pages currently not in semantic web format.	A framework to represent and mine knowledge evolution from Wikipedia revisions	NA:NA:NA:NA:NA:NA:NA	2018
Sihong Xie:Guan Wang:Shuyang Lin:Philip S. Yu	Online reviews play a crucial role in today's electronic commerce. Due to the pervasive spam reviews, customers can be misled to buy low-quality products, while decent stores can be defamed by malicious reviews. We observe that, in reality, a great portion (> 90% in the data we study) of the reviewers write only one review (singleton review). These reviews are so enormous in number that they can almost determine a store's rating and impression. However, existing methods ignore these reviewers. To address this problem, we observe that the normal reviewers' arrival pattern is stable and uncorrelated to their rating pattern temporally. In contrast, spam attacks are usually bursty and either positively or negatively correlated to the rating. Thus, we propose to detect such attacks via unusually correlated temporal patterns. We identify and construct multidimensional time series based on aggregate statistics, in order to depict and mine such correlation. Experimental results show that the proposed method is effective in detecting singleton review attacks. We discover that singleton review is a significant source of spam reviews and largely affects the ratings of online stores.	Review spam detection via time series pattern discovery	NA:NA:NA:NA	2018
Jian Xu:Qin Lu:Zhengzhong Liu	Web Person Disambiguation is often conducted through clustering web documents to identify different namesakes for a given name. This paper presents a new key-phrased clustering method combined with a second step re-classification to identify outliers to improve cluster performance. For document clustering, the hierarchical agglomerative approach is conducted based on the vector space model which uses key phrases as the main feature. Outliers of cluster results are then identified through a centroids-based method. The outliers are then reclassified by the SVM classifier into the more appropriate clusters using a key phrase-based string kernel model as its feature space. The re-classification uses the clustering result in the first step as its training data so as to avoid the use of separate training data required by most classification algorithms. Experiments conducted on the WePS-2 dataset show that the algorithm based on key phrases is effective in improving the WPD performance.	Combining classification with clustering for web person disambiguation	NA:NA:NA	2018
Byoungju Yang:Sangkeun Lee:Sungchan Park:Sang-goo Lee	So far, many researchers have worked on recommender systems using users' implicit feedback, since it is difficult to collect explicit item preferences in most applications. Existing researches generally use a pseudo-rating matrix by adding up the number of item consumption; however, this naive approach may not capture user preferences correctly in that many other important user activities are ignored. In this paper, we show that users' diverse implicit feedbacks can be significantly used to improve recommendation accuracy. We classify various users' behaviors (e.g., search item, skip, add to playlist, etc.) into positive or negative feedback groups and construct more accurate pseudo-rating matrix. Our preliminary experimental result shows significant potential of our approach. Also, we bring out a question to the previous approaches, aggregating item usage count into ratings.	Exploiting various implicit feedback for collaborative filtering	NA:NA:NA:NA	2018
Elad Yom-Tov:Mounia Lalmas:Georges Dupret:Ricardo Baeza-Yates:Pinard Donmez:Janette Lehmann	In the online world, user engagement refers to the phenomena associated with being captivated by a web application and wanting to use it longer and frequently. Nowadays, many providers operate multiple content sites, very different from each other. Due to their extremely varied content, these are usually studied and optimized separately. However, user engagement should be examined not only within individual sites, but also across sites, that is the entire content provider network. In previous work, we investigated networked user engagement, by defining a global measure of engagement that captures the effect that sites have on the engagement on other sites within the same browsing session. Here, we look at the effect of links on networked user engagement, as these are commonly used by online content providers to increase user engagement.	The effect of links on networked user engagement	NA:NA:NA:NA:NA:NA	2018
Arjumand Younus:Muhammad Atif Qureshi:Suneel Kumar Kingrani:Muhammad Saeed:Nasir Touheed:Colm O'Riordan:Pasi Gabriella	It is often the case that traditional media provide coverage of a news event on the basis of journalists' viewpoints -- a problem termed in the literature as media bias. On the other hand social media have given birth to an alternative paradigm of journalism known as "citizen journalism". We take advantage of citizen journalism to detect the bias in traditional media and propose a simple model for empirical measurement of media bias.	Investigating bias in traditional media through social media	NA:NA:NA:NA:NA:NA:NA	2018
Quan Yuan:Gao Cong:Nadia Magnenat Thalmann	Partly due to the proliferance of microblog, short texts are becoming prominent. A huge number of short texts are generated every day, which calls for a method that can efficiently accommodate new data to incrementally adjust classification models. Naive Bayes meets such a need. We apply several smoothing models to Naive Bayes for question topic classification, as an example of short text classification, and study their performance. The experimental results on a large real question data show that the smoothing methods are able to significantly improve the question classification performance of Naive Bayes. We also study the effect of training data size, and question length on performance.	Enhancing naive bayes with various smoothing methods for short text classification	NA:NA:NA	2018
Erika Yumiya:Atsuyuki Morishima:Masami Takahashi:Shigeo Sugimoto:Hiroyuki Kitagawa	This paper addresses the problem of finding inclusion dependencies on the Web. In our approach, we enumerate pairs of HTML/XML elements that possibly represent inclusion dependencies and then rank the results for verification. This paper focuses on the challenges in the finding and ranking processes.	Filtering and ranking schemes for finding inclusion dependencies on the web	NA:NA:NA:NA:NA	2018
Rong Zhang:ChaoFeng Sha:Minqi Zhou:Aoying Zhou	Analysis to product reviews has attracted great attention from both academia and industry. Generally the evaluation scores of reviews are used to generate the average scores of products and shops for future potential users. However, in the real world, there is the inconsistency problem between the evaluation scores and review content, and some customers do not give out fair reviews. In this work, we focus on detecting the credibility of customers by analyzing online shopping and review behaviors, and then we re-score the reviews for products and shops. In the end, we evaluate our algorithm based on the real data set from Taobao, the biggest E-commerce site in China.	Exploiting shopping and reviewing behavior to re-score online evaluations	NA:NA:NA:NA	2018
Maja Vukovic:Soundar Kumara:Vassilis Kostakos	Welcome to the First International Workshop on Social Web for Disaster Management. While traditionally a handful of news channels report updates, recently citizens and organizations have used multiple communication channels to share, predict, detect, discuss and report on large-scale events. Prime examples are the communications patterns and sharing motifs that emerged shortly after the London underground bombings (cell-phone based updates), Hurricane Katrina (Craigslist), or the Hudson River plane crash (Twitter). Existing approaches to mining public feeds are primarily aimed at searching for specific information or providing general trends within the whole dataset, and crucially post event (e.g. time or location based crowd clustering). To enable effective management of large-scale events, whether natural or economic disasters, there is a need for advances in situational awareness and reduction in response times in disaster management processes. In order to achieve that, a system needs to go beyond using post-event system-generated data, and incorporate both algorithm and crowdsourcing techniques to gather, analyze, organize and then visualize Web data and activity around an event in real-time. We are excited about the variety of papers that are included in the workshop program reflecting the global momentum in research in this domain. The selected works address a number of key topics in this space, such as, making sense of social Web Data in disasters, mining community generated feeds to obtain situational awareness of the events, modeling global events from a variety of Web sources, and Web-platforms for disaster management. These papers present interesting approaches to cross-referencing, summarizing and presenting social Web in consumable manner to address case studies from recent disasters. Position papers in this workshop pave the way for stimulating discussion about future of Social Web and its applications to increased disaster awareness and accelerated response.	Session details: SWDM'12 workshop 1	NA:NA:NA	2018
Cindy Hui:Yulia Tyshchuk:William A. Wallace:Malik Magdon-Ismail:Mark Goldberg	The focus of this paper is on demonstrating how a model of the diffusion of actionable information can be used to study information cascades on Twitter that are in response to an actual crisis event, and its concomitant alerts and warning messages from emergency managers. We will: identify the types of information requested or shared during a crisis situation; show how messages spread among the users on Twitter including what kinds of information cascades or patterns are observed; and note what these patterns tell us about information flow and the users. We conclude by noting that emergency managers can use this information to either facilitate the spreading of accurate information or impede the flow of inaccurate or improper messages.	Information cascades in social media in response to a crisis: a preliminary model and a case study	NA:NA:NA:NA:NA	2018
Miki Enoki:Yohei Ikawa:Raymond Rudy	User community recognition in social media services is important to identify hot topics or users' interests and concerns in a timely way when a disaster has occurred. In microblogging services, many short messages are posted every day and some of them represent replies or forwarded messages between users. We extract such conversational messages to link the users as a user network and regard the strongly-connected components in the network as indicators of user communities. However, using all of the microblog data for user community extraction is too costly and requires too much storage space when decomposing strongly-connected components. In contrast, using sampled data may miss some user connections and thus divide one user community into pieces. In this paper, we propose a method for user community reconstruction using the lexical similarity of the messages and the user's link information between separate communities.	User community reconstruction using sampled microblogging data	NA:NA:NA	2018
Nathan Gnanasambandam:Keith Thompson:Ion Florie Ho:Sarah Lam:Sang Won Yoon	Many useful patterns can be derived from analyzing microblogging behavior at different scales (individual and social group). In this paper, we derive patterns relating to spatio-temporal traffic flow, visit regularity, content and social ties as they relate to an individual's activities in an urban environment (e.g., New York City). We also demonstrate, through an example, methods for reasoning about the activities, locations and group structures that may underlie the microblogging messages in the aforementioned context of mining situation patterns. These individual and group situational patterns may be very crucial when planning for disruptions and organized response.	Towards situational pattern mining from microblogging activity	NA:NA:NA:NA:NA	2018
Liam McNamara:Christian Rohner	In recent disaster events, social media has proven to be an effective communication tool for affected people. The corpus of generated messages contains valuable information about the situation, needs, and locations of victims. We propose an approach to extract significant aspects of user discussions to better inform responders and enable an appropriate response. The methodology combines location based division of users together with standard text mining (term frequency inverse document frequency) to identify important topics of conversation in a dynamic geographic network. We further suggest that both topics and movement patterns change during a disaster, which requires identification of new trends. When applied to an area that has suffered a disaster, this approach can provide 'sensemaking' through insights into where people are located, where they are going and what they communicate when moving.	Mining conversations of geographically changing users	NA:NA	2018
Seema Nagar:Aaditeshwar Seth:Anupam Joshi	Online social networking websites such as Twitter and Facebook often serve a breaking-news role for natural disasters: these websites are among the first ones to mention the news, and because they are visited by millions of users regularly the websites also help communicate the news to a large mass of people. In this paper, we examine how news about these disasters spreads on the social network. In addition to this, we also examine the countries of the Tweeting users. We examine Twitter logs from the 2010 Philippines typhoon, the 2011 Brazil flood and the 2011 Japan earthquake. We find that although news about the disaster may be initiated in multiple places in the social network, it quickly finds a core community that is interested in the disaster, and has little chance to escape the community via social network links alone. We also find evidence that the world at large expresses concern about such largescale disasters, and not just countries geographically proximate to the epicenter of the disaster. Our analysis has implications for the design of fund raising campaigns through social networking websites.	Characterization of social media response to natural disasters	NA:NA:NA	2018
Anurag Singh:Yatindra Nath Singh	Over the Internet or on social networks rumors can spread and can affect the society in disaster. The question one asks about this phenomenon is that whether these rumors can be suppressed using suitable mechanisms. One of the possible solutions is to inoculate a certain fraction of nodes against rumors. The inoculation can be done randomly or in targeted fashion. In this paper, small world network model has been used to investigate the efficiency of inoculation. It has been found that if average degree of small world network is small than both inoculation methods are successful. When average degree is large, neither of these methods are able to stop rumor spreading. But if acceptability of rumor is reduced along with inoculation, the rumor spreading can be stopped even in this case.The proposed hypothesis has been verified using simulation experiments.	Rumor spreading and inoculation of nodes in complex networks	NA:NA	2018
Sungjun Lee:Sangjin Lee:Kwanho Kim:Jonghun Park	In this paper, an approach to automatically identifying bursty events from multiple text streams is presented. We investigate the characteristics of bursty terms that appear in the documents generated from text streams, and incorporate those characteristics into a term weighting scheme that distinguishes bursty terms from other non-bursty terms. Experimental results based on the news corpus show that our approach outperforms the existing alternatives in extracting bursty terms from multiple text streams. The proposed research is expected to contribute to increasing the situational awareness of ongoing events particularly when a natural or economic disaster occurs.	Bursty event detection from text streams for disaster management	NA:NA:NA:NA	2018
Daniela Pohl:Abdelhamid Bouchachia:Hermann Hellwagner	Emergency management is about assessing critical situations, followed by decision making as a key step. Clearly, information is crucial in this two-step process. The technology of social (multi)media turns out to be an interesting source for collecting information about an emergency situation. In particular, situational information can be captured in form of pictures, videos, or text messages. The present paper investigates the application of multimedia metadata to identify the set of sub-events related to an emergency situation. The used metadata is compiled from Flickr and YouTube during an emergency situation, where the identification of the events relies on clustering. Initial results presented in this paper show how social media data can be used to detect different sub-events in a critical situation.	Automatic sub-event detection in emergency management using social media	NA:NA:NA	2018
Yohei Ikawa:Miki Enoki:Michiaki Tatsubori	In order to sense and analyze disaster information from social media, microblogs as sources of social data have recently attracted attention. In this paper, we attempt to discover geolocation information from microblog messages to assess disasters. Since microblog services are more timely compared to other social media, understanding the geolocation information of each microblog message is useful for quickly responding to a sudden disasters. Some microblog services provide a function for adding geolocation information to messages from mobile device equipped with GPS detectors. However, few users use this function, so most messages do not have geolocation information. Therefore, we attempt to discover the location where a message was generated by using its textual content. The proposed method learns associations between a location and its relevant keywords from past messages, and guesses where a new message came from.	Location inference using microblog messages	NA:NA:NA	2018
Ouejdane Mejri:Pierluigi Plebani	The definition of the contingency plan during the preparedness phase holds a crucial role in emergency management. A proper emergency response, indeed, requires the implementation of a contingency plan that can be accurate only if different people with different skills are involved. The goal of this paper is to introduce SocialEMIS, a first prototype of a tool that supports the collaborative definition of contingency plans. Although the current implementation is now focused on the role of the emergency operators, the accuracy of the plan will also take advantage of information coming from the citizens in future releases. Moreover, the contingency plans defined with SocialEMIS represent a knowledge base for defining other contingency plans.	SocialEMIS: improving emergency preparedness through collaboration	NA:NA	2018
Mark A. Cameron:Robert Power:Bella Robinson:Jie Yin	This paper describes ongoing work with the Australian Government to detect, assess, summarise, and report messages of interest for crisis coordination published by Twitter. The developed platform and client tools, collectively termed the Emergency Situation Awareness - Automated Web Text Mining (ESA-AWTM) system, demonstrate how relevant Twitter messages can be identified and utilised to inform the situation awareness of an emergency incident as it unfolds. A description of the ESA-AWTM platform is presented detailing how it may be used for real life emergency management scenarios. These scenarios are focused on general use cases to provide: evidence of pre-incident activity; near-real-time notification of an incident occurring; first-hand reports of incident impacts; and gauging the community response to an emergency warning. Our tools have recently been deployed in a trial for use by crisis coordinators.	Emergency situation awareness from twitter for crisis management	NA:NA:NA:NA	2018
Fan Ye:Raghu Ganti:Raheleh Dimaghani:Keith Grueneberg:Seraphin Calo	In this paper, we propose and develop MECA, a common middleware infrastructure for data collection from mobile devices in an efficient, flexible, and scalable manner. It provides a high level abstraction of phenomenon such that applications can express diverse data needs in a declarative fashion. MECA coordinates the data collection and primitive processing activities, so that data can be shared among applications. It addresses the inefficiency issues in the current vertical integration approach. We showcase the benefits of MECA by means of a disaster management application.	MECA: mobile edge capture and analysis middleware for social sensing applications	NA:NA:NA:NA:NA	2018
Beate Stollberg:Tom de Groeve	The Global Disaster Alert and Coordination System (GDACS) collects near real-time hazard information to provide global multi-hazard disaster alerting for earthquakes, tsunamis, tropical cyclones, floods and volcanoes. GDACS alerts are based on calculations from physical disaster parameters and used by emergency responders. In 2011, the Joint Research Centre (JRC) of the European Commission started exploring if and how social media could be an additional valuable data source for international disaster response. The question is if awareness of the situation after a disaster could be improved by the use of social media tools and data. In order to explore this, JRC developed a Twitter account and Facebook page for the dissemination of GDACS alerts, a Twitter parser for the monitoring of information and a mobile application for information exchange. This paper presents the Twitter parser and the intermediate results of the data analysis which shows that the parsing of Twitter feeds (so-called tweets) can provide important information about side effects of disasters, on the perceived impact of a hazard and on the reaction of the affected population. The most important result is that impact information on collapsed buildings were detected through tweets within the first half an hour after an earthquake occurred and before any mass media reported the collapse.	The use of social media within the global disaster alert and coordination system (GDACS)	NA:NA	2018
Ashlea Bennett Milburn:Clarence L. Wardell	In this paper, we describe a model that can be used to evaluate the impact of using imperfect information when routing supplies for disaster relief. Using two objectives, maximizing the population supported, and minimizing response time, we explore the potential tradeoffs (e.g. more information, but possibly less accurate) of using information from social media streams to inform routing and resource allocation decisions immediately after a disaster.	Evaluating the impact of incorporating information from social media streams in disaster relief routing	NA:NA	2018
Akiko Murakami:Tetsuya Nasukawa	On 11th March 2011, a 9.0-magnitude megathrust earthquake occurred in the ocean near Japan. This was the first large-scale natural disaster in Japan since the broad adoption of social media tools (such as Facebook and Twitter). In particular, Twitter is suitable for broadcasting information, naturally making it the most frequently used social medias when disasters strike. This paper presents a topical analysis using text mining tools and shows the tools' effectiveness for the analysis of social media data analysis after a disaster. Though an ad hoc system without prepared resources was useful, an improved system with some syntactic pattern dictionaries showed better results.	Tweeting about the tsunami?: mining twitter for information on the tohoku earthquake and tsunami	NA:NA	2018
Shosuke Sato:Michiaki Tatsubori:Fumihiko Imamura	In this paper, we outline our analysis of mass media and social media as used for disaster management. We looked at the differences among multiple sub-corpuses to find relatively unique keywords based on chronologies, geographic locations, or media types. We are currently analyzing a massive corpus collected from Internet news sources and Twitter after the Great East Japan Earthquake.	Mass and social media corpus analysis after the 2011 great east Japan earthquake	NA:NA:NA	2018
Julie Dugdale:Bartel Van de Walle:Corinna Koeppinghoff	We describe some first results of an empirical study describing how social media and SMS were used in coordinating humanitarian relief after the Haiti Earthquake in January 2010. Current information systems for crisis management are increasingly incorporating information obtained from citizens transmitted via social media and SMS. This information proves particularly useful at the aggregate level. However it has led to some problems: information overload and processing difficulties, variable speed of information delivery, managing volunteer communities, and the high risk of receiving inaccurate or incorrect information.	Social media and SMS in the haiti earthquake	NA:NA:NA	2018
Michiaki Tatsubori:Hideo Watanabe:Akihiro Shibayama:Shosuke Sato:Fumihiko Imamura	Preserving social Web datasets is a crucial part of research work for disaster management based on information from social media. This paper describes the Michinoku Shinrokuden disaster archive project, mainly dedicated to archiving data from the 2011 Great East Japan Earthquake and its aftermath. Social websites should of course be part of this archive. We discuss issues in archiving social websites for the disaster management research communities and introduce our vision for Michinoku Shinrokuden.	Social web in disaster archives	NA:NA:NA:NA:NA	2018
Sarah Jane Delany:Saurav Sahay:Nirmalie Wiratunga	We are pleased to present the proceedings of the XperienceWeb 2012 workshop, which was held as part of the World Wide Web 2012 Conference in Lyon, April 2012. This workshop has evolved out of the webCBR workshop series, which primarily promotes the application of case-based reasoning (CBR) to experiential content on the web. Clearly such content is on the increase with advances in web technology resulting in vast amounts of user generated web content in the form of blogs, tweets, forums and opinion sites. Whilst such content can be treated as documents we take the view that they are more appropriately viewed as a rich source of untapped experience data, a valuable asset that can be used to generate Web experience bases through CBR technology. The key idea in CBR involves the reuse of similar experiences to resolve new problems. Such reasoning is relevant to the web because increasingly people search and browse other people's experiences on travel, medicine, retail, entertainment, etc., for their informational and problem solving needs. Typically this vast repository of untapped knowledge and experience exists in unstructured text and multimedia forms. These rich informational snippets generally describe a particular problem situation often combined with a reflective narrative consisting of decisions made, lessons learnt and opinions expressed. Such content forms a rich source of untapped experience data - an integral resource for problem solving with CBR. For example, if one wants to identify web content that helps them to achieve a particular task (say software configuration and installation), then it is likely that there will be many related user posts describing experiences on similar tasks and crucially include suitable solutions to address such a task. We therefore envisage a new form of user need, one that possibly embodies knowledge, meaning and understanding as opposed to chunks of keywords for queries and snippets. Addressing this need calls for novel extraction, representation, retrieval and assembly strategies from existing experiential content on the web. The focus of this workshop was to provide a forum for the discussion of trends, research issues and practical experiences on the role of tools and technologies in reasoning with web-related experiential content. Six papers were accepted for the workshop, covering a wide range of web experience data. Adekyanju et al. present an approach for combining knowledge from web data and search logs for query recommendation. Kato et al. propose a method for extracting onomatopoeia from restaurant reviews and using these in a restaurant recommender. Schumacher et al. compare two methods for extracting procedural or 'how-to' knowledge from the web and Zarka et al. look at the potential for experiential approaches to recommendation to enable a more context-aware method of generating useful video recommendations, particularly in relation to temporal context. Two of the accepted papers are position papers. The question of how user-generated content can be mined successfully for solutions to specific problems is discussed by Sauer & Roth-Berghofer while Gorg et al. consider the problem of constructing and managing personal workflows over a social network.	Session details: XperienceWeb'12 Workshop 2	NA:NA:NA	2018
Ayumi Kato:Yusuke Fukazawa:Tomomasa Sato:Taketoshi Mori	Onomatopoeia is widely used in food reviews about food or restaurants. In this paper, we propose and evaluate a method to extract onomatopoeia including unknown ones automatically from food reviews sites. From the evaluation result, we found that we can extract onomatopoeia for specific foods with more than 46% precision; we find 18 unknown onomatopoeia, i.e. not registered in an existing onomatopoeia dictionary, in 62 extracted onomatopoeia. In addition, we propose a system that can present the user with a list of onomatopoeia specific to a restaurant she is interested in. The evaluation results indicate that an intuitive restaurant search can be done via a list of onomatopoeia, and that they are helpful for selecting food or restaurants.	Extraction of onomatopoeia used for foods from food reviews and its application to restaurant search	NA:NA:NA:NA	2018
Christian Severin Sauer:Thomas Roth-Berghofer	In this paper we describe the task of automated mining for solutions to highly specific problems. We do so under the premise of mapping the split view on context, introduced by Brézillon and Pomerol, onto three different levels of abstraction of a problem domain. This is done to integrate the notion of activity or focus and its influence on the context into the mining for a solution. We assume that a problem's context describes key characteristics to be decisive criteria in the mining process to mine successful solutions for it. We further detail on the process of a chain of sub problems and their foci adding up to a meta problem solution and how this can used to mine for such solutions. Through a guiding example we introduce basic steps of the solution mining process and common aspects we deem interesting to be analysed closer in upcoming research on solution mining. We further examine the possible integration of these newly established outlines for automatic solution mining for highly specific problems into a SEASALTexp, a currently developed architecture for explanation-aware extraction and case-based processing of experiences from Internet communities. We thereby gained first insights in issues occurring while trying to integrate automatic solution mining.	Solution mining for specific contextualised problems: towards an approach for experience mining	NA:NA	2018
Pol Schumacher:Mirjam Minor:Kirstin Walter:Ralph Bergmann	User generated Web content includes large amounts of procedural knowledge (also called how to knowledge). This paper is on a comparison of two extraction methods for procedural knowledge from the Web. Both methods create workflow representations automatically from text with the aim to reuse the Web experience by reasoning methods. Two variants of the workflow extraction process are introduced and evaluated by experiments with cooking recipes as a sample domain. The first variant is a term-based approach that integrates standard information extraction methods from the GATE system. The second variant is a frame-based approach that is implemented by means of the SUNDANCE system. The expert assessment of the extraction results clearly shows that the more sophisticated frame-based approach outperforms the term-based approach of automated workflow extraction.	Extraction of procedural knowledge from the web: a comparison of two workflow extraction approaches	NA:NA:NA:NA	2018
Sebastian Görg:Ralph Bergmann:Mirjam Minor:Sarah Gessinger:Siblee Islam	We propose a personal workflow management service as part of a social network that enables private users to construct personal workflows according to their specific needs and to keep track of the workflow execution. Unlike traditional workflows, such personal workflows aim at supporting processes that contain personal tasks and data. Our proposal includes a process-oriented case-based reasoning approach to support private users to obtain an appropriate personal workflow through sharing and reuse of respective experience.	Collecting, reusing and executing private workflows on social network platforms	NA:NA:NA:NA:NA	2018
Raafat Zarka:Amélie Cordier:Elöd Egyed-Zsigmond:Alain Mille	People like creating their own videos by mixing various contents. Many applications allow us to generate video clips by merging different media like videos clips, photos, text and sounds. Some of these applications enable us to combine online content with our own resources. Given the large amount of content available, the problem is to quickly find content that truly meet our needs. This is when recommender systems come in. In this paper, we propose an approach for contextual video recommendations based on a Trace-Based Reasoning approach.	Contextual trace-based video recommendations	NA:NA:NA:NA	2018
Ibrahim Adepoju Adeyanju:Dawei Song:M-Dyaa Albakour:Udo Kruschwitz:Anne De Roeck:Maria Fasli	Query recommendation is becoming a common feature of web search engines especially those for Intranets where the context is more restrictive. This is because of its utility for supporting users to find relevant information in less time by using the most suitable query terms. Selection of queries for recommendation is typically done by mining web documents or search logs of previous users. We propose the integration of these approaches by combining two models namely the concept hierarchy, typically built from an Intranet's documents, and the query flow graph, typically built from search logs. However, we build our concept hierarchy model from terms extracted from a subset (training set) of search logs since these are more representative of the user view of the domain than any concepts extracted from the collection. We then continually adapt the model by incorporating query refinements from another subset (test set) of the user search logs. This process implies learning from or reusing previous users' querying experience to recommend queries for a new but similar user query. The adaptation weights are extracted from a query flow graph built with the same logs. We evaluated our hybrid model using documents crawled from the Intranet of an academic institution and its search logs. The hybrid model was then compared to a concept hierarchy model and query flow graph built from the same collection and search logs respectively. We also tested various strategies for combining information in the search logs with respect to the frequency of clicked documents after query refinement. Our hybrid model significantly outperformed the concept hierarchy model and query flow graph when tested over two different periods of the academic year. We intend to further validate our experiments with documents and search logs from another institution and devise better strategies for selecting queries for recommendation from the hybrid model.	Learning from users' querying experience on intranets	NA:NA:NA:NA:NA:NA	2018
Eugene Agichtein:Gideon Dror:Yoelle Maarek	We are delighted to welcome you to CQA 2012, the first Community Question Answering (CQA) workshop, under the umbrella of the WWW 2012 conference held in Lyon, France. Background: Despite the continuous progress of Web search engines in the last 20 years, many users' needs still remain unanswered. Two common reasons for queries not being satisfied are (1) the intent behind the query not being well expressed, and (2) the absence of relevant content. Subjective or narrow needs, for which content was not created before the need was expressed, will keep appearing no matter how Web search engines progress. This innate limitation of web search engines has led to the emergence of Community Question Answering (CQA) services in the last decade, such as Yahoo! Answers, Baidu Zhidao, Quora, Facebook Questions, and many topic-specific forums such as Stack Overflow. These services are designed to help users obtain information from a community, offering a wide variety of approaches (e.g., broadcasting the question to the whole community, just directed to friends, or specifically targeted to topic experts). Most importantly, these sites serve an active community of millions of users, and continue generating a great deal of popular Web content. Goals: While CQA services have been active for almost a decade, and are a popular subject of multiple research studies and publications in a variety of Web-related conferences, they have not, to the best of our knowledge, been a focus of a dedicated workshop in a top conference. The goal of this workshop was to bring together, for the first time, researchers and practitioners from various areas working on CQA. This workshop was specifically designed for the broad WWW audience. Yet, our goal was to also conduct a more focused discussion, and to consider the latest work in progress in the area of Community Question Answering. Process: We issued a call for papers and requested the assistance of a Program Committee of 11 renowned experts to carefully review the paper submissions. They are listed on the next page. We are deeply grateful to all of them for their thorough and insightful reviews. Themes: The themes of our workshop call for papers were diverse and included topics such as question routing, question recommendation, spam and abuse detection techniques, personalization and recommendation, user modeling, monetization, and connections of CQA with search, evaluation, and social network research. The papers that were accepted by our Program Committee could be clustered around 4 main areas: (1) quality (2) routing (3) discovery and (4) users. In addition, Sep Kamvar, Associate Professor of Media Arts and Sciences at MIT, and the Director of the Social Computing Group at the MIT Media Lab, agreed to open the workshop as our keynote speaker and discuss Language, Thought and Community Question Answering. We hope that you will find the workshop interesting and thought provoking. Our goal was not only to provide the opportunity for researchers and practitioners in the Web community to share ideas, but also to foster new research and innovation in the fascinating field of CQA. The Official Web site of the workshop is http://research.yahoo.com/workshops/cqa2012	Session details: CQA'12 workshop 3	NA:NA:NA	2018
Zhi-Min Zhou:Man Lan:Zheng-Yu Niu:Yue Lu	Answer ranking is very important for cQA services due to the high variance in the quality of answers. Most existing works in this area focus on using various features or employing machine learning techniques to address this problem. Only a few of them noticed and involved user profile information in this particular task. In this work, we assume the close relationship between user profile information and the quality of their answers under the ground truth that user information records the user behaviors and histories as a summary. Thus, we exploited the effectiveness of three categories of user profile information, i.e. engagement-related, authority-related and level-related, on answer ranking in cQA. Different from previous work, we only employed the information which is easy to extract without any limitations, such as user privacy. Experimental results on Yahoo! Answers manner questions showed that our system by using the user profile information achieved comparable or even better results over the state-of-the-art baseline system. Moreover, we found that the picture existence of a user in cQA community contributed more than other information in the answer ranking task.	Exploiting user profile information for answer ranking in cQA	NA:NA:NA:NA	2018
Baichuan Li:Tan Jin:Michael R. Lyu:Irwin King:Barley Mak	Users tend to ask and answer questions in community question answering (CQA) services to seek information and share knowledge. A corollary is that myriad of questions and answers appear in CQA service. Accordingly, volumes of studies have been taken to explore the answer quality so as to provide a preliminary screening for better answers. However, to our knowledge, less attention has so far been paid to question quality in CQA. Knowing question quality provides us with finding and recommending good questions together with identifying bad ones which hinder the CQA service. In this paper, we are conducting two studies to investigate the question quality issue. The first study analyzes the factors of question quality and finds that the interaction between askers and topics results in the differences of question quality. Based on this finding, in the second study we propose a Mutual Reinforcement-based Label Propagation (MRLP) algorithm to predict question quality. We experiment with Yahoo!~Answers data and the results demonstrate the effectiveness of our algorithm in distinguishing high-quality questions from low-quality ones.	Analyzing and predicting question quality in community question answering services	NA:NA:NA:NA:NA	2018
Tom Chao Zhou:Michael R. Lyu:Irwin King	Community-based Question and Answering (CQA) services have brought users to a new era of knowledge dissemination by allowing users to ask questions and to answer other users' questions. However, due to the fast increasing of posted questions and the lack of an effective way to find interesting questions, there is a serious gap between posted questions and potential answerers. This gap may degrade a CQA service's performance as well as reduce users' loyalty to the system. To bridge the gap, we present a new approach to Question Routing, which aims at routing questions to participants who are likely to provide answers. We consider the problem of question routing as a classification task, and develop a variety of local and global features which capture different aspects of questions, users, and their relations. Our experimental results obtained from an evaluation over the Yahoo!~Answers dataset demonstrate high feasibility of question routing. We also perform a systematical comparison on how different types of features contribute to the final results and show that question-user relationship features play a key role in improving the overall performance.	A classification-based approach to question routing in community question answering	NA:NA:NA	2018
Fatemeh Riahi:Zainab Zolaktaf:Mahdi Shafiei:Evangelos Milios	Community Question Answering (CQA) websites provide a rapidly growing source of information in many areas. This rapid growth, while offering new opportunities, puts forward new challenges. In most CQA implementations there is little effort in directing new questions to the right group of experts. This means that experts are not provided with questions matching their expertise, and therefore new matching questions may be missed and not receive a proper answer. We focus on finding experts for a newly posted question. We investigate the suitability of two statistical topic models for solving this issue and compare these methods against more traditional Information Retrieval approaches. We show that for a dataset constructed from the Stackoverflow website, these topic models outperform other methods in retrieving a candidate set of best experts for a question. We also show that the Segmented Topic Model gives consistently better performance compared to the Latent Dirichlet Allocation Model.	Finding expert users in community question answering	NA:NA:NA:NA	2018
Atsushi Otsuka:Yohei Seki:Noriko Kando:Tetsuji Satoh	Recently, query suggestions have become quite useful in web searches. Most provide additional and correct terms based on the initial query entered by users. However, query suggestions often recommend queries that differ from the user's search intentions due to different contexts. In such cases, faceted query expansions and their usages are quite efficient. In this paper, we propose faceted query expansion methods using the resources of Community Question Answering (CQA), which is social network service (SNS) that shares user knowledge. In a CQA site, users can post questions in a suitable category. Others answer them based on the category framework. Thus, the CQA "category" makes a "facet" of the query expansion. In addition, the time of year when the question was posted plays an important role in understanding its context. Thus, such seasonality creates another "facet" of the query expansion. We implement two-dimensional faceted query expansion methods based on the results of the Latent Dirichlet Allocation (LDA) analysis of CQA resources. The question articles deriving query expansion are provided for choosing appropriate terms by users. Our sophisticated evaluations using actual and long-term CQA resources, such as "Yahoo! CHIEBUKURO," demonstrate that most parts of the CQA questions are posted in periodicity and in bursts.	QAque: faceted query expansion techniques for exploratory search using community QA resources	NA:NA:NA:NA	2018
Saurav Sahay:Ashwin Ram	We develop an innovative approach to delivering relevant information using a combination of socio-semantic search and filtering approaches. The goal is to facilitate timely and relevant information access through the medium of conversations by mixing past community specific conversational knowledge and web information access to recommend and connect users and information together. Conversational Information Access is a socio-semantic search and recommendation activity with the goal to interactively engage people in conversations by receiving agent supported recommendations. It is useful because people engage in online social discussions unlike solitary search; the agent brings in relevant information as well as identifies relevant users; participants provide feedback during the conversation that the agent uses to improve its recommendations.	Socio-semantic conversational information access	NA:NA	2018
Giovanni Gardelli:Ingmar Weber	We use Yahoo!~Toolbar data to gain insights into why people use Q&A sites. For this purpose we look at tens of thousands of questions asked on both Yahoo!~Answers and on Wiki Answers. We analyze both the pre-question behavior of users as well as their general online behavior. Using an existing approach (Harper et al.), we classify questions into "informational" vs. "conversational". Finally, for a subset of users on Yahoo! Answers we also integrate age and gender into our analysis. Our results indicate that there is a one-dimensional spectrum of users ranging from "social users" to "informational users". In terms of demographics, we found that both younger and female users are more "social" on this scale, with older and male users being more "informational". Concerning the pre-question behavior, users who first issue a question-related query, and especially those who do not click any web results, are more likely to issue informational questions than users who do not search before. Questions asked shortly after the registration of a new user on Yahoo! Answers tend to be social and have a lower probability of being preceded by a web search than other questions. Finally, we observed evidence both for and against topical congruence between a user's questions and his web queries.	Why do you ask this?	NA:NA	2018
Long Chen:Dell Zhang:Levene Mark	Community Question Answering (CQA) services, such as Yahoo! Answers, are specifically designed to address the innate limitation of Web search engines by helping users obtain information from a community. Understanding the user intent of questions would enable a CQA system identify similar questions, find relevant answers, and recommend potential answerers more effectively and efficiently. In this paper, we propose to classify questions into three categories according to their underlying user intent: subjective, objective, and social. In order to identify the user intent of a new question, we build a predictive model through machine learning based on both text and metadata features. Our investigation reveals that these two types of features are conditionally independent and each of them is sufficient for prediction. Therefore they can be exploited as two views in co-training - a semi-supervised learning framework - to make use of a large amount of unlabelled questions, in addition to the small set of manually labelled questions, for enhanced question classification. The preliminary experimental results show that co-training works significantly better than simply pooling these two types of features together.	Understanding user intent in community question answering	NA:NA:NA	2018
Gideon Dror:Dan Pelleg:Oleg Rokhlenko:Idan Szpektor	One of the important targets of community-based question answering (CQA) services, such as Yahoo! Answers, Quora and Baidu Zhidao, is to maintain and even increase the number of active answerers, that is the users who provide answers to open questions. The reasoning is that they are the engine behind satisfied askers, which is the overall goal behind CQA. Yet, this task is not an easy one. Indeed, our empirical observation shows that many users provide just one or two answers and then leave. In this work we try to detect answerers that are about to quit, a task known as churn prediction, but unlike prior work, we focus on new users. To address the task of churn prediction in new users, we extract a variety of features to model the behavior of \YA{} users over the first week of their activity, including personal information, rate of activity, and social interaction with other users. Several classifiers trained on the data show that there is a statistically significant signal for discriminating between users who are likely to churn and those who are not. A detailed feature analysis shows that the two most important signals are the total number of answers given by the user, closely related to the motivation of the user, and attributes related to the amount of recognition given to the user, measured in counts of best answers, thumbs up and positive responses by the asker.	Churn prediction in new users of Yahoo! answers	NA:NA:NA:NA	2018
Romain Vuillemot:Michal Laclavík:Vitor R. Carvalho	The International Workshop Email 2012 on Messaging and Web of Data: Private meets Public (and vice-versa) was held on April 16th 2012 at 21st World Wide Web Conference (WWW 2012) in Lyon, France. The Email 2012 workshop brought together researchers and practitioners working on improving email and messaging, and its interconnection with web of data. The growing amount of public data available on the WWW is a new opportunity to improve Messaging systems (e.g. Email or Social Media) in multiple forms, such as message content, processing and presentation. This workshop is dedicated to exploring how public web data, such as identities, agendas, LinkedData, social networks or various information published on the web can meet private messaging data so as to bring new insight for users, and prevent errors or abuse. Reciprocally, messages can become public (e.g. public email archives, leaked email datasets, Twitter timelines), but have to implement web standards to be efficiently identified, distributed and linked. This private/public duality and potential versatility, applied to messages in general is the basis of this workshop, which goes beyond technical aspects and aims at exploring impacts on users' practices, interfaces and trust. The workshop is built on previous successful email workshops: Next Trends in Email (NextMail'11); Emails in e-Commerce and Enterprise Context (E3C 2009); and Enhanced Messaging (EMAIL-2008). Workshop outcome: The call for papers solicited original email research papers and short position/experience papers including posters and interactive demonstration submissions. In total, the workshop received seven submissions. Two of them have been withdrawn and five have been valid papers matching with the workshop topics. Each paper was independently peer reviewed by at least three members of the international program committee. Four papers were accepted: two full papers and two short papers. Full papers: The first paper, Context-sensitive Business Process Support Based On Emails, is from Thomas Burkhart, Dirk Werth and Peter Loos and presents finalized work on handling flexible business processes within the email. The second paper, Emails as Graph: Relation Discovery in Email Archive, by Michal Laclavík, Marek Ciglan, Štefan Dlugolinský, Martin Šeleng and Ladislav Hluchý discusses representation of email archive as a graph or information network and entity relation discovery within such data structures. Short papers: The contribution from Vojtech Juhasz titled Full-text search in email archives using social evaluation presents work in progress focusing on new approach for email search on the level of email parts including linked web sources or attachments, and benefits of email social networks. The other short paper from Gaëlle Recourcé titled Interpreting Contact Details out of E-mail Signature Blocks discusses industrial perspective and application of email contacts extraction. Invited talk: We have also invited Uwe V. Riss from SAP Research with a talk eminently relevant to the topic of the workshop: Email between private use and organizational purpose.	Session details: EMAIL'12 workshop 4	NA:NA:NA	2018
Uwe V. Riss	Emails have become an eminent source of personal and organizational information. They are not only used for personal communication but also for the management of information and the coordination of activities within organizations. Email traffic also exhibits the social networks existing in organizations. However, the central problem, which we still face, is how to tap this rich source appropriately. Main problems in this respect are the personal character of emails (their privacy) and the mainly unstructured character of their contents. Since these two features are essential success factors for the use of email they cannot be simply ignored. Meanwhile there are various approaches to recover the hidden treasure and make the contained information available to information and process management. For example, semantic or mining technologies play a prominent role in this attempt. The paper gives an overview of different strategies to make organizational use of emails, also touching the role of privacy.	Email between private use and organizational purpose	NA	2018
Michal Laclavík:Štefan Dlugolinský:Martin Šeleng:Marek Ciglan:Ladislav Hluchý	In this paper, we present an approach for representing an email archive in the form of a network, capturing the communication among users and relations among the entities extracted from the textual part of the email messages. We showcase the method on the Enron email corpus, from which we extract various entities and a social network. The extracted named entities (NE), such as people, email addresses and telephone numbers, are organized in a graph along with the emails in which they were found. The edges in the graph indicate relations between NEs and represent a co-occurrence in the same email part, paragraph, sentence or a composite NE. We study mathematical properties of the graphs so created and describe our hands-on experience with the processing of such structures. Enron Graph corpus contains a few million nodes and is large enough for experimenting with various graph-querying techniques, e.g. graph traversal or spread of activation. Due to its size, the exploitation of traditional graph processing libraries might be problematic as they keep the whole structure in the memory. We describe our experience with the management of such data and with the relation discovery among the extracted entities. The described experience might be valuable for practitioners and highlights several research challenges.	Emails as graph: relation discovery in email archive	NA:NA:NA:NA:NA	2018
Gaëlle Recourcé	This paper describes a fully automated process of address book enrichment by means of information extraction in e-mail signature blocks. The main issues we tackle are signature block detection, named entites tagging, mapping with a specific person, standardizing the details and auto-updating of the address book. We adopted a symbolic approach for NLP modules. We describe how the process was designed to handle multiple-type of errors (human or computer-driven) while aiming at 100% precision rate. Last, we tackle the question of automatic updating confronted to users rights over their own data.	Interpreting contact details out of e-mail signature blocks	NA	2018
Thomas Burkhart:Dirk Werth:Peter Loos	In many companies, a majority of business processes take place via email communication. Large enterprises have the possibility to operate enterprise systems for a successful business process management. However, these systems are not appropriate for SMEs, which are the most common enterprise type in Europe. Thus, the European research project Commius addresses the special needs of SMEs and characteristics of email communication, namely highly flexibility and unstructuredness. Commius turns the existing email-system into a structured process management framework. Each incoming email is autonomously matched to the corresponding business process and enhanced by proactive annotations. These context-sensitive annotations include recommendations for the most suitable following process steps. An underlying, self-adjusting recommendation model ensures most appropriate recommendations by observing the actual user behavior. This implies that the proposed process course is in no way obligatory. To provide a high degree of flexibility, any deviation from the given process structure is allowed.	Context-sensitive business process support based on emails	NA:NA:NA	2018
Vojtech Juhász	Emails are important tools for communication and cooperation, they contain large amount of information and connections to knowledge and data sources. Because of this, it is very important to improve the efficiency of their processing. This paper describes an email search system which integrates full-text search with social search while processing also the attached and linked resources. The project described in this paper is still in progress. Due to this fact, some proposed parts of the system are not implemented and also not proven yet. The proposed equation for determining the social importance of an email has also to be tuned during the last phases of the development and the evaluation phase. The already implemented part of the system includes content extraction from the email messages, attached and linked resources and also the textual search and social relation extraction is implemented. The next phase of the development includes tuning of the social evaluation and it's integration with textual search.	Full-text search in email archives using social evaluation, attached and linked resources	NA	2018
Markus Schedl:Peter Knees:Òscar Celma	Music information research (MIR) has been a fast growing field of research during the past decade. In traditional MIR, music-related information were extracted from the audio signal using signal processing techniques. These methods, however, cannot capture semantic information that is not encoded in the audio signal, but nonetheless essential to many consumers, e.g., the meaning of the lyrics of a song or the political motivation or background of a singer. The recent launches of Google Music, Amazon.com's Cloud Player and Apple's iCloud with iTunes' Scan and Match show the huge commercial interest behind music distribution and consumption. Given the fact that music is an omnipresent topic on the Web, techniques to mine the Web of Music are vital for music information research. In recent years, the emergence of various Web 2.0 platforms and services dedicated or related to the music and audio domain, like last.fm, YouTube, MusicBrainz, Pandora, or Echo Nest, has been providing novel and powerful, albeit noisy, sources for high level, semantic information on music artists, albums, songs, and others. The abundance of such information provided by the power of the crowd can therefore contribute to music information research and development considerably. On the other hand, the wealth of newly available, semantically meaningful information offered on Web 2.0 platforms also poses new challenges, e.g., dealing with the huge amount and the noisiness of this kind of data, various user biases, hacking, or the cold start problem. Another recent trend are innovative user interfaces to access the large amounts of music available on smart mobile devices that are always connected to the Web. Dealing with the vast amounts of music requires new interaction paradigms and intelligent services that provide, for example, personalized and context-aware music recommendations. The current emergence and confluence of these challenges make this an interesting field for researchers and industry practitioners alike. Addressing these challenges, the International Workshop on Advances in Music Information Research (AdMIRe) workshop served as a forum for theoretical and practical discussions of cutting edge research in the fields of music information extraction, retrieval, and recommendation in the Web of Music. The workshop brought together researchers and developers from the music and audio community, the multimedia community, and the Web mining community and further initiated interesting and fruitful discussions on the future of MIR. This year was AdMIRe's 4th edition, and we received a considerable number of high-quality submissions. A total of 14 submissions from 7 different countries have been reviewed by at least 4 reviewers each, and 9 papers have finally been accepted for publication and presentation at the workshop. They span a wide field, including topics such as Web-based data and microblogging mining, content-based music processing, music similarity and recommendation from hybrid sources, and new evaluation methods.	Session details: AdMIRe'12 workshop 6	NA:NA:NA	2018
Francesco Ricci	NA	Context-aware music recommender systems: workshop keynote abstract	NA	2018
Xavier Serra	In this paper we describe the data gathering work done within a large research project, CompMusic, which emphasizes a culture specific approach in the automatic description of several world music repertoires. Currently we are focusing on the Hindustani (North India), Carnatic (South India) and Turkish-makam (Turkey) music traditions. The selection and organization of the data to be processed for the characterization of each of these traditions is of the utmost importance.	Data gathering for a culture specific approach in MIR	NA	2018
Yi-Hsuan Yang:Dmitry Bogdanov:Perfecto Herrera:Mohamed Sordo	The emergence of social tagging websites such as Last.fm has provided new opportunities for learning computational models that automatically tag music. Researchers typically obtain music tags from the Internet and use them to construct machine learning models. Nevertheless, such tags are usually noisy and sparse. In this paper, we present a preliminary study that aims at refining (retagging) social tags by exploiting the content similarity between tracks and the semantic redundancy of the track-tag matrix. The evaluated algorithms include a graph-based label propagation method that is often used in semi-supervised learning and a robust principal component analysis (PCA) algorithm that has led to state-of-the-art results in matrix completion. The results indicate that robust PCA with content similarity constraint is particularly effective; it improves the robustness of tagging against three types of synthetic errors and boosts the recall rate of music auto-tagging by 7% in a real-world setting.	Music retagging using label propagation and robust principal component analysis	NA:NA:NA:NA	2018
Markus Schedl:David Hauger	This paper aims at leveraging microblogs to address two challenges in music information retrieval (MIR), similarity estimation between music artists and inferring typical listening patterns at different granularity levels (city, country, global). From two collections of several million microblogs, which we gathered over ten months, music-related information is extracted and statistically analyzed. We propose and evaluate four co-occurrence-based methods to compute artist similarity scores. Moreover, we derive and analyze culture-specific music listening patterns to investigate the diversity of listening behavior around the world.	Mining microblogs to infer music artist similarity and cultural listening patterns	NA:NA	2018
Justin Salamon:Joan Serrà:Emilia Gómez	In this paper we compare the use of different musical representations for the task of version identification (i.e. retrieving alternative performances of the same musical piece). We automatically compute descriptors representing the melody and bass line using a state-of-the-art melody extraction algorithm, and compare them to a harmony-based descriptor. The similarity of descriptor sequences is computed using a dynamic programming algorithm based on nonlinear time series analysis which has been successfully used for version identification with harmony descriptors. After evaluating the accuracy of individual descriptors, we assess whether performance can be improved by descriptor fusion, for which we apply a classification approach, comparing different classification algorithms. We show that both melody and bass line descriptors carry useful information for version identification, and that combining them increases version detection accuracy. Whilst harmony remains the most reliable musical representation for version identification, we demonstrate how in some cases performance can be improved by combining it with melody and bass line descriptions. Finally, we identify some of the limitations of the proposed descriptor fusion approach, and discuss directions for future research.	Melody, bass line, and harmony representations for music version identification	NA:NA:NA	2018
Martín Haro:Joan Serrà:Álvaro Corral:Perfecto Herrera	Many sound-related applications use Mel-Frequency Cepstral Coefficients (MFCC) to describe audio timbral content. Most of the research efforts dealing with MFCCs have been focused on the study of different classification and clustering algorithms, the use of complementary audio descriptors, or the effect of different distance measures. The goal of this paper is to focus on the statistical properties of the MFCC descriptor itself. For that purpose, we use a simple encoding process that maps a short-time MFCC vector to a dictionary of binary code-words. We study and characterize the rank-frequency distribution of such MFCC code-words, considering speech, music, and environmental sound sources. We show that, regardless of the sound source, MFCC code-words follow a shifted power-law distribution. This implies that there are a few code-words that occur very frequently and many that happen rarely. We also observe that the inner structure of the most frequent code-words has characteristic patterns. For instance, close MFCC coefficients tend to have similar quantization values in the case of music signals. Finally, we study the rank-frequency distributions of individual music recordings and show that they present the same type of heavy-tailed distribution as found in the large-scale databases. This fact is exploited in two supervised semantic inference tasks: genre and instrument classification. In particular, we obtain similar classification results as the ones obtained by considering all frames in the recordings by just using 50 (properly selected) frames. Beyond this particular example, we believe that the fact that MFCC frames follow a power-law distribution could potentially have important implications for future audio-based applications.	Power-law distribution in encoded MFCC frames of speech, music, and environmental sound signals	NA:NA:NA:NA	2018
Andrew Hankinson:John Ashley Burgoyne:Gabriel Vigliensoni:Ichiro Fujinaga	In this paper we present our work towards developing a large-scale web application for digitizing, recognizing (via optical music recognition), correcting, displaying, and searching printed music texts. We present the results of a recently completed prototype implementation of our workflow process, from document capture to presentation on the web. We discuss a number of lessons learned from this prototype. Finally, we present some open-source Web 2.0 tools developed to provide essential infrastructure components for making searchable printed music collections available online. Our hope is that these experiences and tools will help in creating next-generation globally accessible digital music libraries.	Creating a large-scale searchable digital collection from printed music materials	NA:NA:NA:NA	2018
Brian McFee:Thierry Bertin-Mahieux:Daniel P.W. Ellis:Gert R.G. Lanckriet	We introduce the Million Song Dataset Challenge: a large-scale, personalized music recommendation challenge, where the goal is to predict the songs that a user will listen to, given both the user's listening history and full information (including meta-data and content analysis) for all songs. We explain the taste profile data, our goals and design choices in creating the challenge, and present baseline results using simple, off-the-shelf recommendation algorithms.	The million song dataset challenge	NA:NA:NA:NA	2018
Julián Urbano:Markus Schedl	Reliable evaluation of Information Retrieval systems requires large amounts of relevance judgments. Making these annotations is quite complex and tedious for many Music Information Retrieval tasks, so performing such evaluations requires too much effort. A low-cost alternative is the application of Minimal Test Collection algorithms, which offer quite reliable results while significantly reducing the annotation effort. The idea is to incrementally select what documents to judge so that we can compute estimates of the effectiveness differences between systems with a certain degree of confidence. In this paper we show a first approach towards its application to the evaluation of the Audio Music Similarity and Retrieval task, run by the annual MIREX evaluation campaign. An analysis with the MIREX 2011 data shows that the judging effort can be reduced to about 35% to obtain results with 95% confidence.	Towards minimal test collections for evaluation of audio music similarity and retrieval	NA:NA	2018
Marcos Aurélio Domingues:Fabien Gouyon:Alípio Mário Jorge:José Paulo Leal:João Vinagre:Luís Lemos:Mohamed Sordo	In this paper we propose a hybrid music recommender system, which combines usage and content data. We describe an online evaluation experiment performed in real time on a commercial music web site, specialised in content from the very long tail of music content. We compare it against two stand-alone recommenders, the first system based on usage and the second one based on content data. The results show that the proposed hybrid recommender shows advantages with respect to usage- and content-based systems, namely, higher user absolute acceptance rate, higher user activity rate and higher user loyalty.	Combining usage and content in an online music recommendation system for music in the long-tail	NA:NA:NA:NA:NA:NA:NA	2018
Daniel Wolff:Tillman Weyde	Predicting user's tastes on music has become crucial for a competitive music recommendation systems, and perceived similarity plays an influential role in this. MIR currently turns towards making recommendation systems adaptive to user preferences and context. Here, we consider the particular task of adapting music similarity measures to user voting data. This work builds on and responds to previous publications based on the MagnaTagATune dataset. We have reproduced the similarity dataset presented by Stober and Nürnberger at AMR 2011 to enable a comparison of approaches. On this dataset, we compare their two-level approach, defining similarity measures on individual facets and combining them in a linear model, to the Metric Learning to Rank (MLR) algorithm. MLR adapts a similarity measure that operates directly on low-level features to the user data. We compare the different algorithms, features and parameter spaces with regards to minimising constraint violations. Furthermore, the effectiveness of the MLR algorithm in generalising to unknown data is evaluated on this dataset. We also explore the effects of feature choice. Here, we find that the binary genre data shows little correlation with the similarity data, but combined with audio features it clearly improves generalisation.	Adapting similarity on the MagnaTagATune database: effects of model and feature choices	NA:NA	2018
Shlomo Berkovsky:Elöd Egyed-Zsigmond:Geert-Jan Houben	You are about to read the proceedings of the International Workshop on Interoperability of User Profiles in Multi-Application Web Environments (MultiA-Pro 2012). Nowadays, many different websites and applications in different areas (elearning, digital libraries, search engines, e-commerce, social networks, ubiquitous computing, web of things) are gathering proprietary user profiles. These are gathered for the purpose of the subsequent provision of adaptive and personalized services to users. Although beneficial and having the potential to upgrade the personalized services provided to users, very little exchange of the gathered user profile information practically occurs between these sites and applications. This can be explained by various commercial and legal limitations. Firstly, commercial competition between companies prevents them from sharing the gathered user profiles. For example, e-commerce services like Amazon and eBay are primarily interested to improve their own service and refrain from establishing an interoperable environment and sharing their own profiles with the rival service. Secondly, many countries impose very stringent legal restrictions on the gathering and storage of personal user information and prohibit such a sharing. Meanwhile, for individual users, it is getting more and more difficult to manage in a secure and practical way their ever multiplying web identities. Overcoming these limitations and developing solutions and standards for multi-application user profiling and interoperability of user modeling and personalization applications is timely and important. This can allow future online service providers to gather richer and more accurate user profiles, and, as a result, facilitate the provision of high-quality personalized services to their users. Providing a standardized, transparent, usercentric and secured means for gathering, modeling, and management of user profiles would facilitate the adoption of user profile data interoperability and sharing in web-based user modeling and personalization applications. The aim of this workshop is to organize a common discussion among scientific and industrial research communities on the challenges evolving around multi-application user profiling and personalization on the web. The main objective is to set new standards and a strategic research agenda for researchers and web technology developers for aspects related to interoperability of user modeling and personalization systems. We received interesting papers, covering several aspects of user profile interoperability, and we are looking forward to a rich and constructive exchange around this topic.	Session details: MultiAPro'12 workshop 7	NA:NA:NA	2018
Martin Wischenbart:Stefan Mitsch:Elisabeth Kapsammer:Angelika Kusel:Birgit Pröll:Werner Retschitzegger:Wieland Schwinger:Johannes Schönböck:Manuel Wimmer:Stephan Lechner	User profile integration from multiple social networks is indispensable for gaining a comprehensive view on users. Although current social networks provide access to user profile data via dedicated APIs, they fail to provide accurate schema information, which aggravates the integration of user profiles, and not least the adaptation of applications in the face of schema evolution. To alleviate these problems, this paper presents, firstly, a semi-automatic approach to extract schema information from instance data. Secondly, transformations of the derived schemas to different technical spaces are utilized, thereby allowing, amongst other benefits, the application of established integration tools and methods. Finally, as a case study, schemas are derived for Facebook, Google+, and LinkedIn. The resulting schemas are analyzed (i) for completeness and correctness according to the documentation, and (ii) for semantic overlaps and heterogeneities amongst each other, building the basis for future user profile integration.	User profile integration made easy: model-driven extraction and transformation of social network schemas	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Nadia Bennani:Max Chevalier:Elöd Egyed-Zsigmond:Gilles Hubert:Marco Viviani	In the field of multi-application personalization, several techniques have been proposed to support user modeling for user data management across different applications. Many of them are based on data reconciliation techniques often implying the concepts of static ontologies and generic user data models. None of them have sufficiently investigated two main issues related to user modeling: (1) profile definition in order to allow every application to build their own view of users while promoting the sharing of these profiles and (2) profile evolution over time in order to avoid data inconsistency and the subsequent loss of income for web-site users and companies. In this paper, we conduct work and propose separated solutions for every issue. We propose a flexible user modeling system, not imposing any fixed user model whom different applications should conform to, but based on the concept of mapping among applications (and mapping functions among their user attributes). We focus in particular on the management of user profile data propagation, as a way to reduce the amount of inconsistent user profile information over several applications. A second goal of this paper is to illustrate, in this context, the benefit obtained by the integration of a Semantic Layer that can help application designers to automatically identify potential user attribute mappings between applications. This paper so illustrates a work-in-progress work where two complementary approaches are integrated to improve a main goal: managing multi-application user profiles in a semi-automatic manner.	Multi-application profile updates propagation: a semantic layer to improve mapping between applications	NA:NA:NA:NA:NA	2018
Jeremy D. Foss:Benedita Malheiro:Juan-Carlos Burguillo	Personalised video can be achieved by inserting objects into a video play-out according to the viewer's profile. Content which has been authored and produced for general broadcast can take on additional commercial service features when personalised either for individual viewers or for groups of viewers participating in entertainment, training, gaming or informational activities. Although several scenarios and use-cases can be envisaged, we are focussed on the application of personalised product placement. Targeted advertising and product placement are currently garnering intense interest in the commercial networked media industries. Personalisation of product placement is a relevant and timely service for next generation online marketing and advertising and for many other revenue generating interactive services. This paper discusses the acquisition and insertion of media objects into a TV video play-out stream where the objects are determined by the profile of the viewer. The technology is based on MPEG-4 standards using object based video and MPEG-7 for metadata. No proprietary technology or protocol is proposed. To trade the objects into the video play-out, a Software-as-a-Service brokerage platform based on intelligent agent technology is adopted. Agencies, libraries and service providers are represented in a commercial negotiation to facilitate the contractual selection and usage of objects to be inserted into the video play-out.	Personalised placement in networked video	NA:NA:NA	2018
Manel Mezghani:Corinne Amel Zayani:Ikram Amous:Faiez Gargouri	As social networks are growing in terms of the number of users, resources and interactions; the user may be lost or unable to find useful information. Social elements could avoid this disorientation like the social annotations (tags) which become more and more popular and contribute to avoid the disorientation of the user. Representing a user based on these social annotations has showed their utility in reflecting an accurate user profile which could be used for a recommendation purpose. In this paper, we give a state of the art of characteristics of social user and techniques which model and update a tag-based profile. We show how to treat social annotations and the utility of modelling tag-based profiles for recommendation purposes.	A user profile modelling using social annotations: a survey	NA:NA:NA:NA	2018
Cédric Dromzée:Sébastien Laborie:Philippe Roose	Currently, multimedia documents can be accessed at anytime and anywhere with a wide variety of mobile devices, e.g., laptops, smartphones, tablets. Obviously, platforms heterogeneity, user's preferences and context variations require documents adaptation according to execution constraints, e.g., audio contents may not be played while a user is participating at a meeting. Current context modeling languages do not handle such a real life user constraints. They generally list multiple information values that are interpreted by adaptation processes in order to deduce implicitly such high-level constraints. This paper overcomes this limitation by proposing a novel context modeling approach based on services where context information are linked according to explicit high-level constraints. In order to validate our proposal, we have used Semantic Web technologies by specifying RDF profiles and experiment their usage on several platforms.	Towards an interoperable device profile containing rich user constraints	NA:NA:NA	2018
C. Lee Giles:Qi He:John McPherson:Yuanyuan Tian:Ding Zhou	Large network data are being produced by various applications in an ever growing rate, from social networks such as Facebook and Twitter, scientific citation networks such as CiteSeerX, to biological networks such as protein interaction networks. Network data analysis is crucial for exploiting the wealth of information encoded in such network data. An effective analysis of this data must take into account complex structure including social, temporal, and spatial dimensions, while an efficient analysis of such data requires scalable techniques. As a result, there has been increasing research in developing novel and scalable solutions for practical network analytics applications. This workshop provides a forum for researchers to share new ideas and techniques for large scale network analysis. We are pleased to present 9 original research papers in this workshop. These original research papers cover a variety of topics in the realm of large scale network analysis, including graph theories, scalable algorithms, insightful analysis of real datasets, and novel applications of network analysis in different vertical domains. We are also excited to present two keynote speeches in this workshop. Dr. Sam Shah, from LinkedIn, will deliver the morning keynote address, titled "Large-Scale Graph Mining for Recommendations: Beyond the Social Graph". The afternoon keynote speech will be delivered by Sivan Yogev, from IBM Research Haifa. He will present IBM's experience in "Extracting Business Advantage from Social Networks".	Session details: LSNA'12 workshop 8	NA:NA:NA:NA:NA	2018
Daniel Ritter	The vision of Large Scale Network Analysis (LSNA) states on large amounts of network data, which are produced by social media applications like Facebook, Twitter, and the competitive domain of biological networks as well as their needs for network data extraction and analysis. That raises data management challenges which are addressed by biological, data mining and linked (web) data management communities. So far, mainly these domains were considered when identifying research topics and measuring approaches and progress. We argue that an important domain, the Business Network Management (BNM), representing business and (technical) integration data, implicitely linked and available in enterprises, has been neglected. Not only do enterprises need visibilities into their business networks, they need ad-hoc analysis capabilities on them. In this paper, we introduce BNM as domain, which comes with large scale network data. We discuss how linked business data can be made explicit by what we called Network Mining (NM) from dynamic, heterogeneous enterprise environments to combine it to a (cross-) enterprise linked business data network and state on its different facets w.r.t large network analysis and highlight challenges and opportunities.	From network mining to large scale business networks	NA	2018
Ryan Rossi:Brian Gallagher:Jennifer Neville:Keith Henderson	To understand the structural dynamics of a large-scale social, biological or technological network, it may be useful to discover behavioral roles representing the main connectivity patterns present over time. In this paper, we propose a scalable non-parametric approach to automatically learn the structural dynamics of the network and individual nodes. Roles may represent structural or behavioral patterns such as the center of a star, peripheral nodes, or bridge nodes that connect different communities. Our novel approach learns the appropriate structural role dynamics for any arbitrary network and tracks the changes over time. In particular, we uncover the specific global network dynamics and the local node dynamics of a technological, communication, and social network. We identify interesting node and network patterns such as stationary and non-stationary roles, spikes/steps in role-memberships (perhaps indicating anomalies), increasing/decreasing role trends, among many others. Our results indicate that the nodes in each of these networks have distinct connectivity patterns that are non-stationary and evolve considerably over time. Overall, the experiments demonstrate the effectiveness of our approach for fast mining and tracking of the dynamics in large networks. Furthermore, the dynamic structural representation provides a basis for building more sophisticated models and tools that are fast for exploring large dynamic networks.	Role-dynamics: fast mining of large dynamic networks	NA:NA:NA:NA	2018
Colin Cooper:Tomasz Radzik:Yiannis Siantos	Sampling from large graphs is an area which is of great interest, particularly with the recent emergence of huge structures such as Online Social Networks. These often contain hundreds of millions of vertices and billions of edges. The large size of these networks makes it computationally expensive to obtain structural properties of the underlying graph by exhaustive search. If we can estimate these properties by taking small but representative samples from the network, then size is no longer a problem. In this paper we develop an analysis of random walks, a commonly used method of sampling from networks. We present a method of biassing the random walk to acquire a complete sample of high degree vertices of social networks, or similar graphs. The preferential attachment model is a common method to generate graphs with a power law degree sequence. For this model, we prove that this sampling method is successful with high probability. We also make experimental studies of the method on various real world networks. For t-vertex graphs G(t) generated by a preferential attachment process, we analyze a biassed random walk which makes transitions along undirected edges {x,y} proportional to [d(x)d(y)]b, where d(x) is the degree of vertex x and b > 0 is a constant parameter. Let S(a) be the set of all vertices of degree at least ta in G(t). We show that for some b approx 2/3, if the biassed random walk starts at an arbitrary vertex of S(a), then with high probability the set S(a) can be discovered completely in ~O(t1-(4/3)a+d) steps, where d is a very small positive constant. The notation ~O ignores poly-log t factors. The preferential attachment process generates graphs with power law 3, so the above example is a special case of this result. For graphs with degree sequence power law c>2 generated by a generalized preferential attachment process, a random walk with transitions along undirected edges {x,y} proportional to (d(x)d(y))(c-2)/2, discovers the set S(a) completely in ~O(t1-a(c-2)+d) steps with high probability. The cover time of the graph is ~O(t). Our results say that if we search preferential attachment graphs with a bias b=(c-2)/2 proportional to the power law c then, (i) we can find all high degree vertices quickly, and (ii) the time to discover all vertices is not much higher than in the case of a simple random walk. We conduct experimental tests on generated networks and real-world networks, which confirm these two properties.	A fast algorithm to find all high degree vertices in power law graphs	NA:NA:NA	2018
Peter Kraker:Christian Körner:Kris Jack:Michael Granitzer	Social reference management systems provide a wealth of information that can be used for the analysis of science. In this paper, we examine whether user library statistics can produce meaningful results with regards to science evaluation and knowledge domain visualization. We are conducting two empirical studies, using a sample of library data from Mendeley, the world's largest social reference management system. Based on the occurrence of references in users' libraries, we perform a large-scale impact factor analysis and an exploratory co-readership analysis. Our preliminary findings indicate that the analysis of user library statistics can produce accurate, timely, and content-rich results. We find that there is a significant relationship between the impact factor and the occurrence of references in libraries. Using a knowledge domain visualization based on co-occurrence measures, we are able to identify two areas of topics within the emerging field of technology-enhanced learning.	Harnessing user library statistics for research evaluation and knowledge domain visualization	NA:NA:NA:NA	2018
Matthias Keller:Martin Nussbaumer	The foundation of almost all web sites' information architecture is a hierarchical content organization. Thus information architects put much effort in designing taxonomies that structure the content in a comprehensible and sound way. The taxonomies are obvious to human users from the site's system of main and sub menus. But current methods of web structure mining are not able to extract these central aspects of the information architecture. This is because they cannot interpret the visual encoding to recognize menus and their rank as humans do. In this paper we show that a web site's main navigation system can not only be distinguished by visual features but also by certain structural characteristics of the HTML tree and the web graph. We have developed a reliable and scalable solution that solves the problem of extracting menus for mining the information architecture. The novel MenuMiner-algorithm allows retrieving the original content organization of large-scale web sites. These data are very valuable for many applications, e.g. the presentation of search results. In an experiment we applied the method for finding site boundaries within a large domain. The evaluation showed that the method reliably delivers menus and site boundaries where other current approaches fail.	MenuMiner: revealing the information architecture of large web sites by analyzing maximal cliques	NA:NA	2018
Chenyi Zhang:Jianling Sun	In the information explosion era, large scale data processing and mining is a hot issue. As microblog grows more popular, microblog services have become information provider on a web scale, so researches on microblog begin to focus more on its content mining than solely user's relationship analysis before. Although traditional text mining methods have been studied well, no algorithm is designed specially for microblog data, which contain structured information on social network besides plain text. In this paper, we introduce a novel probabilistic generative model MicroBlog-Latent Dirichlet Allocation (MB-LDA), which takes both contactor relevance relation and document relevance relation into consideration to improve topic mining in microblogs. Through Gibbs sampling for approximate inference of our model, MB-LDA can discover not only the topics of microblogs, but also the topics focused by contactors. When faced with large datasets, traditional techniques on single node become less practical within limited resources. So we present distributed MB-LDA in MapReduce framework in order to process large scale microblogs with high scalability. Furthermore, we apply a performance model to optimize the execution time by tuning the number of mappers and reducers. Experimental results on actual dataset show MB-LDA outperforms the baseline of LDA and distributed MB-LDA offers an effective solution to topic mining for large scale microblogs.	Large scale microblog mining using distributed MB-LDA	NA:NA	2018
Jürgen Pfeffer:Kathleen M. Carley	A lot of centrality measures have been developed to analyze different aspects of importance. Some of the most popular centrality measures (e.g. betweenness centrality, closeness centrality) are based on the calculation of shortest paths. This characteristic limits the applicability of these measures for larger networks. In this article we elaborate on the idea of bounded-distance shortest paths calculations. We claim criteria for k-centrality measures and we introduce one algorithm for calculating both betweenness and closeness based centralities. We also present normalizations for these measures. We show that k-centrality measures are good approximations for the corresponding centrality measures by achieving a tremendous gain of calculation time and also having linear calculation complexity O(n) for networks with constant average degree. This allows researchers to approximate centrality measures based on shortest paths for networks with millions of nodes or with high frequency in dynamically changing networks.	k-Centralities: local approximations of global measures based on shortest paths	NA:NA	2018
Vanesa Junquero-Trabado:David Dominguez-Sal	A social role is a set of characteristics that describe the behavior of individuals and their interactions between them within a social context. In this paper, we describe the architecture of a search engine for detecting roles in a social network. Our approach, based on indexed clusters, gives the user the possibility to define the roles interactively during a search session and retrieve the users for that role in milliseconds. We found that role selection strategies based on selecting people deviating from the average standards provides flexible query expressions and high quality results.	Building a role search engine for social media	NA:NA	2018
Pascal Molli:John Breslin:Sebastian Schaffert:Hideaki Takeda	We are glad to present the technical program for SWCS2012 workshop associated with WWW2012. SWCS2012 highlights contributions on Semantic Web Collaborative Spaces. Semantic Web Collaborative Spaces such as semantic wikis, semantic social networks, semantic forums, etc. are social semantic software with the mission to bring together human agents and software agents in order to foster knowledge-intensive collaboration, content creation and management, annotated multimedia collection management, social knowledge diffusion and formalizing, and more generally speaking ontology-oriented content management lifecycle. SWCS topics includes representing and reasoning on semantics in social web platforms, interacting with and within SWCS, return of experience and applications, integration, interoperability and reuse of SWCS. The workshop papers included in this volume present different issues and visions related to man-machine collaboration in semantic web collaborative spaces. How software agents can explain to regular users what they do? How to help users to produce content understandable by software agents? How to manage the evolution of ontologies and reach agreements between human and software agents? How to manage synchronizations between human readable content and knowledge?	Session details: SWCS'12 workshop 9	NA:NA:NA:NA	2018
Denny Vrandečić	This year, Wikimedia starts to build a new platform for the collaborative acquisition and maintenance of structured data: Wikidata. Wikidata's prime purpose is to be used within the other Wikimedia projects, like Wikipedia, to provide well-maintained, high-quality data. The nature and requirements of the Wikimedia projects require to develop a few novel, or at least unusual features for Wikidata: Wikidata will be a secondary database, i.e. instead of containing facts it will contain references for facts. It will be fully internationalized. It will contain inconsistent and contradictory facts, in order to represent the diversity of knowledge about a given entity.	Wikidata: a new platform for collaborative data collection	NA	2018
Pierre-Antoine Champin:Amélie Cordier:Elise Lavoué:Marie Lefevre:Hala Skaf-Molli	In this paper, we study tools for providing assistance to users in distributed spaces. More precisely, we focus on the activity of collaborative construction of knowledge, supported by a network of distributed semantic wikis. Assisting the users in such an activity is made necessary mainly by two factors: the inherent complexity of the tools supporting that activity, and the collaborative nature of the activity, involving many interactions between users. In this paper we focus on the second aspect. For this, we propose to build an assistance tool based on users interaction traces. This tool will provide a contextualized assistance by leveraging the valuable knowledge contained in traces. We discuss the issue of assistance in our context and we show the different types of assistance that we intend to provide through three scenarios. We highlight research questions raised by this preliminary study.	User assistance for collaborative knowledge construction	NA:NA:NA:NA:NA	2018
Hala Skaf-Molli:Emmanuel Desmontils:Emmanuel Nauer:Gérôme Canals:Amélie Cordier:Marie Lefevre:Pascal Molli:Yannick Toussaint	Social semantic web creates read/write spaces where users and smart agents collaborate to produce knowledge readable by humans and machines. An important issue concerns the ontology evolution and evaluation in man-machine collaboration. How to perform a change on ontologies in a social semantic space that currently use these ontologies through requests? In this paper, we propose to implement a continuous knowledge integration process named K-CIP. We take advantage of man-machine collaboration to transform feedback of people into tests. This paper presents how K-CIP can be deployed to allow fruitful man-machine collaboration in the context of the Wikitaaable system.	Knowledge continuous integration process (K-CIP)	NA:NA:NA:NA:NA:NA:NA:NA	2018
Rakebul Hasan:Fabien Gandon	Collaborative Semantic Web applications produce ever changing interlinked Semantic Web data. Applications that utilize these data to obtain their results should provide explanations about how the results are obtained in order to ensure the effectiveness and increase the user acceptance of these applications. Justifications providing meta information about why a conclusion has been reached enable generation of such explanations. We present an encoding approach for justifications in a distributed environment focusing on the collaborative platforms. We discuss the usefulness of linking justifications across the Web. We introduce a vocabulary for encoding justifications in a distributed environment and provide examples of our encoding approach.	Linking justifications in the collaborative semantic web applications	NA:NA	2018
Luis Daniel Ibáñez:Hala Skaf-Molli:Pascal Molli:Olivier Corby	Social semantic web technologies led to huge amounts of data and information being available. The production of knowledge from this information is challenging, and major efforts, like DBpedia, has been done to make it reality. Linked data provides interconnection between this information, extending the scope of the knowledge production. The knowledge construction between decentralized sources in the web follows a co-evolution scheme, where knowledge is generated collaboratively and continuously. Sources are also autonomous, meaning that they can use and publish only the information they want. The updating of sources with this criteria is intimately related with the problem of synchronization, and the consistency between all the replicas managed. Recently, a new family of algorithms called Commutative Replicated Data Types have emerged for ensuring eventual consistency in highly dynamic environments. In this paper, we define SU-Set, a CRDT for RDF-Graph that supports SPARQL Update 1.1 operations.	Synchronizing semantic stores with commutative replicated data types	NA:NA:NA:NA	2018
George Anadiotis:Konstantinos Kafentzis:Iannis Pavlopoulos:Adam Westerski	In this paper we outline the design and implementation of the eDialogos Consensus process and platform to support wide-scale collaborative decision making. We present the design space and choices made and perform a conceptual alignement of the domains this space entails, based on the use of the eDialogos Consensus ontology as a crystallization point for platform design and implementation as well as interoperability with existing solutions. We also present a metric for calculating agreement on the issues under debate in the platform, incorporating argumentation structure and user feedback.	Building consensus via a semantic web collaborative space	NA:NA:NA:NA	2018
Diego Torres:Pascal Molli:Hala Skaf-Molli:Alicia Diaz	DBpedia is the semantic mirror of Wikipedia. DBpedia extracts information from Wikipedia and stores it in a semantic knowledge base. This semantic feature allows complex semantic queries, which could infer new relations that are missing in Wikipedia. This is an interesting source of knowledge to increase Wikipedia content. But, what is the best way to add these new relations following the Wikipedia conventions? In this paper, we propose a path indexing algorithm (PIA) which takes the resulting set of a DBPedia query and discovers the best representative path in Wikipedia. We evaluate the algorithm with real data sets from DBpedia.	Improving Wikipedia with DBpedia	NA:NA:NA:NA	2018
Amélie Cordier:Emmanuelle Gaillard:Emmanuel Nauer	This paper shows how humans and machines can better collaborate to acquire adaptation knowledge (AK) in the framework of a case-based reasoning (CBR) system whose knowledge is encoded in a semantic wiki. Automatic processes like the CBR reasoning process itself, or specific tools for acquiring AK are integrated as wiki extensions. These tools and processes are combined on purpose to collect AK. Users are at the center of our approach, as they are in a classical wiki, but they will now benefit from automatic tools for helping them to feed the wiki. In particular, the CBR system, which is currently only a consumer for the knowledge encoded in the semantic wiki, will also be used for producing knowledge for the wiki. A use case in the domain of cooking is given to exemplify the man-machine collaboration.	Man-machine collaboration to acquire cooking adaptation knowledge for the TAAABLE case-based reasoning system	NA:NA:NA	2018
Guo Zhang:Elin K. Jacob	This paper addresses the concepts of community and online community and discusses the physical, functional, and symbolic characteristics of a community that have formed the basis for traditional definitions. It applies a four-dimensional perspective of space and place (i.e., shape, structure, context, and experience) as a framework for refining the definition of traditional offline communities and for developing a definition of online communities that can be effectively operationalized. The methods and quantitative measures of social network analysis are proposed as appropriate tools for investigating the nature and function of communities because they can be used to quantify the typically subjective social phenomena generally associated with communities.	Community: issues, definitions, and operationalization on the web	NA:NA	2018
Hakim Hacid:Shengbo Guo:Julien Velcin	In the real-time Web, the latest evolution of the Web, information (e.g., videos, images, and blogs) is generated very quickly, consumed by millions of users, and updated rapidly by others through commenting, replying, transferring, etc. This is practiced by people who differ in culture, knowledge, background, ideology. Moreover, information generally comes from several channels and is sent out to different ones. This is amplified by the social networking phenomenon - the social Web, which is nowadays a well established set of technologies, based on which users and service providers can exchange messages through an interaction network, share information and collaborate, advertise a product, create communities and influence them, etc. Besides, there is an abundant literature regarding the different aspects of social networks be it their construction or the detection of nodes playing specific roles. Thus, it is difficult to draw a clear image linking the existing models of social networks and the real underlying social mechanisms. As a result, there is a big gap in concertizing and evaluating most of the research efforts in this area. Furthermore, due to the growing complexity of digital social networks and the huge quantity of new data available every day, it becomes crucial for researchers to provide a clear understanding of the dynamics of these networks. It also becomes important for the community to not only understand what is happening currently in the network but also to predict the next evolution and monitor the trends in the network. To address the above mentioned aspects of social network dynamics, we solicited the following topics (but not limited to): Information diffusion in social networks, community extraction, analysis, and evolution, detection of (possibly evolving) roles, content evolution and tracking in social networks, social media recommendations, information quality and evolution in social content, evaluation techniques and benchmarks, new challenges in mining social networks, and example studies and use cases of dynamics of social networks. Such approaches can be applied to various areas including social sciences, politics, economics, marketing, tourism and culture. Among such areas, social networks are generating huge changes in the creation, distribution, and consumption of News creating opportunities for new applications and redefining business models. The workshop includes a special session on Social Media Applications in News, including papers specifically addressing these issues and positions from News business experts and practitioners. This year, the workshop has attracted 29 submissions. All submissions went through a rigorous review process with at least three reviews per paper. The selection was very hard since the submissions are of high quality and we have time constraints for presentations. To this end, 16 papers are accepted for oral presentations, and our workshop program consists of these 16 oral presentations and a keynote talk about the dynamics in social networks and the special session focusing on social networks usage for journalism.	Session details: MSND'12 workshop 10	NA:NA:NA	2018
Jochen Spangenberg	The workshop also includes a business section that will focus on aspects of Social Media in the News domain. Panelists with expertise in innovation management, news provision, journalism and market developments will discuss some of the challenges of and opportunities for the news sector with regards to Social Media. This part of the workshop is organized and brought to you by the SocialSensor project.	Business session "social media and news	NA	2018
Kazumi Saito:Masahiro Kimura:Kouzou Ohara:Hiroshi Motoda	We address the problem of visualizing structure of undirected graphs that have a value associated with each node into a K-dimensional Euclidean space in such a way that 1) the length of the point vector in this space is equal to the value assigned to the node and 2) nodes that are connected are placed as close as possible to each other in the space and nodes not connected are placed as far apart as possible from each other. The problem is reduced to K-dimensional spherical embedding with a proper objective function. The existing spherical embedding method can handle only a bipartite graph and cannot be used for this purpose. The other graph embedding methods, e.g., multi-dimensional scaling, spring force embedding methods, etc., cannot handle the value constraint and thus are not applicable, either. We propose a very efficient algorithm based on a power iteration that employs the double-centering operations. We apply the method to visualize the information diffusion process over a social network by assigning the node activation time to the node value, and compare the results with the other visualization methods. The results applied to four real world networks indicate that the proposed method can visualize the diffusion dynamics which the other methods cannot and the role of important nodes, e.g. mediator, more naturally than the other methods.	Graph embedding on spheres and its application to visualization of information diffusion data	NA:NA:NA:NA	2018
Adrien Guille:Hakim Hacid	Today, online social networks have become powerful tools for the spread of information. They facilitate the rapid and large-scale propagation of content and the consequences of an information -- whether it is favorable or not to someone, false or true -- can then take considerable proportions. Therefore it is essential to provide means to analyze the phenomenon of information dissemination in such networks. Many recent studies have addressed the modeling of the process of information diffusion, from a topological point of view and in a theoretical perspective, but we still know little about the factors involved in it. With the assumption that the dynamics of the spreading process at the macroscopic level is explained by interactions at microscopic level between pairs of users and the topology of their interconnections, we propose a practical solution which aims to predict the temporal dynamics of diffusion in social networks. Our approach is based on machine learning techniques and the inference of time-dependent diffusion probabilities from a multidimensional analysis of individual behaviors. Experimental results on a real dataset extracted from Twitter show the interest and effectiveness of the proposed approach as well as interesting recommendations for future investigation.	A predictive model for the temporal dynamics of information diffusion in online social networks	NA:NA	2018
Václav Belák:Samantha Lam:Conor Hayes	In recent years, many companies have started to utilise online social communities as a means of communicating with and targeting their employees and customers. Such online communities include discussion fora which are driven by the conversational activity of users. For example, users may respond to certain ideas as a result of the influence of their neighbours in the underlying social network. We analyse such influence to target communities rather than individual actors because information is usually shared with the community and not just with individual users. In this paper, we study information diffusion across communities and argue that some communities are more suitable for maximising spread than others. In order to achieve this, we develop a set of novel measures for cross-community influence, and show that it outperforms other targeting strategies on 51 weeks of data of the largest Irish online discussion system, Boards.ie.	Targeting online communities to maximise information diffusion	NA:NA:NA	2018
Ramine Tinati:Leslie Carr:Wendy Hall:Jonny Bentwood	Twitter has redefined the way social activities can be coordinated; used for mobilizing people during natural disasters, studying health epidemics, and recently, as a communication platform during social and political change. As a large scale system, the volume of data transmitted per day presents Twitter users with a problem: how can valuable content be distilled from the back chatter, how can the providers of valuable information be promoted, and ultimately how can influential individuals be identified? To tackle this, we have developed a model based upon the Twitter message exchange which enables us to analyze conversations around specific topics and identify key players in a conversation. A working implementation of the model helps categorize Twitter users by specific roles based on their dynamic communication behavior rather than an analysis of their static friendship network. This provides a method of identifying users who are potentially producers or distributers of valuable knowledge.	Identifying communicator roles in twitter	NA:NA:NA:NA	2018
Alice Albano:Jean-Loup Guillaume:Bénédicte Le Grand	Many studies have been made on diffusion in the field of epidemiology, and in the last few years, the development of social networking has induced new types of diffusion. In this paper, we focus on file diffusion on a peer-to-peer dynamic network using eDonkey protocol. On this network, we observe a linear behavior of the actual file diffusion. This result is interesting, because most diffusion models exhibit exponential behaviors. In this paper, we propose a new model of diffusion, based on the SI (Susceptible / Infected) model, which produces results close to the linear behavior of the observed diffusion. We then justify the linearity of this model, and we study its behavior in more details.	File diffusion in a dynamic peer-to-peer network	NA:NA:NA	2018
Massoud Seifi:Jean-Loup Guillaume	Community structure is a key property of complex networks. Many algorithms have been proposed to automatically detect communities in static networks but few studies have considered the detection and tracking of communities in an evolving network. Tracking the evolution of a given community over time requires a clustering algorithm that produces stable clusters. However, most community detection algorithms are very unstable and therefore unusable for evolving networks. In this paper, we apply the methodology proposed in [seifi2012] to detect what we call community cores in evolving networks. We show that cores are much more stable than "classical" communities and that we can overcome the disadvantages of the stabilized methods.	Community cores in evolving networks	NA:NA	2018
Mehdi Kaytoue:Arlei Silva:Loïc Cerf:Wagner Meira, Jr.:Chedy Raïssi	"Electronic-sport" (E-Sport) is now established as a new entertainment genre. More and more players enjoy streaming their games, which attract even more viewers. In fact, in a recent social study, casual players were found to prefer watching professional gamers rather than playing the game themselves. Within this context, advertising provides a significant source of revenue to the professional players, the casters (displaying other people's games) and the game streaming platforms. For this paper, we crawled, during more than 100 days, the most popular among such specialized platforms: Twitch.tv. Thanks to these gigabytes of data, we propose a first characterization of a new Web community, and we show, among other results, that the number of viewers of a streaming session evolves in a predictable way, that audience peaks of a game are explainable and that a Condorcet method can be used to sensibly rank the streamers by popularity. Last but not least, we hope that this paper will bring to light the study of E-Sport and its growing community. They indeed deserve the attention of industrial partners (for the large amount of money involved) and researchers (for interesting problems in social network dynamics, personalized recommendation, sentiment analysis, etc.).	Watch me playing, i am a professional: a first study on video game live streaming	NA:NA:NA:NA:NA	2018
Manisha Pujari:Rushed Kanawati	In this paper we propose a new topological approach for link prediction in dynamic complex networks. The proposed approach applies a supervised rank aggregation method. This functions as follows: first we rank the list of unlinked nodes in a network at instant t according to different topological measures (nodes characteristics aggregation, nodes neighborhood based measures, distance based measures, etc). Each measure provides its own rank. Observing the network at instant t+1 where some new links appear, we weight each topological measure according to its performances in predicting these observed new links. These learned weights are then used in a modified version of classical computational social choice algorithms (such as Borda, Kemeny, etc) in order to have a model for predicting new links. We show the effectiveness of this approach through different experimentations applied to co-authorship networks extracted from the DBLP bibliographical database. Results we obtain, are also compared with the outcome of classical supervised machine learning based link prediction approaches applied to the same datasets.	Supervised rank aggregation approach for link prediction in complex networks	NA:NA	2018
Anis Najar:Ludovic Denoyer:Patrick Gallinari	Models of information diffusion and propagation over large social media usually rely on a Close World Assumption: information can only propagate onto the network relational structure, it cannot come from external sources, the network structure is supposed fully known by the model. These assumptions are nonrealistic for many propagation processes extracted from Social Websites. We address the problem of predicting information propagation when the network diffusion structure is unknown and without making any closed world assumption. Instead of modeling a diffusion process, we propose to directly predict the final propagation state of the information over a whole user set. We describe a general model, able to learn predicting which users are the most likely to be contaminated by the information knowing an initial state of the network. Different instances are proposed and evaluated on artificial datasets.	Predicting information diffusion on social networks with partial knowledge	NA:NA:NA	2018
Mao Ye:Thomas Sandholm:Chunyan Wang:Christina Aperjis:Bernardo A. Huberman	We present a study of the group purchasing behavior of daily deals in Groupon and LivingSocial and formulate a predictive dynamic model of collective attention for group buying behavior. Using large data sets from both Groupon and LivingSocial we show how the model is able to predict the success of group deals as a function of time.We find that Groupon deals are easier to predict accurately earlier in the deal lifecycle than LivingSocial deals due to the total number of deal purchases saturating quicker. One possible explanation for this is that the incentive to socially propagate a deal is based on an individual threshold in LivingSocial, whereas in Groupon it is based on a collective threshold which is reached very early. Furthermore, the personal benefit of propagating a deal is greater in LivingSocial.	Collective attention and the dynamics of group deals	NA:NA:NA:NA:NA	2018
Athena Vakali:Maria Giatsoglou:Stefanos Antaris	Social networking media generate huge content streams, which leverage, both academia and developers efforts in providing unbiased, powerful indications of users' opinion and interests. Here, we present Cloud4Trends, a framework for collecting and analyzing user generated content through microblogging and blogging applications, both separately and jointly, focused on certain geographical areas, towards the identification of the most significant topics using trend analysis techniques. The cloud computing paradigm appears to offer a significant benefit in order to make such applications viable considering that the massive data sizes produced daily impose the need of a scalable and powerful infrastructure. Cloud4Trends constitutes an efficient Cloud-based approach in order to solve the online trend tracking problem based on Web 2.0 sources. A detailed system architecture model is also proposed, which is largely based on a set of service modules developed within the VENUS-C research project to facilitate the deployment of research applications on Cloud infrastructures.	Social networking trends and dynamics detection via a cloud-based framework design	NA:NA:NA	2018
Thomas Lansdall-Welfare:Vasileios Lampos:Nello Cristianini	Large scale analysis of social media content allows for real time discovery of macro-scale patterns in public opinion and sentiment. In this paper we analyse a collection of 484 million tweets generated by more than 9.8 million users from the United Kingdom over the past 31 months, a period marked by economic downturn and some social tensions. Our findings, besides corroborating our choice of method for the detection of public mood, also present intriguing patterns that can be explained in terms of events and social changes. On the one hand, the time series we obtain show that periodic events such as Christmas and Halloween evoke similar mood patterns every year. On the other hand, we see that a significant increase in negative mood indicators coincide with the announcement of the cuts to public spending by the government, and that this effect is still lasting. We also detect events such as the riots of summer 2011, as well as a possible calming effect coinciding with the run up to the royal wedding.	Effects of the recession on public mood in the UK	NA:NA:NA	2018
Xin Shuai:Xiaozhong Liu:Johan Bollen	Users frequently express their information needs by means of short and general queries that are difficult for ranking algorithms to interpret correctly. However, users' social contexts can offer important additional information about their information needs which can be leveraged by ranking algorithms to provide augmented, personalized results. Existing methods mostly rely on users' individual behavioral data such as clickstream and log data, but as a result suffer from data sparsity and privacy issues. Here, we propose a Community Tweets Voting Model (CTVM) to re-rank Google and Yahoo news search results on the basis of open, large-scale Twitter community data. Experimental results show that CTVM outperforms baseline rankings from Google and Yahoo for certain online communities. We propose an application scenario of CTVM and provide an agenda for further research.	Improving news ranking by community tweets	NA:NA:NA	2018
Matko Bošnjak:Eduardo Oliveira:José Martins:Eduarda Mendes Rodrigues:Luís Sarmento	Modern social network analysis relies on vast quantities of data to infer new knowledge about human relations and communication. In this paper we describe TwitterEcho, an open source Twitter crawler for supporting this kind of research, which is characterized by a modular distributed architecture. Our crawler enables researchers to continuously collect data from particular user communities, while respecting Twitter's imposed limits. We present the core modules of the crawling server, some of which were specifically designed to focus the crawl on the Portuguese Twittosphere. Additional modules can be easily implemented, thus changing the focus to a different community. Our evaluation of the system shows high crawling performance and coverage.	TwitterEcho: a distributed focused crawler to support open research with twitter data	NA:NA:NA:NA:NA	2018
Arnaud J. Le Hors:Steve Speicher	As a result of the Linked Data Basic Profile submission, made by several organizations including IBM, EMC, and Oracle, the W3C launched in June 2012 the Linked Data Platform (LDP) Working Group (WG). The LDP WG is chartered to produce a W3C Recommendation for HTTP-based (RESTful) application integration patterns using read/write Linked Data. This work will benefit both small-scale in-browser applications (WebApps) and large-scale Enterprise Application Integration (EAI) efforts. It will complement SPARQL and will be compatible with standards for publishing Linked Data, bringing the data integration features of RDF to RESTful, data-oriented software development. This presentation introduces developers to the Linked Data Platform, explains its origins in the Open Services Lifecycle Collaboration (OSLC) initiative, describes how it fits with other existing Semantic Web technologies and the problems developers will be able to address using LDP, based on use cases such as the integration challenge the industry faces in the Application Lifecycle Management (ALM) space. By attending this presentation developers will get an understanding of this upcoming W3C Recommendation which is posed to become a major stepping stone in enabling broader adoption of Linked Data in the industry, not only for publishing data but also for integrating applications.	The linked data platform (LDP)	NA:NA	2018
Vivian Genaro Motti:Dave Raggett	Web application development teams face an increasing burden when they need to come up with a consistent user interface across different platforms with different characteristics, for example, desktop, smart phone and tablet devices. This is going to get even worse with the adoption of HTML5 on TVs and cars. This short paper describes a browser-based collaborative design assistant that does the drudge work of ensuring that the user interfaces are kept in sync across all of the target platforms and with changes to the domain data and task models. This is based upon an expert system that dynamically updates the user interface design to reflect the developer's decisions. This is implemented in terms of constraint propagation and search through the design space. An additional benefit is the ease of providing accessible user interfaces in conjunction with assistive technologies.	Quill: a collaborative design assistant for cross platform web application user interfaces	NA:NA	2018
Lyndon Nixon	In this submission, we describe the Linked Services Infrastructure (LSI). It uses Semantic Web Service technology to map individual concepts (identified by Linked Data URIs) to sets of online media content aggegrated from heterogeneous Web APIs. It exposes this mapping service in a RESTful API and returns RDF based responses for further processing if desired. The LSI can be used as a general purpose tool for user agents to retrieve different online media resources to illustrate a concept to a user.	Linked services infrastructure: a single entry point for online media related to any linked data concept	NA	2018
Bernhard Haslhofer:Simeon Warner:Carl Lagoze:Martin Klein:Robert Sanderson:Michael L. Nelson:Herbert Van de Sompel	Many applications need up-to-date copies of collections of changing Web resources. Such synchronization is currently achieved using ad-hoc or proprietary solutions. We propose ResourceSync, a general Web resource synchronization protocol that leverages XML Sitemaps. It provides a set of capabilities that can be combined in a modular manner to meet local or community requirements. We report on work to implement this protocol for arXiv.org and also provide an experimental prototype for the English Wikipedia as well as a client API.	ResourceSync: leveraging sitemaps for resource synchronization	NA:NA:NA:NA:NA:NA:NA	2018
Benjamin Canou:Emmanuel Chailloux:Vincent Botbol	In this paper, after relating a short history of the mostly unhappy relationship between static typing and JavaScript (JS), we explain a new attempt at conciliating them which is more respectful of both worlds than other approaches. As an example, we present Onyo, an advanced binding of the Enyo JS library for the OCaml language. Onyo exploits the expressiveness of OCaml's type system to properly encode the structure of the library, preserving its design while statically checking that it is used correctly, and without introducing runtime overhead.	Static typing & JavaScript libraries: towards a more considerate relationship	NA:NA:NA	2018
Vincent Balat	The evolution of the Web from a content platform into an application platform has raised many new issues for developers. One of the most significant is that we are now developing distributed applications, in the specific context of the underlying Web technologies. In particular, one should be able to compute some parts of the page either on server or client sides, depending on the needs of developers, and preferably in the same language, with the same functions. This paper deals with the particular problem of user interface generation in this client-server setting. Many widget libraries for browsers are fully written in JavaScript and do not allow to generate the interface on server side, making more difficult the indexing of pages by search engines. We propose a solution that makes possible to generate widgets either on client side or on server side in a very flexible way. It is implemented in the Ocsigen framework.	Client-server web applications widgets	NA	2018
Giovanni Grasso:Tim Furche:Christian Schallhart	Even in the third decade of the Web, scraping web sites remains a challenging task: Most scraping programs are still developed as ad-hoc solutions using a complex stack of languages and tools. Where comprehensive extraction solutions exist, they are expensive, heavyweight, and proprietary. OXPath is a minimalistic wrapping language that is nevertheless expressive and versatile enough for a wide range of scraping tasks. In this presentation, we want to introduce you to a new paradigm of scraping: declarative navigation--instead of complex scripting or heavyweight, limited visual tools, OXPath turns scraping into a simple two step process: pick the relevant nodes through an XPath expression and then specify which action to apply to those nodes. OXPath takes care of browser synchronisation, page and state management, making scraping as easy as node selection with XPath. To achieve this, OXPath does not require a complex or heavyweight infrastructure. OXPath is an open source project and has seen first adoption in a wide variety of scraping tasks.	Effective web scraping with OXPath	NA:NA:NA	2018
Richard Duchatsch Johansen:Talita Cristina Pagani Britto:Cesar Augusto Cusin	Developing websites for multiples devices have been a rough task for the past ten years. Devices features - such as screen size, resolution, internet access, operating system, etc. - change frequently and new devices emerge every day. Since W3C introduced media queries in CSS3, it's possible to developed tailored interfaces for multiple devices using a single HTML document. The approach of Responsive Web Design has been used media queries as support for developing adaptive and flexible layouts, however, it's not supported in legacy browsers. In this paper, we present CSS Browser Selector Plus, a cross-browser alternative method using JavaScript to support CSS3 media queries for developing responsive web considering older browsers.	CSS browser selector plus: a JavaScript library to support cross-browser responsive design	NA:NA:NA	2018
Thomas Steiner	We have developed an application called Social Media Illustrator that allows for finding media items on multiple social networks, clustering them by visual similarity, ranking them by different criteria, and finally arranging them in media galleries that were evaluated to be perceived as aesthetically pleasing. In this paper, we focus on the ranking aspect and show how, for a given set of media items, the most adequate ranking criterion combination can be found by interactively applying different criteria and seeing their effect on-the-fly. This leads us to an empirically optimized media item ranking formula, which takes social network interactions into account. While the ranking formula is not universally applicable, it can serve as a good starting point for an individually adapted formula, all within the context of Social Media Illustrator. A demo of the application is available publicly online at the URL http://social-media-illustrator.herokuapp.com/.	A meteoroid on steroids: ranking media items stemming from multiple social networks	NA	2018
Markus Lanthaler	In this paper we describe a novel approach to build hypermedia-driven Web APIs based on Linked Data technologies such as JSON-LD. We also present the result of implementing a first prototype featuring both a RESTful Web API and a generic API client. To the best of our knowledge, no comparable integrated system to develop Linked Data-based APIs exists.	Creating 3rd generation web APIs with hydra	NA	2018
Lei Tang:Patrick Harrington	Recommendation is one of the core problems in eCommerce. In our application, different from conventional collaborative filtering, one user can engage in various types of activities in a sequence. Meanwhile, the number of users and items involved are quite huge, entailing scalable approaches. In this paper, we propose one simple approach to integrate multiple types of user actions for recommendation. A two-stage randomized matrix factorization is presented to handle large-scale collaborative filtering where alternating least squares or stochastic gradient descent is not viable. Empirical results show that the method is quite scalable, and is able to effectively capture correlations between different actions, thus making more relevant recommendations.	Scaling matrix factorization for recommendation with randomness	NA:NA	2018
Dong Li:Zhiming Xu:Sheng Li:Xin Sun	In recent years, online social networks have undergone a significant growth and attracted much attention. In these online social networks, link prediction is a critical task that not only offers insights into the factors behind creation of individual social relationship but also plays an essential role in the whole network growth. In this paper, we propose a novel link prediction method based on hypergraph. In contrast with conventional methods that using ordinary graph, we model the social network as a hypergraph, which can fully capture all types of objects and either the pair wise or high-order relations among these objects in the network. Then the link prediction task is formulated as a ranking problem on this hypergraph. Experimental results on Sina-Weibo dataset have demonstrated the effectiveness of our methods.	Link prediction in social networks based on hypergraph	NA:NA:NA:NA	2018
Ingmar Weber:Venkata Rama Kiran Garimella:Erik Borra	Political campaigning and the corresponding advertisement money are increasingly moving online. Some analysts claim that the U.S.~elections were partly won through a smart use of (i) targeted advertising and (ii) social media. But what type of information do politicized users consume online? And, the other way around, for a given content, e.g. a YouTube video, is it possible to predict its political audience? To address this latter question, we present a large scale study of anonymous YouTube video consumption of politicized users, where political orientation is derived from visits to "beacon pages", namely, political partisan blogs. Though our techniques are relevant for targeted political advertising, we believe that our findings are also of a wider interest.	Inferring audience partisanship for YouTube videos	NA:NA:NA	2018
Ning Zheng:Xiaoming Jin:Lianghao Li	With the rapid growth of location-based social networks (LBSNs), Point-of-Interest (POI) recommendation is in increasingly higher demand these years. In this paper, our aim is to recommend new POIs to a user in regions where he has rarely been before. Different from the classical memory-based recommendation algorithms using user rating data to compute similarity between users or items to make recommendation, we propose a cross-region collaborative filtering method based on hidden topics mined from user check-in records to recommend new POIs. Experimental results on a real-world LBSNs dataset show that our method consistently outperforms naive CF method.	Cross-region collaborative filtering for new point-of-interest recommendation	NA:NA:NA	2018
Subhabrata Mukherjee:Gaurab Basu:Sachindra Joshi	Traditional works in sentiment analysis do not incorporate author preferences during sentiment classification of reviews. In this work, we show that the inclusion of author preferences in sentiment rating prediction of reviews improves the correlation with ground ratings, over a generic author independent rating prediction model. The overall sentiment rating prediction for a review has been shown to improve by capturing facet level rating. We show that this can be further developed by considering author preferences in predicting the facet level ratings, and hence the overall review rating. To the best of our knowledge, this is the first work to incorporate author preferences in rating prediction.	Incorporating author preference in sentiment rating prediction of reviews	NA:NA:NA	2018
Krishna Y. Kamath:Ana-Maria Popescu:James Caverlee	Pinterest is a fast-growing interest network with significant user engagement and monetization potential. This paper explores quality signals for Pinterest boards, in particular the notion of board coherence. We find that coherence can be assessed with promising results and we explore its relation to quality signals based on social interaction.	Board coherence in Pinterest: non-visual aspects of a visual site	NA:NA:NA	2018
Jisun An:Daniele Quercia:Jon Crowcroft	The hypothesis of selective exposure assumes that people crave like-minded information and eschew information that conflicts with their beliefs, and that has negative consequences on political life. Yet, despite decades of research, this hypothesis remains theoretically promising but empirically difficult to test. We look into news articles shared on Facebook and examine whether selective exposure exists or not in social media. We find a concrete evidence for a tendency that users predominantly share like-minded news articles and avoid conflicting ones, and partisans are more likely to do that. Building tools to counter partisanship on social media would require the ability to identify partisan users first. We will show that those users cannot be distinguished from the average user as the two subgroups do not show any demographic difference.	Fragmented social media: a look into selective exposure to political news	NA:NA:NA	2018
Ben Priest:Kevin Gold	We demonstrate that psychological models of utility discounting can explain the pattern of increased hits to weather websites in the days preceding a predicted weather disaster. We parsed the HTTP request lines issued by the web proxy for a mid-sized enterprise leading up to a hurricane, filtering for visits to weather-oriented websites. We fit four discounting models to the observed activity and found that our data matched hyperboloid models extending hyperbolic discounting.	Utility discounting explains informational website traffic patterns before a hurricane	NA:NA	2018
Asmelash Teka Hadgu:Kiran Garimella:Ingmar Weber	We study the change in polarization of hashtags on Twitter over time and show that certain jumps in polarity are caused by "hijackers" engaged in a particular type of hashtag war.	Political hashtag hijacking in the U.S.	NA:NA:NA	2018
Wei Feng:Jianyong Wang	In Twitter, users can annotate tweets with hashtags to indicate the ongoing topics. Hashtags provide users a convenient way to categorize tweets. However, two problems remain unsolved during an annotation: (1) Users have no way to know whether some related hashtags have already been created. (2) Users have their own way to categorize tweets. Thus personalization is needed. To address the above problems, we develop a statistical model for Personalized Hashtag Recommendation. With millions of "tweet, hashtag" pairs being generated everyday, we are able to learn the complex mappings from tweets to hashtags with the wisdom of the crowd. Our model considers rich auxiliary information like URLs, locations, social relation, temporal characteristics of hashtag adoption, etc. We show our model successfully outperforms existing methods on real datasets crawled from Twitter.	Learning to annotate tweets with crowd wisdom	NA:NA	2018
Yanan Zhu:Nazli Goharian	The features available in Twitter provide meaningful information that can be harvested to provide a ranked list of followees to each user. We hypothesize that retweet and mention features can be further enriched by incorporating both temporal and additional/indirect links from within user's community. Our empirical results provide insights into the effectiveness of each feature, and evaluate our proposed similarity measures in ranking the followees. Utilizing temporal information and indirect links improves the effectiveness of retweet and mention features in terms of nDCG.	To follow or not to follow: a feature evaluation	NA:NA	2018
Vidit Jain:Esther Galbrun	On a news website, an article may receive thousands of comments from its readers on a variety of topics. The usual display of these comments in a ranked list, e.g. by popularity, does not allow the user to follow discussions on a particular topic. Organizing them by semantic topics enables the user not only to selectively browse comments on a topic, but also to discover other significant topics of discussion in comments. This topical organization further allows to explicitly capture the immediate interests of the user even when she is not logged in. Here we use this information to recommend content that is relevant in the context of the comments being read by the user. We present an algorithm for building such a topical organization in a practical setting and study different recommendation schemes. In a pilot study, we observe these comments-to-article recommendations to be preferred over the standard article-to-article recommendations.	Topical organization of user comments and application to content recommendation	NA:NA	2018
Yasser Salem:Jun Hong	In this paper we present a new approach to critiquing-based conversational recommendation, which we call History-Aware Critiquing (HAC). It takes a case-based reasoning approach by reusing relevant recommendation sessions of past users to short-cut the recommendation session of the current user. It selects relevant recommendation sessions from a case base that contains the successful recommendation sessions of past users. A past recommendation session can be selected if it contains similar recommended items to the ones in the current session and its critiques sufficiently overlap with the critiques so far in the current session. HAC extends experience-based critiquing (EBC). Our experimental results show that, in terms of recommendation efficiency, while EBC performs better than standard critiquing (STD), it does not perform as well as more recent techniques such as incremental critiquing (IC), whereas HAC achieves better recommendation efficiency over both STD and IC.	History-aware critiquing-based conversational recommendation	NA:NA	2018
Yoshiyuki Inagaki:Jiang Bian:Yi Chang	Local search services have been gaining interests from Web users who seek the information near certain geographical locations. Particularly, those users usually want to find interesting information about what is happening nearby. In this poster, we introduce the localized content optimization problem to provide Web users with authoritative, attractive and fresh information that are really interesting to people around the certain location. To address this problem, we propose a general learning framework and develop a variety of features. Our evaluations based on the data set from a commercial localized Web service demonstrate that our framework is highly effective at providing contents that are more relevant to users' localized information need.	An effective general framework for localized content optimization	NA:NA:NA	2018
Zhi Yang:Ji long Xue:Han Xiao Zhao:Xiao Wang:Ben Y. Zhao:Yafei Dai	Measurement studies of online social networks show that all social links are not equal, and the strength of each link is best characterized by the frequency of interactions between the linked users.To date, few studies have been able to examine detailed interaction data over time, and none have studied the problem of modeling user interactions. This paper proposes a generative model of social interactions that captures the inherently heterogeneous strengths of social links, thus having broad implications on the design of social network algorithms such as friend recommendation, information diffusion and viral marketing.	Unfolding dynamics in a social network: co-evolution oflink formation and user interaction	NA:NA:NA:NA:NA:NA	2018
Claudia Orellana-Rodriguez:Ernesto Diaz-Aviles:Wolfgang Nejdl	Short films are regarded as an alternative form of artistic creation, and they express, in a few minutes, a whole gamma of different emotions oriented to impact the audience and communicate a story. In this paper, we exploit a multi-modal sentiment analysis approach to extract emotions in short films, based on the film criticism expressed through social comments from the video-sharing platform YouTube. We go beyond the traditional polarity detection (i.e., positive/negative), and extract, for each analyzed film, four opposing pairs of primary emotions: joy-sadness, anger-fear, trust-disgust, and anticipation-surprise. We found that YouTube comments are a valuable source of information for automatic emotion detection when compared to human analysis elicited via crowdsourcing.	Mining emotions in short films: user comments or crowdsourcing?	NA:NA:NA	2018
Mitesh M. Khapra:Salil Joshi:Ananthakrishnan Ramanathan:Karthik Visweswariah	With the increase of multilingual content and multilingual users on the web, it is prudent to offer personalized services and ads to users based on their language profile (\textit{i.e.}, the list of languages that a user is conversant with). Identifying the language profile of a user is often non-trivial because (i) users often do not specify all the languages known to them while signing up for an online service (ii) users of many languages (especially Indian languages) largely use Latin/Roman script to write content in their native language. This makes it non-trivial for a machine to distinguish the language of one comment from another. This situation presents an opportunity for offering following language based services for romanized content (i) hide romanized comments which belong to a language which is not known to the user (ii) translate romanized comments which belong to a language which is not known to the user (iii) transliterate romanized comments which belong to a language which is known to the user (iv) show language based ads by identifying languages known to a user based on the romanized comments that he wrote/read/liked. We first use a simple bootstrapping based semi-supervised algorithm for identify the language of a romanized comment. We then apply this algorithm to all the comments written/read/liked by a user to build a language profile of the user and propose that this profile can be used to offer the services mentioned above.	Offering language based services on social media by identifying user's preferred language(s) from romanized text	NA:NA:NA:NA	2018
George Gkotsis:Karen Stepanyan:Alexandra I. Cristea:Mike S. Joy	Data extraction from web pages often involves either human intervention for training a wrapper or a reduced level of granularity in the information acquired. Even though the study of social media has drawn the attention of researchers, weblogs remain a part of the web that cannot be harvested efficiently. In this paper, we propose a fully automated approach in generating a wrapper for weblogs, which exploits web feeds for cheap labelling of weblog properties. Instead of performing a pairwise comparison between posts, the model matches the values of the web feeds against their corresponding HTML elements retrieved from multiple weblog posts. It adopts a probabilistic approach for deriving a set of rules and automating the process of wrapper generation. Our evaluation shows that our approach is robust, accurate and efficient in handling different types of weblogs.	Zero-cost labelling with web feeds for weblog data extraction	NA:NA:NA:NA	2018
Jesus A. Rodriguez Perez:Yashar Moshfeghi:Joemon M. Jose	Microblog Ad-hoc retrieval has received much attention in recent years. As a result of the high vocabulary diversity of the publishing users, a mismatch is formed between the queries being formulated and the tweets representing the actual topics. In this work, we present a re-ranking approach relying on inter-document relations, which attempts to bridge this gap. Experiments with TREC's Microblog 2012 collection show that including such information in the retrieval process, statistically significantly improves retrieval effectiveness in terms of Precision and MAP, when the baseline performs well as a starting point.	On using inter-document relations in microblog retrieval	NA:NA:NA	2018
Besnik Fetahu:Bernardo Pereira Nunes:Stefan Dietze	NA	Towards focused knowledge extraction: query-based extraction of structured summaries	NA:NA:NA	2018
Sihem Amer-Yahia:Francesco Bonchi:Carlos Castillo:Esteban Feuerstein:Isabel Méndez-Díaz:Paula Zabala	NA	Complexity and algorithms for composite retrieval	NA:NA:NA:NA:NA:NA	2018
Elizabeth L. Murnane:Bernhard Haslhofer:Carl Lagoze	We address the Named Entity Disambiguation (NED) problem for short, user-generated texts on the social Web. In such settings, the lack of linguistic features and sparse lexical context result in a high degree of ambiguity and sharp performance drops of nearly 50% in the accuracy of conventional NED systems. We handle these challenges by developing a general model of user-interest with respect to a personal knowledge context and instantiate it using Wikipedia. We conduct systematic evaluations using individuals' posts from Twitter, YouTube, and Flickr and demonstrate that our novel technique is able to achieve performance gains beyond state-of-the-art NED methods.	RESLVE: leveraging user interest to improve entity disambiguation on short text	NA:NA:NA	2018
Karen Stepanyan:George Gkotsis:Vangelis Banos:Alexandra I. Cristea:Mike Joy	We introduce a geolocation-aware semantic annotation model that extends the existing solutions for spotting and disambiguation of places within user-generated texts. The implemented prototype processes the text of weblog posts and annotates the places and toponyms. It outperforms existing solutions by taking into consideration the embedded geolocation data. The evaluation of the model is based on a set of randomly selected 3,165 geolocation embedded weblog posts, obtained from 1,775 web feeds. The results demonstrate a high degree of accuracy in annotation (87.7%) and a considerable gain (27.8%) in identifying additional entities, and therefore support the adoption of the model for supplementing the existing solutions.	A hybrid approach for spotting, disambiguating and annotating places in user-generated text	NA:NA:NA:NA:NA	2018
Sarath Kumar Kondreddi:Peter Triantafillou:Gerhard Weikum	We present HIGGINS, a system for Knowledge Acquisition (KA), placing emphasis on its architecture. The distinguishing characteristic and novelty of HIGGINS lies in its blending of two engines: an automated Information Extraction (IE) engine, aided by semantic resources and statistics, and a game-based Human Computing (HC) engine. We focus on KA from web pages and text sources and, in particular, on deriving relationships between entities. As a running application we utilize movie narratives, from which we wish to derive relationships among movie characters.	HIGGINS: knowledge acquisition meets the crowds	NA:NA:NA	2018
Bianca Pereira:Nitish Aggarwal:Paul Buitelaar	The number of available Linked Data datasets has been increasing over time. Despite this, their use to recognise entities in unstructured plain text (Entity Linking task) is still limited to a small number of datasets. In this paper we propose a framework adaptable to the structure of generic Linked Data datasets. This adaptability allows a broader use of Linked Data datasets for the Entity Linking task.	AELA: an adaptive entity linking approach	NA:NA:NA	2018
Matthew E. Peters:Dan Lecocq	The goal of content extraction or boilerplate detection is to separate the main content from navigation chrome, advertising blocks, copyright notices and the like in web pages. In this paper we explore a machine learning approach to content extraction that combines diverse feature sets and methods. Our main contributions are: a) preliminary results that show combining feature sets generally improves performance; and b) a method for including semantic information via id and class attributes applicable to HTML5. We also show that performance decreases on a new benchmark data set that better represents modern chrome.	Content extraction using diverse feature sets	NA:NA	2018
Giang Binh Tran:Mohammad Alrifai:Dat Quoc Nguyen	This paper presents a framework for automatically constructing timeline summaries from collections of web news articles. We also evaluate our solution against manually created timelines and in comparison with related work.	Predicting relevant news events for timeline summaries	NA:NA:NA	2018
Mrinmaya Sachan:Shashank Srivastava	We outline some matrix factorization approaches for co- clustering polyadic data (like publication data) using non-negative factorization (NMF). NMF approximates the data as a product of non-negative low-rank matrices, and can induce desirable clustering properties in the matrix factors through a flexible range of constraints. We show that simultaneous factorization of one or more matrices provides potent approaches for co-clustering.	Collective matrix factorization for co-clustering	NA:NA	2018
Liheng Xu:Kang Liu:Siwei Lai:Yubo Chen:Jun Zhao	This paper proposes a novel two-stage method for opinion words and opinion targets co-extraction. In the first stage, a Sentiment Graph Walking algorithm is proposed, which naturally incorporates syntactic patterns in a graph to extract opinion word/target candidates. In the second stage, we adopt a self-Learning strategy to refine the results from the first stage, especially for filtering out noises with high frequency and capturing long-tail terms. Preliminary experimental evaluation shows that considering pattern confidence in the graph is beneficial and our approach achieves promising improvement over three competitive baselines.	Walk and learn: a two-stage approach for opinion words and opinion targets co-extraction	NA:NA:NA:NA:NA	2018
Rahul Venkataramani:Atul Gupta:Allahbaksh Asadullah:Basavaraju Muddu:Vasudev Bhat	Online Question and Answer websites for developers have emerged as the main forums for interaction during the software development process. The veracity of an answer in such websites is typically verified by the number of 'upvotes' that the answer garners from peer programmers using the same forum. Although this mechanism has proved to be extremely successful in rating the usefulness of the answers, it does not lend itself very elegantly to model the expertise of a user in a particular domain. In this paper, we propose a model to rank the expertise of the developers in a target domain by mining their activity in different opensource projects. To demonstrate the validity of the model, we built a recommendation system for StackOverflow which uses the data mined from GitHub.	Discovery of technical expertise from open source code repositories	NA:NA:NA:NA:NA	2018
Vinodkumar Prabhakaran:Ajita John:Dorée D. Seligmann	In this paper, we explore how the power differential between participants of an interaction affects the way they interact in the context of political debates. We analyze the 2012 Republican presidential primary debates where we model the power index of each candidate in terms of their poll standings. We find that the candidates' power indices affected the way they interacted with others in the debates as well as how others interacted with them.	Power dynamics in spoken interactions: a case study on 2012 republican primary debates	NA:NA:NA	2018
Jason Soo	We describe an adverse environment spelling correction algorithm, known as Segments. Segments is language and domain independent and does not require any training data. We evaluate Segments' correction rate of transcription errors in web query logs with the state-of-the-art learning approach. We show that in environments where learning approaches are not applicable, such as multilingual documents, Segments has an F1-score within 0.005 of the learning approach.	A non-learning approach to spelling correction in web queries	NA	2018
Yu Zhang:Weixiang Zhu	As the number of customer reviews grows very rapidly, it is essential to summarize useful opinions for buyers, sellers and producers. One key step of opinion mining is feature extraction. Most existing research focus on finding explicit features, only a few attempts have been made to extract implicit features. Nearly all existing research only concentrate on product features, few has paid attention to other features that relates to sellers, services and logistics. Therefore in this paper, we propose a novel co-occurrence association-based method, which aims to extract implicit features in customer reviews and provide more comprehensive and fine-grained mining results.	Extracting implicit features in online customer reviews for opinion mining	NA:NA	2018
Shenghua Liu:Wenjun Zhu:Ning Xu:Fangtao Li:Xue-qi Cheng:Yue Liu:Yuanzhuo Wang	Sentiment classification on tweet events attracts more interest in recent years. The large tweet stream stops people reading the whole classified list to understand the insights. We employ the co-training framework in the proposed algorithm. Features are split into text view features and non-text view features. Two Random Forest (RF) classifiers are trained with the common labeled data on the two views of features separately. Then for each specific event, they collaboratively and periodically train together to boost the classification performance. At last, we propose a "river" graph to visualize the intensity and evolvement of sentiment on an event, which demonstrates the intensity by both color gradient and opinion labels, and the ups and downs of confronting opinions by the river flow. Comparing with the well-known sentiment classifiers, our algorithm achieves consistent increases in accuracy on the tweet events from TREC 2011 Microblogging and our database. The visualization helps people recognize turning and bursting patterns, and predict sentiment trend in an intuitive way.	Co-training and visualizing sentiment evolvement for tweet events	NA:NA:NA:NA:NA:NA:NA	2018
Kai Chen:Yi Zhou:Hongyuan Zha:Jianhua He:Pei Shen:Xiaokang Yang	We propose a cost-effective hot event detection system over Sina Weibo platform, currently the dominant microblogging service provider in China. The problem of finding a proper subset of microbloggers under resource constraints is formulated as a mixed-integer problem for which heuristic algorithms are developed to compute approximate solution. Preliminary results show that by tracking about 500 out of 1.6 million candidate microbloggers and processing 15,000 microposts daily, 62% of the hot events can be detected five hours on average earlier than they are published by Weibo.	Cost-effective node monitoring for online hot eventdetection in sina weibo microblogging	NA:NA:NA:NA:NA:NA	2018
Mrinmaya Sachan:Dirk Hovy:Eduard Hovy	Random walks is one of the most popular ideas in computer science. A critical assumption in random walks is that the probability of the walk being at a given vertex at a time instance converges to a limit independent of the start state. While this makes it computationally efficient to solve, it limits their use to incorporate label information. In this paper, we exploit the connection between Random Walks and Electrical Networks to incorporate label information in classification, ranking, and seed expansion.	Solving electrical networks to incorporate supervision in random walks	NA:NA:NA	2018
Peilei Liu:Jintao Tang:Ting Wang	In this paper we investigate information propagation in Twitter from the geographical view on the global scale. An information propagation phenomenon what we call "information current" has been discovered. According to this phenomenon, we propose a hypothesis that changes of information flows may be related to real-time events. Through analysis of retweets, we show that our hypothesis is supported by experiment results. Moreover, it is discovered that the retweet texts are more effective than common tweet texts for real-time event detection. This means that Twitter could be a good filter of texts for event detection.	Information current in Twitter: which brings hot events to the world	NA:NA:NA	2018
Rouben Amirbekian:Ye Chen:Alan Lu:Tak W. Yan:Liangzhong Yin	While the cost-per-click (CPC) pricing model is main stream in sponsored search, the quality of clicks with respect to conversion rates and hence their values to advertisers may vary considerably from publisher to publisher in a large syndication network. Traffic quality shall be used to establish price discounts for clicks from different publishers. These discounts are intended to maintain incentives for high-quality online traffic and to make it easier for advertisers to maintain long-term bid stability. Conversion signal is noisy as each advertiser defines conversion in their own way. It is also very sparse. Traditional way of overcoming signal sparseness is to allow for longer time in accumulating modeling data. However, due to fast-changing conversion trends, such longer time leads to deterioration of the precision in measuring quality. To allow models to adjust to fast-changing trends with sufficient speed, we had to limit time-window for conversion data collection and make it much shorter than the several weeks window commonly used. Such shorter time makes conversions in the training set extremely sparse. To overcome resulting obstacles, we used two-stage regression similar to hurdle regression. First we employed logistic regression to predict zero conversion outcomes. Next, conditioned on non-zero outcomes, we used random forest regression to predict the value of the quotient of two conversion rates. Two-stage model accounts for the zero inflation due to the sparseness of the conversion signal. The combined model maintains good precision and allows faster reaction to the temporal changes in traffic quality including changes due to certain actions by publishers that may lead to click-price inflation.	Traffic quality based pricing in paid search using two-stage regression	NA:NA:NA:NA:NA	2018
Joel Barajas:Ram Akella:Marius Holtan:Jaimie Kwon:Aaron Flores:Victor Andrei	We perform a randomized experiment to estimate the effects of a display advertising campaign on online user conversions. We present a time series approach using Dynamic Linear Models to decompose the daily aggregated conversions into seasonal and trend components. We attribute the difference between control and study trends to the campaign. We test the method using two real campaigns run for 28 and 21 days respectively from the Advertising.com ad network.	Dynamic evaluation of online display advertising with randomized experiments: an aggregated approach	NA:NA:NA:NA:NA:NA	2018
Ilya Trofimov	Click prediction for sponsored search is an important problem for commercial search engines. Good click prediction algorithm greatly affects on the revenue of the search engine, user experience and brings more clicks to landing pages of advertisers. This paper presents new query-dependent features for the click prediction algorithm based on treating query and advertisement as bags of words. New features can improve prediction accuracy both for ads having many and few views.	New features for query dependent sponsored search click prediction	NA	2018
Wei Vivian Zhang:Ye Chen:Mitali Gupta:Swaraj Sett:Tak W. Yan	Click-through rate (CTR) prediction and relevance ranking are two fundamental problems in web advertising. In this study, we address the problem of modeling the relationship between CTR and relevance for sponsored search. We used normalized relevance scores comparable across all queries to represent relevance when modeling with CTR, instead of directly using human judgment labels or relevance scores valid only within same query. We classified clicks by identifying their relevance quality using dwell time and session information, and compared all clicks versus selective clicks effects when modeling relevance. Our results showed that the cleaned click signal outperforms raw click signal and others we explored, in terms of relevance score fitting. The cleaned clicks include clicks with dwell time greater than 5 seconds and last clicks in session. Besides traditional thoughts that there is no linear relation between click and relevance, we showed that the cleaned click based CTR can be fitted well with the normalized relevance scores using a quadratic regression model. This relevance-click model could help to train ranking models using processed click feedback to complement expensive human editorial relevance labels, or better leverage relevance signals in CTR prediction.	Modeling click and relevance relationship for sponsored search	NA:NA:NA:NA:NA	2018
Alexey Chervonenkis:Anna Sorokina:Valery A. Topinsky	We introduce the optimization problem of target-specific ads allocation. Technique for solving this problem for different target-constraints structures is presented. This technique allows us to find optimal ads allocation which maximize the target such as CTR, Revenue or other system performances subject to some linear constraints. We show that the optimal ads allocation depends on both the target and constraints variables.	Optimization of ads allocation in sponsored search	NA:NA:NA	2018
Dmitry Pechyony:Rosie Jones:Xiaojing Li	A long-standing goal in advertising is to reduce wasted costs due to advertising to people who are unlikely to buy, as well as to those who would make a purchase whether they saw an ad or not. The ideal audience for the advertiser are those incremental users who would buy if shown an ad, and would not buy, if not shown the ad. On the other hand, for publishers who are paid when the user clicks or buys, revenue may be maximized by showing ads to those users who are most likely to click or purchase. We show analytically and empirically that an optimization towards one metric might result in an inferior performance in the other one. We present a novel algorithm, called SLC, that performs a joint optimization towards both advertisers' and publishers' goals and provides superior results in both.	A joint optimization of incrementality and revenue to satisfy both advertiser and publisher	NA:NA:NA	2018
Damir Vandic:Didier Nibbering:Flavius Frasincar	In this paper, we investigate how offline advertising, by means of TV and radio, influences online search engine advertisement. Our research is based on the search engine-driven conversion actions of a 2012 marketing campaign of the potato chips manufacturer Lays. In our analysis we use several models, including linear regression (linear model) and Support Vector Regression (non-linear model). Our results confirm that offline commercials have a positive effect on the number of conversion actions from online marketing campaigns. This effect is especially visible in the first 50 minutes after the advertisement broadcasting.	A case-based analysis of the effect of offline media on online conversion actions	NA:NA:NA	2018
Wei Zhang:Yunbo Cao:Chin-Yew Lin:Jian Su:Chew-Lim Tan	Query segmentation is the task of splitting a query into a sequence of non-overlapping segments that completely cover all tokens in the query. The majority of query segmentation methods are unsupervised. In this paper, we propose an error-driven approach to query segmentation (EDQS) with the help of search logs, which enables unsupervised training with guidance from the system-specific errors. In EDQS, we first detect the system's errors by examining the consistency among the segmentations of similar queries. Then, a model is trained by the detected errors to select the correct segmentation of a new query from the top-n outputs of the system. Our evaluation results show that EDQS can significantly boost the performance of state-of-the-art query segmentation methods on a publicly available data set.	An error driven approach to query segmentation	NA:NA:NA:NA:NA	2018
Maxim Zhukovskiy:Andrei Khropov:Gleb Gusev:Pavel Serdyukov	BrowseRank algorithm and its modifications are based on analyzing users' browsing trails. Our paper proposes a new method for computing page importance using a more realistic and effective search-aware model of user browsing behavior than the one used in BrowseRank.	Introducing search behavior into browsing based models of page's importance	NA:NA:NA:NA	2018
Cristina Ioana Muntean:Franco Maria Nardini:Fabrizio Silvestri:Marcin Sydow	We propose the use of learning to rank techniques to shorten query sessions by maximizing the probability that the query we predict is the "final" query of the current search session. We present a preliminary evaluation showing that this approach is a promising research direction.	Learning to shorten query sessions	NA:NA:NA:NA	2018
Jürgen Umbrich:Claudio Gutierrez:Aidan Hogan:Marcel Karnstedt:Josiane Xavier Parreira	Inspired by the CAP theorem, we identify three desirable properties when querying the Web of Data: Alignment (results up-to-date with sources), Coverage (results covering available remote sources), and Efficiency (bounded resources). In this short paper, we show that no system querying the Web can meet all three ACE properties, but instead must make practical trade-offs that we outline.	The ACE theorem for querying the web of data	NA:NA:NA:NA:NA	2018
Roi Blanco:Gianmarco De Francisci Morales:Fabrizio Silvestri	IntoNow from Yahoo! is a second screen application that enhances the way of watching TV programs. The application uses audio from the TV set to recognize the program being watched, and provides several services for different use cases. For instance, while watching a football game on TV it can show statistics about the teams playing, or show the title of the song performed by a contestant in a talent show. The additional content provided by IntoNow is a mix of editorially curated and automatically selected one. From a research perspective, one of the most interesting and challenging use cases addressed by IntoNow is related to news programs (newscasts). When a user is watching a newscast, IntoNow detects it and starts showing online news articles from the Web. This work presents a preliminary study of this problem, i.e., to find an online news article that matches the piece of news discussed in the newscast currently airing on TV, and display it in real-time.	Towards leveraging closed captions for news retrieval	NA:NA:NA	2018
Wensheng Wu:Tingting Zhong	This paper proposes ipq, a novel search engine that proactively transforms query forms of Deep Web sources into phrase queries, constructs query evaluation plans, and caches results for popular queries offline. Then at query time, keyword queries are simply matched with phrase queries to retrieve results. ipq embodies a novel dual-ranking framework for query answering and novel solutions for discovering frequent attributes and queries. Preliminary experiments show the great potentials of ipq.	Searching the deep web using proactive phrase queries	NA:NA	2018
Andrew Yates:Nazli Goharian:Ophir Frieder	Interest in domain-specific search is steadfastly increasing, yielding a growing need for domain-specific synonym discovery. Existing synonym discovery methods perform poorly when faced with the realistic task of identifying a target term's synonyms from among many candidates. We approach domain-specific synonym discovery as a graded relevance ranking problem in which a target term's synonym candidates are ranked by their quality. In this scenario a human editor uses each ranked list of synonym candidates to build a domain-specific thesaurus. We evaluate our method for graded relevance ranking of synonym candidates and find that it outperforms existing methods.	Graded relevance ranking for synonym discovery	NA:NA:NA	2018
Taku Kuribayashi:Yasuhito Asano:Masatoshi Yoshikawa	In this paper, we propose novel ranking methods of effectively finding content descriptions of classical music compositions. In addition to rather naive methods using technical term frequency and latent Dirichlet allocation(LDA), we proposed a novel classification of web pages about classical music and used the characteristics of the classification for our method of search by labeled LDA(L-LDA). The experimental results showed our method performed well at finding content descriptions of classical music compositions.	Ranking method specialized for content descriptions of classical music	NA:NA:NA	2018
Dirk Ahlers	Geospatial search as a special type of vertical search has specific requirements and challenges. While the general principle of resource discovery, extraction, indexing, and search holds, geospatial search systems are tailored to the specific use case at hand with many individual adaptations. In this short overview, we aim to collect and organize the main organizing principles for the multitude of challenges and adaptations to be considered within the development process to work towards a more formal description.	Towards a development process for geospatial information retrieval and search	NA	2018
Yelena Mejova:Ilaria Bordino:Mounia Lalmas:Aristides Gionis	In many cases, when browsing the Web, users are searching for specific information. Sometimes, though, users are also looking for something interesting, surprising, or entertaining. Serendipitous search puts interestingness on par with relevance. We investigate how interesting are the results one can obtain via serendipitous search, and what makes them so, by comparing entity networks extracted from two prominent social media sites, Wikipedia and Yahoo! Answers.	Searching for interestingness in Wikipedia and Yahoo!: answers	NA:NA:NA:NA	2018
Seung Eun Lee:Dongug Kim	User behavior on search results pages provides a clue about the query intent and the relevance of documents. To incorporate this information into search rankings, a variety of click modeling techniques have been proposed so far and now they are widely used in commercial search engines. For time-sensitive queries, however, applying click models can degrade the search relevance because the best document in the past may not be the current best answer. To address this problem, it is required to detect a time point, a turning point, where the search intent for a given query changes and to reflect it in click models. In this work, we devised a method to detect the turning point of a query from its search volume history. The proposed click model is designed to take only user behavior observed after the turning points. We applied our model in a commercial search engine and evaluated its relevance.	A click model for time-sensitive queries	NA:NA	2018
Subhabrata Mukherjee:Ashish Verma:Kenneth W. Church	Mobile query classification faces the usual challenges of encountering short and noisy queries as in web search. However, the task of mobile query classification is made difficult by the presence of more inter-active and personalized queries like map, command and control, dialogue, joke etc. Voice queries are made more difficult than typed queries due to the errors introduced by the automatic speech recognizer. This is the first paper, to the best of our knowledge, to bring the complexities of voice search and intent classification together. In this paper, we propose some novel features for intent classification, like the url's of the search engine results for the given query. We also show the effectiveness of other features derived from the part-of-speech information of the query and search engine results, in proposing a multi-stage classifier for intent classification. We evaluate the classifier using tagged data, collected from a voice search android application, where we achieve an average of 22% f-score improvement per category, over the commonly used bag-of-words baseline.	Intent classification of voice queries on mobile devices	NA:NA:NA	2018
Alexander Kotov:Yu Wang:Eugene Agichtein	We propose the methods for document, query and relevance model expansion that leverage geographical metadata provided by social media. In particular, we propose a geographically-aware extension of the LDA topic model and utilize the resulting topics and language models in our expansion methods. The proposed approach has been experimentally evaluated over a large sample of Twitter, demonstrating significant improvements in search accuracy over traditional (geographically-unaware) retrieval models.	Leveraging geographical metadata to improve search over social media	NA:NA:NA	2018
Rishiraj Saha Roy:Anusha Suresh:Niloy Ganguly:Monojit Choudhury	With fast changing information needs in today's world, it is imperative that search engines precisely understand and exploit temporal changes in Web queries. In this work, we look at shifts in preferred positions of segments in queries over an interval of four years. We find that such shifts can predict key changes in usage patterns, and explain the observed increase in query lengths. Our findings indicate that recording positional statistics can be vital for understanding user intent in Web search queries.	Place value: word position shifts vital to search dynamics	NA:NA:NA:NA	2018
Alex Morales:Huan Sun:Xifeng Yan	Online reviews are widely adopted in many websites such as Amazon, Yelp, and TripAdvisor. Positive reviews can bring significant financial gains, while negative ones often cause sales loss. This fact, unfortunately, results in strong incentives for opinion spam to mislead readers. Instead of hiring humans to write deceptive reviews, in this work, we bring into attention an automated, low-cost process for generating fake reviews, variations of which could be easily employed by evil attackers in reality. To the best of our knowledge, we are the first to expose the potential risk of machine-generated deceptive reviews. Our simple review synthesis model uses one truthful review as a template, and replaces its sentences with those from other reviews in a repository. The fake reviews generated by this mechanism are extremely hard to detect: Both the state-of-the-art machine detectors and human readers have an error rate of 35%-48%. A novel defense method that leverages the difference of semantic flows between fake and truthful reviews is developed, reducing the detection error rate to approximately 22%. Nevertheless, it is still a challenging research task to further decrease the error rate.	Synthetic review spamming and defense	NA:NA:NA	2018
Jyothsna Rachapalli:Vaibhav Khadilkar:Murat Kantarcioglu:Bhavani Thuraisingham	Resource Description Framework (RDF) is the foundational data model of the Semantic Web, and is essentially designed for integration of heterogeneous data from varying sources. However, lack of security features for managing sensitive RDF data while sharing may result in privacy breaches, which in turn, result in loss of user trust. Therefore, it is imperative to provide an infrastructure to secure RDF data. We present a set of graph sanitization operations that are built as an extension to SPARQL. These operations allow one to sanitize sensitive parts of an RDF graph and further enable one to build more sophisticated security and privacy features, thus allowing RDF data to be shared securely.	REDACT: a framework for sanitizing RDF data	NA:NA:NA:NA	2018
Achint Thomas:Kunal Punera:Lyndon Kennedy:Belle Tseng:Yi Chang	Interactive websites use text-based Captchas to prevent unauthorized automated interactions. These Captchas must be easy for humans to decipher while being difficult to crack by automated means. In this work we present a framework for the systematic study of Captchas along these two competing objectives. We begin by abstracting a set of distortions that characterize current and past commercial text-based Captchas. By means of user studies, we quantify the way human Captcha solving performance varies with changes in these distortion parameters. To quantify the effect of these distortions on the accuracy of automated solvers (bots), we propose a learning-based algorithm that performs automated Captcha segmentation driven by character recognition. Results show that our proposed algorithm is generic enough to solve text-based Captchas with widely varying distortions without requiring the use of hand-coded image processing or heuristic rules.	Framework for evaluation of text captchas	NA:NA:NA:NA:NA	2018
Hyun-Kyo Oh:Jin-Woo Kim:Sang-Wook Kim:Kichun Lee	We propose a probability-based trust prediction model based on trust-message passing which takes advantage of the two kinds of information: an explicit information and an implicit information.	A probability-based trust prediction model using trust-message passing	NA:NA:NA:NA	2018
Zeqian Shen:Neel Sundaresan	Peer-to-peer e-commerce networks exemplify online lemon markets. Trust is key to sustaining these networks. We present a reputation system named RepRank that approaches trust with an intuition that in the peer-to-peer e-commerce world consisting of buyers and sellers, good buyers are those who buy from good sellers, and good sellers are those from whom good buyers buy. We propagate trust and distrust in a network using this mutually recursive definition. We discuss the algorithms and present the evaluation results.	RepRank: reputation in a peer-to-peer online system	NA:NA	2018
Amelie Gyrard:Christian Bonnet:Karima Boudaoud	We present a security ontology to help non-security expert software designers or developers to: (1) design secure software and, (2) to understand and be aware of main security concepts and issues. Our security ontology defines the main security concepts such as attacks, countermeasures, security properties and their relationships. Countermeasures can be cryptographic concepts (encryption algorithm, key management, digital signature, hash function), security tools or security protocols. The purpose of this ontology is to be reused in numerous domains such as security of web applications, network management or communication networks (sensor, cellular and wireless). The ontology and a user interface (to use the ontology) are available online.	The STAC (security toolbox: attacks & countermeasures) ontology	NA:NA:NA	2018
Tom De Nies:Sam Coppens:Erik Mannens:Rik Van de Walle	This paper describes how to model uncertain provenance and provenance of uncertain things in a flexible and unintrusive manner using PROV, W3C's new standard for provenance. Three new attributes with clearly defined values and semantics are proposed. Modeling this information is an important step towards the modeling and derivation of trust from resources whose provenance is described using PROV.	Modeling uncertain provenance and provenance of uncertainty in W3C PROV	NA:NA:NA:NA	2018
Padmashree Ravindra:Kemafor Anyanwu	Flexible exploration of large RDF datasets with unknown relationships can be enabled using 'unbound-property' graph pattern queries. Relational-style processing of such queries using normalized relations results in redundant information in intermediate results due to the repetition of adjoining bound (fixed) properties. Such redundancy negatively impacts the disk I/O, network transfer costs, and the required disk space while processing RDF query workloads on MapReduce-based systems. This work proposes packing and lazy unpacking strategies to minimize the redundancy in intermediate results while processing unbound-property queries. In addition to keeping the results compact, this work evaluates RDF queries using the Nested TripleGroup Data Model and Algebra (NTGA) that enables shorter MapReduce execution workflows. Experimental results demonstrate the benefit of this work over RDF query processing using relational-style systems such as Apache Pig and Hive.	Scalable processing of flexible graph pattern queries on the cloud	NA:NA	2018
Philipp Singer:Thomas Niebler:Markus Strohmaier:Andreas Hotho	This paper presents a novel approach for computing semantic relatedness between concepts on Wikipedia by using human navigational paths for this task. Our results suggest that human navigational paths provide a viable source for calculating semantic relatedness between concepts on Wikipedia. We also show that we can improve accuracy by intelligent selection of path corpora based on path characteristics indicating that not all paths are equally useful. Our work makes an argument for expanding the existing arsenal of data sources for calculating semantic relatedness and to consider the utility of human navigational paths for this task.	Computing semantic relatedness from human navigational paths on Wikipedia	NA:NA:NA:NA	2018
Xiaochen Zhang:Xiaoming Jin:Lianghao Li:Dou Shen	The Internet is experiencing an explosion of information presented in different languages. Though written in different languages, some articles implicitly share common concepts. In this paper, we propose a novel framework to mine cross-language common concepts from unaligned web documents. Specifically, visual words of images are used to bridge articles in different languages and then common concepts of multiple languages are learned by using an existing topic modeling algorithm. We conduct cross-lingual text classification in a real-world data set using the mined multilingual concepts from our method. The experiment results show that our approach is effective to mine cross-lingual common concepts.	Discovering multilingual concepts from unaligned web documents by exploring associated images	NA:NA:NA:NA	2018
Sanghoon Lee:Jongwuk Lee:Seung-won Hwang	This paper proposes Fria, a fast and robust instance alignment framework across two independently built knowledge bases (KBs). Our objective is two-fold: (1) to design an effective instance similarity measure and (2) to build a fast and robust alignment framework. Specifically, Fria consists of two-phases. Fria first achieves high-precision alignment for seed matches which have strong evidence for aligning. To obtain high-recall alignment, Fria then divides non-matched instances according to the types identified from seeds, and gives additional chances to the same-typed instances to be matched. Experimental results show that Fria is fast and robust, by achieving comparable accuracy to state-of-the-arts and a 10-times speed up.	Fria: fast and robust instance alignment	NA:NA:NA	2018
Peng Bao:Hua-Wei Shen:Junming Huang:Xue-Qi Cheng	Predicting the popularity of content is important for both the host and users of social media sites. The challenge of this problem comes from the inequality of the popularity of content. Existing methods for popularity prediction are mainly based on the quality of content, the interface of social media site to highlight contents, and the collective behavior of users. However, little attention is paid to the structural characteristics of the networks spanned by early adopters, i.e., the users who view or forward the content in the early stage of content dissemination. In this paper, taking the Sina Weibo as a case, we empirically study whether structural characteristics can provide clues for the popularity of short messages. We find that the popularity of content is well reflected by the structural diversity of the early adopters. Experimental results demonstrate that the prediction accuracy is significantly improved by incorporating the factor of structural diversity into existing methods.	Popularity prediction in microblogging network: a case study on sina weibo	NA:NA:NA:NA	2018
Marco Bressan:Enoch Peserico:Luca Pretto	Can one assess, by visiting only a small portion of a graph, if a given node has a significantly higher PageRank score than another? We show that the answer strongly depends on the interplay between the required correctness guarantees (is one willing to accept a small probability of error?) and the graph exploration model (can one only visit parents and children of already visited nodes?).	The power of local information in PageRank	NA:NA:NA	2018
Cheng-Lun Yang:Perng-Hwa Kung:Chun-An Chen:Shou-De Lin	Online social networks sampling identifies a representative subnetwork that preserves certain graph property given het- erogeneous semantics, with the full network not observed during sampling. This study presents a property, Relational Profile, to account for conditional dependency of node and relation type semantics in a network, and a sampling method to preserve the property. We show the proposed sampling method better preserves Relational Profile. Next, Relational Profile can design features to boost network prediction. Fi- nally, our sampled network trains more accurate prediction models than other sampling baselines.	Semantically sampling in heterogeneous social networks	NA:NA:NA:NA	2018
Hosung Park:Sue Moon	Recent work on unbiased sampling of OSNs has focused on estimation of the network characteristics such as degree distributions and clustering coefficients. In this work we shift the focus to node attributes. We show that existing sampling methods produce biased outputs and need modifications to alleviate the bias.	Sampling bias in user attribute estimation of OSNs	NA:NA	2018
Dong Li:Zhiming Xu:Sheng Li:Xin Sun:Anika Gupta:Katia Sycara	Online social networks mainly have two functions: social interaction and information diffusion. Most of current link recommendation researches only focus on strengthening the social interaction function, but ignore the problem of how to enhance the information diffusion function. For solving this problem, this paper introduces the concept of user diffusion degree and proposes the algorithm for calculating it, then combines it with traditional recommendation methods for reranking recommended links. Experimental results on Email dataset and Amazon dataset under Independent Cascade Model and Linear Threshold Model show that our method noticeably outperforms the traditional methods in terms of promoting information diffusion.	Link recommendation for promoting information diffusion in social networks	NA:NA:NA:NA:NA:NA	2018
Qingliang Miao:Shu Zhang:Yao Meng:Hao Yu	In this paper, we investigate how to identify domain-sensitive opinion leaders in online review communities, and present a model to rank domain-sensitive opinion leaders. To evaluate the effectiveness of the proposed model, we conduct preliminary experiments on a real-world dataset from Amazon.com. Experimental results indicate that the proposed model is effective in identifying domain-sensitive opinion leaders.	Domain-sensitive opinion leader mining from online review communities	NA:NA:NA:NA	2018
Danish Contractor:Tanveer Afzal Faruquie	The last few years has seen an exponential increase in the amount of social media data generated daily. Thus, researchers have started exploring the use of social media data in building recommendation systems, prediction models, improving disaster management, discovery trending topics etc. An interesting application of social media is for the prediction of election results. The recently conducted 2012 US Presidential election was the "most tweeted" election in history and provides a rich source of social media posts. Previous work on predicting election outcomes from social media has been largely been based on sentiment about candidates, total volumes of tweets expressing electoral polarity and the like. In this paper we use a collection of tweets to predict the daily approval ratings of the two US presidential candidates and also identify topics that were causal to the approval ratings.	Understanding election candidate approval ratings using social media data	NA:NA	2018
Xin Liu:Tsuyoshi Murata:Ken Wakita	Many real-world networks contain nonstructural information on nodes, such as the spatial coordinate of a location, profile of a person, or contents of a web page. In this paper, we propose Dist-Modularity, a unified modularity measure, which is useful in extracting the multilevel communities based on network structural and nonstructural information.	Extracting the multilevel communities based on network structural and nonstructural information	NA:NA:NA	2018
Jia Yantao:Wang Yuanzhuo:Li Jingyuan:Feng Kai:Cheng Xueqi:Li Jianchen	Link prediction in Microblogs by using unsupervised methods aims to find an appropriate similarity measure between users in the network. However, the measures used by existing work lack a simple way to incorporate the structure of the network and the interactions between users. In this work, we define the retweet similarity to measure the interactions between users in Twitter, and propose a structural-interaction based matrix factorization model for following-link prediction. Experiments on the real world Twitter data show our model outperforms state-of-the-art methods.	Structural-interaction link prediction in microblogs	NA:NA:NA:NA:NA:NA	2018
Jay Yoon Lee:U. Kang:Danai Koutra:Christos Faloutsos	Given a large cloud of multi-dimensional points, and an off-the shelf outlier detection method, why does it take a week to finish? After careful analysis, we discovered that duplicate points create subtle issues, that the literature has ignored: if dmax is the multiplicity of the most over-plotted point, typical algorithms are quadratic on dmax. We propose several ways to eliminate the problem; we report wall-clock times and our time savings; and we show that our methods give either exact results, or highly accurate approximate ones.	Fast anomaly detection despite the duplicates	NA:NA:NA:NA	2018
Ping-Han Soh:Yu-Chieh Lin:Ming-Syan Chen	In recent years, online social networks have been dramatically expanded. Active users spend hours communicating with each other via these networks such that an enormous amount of data is created every second. The tremendous amount of newly created information costs users much time to discover interesting messages from their online social feeds. The problem is even exacerbated if users access these networks via mobile devices. To assist users in discovering interesting messages efficiently, in this paper, we propose a new approach to recommend interesting messages for each user by exploiting the user's response behavior. We extract data from the most popular social network, and the experimental results show that the proposed approach is effective and efficient.	Recommendation for online social feeds by exploiting user response behavior	NA:NA:NA	2018
Simon de la Rouviere:Kobus Ehlers	When following too many users on microblogging services, information overload occurs due to increased and varied communication activity. Users then either leave, or employ coping strategies to continue benefiting from the service. Through a crawl of 31 684 random users from Twitter and a qualitative survey with 115 respondents, it has been determined that by using lists as an information management coping strategy (filtering and compartmentalising varied communication activity), users are capable of following more users and experience fewer symptoms of information overload.	Lists as coping strategy for information overload on Twitter	NA:NA	2018
Thomas Steiner:Christopher Chedeau	We have developed an application for the automatic generation of media galleries that visually and audibly summarize events based on media items like videos and photos from multiple social networks. Further, we have evaluated different media gallery styles with online surveys and examined their pros and cons. Besides the survey results, our contribution is also the application itself, where media galleries of different styles can be created on-the-fly. A demo is available at http://social-media-illustrator.herokuapp.com/.	To crop, or not to crop: compiling online media galleries	NA:NA	2018
Nikita Spirin:Karrie Karahalios	Aiming to improve user experience for a job search engine, in this paper we propose an idea to switch from query-biased snippets used by most web search engines to rich structured snippets associated with the main sections of a job posting page, which are more appropriate for job search due to specific user needs and the structure of job pages. We present a very simple yet actionable approach to generate such snippets in an unsupervised way. The advantages of the proposed approach are two-fold: it doesn't require manual annotation and therefore can be easily deployed to many languages, which is a desirable property for a job search engine operating internationally; it fuses naturally with the trend towards Mobile Web where the content needs to be optimized for small screen devices and informativeness.	Unsupervised approach to generate informative structured snippets for job search engines	NA:NA	2018
Lei Guo:Jun Ma:Zhumin Chen	Traditionally, trust-aware recommendation methods that utilize trust relations for recommender systems assume a single type of trust between users. However, this assumption ignores the fact that trust as a social concept inherently has many aspects. A user may place trust differently to different people. Motivated by this observation, we propose a novel probabilistic factor analysis method, which learns the multi-faceted trust relations and user profiles through a shared user latent feature space. Experimental results on the real product rating data set show that our approach outperforms state-of-the-art methods on the RMSE measure.	Learning to recommend with multi-faceted trust in social networks	NA:NA:NA	2018
Jongin Lee:John Kim:KwanHong Lee	Although the Web has abundant information, it does not necessarily contain the latest, most recently updated information. In particular, interactive map websites and the accompanying street view applications often contain information that is a few years old and are somewhat outdated because street views can change quickly. In this work, we propose Hidden View - a human computation mobile game that enables the updating of maps and street views with the latest information. The preliminary implementation of the game is described and some results collected from a sample user study are presented. This work is the first step towards leveraging human computation and an individual's familiarity with different points-of-interest to keep maps and street views up to date.	Hidden view game: designing human computation games to update maps and street views	NA:NA:NA	2018
Vasileios Triglianos:Cesare Pautasso	ASQ is a Web application for creating and delivering interactive HTML5 presentations. It is designed to support teachers that need to gather real-time feedback from the students while delivering their lectures. Presentation slides are delivered to viewers that can answer the questions embedded in the slides. The objective is to maximize the efficiency of bi-directional communication between the lecturer and a large audience. More specifically, in the context of a hybrid MOOC classroom, a teacher can use ASQ to get feedback in real time about the level of comprehension of the presented material while reducing the time for gathering survey data, monitoring attendance and assessing solutions.	ASQ: interactive web presentations for hybrid MOOCs	NA:NA	2018
Yingzhong Xu:Songlin Hu	Although HiveQL offers similar features with SQL, it is still difficult to map complex SQL queries into HiveQL and manual translation often leads to poor performance. A tool named QMapper is developed to address this problem by utilizing query rewriting rules and cost-based MapReduce flow evaluation on the basis of column statistics. Evaluation demonstrates that while assuring the correctness, QMapper improves the performance up to 42% in terms of execution time.	QMapper: a tool for SQL optimization on hive using query rewriting	NA:NA	2018
Rebeca Schroeder:Raqueline Penteado:Carmem Satie Hara	One approach to leverage scalable systems for RDF management is partitioning large datasets across distributed servers. In this paper we consider workload data, given in the form of query patterns and their frequencies, for determining how to partition RDF datasets. Our experimental study shows that our workload-aware method is an effective way to cluster related data and provides better query response times compared to an elementary fragmentation method.	Partitioning RDF exploiting workload information	NA:NA:NA	2018
Lina Yao:Quan Z. Sheng	With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web services, Web of Things (WoT) is gaining a considerable momentum as an emerging paradigm where billions of physical objects will be interconnected and present over the World Wide Web. One inevitable challenge in the new era of WoT lies in how to efficiently and effectively manage things, which is critical for a number of important applications such as object search, recommendation, and composition. In this paper, we propose a novel approach to discover the correlations of things by constructing a relational network of things (RNT) where similar things are linked via virtual edges according to their latent correlations by mining three dimensional information in the things usage events in terms of user, temporality and spatiality. With RNT, many problems centered around things management such as objects classification, discovery and recommendation can be solved by exploiting graph-based algorithms. We conducted experiments using real-world data collected over a period of four months to verify and evaluate our model and the results demonstrate the feasibility of our approach.	Correlation discovery in web of things	NA:NA	2018
Cesare Pautasso:Masiar Babazadeh	The Atomic Web Browser achieves atomicity for distributed transactions across multiple RESTful APIs. Assuming that the participant APIs feature support for the Try-Confirm/Cancel pattern, the user may navigate with the Atomic Web Browser among multiple Web sites to perform local resource state transitions (e.g., reservations or bookings). Once the user indicates that the navigation has successfully completed, the Atomic Web browser takes care of confirming the local transitions to achieve the atomicity of the global transaction.	The atomic web browser	NA:NA	2018
Pierre Geneves:Nabil Layaida	One major concept in web development using XML is validation: checking whether some document instance fulfills structural constraints described by some schema. Over the last few years, there has been a growing debate about XML validation, and two main schools of thought emerged about the way it should be done. On the one hand, some advocate the use of validation with respect to complete grammar-based descriptions such as DTDs and XML Schemas. On the other hand, motivated by a need for greater flexibility, others argue for no validation at all, or prefer the use of lightweight constraint languages such as Schematron with the aim of validating only required constraints, while making schema descriptions more compositional and more reusable. Owing to a logical compilation, we show that validators used in each of these approaches share the same theoretical foundations, meaning that the two approaches are far from being incompatible. Our findings include that the logic in [2] can be seen as a unifying formal ground for the construction of robust and efficient validators and static analyzers using any of these schema description techniques. This reconciles the two approaches from both a theoretical and a practical perspective, therefore facilitating any combination of them.	XML validation: looking backward - strongly typed and flexible XML processing are not incompatible	NA:NA	2018
Ayush Dubey:Pradipta De:Kuntal Dey:Sumit Mittal:Vikas Agarwal:Malolan Chetlur:Sougata Mukherjea	Mobile Web is characterized by two salient features, ubiquitous access to content and limited resources, like bandwidth and battery. Since most web pages are designed for the wired Internet, it is challenging to adapt the pages seamlessly to ensure a satisfactory mobile web experience. Content heavy web pages lead to longer load time on mobile browsers. Pre-defined load order of items in a page does not adapt to mobile browsing habits, where user looks for different snippets of a page to load under different contexts. Web content adaptation for mobile web has mainly focused on the user to define her preferences for content. We propose a framework where content creator is additionally included in guiding the adaptation. Allowing content creator to specify importance of items in a page also helps in factoring her incentives by pushing revenue generating content. We present mechanisms to enable cooperative content adaptation. Preliminary results show the efficacy of cooperative content adaptation in resource constrained mobile browsing scenario.	Co-operative content adaptation framework: satisfying consumer and content creator in resource constrained browsing	NA:NA:NA:NA:NA:NA:NA	2018
Guansong Pang:Huidong Jin:Shengyi Jiang	Motivated by the effectiveness of centroid-based text classification techniques, we propose a classification-oriented class-centroid-based dimension reduction (DR) method, called CentroidDR. Basically, CentroidDR projects high-dimensional documents into a low-dimensional space spanned by class centroids. On this class-centroid-based space, the centroid-based classifier essentially becomes CentroidDR plus a simple linear classifier. Other classification techniques, such as K-Nearest Neighbor (KNN) classifiers, can be used to replace the simple linear classifier to form much more effective text classification algorithms. Though CentroidDR is simple, non-parametric and runs in linear time, preliminary experimental results show that it can improve the accuracy of the classifiers and perform better than general DR methods such as Latent Semantic Indexing (LSI).	An effective class-centroid-based dimension reduction method for text classification	NA:NA:NA	2018
Arkaitz Zubiaga:Heng Ji	Classification is paramount for an optimal processing of tweets, albeit performance of classifiers is hindered by the need of large sets of training data to encompass the diversity of contents one can find on Twitter. In this paper, we introduce an inexpensive way of labeling large sets of tweets, which can be easily regenerated or updated when needed. We use human-edited web page directories to infer categories from URLs contained in tweets. By experimenting with a large set of more than 5 million tweets categorized accordingly, we show that our proposed model for tweet classification can achieve 82% in accuracy, performing only 12.2% worse than for web page classification.	Harnessing web page directories for large-scale classification of tweets	NA:NA	2018
Youngki Park:Sungchan Park:Sang-goo Lee:Woosung Jung	K-Nearest Neighbor Graph (K-NNG) construction is a primitive operation in the field of Information Retrieval and Recommender Systems. However, existing approaches to K-NNG construction do not perform well as the number of nodes or dimensions scales up. In this paper, we present greedy filtering, an effcient and scalable algorithm for selecting the candidates for nearest neighbors by matching only the dimensions of large values. The experimental results show that our K-NNG construction scheme, based on greedy filtering, guarantees a high recall while also being 5 to 6 times faster than state-of-the-art algorithms for large, high-dimensional data.	Scalable k-nearest neighbor graph construction based on greedy filtering	NA:NA:NA:NA	2018
Jie Wu:Yi Liu:Ji-Rong Wen	We handle a special category of Web queries, queries containing numeric terms. We call them numeric queries. Motivated by some issues in ranking of numeric queries, we detect numeric sensitive queries by mining from retrieved documents using phrase operator. We also propose features based on numeric terms by extracting reliable numeric terms for each document. Finally, a ranking model is trained for numeric sensitive queries, combining proposed numeric-related features and traditional features. Experiments show that our model can significantly improve relevance for numeric sensitive queries.	Numeric query ranking approach	NA:NA:NA	2018
Defu Lian:Vincent W. Zheng:Xing Xie	With the increasing popularity of Location-based Social Networks, a vast amount of location check-ins have been accumulated. Though location prediction in terms of check-ins has been recently studied, the phenomena that users often check in novel locations has not been addressed. To this end, in this paper, we leveraged collaborative filtering techniques for check-in location prediction and proposed a short- and long-term preference model. We extensively evaluated it on two large-scale check-in datasets from Gowalla and Dianping with 6M and 1M check-ins, respectively, and showed that the proposed model can outperform the competing baselines.	Collaborative filtering meets next check-in location prediction	NA:NA:NA	2018
Yu Jiang:Jing Liu:Xi Zhang:Zechao Li:Hanqing Lu	In this paper, we develop a novel product recommendation method called TCRec, which takes advantage of consumer rating history record, social-trust network and product category information simultaneously. Compared experiments are conducted on two real-world datasets and outstanding performance is achieved, which demonstrates the effectiveness of TCRec.	TCRec: product recommendation via exploiting social-trust network and product category information	NA:NA:NA:NA:NA	2018
Takeshi Sakaki:Fujio Toriumi:Kosuke Shinoda:Kazuhiro Kazama:Satoshi Kurihara:Itsuki Noda:Yutaka Matsuo	Social media attract attention for sharing information, especially Twitter, which is now being used in times of disasters. In this paper, we perform regional analysis of user interactions on Twittter during the Great East Japan Earthquake and arrived at the following two conclusions:People diffused much more information after the earthquake, especially in the heavily-damaged areas; People communicated with nearby users but diffused information posted by distant users. We conclude that social media users changed their behavior to widely diffuse information.	Regional analysis of user interactions on social media in times of disaster	NA:NA:NA:NA:NA:NA:NA	2018
Ricardo M. Marcacini:Marcos A. Domingues:Solange O. Rezende	Consensus clustering and interactive feature selection are very useful methods to extract and manage knowledge from texts. While consensus clustering allows the aggregation of different clustering solutions into a single robust clustering solution, the interactive feature selection facilitates the incorporation of the users experience in text clustering tasks by selecting a set of high-level features. In this paper, we propose an approach to improve the robustness of consensus clustering using interactive feature selection. We have reported some experimental results on real-world datasets that show the effectiveness of our approach.	Improving consensus clustering of texts using interactive feature selection	NA:NA:NA	2018
James Lo:Eric Wohlstadter:Ali Mesbah	Due to the increasing complexity of web applications and emerging HTML5 standards, a large amount of runtime state is created and managed in the user's browser. While such complexity is desirable for user experience, it makes it hard for developers to implement mechanisms that provide users ubiquitous access to the data they create during application use. This work showcases Imagen, our implemented platform for browser session migration of JavaScript-based web applications. Session migration is the act of transferring a session between browsers at runtime. Without burden to developers, Imagen allows users to create a snapshot image that captures the runtime state needed to resume the session elsewhere. Our approach works completely in the JavaScript layer and we demonstrate that snapshots can be transferred between different browser vendors and hardware devices. The demo will illustrate our system's performance and interoperability using two HTML5 apps, four different browsers and three different devices.	Live migration of JavaScript web apps	NA:NA:NA	2018
Gabriel Le Breton:Fabien Maronnaud:Sylvain Hallé	WebMole is a browser-based tool that automatically and exhaustively explores all pages inside a web application. Contrarily to classical web crawlers, which only explore pages accessible through regular anchors, WebMole can find its way through Ajax applications that use JavaScript-triggered links, and handles state changes that do not involve a page reload. User-defined functions called oracles can be used to bound the range of pages explored by WebMole to specific parts of an application, as well as to evaluate Boolean test conditions on all visited pages. Overall, WebMole can prove a more flexible alternative to automated testing suites such as Selenium WebDriver.	Automated exploration and analysis of ajax web applications with WebMole	NA:NA:NA	2018
Matthias Heinrich:Franz Lehmann:Franz Josef Grüneberger:Thomas Springer:Martin Gaedke	Multi-user web applications like Google Docs or Etherpad are crucial to efficiently support collaborative work (e.g. jointly create texts, graphics, or presentations). Nevertheless, enhancing single-user web applications with multi-user capabilities (i.e. document synchronization and conflict resolution) is a time-consuming and intricate task since traditional approaches adopting concurrency control libraries (e.g. Apache Wave) require numerous scattered source code changes. Therefore, we devised the Generic Collaboration Infrastructure (GCI) [8] that is capable of converting single-user web applications non-invasively into collaborative ones, i.e. no source code changes are required. In this paper, we present a catalog of vital application properties that allows determining if a web application is suitable for a GCI transformation. On the basis of the introduced catalog, we analyze 12 single-user web applications and show that 6 are eligible for a GCI transformation. Moreover, we demonstrate (1) the transformation of one qualified application, namely, the prominent text editor TinyMCE, and (2) showcase the resulting multi-user capabilities. Both demo parts are illustrated in a dedicated screencast that is available at http://vsr.informatik.tu-chemnitz.de/demo/TinyMCE/.	Analyzing the suitability of web applications for a single-user to multi-user transformation	NA:NA:NA:NA:NA	2018
Philipp Langhans:Christoph Wieser:François Bry	JSMapReduce is an implementation of MapReduce which exploits the computing power available in the computers of the users of a web platform by giving tasks to the JavaScript engines of their web browsers. This article describes the implementation of JSMapReduce exploiting HTML 5 features, the heuristics it uses for distributing tasks to workers, and reports on an experimental evaluation of JSMapReduce.	Crowdsourcing MapReduce: JSMapReduce	NA:NA:NA	2018
Christoph Boden:Marcel Karnstedt:Miriam Fernandez:Volker Markl	The importance of social-media platforms and online communities - in business as well as public context - is more and more acknowledged and appreciated by industry and researchers alike. Consequently, a wide range of analytics has been proposed to understand, steer, and exploit the mechanics and laws driving their functionality and creating the resulting benefits. However, analysts usually face significant problems in scaling existing and novel approaches to match the data volume and size of modern online communities. In this work, we propose and demonstrate the usage of the massively parallel data processing system Stratosphere, based on second order functions as an extended notion of the MapReduce paradigm, to provide a new level of scalability to such social-media analytics. Based on the popular example of role analysis, we present and illustrate how this massively parallel approach can be leveraged to scale out complex data-mining tasks, while providing a programming approach that eases the formulation of complete analytical workflows.	Large-scale social-media analytics on stratosphere	NA:NA:NA:NA	2018
HyeongSik Kim:Padmashree Ravindra:Kemafor Anyanwu	Scalable processing of Semantic Web queries has become a critical need given the rapid upward trend in availability of Semantic Web data. The MapReduce paradigm is emerging as a platform of choice for large scale data processing and analytics due to its ease of use, cost effectiveness, and potential for unlimited scaling. Processing queries on Semantic Web triple models is a challenge on the mainstream MapReduce platform called Apache Hadoop, and its extensions such as Pig and Hive. This is because such queries require numerous joins which leads to lengthy and expensive MapReduce workflows. Further, in this paradigm, cloud resources are acquired on demand and the traditional join optimization machinery such as statistics and indexes are often absent or not easily supported. In this demonstration, we will present RAPID+, an extended Apache Pig system that uses an algebraic approach for optimizing queries on RDF data models including queries involving inferencing. The basic idea is that by using logical and physical operators that are more natural to MapReduce processing, we can reinterpret such queries in a way that leads to more concise execution workflows and small intermediate data footprints that minimize disk I/Os and network transfer overhead. RAPID+ evaluates queries using the Nested TripleGroup Data Model and Algebra(NTGA). The demo will show comparative performance of NTGA query plans vs. relational algebra-like query plans used by Apache Pig and Hive.	Optimizing RDF(S) queries on cloud platforms	NA:NA:NA	2018
Marcio dos Santos Galli:Eduardo Pezutti Beletato Santos	New interaction experiences are fundamentally changing the way we interact with the web. Emerging touch-based devices and a variety of web-connected appliances represents challenges that prevents the seamless reach of web resources originally tailored for the standard browser experience. This paper explores how web pages can be re-purposed and become interactive presentations that effectively supports communication in scenarios such as digital signage and other presentation use cases. We will cover the TagVisor project which is a JavaScript run-time that uses modern animation effects and provides an HTML5 extension approach to support the authoring of visual narratives using plain web pages.	TagVisor: extending web pages with interaction events to support presentation in digital signage	NA:NA	2018
Soudip Roy Chowdhury:Olexiy Chudnovskyy:Matthias Niederhausen:Stefan Pietschmann:Paul Sharples:Florian Daniel:Martin Gaedke	Despite several efforts for simplifying the composition process, learning efforts required for using existing mashup editors to develop mashups remain still high. In this paper, we describe how this barrier can be lowered by means of an assisted development approach that seamlessly integrates automatic composition and interactive pattern recommendation techniques into existing mashup platforms for supporting easy mashup development by end users. We showcase the use of such an assisted development environment in the context of an open-source mashup platform Apache Rave. Results of our user studies demonstrate the benefits of our approach for end user mashup development.	Complementary assistance mechanisms for end user mashup composition	NA:NA:NA:NA:NA:NA:NA	2018
Tiago Rodrigues:Prateek Dewan:Ponnurangam Kumaraguru:Raquel Melo Minardi:Virgílio Almeida	The past one decade has witnessed an astounding outburst in the number of online social media (OSM) services, and a lot of these services have enthralled millions of users across the globe. With such tremendous number of users, the amount of content being generated and shared on OSM services is also enormous. As a result, trying to visualize all this overwhelming amount of content, and gain useful insights from it has become a challenge. In this work, we present uTrack, a personalized web service to analyze and visualize the diffusion of content shared by users across multiple OSM platforms. To the best of our knowledge, there exists no work which concentrates on monitoring information diffusion for personal accounts. Currently, uTrack monitors and supports logging in from Facebook, Twitter, and Google+. Once granted permissions by the user, uTrack monitors all URLs (like videos, photos, news articles) the user has shared in all OSM services supported, and generates useful visualizations and statistics from the collected data.	uTrack: track yourself! monitoring information on online social media	NA:NA:NA:NA:NA	2018
Bifan Wei:Jun Liu:Jian Ma:Qinghua Zheng:Wei Zhang:Boqin Feng	Extracting faceted taxonomies from the Web has received increasing attention in recent years from the web mining community. We demonstrate in this study a novel system called DFT-Extractor, which automatically constructs domain-specific faceted taxonomies from Wikipedia in three steps: 1) It crawls domain terms from Wikipedia by using a modified topical crawler. 2) Then it exploits a classification model to extract hyponym relations with the use of motif-based features. 3) Finally, it constructs a faceted taxonomy by applying a community detection algorithm and a group of heuristic rules. DFT-Extractor also provides a graphical user interface to visualize the learned hyponym relations and the tree structure of taxonomies.	DFT-extractor: a system to extract domain-specific faceted taxonomies from wikipedia	NA:NA:NA:NA:NA:NA	2018
Mihai Georgescu:Dang Duc Pham:Nattiya Kanhabua:Sergej Zerr:Stefan Siersdorfer:Wolfgang Nejdl	Wikipedia is a free multilingual online encyclopedia covering a wide range of general and specific knowledge. Its content is continuously maintained up-to-date and extended by a supporting community. In many cases, real-world events influence the collaborative editing of Wikipedia articles of the involved or affected entities. In this paper, we present Wikipedia Event Reporter, a web-based system that supports the entity-centric, temporal analytics of event-related information in Wikipedia by analyzing the whole history of article updates. For a given entity, the system first identifies peaks of update activities for the entity using burst detection and automatically extracts event-related updates using a machine-learning approach. Further, the system determines distinct events through the clustering of updates by exploiting different types of information such as update time, textual similarity, and the position of the updates within an article. Finally, the system generates the meaningful temporal summarization of event-related updates and automatically annotates the identified events in a timeline.	Temporal summarization of event-related updates in wikipedia	NA:NA:NA:NA:NA:NA	2018
Vuk Milicic:Giuseppe Rizzo:José Luis Redondo Garcia:Raphaël Troncy:Thomas Steiner	Social platforms constantly record streams of heterogeneous data about human's activities, feelings, emotions and conversations opening a window to the world in real-time. Trends can be computed but making sense out of them is an extremely challenging task due to the heterogeneity of the data and its dynamics making often short-lived phenomena. We develop a framework which collects microposts shared on social platforms that contain media items as a result of a query, for example a trending event. It automatically creates different visual storyboards that reflect what users have shared about this particular event. More precisely it leverages on: (i) visual features from media items for near-deduplication, and (ii) textual features from status updates to interpret, cluster, and visualize media items. A screencast showing an example of these functionalities is published at: http://youtu.be/8iRiwz7cDYY while the prototype is publicly available at http://mediafinder.eurecom.fr.	Live topic generation from event streams	NA:NA:NA:NA:NA	2018
Pramod Verma	This paper presents the design and implementation of a social networking website for classifieds, called Serefind. We designed search interfaces with focus on security, privacy, usability, design, ranking, and communications. We deployed this site at the Johns Hopkins University, and the results show it can be used as a self-sustaining classifieds site for public or private communities.	Serefind: a social networking website for classifieds	NA	2018
Seth B. Cleveland:Byron J. Gao	Faceted search combines faceted navigation with direct keyword search, providing exploratory search capacities allowing progressive query refinement. It has become the de facto standard for e-commerce and product-related websites such as amazon.com and ebay.com. However, faceted search has not been effectively incorporated into non-commercial online community portals such as craigslist.org. This is mainly because unlike keyword search, faceted search systems require metadata that constantly evolve, making them very costly to build and maintain. In this paper, we propose a framework MASFA that utilizes a set of non-domain-specific techniques to build and maintain effective, portable, and cost-free faceted search systems in a mass-collaborative manner. We have implemented and deployed the framework on selected categories of Craigslist to demonstrate its utility.	MASFA: mass-collaborative faceted search for online communities	NA:NA	2018
Valter Crescenzi:Paolo Merialdo:Disheng Qiu	The development of solutions to scale the extraction of data from Web sources is still a challenging issue. High accuracy can be achieved by supervised approaches, but the costs of training data, i.e., annotations over a set of sample pages, limit their scalability. Crowdsourcing platforms are making the manual annotation process more affordable. However, the tasks demanded to these platforms should be extremely simple, to be performed by non-expert people, and their number should be minimized, to contain the costs. We demonstrate ALFRED, a wrapper inference system supervised by the workers of a crowdsourcing platform. Training data are labeled values generated by means of membership queries, the simplest form of queries, posed to the crowd. ALFRED includes several original features: it automatically selects a representative sample set from the input collection of pages; in order to minimize the wrapper inference costs, it dynamically sets the expressiveness of the wrapper formalism and it adopts an active learning algorithm to select the queries posed to the crowd; it is able to manage inaccurate answers that can be provided by the workers engaged by crowdsourcing platforms.	ALFRED: crowd assisted data extraction	NA:NA:NA	2018
Roberto Yus:Eduardo Mena:Sergio Ilarri:Arantza Illarramendi	Nowadays people are exposed to huge amounts of information that are generated continuously. However, current mobile applications, Web pages, and Location-Based Services (LBSs) are designed for specific scenarios and goals. In this demo we show the system SHERLOCK, which searches and shares up-to-date knowledge from nearby devices to relieve the user from knowing and managing such knowledge directly. Besides, the system guides the user in the process of selecting the service that best fits his/her needs in the given context.	SHERLOCK: a system for location-based services in wireless environments using semantics	NA:NA:NA:NA	2018
Mozhgan Tavakolifard:Jon Atle Gulla:Kevin C. Almeroth:Jon Espen Ingvaldesn:Gaute Nygreen:Erik Berg	Mobile news recommender systems help users retrieve news that is relevant in their particular context and can be presented in ways that require minimal user interaction. In spite of the availability of contextual information about mobile users, though, current mobile news applications employ rather simple strategies for news recommendation. Our multi-perspective approach unifies temporal, locational, and preferential information to provide a more fine-grained recommendation strategy. This demo paper presents the implementation of our solution to efficiently recommend specific news articles from a large corpus of newly-published press releases in a way that closely matches a reader's reading preferences.	Tailored news in the palm of your hand: a multi-perspective transparent approach to news recommendation	NA:NA:NA:NA:NA:NA	2018
Lyndon Nixon:Matthias Bauer:Cristian Bara	This demo submission presents a set of tools and an extended framework with API for enabling the semantically empowered enrichment of online video with Web content. As audiovisual media is increasingly transmitted online, new services deriving added value from such material can be imagined. For example, combining it with other material elsewhere on the Web which is related to it or enhances it in a meaningful way, to the benefit of the owner of the original content, the providers of the content enhancing it and the end consumer who can access and interact with these new services. Since the services are built around providing new experiences through connecting different related media together, we consider such services to be Connected Media Experiences (ConnectME). This paper presents a toolset for ConnectME - an online annotation tool for video and a HTML5-based enriched video player - as well as the ConnectME framework which enables these media experiences to be generated on the server side with semantic technology.	Connected media experiences: web based interactive video using linked data	NA:NA:NA	2018
Álvaro R. Pereira, Jr.:Diego Dutra:Milton Stiilpen, Jr.:Alex Amorim Dutra:Felipe Martins Melo:Paulo H. C. Mendonça:Ângelo Magno de Jesus:Kledilson Ferreira	Radialize represents a service for listening to music and radio programs through the web. The service allows the discovery of the content being played by radio stations on the web, either by managing explicit information made available by those stations or by means of our technology for automatic recognition of audio content in a stream. Radialize then offers a service in which the user can search, be recommended, and provide feedback on artists and songs being played in traditional radio stations, either explicitly or implicitly, in order to compose an individual profile. The recommender system utilizes every user interaction as a data source, as well as the similarity abstraction extracted out of the radios' musical programs, making use of the wisdom of crowds implicitly present in the radio programs.	Radialize: a tool for social listening experience on the web based on radio station programs	NA:NA:NA:NA:NA:NA:NA:NA	2018
Steven C.H. Hoi:Dayong Wang:I. Yeu Cheng:Elmer Weijie Lin:Jianke Zhu:Ying He:Chunyan Miao	Auto face annotation is an important technique for many real-world applications, such as online photo album management, new video summarization, and so on. It aims to automatically detect human faces from a photo image and further name the faces with the corresponding human names. Recently, mining web facial images on the internet has emerged as a promising paradigm towards auto face annotation. In this paper, we present a demonstration system of search-based face annotation: FANS - Face ANnotation by Searching large-scale web facial images. Given a query facial image for annotation, we first retrieve a short list of the most similar facial images from a web facial image database, and then annotate the query facial image by mining the top-ranking facial images and their corresponding labels with sparse representation techniques. Our demo system was built upon a large-scale real-world web facial image database with a total of 6,025 persons and about 1 million facial images. This paper demonstrates the potential of searching and mining web-scale weakly labeled facial images on the internet to tackle the challenging face annotation problem, and addresses some open problems for future exploration by researchers in web community. The live demo of FANS is available online at http://msm.cais.ntu.edu.sg/FANS/.	FANS: face annotation by searching large-scale web facial images	NA:NA:NA:NA:NA:NA:NA	2018
Daniel Gomes:David Cruz:João Miranda:Miguel Costa:Simão Fontes	The web was invented to quickly exchange data between scientists, but it became a crucial communication tool to connect the world. However, the web is extremely ephemeral. Most of the information published online becomes quickly unavailable and is lost forever. There are several initiatives worldwide that struggle to archive information from the web before it vanishes. However, search mechanisms to access this information are still limited and do not satisfy their users who demand performance similar to live-web search engines. This demo presents the Portuguese Web Archive, which enables search over 1.2 billion files archived from 1996 to 2012. It is the largest full-text searchable web archive publicly available [17]. The software developed to support this service is also publicly available as a free open source project at Google Code, so that it can be reused and enhanced by other web archivists. A short video about the Portuguese Web Archive is available at vimeo.com/59507267. The service can be tried live at archive.pt.	Search the past with the portuguese web archive	NA:NA:NA:NA:NA	2018
Joanna Biega:Erdal Kuzey:Fabian M. Suchanek	YAGO [9, 6] is one of the largest public ontologies constructed by information extraction. In a recent refactoring called YAGO2s, the system has been given a modular and completely transparent architecture. In this demo, users can see how more than 30 individual modules of YAGO work in parallel to extract facts, to check facts for their correctness, to deduce facts, and to merge facts from different sources. A GUI allows users to play with different input files, to trace the provenance of individual facts to their sources, to change deduction rules, and to run individual extractors. Users can see step by step how the extractors work together to combine the individual facts to the coherent whole of the YAGO ontology.	Inside YAGO2s: a transparent information extraction architecture	NA:NA:NA	2018
Axel-Cyrille Ngonga Ngomo:Lorenz Bühmann:Christina Unger:Jens Lehmann:Daniel Gerber	Linked Data technologies are now being employed by a large number of applications. While experts can query the backend of these applications using the standard query language SPARQL, most lay users lack the expertise necessary to proficiently interact with these applications. Consequently, non-expert users usually have to rely on forms, query builders, question answering or keyword search tools to access RDF data. Yet, these tools are usually unable to make the meaning of the queries they generate plain to lay users, making it difficult for these users to i) assess the correctness of the query generated out of their input, and ii) to adapt their queries or iii) to choose in an informed manner between possible interpretations of their input. We present SPARQL2NL, a generic approach that allows verbalizing SPARQL queries, i.e., converting them into natural language. In addition to generating verbalizations, our approach can also explain the output of queries by providing a natural-language description of the reasons that led to each element of the result set being selected. Our evaluation of SPARQL2NL within a large-scale user survey shows that SPARQL2NL generates complete and easily understandable natural language descriptions. In addition, our results suggest that even SPARQL experts can process the natural language representation of SPARQL queries computed by our approach more efficiently than the corresponding SPARQL queries. Moreover, non-experts are enabled to reliably understand the content of SPARQL queries. Within the demo, we present the results generated by our approach on arbitrary questions to the DBpedia and MusicBrainz datasets. Moreover, we present how our framework can be used to explain results of SPARQL queries in natural language	SPARQL2NL: verbalizing sparql queries	NA:NA:NA:NA:NA	2018
Yiyuan Bai:Chaokun Wang:Yuanchi Ning:Hanzhao Wu:Hao Wang	With the socialization trend of web sites and applications, the techniques of effective management of graph-structured data have become one of the most important modern web technologies. In this paper, we present a system of path query on large graphs, known as G-Path. Based on Hadoop distributed framework and bulk synchronized parallel model, the system can process generic queries without preprocessing or building indices. To demonstrate the system, we developed a web-based application which allows searching entities and relationships on a large social network, e.g., DBLP publication network or Twitter dataset. With the flexibility of G-Path, the application is able to handle different kinds of queries. For example, a user may want to search for a publication graph of an author while another user may want to search for all publications of the author's co-authors. All these queries can be done by an interactive user interface and the results will be shown in a visual graph.	G-path: flexible path pattern query on large graphs	NA:NA:NA:NA:NA	2018
Edward Benson	Dynamic web development still borrows heavily from its origins in CGI scripts: modern web applications are largely designed and developed as programs that happen to output HTML. This thesis proposes to investigate the idea taking a mockup-centric approach instead, in which self-contained, full page web mockups are the central artifact driving the application development process. In some cases, these mockups are sufficient to infer the dynamic application structure completely. This approach to mockup driven development is made possible by the development of a language the thesis develops, called Cascading Tree Sheets (CTS), that enables a mockup to be annotated with enough information so that many common web development tasks and workflows can be eliminated or vastly simplified. CTS describes and encapsulates a web page's design structure the same way CSS describes its styles. This enables mockups to serve as the input of a web application rather than simply a design artifact. Using this capability, I will study the feasibility and usability of mockup driven development for a range of novice and expert authorship tasks. The thesis aims to finish by demonstrating that the functionality of a domain-specific content management system can be inferred automatically from site mockups.	Mockup driven web development	NA	2018
Giang Binh Tran	Helping users to understand the news is an acute problem nowadays as the users are struggling to keep up with tremendous amount of information published every day in the Internet. In this research, we focus on modelling the content of news events by their semantic relations with other events, and generating structured summarization.	Structured summarization for news events	NA	2018
Teresa Bracamonte	Efforts have been made to obtain more accurate results for multimedia searches on the Web. Nevertheless, not all multimedia objects have related text descriptions available. This makes bridging the semantic gap more difficult. Approaches that combine context and content information of multimedia objects are the most popular for indexing and later retrieving these objects. However, scaling these techniques to Web environments is still an open problem. In this thesis, we propose the use of user-generated content (UGC) from the Web and social platforms as well as multimedia content information to describe the context of multimedia objects. We aim to design tag-oriented algorithms to automatically tag multimedia objects, filter irrelevant tags, and cluster tags in semantically-related groups. The novelty of our proposal is centered on the design of Web-scalable algorithms that enrich multimedia context using the social information provided by users as a result of their interaction with multimedia objects. We validate the results of our proposal with a large-scale evaluation in crowdsourcing platforms.	Multimedia information retrieval on the social web	NA	2018
Birhanu Eshete	The steady evolution of the Web has paved the way for miscreants to take advantage of vulnerabilities to embed malicious content into web pages. Up on a visit, malicious web pages steal sensitive data, redirect victims to other malicious targets, or cease control of victim's system to mount future attacks. Approaches to detect malicious web pages have been reactively effective at special classes of attacks like drive-by-downloads. However, the prevalence and complexity of attacks by malicious web pages is still worrisome. The main challenges in this problem domain are (1) fine-grained capturing and characterization of attack payloads (2) evolution of web page artifacts and (3) exibility and scalability of detection techniques with a fast-changing threat landscape. To this end, we proposed a holistic approach that leverages static analysis, dynamic analysis, machine learning, and evolutionary searching and optimization to effectively analyze and detect malicious web pages. We do so by: introducing novel features to capture fine-grained snapshot of malicious web pages, holistic characterization of malicious web pages, and application of evolutionary techniques to fine-tune learning-based detection models pertinent to evolution of attack payloads. In this paper, we present key intuition and details of our approach, results obtained so far, and future work.	Effective analysis, characterization, and detection of malicious web pages	NA	2018
Fabian Flöck	In this doctoral proposal, we describe an approach to identify recurring, collective behavioral mechanisms in the collaborative interactions of Wikipedia editors that have the potential to undermine the ideals of quality, neutrality and completeness of article content. We outline how we plan to parametrize these patterns in order to understand their emergence and evolution and measure their effective impact on content production in Wikipedia. On top of these results we intend to build end-user tools to increase the transparency of the evolution of articles and equip editors with more elaborated quality monitors. We also sketch out our evaluation plans and report on already accomplished tasks.	Identifying, understanding and detecting recurring, harmful behavior patterns in collaborative Wikipedia editing: doctoral proposal	NA	2018
Larissa A. Freitas:Renata Vieira	This paper presents a thesis whose goal is to propose and evaluate methods to identify polarity in Portuguese user generated reviews according to features described in domain ontologies (experiments will consider movie and hotel ontologies Movie Ontology1 and Hontology2).	Ontology based feature level opinion mining for portuguese reviews	NA:NA	2018
Amelie Gyrard	The emerging field Machine-to-Machine (M2M) enables machines to communicate with each other without human intervention. Existing semantic sensor networks are domain-specific and add semantics to the context. We design a Machine-to-Machine (M2M) architecture to merge heterogeneous sensor networks and we propose to add semantics to the measured data rather than to the context. This architecture enables to: (1) get sensor measurements, (2) enrich sensor measurements with semantic web technologies, domain ontologies and the Link Open Data, and (3) reason on these semantic measurements with semantic tools, machine learning algorithms and recommender systems to provide promising applications.	A machine-to-machine architecture to merge semantic sensor measurements	NA	2018
Mohamamdreza Khelghati:Djoerd Hiemstra:Maurice Van Keulen	NA	Deep web entity monitoring	NA:NA:NA	2018
Julia Kiseleva	Predictive Web Analytics is aimed at understanding behavioural patterns of users of various web-based applications: e-commerce, ubiquitous and mobile computing, and computational advertising. Within these applications business decisions often rely on two types of predictions: an overall or particular user segment demand predictions and individualised recommendations for visitors. Visitor behaviour is inherently sensitive to the context, which can be defined as a collection of external factors. Context-awareness allows integrating external explanatory information into the learning process and adapting user behaviour accordingly. The importance of context-awareness has been recognised by researchers and practitioners in many disciplines, including recommendation systems, information retrieval, personalisation, data mining, and marketing. We focus on studying ways of context discovery and its integration into predictive analytics.	Context mining and integration into predictive web analytics	NA	2018
Jaeseok Myung	Although there are numerous websites that provide recommendation services for various items such as movies, music, and books, most of studies on recommender systems only focus on one specific item type. As recommender sites expand to cover several types of items, though, it is important to build a hybrid web recommender system that can handle multiple types of items. The switch hybrid recommender model provides a solution to this problem by choosing an appropriate recommender system according to given selection criteria, thereby facilitating cross-domain recommendations supported by individual recommender systems. This paper seeks to answer the question of how to deal with situations where no appropriate recommender system exists to deal with a required type of item. In such cases, the switch model cannot generate recommendation results, leading to the need for a fallback model that can satisfy most users most of the time. Our fallback model exploits a graph-based proximity search, ranking every entity on the graph according to a given proximity measure. We study how to incorporate the fallback model into the switch model, and propose a general architecture and simple algorithms for implementing these ideas. Finally, we present the results of our research result and discuss remaining challenges and possibilities for future research.	A proximity-based fallback model for hybrid web recommender systems	NA	2018
Rishiraj Saha Roy	It is believed that Web search queries are becoming more structurally complex over time. However, there has been no systematic study that quantifies such characteristics. In this thesis, we propose that queries are evolving into a unique linguistic system. We demonstrate proof of this hypothesis by examining the structure of Web queries by applying well-established techniques from natural language understanding. Preliminary results of these experiments show quantitative and qualitative proof that queries are not just some form of text between random sequences of words and natural language - they have distinct properties of their own.	Analyzing linguistic structure of web search queries	NA	2018
Pinar Yanardag Delul	Microblogging is a form of blogging where posts typically consist of short content such as quick comments, phrases, URLs, or media, like images and videos. Because of the fast and compact nature of microblogs, users have adopted them for novel purposes, including sharing personal updates, spreading breaking news, promoting political views, marketing and tracking real time events. Thus, finding relevant information sources out of the rapidly growing content is an essential task. In this paper, we study the problem of understanding and analysing microblogs. We present a novel 2-stage framework to find potentially relevant content by extracting topics from the tweets and by taking advantage of submodularity.	Understanding and analysing microblogs	NA	2018
Stefan Dietze:Mathieu d'Aquin:Dragan Gasevic	LILE2013 Welcome and organization	Session details: LILE'13 workshop	NA:NA:NA	2018
Stefan Dietze	NA	Session details: LILE'13 keynote talk	NA	2018
Sweitze Roffel	Publishing has undergone many changes since the 1960's, often driven by rapid technological development. Technology impacts the creation and dissemination of knowledge only to a certain extent, and in this talk I'll try to give a publisher's perspective of some technological drivers impacting Academic publishing today, and how the many actors involved are learning to cooperate as well as compete in an increasingly distributed environment to better turn information into knowledge. Technically, organizationally, and with regard to shared standards and infrastructure. Publishing has been called many different things by many different people. A simple definition could be that publishing is 'organizing content', so the focus of this talk will be on Elsevier's current use of Linked Data & Semantic technology in organizing scientific content, including some early lessons learned. This view from a publisher aims to help the discussion on how we can all contribute to better disseminate and promote the enormous creativity made through core research contributions.	Linking data in and outside a scientific publishing house	NA	2018
Stefan Dietze	NA	Session details: LILE'13 session 1	NA	2018
Farhana Sarker:Thanassis Tiropanis:Hugh C Davis	Research in student retention and progression to completion is traditionally survey-based, where researchers collect data through questionnaires and interviewing students. The major issues with survey-based study are the potentially low response rates and cost. Nevertheless, a large number of datasets that could inform the questions that students are explicitly asked in surveys is commonly available in the external open datasets. This paper describes a new student predictive model for student progression that relies on the data available in institutional internal databases and external open data, without the need for surveys. The results of empirical study for undergraduate students in their first year of study shows that this model can perform as well as or even out-perform traditional survey-based ones.	Exploring student predictive model that relies on institutional databases and open data instead of traditional questionnaires	NA:NA:NA	2018
Davide Taibi:Besnik Fetahu:Stefan Dietze	Personalisation, adaptation and recommendation are central aims of Technology Enhanced Learning (TEL) environments. In this context, information retrieval and clustering techniques are more and more often applied to filter and deliver learning resources according to user preferences and requirements. However, the suitability and scope of possible recommendations is fundamentally dependent on the available data, such as metadata about learning resources as well as users. However, quantity and quality of both is still limited. On the other hand, throughout the last years, the Linked Data (LD) movement has succeeded to provide a vast body of well-interlinked and publicly accessible Web data. This in particular includes Linked Data of explicit or implicit educational nature. In this paper, we propose a large-scale educational dataset which has been generated by exploiting Linked Data methods together with clustering and interlinking techniques to extract import and interlink a wide range of educationally relevant data. We also introduce a set of reusable techniques which were developed to realise scalable integration and alignment of Web data in educational settings.	Towards integration of web data into a coherent educational data graph	NA:NA:NA	2018
Patrick Siehndel:Ricardo Kawase:Asmelash Teka Hadgu:Eelco Herder	Reference sites play an increasingly important role in learning processes. Teachers use these sites in order to identify topics that should be covered by a course or a lecture. Learners visit online encyclopedias and dictionaries to find alternative explanations of concepts, to learn more about a topic, or to better understand the context of a concept. Ideally, a course or lecture should cover all key concepts of the topic that it encompasses, but often time constraints prevent complete coverage. In this paper, we propose an approach to identify missing references and key concepts in a corpus of educational lectures. For this purpose, we link concepts in educational material to the organizational and linking structure of Wikipedia. Identifying missing resources enables learners to improve their understanding of a topic, and allows teachers to investigate whether their learning material covers all necessary concepts.	Finding relevant missing references in learning courses	NA:NA:NA:NA	2018
Stefan Dietze	NA	Session details: LILE'13 session 2	NA	2018
Alexander Mikroyannidis:John Domingue	There is currently a huge potential for eLearning in several new online learning initiatives like Massive Open Online Courses (MOOCs) and Open Educational Resources (OERs). These initiatives enable learners to self-regulate their learning by providing them with an abundant amount of free learning materials of high quality. This paper presents FORGE, a new European initiative for online learning using Future Internet Research and Experimentation (FIRE) facilities. FORGE is a step towards turning FIRE into a pan-European educational platform for Future Internet through Linked Data. This will benefit learners and educators by giving them both access to world class facilities in order to carry out experiments on e.g. new internet protocols. In turn, this supports constructivist and self-regulated learning approaches, through the use of interactive learning resources, such as eBooks.	Interactive learning resources and linked data for online scientific experimentation	NA:NA	2018
Danica Damljanovic:David Miller:Daniel O'Sullivan	It is widely recognised that engaging games can have a profound impact on learning. Integrating a conversational Artificial Intelligence (AI) into the mix makes the experience of learning even more engaging and enriching. In this paper we describe a conversational agent which is built with the purpose of acting as a personal tutor. The tutor can prompt, question, stimulate and guide a learner and then adapt exercises and challenges to specific needs. We illustrate how automatic generation of quizzes can be used to build learning exercises and activities.	Learning from quizzes using intelligent learning companions	NA:NA:NA	2018
Kai Michael Höver:Max Mühlhäuser	In the world of Linked Data, HTTP URIs are names. A URI is dereferenced to obtain a copy or description of the referred resource. If only a fragment of a resource should be referred, pointing to the whole resource is not sufficient. Therefore, it is necessary to be able to refer to fragments of resources, and to name them with URIs to interlink them in the Web of Data. This is especially helpful in the educational context where learning processes including discussion and social interaction demand for exact references and granular selections of media. This paper presents the specification of Linked Data Selectors, an OWL ontology for describing dereferenceable fragments of Web resources.	Linked data selectors	NA:NA	2018
Ricardo Kawase:Marco Fisichella:Katja Niemann:Vassilis Pitsilis:Aristides Vidalis:Philipp Holtkamp:Bernardo Nunes	Already existing open educational resources in the field of Business and Management have a high potential for enterprises to address the increasing training needs of their employees. However, it is difficult to act on OERs as some data is hidden. In the meanwhile, numerous repositories provide Linked Open Data on this field. Though, users have to search a number of repositories with heterogeneous interfaces in order to retrieve the desired content. In this paper, we present the strategies to gather heterogeneous learning objects from the Web of Data, and we provide an overview of the benefits of the OpenScout platform. Despite the fact that not all data repositories strictly follow Linked Data principles, OpenScout addressed individual variations in order to harvest, align, and provide a single end-point. In the end, OpenScout provides a full-fledged environment that leverages on the Linked Open Data available on the Web and additionally exposes it in an homogeneous format.	OpenScout: harvesting business and management learning objects from the web of data	NA:NA:NA:NA:NA:NA:NA	2018
Johan Oomen:Raphaël Troncy:Vasileios Mezaris	LiME'13 Welcome and organization	Session details: LiME'13 workshop	NA:NA:NA	2018
Lyndon Nixon	If the future Web will be able to fully leverage the scale and quality of online media, a Web scale layer of structured, interlinked media annotations is needed, which we will call Linked Media, inspired by the Linked Data movement for making structured, interlinked descriptions of resources better available online. Mobile and tablet devices, as well as connected TVs, introduce novel application domains that will benefit from broad understanding and acceptance of Linked Media standards. In the keynote, I will provide an overview of current practices and specification efforts in the domain of video and Web content integration, drawing from the LinkedTV1 and MediaMixer2 projects. From this, I will present a vision for a Linked Media layer on the future Web will can empower new media-centric applications in a world of ubiquitous online multimedia.	The importance of linked media to the future web: lime 2013 keynote talk -- a proposal for the linked media research agenda	NA	2018
Robin Aly:Roeland J.F. Ordelman:Maria Eskevich:Gareth J.F. Jones:Shu Chen	Although linking video to additional information sources seems to be a sensible approach to satisfy information needs of user, the perspective of users is not yet analyzed on a fundamental level in real-life scenarios. However, a better understanding of the motivation of users to follow links in video, which anchors users prefer to link from within a video, and what type of link targets users are typically interested in, is important to be able to model automatic linking of audiovisual content appropriately. In this paper we report on our methodology towards eliciting user requirements with respect to video linking in the course of a broader study on user requirements in searching and a series of benchmark evaluations on searching and linking.	Linking inside a video collection: what and how to measure?	NA:NA:NA:NA:NA	2018
Michiel Hildebrand:Lynda Hardman	Video content analysis and named entity extraction are increasingly used to automatically generate content annotations for TV programs. A potential use of these annotations is to provide an entry point to background information that users can consume on a second screen. Automatic enrichments are, however, meaningless when it is unclear to the user what they can do with them and why they would. We propose to contextualize the annotations by an explicit representation of discourse in the form of scene templates. Through content rules these templates are populated with the relevant annotations. We illustrate this idea with an example video and annotations generated in the LinkedTV1 project.	Using explicit discourse rules to guide video enrichment	NA:NA	2018
Julien Leroy:François Rocca:Matei Mancas:Bernard Gosselin	In this paper, we present our "work-in-progress" approach to implicitly track user interaction and infer the interest a user can have for TV media. The aim is to identify moments of attentive focus, noninvasively and continuously, to dynamicaly improve the user profile by detecting which annotated media have drawn the user attention. Our method is based on the detection and estimation of face pose in 3D using a consumer depth camera. This allows us to determine when a user is or not looking at his television. This study is realized in the scenario of second screen interaction (tablet, smartphone), a behavior that has become common for spectators. We present our progress on the system and its integration in the LinkedTV project.	Second screen interaction: an approach to infer tv watcher's interest using 3d head pose estimation	NA:NA:NA:NA	2018
Yunjia Li:Giuseppe Rizzo:José Luis Redondo García:Raphaël Troncy:Mike Wald:Gary Wills	With the steady increase of videos published on media sharing platforms such as Dailymotion and YouTube, more and more efforts are spent to automatically annotate and organize these videos. In this paper, we propose a framework for classifying video items using both textual features such as named entities extracted from subtitles, and temporal features such as the duration of the media fragments where particular entities are spotted. We implement four automatic machine learning algorithms for multiclass classification problems, namely Logistic Regression (LG), K-Nearest Neighbour (KNN), Naive Bayes (NB) and Support Vector Machine (SVM). We study the temporal distribution patterns of named entities extracted from 805 Dailymotion videos. The results show that the best performance using the entity distribution is obtained with KNN (overall accuracy of 46.58%) while the best performance using the temporal distribution of named entities for each type is obtained with SVM (overall accuracy of 43.60%). We conclude that this approach is promising for automatically classifying online videos.	Enriching media fragments with named entities for video classification	NA:NA:NA:NA:NA:NA	2018
Lionel Médini:Florian Bâcle:Hoang Duy Tan Nguyen	This paper describes a mobile Web application that allows browsing conference publications, their authors, authors' organizations, and even authors' other publications or publications related to the same keywords. It queries a main SPARQL endpoint that serves the conference metadata set, as well as other endpoints to enrich and explore data. It provides extra functions, such as flashing a publication QR code from the Web browser, accessing external resources about the publications, and it can be linked to external Web services. This application exploits the Linked Data paradigm and performs client-side reasoning. It follows recent W3C technical advances and as a mashup, requires few server resources. It can easily be deployed for any conference with available metadata on the Web.	DataConf: enriching conference publications with a mobile mashup application	NA:NA:NA	2018
Philipp Oehme:Michael Krug:Fabian Wiedemann:Martin Gaedke	The Internet has become an important source for media content. Content types are not limited to text and pictures but also include video and audio. Currently audiovisual media is presented as it is. However, these media do not integrate the huge amount of related information, which is available on the Web. In this paper we present the Chrooma+ approach to improve the user experience of media consumption by enriching media content with additional information from various sources in the Web. Our approach focuses on the aggregation and combination of this related information with audiovisual media. This approach involves using new HTML5 technologies and with WebVTT a new annotation format to display relevant information at definite times. Some of the advantages of this approach are the usage of a rich annotation format and extensibility to include heterogeneous information sources.	The chrooma+ approach to enrich video content using HTML5	NA:NA:NA:NA	2018
Johan Oomen:Vassilis Tzouvaras:Kati Hyyppaä	The EUscreen initiative represents the European television archives and acts as a domain aggregator for Europeana, Europe's digital library, which provides access to over 20 million digitized cultural objects. The main motivation for the initiative is to provide unified access to a representative collection of television programs, secondary sources and articles, and in this way to allow students, scholars and the general public to study the history of television in its wider context. This paper explores the EUscreen activities related to novel ways to present curated content and publishing EUscreen metadata as Linked Open Data.	Linking and visualizing television heritage: the EUscreen virtual exhibitions and the linked open data pilot	NA:NA:NA	2018
Qi He:Yuanyuan Tian:Hanghang Tong:John McPherson:David Konopnicki:Jimeng Sun:Ana Paula Appel	LSNA'13 Welcome and organization	Session details: LSNA'13 workshop	NA:NA:NA:NA:NA:NA:NA	2018
Ricardo Baeza-Yates:Diego Saez-Trumper	One of the main differences between traditional Web analysis and online Social Networks (OSNs) studies, is that in the first case the information is organized around content, while in the second case it is organized around people. While search engines have done a good job finding relevant content across billions of pages, nowadays we do not have an equivalent tool to find relevant people in OSNs. Even though an impressive amount of research has been done in this direction, there are still a lot of gaps to cover. Although the first intuition could be (and was!) search for popular people, previous research have shown that users' in-degree (e.g. number of friends or followers) is important but not enough to represent the importance and reputation of a person. Another approach is to study the content of the messages exchanged between users, trying to identify topical experts. However the computational cost of such approach - including language diversity - is a big limitation. In our work we take a content-agnostic approach, focusing in frequency, type, and time properties of user actions rather than content, mixing their static characteristics (social graph) and their activities (dynamic graphs). Our goal is to understand the role of popular users in OSNs, and also find "hidden important users": do popular users create new trends and cascades? Do they add value to the network? And, if they don't, who does it? Our research provides preliminary answers for these questions.	Online social networks: beyond popularity	NA:NA	2018
Anirban Dasgupta	In social systems, information often exists in a dispersed manner, as individual opinions, local insights and preferences. In order to make a global decision however, we need to be able to aggregate such local pieces of information into a global description of the system. Such information aggregation problems are key in setting up crowdsourcing or human computation systems. How do we formally build and analyze such information aggregation systems? In this talk we will discuss three different vignettes based on the particular information aggregation problem and the "social system" that we are extracting the information from. In our first result, we will analyze a crowdsourcing system consisting of a set of users and binary choice questions. Each user has a specific reliability that determines the user's error rate in answering the questions. We show how to give an unsupervised algorithm for aggregating the user answers in order to simultaneously derive the user expertise as well as the truth values of the questions. Our second result will deal with the case when there is an interacting user community on a question answer forum. User preferences of quality are now expressed in terms of ("best answer" and "thumbs up/down") votes cast on each other's content. We will analyze a set of possible factors that indicate bias in user voting behavior - these factors encompass different gaming behavior, as well as other eccentricities. We address the problem of aggregating user preferences (votes) using a supervised machine learning framework to calibrate such votes. We will see that this supervised learning method of content-agnostic vote calibration can significantly improve the performance of answer ranking and expert ranking. The last part of the talk will describe how it is possible to exploit local insights that users have about their friends in order to improve the efficiency of surveying in a (networked) population. We will describe the notion of "social sampling", where participants in a poll respond with a summary of their friends' putative responses to the poll. The analysis of social sampling leads to novel trade-off questions: the savings in the number of samples(roughly the average size of neighborhood of participants) vs. the systematic bias in the poll due to the network structure. We show bounds on the variances of few such estimators - experiments on real world networks show this to be a useful paradigm in obtaining accurate information with small number of samples.	Aggregating information from the crowd and the network	NA	2018
Rogério de Paula	In this talk, I examine the manifest, emic meanings of social networking in the context of social network analysis and it uses this to discuss how the confluence of social science and computational sociology can contribute to a richer understanding of how emerging social technologies shape and are shaped by people's everyday practices.	The social meanings of social networks: integrating SNA and ethnography of social networking	NA	2018
Michalis Faloutsos	In this talk, we highlight two topics on security from our lab. First, we address the problem of Internet traffic classification (e.g. web, filesharing, or botnet?). We present a fundamentally different approach to classifying traffic that studies the network wide behavior by modeling the interactions of users as a graph. By contrast, most previous approaches use statistics such as packet sizes and inter-packet delays. We show how our approach gives rise to novel and powerful ways to: (a) visualize the traffic, (b) model the behavior of applications, and (c) detect abnormalities and attacks. Extending this approach, we develop ENTELECHEIA, a botnet-detection method. Tests with real data suggests that our graph-based approach is very promising. Second, we present, MyPageKeeper, a security Facebook app, with 13K downloads, which we deployed to: (a) quantify the presence of malware on Facebook, and (b) protect end-users. We designed MyPageKeeper in a way that strikes the balance between accuracy and scalability. Our initial results are scary and interesting: (a) malware is widespread, with 49% of our users are exposed to at least one malicious post from a friend, and (b) roughly 74% of all malicious posts contain links that point back to Facebook, and thus would evade any of the current web-based filtering approaches.	Detecting malware with graph-based methods: traffic classification, botnets, and facebook scams	NA	2018
Ido Guy	Today's enterprises hold ever-growing amounts of public data, stemming from different organizational systems, such as development environments, CRM systems, business intelligence systems, and enterprise social media. This data unlocks rich and diverse information about entities, people, terms, and the relationships among them. A lot of insight can be gained through analyzing this knowledge graph, both by individual employees and by the organization as a whole. In this talk, I will review recent work done by the Social Technologies & Analytics group at IBM Research-Haifa to mine these relationships, represent them in a generalized model, and use the model for different aims within the enterprise, including social search [5], expertise location [1], social recommendation [2, 3], and network analysis [4].	Mining and analyzing the enterprise knowledge graph	NA	2018
Johan Ugander	With over a billion nodes and hundreds of billions of edges, scalability is at the forefront of concerns when dealing with the Facebook social graph. This talk will focus on two recent advances in graph computations at Facebook. The first focus concerns the development of a novel graph sharding algorithm - Balanced Label Propagation - for load-balancing distributed graph computations. Using Balanced Label Propagation, we were able to reduce by 50% the query time of Facebook's 'People You May Know' service, the realtime distributed system responsible for the feature extraction and ranking of the friends-of-friends of all active Facebook users. The second focus concerns the 2011 computation of the average distance distribution between all active Facebook users. This computation, which produced an average distance of 4.74, was made possible by two recent computational advances: Hyper-ANF, a modern probabilistic algorithm for computing distance distributions, and Layered Label Propagation, a modern compression scheme suited for social graphs. The details of how this computation was coordinated will be described. The talk describes joint work with Lars Backstrom, Paolo Boldi, Marco Rosa, and Sebastiano Vigna.	Scaling graph computations at facebook	NA	2018
Nguyen Thien Bao:Toyotaro Suzumura	Many practical computing problems concern large graph. Standard problems include web graph analysis and social networks analysis like Facebook, Twitter. The scale of these graph poses challenge to their efficient processing. To efficiently process large-scale graph, we create X-Pregel, a graph processing system based on Google's Computing Pregel model [1], by using the state-of-the-art PGAS programming language X10. We do not purely implement Google Pregel by using X10 language, but we also introduce two new features that do not exists in the original model to optimize the performance: (1) an optimization to reduce the number of messages which is exchanged among workers, (2) a dynamic re-partitioning scheme that effectively reassign vertices to different workers during the computation. Our performance evaluation demonstrates that our optimization method of sending messages achieves up to 200% speed up on Pagerank by reducing the network I/O to 10 times in comparison with the default method of sending messages when processing SCALE20 Kronecker graph [2](vertices = 1,048,576, edges = 33,554,432). It also demonstrates that our system processes large graph faster than prior implementation of Pregel such as GPS [3](stands for graph processing system) and Giraph [4].	Towards highly scalable pregel-based graph processing platform with x10	NA:NA	2018
Miyuru Dayarathna:Toyotaro Suzumura	In recent years, many programming models, software libraries, and middleware have appeared for processing large graphs of various forms. However, there exists a significant usability gap between the graph analysis scientists, and High Performance Computing (HPC) application programmers due to the complexity of HPC graph analysis software. In this paper we provide a basic view of Exedra, a domain-specific language (DSL) for large graph analysis in which we aim to eliminate the aforementioned complexities. Exedra consists of high level language constructs for specifying different graph analysis tasks on distributed environments. We implemented Exedra DSL on a scalable graph analysis platform called Dipper. Dipper uses Igraph/R interface for creating graph analysis workflows which in turn gets translated to Exedra statements. Exedra statements are interpreted by Dipper interpreter, and gets mapped to user specified libraries/middleware. Exedra DSL allows for synthesize of graph algorithms that are more efficient compared to bare use of graph libraries while maintaining a standard interface that could use even future graph analysis software. We evaluated Exedra's feasibility for expressing graph analysis tasks by running Dipper on a cluster of four nodes. We observed that Dipper has the ability of reducing the time taken for graph analysis when the workflow was distributed on all four nodes despite the communication, and data format conversion overhead of the Dipper framework.	A first view of exedra: a domain-specific language for large graph analytics workflows	NA:NA	2018
Santiago A. Nunes:Luciana A.S. Romani:Ana M.H. Avila:Priscila P. Coltri:Caetano Traina, Jr.:Robson L.F. Cordeiro:Elaine P.M. de Sousa:Agma J.M. Traina	Research on global warming and climate changes has attracted a huge attention of the scientific community and of the media in general, mainly due to the social and economic impacts they pose over the entire planet. Climate change simulation models have been developed and improved to provide reliable data, which are employed to forecast effects of increasing emissions of greenhouse gases on a future global climate. The data generated by each model simulation amount to Terabytes of data, and demand fast and scalable methods to process them. In this context, we propose a new process of analysis aimed at discriminating between the temporal behavior of the data generated by climate models and the real climate observations gathered from ground-based meteorological station networks. Our approach combines fractal data analysis and the monitoring of real and model-generated data streams to detect deviations on the intrinsic correlation among the time series defined by different climate variables. Our measurements were made using series from a regional climate model and the corresponding real data from a network of sensors from meteorological stations existing in the analyzed region. The results show that our approach can correctly discriminate the data either as real or as simulated, even when statistical tests fail. Those results suggest that there is still room for improvement of the state-of-the-art climate change models, and that the fractal-based concepts may contribute for their improvement, besides being a fast, parallelizable, and scalable approach.	Analysis of large scale climate data: how well climate change models and data from real sensor networks agree?	NA:NA:NA:NA:NA:NA:NA:NA	2018
Lovro Šubelj:Marko Bajec	Complex networks of real-world systems are believed to be controlled by common phenomena, producing structures far from regular or random. These include scale-free degree distributions, small-world structure and assortative mixing by degree, which are also the properties captured by different random graph models proposed in the literature. However, many (non-social) real-world networks are in fact disassortative by degree. Thus, we here propose a simple evolving model that generates networks with most common properties of real-world networks including degree disassortativity. Furthermore, the model has a natural interpretation for citation networks with different practical applications.	Model of complex networks based on citation dynamics	NA:NA	2018
Masaru Watanabe:Toyotaro Suzumura	Recently, social network services such as Twitter, Facebook, MySpace, LinkedIn have been remarkably growing. There are various studies about social networks analysis. Haewoon Kwak performed the analysis of the Twitter network on 2009 and shows the degree of separation. However, the number of users on 2009 is about 41.7 million, the graph scale is not very large compared with the current graph. In this paper, we conduct a Twitter network analysis in terms growth by region, scale-free, reciprocity, degree of separation and diameter using Twitter user data with 469.9 million users and 28.7 billion relationships. We report that the value of degree of separation is 4.59 in current Twitter network through our experiments.	How social network is evolving?: a preliminary study on billion-scale twitter network	NA:NA	2018
Erik Cambria:Yunqing Xia:Newton Howard	MABSDA'13 Welcome and organization	Session details: MABSDA'13 workshop	NA:NA:NA	2018
Bebo White	Insights from Web Science and Big Data Analysis have led many researchers to the conclusion that the Web not only represents an almost unlimited data store but also a remarkable multi-disciplinary laboratory environment. A new challenge is how to best leverage the potential of this experimental space. What are the procedures for defining, implementing and evaluating "Web-scale" experiments? What are acceptable measures of robustness and repeatability? What are the opportunities for experimental collaboration? What disciplines are likely to benefit from this new research model? The Web Laboratory model provides an exciting new and fertile model for future research.	The web as a laboratory	NA	2018
Shohei Ohsawa:Yutaka Matsuo	Recent growth of social media has produced a new market for branding of people and businesses. Facebook provides Facebook Pages (Pages in short) for public figures and businesses (we call entities) to communicate with their fans through a Like button. Because Like counts sometimes reflect the popularity of entities, techniques to increase the Like count can be a matter of interest, and might be known as social media marketing. From an academic perspective, Like counts of Pages depend not only on the popularity of the entity, but also on the popularity of semantically related entities. For example, Lady Gaga's Page has many Likes; her song "Poker Face" does too. We can infer that her next song will acquire many Likes immediately. Important questions are these: How does the Like count of Lady Gaga affect the Like count of her song? Alternatively, how does the Like count of her song constitute some fraction of the Like count of Lady Gaga herself? As described in this paper, we strive to reveal the mutual influences of Like counts among semantically related entities. To measure the influence of related entities, we propose a problem called the Like prediction problem (LPP). It models Like counts of a given entity using information of related entities. The semantic relations among entities, expressed as RDF predicates, are obtained by linking each Page with the most similar DBpedia entity. Using the model learned by support vector regression (SVR) on LPP, we can estimate the Like count of a new entity e.g., Lady Gaga's new song. More importantly, we can analyze which RDF predicates are important to infer Like counts, providing a mutual influence network among entities. Our study comprises three parts: (1) crawling the Pages and their Like counts, (2) linking Pages to DBpedia, and (3) constructing features to solve the LPP. Our study, based on 20 million Pages with 30 billion Likes, is the largest-scale study of Facebook Likes ever reported. This research constitutes a new attempt to integrate unstructured emotional data such as Likes, with Linked data, and to provide new insights for branding with social media.	Like prediction: modeling like counts by bridging facebook pages with linked data	NA:NA	2018
Yoonsung Hong:Haewoon Kwak:Youngmin Baek:Sue Moon	With the growing amount of textual data produced by online social media today, the demands for sentiment analysis are also rapidly increasing; and, this is true for worldwide. However, non-English languages often lack sentiment lexicons, a core resource in performing sentiment analysis. Our solution, Tower of Babel (ToB), is a language-independent sentiment-lexicon-generating crowdsourcing game. We conducted an experiment with 135 participants to explore the difference between our solution and a conventional manual annotation method. We evaluated ToB in terms of effectiveness, efficiency, and satisfactions. Based on the result of the evaluation, we conclude that sentiment classification via ToB is accurate, productive and enjoyable.	Tower of babel: a crowdsourcing game building sentiment lexicons for resource-scarce languages	NA:NA:NA:NA	2018
Stefan Gindl:Albert Weichselbraun:Arno Scharl	Opinion holder and opinion target extraction are among the most popular and challenging problems tackled by opinion mining researchers, recognizing the significant business value of such components and their importance for applications such as media monitoring and Web intelligence. This paper describes an approach that combines opinion target extraction with aspect extraction using syntactic patterns. It expands previous work limited by sentence boundaries and includes a heuristic for anaphora resolution to identify targets across sentences. Furthermore, it demonstrates the application of concepts known from research on open information extraction to the identification of relevant opinion aspects. Qualitative analyses performed on a corpus of 100,000 Amazon product reviews show that the approach is promising. The extracted opinion targets and aspects are useful for enriching common knowledge resources and opinion mining ontologies, and support practitioners and researchers to identify opinions in document collections.	Rule-based opinion target and aspect extraction to acquire affective knowledge	NA:NA:NA	2018
Dheeraj Rajagopal:Erik Cambria:Daniel Olsher:Kenneth Kwok	Commonsense knowledge representation and reasoning support a wide variety of potential applications in fields such as document auto-categorization, Web search enhancement, topic gisting, social process modeling, and concept-level opinion and sentiment analysis. Solutions to these problems, however, demand robust knowledge bases capable of supporting flexible, nuanced reasoning. Populating such knowledge bases is highly time-consuming, making it necessary to develop techniques for deconstructing natural language texts into commonsense concepts. In this work, we propose an approach for effective multi-word commonsense expression extraction from unrestricted English text, in addition to a semantic similarity detection technique allowing additional matches to be found for specific concepts not already present in knowledge bases.	A graph-based approach to commonsense concept extraction and semantic similarity detection	NA:NA:NA:NA	2018
Arturo Montejo-Ráez:Manuel Carlos Díaz-Galiano:José Manuel Perea-Ortega:Luis Alfonso Ureña-López	This work presents a novel method for the generation of a knowledge base oriented to Sentiment Analysis from the continuous stream of published micro-blogs in social media services like Twitter. The method is simple in its approach and has shown to be effective compared to other knowledge based methods for Polarity Classification. Due to independence from language, the method has been tested on different Spanish corpora, with a minimal effort in the lexical resources involved. Although for two of the three studied corpora the obtained results did not improve those officially obtained on the same corpora, it should be noted that this is an unsupervised approach and the accuracy levels achieved were close to those levels obtained with well-known supervised algorithms.	Spanish knowledge base generation for polarity classification from masses	NA:NA:NA:NA	2018
Farag Saad:Brigitte Mathiak	The significant increase in content of online social media such as product reviews, blogs, forums etc., have led to an increasing attention to sentiment analysis tools and approaches that make use of mining this substantially growing content. The aim of this paper is to develop a robust classification approach of customer reviews based on a self-annotated domain-specific corpus by applying a statistical approach i.e., mutual information. First, subjective words in each test sentence are identified. Second, ambiguous adjectives such as high, low, large, many etc., are disambiguated based on their accompanying noun using a conditional mutual information approach. Third, a mutual information approach is applied to find the sentiment orientation (polarity) of the identified subjective words based on analyzing their statistical relationship with the manually annotated sentiment labels within a sizeable sentiment training data. Fourth, since negation plays a significant role in flipping the sentiment polarity of an identified sentiment word, we estimate the role of negation in affecting the classification accuracy. Finally, the identified polarity for each test sentence is evaluated against experts' annotation.	Revised mutual information approach for german text sentiment classification	NA:NA	2018
Matthew Rowe:Milan Stankovic:Aba-Sah Dadzie	MSM'13 Welcome and organization	Session details: MSM'13 workshop	NA:NA:NA	2018
Aba-Sah Dadzie	NA	Session details: MSM'13 keynote talk	NA	2018
Daniele Quercia	For the last few years, we have been studying existing social media sites and created new ones in the context of London. By combining what Twitter users in a variety of London neighborhoods talk about with census data, we showed that neighborhood deprivation was associated (positively and negatively) with use of emotion words (sentiment) [2] and with specific topics [5]. Users in more deprived neighborhoods tweeted about wedding parties, matters expressed in Spanish/Portuguese, and celebrity gossips. By contrast, those in less deprived neighborhoods tweeted about vacations, professional use of social media, environmental issues, sports, and health issues. Also, upon data about 76 million London underground and overground rail journeys, we found that people from deprived areas visited both other deprived areas and prosperous areas, while residents of better-off communities tended to only visit other privileged neighborhoods - suggesting a geographic segregation effect [1, 6]. More recently, we created and launched two crowdsourcing websites. First, we launched urbanopticon.org, which extracts Londoners' mental images of the city. By testing which places are remarkable and unmistakable and which places represent faceless sprawl, we were able to draw the recognizability map of London. We found that areas with low recognizability did not fare any worse on the economic indicators of income, education, and employment, but they did significantly suffer from social problems of housing deprivation, poor living conditions, and crime [4]. Second, we launched urbangems.org. This crowdsources visual perceptions of quiet, beauty and happiness across the city using Google Street View pictures. The aim is to identify the visual cues that are generally associated with concepts difficult to define such beauty, happiness, quietness, or even deprivation. By using state-of-the-art image processing techniques, we determined the visual cues that make a place appear beautiful, quiet, and happy [3]: the amount of greenery was the most positively associated visual cue with each of three qualities; by contrast, broad streets, fortress-like buildings, and council houses tended to be negatively associated. These two sites offer the ability to conduct specific urban sociological experiments at scale. More generally, this line of work is at the crossroad of two emerging themes in computing research - a crossroad where "web science" meets the "smart city" agenda.	Urban: crowdsourcing for the good of London	NA	2018
Aba-Sah Dadzie	NA	Session details: MSM'13 machine learning & statistical analysis	NA	2018
Fréderic Godin:Viktor Slavkovikj:Wesley De Neve:Benjamin Schrauwen:Rik Van de Walle	Since the introduction of microblogging services, there has been a continuous growth of short-text social networking on the Internet. With the generation of large amounts of microposts, there is a need for effective categorization and search of the data. Twitter, one of the largest microblogging sites, allows users to make use of hashtags to categorize their posts. However, the majority of tweets do not contain tags, which hinders the quality of the search results. In this paper, we propose a novel method for unsupervised and content-based hashtag recommendation for tweets. Our approach relies on Latent Dirichlet Allocation (LDA) to model the underlying topic assignment of language classified tweets. The advantage of our approach is the use of a topic distribution to recommend general hashtags.	Using topic models for Twitter hashtag recommendation	NA:NA:NA:NA:NA	2018
Diego Marinho de Oliveira:Alberto H.F. Laender:Adriano Veloso:Altigran S. da Silva	Microblog platforms such as Twitter are being increasingly adopted by Web users, yielding an important source of data for web search and mining applications. Tasks such as Named Entity Recognition are at the core of many of these applications, but the effectiveness of existing tools is seriously compromised when applied to Twitter data, since messages are terse, poorly worded and posted in many different languages. Also, Twitter follows a streaming paradigm, imposing that entities must be recognized in real-time. In view of these challenges and the inappropriateness of existing tools, we propose a novel approach for Named Entity Recognition on Twitter data called FS-NER (Filter-Stream Named Entity Recognition). FS-NER is characterized by the use of filters that process unlabeled Twitter messages, being much more practical than existing supervised CRF-based approaches. Such filters can be combined either in sequence or in parallel in a flexible way. Moreover, because these filters are not language dependent, FS-NER can be applied to different languages without requiring a laborious adaptation. Through a systematic evaluation using three Twitter collections and considering seven types of entity, we show that FS-NER performs 3% better than a CRF-based baseline, besides being orders of magnitude faster and much more practical.	FS-NER: a lightweight filter-stream approach to named entity recognition on twitter data	NA:NA:NA:NA	2018
Aba-Sah Dadzie	NA	Session details: MSM'13 trend & topic detection in microposts	NA	2018
Victoria Uren:Aba-Sah Dadzie	This paper presents an analysis of tweets collected over six days before, during and after the landing of the Mars Science Laboratory, known as Curiosity, in the Gale Crater on the 6th of August 2012. A sociological application of web science is demonstrated by use of parallel coordinate visualization as part of a mixed methods study. The results show strong, predominantly positive, international interest in the event. Scientific details dominated the stream, but, following the successful landing, other themes emerged such as fun, and national pride.	Nerding out on twitter: fun, patriotism and #curiosity	NA:NA	2018
Ruchi Parikh:Kamalakar Karlapalem	Social media sites such as Twitter and Facebook have emerged as popular tools for people to express their opinions on various topics. The large amount of data provided by these media is extremely valuable for mining trending topics and events. In this paper, we build an efficient, scalable system to detect events from tweets (ET). Our approach detects events by exploring their textual and temporal components. ET does not require any target entity or domain knowledge to be specified; it automatically detects events from a set of tweets. The key components of ET are (1) an extraction scheme for event representative keywords, (2) an efficient storage mechanism to store their appearance patterns, and (3) a hierarchical clustering technique based on the common co-occurring features of keywords. The events are determined through the hierarchical clustering process. We evaluate our system on two data-sets; one is provided by VAST challenge 2011, and the other published by US based users in January 2013. Our results show that we are able to detect events of relevance efficiently.	ET: events from tweets	NA:NA	2018
Aba-Sah Dadzie	NA	Session details: MSM'13 filtering & cassification of microposts	NA	2018
Lisa Posch:Claudia Wagner:Philipp Singer:Markus Strohmaier	This paper sets out to explore whether data about the usage of hashtags on Twitter contains information about their semantics. Towards that end, we perform initial statistical hypothesis tests to quantify the association between usage patterns and semantics of hashtags. To assess the utility of pragmatic features - which describe how a hashtag is used over time - for semantic analysis of hashtags, we conduct various hashtag stream classification experiments and compare their utility with the utility of lexical features. Our results indicate that pragmatic features indeed contain valuable information for classifying hashtags into semantic categories. Although pragmatic features do not outperform lexical features in our experiments, we argue that pragmatic features are important and relevant for settings in which textual information might be sparse or absent (e.g., in social video streams).	Meaning as collective use: predicting semantic hashtag categories on twitter	NA:NA:NA:NA	2018
Bernd Hollerit:Mark Kröll:Markus Strohmaier	Since more and more people use the micro-blogging platform Twitter to convey their needs and desires, it has become a particularly interesting medium for the task of identifying commercial activities. Potential buyers and sellers can be contacted directly thereby opening up novel perspectives and economic possibilities. By detecting commercial intent in tweets, this work is considered a first step to bring together buyers and sellers. In this work, we present an automatic method for detecting commercial intent in tweets where we achieve reasonable precision 57% and recall 77% scores. In addition, we provide insights into the nature and characteristics of tweets exhibiting commercial intent thereby contributing to our understanding of how people express commercial activities on Twitter.	Towards linking buyers and sellers: detecting commercial Intent on twitter	NA:NA:NA	2018
Aba-Sah Dadzie	NA	Session details: MSM'13 posters & demonstrations	NA	2018
Ryadh Dahimene:Cédric du Mouza	Microblogging systems have become a major trend over the Web. After only 7 years of existence, Twitter for instance claims more than 500 million users with more than 350 billion delivered update each day. As a consequence the user must today manage possibly extremely large feeds, resulting in poor data readability and loss of valuable information and the system must face a huge network load. In this demonstration, we present and illustrate the features of MicroFilter (MF in the the following), an inverted list-based filtering engine that nicely extends existing centralized microblogging systems by adding a real-time filtering feature. The demonstration proposed illustrates how the user experience is improved, the impact on the traffic for the overall system, and how the characteristics of microblogs drove the design of the indexing structures.	MicroFilter: real time filtering of microblogging content	NA:NA	2018
Aline A. Vanin:Larissa A. Freitas:Renata Vieira:Marco Bochernitsan	NA	Some clues on irony detection in tweets	NA:NA:NA:NA	2018
Hakim Hacid:Shengbo Guo:Athena Vakali	MSND'13 Welcome	Session details: MSND'13 workshop	NA:NA:NA	2018
Anupama Aggarwal:Jussara Almeida:Ponnurangam Kumaraguru	In Foursquare, one of the currently most popular online location based social networking sites (LBSNs), users may not only check-in at specific venues but also post comments (or tips), sharing their opinions and previous experiences at the corresponding physical places. Foursquare tips, which are visible to everyone, provide venue owners with valuable user feedback besides helping other users to make an opinion about the specific venue. However, they have been the target of spamming activity by users who exploit this feature to spread tips with unrelated content. In this paper, we present what, to our knowledge, is the first effort to identify and analyze different patterns of tip spamming activity in Foursquare, with the goal of developing automatic tools to detect users who post spam tips - tip spammers. A manual investigation of a real dataset collected from Foursquare led us to identify four categories of spamming behavior, viz. Advertising/Spam, Self-promotion, Abusive and Malicious. We then applied machine learning techniques, jointly with a selected set of user, social and tip's content features associated with each user, to develop automatic detection tools. Our experimental results indicate that we are able to not only correctly distinguish legitimate users from tip spammers with high accuracy (89.76%) but also correctly identify a large fraction (at least 78.88%) of spammers in each identified category.	Detection of spam tipping behaviour on foursquare	NA:NA:NA	2018
Bruno Leite Alves:Fabrício Benevenuto:Alberto H.F. Laender	There have been considerable efforts in the literature towards understanding and modeling dynamic aspects of scientific communities. Despite the great interest, little is known about the role that different members play in the formation of the underlying network structure of such communities. In this paper, we provide a wide investigation of the roles that members of the core of scientific communities play in the collaboration network structure formation and evolution. To do that, we define a community core based on an individual metric, core score, which is an h-index derived metric that captures both, the prolificness and the involvement of researchers in a community. Our results provide a number of key observations related to community formation and evolving patterns. Particularly, we show that members of the community core work as bridges that connect smaller clustered research groups. Furthermore, these members are responsible for an increase in the average degree of the whole community underlying network and a decrease on the overall network assortativeness. More important, we note that variations on the members of the community core tend to be strongly correlated with variations on these metrics. We argue that our observations are important for shedding a light on the role of key members on community formation and structure.	The role of research leaders on the evolution of scientific communities	NA:NA:NA	2018
Maximilian Jenders:Gjergji Kasneci:Felix Naumann	Twitter and other microblogging services have become indispensable sources of information in today's web. Understanding the main factors that make certain pieces of information spread quickly in these platforms can be decisive for the analysis of opinion formation and many other opinion mining tasks. This paper addresses important questions concerning the spread of information on Twitter. What makes Twitter users retweet a tweet? Is it possible to predict whether a tweet will become "viral", i.e., will be frequently retweeted? To answer these questions we provide an extensive analysis of a wide range of tweet and user features regarding their influence on the spread of tweets. The most impactful features are chosen to build a learning model that predicts viral tweets with high accuracy. All experiments are performed on a real-world dataset, extracted through a public Twitter API based on user IDs from the TREC 2011 microblog corpus.	Analyzing and predicting viral tweets	NA:NA:NA	2018
Jeongin Ju:Hosung Park:Sue Moon	As scholarly data increases rapidly, scholarly digital libraries, supplying publication data through convenient online interfaces, become popular and important tools for researchers. Researchers use SDLs for various purposes, including searching the publications of an author, assessing one's impact by the citations, and identifying one's research topics. However, common names among authors cause difficulties in correctly identifying one's works among a large number of scholarly publications. Abbreviated first and middle names make it even harder to identify and distinguish authors with the same representation (i.e. spelling) of names. Several disambiguation methods have solved the problem under their own assumptions. The assumptions are usually that inputs such as the number of same-named authors, training sets, or rich and clear information about papers are given. Considering the size of scholarship records today and their inconsistent formats, we expect their assumptions be very hard to be met. We use common assumption that coauthors are likely to write more than one paper together and propose an unsupervised approach to group papers from the same author only using the most common information, author lists. We represent each paper as a point in an author name space, take dimension reduction to find author names shown frequently together in papers, and cluster papers with vector similarity measure well fitted for name disambiguation task. The main advantage of our approach is to use only coauthor information as input. We evaluate our method using publication records collected from DBLP, and show that our approach results in better disambiguation compared to other five clustering methods in terms of cluster purity and fragmentation.	Resolving homonymy with correlation clustering in scholarly digital libraries	NA:NA:NA	2018
Srikar Velichety:Sudha Ram	We report on an exploratory analysis of pairwise relationships between three different forms of information consumption on Twitter viz., following, listing and subscribing. We develop a systematic framework to examine the relationships between these three forms. Using our framework, we conducted an empirical analysis of a dataset from Twitter. Our results show that people not only consume information by explicitly following others, but also by listing and subscribing to lists and that the people they list or subscribe to are not the same as the ones they follow. Our work has implications for understanding information propagation and diffusion via Twitter and for generating recommendations for adding users to lists, subscribing and merging or splitting them.	Examining lists on Twitter to uncover relationships between following, membership and subscription	NA:NA	2018
Patty Kostkova:Daniela Paolotti:John Brownstein	PHDA'13 Welcome and organization	Session details: PHDA'13 workshop	NA:NA:NA	2018
Allisson D. Oliveira:Giordano Cabral:D. López:Caetano Firmo:F. Zarzuela Serrat:J. Albuquerque	This paper presents a methodology for automatic diagnosis of malaria using computer vision techniques combined with artificial intelligence. We had obtained an accuracy rate of 74% in the detection system.	A proposal for automatic diagnosis of malaria: extended abstract	NA:NA:NA:NA:NA:NA	2018
Stephanie Brien:Nona Naderi:Arash Shaban-Nejad:Luke Mondor:Doerthe Kroemker:David L. Buckeridge	This paper reports work in progress to semantically annotate blog posts about vaccines to use in the Vaccine Attitude Surveillance using Semantic Analysis (VASSA) framework. The VASSA framework combines semantic web and natural language processing (NLP) tools and techniques to provide a coherent semantic layer across online social media for assessment and analysis of vaccination attitudes and beliefs. We describe how the blog posts were sampled and selected, our schema to semantically annotate concepts defined in our ontology, details of the annotation process, and inter-annotator agreement on a sample of blog posts.	Vaccine attitude surveillance using semantic analysis: constructing a semantically annotated corpus	NA:NA:NA:NA:NA:NA	2018
Patty Kostkova	The exponentially increasing stream of real time big data produced by Web 2.0 Internet and mobile networks created radically new interdisciplinary challenges for public health and computer science. Traditional public health disease surveillance systems have to utilize the potential created by new situation-aware realtime signals from social media, mobile/sensor networks and citizens? participatory surveillance systems providing invaluable free realtime event-based signals for epidemic intelligence. However, rather than improving existing isolated systems, an integrated solution bringing together existing epidemic intelligence systems scanning news media (e.g., GPHIN, MedISys) with real-time social media intelligence (e.g., Twitter, participatory systems) is required to substantially improve and automate early warning, outbreak detection and preparedness operations. However, automatic monitoring and novel verification methods for these multichannel event-based real time signals has to be integrated with traditional case-based surveillance systems from microbiological laboratories and clinical reporting. Finally, the system needs effectively support coordination of epidemiological teams, risk communication with citizens and implementation of prevention measures. However, from computational perspective, signal detection, analysis and verification of very high noise realtime big data provide a number of interdisciplinary challenges for computer science. Novel approaches integrating current systems into a digital public health dashboard can enhance signal verification methods and automate the processes assisting public health experts in providing better informed and more timely response. In this paper, we describe the roadmap to such a system, components of an integrated public health surveillance services and computing challenges to be resolved to create an integrated real world solution.	A roadmap to integrated digital public health surveillance: the vision and the challenges	NA	2018
Michael Johansson:Oktawia Wojcik:Rumi Chunara:Mark Smolinski:John Brownstein	Participatory disease surveillance systems are dynamic, sensitive, and accurate. They also offer an opportunity to directly connect the public to public health. Implementing them in Latin America requires targeting multiple acute febrile illnesses, designing a system that is appropriate and scalable, and developing local strategies for encouraging participation.	Participatory disease surveillance in Latin America	NA:NA:NA:NA:NA	2018
Natalia Barbara Mantilla-Beniers:Rocio Rodriguez-Ramirez:Christopher Rhodes Stephens	Monitoring of influenza like illnesses (ILI) using the Internet has become more common since its beginnings nearly a decade ago. The initial project of Der Grote Griep Meting was launched in 2003 in the Netherlands and Belgium. It was designed as a means of engaging people in matters of scientific and public health importance, and indeed attracted participation from over 30,000 people in its first year. Its success thus gathered a wealth of potentially valuable epidemiological data complementary to those obtained through the established disease surveillance networks, and linked to rich background information on each participant. Since then, there has been an accelerated increase in the number of countries hosting similar websites, and many of these have generated rather promising results In this talk, an analysis of the data from the Mexican monitoring website, "Reporta" is presented, and the risk factors that are linked to reporting of ILI symptoms among its participants are determined and analyzed. The data base gathered from the launching of Reporta in May 2009 to September 2011 is used for this purpose. The definition of suspect ILI case employed by the Mexican Health Ministry is applied to distinguish a class C of participants; the traits gathered in the background questionnaire are labeled Xi. Risk associated to any given trait Xi is evaluated by considering the difference between the frequency with which C occurs among participants with trait Xi and in the general population. This difference is then normalized to assess its statistical significance Interestingly, while some of the results confirm the suspected importance of certain traits indicative of enhanced susceptibility or a large contact network, others are unexpected and must be interpreted within an adequate framework. Thus, a taxonomy of background traits is proposed to aid interpretation, and tested through a new assessment of the associated risks. This work illustrates a way in which Internet-based monitoring can contribute to our understanding of disease spread.	Crowdsourced risk factors of influenza-like-illness in Mexico	NA:NA:NA	2018
Todd Bodnar:Marcel Salathé	Data mining social media has become a valuable resource for infectious disease surveillance. However, there are considerable risks associated with incorrectly predicting an epidemic. The large amount of social media data combined with the small amount of ground truth data and the general dynamics of infectious diseases present unique challenges when evaluating model performance. In this paper, we look at several methods that have been used to assess influenza prevalence using Twitter. We then validate them with tests that are designed to avoid and illustrate issues with the standard k-fold cross validation method. We also find that small modifications to the way that data are partitioned can have major effects on a model's reported performance.	Validating models for disease detection using twitter	NA:NA	2018
Erik van der Goot:Hristo Tanev:Jens P. Linge	We describe the harvesting and subsequent analysis of tweets that are linked to media reports on public health events in order to identify which Internet resources are being referred to in these tweets. The aim was to automatically detect resources that are traditionally not considered mainstream media, but play a role in the discussion of public health events on the Internet. Interestingly, our initial evaluation of the results showed that most references related to public health events lead to traditional news media sites, even though URLs to non-traditional media receive a higher rank. We will briefly describe the Medical Information System (MedISys) and the methodology used to obtain and analyse tweets.	Combining twitter and media reports on public health events in medisys	NA:NA:NA	2018
Ponnurangam Kumaraguru:Virgilio Almeida	PSOM'13 Welcome and organization	Session details: PSOM'13 workshop	NA:NA	2018
Yuan Cheng:Jaehong Park:Ravi Sandhu	Online social networks (OSNs) facilitate many third-party applications (TPAs) that offer users additional functionality and services. However, they also pose serious user privacy risk as current OSNs provide little control over disclosure of user data to TPAs. Addressing the privacy and security issues related to TPAs (and the underlying social networking platforms) requires solutions beyond a simple all-or-nothing strategy. In this paper, we outline an access control framework that provides users flexible controls over how TPAs can access user data and activities in OSNs while still retaining the functionality of TPAs. The proposed framework specifically allows TPAs to utilize some private data without actually transmitting this data to TPAs. Our approach determines access from TPAs based on user-specified policies in terms of relationships between the user and the application.	Preserving user privacy from third-party applications in online social networks	NA:NA:NA	2018
Aditi Gupta:Hemank Lamba:Ponnurangam Kumaraguru:Anupam Joshi	In today's world, online social media plays a vital role during real world events, especially crisis events. There are both positive and negative effects of social media coverage of events, it can be used by authorities for effective disaster management or by malicious entities to spread rumors and fake news. The aim of this paper, is to highlight the role of Twitter, during Hurricane Sandy (2012) to spread fake images about the disaster. We identified 10,350 unique tweets containing fake images that were circulated on Twitter, during Hurricane Sandy. We performed a characterization analysis, to understand the temporal, social reputation and influence patterns for the spread of fake images. Eighty six percent of tweets spreading the fake images were retweets, hence very few were original tweets. Our results showed that top thirty users out of 10,215 users (0.3%) resulted in 90% of the retweets of fake images; also network links such as follower relationships of Twitter, contributed very less (only 11%) to the spread of these fake photos URLs. Next, we used classification models, to distinguish fake images from real images of Hurricane Sandy. Best results were obtained from Decision Tree classifier, we got 97% accuracy in predicting fake images from real. Also, tweet based features were very effective in distinguishing fake images tweets from real, while the performance of user based features was very poor. Our results, showed that, automated techniques can be used in identifying real images from fake images posted on Twitter.	Faking Sandy: characterizing and identifying fake images on Twitter during Hurricane Sandy	NA:NA:NA:NA	2018
Tzipora Halevi:James Lewis:Nasir Memon	Recent research has begun to focus on the factors that cause people to respond to phishing attacks as well as affect user behavior on social networks. This study examines the correlation between the Big Five personality traits and email phishing response. Another aspect examined is how these factors relate to users' tendency to share information and protect their privacy on Facebook (which is one of the most popular social networking sites). This research shows that when using a prize phishing email, neuroticism is the factor most correlated to responding to this email, in addition to a gender-based difference in the response. This study also found that people who score high on the openness factor tend to both post more information on Facebook as well as have less strict privacy settings, which may cause them to be susceptible to privacy attacks. In addition, this work detected no correlation between the participants estimate of being vulnerable to phishing attacks and actually being phished, which suggests susceptibility to phishing is not due to lack of awareness of the phishing risks and that real-time response to phishing is hard to predict in advance by online users. The goal of this study is to better understand the traits that contribute to online vulnerability, for the purpose of developing customized user interfaces and secure awareness education, designed to increase users' privacy and security in the future.	A pilot study of cyber security and privacy related behavior and personality traits	NA:NA:NA	2018
Lilian Edwards:Andrea M. Matwyshyn	Using Twitter as a case study, this paper sets forth the legal tensions faced by social networks that seek to defend privacy interests of users. Recent EC and UN initiatives have begun to suggest an increased role for corporations as protectors of human rights. But, as yet, binding rather than voluntary obligations of this kind under international human rights law seem either non-existent or highly conflicted, and structural limitations to such a shift may currently exist under both U.S. and UK law. Companies do not face decisions regarding disclosure in a vacuum, rather they face them constrained by existing obligations under (sometimes conflicting) legal demands. Yet, companies such as Twitter are well-positioned to be advocates for consumers' interests in these legal debates. Using several recent corporate disclosure decisions regarding user identity as illustration, this paper places questions of privacy, free speech and disclosure in broader legal context. More scholarship is needed on the mechanics of how online intermediaries, especially social media, manage their position as crucial speech platforms in democratic as well as less democratic regimes.	Twitter (R)evolution: privacy, free speech and disclosure	NA:NA	2018
Tarun Parwani:Ramin Kholoussi:Panagiotis Karras	The proliferation of online social networking services has aroused privacy concerns among the general public. The focus of such concerns has typically revolved around providing explicit privacy guarantees to users and letting users take control of the privacy-threatening aspects of their online behavior, so as to ensure that private personal information and materials are not made available to other parties and not used for unintended purposes without the user's consent. As such protective features are usually opt-in, users have to explicitly opt-in for them in order to avoid compromising their privacy. Besides, third-party applications may acquire a user's personal information, but only after they have been granted consent by the user. If we also consider potential network security attacks that intercept or misdirect a user's online communication, it would appear that the discussion of user vulnerability has accurately delimited the ways in which a user may be exposed to privacy threats. In this paper, we expose and discuss a previously unconsidered avenue by which a user's privacy can be gravely exposed. Using this exploit, we were able to gain complete access to some popular online social network accounts without using any conventional method like phishing, brute force, or trojans. Our attack merely involves a legitimate exploitation of the vulnerability created by the existence of obsolete web-based email addresses. We present the results of an experimental study on the spread that such an attack can reach, and the ethical dilemmas we faced in the process. Last, we outline our suggestions for defense mechanisms that can be employed to enhance online security and thwart the kind of attacks that we expose.	How to hack into Facebook without being a hacker	NA:NA:NA	2018
Blase Ur:Yang Wang	Social media has become truly global in recent years. We argue that support for users' privacy, however, has not been extended equally to all users from around the world. In this paper, we survey existing literature on cross-cultural privacy issues, giving particular weight to work specific to online social networking sites. We then propose a framework for evaluating the extent to which social networking sites' privacy options are offered and communicated in a manner that supports diverse users from around the world. One aspect of our framework focuses on cultural issues, such as norms regarding the use of pseudonyms or posting of photographs. A second aspect of our framework discusses legal issues in cross-cultural privacy, including data-protection requirements and questions of jurisdiction. The final part of our framework delves into user expectations regarding the data-sharing practices and the communication of privacy information. The framework can enable service providers to identify potential gaps in support for user privacy. It can also help researchers, regulators, or consumer advocates reason systematically about cultural differences related to privacy in social media.	A cross-cultural framework for protecting user privacy in online social media	NA:NA	2018
Yang Wang:Pedro Giovanni Leon:Kevin Scott:Xiaoxuan Chen:Alessandro Acquisti:Lorrie Faith Cranor	Anecdotal evidence and scholarly research have shown that a significant portion of Internet users experience regrets over their online disclosures. To help individuals avoid regrettable online disclosures, we employed lessons from behavioral decision research and research on soft paternalism to design mechanisms that "nudge" users to consider the content and context of their online disclosures before posting them. We developed three such privacy nudges on Facebook. The first nudge provides visual cues about the audience for a post. The second nudge introduces time delays before a post is published. The third nudge gives users feedback about their posts. We tested the nudges in a three-week exploratory field trial with 21 Facebook users, and conducted 13 follow-up interviews. Our system logs, results from exit surveys, and interviews suggest that privacy nudges could be a promising way to prevent unintended disclosure. We discuss limitations of the current nudge designs and future directions for improvement.	Privacy nudges for social media: an exploratory Facebook study	NA:NA:NA:NA:NA:NA	2018
Arkaitz Zubiaga:Damiano Spina:Maarten de Rijke:Markus Strohmaier	RAMSS'13 Welcome and organization	Session details: RAMSS'13 workshop	NA:NA:NA:NA	2018
Ramesh R. Sarukkai	Real-time analysis and modeling of users for improving engagement, and interaction is a burgeoning area of interest with applications to web sites, social networks and mobile applications. Apart from scalability issues, this domain poses a number of modeling and algorithmic challenges. In this talk, as an illustrative example, we present DAL, a system that leverages real-time user activity/signals for dynamic ad loads, and designed to improve the overall user experience on YouTube. This system uses machine learning to optimize for user activity during a visit and helps decide on real-time advertising policies dynamically for the user. We conclude the talk with challenges and opportunities in this important area of real-time user analysis and social modeling.	Real-time user modeling and prediction: examples from youtube	NA	2018
Gianmarco De Francisci Morales	Social media and user generated content are causing an ever growing data deluge. The rate at which we produce data is growing steadily, thus creating larger and larger streams of continuously evolving data. Online news, micro-blogs, search queries are just a few examples of these continuous streams of user activities. The value of these streams relies in their freshness and relatedness to ongoing events. However, current (de-facto standard) solutions for big data analysis are not designed to deal with evolving streams. In this talk, we offer a sneak preview of SAMOA, an upcoming platform for mining dig data streams. SAMOA is a platform for online mining in a cluster/cloud environment. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as S4 and Storm. SAMOA includes algorithms for the most common machine learning tasks such as classification and clustering. Finally, SAMOA will soon be open sourced in order to foster collaboration and research on big data stream mining.	SAMOA: a platform for mining big data streams	NA	2018
Ernesto Diaz-Aviles:Wolfgang Nejdl:Lucas Drumond:Lars Schmidt-Thieme	The Web of people is highly dynamic and the life experiences between our on-line and "real-world" interactions are increasingly interconnected. For example, users engaged in the Social Web more and more rely upon continuous social streams for real-time access to information and fresh knowledge about current affairs. However, given the deluge of data items, it is a challenge for individuals to find relevant and appropriately ranked information at the right time. Having Twitter as test bed, we tackle this information overload problem by following an online collaborative approach. That is, we go beyond the general perspective of information finding in Twitter, that asks: "What is happening right now?", towards an individual user perspective, and ask: "What is interesting to me right now within the social media stream?". In this paper, we review our recently proposed online collaborative filtering algorithms and outline potential research directions.	Towards real-time collaborative filtering for big fast data	NA:NA:NA:NA	2018
Lumin Zhang:Yan Jia:Bin Zhou:Yi Han	Microblog has become an increasing valuable resource of up-to-date topics about what is happening in the world. In this paper, we propose a novel approach of detecting real-time events in microblog streams based on bursty sentiments detection. Instead of traditional sentiment orientation like positive, negative and neutral, we use sentiment vector as our sentiment model to abstract subjective messages which are then used to detect bursts and clustered into new events. Experimental evaluations show that our approach could perform effectively for online event detection. Although we worked with Chinese in our research, the technique can be used with any other language.	Detecting real-time burst topics in microblog streams: how sentiment can help	NA:NA:NA:NA	2018
Dhekar Abhik:Durga Toshniwal	Social networking sites such as Flickr, YouTube, Facebook, etc. contain a huge amount of user-contributed data for a variety of real-world events. These events can be some natural calamities such as earthquakes, floods, forest fires, etc. or some man-made hazards like riots. This work focuses on getting better knowledge about a natural hazard event using the data available from social networking sites. Rescue and relief activities in emergency situations can be enhanced by identifying sub-events of a particular event. Traditional topic discovery techniques used for event identification in news data cannot be used for social media data because social network data may be unstructured. To address this problem the features or metadata associated with social media data can be exploited. These features can be user-provided annotations (e.g., title, description) and automatically generated information (e.g., content creation time). Considerable improvement in performance is observed by using multiple features of social media data for sub-event detection rather than using individual feature. Proposed here is a two-step process. In the first step, clusters are formed from social network data using relevant features individually. Based on the significance of features weights are assigned to them. And in the second step all the clustering solutions formed in the first step are combined in a principal weighted manner to give the final clustering solution. Each cluster represents a sub-event for a particular natural hazard.	Sub-event detection during natural hazards using features of social media data	NA:NA	2018
Raphaël Troncy:Vuk Milicic:Giuseppe Rizzo:José Luis Redondo García	Social networks play an increasingly important role for sharing media items related to human's activities, feelings, emotions and conversations opening a window to the world in real-time. However, these images and videos are spread over multiple social networks. In this paper, we first describe a so-called media server that collect recent images and videos which can be potentially attached to an event. These media items can then be used for the automatic generation of visual summaries. However, making sense out of the resulting media galleries is an extremely challenging task. We present a framework that leverages on: (i) visual features from media items for near-deduplication and (ii) textual features from status updates to enrich, cluster and generate storyboards. A prototype is publicly available at http://mediafinder.eurecom.fr.	MediaFinder: collect, enrich and visualize media memes shared by the crowd	NA:NA:NA:NA	2018
Thomas Steiner:Seth van Hooland:Ed Summers	We have developed an application called Wikipedia Live Monitor that monitors article edits on different language versions of Wikipedia--as they happen in realtime. Wikipedia articles in different languages are highly interlinked. For example, the English article "en:2013_Russian_meteor_event" on the topic of the February 15 meteoroid that exploded over the region of Chelyabinsk Oblast, Russia, is interlinked with "ru:ПaДehne_meteopnta_ha_Ypajie_B_2013_roДy?, the Russian article on the same topic. As we monitor multiple language versions of Wikipedia in parallel, we can exploit this fact to detect concurrent edit spikes of Wikipedia articles covering the same topics, both in only one, and in different languages. We treat such concurrent edit spikes as signals for potential breaking news events, whose plausibility we then check with full-text cross-language searches on multiple social networks. Unlike the reverse approach of monitoring social networks first, and potentially checking plausibility on Wikipedia second, the approach proposed in this paper has the advantage of being less prone to false-positive alerts, while being equally sensitive to true-positive events, however, at only a fraction of the processing cost. A live demo of our application is available online at the URL http://wikipedia-irc.herokuapp.com/, the source code is available under the terms of the Apache 2.0 license at https://github.com/tomayac/wikipedia-irc.	MJ no more: using concurrent wikipedia edit spikes with social network plausibility checks for breaking news detection	NA:NA:NA	2018
Dmitrijs Milajevs:Gosse Bouma	While social media receive a lot of attention from the scientific community in general, there is little work on high recall retrieval of messages relevant to a discussion. Hash tag based search is widely used for data retrieval from social media. This work shows limitations of this approach, because the majority of the relevant messages do not even contain any hash tag, and unpredictable hash tags are used as the conversation evolves in time. To overcome these limitations, we propose an alternative retrieval method. Given an input stream of messages as an example of the discussion, our method extracts the most relevant words from it and queries the social network for more messages with these words. Our method filters messages that do not belong to the discussion using an LDA topic model. We demonstrate this concept on manually built collections of tweets about major sport and music events.	Real time discussion retrieval from twitter	NA:NA	2018
Abedelaziz Mohaisen:Stefano Ferretti:Fabricio Benevenuto	SIMPLEX'13 Welcome and organization	Session details: SIMPLEX'13 workshop	NA:NA:NA	2018
Nishanth Sastry	NA	Session details: SIMPLEX'13 technical session 1	NA	2018
Fabricio Murai:Bruno Ribeiro:Donald Towsley:Krista Gile	Branching processes model the evolution of populations of agents that randomly generate offspring (children). These processes, more patently Galton-Watson processes, are widely used to model biological, social, cognitive, and technological phenomena, such as the diffusion of ideas, knowledge, chain letters, viruses, and the evolution of humans through their Y-chromosome DNA or mitochondrial RNA. A practical challenge of modeling real phenomena using a Galton-Watson process is the choice of the offspring distribution, which must be measured from the population. In most cases, however, directly measuring the offspring distribution is unrealistic due to lack of resources or the death of agents. So far, researchers have relied on informed guesses to guide their choice of offspring distribution. In this work we propose two methods to estimate the offspring distribution from real sampled data. Using a small sampled fraction of the agents and instrumented with the identity of the ancestors of the sampled agents, we show that accurate offspring distribution estimates can be obtained by sampling as little as 14% of the population.	Characterizing branching processes from sampled data	NA:NA:NA:NA	2018
Stefano Ferretti	This paper presents a self-organizing protocol for dynamic (unstructured P2P) overlay networks, which allows to react to the variability of node arrivals and departures. Through local interactions, the protocol avoids that the departure of nodes causes a partitioning of the overlay. We show that it is sufficient to have knowledge about 1st and 2nd neighbours, plus a simple interaction P2P protocol, to make unstructured networks resilient to node faults. A simulation assessment over different kinds of overlay networks demonstrates the viability of the proposal.	Resilience of dynamic overlays through local interactions	NA	2018
Abraão Guimarães:Alex B. Vieira:Ana Paula Couto Silva:Artur Ziviani	Diffusion processes in complex dynamic networks can arise, for instance, on data search, data routing, and information spreading. Therefore, understanding how to speed up the diffusion process is an important topic in the study of complex dynamic networks. In this paper, we shed light on how centrality measures and node dynamics coupled with simple diffusion models can help on accelerating the cover time in dynamic networks. Using data from systems with different characteristics, we show that if dynamics is disregarded, network cover time is highly underestimated. Moreover, using centrality accelerates the diffusion process over a different set of complex dynamic networks when compared with the random walk approach. For the best case, in order to cover 80% of nodes, fast centrality-driven diffusion reaches an improvement of 60%, i.e. when next-hop nodes are selected by using centrality measures. Additionally, we also propose and present the first results on how link prediction can help on speeding up the diffusion process in dynamic networks.	Fast centrality-driven diffusion in dynamic networks	NA:NA:NA:NA	2018
Abedelaziz Mohaisen:Omar Alrawi	Malware family classification is an age old problem that many Anti-Virus (AV) companies have tackled. There are two common techniques used for classification, signature based and behavior based. Signature based classification uses a common sequence of bytes that appears in the binary code to identify and detect a family of malware. Behavior based classification uses artifacts created by malware during execution for identification. In this paper we report on a unique dataset we obtained from our operations and classified using several machine learning techniques using the behavior-based approach. Our main class of malware we are interested in classifying is the popular Zeus malware. For its classification we identify 65 features that are unique and robust for identifying malware families. We show that artifacts like file system, registry, and network features can be used to identify distinct malware families with high accuracy - in some cases as high as 95 percent.	Unveiling Zeus: automated classification of malware samples	NA:NA	2018
Fabricio Benevenuto	NA	Session details: SIMPLEX'13 technical session 2	NA	2018
Michele A. Brandão:Mirella M. Moro:Giseli Rabello Lopes:José P.M. Oliveira	Social network analysis (SNA) has been explored in many contexts with different goals. Here, we use concepts from SNA for recommending collaborations in academic networks. Recent work shows that research groups with well connected academic networks tend to be more prolific. Hence, recommending collaborations is useful for increasing a group's connections, then boosting the group research as a collateral advantage. In this work, we propose two new metrics for recommending new collaborations or intensification of existing ones. Each metric considers a social principle (homophily and proximity) that is relevant within the academic context. The focus is to verify how these metrics influence in the resulting recommendations. We also propose new metrics for evaluating the recommendations based on social concepts (novelty, diversity and coverage) that have never been used for such a goal. Our experimental evaluation shows that considering our new metrics improves the quality of the recommendations when compared to the state-of-the-art.	Using link semantics to recommend collaborations in academic social networks	NA:NA:NA:NA	2018
Krishna P. Gummadi	The sharing of personal data has emerged as a popular activity over online social networking sites like Facebook. As a result, the issue of online social network privacy has received significant attention in both the research literature and the mainstream media. Our overarching goal is to improve defaults and provide better tools for managing privacy, but we are limited by the fact that the full extent of the privacy problem remains unknown; there is little quantification of the incidence of incorrect privacy settings or the difficulty users face when managing their privacy. In this talk, I will first focus on measuring the disparity between the desired and actual privacy settings, quantifying the magnitude of the problem of managing privacy. Later, I will discuss how social network analysis techniques can be leveraged towards addressing the privacy management crisis.	Addressing the privacy management crisis in online social networks	NA	2018
Gianmarco De Francisci Morales:Aristides Gionis:Fabrizio Silvestri	SNOW'13 Welcome and organization	Session details: SNOW'13 workshop	NA:NA:NA	2018
Fabrizio Silvestri	NA	Session details: SNOW'13 opening	NA	2018
Steve Schifferes	This paper draws on the parallels between the current period and other periods of historic change in journalism to examine what is new in today's world of social media and what continuities there are with the past. It examines the changing relationship between the public and the press and how it is being continuously reinterpreted. It addresses the questions of whether we are the beginning or end of a process of revolutionary media change.	Social media, journalism and the public	NA	2018
Kanak Kiscuitwala:Willem Bult:Mathias Lécuyer:T.J. Purtell:Madeline K.B. Ross:Augustin Chaintreau:Chris Haseman:Monica S. Lam:Susan E. McGregor	The rise of social media and data-capable mobile devices in recent years has transformed the face of global journalism, supplanting the broadcast news anchor with a new source for breaking news: the citizen reporter. Social media's decentralized networks and instant re-broadcasting mechanisms mean that the reach of a single tweet can easily trump that of the most powerful broadcast satellite. Brief, text-based and easy to translate, social messages allow news audiences to skip the middleman and get news "straight from the source." Whether used by "citizen" or professional reporters, however, social media technologies can also pose risks that endanger these individuals and, by extension, the press as a whole. First, social media platforms are usually proprietary, leaving users' data and activities on the system open to scrutiny by collaborating companies and/or governments. Second, the networks upon which social media reporting relies are inherently fragile, consisting of easily targeted devices and relatively centralized message-routing systems that authorities may block or simply shut down. Finally, this same privileged access can be used to flood the network with inaccurate or discrediting messages, drowning the signal of real events in misleading noise. A citizen journalist can be anyone who is simply in the right place at the right time. Typically untrained and unevenly tech-savvy, citizen reporters are unaccustomed to thinking of their social media activities as high-risk, and may not consider the need to defend themselves against potential threats. Though often part of a crowd, they may have no formal affiliations; if targeted for retaliation, they may have nowhere to turn for help. The dangers citizen journalists face are personal and physical. They may be targeted in the act of reporting, and/or online through the tracking of their digital communications. Addressing their needs for protection, resilience, and recognition requires a move away from the major assumptions of in vitro communication security. For citizen journalists using social networks, the adversary is already inside, as the network itself may be controlled or influenced by the threatening party, while "outside" nodes, such as public figures, protest organizers, and other journalists can be trusted to handle content appropriately. In these circumstances there can be no seamless, guaranteed solution. Yet the need remains for technologies that improve the security of these journalists who in many cases may constitute a region's only independent press. In this paper, we argue that a comprehensive and collaborative effort is required to make publishing and interacting with news websites more secure. Journalists typically enjoy stronger legal protection at least in some countries, such as the United States. However, this protection may prove ineffective, as many online tools compromise source protection. In the remaining sections, we identify a set of discussion topics and challenges to encourage a broader research agenda aiming to address jointly the need for social features and security for citizens journalists and readers alike. We believe communication technologies should embrace the methods and possibilities of social news rather than treating this as a pure security problem. We briefly touch upon a related initiative, Dispatch, that focuses on providing security to citizen journalists for publisihing content.	Weaving a safe web of news	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Gianmarco De Francisci Morales	NA	Session details: SNOW'13 breaking the news	NA	2018
Carlos Castillo	NA	Traffic prediction and discovery of news via news crowds	NA	2018
Matthias Gallé:Jean-Michel Renders:Eric Karstens	We present a data-driven study on which sources were the first to report on news events. For this, we implemented a news-aggregator that included a large number of established news sources and covered one year of data. We present a novel framework that is able to retrieve a large number of events and not only the most salient ones, while at the same time making sure that they are not exclusively of local impact. Our analysis then focuses on different aspects of the news cycle. In particular we analyze which are the sources to break most of the news. By looking when certain events become bursty, we are able to perform a finer analysis on those events and the associated sources that dominate the global news-attention. Finally we study the time it takes news outlet to report on these events and how this reects different strategies of which news to report. A general finding of our study is that big news agencies remain an important threshold to cross to bring global attention to particular news, but it also shows the importance of focused (by region or topic) outlets.	Who broke the news?: an analysis on first reports of news events	NA:NA:NA	2018
Fabrizio Silvestri	NA	Session details: SNOW'13 social news	NA	2018
Janette Lehmann:Carlos Castillo:Mounia Lalmas:Ethan Zuckerman	Users interact with online news in many ways, one of them being sharing content through online social networking sites such as Twitter. There is a small but important group of users that devote a substantial amount of effort and care to this activity. These users monitor a large variety of sources on a topic or around a story, carefully select interesting material on this topic, and disseminate it to an interested audience ranging from thousands to millions. These users are news curators, and are the main subject of study of this paper. We adopt the perspective of a journalist or news editor who wants to discover news curators among the audience engaged with a news site. We look at the users who shared a news story on Twitter and attempt to identify news curators who may provide more information related to that story. In this paper we describe how to find this specific class of curators, which we refer to as news story curators. Hence, we proceed to compute a set of features for each user, and demonstrate that they can be used to automatically find relevant curators among the audience of two large news organizations.	Finding news curators in twitter	NA:NA:NA:NA	2018
Tom De Nies:Gerald Haesendonck:Fréderic Godin:Wesley De Neve:Erik Mannens:Rik Van de Walle	In this paper, we investigate the possibilities to estimate the impact the content of a news article has on social media, and in particular on Twitter. We propose an approach that makes use of captured and temporarily stored microposts found in social media, and compares their relevance to an arbitrary news article. These results are used to derive key indicators of the social media impact of the specified content. We describe each step of our approach, provide a first implementation, and discuss the most imminent challenges and discussion points.	Towards automatic assessment of the social media impact of news content	NA:NA:NA:NA:NA:NA	2018
Steve Schifferes:Nic Newman	The problem of verification is the key issue for journalists who use social media. This paper argues for the importance of a user-centered approach in finding solutions to this problem. Because journalists have different needs for different types of stories, there is no one magic bullet that can verify social media. Any tool will need to have a multi-faceted approach to the problem, and will have to be adjustable to suit the particular needs of individual journalists and news organizations.	Verifying news on the social web: challenges and prospects	NA:NA	2018
Arkaitz Zubiaga	The front page is the showcase that might condition whether one buys a newspaper, and so editors carefully select the news of the day that they believe will attract as many readers as possible. Little is known about the extent to which editors' criteria for front page news selection are appropriate so as to matching the actual interests of the crowd. In this paper, we compare the news stories in The New York Times over the period of a year to their popularity on Twitter and Facebook. Our study questions the current news selection criteria, revealing that while editors focus on picking hard news such as politics for the front page, social media users are rather into soft news such as science and fashion.	Newspaper editors vs the crowd: on the appropriateness of front page news selection	NA	2018
Nigel Shadbolt:David De Roure	SOCM'13 welcome and organization	Session details: SOCM'13 workshop	NA:NA	2018
Vanilson Buregio:Silvio Meira:Nelson Rosa	Blending computational and social elements into software has gained significant attention in key conferences and journals. In this context, "Social Machines" appears as a promising model for unifying both computational and social processes. However, it is a fresh topic, with concepts and definitions coming from different research fields, making a unified understanding of the concept a somewhat challenging endeavor. This paper aims to investigate efforts related to this topic and build a preliminary classification scheme to structure the science of Social Machines. We provide a preliminary overview of this research area through the identification of the main visions, concepts, and approaches; we additionally examine the result of the convergence of existing contributions. With the field still in its early stage, we believe that this work can collaborate to the process of providing a more common and coherent conceptual basis for understanding Social Machines as a paradigm. Furthermore, this study helps detect important research issues and gaps in the area.	Social machines: a unified paradigm to describe social web-oriented systems	NA:NA:NA	2018
Maire Byrne Evans:Kieron O'Hara:Thanassis Tiropanis:Craig Webber	The authors explore some issues with the United Kingdom (U.K.) crime reporting and recording systems which currently produce Open Crime Data. The availability of Open Crime Data seems to create a potential data ecosystem which would encourage crowdsourcing, or the creation of social machines, in order to counter some of these issues. While such solutions are enticing, we suggest that in fact the theoretical solution brings to light fairly compelling problems, which highlight some limitations of crowdsourcing as a means of addressing Berners-Lee's "social constraint." The authors present a thought experiment -- a Gendankenexperiment - in order to explore the implications, both good and bad, of a social machine in such a sensitive space and suggest a Web Science perspective to pick apart the ramifications of this thought experiment as a theoretical approach to the characterisation of social machines.	Crime applications and social machines: crowdsourcing sensitive data	NA:NA:NA:NA	2018
Ben Dalton	This paper describes the potential of systems in which many people collectively control a single constructed identity mediated by socio-technical networks. By looking to examples of identities that have spontaneously emerged from anonymous communities online, a model for pseudonym design in social machines is proposed. A framework of identity dimensions is presented as a means of exploring the functional types of identity encountered in social machines, and design guidelines are outlined that suggest possible approaches to this task.	Pseudonymity in social machines	NA	2018
David De Roure:Clare Hooper:Megan Meredith-Lobay:Kevin Page:Ségolène Tarte:Don Cruickshank:Catherine De Roure	As a scoping exercise in the design of our Social Machines Observatory we consider the observation of Social Machines "in the wild", as illustrated through two scenarios. More than identifying and classifying individual machines, we argue that we need to study interactions between machines and observe them throughout their lifecycle. We suggest that purpose may be a key notion to help identify individual Social Machines in composed systems, and that mixed observation methods will be required. This exercise provides a basis for later work on how we instrument and observe the ecosystem.	Observing social machines part 1: what to observe?	NA:NA:NA:NA:NA:NA:NA	2018
Nigel R. Shadbolt:Daniel A. Smith:Elena Simperl:Max Van Kleek:Yang Yang:Wendy Hall	The state of the art in human interaction with computational systems blurs the line between computations performed by machine logic and algorithms, and those that result from input by humans, arising from their own psychological processes and life experience. Current socio-technical systems, known as "social machines" exploit the large-scale interaction of humans with machines. Interactions that are motivated by numerous goals and purposes including financial gain, charitable aid, and simply for fun. In this paper we explore the landscape of social machines, both past and present, with the aim of defining an initial classificatory framework. Through a number of knowledge elicitation and refinement exercises we have identified the polyarchical relationship between infrastructure, social machines, and large-scale social initiatives. Our initial framework describes classification constructs in the areas of contributions, participants, and motivation. We present an initial characterisation of some of the most popular social machines, as demonstration of the use of the identified constructs. We believe that it is important to undertake an analysis of the behaviour and phenomenology of social machines, and of their growth and evolution over time. Our future work will seek to elicit additional opinions, classifications and validation from a wider audience, to produce a comprehensive framework for the description, analysis and comparison of social machines.	Towards a classification framework for social machines	NA:NA:NA:NA:NA:NA	2018
Priyanka Singh:Nigel Shadbolt	Internet is an easy medium for people to collaborate and crowdsourcing is an efficient feature of social web where people with common interest and expertise come together to solve specific problems by collective thinking and create a community. It can also be used to filter out important information from large data, remove spams, and gamification techniques are used to reward the users for their contribution and keep a sustainable environment for the growth of the community. Semantic web technologies can be used to structure the community data so it can be combined, decentralized and be used across platform. Using such tools knowledge can be enhanced and easily discovered and merged together. This paper discusses the concept of a purposive social network where people with similar interest and varied expertise come together, use crowdsourcing technique to solve a common problem and build tools for common purpose. The StackOverflow website is chosen to study the purposive network, different network ties and roles of user is studied. Linked Data is used for name disambiguation of keywords and topics for easier search and discovery of experts in a field and provide useful information that is otherwise unavailable in the website.	Linked data in crowdsourcing purposive social network	NA:NA	2018
Markus Strohmaier	Social machines are integrated systems of people and computers. What distinguishes social machines from other types of software systems - such as software for cars or air planes - is the unprecedented involvement of data about user behavior, -goals and -motivations into the software system's structure. In social machines, the interaction between a user and the system is mediated by the aggregation of explicit or implicit data from other users. This is the case with systems where, for example, user data is used to suggest search terms (e.g. Google Autosuggest), to recommend products (e.g. Amazon recommendations), to aid navigation (e.g. tag-based navigation) or to filter content (e.g. Digg.com). This makes social machines a novel class of software systems (as opposed to for example safety-related software that is being used in cars) and unique in a sense that potentially essential system properties and functions - such as navigability - are dynamically influenced by aggregate user behavior. Such properties can not be satisfied through the implementation of requirements alone, what is needed is regulation, i.e. a dynamic integration of users' goals and behavior into the continuous process of engineering. Functional and non-functional properties of software systems have been the subject of software engineering research for decades [1]. The notion of non-functional requirements (softgoals) captures a recognition by the software engineering community that software requirements can be subjective and interdependent, they can lack a clear-cut success criteria, exhibit different priorities and can require decomposition or operationalization. Resulting approaches to analyzing and designing software systems emphasize the role of users (or more general: agents) in this process (such as [1]). i* for example has been used to capture and represent user goals during system design and run time. With the emergence of social machines, such as the WWW, and social-focussed applications running on top of the web, such as facebook.com, delicious.com and others, social machines and their emergent properties have become a crucial infrastructure for many aspects of our daily lives. To give an example: the navigability of the web depends on the behavior of web editors who are interlinking documents, or the usefulness of tags for classification depends on the tagging behavior of users [2]. The rise of social machines can be expected to fundamentally change the way in which such properties and functions of software systems are designed and maintained. Rather than planning for certain system properties (such as navigability, usefulness for certain tasks) and functions at design time, the task of engineers is to build a platform which allows to influence and regulate emergent user behavior in such a way that desired system attributes are achieved at run time. It is through the process of social computation, i.e. the combination of social behavior and algorithmic computation, that desired system properties and functions emerge. For a science of social machines, specifically understanding the relationship between individual and social behavior on one hand, and desired system properties and functions on the other is crucial. In order to maintain control, research must focus on understanding a wide variety of social machine properties such as semantic, intentional and navigational properties across different systems and applications including - but not limited to - social media. Summarizing, the full implications of the genesis of social machines for related domains including software engineering, knowledge acquisition or peer production systems are far from being well understood, and warrant future work. For example, the interactions between the pragmatics of such systems (how they are used) and the semantics emerging in those systems (what the words, symbols, etc mean) is a fundamental issue that deserves greater attention. Equipping engineers of social machines with the right tools to achieve and maintain desirable system properties is a problem of practical relevance that needs to be addressed by future research.	A few thoughts on engineering social machines: extended abstract	NA	2018
Ramine Tinati:Leslie Carr:Susan Halford:Catherine J. Pope	The Web represents a collection of socio-technical activities inter-operating using a set of common protocols and standards. Online banking, web TV, internet shopping, e-government and social networking are all different kinds of human interaction that have recently leveraged the capabilities of the Web architecture. Activities that have human and computer components are referred to as social machines. This paper introduces HTP, a socio-technical model to understand, describe and analyze the formation and development of social machines and other web activities. HTP comprises three components: heterogeneous networks of actors involved in a social machine; the iterative process of translation of the actors' activities into a temporarily stable and sustainable social machine; and the different phases of this machine's adaptation from one stable state to another as the surrounding networks restructure and global agendas ebb and flow. The HTP components are drawn from an interdisciplinary range of theoretical positions and concepts. HTP provides an analytical framework to explain why different Web activities remain stable and functional, whilst others fail. We illustrate the use of HTP by examining the formation of a classic social machine (Wikipedia), and the stabilization points corresponding to its different phases of development.	The HTP model: understanding the development of social machines	NA:NA:NA:NA	2018
Max Van Kleek:Daniel A. Smith:Wendy Hall:Nigel Shadbolt	Can the Web help people live healthier lives? This paper seeks to answer this question through an examination of sites, apps and online communities designed to help people improve their fitness, better manage their disease(s) and conditions, and to solve the often elusive connections between the symptoms they experience, diseases and treatments. These health social machines employ a combination of both simple and complex social and computational processes to provide such support. We first provide a descriptive classification of the kinds of machines currently available, and the support each class offers. We then describe the limitations exhibited by these systems and potential ways around them, towards the design of more effective machines in the future.	"the crowd keeps me in shape": social psychology and the present and future of health social machines	NA:NA:NA:NA	2018
Ido Guy:Li Chen:Michelle X. Zhou	SRS'13 welcome and organization	Session details: SRS'13 workshop	NA:NA:NA	2018
Jure Leskovec	Recommender systems are inherently driven by evaluations and reviews provided by the users of these systems. Understanding ways in which users form judgments and produce evaluations can provide insights for modern recommendation systems. Many online social applications include mechanisms for users to express evaluations of one another, or of the content they create. In a variety of domains, mechanisms for evaluation allow one user to say whether he or she trusts another user, or likes the content they produced, or wants to confer special levels of authority or responsibility on them. We investigate a number of fundamental ways in which user and item characteristics affect the evaluations in online settings. For example, evaluations are not unidimensional but include multiple aspects that all together contribute to user's overall rating. We investigate methods for modeling attitudes and attributes from online reviews that help us better understand user's individual preferences. We also examine how to create a composite description of evaluations that accurately reflects some type of cumulative opinion of a community. Natural applications of these investigations include predicting the evaluation outcomes based on user characteristics and to estimate the chance of a favorable overall evaluation from a group knowing only the attributes of the group's members, but not their expressed opinions.	How status and reputation shape human evaluations: consequences for recommender systems	NA	2018
Mitul Tiwari	Online social networks have become very important for networking, communication, sharing, and content discovery. Recommender systems play a significant role on any online social network for engaging members, recruiting new members, and recommending other members to connect with. This talk presents challenges in recommender systems, graph analysis, social stream relevance and virality on a large-scale social networks such as LinkedIn, the largest professional network with more than 200M members. First, social recommender systems for recommending jobs, groups, companies to follow, other members to connect with, are very important part of a professional network like LinkedIn [1, 6, 7, 9]. Each one of these entity recommender systems present novel challenges to use social and member generated data. Second, various problems, such as, link prediction, visualizing connection network, finding the strength of each connection, and the best path among members, require large-scale social graph analysis, and present unique research opportunities [2, 5]. Third, social stream relevance and capturing virality in social products are crucial for engaging users on any online social network [4]. Final, systems challenges must be addressed in scaling recommender systems on a large-scale social networks [3, 8, 10]. This talk presents challenges and interesting problems in large-scale social recommender systems, and describes some of the solutions.	Large-scale social recommender systems: challenges and opportunities	NA	2018
Giuliano Arru:Davide Feltoni Gurini:Fabio Gasparetti:Alessandro Micarelli:Giuseppe Sansonetti	In recent years, social networks have become one of the best ways to access information. The ease with which users connect to each other and the opportunity provided by Twitter and other social tools in order to follow person activities are increasing the use of such platforms for gathering information. The amount of available digital data is the core of the new challenges we now face. Social recommender systems can suggest both relevant content and users with common social interests. Our approach relies on a signal-based model, which explicitly includes a time dimension in the representation of the user interests. Specifically, this model takes advantage of a signal processing technique, namely, the wavelet transform, for defining an efficient pattern-based similarity function among users. Experimental comparisons with other approaches show the benefits of the proposed approach.	Signal-based user recommendation on twitter	NA:NA:NA:NA:NA	2018
Lucas Augusto M.C. Carvalho:Hendrik T. Macedo	Group recommender systems usually provide recommendations to a fixed and predetermined set of members. In some situations, however, there is a set of people (N) that should be organized into smaller and cohesive groups, so it is possible to provide more effective recommendations to each of them. This is not a trivial task. In this paper we propose an innovative approach for grouping people within the recommendation problem context. The problem is modeled as a coalitional game from Game Theory. The goal is to group people into exhaustive and disjoint coalitions so as to maximize the social welfare function of the group. The optimal coalition structure is that with highest summation over all social welfare values. Similarities between recommendation system users are used to define the social welfare function. We compare our approach with K-Means clustering for a dataset from Movielens. Results have shown that the proposed approach performs better than K-Means for both average group satisfaction and Davies-Bouldin index metrics when the number of coalitions found is not greater than 4 (K <= 4) for a population size of 12 (N = 12).	Generation of coalition structures to provide proper groups' formation in group recommender systems	NA:NA	2018
Lucas Augusto Montalvão Costa Carvalho:Hendrik Teixeira Macedo	A major difficulty in a recommendation system for groups is to use a group aggregation strategy to ensure, among other things, the maximization of the average satisfaction of group members. This paper presents an approach based on the theory of noncooperative games to solve this problem. While group members can be seen as game players, the items for potential recommendation for the group comprise the set of possible actions. Achieving group satisfaction as a whole becomes, then, a problem of finding the Nash equilibrium. Experiments with a MovieLens dataset and a function of arithmetic mean to compute the prediction of group satisfaction for the generated recommendation have shown statistically significant results when compared to state-of-the-art aggregation strategies, in particular, when evaluation among group members are more heterogeneous. The feasibility of this unique approach is shown by the development of an application for Facebook, which recommends movies to groups of friends.	Users' satisfaction in recommendation systems for groups: an approach based on noncooperative games	NA:NA	2018
Sara Cohen:Lior Ebel	This paper studies the problem of recommending collaborators in a social network, given a set of keywords. Formally, given a query q, consisting of a researcher s (who is a member of a social network) and a set of keywords k (e.g., an article name or topic of future work), the collaborator recommendation problem is to return a high-quality ranked list of possible collaborators for s on the topic k. Extensive effort was expended to define ranking functions that take into consideration a variety of properties, including structural proximity to s, textual relevance to k, and importance. The effectiveness of our methods have been experimentally proven over two large subsets of the social network determined by DBLP co-authorship data. The results show that the ranking methods developed in this paper work well in practice.	Recommending collaborators using keywords	NA:NA	2018
Yao Lu:Sandy El Helou:Denis Gillet	In this paper, a hybrid recommender system for job seeking and recruiting websites is presented. The various interaction features designed on the website help the users organize the resources they need as well as express their interest. The hybrid recommender system exploits the job and user profiles and the actions undertaken by users in order to generate personalized recommendations of candidates and jobs. The data collected from the website is modeled using a directed, weighted, and multi-relational graph, and the 3A ranking algorithm is exploited to rank items according to their relevance to the target user. A preliminary evaluation is conducted based on simulated data and production data from a job hunting website in Switzerland.	A recommender system for job seeking and recruiting website	NA:NA:NA	2018
Danilo Menezes:Anisio Lacerda:Leila Silva:Adriano Veloso:Nivio Ziviani	Recommender systems are used to help people in specific life choices, like what items to buy, what news to read or what movies to watch. A relevant work in this context is the Slope One algorithm, which is based on the concept of differential popularity between items (i.e., how much better one item is liked than another). This paper proposes new approaches to extend Slope One based predictors for collaborative filtering, in which the predictions are weighted based on the number of users that co-rated items. We propose to improve collaborative filtering by exploiting the web of trust concept, as well as an item utility measure based on the error of predictions based on specific items to specific users. We performed experiments using three application scenarios, namely Movielens, Epinions, and Flixter. Our results demonstrate that, in most cases, exploiting the web of trust is benefitial to prediction performance, and improvements are reported when comparing the proposed approaches against the original Weighted Slope One algorithm.	Weighted slope one predictors revisited	NA:NA:NA:NA:NA	2018
Maximilien Servajean:Esther Pacitti:Sihem Amer-Yahia:Pascal Neveu	We investigate profile diversity, a novel idea in searching scientic documents. Combining keyword relevance with popularity in a scoring function has been the subject of different forms of social relevance [2, 6, 9]. Content diversity has been thoroughly studied in search and advertising [4, 11], database queries [16, 5, 8], and recommendations [17, 10, 18]. We believe our work is the first to investigate profile diversity to address the problem of returning highly popular but too-focused documents. We show how to adapt Fagin's threshold-based algorithms to return the most relevant and most popular documents that satisfy content and profile diversities and run preliminary experiments on two benchmarks to validate our scoring function.	Profile diversity in search and recommendation	NA:NA:NA:NA	2018
Mozhgan Tavakolifard:Kevin C. Almeroth:Jon Atle Gulla	Social recommender systems aim to alleviate the information overload problem on social network sites. The social network structure is often an important input to these recommender systems. Typically, this structure cannot be inferred directly from declared relationships among users. The goal of our work is to extract an underlying hidden and sparse network which more strongly represents the actual interactions among users. We study how to leverage Twitter activities like micro-blogging and the network structure to find a simple, efficient, but accurate method to infer and expand this hidden network. We measure and compare the performance of several different modeling strategies using a crawled data set from Twitter. Our results reveal that the structural similarity in the network generated by users' retweeting behavior outweighs the other discussed methods.	Does social contact matter?: modelling the hidden web of trust underlying twitter	NA:NA:NA	2018
Jun Zhang:Chun-yuen Teng:Yan Qu	In this paper, we introduce a network-based method to study user spatial behaviors based on check-in histories. The results of this study have direct implications for location-based recommendation systems.	Understanding user spatial behaviors for location-based recommendations	NA:NA:NA	2018
Maja Vukovic:Soundar Kumara:Patrick Meier	SWDM'13 welcome and organization	Session details: SWDM'13 workshop	NA:NA:NA	2018
Soundar Kumara	NA	Session details: SWDM'13 keynote	NA	2018
Ramesh C. Jain	Connecting people to required resources efficiently, effectively and promptly is one of the most important challenges for our society. Disasters make it the challenge for life and death. During disasters many normal sources of information to assess situations as well as distributing vital information to individuals break down. Unfortunately, during disastrous situations, most current practices are forced to follow bureaucratic processes and procedures that may delay help in critical life and death moments. Social media brings together different media as well as modes of distribution - focused, narrowcast, and broadcast -- and has revolutionized communication among people. Mobile phones, equipped with myriads of sensors are bringing the next generation of social networks not only to connect people with other people, but also to connect people with other people and essential life resources based on the disaster situation and personal context. We believe that such Social Life Networks (SLN) may play very important role for solving some essential human problems, including providing vital help to people during disasters. We will present early design of such systems and use a few examples of such systems explored in our group during disasters. Focused Micro Blogs (FMBs) will be discussed as an alternative to less noisy and more direct versions of current microblogs, such as Tweets and Status Updates. An important part of our discussion will be to list challenges and opportunities in this area.	Disasters response using social life networks	NA	2018
Maja Vukovic	NA	Session details: SWDM'13 twitter in action	NA	2018
Bella Robinson:Robert Power:Mark Cameron	This paper describes early work at developing an earthquake detector for Australia and New Zealand using Twitter. The system is based on the Emergency Situation Awareness (ESA) platform which provides all-hazard information captured, filtered and analysed from Twitter. The detector sends email notifications of evidence of earthquakes from Tweets to the Joint Australian Tsunami Warning Centre. The earthquake detector uses the ESA platform to monitor Tweets and checks for specific earthquake related alerts. The Tweets that contribute to an alert are then examined to determine their locations: when the Tweets are identified as being geographically close and the retweet percentage is low an email notification is generated. The earthquake detector has been in operation since December 2012 with 31 notifications generated where 17 corresponded with real, although minor, earthquake events. The remaining 14 were a result of discussions about earthquakes but not prompted by an event. A simple modification to our algorithm results in 20 notifications identifying the same 17 real events and reducing the false positives to 3. Our detector is sensitive in that it can generate alerts from only a few Tweets when they are determined to be geographically close.	A sensitive Twitter earthquake detector	NA:NA:NA	2018
Yuan Liang:James Caverlee:John Mander	In this paper, we investigate the potential of social media to provide rapid insights into the location and extent of damage associated with two recent earthquakes - the 2011 Tohoku earthquake in Japan and the 2011 Christchurch earthquake in New Zealand. Concretely, we (i) assess and model the spatial coverage of social media; and (ii) study the density and dynamics of social media in the aftermath of these two earthquakes. We examine the difference between text tweets and media tweets (containing links to images and videos), and investigate tweet density, re-tweet density, and user tweeting count to estimate the epicenter and to model the intensity attenuation of each earthquake. We find that media tweets provide more valuable location information, and that the relationship between social media activity vs. loss/damage attenuation suggests that social media following a catastrophic event can provide rapid insight into the extent of damage.	Text vs. images: on the viability of social media to assess earthquake damage	NA:NA:NA	2018
Robert Power:Bella Robinson:Catherine Wise	This paper describes ongoing work with the Australian Government to assemble information from a collection of web feeds describing emergency incidents of interest for emergency managers. The developed system, the Emergency Response Intelligence Capability (ERIC) tool, has been used to gather information about emergency events during the Australian summer of 2012/13. The web feeds are an authoritative source of structured information summarising incidents that includes links to emergency services web sites containing further details about the events underway. The intelligence obtained using ERIC for a specific fire event has been compared with information that was available in Twitter using the Emergency Situation Awareness (ESA) platform. This information would have been useful as a new source of intelligence: it was reported faster than via the web feed, contained more specific event information, included details of impact to the community, was updated more frequently, included information from the public and remains available as a source of information long after the web feed contents have been removed.	Comparing web feeds and tweets for emergency management	NA:NA:NA	2018
Patrick Meier	NA	Session details: SWDM'13 keynote 2	NA	2018
David Stevens	This paper presents a summary of the main points put forward during the presentation delivered at the 2nd International Workshop on Social Web for Disaster Management which was held in conjunction with WWW 2013 on May 14th 2013 in Rio de Janeiro, Brazil.	Leveraging on social media to support the global building resilient cities campaign	NA	2018
Soundar Kumara	NA	Session details: SWDM'13 insights from social web	NA	2018
Yohei Ikawa:Maja Vukovic:Jakob Rogstadius:Akiko Murakami	Citizens, news reporters, relief organizations, and governments are increasingly relying on the Social Web to report on and respond to disasters as they occur. The capability to rapidly react to important events, which can be identified from high-volume streams even when the sources are unknown, still requires precise localization of the events and verification of the reports. In this paper, we propose a framework for classifying location elements and a method for their extraction from Social Web data. We describe the framework in the context of existing Social Web systems used for disaster management. We present a new location-inferencing architecture and evaluate its performance with a data set from a real-world disaster.	Location-based insights from the social web	NA:NA:NA:NA	2018
John Lingad:Sarvnaz Karimi:Jie Yin	Location information is critical to understanding the impact of a disaster, including where the damage is, where people need assistance and where help is available. We investigate the feasibility of applying Named Entity Recognizers to extract locations from microblogs, at the level of both geo-location and point-of-interest. Our experimental results show that such tools once retrained on microblog data have great potential to detect the where information, even at the granularity of point-of-interest.	Location extraction from disaster-related microblogs	NA:NA:NA	2018
Muhammad Imran:Shady Elbassuoni:Carlos Castillo:Fernando Diaz:Patrick Meier	During times of disasters online users generate a significant amount of data, some of which are extremely valuable for relief efforts. In this paper, we study the nature of social-media content generated during two different natural disasters. We also train a model based on conditional random fields to extract valuable information from such content. We evaluate our techniques over our two datasets through a set of carefully designed experiments. We also test our methods over a non-disaster dataset to show that our extraction model is useful for extracting information from socially-generated content in general.	Practical extraction of disaster-relevant information from social media	NA:NA:NA:NA:NA	2018
Fujio Toriumi:Takeshi Sakaki:Kosuke Shinoda:Kazuhiro Kazama:Satoshi Kurihara:Itsuki Noda	Such large disasters as earthquakes and hurricanes are very unpredictable. During a disaster, we must collect information to save lives. However, in time disaster, it is difficult to collect information which is useful for ourselves from such traditional mass media as TV and newspapers that contain information for the general public. Social media attract attention for sharing information, especially Twitter, which is a hugely popular social medium that is now being used during disasters. In this paper, we focus on the information sharing behaviors on Twitter during disasters. We collected data before and during the Great East Japan Earthquake and arrived at the following conclusions: Many users with little experience with such specific functions as reply and retweet did not continuously use them after the disaster. Retweets were well used to share information on Twitter.  Retweets were used not only for sharing the information provided by general users but used for relaying the information from the mass media. We conclude that social media users changed their behavior to widely diffuse important information and decreased non-emergency tweets to avoid interrupting critical information.	Information sharing on Twitter during the 2011 catastrophic earthquake	NA:NA:NA:NA:NA:NA	2018
Abdulfatai Popoola:Dmytro Krasnoshtan:Attila-Peter Toth:Victor Naroditskiy:Carlos Castillo:Patrick Meier:Iyad Rahwan	Large amounts of unverified and at times contradictory information often appear on social media following natural disasters. Timely verification of this information can be crucial to saving lives and for coordinating relief efforts. Our goal is to enable this verification by developing an online platform that involves ordinary citizens in the evidence gathering and evaluation process. The output of this platform will provide reliable information to humanitarian organizations, journalists, and decision makers involved in relief efforts.	Information verification during natural disasters	NA:NA:NA:NA:NA:NA:NA	2018
Ricardo Baeza-Yates:Julien Masanàs:Marc Spaniol	TempWeb'13 welcome and organization	Session details: TempWeb'13 workshop	NA:NA:NA	2018
Omar Alonso:Kyle Shiells	Known events that are scheduled in advance, such as popular sports games, usually get a lot of attention from the public. Communications media like TV, radio, and newspapers will report the salient aspects of such events live or post-hoc for general consumption. However, certain actions, facts, and opinions would likely be omitted from those objective summaries. Our approach is to construct a particular game's timeline in such a way that it can be used as a quick summary of the main events that happened along with popular subjective and opinionated items that the public inject. Peaks in the volume of posts discussing the event reflect both objectively recognizable events in the game - in the sports example, a change in score - and subjective events such as a referee making a call fans disagree with. In this work, we introduce a novel timeline design that captures a more complete story of the event by placing the volume of Twitter posts alongside keywords that are driving the additional traffic. We demonstrate our approach using events of major international social impact from the World Cup 2010 and evaluate against professional liveblog coverage of the same events.	Timelines as summaries of popular scheduled events	NA:NA	2018
Miguel Costa:Daniel Gomes:Francisco Couto:Mário Silva	Web archives already hold more than 282 billion documents and users demand full-text search to explore this historical information. This survey provides an overview of web archive search architectures designed for time-travel search, i.e. full-text search on the web within a user-specified time interval. Performance, scalability and ease of management are important aspects to take in consideration when choosing a system architecture. We compare these aspects and initialize the discussion of which search architecture is more suitable for a large-scale web archive.	A survey of web archive search architectures	NA:NA:NA:NA	2018
Ahmed AlSum:Michael L. Nelson:Robert Sanderson:Herbert Van de Sompel	When retrieving archived copies of web resources (mementos) from web archives, the original resource's URI-R is typically used as the lookup key in the web archive. This is straightforward until the resource on the live web issues a redirect: R ->R`. Then it is not clear if R or R` should be used as the lookup key to the web archive. In this paper, we report on a quantitative study to evaluate a set of policies to help the client discover the correct memento when faced with redirection. We studied the stability of 10,000 resources and found that 48% of the sample URIs tested were not stable, with respect to their status and redirection location. 27% of the resources were not perfectly reliable in terms of the number of mementos of successful responses over the total number of mementos, and 2% had a reliability score of less than 0.5. We tested two retrieval policies. The first policy covered the resources which currently issue redirects and successfully resolved 17 out of 77 URIs that did not have mementos of the original URI, but did of the resource that was being redirected to. The second policy covered archived copies with HTTP redirection and helped the client in 58% of the cases tested to discover the nearest memento to the requested datetime.	Archival HTTP redirection retrieval policies	NA:NA:NA:NA	2018
Daniel Gomes:Miguel Costa:David Cruz:João Miranda:Simão Fontes	Web information is ephemeral. Several organizations around the world are struggling to archive information from the web before it vanishes. However, users demand efficient and effective search mechanisms to access the already vast collections of historical information held by web archives. The Portuguese Web Archive is the largest full-text searchable web archive publicly available. It supports search over 1.2 billion files archived from the web since 1996. This study contributes with an overview of the lessons learned while developing the Portuguese Web Archive, focusing on web data acquisition, ranking search results and user interface design. The developed software is freely available as an open source project. We believe that sharing our experience obtained while developing and operating a running service will enable other organizations to start or improve their web archives.	Creating a billion-scale searchable web archive	NA:NA:NA:NA:NA	2018
Julia Kiseleva:Hoang Thanh Lam:Mykola Pechenizkiy:Toon Calders	In many web information systems such as e-shops and information portals, predictive modeling is used to understand user's intentions based on their browsing behaviour. User behavior is inherently sensitive to various hidden contexts. It has been shown in different experimental studies that exploitation of contextual information can help in improving prediction performance significantly. It is reasonable to assume that users may change their intents during one web session and that changes are influenced by some external factors such as switch in temporal context e.g. 'users want to find information about a specific product' and after a while 'they want to buy this product'. A web session can be represented as a sequence of user's actions where actions are ordered by time. The generation of a web session might be influenced by several hidden temporal contexts. Each session can be represented as a concatenation of independent segments, each of which is influenced by one corresponding context. We show how to learn how to apply different predictive models for each segment in this work. We define the problem of discovering temporal hidden contexts in such way that we optimize directly the accuracy of predictive models (e.g. users' trails prediction) during the process of context acquisition. Our empirical study on a real dataset demonstrates the effectiveness of our method.	Discovering temporal hidden contexts in web sessions for user trail prediction	NA:NA:NA:NA	2018
Hany M. SalahEldeen:Michael L. Nelson	In the course of web research it is often necessary to estimate the creation datetime for web resources (in the general case, this value can only be estimated). While it is feasible to manually establish likely datetime values for small numbers of resources, this becomes infeasible if the collection is large. We present "carbon date", a simple web application that estimates the creation date for a URI by polling a number of sources of evidence and returning a machine-readable structure with their respective values. To establish a likely datetime, we poll bitly for the first time someone shortened the URI, topsy for the first time someone tweeted the URI, a Memento aggregator for the first time it appeared in a public web archive, Google's time of last crawl, and the Last-Modified HTTP response header of the resource itself. We also examine the backlinks of the URI as reported by Google and apply the same techniques for the resources that link to the URI. We evaluated our tool on a gold standard data set of 1200 URIs in which the creation date was manually verified. We were able to estimate a creation date for 75.90% of the resources, with 32.78% having the correct value. Given the different nature of the URIs, the union of the various methods produces the best results. While the Google last crawl date and topsy account for nearly 66% of the closest answers, eliminating the web archives or Last-Modified from the results produces the largest overall negative impact on the results. The carbon date application is available for download or use via a web API.	Carbon dating the web: estimating the age of web resources	NA:NA	2018
Omar Alonso	In the last few years there has been an increased interest from researchers and practitioners in exploring time as a dimension that can benefit several information retrieval tasks. There is exciting work in analyzing and exploiting temporal information embedded in documents as relevance cues for the presentation, organization, and the exploration of search results in a temporal context. Most of the current approaches focus on leveraging the temporal information available in document sources like web pages or news articles. However, the Web keeps evolving beyond simple web pages and new information sources and services are adopted very rapidly. For example, the incredible amount of content that is generated by users in social networks offers another aspect to examine how people produce and consume content over time. We review the current activities centered on identifying and extracting time information from document collections and the applications to the information seeking process. We outline the potential of new sources for studying temporal information by presenting new problems. Finally, we discuss a number of scenarios where a temporal perspective can provide insights when exploring Web contents.	Stuff happens continuously: exploring web contents with temporal information	NA	2018
Lucas C.O. Miranda:Rodrygo L.T. Santos:Alberto H.F. Laender	Watching online videos is part of the daily routine of a considerable fraction of Internet users nowadays. Understanding the patterns of access to these videos is paramount for improving the capacity planning for video providers, the conversion rate for advertisers, and the relevance of the whole online video watching experience for end users. While much research has been conducted to analyze video access patterns in user-generated content (UGC), little is known of how such patterns manifest in mainstream media (MSM) portals. In this paper, we perform the first large-scale analysis of video access patterns in MSM portals. As a case study, we analyze interaction logs across a total of 38 Brazilian MSM portals, including six of the largest portals in the country, over a period of eight weeks. Our analysis reveals interesting static and temporal video access patterns in MSM portals, which we compare and contrast to the access patterns reported for UGC websites. Overall, our analysis provides several insights for an improved understanding of video access on the Internet beyond UGC websites.	Characterizing video access patterns in mainstream media portals	NA:NA:NA	2018
L. Elisa Celis:Koustuv Dasgupta:Vaibhav Rajan	Crowdsourcing is rapidly emerging as a computing paradigm that can employ the collective intelligence of a distributed human population to solve a wide variety of tasks. However, unlike organizational environments where workers have set work hours, known skill sets and performance indicators that can be monitored and controlled, most crowdsourcing platforms leverage the capabilities of fleeting workers who exhibit changing work patterns, expertise, and quality of work. Consequently, platforms exhibit significant variability in terms of performance characteristics (like response time, accuracy, and completion rate). While this variability has been folklore in the crowdsourcing community, we are the first to show data that displays this kind of changing behavior. Notably, these changes are not due to a distribution with high variance; rather, the distribution itself is changing over time. Deciding which platform is most suitable given the requirements of a task is of critical importance in order to optimize performance; further, making the decision(s) adaptively to accommodate the dynamically changing crowd characteristics is a problem that has largely been ignored. In this paper, we address the changing crowds problem and, specifically, propose a multi-armed bandit based framework. We introduce the simple epsilon-smart algorithm that performs robustly. Counterfactual results based on real-life data from two popular crowd platforms demonstrate the efficacy of the proposed approach. Further simulations using a random-walk model for crowd performance demonstrate its scalability and adaptability to more general scenarios.	Adaptive crowdsourcing for temporal crowds	NA:NA:NA	2018
Hideo Joho:Adam Jatowt:Blanco Roi	Temporal aspects of web search have gained a great level of attention in the recent years. However, many of the research attempts either focused on technical development of various tools or behavioral analysis based on log data. This paper presents the results of user survey carried out to investigate the practice and experience of temporal web search. A total of 110 people was recruited and answered 18 questions regarding their recent experience of web search. Our results suggest that an interplay of seasonal interests, technicality of information needs, target time of information, re-finding behaviour, and freshness of information can be important factors for the application of temporal search. These findings should be complementary to log analyses for further development of temporally aware search engines.	A survey of temporal web search experience	NA:NA:NA	2018
Adam Jatowt:Carlos Castillo:Zoltan Gyongyi:Katsumi Tanaka	WebQuality'13 welcome and organization	Session details: WebQuality'13 workshop	NA:NA:NA:NA	2018
Adam Jatowt	NA	Session details: WEBQUALITY'13 keynote talk	NA	2018
Ricardo Baeza-Yates	Measuring the quality of web content, either at page level or website level, is at the heart of several key challenges in the Web. Without doubt, the main one is web search, to be able to rank results. However, there are other important problems such as web reputation or trust, and web spam detection and filtering. However, measuring intrinsic web quality is a hard problem, because of our limited (automatic) understanding of text semantics, which is even worse for other media. Hence, similarly to human trust assessing, where we use past actions, face expressions, body language, etc; in the Web we need to use indirect signals that serve as surrogates for web quality. In this keynote we attempt to present the most important signals as well as new signals that are or can be used to measure quality in the Web. We divide them using the traditional web content, structure, and usage trilogy. We also characterize them according to how easy is to measure these signals, who can measure them, and how well they scale to the whole Web.	Measuring web quality	NA	2018
Adam Jatowt	NA	Session details: WEBQUALITY'13 web content quality session	NA	2018
Xin Liu:Radoslaw Nielek:Adam Wierzbicki:Karl Aberer	Unlike traditional media such as television and newspapers, web contents are relatively easy to be published without being rigorously fact-checked. This seriously influences people's daily life if non-credible web contents are utilized for decision making. Recently, web credibility evaluation systems have emerged where web credibility is derived by aggregating ratings from the community (e.g., MyWOT). In this paper, We focus on the robustness of such systems by identifying a new type of attack scenario where an attacker imitates the behavior of trustworthy experts by copying system's credibility ratings to quickly build high reputation and then attack certain web contents. In order to defend this attack, we propose a two-stage defence algorithm. At stage 1, our algorithm applies supervised learning algorithm to predict the credibility of a web content and compare it with a user's rating to estimate whether this user is malicious or not. In case the user's maliciousness can not be determined with high confidence, the algorithm goes to stage 2 where we investigate users' past rating patterns and detect the malicious one by applying hierarchical clustering algorithm. Evaluation using real datasets demonstrates the efficacy of our approach.	Defending imitating attacks in web credibility evaluation systems	NA:NA:NA:NA	2018
Jarutas Pattanaphanchai:Kieron O'Hara:Wendy Hall	Assessing the quality of information on the Web is a challenging issue for at least two reasons. First, as a decentralized data publishing platform in which anyone can share nearly anything, the Web has no inherent quality control mechanisms to ensure that content published is valid, legitimate, or even just interesting. Second, when assessing the trustworthiness of web pages, users tend to base their judgments upon descriptive criteria such as the visual presentation of the website rather than more robust normative criteria such as the author's reputation and the source's review process. As a result, Web users are liable to make incorrect assessments, particularly when making quick judgments on a large scale. Therefore, Web users need credibility criteria and tools to help them assess the trustworthiness of Web information in order to place trust in it. In this paper, we investigate the criteria that can be used to collect supportive data about a piece of information in order to improve a person's ability to quickly judge the trustworthiness of the information. We propose the normative trustworthiness criteria namely, authority, currency, accuracy and relevance which can be used to support users' assessments of the trustworthiness of Web information. In addition, we validate these criteria using an expert panel. The results show that the proposed criteria are helpful. Moreover, we obtain weighting scores for criteria which can be used to calculate the trustworthiness of information and suggest a piece of information that is more likely to be trustworthy to Web users.	Trustworthiness criteria for supporting users to assess the credibility of web information	NA:NA:NA	2018
Michał Kąkol:Michał Jankowski-Lorek:Katarzyna Abramczuk:Adam Wierzbicki:Michele Catasta	In this paper we describe the initial outcomes of the Reconcile1 study concerning Web content credibility evaluations. The study was run with a balanced sample of 1503 respondents who independently evaluated 154 web pages from several thematic categories. Users taking part in the study not only evaluated credibility, but also filled a questionnaire covering additional respondents' traits. Using the gathered information about socio-economic status and psychological features of the users, we studied the influence of subjectivity and bias in the credibility ratings. Subjectivity and bias, in fact, represent a key design issue for Web Credibility systems, to the extent that they could jeopardize the system performance if not taken into account. We found out that evaluations of Web content credibility are slightly subjective. On the other hand, the evaluations exhibit a strong acquiescence bias.	On the subjectivity and bias of web content credibility evaluations	NA:NA:NA:NA:NA	2018
Zoltan Gyongyi	NA	Session details: WEBQUALITY'13 industry experience session	NA	2018
Sergey Pevtsov:Sergey Volkov	Web spam has a negative impact on the search quality and users' satisfaction and forces search engines to waste resources to crawl, index, and rank it. Thus search engines are compelled to make significant efforts in order to fight web spam. Traffic from search engines plays a great role in online economics. It causes a tough competition for high positions in search results and increases the motivation of spammers to invent new spam techniques. At the same time, ranking algorithms become more complicated, as well as web spam detection methods. So, web spam constantly evolves which makes the problem of web spam detection always relevant and challenging. As the most popular search engine in Russia Yandex faces the problem of web spam and has some expertise in this matter. This article describes our experience in detection different types of web spam based on content, links, clicks, and user behavior. We also review aggressive advertising and fraud because they affect the user experience. Besides, we demonstrate the connection between classic web spam and modern social engineering approaches in fraud.	Russian web spam evolution: yandex experience	NA:NA	2018
Andrei Venzhega:Polina Zhinalieva:Nikolay Suboch	Search engines are currently facing a problem of websites that distribute malware. In this paper we present a novel efficient algorithm that learns to detect such kind of spam. We have used a bipartite graph with two types of nodes, each representing a layer in the graph: web-sites and file hostings (FH), connected with edges representing the fact that a file can be downloaded from the hosting via a link on the web-site. The performance of this spam detection method was verified using two set of ground truth labels: manual assessments of antivirus analysts and automatically generated assessments obtained from antivirus companies. We demonstrate that the proposed method is able to detect new types of malware even before the best known antivirus solutions are able to detect them.	Graph-based malware distributors detection	NA:NA:NA	2018
Alexander Shishkin:Polina Zhinalieva:Kirill Nikolaev	Modern search engines are good enough to answer popular commercial queries with mainly highly relevant documents. However, our experiments show that users behavior on such relevant commercial sites may differ from one to another web-site with the same relevance label. Thus search engines face the challenge of ranking results that are equally relevant from the perspective of the traditional relevance grading approach. To solve this problem we propose to consider additional facets of relevance, such as trustability, usability, design quality and the quality of service. In order to let a ranking algorithm take these facets in account, we proposed a number of features, capturing the quality of a web page along the proposed dimensions. We aggregated new facets into the single label, commercial relevance, that represents cumulative quality of the site. We extrapolated commercial relevance labels for the entire learning-to-rank dataset and used weighted sum of commercial and topical relevance instead of default relevance labels. For evaluating our method we created new DCG-like metrics and conducted off-line evaluation as well as on-line interleaving experiments demonstrating that a ranking algorithm taking the proposed facets of relevance into account is better aligned with user preferences.	Quality-biased ranking for queries with commercial intent	NA:NA:NA	2018
Carlos Castillo	NA	Session details: WEBQUALITY'13 web spam detection session	NA	2018
András Garzó:Bálint Daróczy:Tamás Kiss:Dávid Siklósi:András A. Benczúr	While Web spam training data exists in English, we face an expensive human labeling procedure if we want to filter a Web domain in a different language. In this paper we overview how existing content and link based classification techniques work, how models can be "translated" from English into another language, and how language-dependent and independent methods combine. In particular we show that simple bag-of-words translation works very well and in this procedure we may also rely on mixed language Web hosts, i.e. those that contain an English translation of part of the local language text. Our experiments are conducted on the ClueWeb09 corpus as the training English collection and a large Portuguese crawl of the Portuguese Web Archive. To foster further research, we provide labels and precomputed values of term frequencies, content and link based features for both ClueWeb09 and the Portuguese data.	Cross-lingual web spam classification	NA:NA:NA:NA:NA	2018
Yoshihiko Suhara:Hiroyuki Toda:Shuichi Nishioka:Seiji Susaki	Spammers use a wide range of content generation techniques with low quality pages known as content spam to achieve their goals. We argue that content spam must be tackled using a wide range of content quality features. In this paper, we propose novel sentence-level diversity features based on the probabilistic topic model. We combine them with other content features to build a content spam classifier. Our experiments show that our method outperforms the conventional methods.	Automatically generated spam detection based on sentence-level topic information	NA:NA:NA:NA	2018
Rajendra Akerkar:Pierre Maret:Laurent Vercouter	WI&C'13 welcome and organization	Session details: WI&C'13 workshop	NA:NA:NA	2018
Pierre Maret	NA	Session details: WI&C'13 keynote talk	NA	2018
Virgilio Almeida	The explosion in the volume of digital data currently available in social networks has created new opportunities for scientific discoveries in the realm of social media. In particular, I show our recent progress in user preference understanding, data mining, summarization and explorative analysis of very large data sets. In information networks where users send messages to one another, the issue of information overload naturally arises: which are the most important messages? Based on a very large dataset with more 54 million user accounts and with all tweets ever posted by the collected users - more than 1.7 billion tweets, I discuss the problem of understanding the importance of messages in Twitter. In another work based on large-scale crawls of over 27 million user profiles that represented nearly 50% of the entire network in 2011, I show a detailed analysis of the Google+ social network. I discuss the key differences and similarities with other popular networks like Facebook and Twitter, in order to determine whether Google+ is a new paradigm or yet another social network.	Exploring very large data sets from online social networks	NA	2018
Pierre Maret	NA	Session details: WI&C'13 session 1	NA	2018
Suhas Aggarwal	In this paper, we discuss animated captcha systems which can be very useful for advertising. They are hardly any Animated Advertisement Captchas, available these days which are more secure than single image based Captchas and more fun as well. Some solutions are available such as yo!Captcha, NLPcaptcha. Solve Media TYPE-IN Captchas. These Captchas are single image based Captchas which ask users to type in Brand message to solve Captcha for brand recall. In this paper, we discuss some more appealing media which can be used for Captcha advertising. We also present Interactive Environment/Game Captcha which provide a more powerful medium for advertising. Finally, we showcase a Game with a purpose, named 'Pick Brands' which promote advertising and and can be used to obtain feedback/reviews, collect user questions concerning products/Advertisements.	Animated CAPTCHAs and games for advertising	NA	2018
Jonathan Bell:Swapneel Sheth:Gail Kaiser	We present a survey of usage of the popular Massively Multiplayer Online Role Playing Game, World of Warcraft. Players within this game often self-organize into communities with similar interests and/or styles of play. By mining publicly available data, we collected a dataset consisting of the complete player history for approximately six million characters, with partial data for another six million characters. The paper provides a thorough description of the distributed approach used to collect this massive community data set, and then focuses on an analysis of player achievement data in particular, exposing trends in play from this highly successful game. From this data, we present several findings regarding player profiles. We correlate achievements with motivations based upon a previously-defined motivation model, and then classify players based on the categories of achievements that they pursued. Experiments show players who fall within each of these buckets can play differently, and that as players progress through game content, their play style evolves as well.	A large-scale, longitudinal study of user profiles in world of warcraft	NA:NA:NA	2018
Nataliia Pobiedina:Julia Neidhardt:Maria del Carmen Calatrava Moreno:Hannes Werthner	As an increasing number of human activities are moving to the Web, more and more teams are predominantly virtual. Therefore, formation and success of virtual teams is an important issue in a wide range of fields. In this paper we model social behavior patterns of team work using data from virtual communities. In particular, we use data about the Web community of the multiplayer online game Dota 2 to study cooperation within teams. By applying statistical analysis we investigate how and to which extent different factors of the team in the game, such as role distribution, experience, number of friends and national diversity, have an influence on the team's success. In order to complete the picture we also rank the factors according to their influence. The results of our study imply that cooperation within the team is better than competition.	Ranking factors of team success	NA:NA:NA:NA	2018
Pierre Maret	NA	Session details: WI&C'13 session 2	NA	2018
Saulo D.S. Pedro:Ana Paula Appel:Estevam R. Hruschka, Jr.	The amount of information available on the Web has been increasing daily. However, how one might know what is right or wrong? Does the Web itself can be used as a source for verification of information? NELL (Never-Ending Language Learner) is a computer system that gathers knowledge from Web. Prophet is a link prediction component on NELL that has been successfully used to help populate its knowledge database. However, during link prediction task performance Prophet classify some edges as misplaced edges, that is, edges that we can not assure if they are right or not. In this paper we use the Web itself, using question answer (QA) systems, as a Prophet extension to validate these edges. This is an important issue when working with a self-supervised system where inserted errors might be propagate and generate dangerous concept drifting.	Autonomously reviewing and validating the knowledge base of a never-ending learning system	NA:NA:NA	2018
Juwel Rana:Sarwar Morshed:Kåre Synnes	This paper presents a social component framework for the SatinII App Development Environment. The environment provides a systematic way of designing, developing and deploying personalized apps and enables end-users to develop their own apps without requiring prior knowledge of programming. A wide range of social components based on the framework have been deployed in the SatinII Editor, including components that utilize aggregated social graphs to automatically create groups or recommending/filtering information. The resulting social apps are web-based and target primarily mobile clients such as smartphones. The paper also presents a classification of social components and provides an initial user-evaluation with a small group of users. Initial results indicate that social apps can be built and deployed by end-users within 17 minutes on average after 20 to 30 minutes of being introduced to the SatinII Editor.	End-user creation of social apps by utilizing web-based social components and visual app composition	NA:NA:NA	2018
Mohamed Nader Jelassi:Sadok Ben Yahia:Engelbert Mephu Nguifo	Thanks to the high popularity and simplicity of folksonomies, many users tend to share objects (movies, songs, bookmarks, etc.) by annotating them with a set of tags of their own choice. Users represent the core of the system since they are both the contributors and the creators of the information. Yet, each user has its own profile and its own ideas making thereby the strength as well as the weakness of folksonomies. Indeed, it would be helpful to take account of users' profile when suggesting a list of tags and resources or even a list of friends, in order to make a more personal recommandation. The goal is to suggest tags (or resources) which may correspond to a user's vocabulary or interests rather than a list of most used and popular tags in folksonomies. In this paper, we consider users' profile as a new dimension of a folksonomy classically composed of three dimensions "users, tags, ressources" and we propose an approach to group users with equivalent profiles and equivalent interests as quadratic concepts. Then, we use quadratic concepts in order to propose our personalized recommendation system of users, tags and resources according to each user's profile. Carried out experiments on the large-scale real-world filmography dataset MovieLens highlight encouraging results in terms of precision.	A personalized recommender system based on users' information in folksonomies	NA:NA:NA	2018
Pablo Mendes:Giuseppe Rizzo:Eric Charton	WOLE'13 welcome and organization	Session details: WOLE'13 workshop	NA:NA:NA	2018
Peter Mika	More than the half of queries in the logs of a web search engine refer directly to a single named entity or a named set of entities [1]. To support entity search queries, search engines have begun developing targeted functionality, such as rich displays of factual information, question-answering and related entity recommendations. In this talk, we will provide an overview of recent work in the field of entity search, illustrated by the example of the Spark system, a large-scale system currently in use at Yahoo! for related entity recommendations in web search. Spark combines various knowledge bases and collects evidence from query logs and social media to provide the most relevant related entities for every web query with an entity intent. We discuss the methods used in Spark as well as how the system is evaluated in daily use.	Entity search on the web	NA	2018
Neil Anderson:Jun Hong	Web sites that rely on databases for their content are now ubiquitous. Query result pages are dynamically generated from these databases in response to user-submitted queries. Automatically extracting structured data from query result pages is a challenging problem, as the structure of the data is not explicitly represented. While humans have shown good intuition in visually understanding data records on a query result page as displayed by a web browser, no existing approach to data record extraction has made full use of this intuition. We propose a novel approach, in which we make use of the common sources of evidence that humans use to understand data records on a displayed query result page. These include structural regularity, and visual and content similarity between data records displayed on a query result page. Based on these observations we propose new techniques that can identify each data record individually, while ignoring noise items, such as navigation bars and adverts. We have implemented these techniques in a software prototype, rExtractor, and tested it using two datasets. Our experimental results show that our approach achieves significantly higher accuracy than previous approaches. Furthermore, it establishes the case for use of vision-based algorithms in the context of data extraction from web sites.	Visually extracting data records from the deep web	NA:NA	2018
Michel Gagnon:Amal Zouaq:Ludovic Jean-Louis	Semantic annotation is the process of identifying expressions in texts and linking them to some semantic structure. In particular, Linked data-based Semantic Annotators are now becoming the new Holy Grail for meaning extraction from unstructured documents. This paper presents an evaluation of the main linked data-based annotators available with a focus on domain topics and named entities. In particular, we compare the ability of each tool to annotate relevant domain expressions in text. The paper also proposes a combination of annotators through voting methods and machine learning. Our results show that some linked-data annotators, especially Alchemy, can be considered as a useful resource for topic extraction. They also show that a substantial increase in recall can be achieved by combining the annotators with a weighted voting scheme. Finally, an interesting result is that by removing Alchemy from the combination, or by combining only the more precise annotators, we get a significant increase in precision, at the cost of a lower recall.	Can we use linked data semantic annotators for the extraction of domain-relevant expressions?	NA:NA:NA	2018
Neel Guha:Matt Wytock	Web search is an important research tool for many high school courses. However, generic search engines have a number of problems that arise out of not understanding the context of search (the high school course), leading to results that are off-topic or inappropriate as reference material. In this paper, we introduce the concept of a course-specific search engine and build such a search engine for the Advanced Placement US History (APUSH) course; the results of which are preferred by subject matter experts (high school teachers) over existing search engines. This reference search engine for APUSH relies on a hand-curated set of sites picked specifically for this educational context. In order to automate this expensive process, we describe two algorithms for indentifying high quality topical sites using an authoritative source such as a textbook: one based on textual similarity and another using structured data from knowledge bases. Initial experimental results indicate that these algorithms can successfully classify high quality documents leading to the automatic creation of topic-specific corpora for any course.	Course-specific search engines: semi-automated methods for identifying high quality topic-specific corpora	NA:NA	2018
Bernhard Haslhofer:Flávio Martins:João Magalhães	Knowledge organization systems such as thesauri or taxonomies are increasingly being expressed using the Simple Knowledge Organization System (SKOS) and published as structured data on the Web. Search engines can exploit these vocabularies and improve search by expanding terms at query or document indexing time. We propose a SKOS-based term expansion and scoring technique that leverages labels and semantic relationships of SKOS concept definitions. We also implemented this technique for Apache Lucene and Solr. Experiments with the Medical Subject Headings vocabulary and an early evaluation with Library of Congress Subject Headings indicated gains in precision when using SKOS-based expansion compared to pseudo relevance feedback and no expansion. Our findings are important for publishers and consumer of Web vocabularies who want to use them for improving search over Web documents.	Using SKOS vocabularies for improving web search	NA:NA:NA	2018
Paridhi Jain:Ponnurangam Kumaraguru:Anupam Joshi	An online user joins multiple social networks in order to enjoy different services. On each joined social network, she creates an identity and constitutes its three major dimensions namely profile, content and connection network. She largely governs her identity formulation on any social network and therefore can manipulate multiple aspects of it. With no global identifier to mark her presence uniquely in the online domain, her online identities remain unlinked, isolated and difficult to search. Literature has proposed identity search methods on the basis of profile attributes, but has left the other identity dimensions e.g. content and network, unexplored. In this work, we introduce two novel identity search algorithms based on content and network attributes and improve on traditional identity search algorithm based on profile attributes of a user. We apply proposed identity search algorithms to find a user's identity on Facebook, given her identity on Twitter. We report that a combination of proposed identity search algorithms found Facebook identity for 39% of Twitter users searched while traditional method based on profile attributes found Facebook identity for only 27.4%. Each proposed identity search algorithm access publicly accessible attributes of a user on any social network. We deploy an identity resolution system, Finding Nemo, which uses proposed identity search methods to find a Twitter user's identity on Facebook. We conclude that inclusion of more than one identity search algorithm, each exploiting distinct dimensional attributes of an identity, helps in improving the accuracy of an identity resolution process.	@i seek 'fb.me': identifying users across multiple online social networks	NA:NA:NA	2018
Matthias Keller:Patrick Mühlschlegel:Hannes Hartenstein	As a result of additional semantic annotations and novel mining methods, Web site taxonomies are more and more available to machines, including search engines. Recent research shows that after a search result is clicked, users often continue navigating on the destination site because in many cases a single document cannot satisfy the information need. The role Web site taxonomies play in this post-search navigation phase has not yet been researched. In this paper we analyze in an empirical study of three highly-frequented Web sites how Web site taxonomies influence the next browsing steps of users arriving from a search engine. The study reveals that users not randomly explore the destination site, but proceed to the direct child nodes of the landing page with significantly higher frequency compared to the other linked pages. We conclude that the common post-search navigation strategy in taxonomies is to descend towards more specific results. The study has interesting implications for the presentation of search results. Current search engines focus on summarizing the linked document only. In doing so, search engines ignore the fact the linked documents are in many cases just the starting point for further navigation. Based on the observed post-search navigation strategy, we propose to include information about child nodes of linked documents in the presentation of search results. Users would benefit by saving clicks, because they could not only estimate whether the linked document provides useful information, but also whether post-search navigation is promising.	Search result presentation: supporting post-search navigation by integration of taxonomy data	NA:NA:NA	2018
Elizabeth L. Murnane:Bernhard Haslhofer:Carl Lagoze	We address the Named Entity Disambiguation (NED) problem for short, user-generated texts on the social Web. In such settings, the lack of linguistic features and sparse lexical context result in a high degree of ambiguity and sharp performance drops of nearly 50% in the accuracy of conventional NED systems. We handle these challenges by developing a model of user-interest with respect to a personal knowledge context; and Wikipedia, a particularly well-established and reliable knowledge base, is used to instantiate the procedure. We conduct systematic evaluations using individuals' posts from Twitter, YouTube, and Flickr and demonstrate that our novel technique is able to achieve substantial performance gains beyond state-of-the-art NED methods.	RESLVE: leveraging user interest to improve entity disambiguation on short text	NA:NA:NA	2018
Salvatore Orlando:Francesco Pizzolon:Gabriele Tolomei	Everyday people are exchanging a huge amount of data through the Internet. Mostly, such data consist of unstructured texts, which often contain references to structured information (e.g., person names, contact records, etc.). In this work, we propose a novel solution to discover social events from actual press news edited by humans. Concretely, our method is divided in two steps, each one addressing a specific Information Extraction (IE) task: first, we use a technique to automatically recognize four classes of named-entities from press news: DATE, LOCATION, PLACE, and ARTIST. Furthermore, we detect social events by extracting ternary relations between such entities, also exploiting evidence from external sources (i.e., the Web). Finally, we evaluate both stages of our proposed solution on a real-world dataset. Experimental results highlight the quality of our first-step Named-Entity Recognition (NER) approach, which indeed performs consistently with state-of-the-art solutions. Eventually, we show how to precisely select true events from the list of all candidate events (i.e., all the ternary relations), which result from our second-step Relation Extraction (RE) method. Indeed, we discover that true social events can be detected if enough evidence of those is found in the result list of Web search engines.	SEED: a framework for extracting social events from press news	NA:NA:NA	2018
Vincent Simonet	This paper presents a framework for categorizing channels of videos in a thematic taxonomy with high precision and coverage. The proposed approach consists of three main steps.First, videos are annotated by semantic entities describing their central topics. Second, semantic entities are mapped to categories using a combination of classifiers.Last, the categorization of channels is obtained by combining the results of both previous steps. This framework has been deployed on the whole corpus of YouTube, in 8 languages, and used to build several user facing products. Beyond the description of the framework, this paper gives insight into practical aspects and experience: rationale from product requirements to the choice of the solution, spam filtering, human-based evaluations of the quality of the results, and measured metrics on the live site.	Classifying YouTube channels: a practical system	NA	2018
David De Roure:Wolfgang Nejdl	WOW'13 welcome and organization	Session details: WOW'13 workshop	NA:NA	2018
Paul Booth:Paul Gaskell:Chris Hughes	The aim of this paper is to present a requirement for assessing the quality of data and the development of efficient methods of valuing and exchanging data among Web Observatories. Using economic and business theory a range of concepts are explored which include a brief review of existing business structures related to the exchange of goods, data or otherwise. The paper calls for a wider discussion by the Web Observatory community to begin to define relevant criteria by which data can be assessed and improved over time. The economic incentives are addressed as part of a price by proxy framework we introduce, which is supported by the need to strive for clear pricing signals and the reduction of information asymmetries. What is presented here is a way of establishing and improving data quality with a view to valuing data exchanges that does not require the presence of money in the transaction, yet it remains tied to revenue generation models as they exist online.	The economics of data: quality, value & exchange in web observatories	NA:NA:NA	2018
Ian Brown:Wendy Hall:Lisa Harris	In this paper, we propose a set of concepts underlying the process and requirements of observation: that is, the process of employing web observatories for research. We refer to observation as a new concept, distinct from search, which we believe is worthy of study in its own right and note that the process of observation moves the focus of information retrieval away from universal coverage and towards improved quality of results and thus has many potential facets not necessarily present in traditional search.	From search to observation	NA:NA:NA	2018
Ernesto Diaz-Aviles	The collective effervescence of social media production has been enjoying a great deal of success in recent years. The hundred of millions of users who are actively participating in the Social Web are exposed to ever-growing amounts of sites, relationships, and information. In this paper, we report part of the efforts towards the realization of a Web Observatory at the L3S Research Center (www.L3S.de). In particular, we present our approach based on Living Analytics methods, whose main goal is to capture people interactions in real-time and to analyze multidimensional relationships, metadata, and other data becoming ubiquitous in the social web, in order to discover the most relevant and attractive information to support observation, understanding and analysis of the Web. We center the discussion on two areas: (i) Recommender Systems for Big Fast Data and (ii) Collective Intelligence, both key components towards an analytics toolbox for our Web Observatory.	Living analytics methods for the web observatory	NA	2018
Marie Joan Kristine Gloria:Deborah L. McGuinness:Joanne S. Luciano:Qingpeng Zhang	The following contribution highlights selected work conducted by Rensselaer Polytechnic Institute's Web Science Research Center. (RPI WSRC). Specifically, it brings to light four different themed Web Observatories - Science Data, Health and Life Sciences, Open Government, and Social Spaces. Each of these observatories serves as a repository of data, tools, and methods that help answer complicated questions in each of these research areas. We present six case studies featuring tools and methods developed by RPI WSRC to aide in the exploration, discovery, and analysis of large data sets. These case studies along with our web observatory developments are aimed to increase our understanding of web science in general and to serve as test beds for our research.	Exploration in web science: instruments for web observatories	NA:NA:NA:NA	2018
Ramesh Jain:Laleh Jalali:Mingming Fan	In this position paper, we propose an approach for Web Observatories that builds on using social media, personal data, and sensors to build Persona for an individual, but also use this data and the concept of Focused Micro Blogs (FMB) for situation detection, helping individual using situation action rules, and finally gaining insights for obtaining insights about society. We demonstrate this in a concrete use case of fitness and health care related sensors for building health persona and using this for understanding societal health issues.	From health-persona to societal health	NA:NA:NA	2018
Nattiya Kanhabua:Wolfgang Nejdl	A microblogging service like Twitter continues to surge in importance as a means of sharing information in social networks. In the medical domain, several works have shown the potential of detecting public health events (i.e., infectious disease outbreaks) using Twitter messages or tweets. Given its real-time nature, Twitter can enhance early outbreak warning for public health authorities in order that a rapid response can take place. Most of previous works on detecting outbreaks in Twitter simply analyze tweets matched disease names and/or locations of interests. However, the effectiveness of such method is limited for two main reasons. First, disease names are highly ambiguous, i.e., referring slangs or non health-related contexts. Second, the characteristics of infectious diseases are highly dynamic in time and place, namely, strongly time-dependent and vary greatly among different regions. In this paper, we propose to analyze the temporal diversity of tweets during the known periods of real-world outbreaks in order to gain insight into a temporary focus on specific events. More precisely, our objective is to understand whether the temporal diversity of tweets can be used as indicators of outbreak events, and to which extent. We employ an efficient algorithm based on sampling to compute the diversity statistics of tweets at particular time. To this end, we conduct experiments by correlating temporal diversity with the estimated event magnitude of 14 real-world outbreak events manually created as ground truth. Our analysis shows that correlation results are diverse among different outbreaks, which can reflect the characteristics (severity and duration) of outbreaks.	Understanding the diversity of tweets in the time of outbreaks	NA:NA	2018
Jérôme Kunegis	We present the Koblenz Network Collection (KONECT), a project to collect network datasets in the areas of web science, network science and related areas, as well as provide tools for their analysis. In the cited areas, a surprisingly large number of very heterogeneous data can be modeled as networks and consequently, a unified representation of networks can be used to gain insight into many kinds of problems. Due to the emergence of the World Wide Web in the last decades many such datasets are now openly available. The KONECT project thus has the goal of collecting many diverse network datasets from the Web, and providing a way for their systematic study. The main parts of KONECT are (1) a collection of over 160 network datasets, consisting of directed, undirected, unipartite, bipartite, weighted, unweighted, signed and temporal networks collected from the Web, (2) a Matlab toolbox for network analysis and (3) a website giving a compact overview the various computed statistics and plots. In this paper, we describe KONECT's taxonomy of networks datasets, give an overview of the datasets included, review the supported statistics and plots, and briefly discuss KONECT's role in the area of web science and network science.	KONECT: the Koblenz network collection	NA	2018
Karissa McKelvey:Filippo Menczer	The broad adoption of online social networking platforms has made it possible to study communication networks at an unprecedented scale. With social media and micro-blogging platforms such as Twitter, we can observe high-volume data streams of online discourse. However, it is a challenge to collect, manage, analyze, visualize, and deliver large amounts of data, even by experts in the computational sciences. In this paper, we describe our recent extensions to Truthy, a social media observatory that collects and analyzes discourse on Twitter dating from August 2010. We introduce several interactive visualizations and analytical tools with the goal of enabling researchers to study online social networks with mixed methods at multiple scales. We present design considerations and a prototype for integrating social media observatories as important components of a web observatory framework.	Design and prototyping of a social media observatory	NA:NA	2018
Siripen Pongpaichet:Vivek K. Singh:Mingyan Gao:Ramesh Jain	Web Observatories must address fundamental societal challenges using enormous volumes of data being created due to the significant progress in technology. The proliferation of heterogeneous data streams generated by social media, sensor networks, internet of things, and digitalization of transactions in all aspect of humans? life presents an opportunity to establish a new era of networks called Social Life Networks (SLN). The main goal of SLN is to connect People to Resources effectively, efficiently, and promptly in given Situations. Towards this goal, we present a computing framework, called EventShop, to recognize evolving situations from massive web streams in real-time. These web streams can be fundamentally considered as spatio-temporal-thematic streams and can be combined using a set of generic spatio-temporal analysis operators to recognize evolving situations. Based on the detected situations, the relevant information and alerts can be provided to both individuals and organizations. Several examples from the real world problems have been developed to test the efficacy of EventShop framework.	EventShop: recognizing situations in web data streams	NA:NA:NA:NA	2018
A. Patrice Seyed:Tim Lebo:Evan Patton:Jim McCusker:Deborah McGuinness	A web observatory for empirical research of Web data benefits from software frameworks that are modular, has a clear underlying semantic model, and that includes metadata enabling a trace and inspection of the source data and justifications for derived datasets. We present SemantEco as an architecture that can serve as an exemplar abstraction for infrastructure design and metadata based on best practices in Semantic Web, Provenance, and Software Engineering, that can be employed in any Web Observatory, that may grow out of a community. We will describe how the SemantEco framework allows for searching, visualizing, and tracing a wide variety of data.	SemantEco: a next-generation web observatory	NA:NA:NA:NA:NA	2018
Ramine Tinati:Thanassis Tiropanis:Lesie Carr	Wikipedia has grown to become the most successful online encyclopedia on the Web, containing over 24 million articles, offered in over 240 languages. In just over 10 years Wikipedia has transformed from being just an encyclopedia of knowledge, to a wealth of facts and information, from articles discussing trivia, political issues, geographies and demographics, to popular culture, news articles, and social events. In this paper we explore the use of Wikipedia for identifying the flow of information and trends across the world. We start with the hypothesis that, given that Wikipedia is a resource that is globally available in different languages across countries, access to its articles could be a reflection human activity. To explore this hypothesis we try to establish metrics on the use of Wikipedia in order to identify potential trends and to establish whether or how those trends flow from one county to another. We subsequently compare the outcome of this analysis to that of more established methods that are based on online social media or traditional media. We explore this hypothesis by applying our approach to a subset of Wikipedia articles and also a specific worldwide social phenomenon that occurred during 2012; we investigate whether access to relevant Wikipedia articles correlates to the viral success of the South Korean pop song, "Gangnam Style" and the associated artist "PSY" as evidenced by traditional and online social media. Our analysis demonstrates that Wikipedia can indeed provide a useful measure for detecting social trends and events, and in the case that we studied; it could have been possible to identify the specific trend quicker in comparison to other established trend identification services such as Google Trends.	An approach for using Wikipedia to measure the flow of trends across countries	NA:NA:NA	2018
Ionut Trestian:Chunjing Xiao:Aleksandar Kuzmanovic	Although according to surveys related to internet user activity it is considered one of the most popular aspects, few studies are actually concerned with internet pornography. This paper is aimed at rectifying that overlook. In particular, we study user activity related to internet pornography by looking at two main behaviors: (i) watching pornography, and (ii) providing feedback on pornography items in the form of ratings and comments. By using appropriate datasets that we collect, we make contributions related to the study of both behaviors pointed out above. With regards to viewing, we observe that views are highly dependent on pornography category and video size. By studying the feedback system of pornography video websites, we observe differences in the way users rate items across websites popular in different parts of the world. Finally, we employ sentiment analysis to study the comments that users leave on pornography websites and we find surprising similarities across the analyzed websites. Our results pave the way to understanding more about human behavior related to internet pornography and can impact, among others, fields such as content personalization, video content delivery, recommender systems	A glance at an overlooked part of the world wide web	NA:NA:NA	2018
Rosa Alarcon:Cesare Pautasso:Erik Wilde	WS-REST'13 welcome and organization	Session details: WS-REST'13 workshop	NA:NA:NA	2018
Markus Gulden:Stefan Kugele	Today, innovative companies are forced to evolve their software systems faster and faster, either for providing customer services and products or for supporting internal processes. At the same time, already existing, maybe even legacy systems are crucial for different reasons and by that cannot be abolished easily. While integrating legacy software into new systems in general is considered by well-known approaches like SOA (service-oriented architecture), at the best of our knowledge, it lacks of ways to make legacy systems available for remote clients like smart phones or embedded devices. In this paper, we propose an approach to leverage heterogeneous (legacy) applications by adding RESTful web-based interfaces in a model-driven way. We introduce an additional application layer, which encapsulates services of one or several existing applications, and provides a unified, web-based, and seamless interface. This interface is modelled in our own DSL (domain-specific language), the belonging code generator produces productive Java code. Finally, we report on an case study proving our concept by means of an e-bike sharing service.	A concept for generating simplified RESTful interfaces	NA:NA	2018
Ruben Verborgh:Michael Hausenblas:Thomas Steiner:Erik Mannens:Rik Van de Walle	Hypermedia links and controls drive the Web by transforming information into affordances through which users can choose actions. However, publishers of information cannot predict all actions their users might want to perform and therefore, hypermedia can only serve as the engine of application state to the extent the user's intentions align with those envisioned by the publisher. In this paper, we introduce distributed affordance, a concept and architecture that extends application state to the entire Web. It combines information inside the representation with knowledge of action providers to generate affordance from the user's perspective. Unlike similar approaches such as Web Intents, distributed affordance scales both in the number of actions and the number of action providers, because it is resource-oriented instead of action-oriented. A proof-of-concept shows that distributed affordance is a feasible strategy on today's Web.	Distributed affordance: an open-world assumption for hypermedia	NA:NA:NA:NA:NA	2018
Luca Panziera:Flavio De Paoli	REST principles define services as resources that can be manipulated by a set of well-known methods. The same approach is suitable to define service descriptions as resources. In this paper, we try to unify the two concepts (services and their descriptions) by proposing a set of best practices to build self-descriptive RESTful services accessible by both humans and machines. Moreover, to make those practices usable with little manual effort, we provide a software framework that extracts compliant descriptions from documents published on the Web, and makes them available to clients as resources.	A framework for self-descriptive RESTful services	NA:NA	2018
Fred Douglis	NA	Session details: Performance	NA	2018
Anoop Ninan:Purushottam Kulkarni:Prashant Shenoy:Krithi Ramamritham:Renu Tewari	In this paper, we argue that cache consistency mechanisms designed for stand-alone proxies do not scale to the large number of proxies in a content distribution network and are not flexible enough to allow consistency guarantees to be tailored to object needs. To meet the twin challenges of scalability and flexibility, we introduce the notion of cooperative consistency along with a mechanism, called cooperative leases, to achieve it. By supporting &Dgr;-consistency semantics and by using a single lease for multiple proxies, cooperative leases allows the notion of leases to be applied in a flexible, scalable manner to CDNs. Further, the approach employs application-level multicast to propagate server notifications to proxies in a scalable manner. We implement our approach in the Apache web server and the Squid proxy cache and demonstrate its efficacy using a detailed experimental evaluation. Our results show a factor of 2.5 reduction in server message overhead and a 20% reduction in server state space overhead when compared to original leases albeit at an increased inter-proxy communication overhead.	Cooperative leases: scalable consistency maintenance in content distribution networks	NA:NA:NA:NA:NA	2018
Marcel-Cǎtǎlin Roşu:Daniela Roşu	This study is the first to evaluate the performance benefits of using the recently proposed TCP Splice kernel service in Web proxy servers. Previous studies show that splicing client and server TCP connections in the IP layer improves the throughput of proxy servers like firewalls and content routers by reducing the data transfer overheads. In a Web proxy server, data transfer overheads represent a relatively large fraction of the request processing overheads, in particular when content is not cacheable or the proxy cache is memory-based. The study is conducted with a socket-level implementation of TCP Splice. Compared to IP-level implementations, socket-level implementations make possible the splicing of connections with different TCP characteristics, and improve response times by reducing recovery delay after a packet loss. The experimental evaluation is focused on HTTP request types for which the proxy can fully exploit the TCP Splice service, which are the requests for non-cacheabl.content and SSL tunneling. The experimental testbed includes an emulated WAN environment and benchmark applications for HTTP/1.0 Web client, Web server, and Web proxy running on AIX RS/6000 machines. Our experiments demonstrate that TCP Splice enables reductions in CPU utilization of 10-43% of the CPU, depending on file sizes and request rates. Larger relative reductions are observed when tunneling SSL connections, in particular for small file transfers. Response times are also reduced by up to 1.8sec.	An evaluation of TCP splice benefits in web proxy servers	NA:NA	2018
Jeffery C. Mogul	The simplicity of HTTP was a major factor in the success of the Web. However, as both the protocol and its uses have evolved, HTTP has grown complex. This complexity results in numerous problems, including confused implementors, interoperability failures, difficulty in extending the protocol, and a long specification without much documented rationale.Many of the problems with HTTP can be traced to unfortunate choices about fundamental definitions and models. This paper analyzes the current (HTTP/1.1) protocol design, showing how it fails in certain cases, and how to improve these fundamentals. Some problems with HTTP can be fixed simply by adopting new models and terminology, allowing us to think more clearly about implementations and extensions. Other problems require explicit (but compatible) protocol changes.	Clarifying the fundamentals of HTTP	NA	2018
Erik Wilde	NA	Session details: Multimedia	NA	2018
Stuart Goose:Sreedhar Kodlahalli:William Pechter:Rune Hjelsvold	While monochrome unformatted text and richly colored graphical content are both capable of conveying a message, well designed graphical content has the potential for better engaging the human sensory system. It is our contention that the author of an audio presentation should be afforded the benefit of judiciously exploiting the human aural perceptual ability to deliver content in a more compelling, concise and realistic manner. While contemporary streaming media players and voice browsers share the ability to render content non-textually, neither technology is currently capable of rendering three dimensional media. The contributions described in this paper are proposed 3D audio extensions to SMIL and a server-based framework able to receive a request and, on-demand, process such a SMIL file and dynamically create the multiple simultaneous audio objects, spatialize them in 3D space, multiplex them into a single stereo audio and prepare it for transmission over an audio stream to a mobile device. To the knowledge of the authors, this is the first reported solution for delivering and rendering on a commercially available wireless handheld device a rich 3D audio listening experience as described by a markup language. Naturally, in addition to mobile devices this solution also works with desktop streaming media players.	Streaming speech3: a framework for generating and streaming 3D text-to-speech and audio presentations to wireless PDAs as specified using extensions to SMIL	NA:NA:NA:NA	2018
Patrick Schmitz	Multimedia scheduling models provide a rich variety of tools for managing the synchronization of media like video and audio, but generally have an inflexible model for time itself. In contrast, modern animation models in the computer graphics community generally lack tools for synchronization and structural time, but allow for a flexible concept of time, including variable pacing, acceleration and deceleration and other tools useful for controlling and adapting animation behaviors. Multimedia authors have been forced to choose one set of features over the others, limiting the range of presentations they can create. Some programming models addressed some of these problems, but provided no declarative means for authors and authoring tools to leverage the functionality. This paper describes a new model incorporated into SMIL 2.0 that combines the strengths of scheduling models with the flexible time manipulations of animation models. The implications of this integration are discussed with respect to scheduling and structured time, drawing upon experience with SMIL 2.0 timing and synchronization, and the integration with XHTML.	Multimedia meets computer graphics in SMIL2.0: a time model for the web	NA	2018
Jun Yang:Qing Li:Yueting Zhuang	An important trend in Web information processing is the support of multimedia retrieval. However, the most prevailing paradigm for multimedia retrieval, content-based retrieval (CBR), is a rather conservative one whose performance depends on a set of specifically defined low-level features and a carefully chosen sample object. In this paper, an aggressive search mechanism called Octopus is proposed which addresses the retrieval of multi-modality data using multifaceted knowledge. In particular, Octopus promotes a novel scenario in which the user supplies seed objects of arbitrary modality as the hint of his information need, and receives a set of multi-modality objects satisfying his need. The foundation of Octopus is a multifaceted knowledge base constructed on a layered graph model (LGM), which describes the relevance between media objects from various perspectives. Link analysis based retrieval algorithm is proposed based on the LGM. A unique relevance feedback technique is developed to update the knowledge base by learning from user behaviors, and to enhance the retrieval performance in a progressive manner. A prototype implementing the proposed approach has been developed to demonstrate its feasibility and capability through illustrative examples.	OCTOPUS: aggressive search of multi-modality data using multifaceted knowledge base	NA:NA:NA	2018
Steffen Staab	NA	Session details: Semantic Web Services	NA	2018
Daniela Florescu:Andreas Grünhagen:Donald Kossmann	We present an XML programming language specially designed for the implementation of Web services. XL is portable and fully compliant with W3C standards such as XQuery, XML Protocol, and XML Schema. One of the key features of XL is that it allows programmers to concentrate on the logic of their application. XL provides high-level and declarative constructs for actions which are typically carried out in the implementation of a Web service; e.g., logging, error handling, retry of actions, workload management, events, etc. Issues such as performance tuning (e.g., caching, horizontal partitioning, etc.) should be carried out automatically by an implementation of the language. This way, the productivity of the programmers, the ability of evolution of the programs, and the chances to achieve good performance are substantially enhanced.	XL: an XML programming language for web service specification and composition	NA:NA:NA	2018
Srini Narayanan:Sheila A. McIlraith	Web services -- Web-accessible programs and devices - are a key application area for the Semantic Web. With the proliferation of Web services and the evolution towards the Semantic Web comes the opportunity to automate various Web services tasks. Our objective is to enable markup and automated reasoning technology to describe, simulate, compose, test, and verify compositions of Web services. We take as our starting point the DAML-S DAML+OIL ontology for describing the capabilities of Web services. We define the semantics for a relevant subset of DAML-S in terms of a first-order logical language. With the semantics in hand, we encode our service descriptions in a Petri Net formalism and provide decision procedures for Web service simulation, verification and composition. We also provide an analysis of the complexity of these tasks under different restrictions to the DAML-S composite services we can describe. Finally, we present an implementation of our analysis techniques. This implementation takes as input a DAML-S description of a Web service, automatically generates a Petri Net and performs the desired analysis. Such a tool has broad applicability both as a back end to existing manual Web service composition tools, and as a stand-alone tool for Web service developers.	Simulation, verification and automated composition of web services	NA:NA	2018
David Trastour:Claudio Bartolini:Chris Preist	If an e-services approach to electronic commerce is to become widespread, standardisation of ontologies, message content and message protocols will be necessary. In this paper, we present a lifecycle of a business-to-business e-commerce interaction, and show how the Semantic Web can support a service description language that can be used throughout this lifecycle. By using DAML, we develop a service description language sufficiently expressive and flexible to be used not only in advertisements, but also in matchmaking queries, negotiation proposals and agreements. We also identify which operations must be carried out on this description language if the B2B lifecycle is to be fully supported. We do not propose specific standard protocols, but instead argue that our operators are able to support a wide variety of interaction protocols, and so will be fundamental irrespective of which protocols are finally adopted.	Semantic web support for the business-to-business e-commerce lifecycle	NA:NA:NA	2018
Terence Kelly	NA	Session details: Auctions and E-commerce	NA	2018
Marlon Dumas:Lachlan Aldred:Guido Governatori:Arthur ter Hofstede:Nick Russell	This paper presents an approach to develop bidding agents that participate in multiple alternative auctions, with the goal of obtaining an item at the lowest price. The approach consists of a prediction method and a planning algorithm. The prediction method exploits the history of past auctions in order to build probability functions capturing the belief that a bid of a given price may win a given auction. The planning algorithm computes the lowest price, such that by sequentially bidding in a subset of the relevant auctions, the agent can obtain the item at that price with an acceptable probability. The approach addresses the case where the auctions are for substitutable items with different values. Experimental results are reported, showing that the approach increases the payoff of their users and the welfare of the market.	A probabilistic approach to automated bidding in alternative auctions	NA:NA:NA:NA:NA	2018
Marcus Fontoura:Mihail Ionescu:Naftaly Minsky	This paper proposes a flexible architecture for the creation of Internet auctions. It allows the custom definition of the auction parameters, and provides a decentralized control of the auction process. Auction policies are defined as laws in the Law Governed Interaction (LGI) paradigm. Each of these laws specifies not only the auction algorithm itself (e.g. open-cry, dutch, etc.) but also how to handle the other parameters usually involved in the online auctions, such as certification, auditioning, and treatment of complaints. LGI is used to enforce the rules established in the auction policy within the agents involved in the process. After the agents find out about the actions, they interact in a peer-to-peer communication protocol, reducing the role of the centralized auction room to an advertising registry, and taking profit of the distributed nature of the Internet to conduct the auction. The paper presents an example of an auction law, illustrating the use of the proposed architecture.	Law-governed peer-to-peer auctions	NA:NA:NA	2018
Hemant K. Bhargava:Juan Feng	Internet search engines and comparison shopping have recently begun implementing a paid placement strategy, where some content providers are given prominent positioning in return for a placement fee. This bias generates placement revenues but creates a disutility to users, thus reducing user-based revenues. We formulate the search engine design problem as a tradeoff between these two types of revenues. We demonstrate that the optimal placement strategy depends on the relative benefits (to providers) and disutilities (to users) of paid placement. We compute the optimal placement fee, characterize the optimal bias level, and analyze sensitivity of the placement strategy to various factors. In the optimal paid placement strategy, the placement revenues are set below the monopoly level due to its negative impact on advertising revenues. An increase in the search engine's quality of service allows it to improve profits from paid placement, moving it closer to the ideal. However, an increase in the value-per-user motivates the gatekeeper to increase market share by reducing further its reliance on paid placement and fraction of paying providers.	Paid placement strategies for internet search engines	NA:NA	2018
Prabhakar Raghavan	NA	Session details: Crawling	NA	2018
Junghoo Cho:Hector Garcia-Molina	In this paper we study how we can design an effective parallel crawler. As the size of the Web grows, it becomes imperative to parallelize a crawling process, in order to finish downloading pages in a reasonable amount of time. We first propose multiple architectures for a parallel crawler and identify fundamental issues related to parallel crawling. Based on this understanding, we then propose metrics to evaluate a parallel crawler, and compare the proposed architectures using 40 million pages collected from the Web. Our results clarify the relative merits of each architecture and provide a good guideline on when to adopt which architecture.	Parallel crawlers	NA:NA	2018
J. L. Wolf:M. S. Squillante:P. S. Yu:J. Sethuraman:L. Ozsen	Web Search Engines employ multiple so-called crawlers to maintain local copies of web pages. But these web pages are frequently updated by their owners, and therefore the crawlers must regularly revisit the web pages to maintain the freshness of their local copies. In this paper, we propose a two-part scheme to optimize this crawling process. One goal might be the minimization of the average level of staleness over all web pages, and the scheme we propose can solve this problem. Alternatively, the same basic scheme could be used to minimize a possibly more important search engine embarrassment level metric: The frequency with which a client makes a search engine query and then clicks on a returned url only to find that the result is incorrect. The first part our scheme determines the (nearly) optimal crawling frequencies, as well as the theoretically optimal times to crawl each web page. It does so within an extremely general stochastic framework, one which supports a wide range of complex update patterns found in practice. It uses techniques from probability theory and the theory of resource allocation problems which are highly computationally efficient -- crucial for practicality because the size of the problem in the web environment is immense. The second part employs these crawling frequencies and ideal crawl times as input, and creates an optimal achievable schedule for the crawlers. Our solution, based on network flow theory, is exact as well as highly efficient. An analysis of the update patterns from a highly accessed and highly dynamic web site is used to gain some insights into the properties of page updates in practice. Then, based on this analysis, we perform a set of detailed simulation experiments to demonstrate the quality and speed of our approach.	Optimal crawling strategies for web search engines	NA:NA:NA:NA:NA	2018
Soumen Chakrabarti:Kunal Punera:Mallela Subramanyam	The organization of HTML into a tag tree structure, which is rendered by browsers as roughly rectangular regions with embedded text and HREF links, greatly helps surfers locate and click on links that best satisfy their information need. Can an automatic program emulate this human behavior and thereby learn to predict the relevance of an unseen HREF target page w.r.t. an information need, based on information limited to the HREF source page? Such a capability would be of great interest in focused crawling and resource discovery, because it can fine-tune the priority of unvisited URLs in the crawl frontier, and reduce the number of irrelevant pages which are fetched and discarded.	Accelerated focused crawling through online relevance feedback	NA:NA:NA	2018
Leslie Carr	NA	Session details: Hypermedia in the Small	NA	2018
Niels Olof Bouvin:Polle T. Zellweger:Kaj Grønbæk:Jock D. Mackinlay	The Fluid Documents project has developed various research prototypes that show that powerful annotation techniques based on animated typographical changes can help readers utilize annotations more effectively. Our recently-developed Fluid Open Hypermedia prototype supports the authoring and browsing of fluid annotations on third-party Web pages. This prototype is an extension of the Arakne Environment, an open hypermedia application that can augment Web pages with externally stored hypermedia structures. This paper describes how various Web standards, including DOM, CSS, XLink, XPointer, and RDF, can be used and extended to support fluid annotations.	Fluid annotations through open hypermedia: using and extending emerging web standards	NA:NA:NA:NA	2018
M. C. schraefel:Yuxiang Zhu:David Modjeska:Daniel Wigdor:Shengdong Zhao	Hunter Gatherer is an interface that lets Web users carry out three main tasks: (1) collect components from within Web pages; (2) represent those components in a collection; (3) edit those component collections. Our research shows that while the practice of making collections of content from within Web pages is common, it is not frequent, due in large part to poor interaction support in existing tools. We engaged with users in task analysis as well as iterative design reviews in order to understand the interaction issues that are part of within-Web-page collection making and to design an interaction that would support that process.We report here on that design development, as well as on the evaluations of the tool that evolved from that process, and the future work stemming from these results, in which our critical question is: what happens to users perceptions and expectations of web-based information (their web-based information management practices) when they can treat this information as harvestable, recontextualizable data, rather than as fixed pages?	Hunter gatherer: interaction support for the creation and management of within-web-page collections	NA:NA:NA:NA:NA	2018
David Stotts:Jaime Navon	HTML documents composed of frames can be difficult to write correctly. We demonstrate a technique that can be used by authors manually creating HTML documents (or by document editors) to verify that complex frame construction exhibits the intended behavior when browsed. The method is based on model checking (an automated program verification technique), and on temporal logic specifications of expected frames behavior. We show how to model the HTML frames source as a CobWeb protocol, related to the Trellis model of hypermedia documents. We show how to convert the CobWeb protocol to input for a model checker, and discuss several ways for authors to create the necessary behavior specifications. Our solution allows Web documents to be built containing a large number of frames and content pages interacting in complex ways. We expect such Web structures to be more useful in "literary" hypermedia than for Web "sites" used as interfaces to organizational information or databases.	Model checking cobweb protocols for verification of HTML frames behavior	NA:NA	2018
Azzedine Boukerche	NA	Session details: Ubiquitous WWW	NA	2018
Tim Kindberg	Identifier resolution is presented as a way to link the physical world with virtual Web resources. In this paradigm, designed to support nomadic users, the user employs a handheld, wirelessly connected, sensor-equipped device to read identifiers associated with physical entities. The identifiers are resolved into virtual resources or actions related to the physical entities - as though the user 'clicked on a physical hyperlink'. We have integrated identifier resolution with the Web so that it can be deployed as ubiquitously as the Web, in the infrastructure and on wirelessly connected handheld devices. We enable users to capture resolution services and applications as Web resources in their local context. We use the Web to invoke resolution services, with a model of 'physical' Web form-filling. We propose a scheme for binding identifiers to resources, to promote services and applications linking the physical and virtual worlds.	Implementing physical hyperlinks using ubiquitous identifier resolution	NA	2018
Lalitha Suryanarayana:Johan Hjelm	The World Wide Web is evolving into a medium that will soon make it possible for conceiving and implementing situation-aware services. A situation-aware or situated web application is one that renders the user with an experience (content, interaction and presentation) that is so tailored to his/her current situation. This requires the facts and opinions regarding the context to be communicated to the server by means of a profile, which is then applied against the description of the application objects at the server in order to generate the required experience. This paper discusses a profiles view of the situated web architecture and analyzes the key technologies and capabilities that enable them. We conclude that trusted frameworks wherein rich vocabularies describing users and their context, applications and documents, along with rules for processing them, are critical elements of such architectures.	Profiles for the situated web	NA:NA	2018
James H. Kaufman:Stefan Edlund:Daniel A. Ford:Calvin Powers	The information age has brought with it the promise of unprecedented economic growth based on the efficiencies made possible by new technology. This same greater efficiency has left society with less and less time to adapt to technological progress. Perhaps the greatest cost of this progress is the threat to privacy we all face from unconstrained exchange of our personal information. In response to this threat, the World Wide Web Consortium has introduced the "Platform for Privacy Preferences" (P3P) to allow sites to express policies in machine-readable form and to expose these policies to site visitors [1]. However, today P3P does not protect the privacy of individuals, nor does its implementation empower communities or groups to negotiate and establish standards of behavior. We propose a privacy architecture we call the Social Contract Core (SCC), designed to speed the establishment of new "Social Contracts" needed to protect private data. The goal of SCC is to empower communities, speed the "socialization" of new technology, and encourage the rapid access to, and exchange of, information. Addressing these issues is essential, we feel, to both liberty and economic prosperity in the information age[2].	The social contract core	NA:NA:NA:NA	2018
Sridhar Rajagopalan	NA	Session details: Extraction and Visualization	NA	2018
Jennifer Leopold:Meg Heimovics:Tyler Palmer	Today there is a plethora of data accessible via the Internet. The Web has greatly simplified the process of searching for, accessing, and sharing information. However, a considerable amount of Internet-distributed data still goes unnoticed and unutilized, particularly in the case of frequently-updated, Internet-distributed databases. In this paper we give an overview of WebFormulate, a Web-based visual continual query system that addresses the problems associated with formulating temporal ad hoc analyses over networks of heterogeneous, frequently-updated data sources. The main distinction between this system and existing Internet facilities to retrieve information and assimilate it into computations is that WebFormulate provides the necessary facilities to perform continual queries, developing and maintaining dynamic links such that Web-based computations and reports automatically maintain themselves. A further distinction is that this system is specifically designed for users of spreadsheet-level ability, rather than professional programmers.	Webformulate: a web-based visual continual query system	NA:NA:NA	2018
William W. Cohen:Matthew Hurst:Lee S. Jensen	A program that makes an existing website look like a database is called a wrapper. Wrapper learning is the problem of learning website wrappers from examples. We present a wrapper-learning system called WL2 that can exploit several different representations of a document. Examples of such different representations include DOM-level and token-level representations, as well as two-dimensional geometric views of the rendered page (for tabular data) and representations of the visual appearance of text asm it will be rendered. Additionally, the learning system is modular, and can be easily adapted to new domains and tasks. The learning system described is part of an "industrial-strength" wrapper management system that is in active use at WhizBang Labs. Controlled experiments show that the learner has broader coverage and a faster learning rate than earlier wrapper-learning systems.	A flexible learning system for wrapping tables and lists in HTML documents	NA:NA:NA	2018
Yalin Wang:Jianying Hu	Table is a commonly used presentation scheme, especially for describing relational information. However, table understanding remains an open problem. In this paper, we consider the problem of table detection in web documents. Its potential applications include web mining, knowledge management, and web content summarization and delivery to narrow-bandwidth devices. We describe a machine learning based approach to classify each given table entity as either genuine or non-genuine. Various features reflecting the layout as well as content characteristics of tables are studied.In order to facilitate the training and evaluation of our table classifier, we designed a novel web document table ground truthing protocol and used it to build a large table ground truth database. The database consists of 1,393 HTML files collected from hundreds of different web sites and contains 11,477 leaf TABLE elements, out of which 1,740 are genuine tables. Experiments were conducted using the cross validation method and an F-measure of 95.89% was achieved.	A machine learning based approach for table detection on the web	NA:NA	2018
Monica Schraefel	NA	Session details: Hypermedia in the Large	NA	2018
Soumen Chakrabarti:Mukul M. Joshi:Kunal Punera:David M. Pennock	The Web graph is a giant social network whose properties have been measured and modeled extensively in recent years. Most such studies concentrate on the graph structure alone, and do not consider textual properties of the nodes. Consequently, Web communities have been characterized purely in terms of graph structure and not on page content. We propose that a topic taxonomy such as Yahoo! or the Open Directory provides a useful framework for understanding the structure of content-based clusters and communities. In particular, using a topic taxonomy and an automatic classifier, we can measure the background distribution of broad topics on the Web, and analyze the capability of recent random walk algorithms to draw samples which follow such distributions. In addition, we can measure the probability that a page about one broad topic will link to another broad topic. Extending this experiment, we can measure how quickly topic context is lost while walking randomly on the Web graph. Estimates of this topic mixing distance may explain why a global PageRank is still meaningful in the context of broad queries. In general, our measurements may prove valuable in the design of community-specific crawlers and link-based ranking systems.	The structure of broad topics on the web	NA:NA:NA:NA	2018
Michael Evans:Steven Furnell	The web's hyperlinks are notoriously brittle, and break whenever a resource migrates. One solution to this problem is a transparent resource migration mechanism, which separates a resource's location from its identity, and helps provide referential integrity. However, although several such mechanisms have been designed, they have not been widely adopted, due largely to a lack of compliance with current web standards. In addition, these mechanisms must be updated manually whenever a resource migrates, limiting their effectiveness for large web sites. Recently, however, new web protocols such as WebDAV (Web Distributed Authoring and Versioning) have emerged, which extend the HTTP protocol and provide a new level of control over web resources. In this paper, we show how we have used these protocols in the design of a new Resource Migration Protocol (RMP), which enables transparent resource migration across standard web servers. The RMP works with a new resource migration mechanism we have developed called the Resource Locator Service (RLS), and is fully backwards compatible with the web's architecture, enabling all web servers and all web content to be involved in the migration process. We describe the protocol and the new RLS in full, together with a prototype implementation and demonstration applications that we have developed. The paper concludes by presenting performance data taken from the prototype that show how the RLS will scale well beyond the size of today's web.	A web-based resource migration protocol using WebDAV	NA:NA	2018
Emilia Mendes:Nile Mosley:Ian Watson	Over the years software engineering researchers have suggested numerous techniques for estimating development effort. These techniques have been classified mainly as algorithmic, machine learning and expert judgement. Several studies have compared the prediction accuracy of those techniques, with emphasis placed on linear regression, stepwise regression, and Case-based Reasoning (CBR). To date no converging results have been obtained and we believe they may be influenced by the use of the same CBR configuration.The objective of this paper is twofold. First, to describe the application of case-based reasoning for estimating the effort for developing Web hypermedia applications. Second, comparing the prediction accuracy of different CBR configurations, using two Web hypermedia datasets.Results show that for both datasets the best estimations were obtained with weighted Euclidean distance, using either one analogy (dataset 1) or 3 analogies (dataset 2). We suggest therefore that case-based reasoning is a candidate technique for effort estimation and, with the aid of an automated environment, can be applied to Web hypermedia development effort prediction.	A comparison of case-based reasoning approaches	NA:NA:NA	2018
Geoffrey M. Voelker	NA	Session details: Performance Workload Char. and Adaptation	NA	2018
Terence Kelly:Jeffrey Mogul	Aliasing occurs in Web transactions when requests containing different URLs elicit replies containing identical data payloads. Conventional caches associate stored data with URLs and can therefore suffer redundant payload transfers due to aliasing and other causes. Existing research literature, however, says little about the prevalence of aliasing in user-initiated transactions, or about redundant payload transfers in conventional Web cache hierarchies.This paper quantifies the extent of aliasing and the performance impact of URL-indexed cache management using a large client trace from WebTV Networks. Fewer than 5% of reply payloads are aliased (referenced via multiple URLs) but over 54% of successful transactions involve aliased payloads. Aliased payloads account for under 3.1% of the trace's "working set size" (sum of payload sizes) but over 36% of bytes transferred. For the WebTV workload, roughly 10% of payload transfers to browser caches and 23% of payload transfers to a shared proxy are redundant, assuming infinite-capacity conventional caches. Our analysis of a large proxy trace from Compaq Corporation yields similar results.URL-indexed caching does not entirely explain the large number of redundant proxy-to-browser payload transfers previously reported in the WebTV system. We consider other possible causes of redundant transfers (e.g., reply metadata and browser cache management policies) and discuss a simple hop-by-hop protocol extension that completely eliminates all redundant transfers, regardless of cause.	Aliasing on the world wide web: prevalence and performance implications	NA:NA	2018
Jaeyeon Jung:Balachander Krishnamurthy:Michael Rabinovich	The paper studies two types of events that often overload Web sites to a point when their services are degraded or disrupted entirely - flash events (FEs) and denial of service attacks (DoS). The former are created by legitimate requests and the latter contain malicious requests whose goal is to subvert the normal operation of the site. We study the properties of both types of events with a special attention to characteristics that distinguish the two. Identifying these characteristics allows a formulation of a strategy for Web sites to quickly discard malicious requests. We also show that some content distribution networks (CDNs) may not provide the desired level of protection to Web sites against flash events. We therefore propose an enhancement to CDNs that offers better protection and use trace-driven simulations to study the effect of our enhancement on CDNs and Web sites.	Flash crowds and denial of service attacks: characterization and implications for CDNs and web sites	NA:NA:NA	2018
Balachander Krishnamurthy:Craig E. Wills	We categorize the set of clients communicating with a server on the Web based on information that can be determined by the server. The Web server uses the information to direct tailored actions. Users with poor connectivity may choose not to stay at a Web site if it takes a long time to receive a page, even if the Web server at the site is not the bottleneck. Retaining such clients may be of interest to a Web site. Better connected clients can receive enhanced representations of Web pages, such as with higher quality images.We explore a variety of considerations that could be used by a Web server in characterizing a client. Once a client is characterized as poor or rich, the server can deliver altered content, alter how content is delivered, alter policy and caching decisions, or decide when to redirect the client to a mirror site. We also use network-aware client clustering techniques to provide a coarser level of client categorization and use it to categorize subsequent clients from that cluster for which a client-specific categorization is not available.Our results for client characterization and applicable server actions are derived from real, recent, and diverse set of Web server logs. Our experiments demonstrate that a relatively simple characterization policy can classify poor clients such that these clients subsequently make the majority of badly performing requests to a Web server. This policy is also stable in terms of clients staying in the same class for a large portion of the analysis period. Client clustering can significantly help in initially classifying clients for which no previous information about the client is known. We also show that different server actions can be applied to a significant number of request sequences with poor performance.	Improving web performance by client characterization driven server adaptation	NA:NA	2018
Nick Craswell	NA	Session details: Search 1	NA	2018
Gary W. Flake:Eric J. Glover:Steve Lawrence:C. Lee Giles	When searching the WWW, users often desire results restricted to a particular document category. Ideally, a user would be able to filter results with a text classifier to minimize false positive results; however, current search engines allow only simple query modifications. To automate the process of generating effective query modifications, we introduce a sensitivity analysis-based method for extracting rules from nonlinear support vector machines. The proposed method allows the user to specify a desired precision while attempting to maximize the recall. Our method performs several levels of dimensionality reduction and is vastly faster than searching the combination feature space; moreover, it is very effective on real-world data.	Extracting query modifications from nonlinear SVMs	NA:NA:NA:NA	2018
Hang Cui:Ji-Rong Wen:Jian-Yun Nie:Wei-Ying Ma	Query expansion has long been suggested as an effective way to resolve the short query and word mismatching problems. A number of query expansion methods have been proposed in traditional information retrieval. However, these previous methods do not take into account the specific characteristics of web searching; in particular, of the availability of large amount of user interaction information recorded in the web query logs. In this study, we propose a new method for query expansion based on query logs. The central idea is to extract probabilistic correlations between query terms and document terms by analyzing query logs. These correlations are then used to select high-quality expansion terms for new queries. The experimental results show that our log-based probabilistic query expansion method can greatly improve the search performance and has several advantages over other existing methods.	Probabilistic query expansion using query logs	NA:NA:NA:NA	2018
B. Uygar Oztekin:George Karypis:Vipin Kumar	Recent increase in the number of search engines on the Web and the availability of meta search engines that can query multiple search engines makes it important to find effective methods for combining results coming from different sources. In this paper we introduce novel methods for reranking in a meta search environment based on expert agreement and contents of the snippets. We also introduce an objective way of evaluating different methods for ranking search results that is based upon implicit user judgements. We incorporated our methods and two variations of commonly used merging methods in our meta search engine, Mearf, and carried out an experimental study using logs accumulated over a period of twelve months. Our experiments show that the choice of the method used for merging the output produced by different search engines plays a significant role in the overall quality of the search results. In almost all cases examined, results produced by some of the new methods introduced were consistently better than the ones produced by traditional methods commonly used in various meta search engines. These observations suggest that the proposed methods can offer a relatively inexpensive way of improving the meta search experience over existing methods.	Expert agreement and content based reranking in a meta search environment using Mearf	NA:NA:NA	2018
Philipp Hoschka	NA	Session details: Applications	NA	2018
Roberto J. Bayardo Jr.:Rakesh Agrawal:Daniel Gruhl:Amit Somani	YouServ is a system that allows its users to pool existing desktop computing resources for high availability web hosting and file sharing. By exploiting standard web and internet protocols (e.g. HTTP and DNS), YouServ does not require those who access YouServ-published content to install special purpose software. Because it requires minimal server-side resources and administration, YouServ can be provided at a very low cost. We describe the design, implementation, and a successful intranet deployment of the YouServ system, and compare it with several alternatives.	YouServ: a web-hosting and content sharing tool for the masses	NA:NA:NA:NA	2018
In-Young Ko:Ke-Thia Yao:Robert Neches	Dynamic Web content provides us with time-sensitive and continuously changing data. To glean up-to-date information, users need to regularly browse, collect and analyze this Web content. Without proper tool support this information management task is tedious, time-consuming and error prone, especially when the quantity of the dynamic Web content is large, when many information management services are needed to analyze it, and when underlying services/network are not completely reliable. This paper describes a multi-level, lifecycle (design-time and run-time) coordination mechanism that enables rapid, efficient development and execution of information management applications that are especially useful for processing dynamic Web content. Such a coordination mechanism brings dynamism to coordinating independent, distributed information management services. Dynamic parallelism spawns/merges multiple execution service branches based on available data, and dynamic run-time reconfiguration coordinates service execution to overcome faulty services and bottlenecks. These features enable information management applications to be more efficient in handling content and format changes in Web resources, and enable the applications to be evolved and adapted to process dynamic Web content.	Dynamic coordination of information management services for processing dynamic web content	NA:NA:NA	2018
Oliver Kelkar:Joerg Leukel:Volker Schmitz	The fast spreading of electronic business-to-business procurement systems has led to the development of new standards for the exchange of electronic product catalogs (e-catalogs). E-catalogs contain various information about products, essential is price information. Prices are used for buying decisions and following order transactions. While simple price models are often sufficient for the description of indirect goods (e.g. office supplies), other goods and lines of business make higher demands. In this paper we examine what price information is contained in commercial XML standards for the exchange of product catalog data. For that purpose we bring the different implicit price models of the examined catalog standards together and provide a generalized model.	Price modeling in standards for electronic product catalogs based on XML	NA:NA:NA	2018
Jose Kahan	NA	Session details: Security for Web Applications and P2P	NA	2018
Fabrizio Cornelli:Ernesto Damiani:Sabrina De Capitani di Vimercati:Stefano Paraboschi:Pierangela Samarati	Peer-to-peer information sharing environments are increasingly gaining acceptance on the Internet as they provide an infrastructure in which the desired information can be located and downloaded while preserving the anonymity of both requestors and providers. As recent experience with P2P environments such as Gnutella shows, anonymity opens the door to possible misuses and abuses by resource providers exploiting the network as a way to spread tampered with resources, including malicious programs, such as Trojan Horses and viruses.In this paper we propose an approach to P2P security where servents can keep track, and share with others, information about the reputation of their peers. Reputation sharing is based on a distributed polling algorithm by which resource requestors can assess the reliability of perspective providers before initiating the download. The approach nicely complements the existing P2P protocols and has a limited impact on current implementations. Furthermore, it keeps the current level of anonymity of requestors and providers, as well as that of the parties sharing their view on others' reputations.	Choosing reputable servents in a P2P network	NA:NA:NA:NA:NA	2018
Martín Abadi:Neal Glew	This paper presents a new protocol for certified email. The protocol aims to combine security, scalability, easy implementation, and viable deployment. The protocol relies on a light on-line trusted third party; it can be implemented without any special software for the receiver beyond a standard email reader and web browser, and does not require any public-key infrastructure.	Certified email with a light on-line trusted third party: design and implementation	NA:NA	2018
David Scott:Richard Sharp	Application-level web security refers to vulnerabilities inherent in the code of a web-application itself (irrespective of the technologies in which it is implemented or the security of the web-server/back-end database on which it is built). In the last few months application-level vulnerabilities have been exploited with serious consequences: hackers have tricked e-commerce sites into shipping goods for no charge, user-names and passwords have been harvested and condential information (such as addresses and credit-card numbers) has been leaked.In this paper we investigate new tools and techniques which address the problem of application-level web security. We (i) describe a scalable structuring mechanism facilitating the abstraction of security policies from large web-applications developed in heterogenous multi-platform environments; (ii) present a tool which assists programmers develop secure applications which are resilient to a wide range of common attacks; and (iii) report results and experience arising from our implementation of these techniques.	Abstracting application-level web security	NA:NA	2018
Aya Soffer	NA	Session details: Search 2	NA	2018
Dragomir Radev:Weiguo Fan:Hong Qi:Harris Wu:Amardeep Grewal	Web-based search engines such as Google and NorthernLight return documents that are relevant to a user query, not answers to user questions. We have developed an architecture that augments existing search engines so that they support natural language question answering. The process entails five steps: query modulation, document retrieval, passage extraction, phrase extraction, and answer ranking. In this paper we describe some probabilistic approaches to the last three of these stages. We show how our techniques apply to a number of existing search engines and we also present results contrasting three different methods for question answering. Our algorithm, probabilistic phrase reranking (PPR) using proximity and question type features achieves a total reciprocal document rank of .20 on the TREC 8 corpus. Our techniques have been implemented as a Web-accessible system, called NSIR.	Probabilistic question answering on the web	NA:NA:NA:NA:NA	2018
Rakesh Agrawal:Ramakrishnan Srikant	A large fraction of the useful web comprises of specification documents that largely consist of hattribute name, numeric valuei pairs embedded in text. Examples include product information, classified advertisements, resumes, etc. The approach taken in the past to search these documents by first establishing correspondences between values and their names has achieved limited success because of the difficulty of extracting this information from free text. We propose a new approach that does not require this correspondence to be accurately established. Provided the data has "low reflectivity", we can do effective search even if the values in the data have not been assigned attribute names and the user has omitted attribute names in the query. We give algorithms and indexing structures for implementing the search. We also show how hints (i. e, imprecise, partial correspondences) from automatic data extraction techniques can be incorporated into our approach for better accuracy on high reflectivity datasets. Finally, we validate our approach by showing that we get high precision in our answers on real datasets from a variety of domains.	Searching with numbers	NA:NA	2018
Taher H. Haveliwala:Aristides Gionis:Dan Klein:Piotr Indyk	Finding pages on the Web that are similar to a query page (Related Pages) is an important component of modern search engines. A variety of strategies have been proposed for answering Related Pages queries, but comparative evaluation by user studies is expensive, especially when large strategy spaces must be searched (e.g., when tuning parameters). We present a technique for automatically evaluating strategies using Web hierarchies, such as Open Directory, in place of user feedback. We apply this evaluation methodology to a mix of document representation strategies, including the use of text, anchor-text, and links. We discuss the relative advantages and disadvantages of the various approaches examined. Finally, we describe how to efficiently construct a similarity index out of our chosen strategies, and provide sample results from our index.	Evaluating strategies for similarity search on the web	NA:NA:NA:NA	2018
Stu Weibel	NA	Session details: Languages & Authoring for the Semnatic Web	NA	2018
Peter Patel-Schneider:Jérôme Siméon	XML is the W3C standard document format for writing and exchanging information on the Web. RDF is the W3C standard model for describing the semantics and reasoning about information on the Web. Unfortunately, RDF and XML---although very close to each other---are based on two different paradigms. We argue that in order to lead the Semantic Web to its full potential, the syntax and the semantics of information needs to work together. To this end, we develop a model-theoretic semantics for the XML XQuery 1.0 and XPath 2.0 Data Model, which provides a unified model for both XML and RDF. This unified model can serve as the basis for Web applications that deal with both data and semantics. We illustrate the use of this model on a concrete information integration scenario. Our approach enables each side of the fence to benefit from the other, notably, we show how the RDF world can take advantage of XML query languages, and how the XML world can take advantage of the reasoning capabilities available for RDF.	The Yin/Yang web: XML syntax and RDF semantics	NA:NA	2018
Jeremy J. Carroll	It is difficult to serialize an RDF graph as a humanly readable RDF/XML document. This paper describes the approach taken in Jena 1.2, in which a design pattern of guarded procedures invoked using top down recursive descent is used. Each procedure corresponds to a grammar rule; the guard makes the choice about the applicability of the production. This approach is seen to correspond closely to the design of an LL(k) parser, and a theoretical justification of this correspondence is found in universal algebra.	Unparsing RDF/XML	NA	2018
Siegfried Handschuh:Steffen Staab	Richly interlinked, machine-understandable data constitute the basis for the Semantic Web. We provide a framework, CREAM, that allows for creation of metadata. While the annotation mode of CREAM allows to create metadata for existing web pages, the authoring mode lets authors create metadata --- almost for free --- while putting together the content of a page.As a particularity of our framework, CREAM allows to create relational metadata, i.e. metadata that instantiate interrelated definitions of classes in a domain ontology rather than a comparatively rigid template-like schema asm Dublin Core. We discuss some of the requirements one has to meet when developing such an ontology-based framework, e.g. the integration of a metadata crawler, inference services, document management and a meta-ontology, and describe its implementation, viz. Ont-O-Mat, a component-based, ontology-driven Web page authoring and annotation tool.	Authoring and annotation of web pages in CREAM	NA:NA	2018
Norman Walsh	NA	Session details: XML Applications	NA	2018
Lionel Villard:Nabil Layaïda	In this paper, we present an incremental transformation framework called incXSLT. This framework has been experimented for the XSLT language defined at the World Wide Web Consortium. For the currently available tools, designing the XML content and the transformation sheets is an inefficient, a tedious and an error prone experience. Incremental transformation processors such as incXSLT represent a better alternative to help in the design of both the content and the transformation sheets. We believe that such frameworks are a first step toward fully interactive transformation-based authoring environments.	An incremental XSLT transformation processor for XML document manipulation	NA:NA	2018
James Bailey:Alexandra Poulovassilis:Peter T. Wood	XML repositories are now a widespread means for storing and exchanging information on the Web. As these repositories become increasingly used in dynamic applications such as e-commerce, there is a rapidly growing need for a mechanism to incorporate reactive functionality in an XML setting. Event-condition-action (ECA) rules are a technology from active databases and are a natural method for supporting suchfunctionality. ECA rules can be used for activities such as automatically enforcing document constraints, maintaining repository statistics, and facilitating publish/subscribe applications. An important question associated with the use of a ECA rules is how to statically predict their run-time behaviour. In this paper, we define a language for ECA rules on XML repositories. We then investigate methods for analysing the behaviour of a set of ECA rules, a task which has added complexity in this XML setting compared with conventional active databases.	An event-condition-action language for XML	NA:NA:NA	2018
Kim Marriott:Bernd Meyer:Laurent Tardif	The Scalable Vector Graphics format SVG is already substantially improving graphics delivery on the web, but some important issues still remain to be addressed. In particular, SVG does not support client-side adaption of documents to different viewing conditions, such as varying screen sizes, style preferences or different device capabilities. Based on our earlier work we show how SVG can be extended with constraint-based specification of document layout to augment it with adaptive capabilities. The core of our proposal is to include one-way constraints into SVG, which offer more expressiveness than the previously suggested class of linear constraints and at the same time require substantially less computational effort.	Fast and efficient client-side adaptivity for SVG	NA:NA:NA	2018
Pedro Domingos	NA	Session details: Link Analysis	NA	2018
Michelangelo Diligenti:Marco Gori:Marco Maggini	Page ranking is a fundamental step towards the construction of effective search engines for both generic (horizontal) and focused (vertical) search. Ranking schemes for horizontal search like the PageRank algorithm used by Google operate on the topology of the graph, regardless of the page content. On the other hand, the recent development of vertical portals (vortals) makes it useful to adopt scoring systems focussed on the topic and taking the page content into account.In this paper, we propose a general framework for Web Page Scoring Systems (WPSS) which incorporates and extends many of the relevant models proposed in the literature. Finally, experimental results are given to assess the features of the proposed scoring systems with special emphasis on vertical search.	Web page scoring systems for horizontal and vertical search	NA:NA:NA	2018
Taher H. Haveliwala	In the original PageRank algorithm for improving the ranking of search-query results, a single PageRank vector is computed, using the link structure of the Web, to capture the relative "importance" of Web pages, independent of any particular search query. To yield more accurate search results, we propose computing a set of PageRank vectors, biased using a set of representative topics, to capture more accurately the notion of importance with respect to a particular topic. By using these (precomputed) biased PageRank vectors to generate query-specific importance scores for pages at query time, we show that we can generate more accurate rankings than with a single, generic PageRank vector. For ordinary keyword search queries, we compute the topic-sensitive PageRank scores for pages satisfying the query using the topic of the query keywords. For searches done in context (e.g., when the search query is performed by highlighting words in a Web page), we compute the topic-sensitive PageRank scores using the topic of the context in which the query appeared.	Topic-sensitive PageRank	NA	2018
Longzhuang Li:Yi Shang:Wei Zhang	In this paper, we present two ways to improve the precision of HITS-based algorithms on Web documents. First, by analyzing the limitations of current HITS-based algorithms, we propose a new weighted HITS-based method that assigns appropriate weights to in-links of root documents. Then, we combine content analysis with HITS-based algorithms and study the effects of four representative relevance scoring methods, VSM, Okapi, TLS, and CDR, using a set of broad topic queries. Our experimental results show that our weighted HITS-based method performs significantly better than Bharat's improved HITS algorithm. When we combine our weighted HITS-based method or Bharat's HITS algorithm with any of the four relevance scoring methods, the combined methods are only marginally better than our weighted HITS-based method. Between the four relevance-scoring methods, there is no significant quality difference when they are combined with a HITS-based algorithm.	Improvement of HITS-based algorithms on web documents	NA:NA:NA	2018
Mary Ellen Zurko	NA	Session details: Advertising and Security for E-Commerence	NA	2018
Atsuyoshi Nakamura	We addressed two issues concerning the practical aspects of optimally scheduling web advertising proposed by Langheinrich et al. [5], which scheduling maximizes the total number of click-throughs for all banner advertisements. One is the problem of multi-impressions in which two or more banner ads are impressed at the same time. The other is inventory management, which is important in order to prevent over-selling and maximize revenue. We propose efficient methods which deal with these two issues.	Improvements in practical aspects of optimally scheduling web advertising	NA	2018
Carlo Blundo:Stelvio Cimato:Annalisa De Bonis	A form of advertisement which is becoming very popular on the web is based on electronic coupon (e-coupon) distribution. E-coupons are the digital analogue of paper coupons which are used to provide customers with discounts or gift in order to incentive the purchase of some products. Nowadays, the potential of digital coupons has not been fully exploited on the web. This is mostly due to the lack of "efficient" techniques to handle the generation and distribution of e-coupons. In this paper we discuss models and protocols for e-coupons satisfying a number of security requirements. Our protocol is lightweight and preserves the privacy of the users, since it does not require any registration phase.	A lightweight protocol for the generation and distribution of secure e-coupons	NA:NA:NA	2018
José Brustoloni	It is widely recognized that distributed denial-of-service (DDoS) attacks can disrupt electronic commerce and cause large revenue losses. However, effective defenses continue to be mostly unavailable. We describe and evaluate VIPnet, a novel value-added network service for protecting e-commerce and other transaction-based sites from DDoS attacks. In VIPnet, e-merchants pay Internet Service Providers (ISPs) to carry the packets of the e-merchants' best clients (called VIPs) in a privileged class of service (CoS), protected from congestion, whether malicious or not, in the regular CoS. VIPnet rewards VIPs with not only better quality of service, but also greater availability. Because VIP rights are client- and server-specific, cannot be forged, are usage-limited, and are only replenished after successful client transactions (e.g., purchases), it is impractical for attackers to mount and sustain DDoS attacks against an e-merchant's VIPs. VIPnet can be deployed incrementally and does not require universal adoption. Experiments demonstrate VIPnet's benefits.	Protecting electronic commerce from distributed denial-of-service attacks	NA	2018
Soumen Chakrabarti	NA	Session details: Description and Analysis	NA	2018
Eric J. Glover:Kostas Tsioutsiouliklis:Steve Lawrence:David M. Pennock:Gary W. Flake	The structure of the web is increasingly being used to improve organization, search, and analysis of information on the web. For example, Google uses the text in citing documents (documents that link to the target document) for search. We analyze the relative utility of document text, and the text in citing documents near the citation, for classification and description. Results show that the text in citing documents, when available, often has greater discriminative and descriptive power than the text in the target document itself. The combination of evidence from a document and citing documents can improve on either information source alone. Moreover, by ranking words and phrases in the citing documents according to expected entropy loss, we are able to accurately name clusters of web pages, even with very few positive examples. Our results confirm, quantify, and extend previous research using web structure in these areas, introducing new methods for classification and description of pages.	Using web structure for classifying and describing web pages	NA:NA:NA:NA:NA	2018
Vijay Boyapati:Kristie Chevrier:Avi Finkel:Natalie Glance:Tom Pierce:Robert Stockton:Chip Whitmer	This paper presents a new challenge for Web monitoring tools: to build a system that can monitor entire web sites effectively. Such a system could potentially be used to discover "silent news" hidden within corporate web sites. Examples of silent news include reorganizations in the executive team of a company or in the retirement of a product line. ChangeDetector, an implemented prototype, addresses this challenge by incorporating a number of machine learning techniques. The principal backend components of ChangeDetector all rely on machine learning: intelligent crawling, page classification and entity-based change detection. Intelligent crawling enables ChangeDetector to selectively crawl the most relevant pages of very large sites. Classification allows change detection to be filtered by topic. Entity extraction over changed pages permits change detection to be filtered by semantic concepts, such as person names, dates, addresses, and phone numbers. Finally, the front end presents a flexible way for subscribers to interact with the database of detected changes to pinpoint those changes most likely to be of interest.	ChangeDetector™: a site-level monitoring tool for the WWW	NA:NA:NA:NA:NA:NA:NA	2018
Ziv Bar-Yossef:Sridhar Rajagopalan	We formulate and propose the template detection problem, and suggest a practical solution for it based on counting frequent item sets. We show that the use of templates is pervasive on the web. We describe three principles, which characterize the assumptions made by hypertext information retrieval (IR) and data mining (DM) systems, and show that templates are a major source of violation of these principles. As a consequence, basic "pure" implementations of simple search algorithms coupled with template detection and elimination show surprising increases in precision at all levels of recall.	Template detection via data mining and its applications	NA:NA	2018
Sheila Mcllraith	NA	Session details: Query Language for Semantic Web	NA	2018
Gregory Karvounarakis:Sofia Alexaki:Vassilis Christophides:Dimitris Plexousakis:Michel Scholl	Real-scale Semantic Web applications, such as Knowledge Portals and E-Marketplaces, require the management of large volumes of metadata, i.e., information describing the available Web content and services. Better knowledge about their meaning, usage, accessibility or quality will considerably facilitate an automated processing of Web resources. The Resource Description Framework (RDF) enables the creation and exchange of metadata as normal Web data. Although voluminous RDF descriptions are already appearing, sufficiently expressive declarative languages for querying both RDF descriptions and schemas are still missing. In this paper, we propose a new RDF query language called RQL. It is a typed functional language (a la OQL) and relies on a formal model for directed labeled graphs permitting the interpretation of superimposed resource descriptions by means of one or more RDF schemas. RQL adapts the functionality of semistructured/XML query languages to the peculiarities of RDF but, foremost, it enables to uniformly query both resource descriptions and schemas. We illustrate the RQL syntax, semantics and typing system by means of a set of example queries and report on the performance of our persistent RDF Store employed by the RQL interpreter.	RQL: a declarative query language for RDF	NA:NA:NA:NA:NA	2018
Wolfgang Nejdl:Boris Wolf:Changtao Qu:Stefan Decker:Michael Sintek:Ambjörn Naeve:Mikael Nilsson:Matthias Palmér:Tore Risch	Metadata for the World Wide Web is important, but metadata for Peer-to-Peer (P2P) networks is absolutely crucial. In this paper we discuss the open source project Edutella which builds upon metadata standards defined for the WWW and aims to provide an RDF-based metadata infrastructure for P2P applications, building on the recently announced JXTA Framework. We describe the goals and main services this infrastructure will provide and the architecture to connect Edutella Peers based on exchange of RDF metadata. As the query service is one of the core services of Edutella, upon which other services are built, we specify in detail the Edutella Common Data Model (ECDM) as basis for the Edutella query exchange language (RDF-QEL-i) and format implementing distributed queries over the Edutella network. Finally, we shortly discuss registration and mediation services, and introduce the prototype and application scenario for our current Edutella aware peers.	EDUTELLA: a P2P networking infrastructure based on RDF	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Sushant Jain:Ratul Mahajan:Dan Suciu	We present an algorithm for translating XSLT programs into SQL. Our context is that of virtual XML publishing, in which a single XML view is defined from a relational database, and subsequently queried with XSLT programs. Each XSLT program is translated into a single SQL query and run entirely in the database engine. Our translation works for a large fragment of XSLT, which we define, that includes descendant/ancestor axis, recursive templates, modes, parameters, and aggregates. We put considerable effort in generating correct and efficient SQL queries and describe several optimization techniques to achieve this efficiency. We have tested our system on all 22 SQL queries of the TPC-H database benchmark which we represented in XSLT and then translated back to SQL using our translator.	Translating XSLT programs to Efficient SQL queries	NA:NA:NA	2018
Alois Ferscha	NA	Session details: Mobility and Wireless Access	NA	2018
Doron Cohen:Michael Herscovici:Yael Petruschka:Yoëlle S. Maarek:Aya Soffer	In spite of the increase in the availability of mobile devices in the last few years, Web information is not yet as accessible from PDAs or WAP phones as it is from the desktop. In this paper, we propose a solution for supporting one of the most popular information discovery mechanisms, namely Web directory navigation, from mobile devices. Our proposed solution consists of caching enough information on the device itself in order to conduct most of the navigation actions locally (with subsecond response time) while intermittently communicating with the server to receive updates and additional data requested by the user. The cached information is captured in a "directory capsule". The directory capsule represents only the portion of the directory that is of interest to the user in a given context and is sufficiently rich and consistent to support the information needs of the user in disconnected mode. We define a novel subscription model specifically geared for Web directories and for the special needs of PDAs. This subscription model enables users to specify the parts of the directory that are of interest to them as well as the preferred granularity. We describe a mechanism for keeping the directory capsule in sync over time with the Web directory and user subscription requests. Finally, we present the Pocket Directory Browser for Palm powered computers that we have developed. The pocket directory can be used to define, view and manipulate the capsules that are stored on the Palm. We provide several usage examples of our system on the Open Directory Project, one of the largest and most popular Web directories.	Personalized pocket directories for mobile devices	NA:NA:NA:NA:NA	2018
Jesse Steinberg:Joseph Pasquale	We present a new Web middleware architecture that allows users to customize their view of the Web for optimal interaction and system operation when using non-traditional resource-limited client machines such as wireless PDAs (personal digital assistants). Web Stream Customizers (WSC) are dynamically deployable software modules and can be strategically located between client and server to achieve improvements in performance, reliability, or security. An important design feature is that Customizers provide two points of control in the communication path between client and server, supporting adaptive system-based and content-based customization. Our architecture exploits HTTP's proxy capabilities, allowing Customizers to be seamlessly integrated with the basic Web transaction model. We describe the WSC architecture and implementation, and illustrate its use with three non-trivial, adaptive Customizer applications that we have built. We show that the overhead in our implementation is small and tolerable, and is outweighed by the benefits that Customizers provide.	A web middleware architecture for dynamic customization of content for wireless clients	NA:NA	2018
Takeshi Yoshimura:Yoshifumi Yonemoto:Tomoyuki Ohya:Minoru Etoh:Susie Wee	In this paper, we present a mobile streaming media CDN (Content Delivery Network) architecture in which content segmentation, request routing, pre-fetch scheduling, and session handoff are controlled by SMIL (Synchronized Multimedia Integrated Language) modification. In this architecture, mobile clients simply follow modified SMIL files downloaded from a streaming portal server; these modifications enable multimedia content to be delivered to the mobile clients from the best surrogates in the CDN. The key components of this architecture are 1) content segmentation with SMIL modification, 2) on-demand rewriting of URLs in SMIL, 3) pre-fetch scheduling based on timing information derived from SMIL, 4) SMIL updates by SOAP (Simple Object Access Protocol) messaging for session handoffs due to clients mobility. We also introduce QoS control with a network agent called an "RTP monitoring agent" to enable appropriate control of media quality based on both network congestion and radio link conditions. The current status of our prototyping on a mobile QoS testbed "MOBIQ" is reported in this paper. We are currently designing the SOAP-based APIs (Application Programmable Interfaces) needed for the mobile streaming media CDN and building the CDN over the current testbed.	Mobile streaming media CDN enabled by dynamic SMIL	NA:NA:NA:NA:NA	2018
Vipul Kashyap	NA	Session details: Ontologies	NA	2018
AnHai Doan:Jayant Madhavan:Pedro Domingos:Alon Halevy	Ontologies play a prominent role on the Semantic Web. They make possible the widespread publication of machine understandable data, opening myriad opportunities for automated information processing. However, because of the Semantic Web's distributed nature, data on it will inevitably come from many different ontologies. Information processing across ontologies is not possible without knowing the semantic mappings between their elements. Manually finding such mappings is tedious, error-prone, and clearly not possible at the Web scale. Hence, the development of tools to assist in the ontology mapping process is crucial to the success of the Semantic Web.We describe glue, a system that employs machine learning techniques to find such mappings. Given two ontologies, for each concept in one ontology glue finds the most similar concept in the other ontology. We give well-founded probabilistic definitions to several practical similarity measures, and show that glue can work with all of them. This is in contrast to most existing approaches, which deal with a single similarity measure. Another key feature of glue is that it uses multiple learning strategies, each of which exploits a different type of information either in the data instances or in the taxonomic structure of the ontologies. To further improve matching accuracy, we extend glue to incorporate commonsense knowledge and domain constraints into the matching process. For this purpose, we show that relaxation labeling, a well-known constraint optimization technique used in computer vision and other fields, can be adapted to work efficiently in our context. Our approach is thus distinguished in that it works with a variety of well-defined similarity notions and that it efficiently incorporates multiple types of knowledge. We describe a set of experiments on several real-world domains, and show that glue proposes highly accurate semantic mappings.	Learning to map between ontologies on the semantic web	NA:NA:NA:NA	2018
Raphael Malyankar	This paper describes the process of constructing a markup language for maritime information from the starting point of ontology building. Ontology construction from source materials in the maritime information domain is outlined. The structure of the markup language is described in terms of XML schemas and DTDs. A prototype application that uses the markup language is also described.	Vocabulary development for markup languages: a case study with maritime information	NA	2018
Christian Fillies:Gay Wood-Albrecht:Frauke Weichardt	The is a new layer of the Internet that enables semantic representation of the contents of existing web pages. Using common ontologies, human users sketch out the most important facts in models that act as intelligent whiteboards. Once models are broadcasted to the Internet, new and intelligent search engines, "ambient" intelligent devices and agents would be able to exploit this knowledge network. [1].The main idea of SemTalk is to empower end users to contribute to the Semantic Web by offering an easy to use -based graphical editor to create RDF-like schema and workflows. Since the modeled data is found by Microsoft's SmartTags, users can benefit from these Semantic Webs as part of their daily work with other Microsoft Office products such as Word, Excel or Outlook.SemTalk's graphically configurable meta model also extends the functionality of the Visio modeling tool because it makes it easy to configure Visio to different modeling worlds such as Business Engineering and CASE methodologies but also to these features can be applied to any other Visio drawings.Ontology Project: Department-wide information modeling at the Credit Suisse Bank. Main emphasis was on linguistic standardization of terms. Based on a common central glossary, local knowledge management teams were able to develop specialized models for their decentralized departments. As part of the knowledge management process local glossaries were continually carried over into a common shared model.Business Process Management Project: Distributed process modeling of the Bausparkasse Deutscher Ring, a German financial institution. Several groups of students from the Technical University FH Brandenburg explored how to develop and apply an industry-specific Semantic Web to Business Process Modeling.	A pragmatic application of the semantic web using SemTalk	NA:NA:NA	2018
Marc Najork	NA	Session details: Browsing	NA	2018
Bing Liu:Kaidi Zhao:Lan Yi	The Web is increasingly becoming an important channel for conducting businesses, disseminating information, and communicating with people on a global scale. More and more companies, organizations, and individuals are publishing their information on the Web. With all this information publicly available, naturally companies and individuals want to find useful information from these Web pages. As an example, companies always want to know what their competitors are doing and what products and services they are offering. Knowing such information, the companies can learn from their competitors and/or design countermeasures to improve their own competitiveness. The ability to effectively find such business intelligence information is increasingly becoming crucial to the survival and growth of any company. Despite its importance, little work has been done in this area. In this paper, we propose a novel visualization technique to help the user find useful information from his/her competitors' Web site easily and quickly. It involves visualizing (with the help of a clustering system) the comparison of the user's Web site and the competitor's Web site to find similarities and differences between the sites. The visualization is such that with a single glance, the user is able to see the key similarities and differences of the two sites. He/she can then quickly focus on those interesting clusters and pages to browse the details. Experiment results and practical applications show that the technique is effective.	Visualizing web site comparisons	NA:NA:NA	2018
Corin R. Anderson:Eric Horvitz	Despite the connotation of the words "browsing" and "surfing," web usage often follows routine patterns of access. However, few mechanisms exist to assist users with these routine tasks; bookmarks or portal sites must be maintained manually and are insensitive to the user's browsing context. To fill this void, we designed and implemented the montage system. A web montage is an ensemble of links and content fused into a single view. Such a coalesced view can be presented to the user whenever he or she opens the browser or returns to the start page. We pose a number of hypotheses about how users would interact with such a system, and test these hypotheses with a fielded user study. Our findings support some design decisions, such as using browsing context to tailor the montage, raise questions about others, and point the way toward future work.	Web montage: a dynamic personalized start page	NA:NA	2018
Brian Eberman:Jerry Carter:Darren Meyer:David Goddeau	The OpenVXI is a portable open source based toolkit that interprets the VoiceXML dialog markup language. It is designed to serve as a framework for system integrators and platform vendors who want to incorporate VoiceXML into their platform. A first version of the toolkit was released in the winter of 2001, with a second version released in September of 2001. A number of companies and individuals have adopted the toolkit for their platforms. In this paper we discuss the architecture of the toolkit, the architectural issues involved with implementing a framework for VoiceXML, performance results with the OpenVXI, and future directions for the toolkit.	Building voiceXML browsers with openVXI	NA:NA:NA:NA	2018
Juliana Freire	NA	Session details: UI and Applications	NA	2018
Simon Lok:Steven K. Feiner:William M. Chiong:Yoav J. Hirsch	Network and server-centric computing paradigms are quickly returning to being the dominant methods by which we use computers. Web applications are so prevalent that the role of a PC today has been largely reduced to a terminal for running a client or viewer such as a Web browser. Implementers of network-centric applications typically rely on the limited capabilities of HTML, employing proprietary "plug ins" or transmitting the binary image of an entire application that will be executed on the client. Alternatively, implementers can develop without regard for remote use, requiring users who wish to run such applications on a remote server to rely on a system that creates a virtual frame buffer on the server, and transmits a copy of its raster image to the local client.We review some of the problems that these current approaches pose, and show how they can be solved by developing a distributed user interface toolkit. A distributed user interface toolkit applies techniques to the high level components of a toolkit that are similar to those used at a low level in the X Window System. As an example of this approach, we present RemoteJFC, a working distributed user interface toolkit that makes it possible to develop thin-client applications using a distributed version of the Java Foundation Classes.	A graphical user interface toolkit approach to thin-client computing	NA:NA:NA:NA	2018
Jay Budzik:Shannon Bradshaw:Xiaobin Fu:Kristian J. Hammond	We describe ongoing work on I2I, a system aimed at fostering opportunistic communication among users viewing or manipulating content on the Web and in productivity applications. Unlike previous work in which the URLs of Web resources are used to group users visiting the same resource, we present a more general framework for clustering work contexts to group users together that accounts for dynamic content and distributional properties of Web accesses which can limit the utility URL based systems. In addition, we describe a method for scaffolding asynchronous communication in the context of an ongoing task that takes into account the ephemeral nature of the location of content on the Web. The techniques we describe also nicely cover local files in progress, in addition to publicly available Web content. We present the results of several evaluations that indicate systems that use the techniques we employ may be more useful than systems that are strictly URL based.	Clustering for opportunistic communication	NA:NA:NA:NA	2018
Irwin King	NA	Session details: Information Retrieval	NA	2018
Monika Henzinger:Bay-Wei Chang:Brian Milch:Sergey Brin	Many daily activities present information in the form of a stream of text, and often people can benefit from additional information on the topic discussed. TV broadcast news can be treated as one such stream of text; in this paper we discuss finding news articles on the web that are relevant to news currently being broadcast.We evaluated a variety of algorithms for this problem, looking at the impact of inverse document frequency, stemming, compounds, history, and query length on the relevance and coverage of news articles returned in real time during a broadcast. We also evaluated several postprocessing techniques for improving the precision, including reranking using additional terms, reranking by document similarity, and filtering on document similarity. For the best algorithm, 84%-91% of the articles found were relevant, with at least 64% of the articles being on the exact topic of the broadcast. In addition, a relevant article was found for at least 70% of the topics.	Query-free news search	NA:NA:NA:NA	2018
Shipeng Yu:Deng Cai:Ji-Rong Wen:Wei-Ying Ma	In contrast to traditional document retrieval, a web page as a whole is not a good information unit to search because it often contains multiple topics and a lot of irrelevant information from navigation, decoration, and interaction part of the page. In this paper, we propose a VIsion-based Page Segmentation (VIPS) algorithm to detect the semantic content structure in a web page. Compared with simple DOM based segmentation method, our page segmentation scheme utilizes useful visual cues to obtain a better partition of a page at the semantic level. By using our VIPS algorithm to assist the selection of query expansion terms in pseudo-relevance feedback in web information retrieval, we achieve 27% performance improvement on Web Track dataset.	Improving pseudo-relevance feedback in web information retrieval using web page segmentation	NA:NA:NA:NA	2018
Ronny Lempel:Shlomo Moran	We study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53.	Predictive caching and prefetching of query results in search engines	NA:NA	2018
Amit Sheth	NA	Session details: Foundations of the semantic web	NA	2018
James Farrugia	Model-theoretic semantics is a formal account of the interpretations of legitimate expressions of a language. It is increasingly being used to provide Web markup languages with well-defined semantics. But a discussion of its roles and limitations for the Semantic Web has not yet received a coherent and detailed treatment. This paper takes the first steps towards such a treatment. The major result is an introductory explication of key ideas that are usually only implicit in existing accounts of semantics for the Web. References to more detailed accounts of these ideas are also provided. The benefit of this explication is increased awareness among Web users of some important issues inherent in using model-theoretic semantics for Web markup languages.	Model-theoretic semantics for the web	NA	2018
Ian Horrocks:Peter F. Patel-Schneider	The Semantic Web is vitally dependent on a formal meaning for the constructs of its languages. For Semantic Web languages to work well together their formal meanings must employ a common view (or thesis) of representation, otherwise it will not be possible to reconcile documents written in different languages. The thesis of representation underlying RDF and RDFS is particularly troublesome in this regard, as it has several unusual aspects, both semantic and syntactic. A more-standard thesis of representation would result in the ability to reuse existing results and tools in the Semantic Web.	Three theses of representation in the semantic web	NA:NA	2018
Benjamin N. Grosof:Ian Horrocks:Raphael Volz:Stefan Decker	We show how to interoperate, semantically and inferentially, between the leading Semantic Web approaches to rules (RuleML Logic Programs) and ontologies (OWL/DAML+OIL Description Logic) via analyzing their expressive intersection. To do so, we define a new intermediate knowledge representation (KR) contained within this intersection: Description Logic Programs (DLP), and the closely related Description Horn Logic (DHL) which is an expressive fragment of first-order logic (FOL). DLP provides a significant degree of expressiveness, substantially greater than the RDF-Schema fragment of Description Logic. We show how to perform DLP-fusion: the bidirectional translation of premises and inferences (including typical kinds of queries) from the DLP fragment of DL to LP, and vice versa from the DLP fragment of LP to DL. In particular, this translation enables one to "build rules on top of ontologies": it enables the rule KR to have access to DL ontological definitions for vocabulary primitives (e.g., predicates and individual constants) used by the rules. Conversely, the DLP-fusion technique likewise enables one to "build ontologies on top of rules": it enables ontological definitions to be supplemented by rules, or imported into DL from rules. It also enables available efficient LP inferencing algorithms/implementations to be exploited for reasoning over large-scale DL ontologies.	Description logic programs: combining logic programs with description logic	NA:NA:NA:NA	2018
Johan Hjelm	NA	Session details: Mobility & wireless access	NA	2018
Siu-Nam Chuang:Alvin T.S. Chan:Jiannong Cao:Ronnie Cheung	This paper describes a dynamic service reconfiguration model where the proxy is composed of a chain of service objects called mobilets (pronounced as mo-be-lets), which can be deployed onto the network actively. This model offers flexibility because the chain of mobilets can be dynamically reconfigured to adapt to the vigorous changes in the characteristics of the wireless environment, without interrupting the service provision for other mobile nodes. Furthermore, mobilets can also be migrated to a new proxy server when the mobile node moves to a different network domain. We have realized the dynamic service reconfiguration model by crafting its design into a programmable infrastructure that forms the baseline architecture of the WebPADS (short for Web Proxy for Actively Deployable Services) system.	Dynamic service reconfiguration for wireless web access	NA:NA:NA:NA	2018
S. Jae Yang:Jason Nieh:Shilpa Krishnappa:Aparna Mohla:Mahdi Sajjadpour	Web applications are becoming increasingly popular for mobile wireless systems. However, wireless networks can have high packet loss rates, which can degrade web browsing performance on wireless systems. An alternative approach is wireless thin-client computing, in which the web browser runs on a remote thin server with a more reliable wired connection to the Internet. A mobile client then maintains a connection to the thin server to receive display updates over the lossy wireless network. To assess the viability of this thin-client approach, we compare the web browsing performance of thin clients against fat clients that run the web browser locally in lossy wireless networks. Our results show that thin clients can operate quite effectively over lossy networks. Compared to fat clients running web browsers locally, our results show surprisingly that thin clients can be faster and more resilient on web applications over lossy wireless LANs despite having to send more data over the network. We characterize and analyze different design choices in various thin-client systems and explain why these approaches can yield superior web browsing performance in lossy wireless networks.	Web browsing performance of wireless thin-client computing	NA:NA:NA:NA:NA	2018
John Barton:Tim Kindberg:Hui Dai:Nissanka B. Priyantha:Fahd Al-bin-ali	This paper describes methods for service selection and service access for mobile, sensor-enhanced web clients such as wireless cameras or wireless PDAs with sensor devices attached. The clients announce their data-creating capabilities in "Produce" headers sent to servers; servers respond with forms that match these capabilities. Clients fill in these forms with sensor data as well as text or file data. The resultant system enables clients to access dynamically discovered services spontaneously, as their users engage in everyday nomadic activities.	Sensor-enhanced mobile web clients: an XForms approach	NA:NA:NA:NA:NA	2018
Monika Henzinger	NA	Session details: Information retrieval 2	NA	2018
Luis Gravano:Panagiotis G. Ipeirotis:Nick Koudas:Divesh Srivastava	The integration of data produced and collected across autonomous, heterogeneous web services is an increasingly important and challenging problem. Due to the lack of global identifiers, the same entity (e.g., a product) might have different textual representations across databases. Textual data is also often noisy because of transcription errors, incomplete information, and lack of standard formats. A fundamental task during data integration is matching of strings that refer to the same entity. In this paper, we adopt the widely used and established cosine similarity metric from the information retrieval field in order to identify potential string matches across web sources. We then use this similarity metric to characterize this key aspect of data integration as a join between relations on textual attributes, where the similarity of matches exceeds a specified threshold. Computing an exact answer to the text join can be expensive. For query processing efficiency, we propose a sampling-based join approximation strategy for execution in a standard, unmodified relational database management system (RDBMS), since more and more web sites are powered by RDBMSs with a web-based front end. We implement the join inside an RDBMS, using SQL queries, for scalability and robustness reasons. Finally, we present a detailed performance evaluation of an implementation of our algorithm within a commercial RDBMS, using real-life data sets. Our experimental results demonstrate the efficiency and accuracy of our techniques.	Text joins in an RDBMS for web data integration	NA:NA:NA:NA	2018
Lipyeow Lim:Min Wang:Sriram Padmanabhan:Jeffrey Scott Vitter:Ramesh Agarwal	Recent work on incremental crawling has enabled the indexed document collection of a search engine to be more synchronized with the changing World Wide Web. However, this synchronized collection is not immediately searchable, because the keyword index is rebuilt from scratch less frequently than the collection can be refreshed. An inverted index is usually used to index documents crawled from the web. Complete index rebuild at high frequency is expensive. Previous work on incremental inverted index updates have been restricted to adding and removing documents. Updating the inverted index for previously indexed documents that have changed has not been addressed.In this paper, we propose an efficient method to update the inverted index for previously indexed documents whose contents have changed. Our method uses the idea of landmarks together with the diff algorithm to significantly reduce the number of postings in the inverted index that need to be updated. Our experiments verify that our landmark-diff method results in significant savings in the number of update operations on the inverted index.	Dynamic maintenance of web indexes using landmarks	NA:NA:NA:NA:NA	2018
Jussi Myllymaki:James Kaufman	Much attention has been accorded to Location-Based Services and location tracking, a necessary component in active, trigger-based LBS applications. Tracking the location of a large population of moving objects requires very high update and query performance of the underlying spatial index. In this paper we investigate the performance and scalability of three main-memory based spatial indexing methods under dynamic update and query loads: an R-tree, a ZB-tree, and an array/hashtable method. By leveraging the LOCUS performance evaluation testbed and the City Simulator dynamic spatial data generator, we are able to demonstrate the scalability of these methods and determine the maximum population size supported by each method, a useful parameter for capacity planning by wireless carriers.	High-performance spatial indexing for location-based services	NA:NA	2018
Mike Dahlin	NA	Session details: Provisioning	NA	2018
Z. Morley Mao:David Johnson:Oliver Spatscheck:Jacobus E. van der Merwe:Jia Wang	Today, most large companies maintain virtual private networks (VPNs) to connect their remote locations into a single secure network. VPNs can be quite large covering more than 1000 locations and in most cases use standard Internet protocols and services. Such VPNs are implemented using a diverse set of technologies such as Frame Relay, MPLS, or IPSEC to achieve the goal of privacy and performance isolation from the public Internet.Using VPNs to distribute live content has recently received tremendous interest. For example, a VPN could be used to broadcast a CEO-employee town hall meeting. To distribute this type of content economically without overloading the network, the deployment of streaming caches or splitters is most likely required.In this paper, we address the problem of optimally placing such streaming splitters or caches to broadcast to a given set of VPN endpoints under the constraints typically found within a VPN. In particular, we introduce an efficient algorithm with complexity O(V), V being the number of routers in the VPN. This guarantees the optimal cache placement if interception is used for redirection. We prove that the general problem is NP-hard and introduce multiple heuristics for efficient and robust cache placement suitable under different constraints. At the expense of increased implementation complexity, each heuristic solution provides additional saving in the number of caches required. We evaluate proposed solutions using extensive simulations. In particular, we show our flow-based solution is very close to the optimal.	Efficient and robust streaming provisioning in VPNs	NA:NA:NA:NA:NA	2018
Akshat Verma:Sugata Ghosal	Variability and diverseness among incoming requests to a service hosted on a finite capacity resource necessitates sophisticated request admission control techniques for providing guaranteed quality of service (QoS). We propose in this paper a service time based online admission control methodology for maximizing profits of a service provider. The proposed methodology chooses a subset of incoming requests such that the revenue of the provider is maximized. Admission control decision in our proposed system is based upon an estimate of the service time of the request, QoS bounds, prediction of arrivals and service times of requests to come in the short-term future, and rewards associated with servicing a request within its QoS bounds. Effectiveness of the proposed admission control methodology is demonstrated using experiments with a content-based messaging middleware service.	On admission control for profit maximization of networked service providers	NA:NA	2018
Balachander Krishnamurthy:Yin Zhang:Craig E. Wills:Kashi Vishwanath	In earlier work we proposed a way for a Web server to detect connectivity information about clients accessing it in order to take tailored actions for a client request. This paper describes the design, implementation, and evaluation of such a working system. A Web site has a strong incentive to reduce the 'time-to-glass' to retain users who may otherwise lose interest and leave the site. We have performed a measurement study from multiple client sites around the world with various levels of connectivity to the Internet communicating with modified Apache Web servers under our control. The results show that clients can be classified in a correct and stable manner and that user-perceived latency can be reduced via tailored actions. Our measurements show that classification and determination of server actions are done without significant overhead on the Web server. We explore a variety of modified actions ranging from selecting a lower quality version of the resource to altering the manner of content delivery. By studying numerous performance related factors in a single unified framework and examining both individual actions as well as combination of actions, our modified Web server implementation shows the efficacy of various server actions.	Design, implementation, and evaluation of a client characterization driven web server	NA:NA:NA:NA	2018
TBA	NA	Session details: Data integrity	NA	2018
Yao-Wen Huang:Shih-Kun Huang:Tsung-Po Lin:Chung-Hung Tsai	As a large and complex application platform, the World Wide Web is capable of delivering a broad range of sophisticated applications. However, many Web applications go through rapid development phases with extremely short turnaround time, making it difficult to eliminate vulnerabilities. Here we analyze the design of Web application security assessment mechanisms in order to identify poor coding practices that render Web applications vulnerable to attacks such as SQL injection and cross-site scripting. We describe the use of a number of software-testing techniques (including dynamic analysis, black-box testing, fault injection, and behavior monitoring), and suggest mechanisms for applying these techniques to Web applications. Real-world situations are used to test a tool we named the Web Application Vulnerability and Error Scanner (WAVES, an open-source project available at http://waves.sourceforge.net) and to compare it with other tools. Our results show that WAVES is a feasible platform for assessing Web application security.	Web application security assessment by fault injection and behavior monitoring	NA:NA:NA:NA	2018
Marco Casassa Mont:Keith Harrison:Martin Sadler	Digital information is increasingly more and more important to enable interactions and transactions on the Internet. On the other hand, leakages of sensitive information can have harmful effects for people, enterprises and governments.This paper focuses on the problems of dealing with timed release of confidential information and simplifying its access once public: it is a common issue in the industry, government and day-to-day life.We introduce the "HP Time Vault Service", based on the emerging Identifier-based Encryption (IBE) cryptography schema. IBE (public) encryption keys specify the disclosure time. These keys are used to encrypt confidential information. An independent time server generates and publishes IBE decryption keys correspondent to the current time, at predefined intervals.We discuss the advantages of this approach against current approaches based on traditional cryptography. A web-service based prototype is described, as a proof of concept.	The HP time vault service: exploiting IBE for timed release of confidential information	NA:NA:NA	2018
Laurence Bull:Peter Stanski:David McG. Squire	Content Extraction Signatures (CES) enable selective disclosure of verifiable content, provide privacy for blinded content, and enable the signer to specify the content the document owner is allowed to extract or blind. Combined, these properties give what we call CES functionality. In this paper we describe our work in developing custom transform algorithms to expand the functionality of an XML Signature to include CES functionality in XML Signature Core Validation.We also describe a custom revocation mechanism and our implementation for non-XML content where the custom transforms are dynamically loaded demonstrating that custom signing and verification is not constrained to a 'closed system'. Through the use of dynamic loading we show that a verifier can still verify an XML Signature-compliant signature even though a custom signature was produced.	Content extraction signatures using XML digital signatures and custom transforms on-demand	NA:NA:NA	2018
Ian Horrocks	NA	Session details: Establishing the semantic web 1	NA	2018
Stephen Dill:Nadav Eiron:David Gibson:Daniel Gruhl:R. Guha:Anant Jhingran:Tapas Kanungo:Sridhar Rajagopalan:Andrew Tomkins:John A. Tomlin:Jason Y. Zien	This paper describes Seeker, a platform for large-scale text analytics, and SemTag, an application written on the platform to perform automated semantic tagging of large corpora. We apply SemTag to a collection of approximately 264 million web pages, and generate approximately 434 million automatically disambiguated semantic tags, published to the web as a label bureau providing metadata regarding the 434 million annotations. To our knowledge, this is the largest scale semantic tagging effort to date.We describe the Seeker platform, discuss the architecture of the SemTag application, describe a new disambiguation algorithm specialized to support ontological disambiguation of large-scale data, evaluate the algorithm, and present our final results with information about acquiring and making use of the semantic tags. We argue that automated large scale semantic tagging of ambiguous content can bootstrap and accelerate the creation of the semantic web.	SemTag and seeker: bootstrapping the semantic web via automated semantic annotation	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Jiying Wang:Fred H. Lochovsky	Many tools have been developed to help users query, extract and integrate data from web pages generated dynamically from databases, i.e., from the Hidden Web. A key prerequisite for such tools is to obtain the schema of the attributes of the retrieved data. In this paper, we describe a system called, DeLa, which reconstructs (part of) a "hidden" back-end web database. It does this by sending queries through HTML forms, automatically generating regular expression wrappers to extract data objects from the result pages and restoring the retrieved data into an annotated (labelled) table. The whole process needs no human involvement and proves to be fast (less than one minute for wrapper induction for each site) and accurate (over 90% correctness for data extraction and around 80% correctness for label assignment).	Data extraction and label assignment for web databases	NA:NA	2018
Karl Aberer:Philippe Cudré-Mauroux:Manfred Hauswirth	This paper describes a novel approach for obtaining semantic interoperability among data sources in a bottom-up, semi-automatic manner without relying on pre-existing, global semantic models. We assume that large amounts of data exist that have been organized and annotated according to local schemas. Seeing semantics as a form of agreement, our approach enables the participating data sources to incrementally develop global agreement in an evolutionary and completely decentralized process that solely relies on pair-wise, local interactions: Participants provide translations between schemas they are interested in and can learn about other translations by routing queries (gossiping). To support the participants in assessing the semantic quality of the achieved agreements we develop a formal framework that takes into account both syntactic and semantic criteria. The assessment process is incremental and the quality ratings are adjusted along with the operation of the system. Ultimately, this process results in global agreement, i.e., the semantics that all participants understand. We discuss strategies to efficiently find translations and provide results from a case study to justify our claims. Our approach applies to any system which provides a communication infrastructure (existing websites or databases, decentralized systems, P2P systems) and offers the opportunity to study semantic interoperability as a global phenomenon in a network of information sharing parties.	The chatty web: emergent semantics through gossiping	NA:NA:NA	2018
Marc Najork	NA	Session details: Adapting content to mobile devices	NA	2018
Suhit Gupta:Gail Kaiser:David Neistadt:Peter Grimm	Web pages often contain clutter (such as pop-up ads, unnecessary images and extraneous links) around the body of an article that distracts a user from actual content. Extraction of "useful and relevant" content from web pages has many applications, including cell phone and PDA browsing, speech rendering for the visually impaired, and text summarization. Most approaches to removing clutter or making content more readable involve changing font size or removing HTML and data components such as images, which takes away from a webpage's inherent look and feel. Unlike "Content Reformatting", which aims to reproduce the entire webpage in a more convenient form, our solution directly addresses "Content Extraction". We have developed a framework that employs easily extensible set of techniques that incorporate advantages of previous work on content extraction. Our key insight is to work with the DOM trees, rather than with raw HTML markup. We have implemented our approach in a publicly available Web proxy to extract content from HTML web pages.	DOM-based content extraction of HTML documents	NA:NA:NA:NA	2018
Christopher C. Yang:Fu Lee Wang	Wireless access with mobile (or handheld) devices is a promising addition to the WWW and traditional electronic business. Mobile devices provide convenience and portable access to the huge information space on the Internet without requiring users to be stationary with network connection. However, the limited screen size, narrow network bandwidth, small memory capacity and low computing power are the shortcomings of handheld devices. Loading and visualizing large documents on handheld devices become impossible. The limited resolution restricts the amount of information to be displayed. The download time is intolerably long. In this paper, we introduce the fractal summarization model for document summarization on handheld devices. Fractal summarization is developed based on the fractal theory. It generates a brief skeleton of summary at the first stage, and the details of the summary on different levels of the document are generated on demands of users. Such interactive summarization reduces the computation load in comparing with the generation of the entire summary in one batch by the traditional automatic summarization, which is ideal for wireless access. Three-tier architecture with the middle-tier conducting the major computation is also discussed. Visualization of summary on handheld devices is also investigated.	Fractal summarization for mobile devices to access large documents on the web	NA:NA	2018
Yu Chen:Wei-Ying Ma:Hong-Jiang Zhang	Mobile devices have already been widely used to access the Web. However, because most available web pages are designed for desktop PC in mind, it is inconvenient to browse these large web pages on a mobile device with a small screen. In this paper, we propose a new browsing convention to facilitate navigation and reading on a small-form-factor device. A web page is organized into a two level hierarchy with a thumbnail representation at the top level for providing a global view and index to a set of sub-pages at the bottom level for detail information. A page adaptation technique is also developed to analyze the structure of an existing web page and split it into small and logically related units that fit into the screen of a mobile device. For a web page not suitable for splitting, auto-positioning or scrolling-by-block is used to assist the browsing as an alterative. Our experimental results show that our proposed browsing convention and developed page adaptation scheme greatly improve the user's browsing experiences on a device with a small display.	Detecting web page structure for adaptive viewing on small form factor devices	NA:NA:NA	2018
TBA	NA	Session details: Writing the web	NA	2018
Timothy Miles-Board:Leslie Carr:Simon Kampa:Wendy Hall	The World-Wide Web was originally developed as a shared, writable, hypertext medium, a facility that is still widely needed.We have recently developed a Web-based management reporting system for a legal firm in an attempt to improve the efficiency and management of their overall business process. This paper shares our experiences in relating the firm's specific writing and issue tracking tasks to existing Web, open hypermedia, and Semantic Web research, and describes why we chose to develop a new solution --- a set of open hypermedia components collectively called the Management Reporting System --- rather than employ an existing system.	Supporting management reporting: a writable web case study	NA:NA:NA:NA	2018
Victoria Uren:Simon Buckingham Shum:Gangmin Li:John Domingue:Enrico Motta	The World Wide Web is opening up access to documents and data for scholars. However it has not yet impacted on one of the primary activities in research: assessing new findings in the light of current knowledge and debating it with colleagues. The ClaiMaker system uses a directed graph model with similarities to hypertext, in which new ideas are published as nodes, which other contributors can build on or challenge in a variety of ways by linking to them. Nodes and links have semantic structure to facilitate the provision of specialist services for interrogating and visualizing the emerging network. By way of example, this paper is grounded in a ClaiMaker model to illustrate how new claims can be described in this structured way.	Scholarly publishing and argument in hyperspace	NA:NA:NA:NA:NA	2018
Bing Liu:Chee Wee Chin:Hwee Tou Ng	Traditionally, when one wants to learn about a particular topic, one reads a book or a survey paper. With the rapid expansion of the Web, learning in-depth knowledge about a topic from the Web is becoming increasingly important and popular. This is also due to the Web's convenience and its richness of information. In many cases, learning from the Web may even be essential because in our fast changing world, emerging topics appear constantly and rapidly. There is often not enough time for someone to write a book on such topics. To learn such emerging topics, one can resort to research papers. However, research papers are often hard to understand by non-researchers, and few research papers cover every aspect of the topic. In contrast, many Web pages often contain intuitive descriptions of the topic. To find such Web pages, one typically uses a search engine. However, current search techniques are not designed for in-depth learning. Top ranking pages from a search engine may not contain any description of the topic. Even if they do, the description is usually incomplete since it is unlikely that the owner of the page has good knowledge of every aspect of the topic. In this paper, we attempt a novel and challenging task, mining topic-specific knowledge on the Web. Our goal is to help people learn in-depth knowledge of a topic systematically on the Web. The proposed techniques first identify those sub-topics or salient concepts of the topic, and then find and organize those informative pages, containing definitions and descriptions of the topic and sub-topics, just like those in a book. Experimental results using 28 topics show that the proposed techniques are highly effective.	Mining topic-specific concepts and definitions on the web	NA:NA:NA	2018
Soumen Chakrabarti	NA	Session details: Link-based ranking 1	NA	2018
Sepandar D. Kamvar:Taher H. Haveliwala:Christopher D. Manning:Gene H. Golub	We present a novel algorithm for the fast computation of PageRank, a hyperlink-based estimate of the ''importance'' of Web pages. The original PageRank algorithm uses the Power Method to compute successive iterates that converge to the principal eigenvector of the Markov matrix representing the Web link graph. The algorithm presented here, called Quadratic Extrapolation, accelerates the convergence of the Power Method by periodically subtracting off estimates of the nonprincipal eigenvectors from the current iterate of the Power Method. In Quadratic Extrapolation, we take advantage of the fact that the first eigenvalue of a Markov matrix is known to be 1 to compute the nonprincipal eigenvectors using successive iterates of the Power Method. Empirically, we show that using Quadratic Extrapolation speeds up PageRank computation by 25-300% on a Web graph of 80 million nodes, with minimal overhead. Our contribution is useful to the PageRank community and the numerical linear algebra community in general, as it is a fast method for determining the dominant eigenvector of a matrix that is too large for standard fast methods to be practical.	Extrapolation methods for accelerating PageRank computations	NA:NA:NA:NA	2018
Glen Jeh:Jennifer Widom	Recent web search techniques augment traditional text matching with a global notion of "importance" based on the linkage structure of the web, such as in Google's PageRank algorithm. For more refined searches, this global notion of importance can be specialized to create personalized views of importance--for example, importance scores can be biased according to a user-specified set of initially-interesting pages. Computing and storing all possible personalized views in advance is impractical, as is computing personalized views at query time, since the computation of each view requires an iterative computation over the web graph. We present new graph-theoretical results, and a new technique based on these results, that encode personalized views as partial vectors. Partial vectors are shared across multiple personalized views, and their computation and storage costs scale well with the number of views. Our approach enables incremental computation, so that the construction of personalized views from partial vectors is practical at query time. We present efficient dynamic programming algorithms for computing partial vectors, an algorithm for constructing personalized views from partial vectors, and experimental results demonstrating the effectiveness and scalability of our techniques.	Scaling personalized web search	NA:NA	2018
Serge Abiteboul:Mihai Preda:Gregory Cobena	The computation of page importance in a huge dynamic graph has recently attracted a lot of attention because of the web. Page importance, or page rank is defined as the fixpoint of a matrix equation. Previous algorithms compute it off-line and require the use of a lot of extra CPU as well as disk resources (e.g. to store, maintain and read the link matrix). We introduce a new algorithm OPIC that works on-line, and uses much less resources. In particular, it does not require storing the link matrix. It is on-line in that it continuously refines its estimate of page importance while the web/graph is visited. Thus it can be used to focus crawling to the most interesting pages. We prove the correctness of OPIC. We present Adaptive OPIC that also works on-line but adapts dynamically to changes of the web. A variant of this algorithm is now used by Xyleme.We report on experiments with synthetic data. In particular, we study the convergence and adaptiveness of the algorithms for various scheduling strategies for the pages to visit. We also report on experiments based on crawls of significant portions of the web.	Adaptive on-line page importance computation	NA:NA:NA	2018
Philipp Hoschka	NA	Session details: Applications and architecture	NA	2018
Rajan M. Lukose:Eytan Adar:Joshua R. Tyler:Caesar Sengupta	A computationally enhanced message contains some embedded programmatic components that are interpreted and executed automatically upon receipt. Unlike ordinary text email or instant messages, they make possible a number of useful applications. In this paper, we describe a general and flexible messaging system called SHOCK that extends the functionality of prior computational email systems by allowing XML-encoded SHOCK messages to interact with an automatically created profile of a user. These profiles consist of information about the most common tasks users perform, such as their Web browsing behavior, their conventional email usage, etc. Since users are sensitive about such data, the system is designed with privacy as a central design goal, and employs a distributed peer-to-peer architecture to achieve it. The system is largely implemented with commodity Web technologies and provides both a Web interface as well as one that is tightly integrated with users ordinary email clients. With SHOCK, users can send highly targeted messages without violating others privacy, and engage in structured conversation appropriate to the context without disrupting their existing work practices. We describe our implementation in detail, the most useful novel applications of the system, and our experiences with the system in a pilot field test.	SHOCK: communicating with computational messages and automatic private profiles	NA:NA:NA:NA	2018
Yang Guo:Kyoungwon Suh:Jim Kurose:Don Towsley	Providing video on demand (VoD) service over the Internet in a scalable way is a challenging problem. In this paper, we propose P2Cast - an architecture that uses a peer-to-peer approach to cooperatively stream video using patching techniques, while only relying on unicast connections among peers. We address the following two key technical issues in P2Cast: (1) constructing an application overlay appropriate for streaming; and (2) providing continuous stream playback (without glitches) in the face of disruption from an early departing client. Our simulation experiments show that P2Cast can serve many more clients than traditional client-server unicast service, and that it generally out-performs multicast-based patching if clients can cache more than of a stream's initial portion. We handle disruptions by delaying the start of playback and applying the shifted forwarding technique. A threshold on the length of time during which arriving clients are served in a single session in P2Cast serves as a knob to adjust the balance between the scalability and the clients' viewing quality in P2Cast.	P2Cast: peer-to-peer patching scheme for VoD service	NA:NA:NA:NA	2018
Balachander Krishnamurthy:Richard Liston:Michael Rabinovich	With a key component of latency on the Web being connection set up between clients and Web servers, several ways to avoid connections have been explored. While the work in recent years on Content Distribution Networks (CDNs) have moved some content 'closer' to users at the cost of increasing DNS traffic, they have not fully exploited the available unused potential of existing protocols. We explore ways by which a variety of Web responses can be piggybacked on DNS messages. While we evaluated our idea in the Web context, the approach is generic and not restricted to Web responses. We propose an architecture for HTTP piggybacking in DNS messages and carry out a detailed performance analysis based on a trace-driven simulation study. Our architecture requires minimal extensions to existing protocols, utilizing only the allowed optional fields for these extensions. It is fully compatible and can coexist with the current Web.	DEW: DNS-enhanced web for faster content delivery	NA:NA:NA	2018
TBA	NA	Session details: E-commerce	NA	2018
Tommaso Di Noia:Eugenio Di Sciascio:Francesco M. Donini:Marina Mongiello	More and more resources are becoming available on the Web, and there is a growing need for infrastructures that, based on advertised descriptions, are able to semantically match demands with supplies.We formalize general properties a matchmaker should have, then we present a matchmaking facilitator, compliant with desired properties.The system embeds a NeoClassic reasoner, whose structural subsumption algorithm has been modified to allow match categorization into potential and partial, and ranking of matches within categories. Experiments carried out show the good correspondence between users and system rankings.	A system for principled matchmaking in an electronic marketplace	NA:NA:NA:NA	2018
Lei Li:Ian Horrocks	An important objective of the Semantic Web is to make Electronic Commerce interactions more flexible and automated. To achieve this, standardization of ontologies, message content and message protocols will be necessary.In this paper we investigate how Semantic and Web Services technologies can be used to support service advertisement and discovery in e-commerce. In particular, we describe the design and implementation of a service matchmaking prototype which uses a DAML-S based ontology and a Description Logic reasoner to compare ontology based service descriptions. We also present the results of initial experiments testing the performance of this prototype implementation in a realistic agent based e-commerce scenario.	A software framework for matchmaking based on semantic web technology	NA:NA	2018
Benjamin N. Grosof:Terrence C. Poon	SweetDeal is a rule-based approach to representation of business contracts that enables software agents to create, evaluate, negotiate, and execute contracts with substantial automation and modularity. It builds upon the situated courteous logic programs knowledge representation in RuleML, the emerging standard for Semantic Web XML rules. Here, we newly extend the SweetDeal approach by also incorporating process knowledge descriptions whose ontologies are represented in DAML+OIL (emerging standard for Semantic Web ontologies) thereby enabling more complex contracts with behavioral provisions, especially for handling exception conditions (e.g., late delivery or non-payment) that might arise during the execution of the contract. This provides a foundation for representing and automating deals about services -- in particular, about Web Services, so as to help search, select, and compose them. Our system is also the first to combine emerging Semantic Web standards for knowledge representation of rules (RuleML) with ontologies (DAML+OIL) for a practical e-business application domain, and further to do so with process knowledge. This also newly fleshes out the evolving concept of Semantic Web Services. A prototype (soon public) is running.	SweetDeal: representing agent contracts with exceptions using XML rules, ontologies, and process descriptions	NA:NA	2018
Peter Anick	NA	Session details: Link-based ranking 2	NA	2018
John A. Tomlin	This paper describes a new paradigm for modeling traffic levels on the world wide web (WWW) using a method of entropy maximization. This traffic is subject to the conservation conditions of a circulation flow in the entire WWW, an aggregation of the WWW, or a subgraph of the WWW (such as an intranet or extranet). We specifically apply the primal and dual solutions of this model to the (static) ranking of web sites. The first of these uses an imputed measure of total traffic through a web page, the second provides an analogy of local "temperature", allowing us to quantify the "HOTness" of a page.	A new paradigm for ranking pages on the world wide web	NA	2018
Ah Chung Tsoi:Gianni Morini:Franco Scarselli:Markus Hagenbuchner:Marco Maggini	In this paper, we consider the possibility of altering the PageRank of web pages, from an administrator's point of view, through the modification of the PageRank equation. It is shown that this problem can be solved using the traditional quadratic programming techniques. In addition, it is shown that the number of parameters can be reduced by clustering web pages together through simple clustering techniques. This problem can be formulated and solved using quadratic programming techniques. It is demonstrated experimentally on a relatively large web data set, viz., the WT10G, that it is possible to modify the PageRanks of the web pages through the proposed method using a set of linear constraints. It is also shown that the PageRank of other pages may be affected; and that the quality of the result depends on the clustering technique used. It is shown that our results compared well with those obtained by a HITS based method.	Adaptive ranking of web pages	NA:NA:NA:NA:NA	2018
Ronald Fagin:Ravi Kumar:Kevin S. McCurley:Jasmine Novak:D. Sivakumar:John A. Tomlin:David P. Williamson	The social impact from the World Wide Web cannot be underestimated, but technologies used to build the Web are also revolutionizing the sharing of business and government information within intranets. In many ways the lessons learned from the Internet carry over directly to intranets, but others do not apply. In particular, the social forces that guide the development of intranets are quite different, and the determination of a "good answer" for intranet search is quite different than on the Internet. In this paper we study the problem of intranet search. Our approach focuses on the use of rank aggregation, and allows us to examine the effects of different heuristics on ranking of search results.	Searching the workplace web	NA:NA:NA:NA:NA:NA:NA	2018
Wei-Ying Ma	NA	Session details: Multimedia	NA	2018
Cheng Yang	In traditional peer-to-peer search networks, operations focus on properly labeled files such as music or video, and the actual search is often limited to text tags. The explosive growth of available multimedia documents in recent years calls for more flexible search capabilities, namely search by content. Most content-based search algorithms are computationally intensive, making them inappropriate for a peer-to-peer environment. In this paper, we discuss a content-based music retrieval algorithm that can be decomposed and parallelized efficiently. We present a peer-to-peer architecture for such a system that makes use of spare resources among subscribers, with protocols that dynamically redistribute load in order to maximize throughput and minimize inconvenience to subscribers. Our framework can be extended beyond the music retrieval domain and adapted to other scenarios where resource pooling is desired, as long as the underlying algorithm satisfies certain conditions.	Peer-to-peer architecture for content-based music retrieval on acoustic data	NA	2018
Jacco van Ossenbruggen:Lynda Hardma:Joost Geurts:Lloyd Rutledge	Time-based, media-centric Web presentations can be described declaratively in the XML world through the development of languages such as SMIL. It is difficult, however, to fully integrate them in a complete document transformation processing chain. In order to achieve the desired processing of data-driven, time-based, media-centric presentations, the text-flow based formatting vocabularies used by style languages such as XSL, CSS and DSSSL need to be extended. The paper presents a selection of use cases which are used to derive a list of requirements for a multimedia style and transformation formatting vocabulary. The boundaries of applicability of existing text-based formatting models for media-centric transformations are analyzed. The paper then discusses the advantages and disadvantages of a fully-fledged time-based multimedia formatting model. Finally, the discussion is illustrated by describing the key properties of the example multimedia formatting vocabulary currently implemented in the back-end of our Cuypers multimedia transformation engine.	Towards a multimedia formatting vocabulary	NA:NA:NA:NA	2018
Peter Schojer:Laszlo Böszörmenyi:Hermann Hellwagner:Bernhard Penz:Stefan Podlipnig	Due to the increasing availability and use of digital video data on the Web, video caching will be an important performance factor in the future WWW. We propose an architecture of a video proxy cache that integrates modern multimedia and communication standards. Especially we describe features of the MPEG-4 and MPEG-7 multimedia standards that can be helpful for a video proxy cache. QBIX supports real-time adaptation in the compressed and in the decompressed domain. It uses adaptation to improve the cache replacement strategies in the proxy, but also to realize media gateway functionality driven by the clients' terminal capabilities.	Architecture of a quality based intelligent proxy (QBIX) for MPEG-4 videos	NA:NA:NA:NA:NA	2018
Martin Gaedke	NA	Session details: Web engineering	NA	2018
Tevfik Bultan:Xiang Fu:Richard Hull:Jianwen Su	This paper introduces a framework for modeling and specifying the global behavior of e-service compositions. Under this framework, peers (individual e-services) communicate through asynchronous messages and each peer maintains a queue for incoming messages. A global "watcher" keeps track of messages as they occur. We propose and study a central notion of a "conversation", which is a sequence of (classes of) messages observed by the watcher. We consider the case where the peers are represented by Mealy machines (finite state machines with input and output). The sets of conversations exhibit unexpected behaviors. For example, there exists a composite e-service based on Mealy peers whose set of conversations is not context free (and not regular). (The set of conversations is always context sensitive.) One cause for this is the queuing of messages; we introduce an operator "prepone" that simulates queue delays from a global perspective and show that the set of conversations of each Mealy e-service is closed under prepone. We illustrate that the global prepone fails to completely capture the queue delay effects and refine prepone to a "local" version on conversations seen by individual peers. On the other hand, Mealy implementations of a composite e-service will always generate conversations whose "projections" are consistent with individual e-services. We use projection-join to reflect such situations. However, there are still Mealy peers whose set of conversations is not the local prepone and projection-join closure of any regular language. Therefore, we propose conversation specifications as a formalism to define the conversations allowed by an e-service composition. We give two technical results concerning the interplay between the local behaviors of Mealy peers and the global behaviors of their compositions. One result shows that for each regular language, its local prepone and projection-join closure corresponds to the set of conversations by some Mealy peers effectively constructed from . The second result gives a condition on the shape of a composition which guarantees that the set of conversations that can be realized is the local prepone and projection-join closure of a regular language.	Conversation specification: a new approach to design and analysis of e-service composition	NA:NA:NA:NA	2018
Liangzhao Zeng:Boualem Benatallah:Marlon Dumas:Jayant Kalagnanam:Quan Z. Sheng	The process-driven composition of Web services is emerging as a promising approach to integrate business applications within and across organizational boundaries. In this approach, individual Web services are federated into composite Web services whose business logic is expressed as a process model. The tasks of this process model are essentially invocations to functionalities offered by the underlying component services. Usually, several component services are able to execute a given task, although with different levels of pricing and quality. In this paper, we advocate that the selection of component services should be carried out during the execution of a composite service, rather than at design-time. In addition, this selection should consider multiple criteria (e.g., price, duration, reliability), and it should take into account global constraints and preferences set by the user (e.g., budget constraints). Accordingly, the paper proposes a global planning approach to optimally select component services during the execution of a composite service. Service selection is formulated as an optimization problem which can be solved using efficient linear programming methods. Experimental results show that this global planning approach outperforms approaches in which the component services are selected individually for each task in a composite service.	Quality driven web services composition	NA:NA:NA:NA:NA	2018
Yeliz Yesilada:Robert Stevens:Carole Goble	Users make journeys through the Web. Web travel encompasses the tasks of orientation and navigation, the environment and the purpose of the journey. The ease of travel, its mobility, varies from page to page and site to site. For visually impaired users, in particular, mobility is reduced; the objects that support travel are inaccessible or missing altogether. Web development tools need to include support to increase mobility. We present a framework for finding and classifying travel objects within Web pages. The evaluation carried out has shown that this framework supports a systematic and consistent method for assessing travel upon the Web. We propose that such a framework can provide the foundation for a semi-automated tool for the support of travel upon the Web.	A foundation for tool based mobility support for visually impaired web users	NA:NA:NA	2018
Brian McBride	NA	Session details: Establishing the semantic web 11	NA	2018
Siegfried Handschuh:Steffen Staab:Raphael Volz	The success of the Semantic Web crucially depends on the easy creation, integration and use of semantic data. For this purpose, we consider an integration scenario that defies core assumptions of current metadata construction methods. We describe a framework of metadata creation when web pages are generated from a database and the database owner is cooperatively participating in the Semantic Web. This leads us to the definition of ontology mapping rules by manual semantic annotation and the usage of the mapping rules and of web services for semantic queries. In order to create metadata, the framework combines the presentation layer with the data description layer -- in contrast to "conventional" annotation, which remains at the presentation layer. Therefore, we refer to the framework as deep annotation 1.We consider deep annotation as particularly valid because, (i), web pages generated from databases outnumber static web pages, (ii), annotation of web pages may be a very intuitive way to create semantic data from a database and, (iii), data from databases should not be materialized as RDF files, it should remain where it can be handled most efficiently -- in its databases.	On deep annotation	NA:NA:NA	2018
A. Maedche:B. Motik:L. Stojanovic:R. Studer:R. Volz	The vision of the Semantic Web can only be realized through proliferation of well-known ontologies describing different domains. To enable interoperability in the Semantic Web, it will be necessary to break these ontologies down into smaller, well-focused units that may be reused. Currently, three problems arise in that scenario. Firstly, it is difficult to locate ontologies to be reused, thus leading to many ontologies modeling the same thing. Secondly, current tools do not provide means for reusing existing ontologies while building new ontologies. Finally, ontologies are rarely static, but are being adapted to changing requirements. Hence, an infrastructure for management of ontology changes, taking into account dependencies between ontologies is needed. In this paper we present such an infrastructure addressing the aforementioned problems.	An infrastructure for searching, reusing and evolving distributed ontologies	NA:NA:NA:NA:NA	2018
Craig Wills	NA	Session details: Consistency and replication	NA	2018
Lei Gao:Mike Dahlin:Amol Nayate:Jiandan Zheng:Arun Iyengar	The emerging edge services architecture promises to improve the availability and performance of web services by replicating servers at geographically distributed sites. A key challenge in such systems is data replication and consistency so that edge server code can manipulate shared data without incurring the availability and performance penalties that would be incurred by accessing a traditional centralized database. This paper explores using a distributed object architecture to build an edge service system for an e-commerce application, an online bookstore represented by the TPC-W benchmark. We take advantage of application specific semantics to design distributed objects to manage a specific subset of shared information using simple and effective consistency models. Our experimental results show that by slightly relaxing consistency within individual distributed objects, we can build an edge service system that is highly available and efficient. For example, in one experiment we find that our object-based edge server system provides a factor of five improvement in response time over a traditional centralized cluster architecture and a factor of nine improvement over an edge service system that distributes code but retains a centralized database.	Application specific data replication for edge services	NA:NA:NA:NA:NA	2018
Chun Yuan:Yu Chen:Zheng Zhang	As dynamic content becomes increasingly dominant, it becomes an important research topic as how the edge resources such as client-side proxies, which are otherwise underutilized for such content, can be put into use. However, it is unclear what will be the best strategy and the design/deployment tradeoffs lie therein. In this paper, using one representative e-commerce benchmark, we report our experience of an extensive investigation of different offloading and caching options. Our results point out that, while great benefits can be reached in general, advanced offloading strategies can be overly complex and even counter-productive. In contrast, simple augmentation at proxies to enable fragment caching and page composition achieves most of the benefit without compromising important considerations such as security.	Evaluation of edge caching/offloading for dynamic content delivery	NA:NA:NA	2018
Lisa Amini:Anees Shaikh:Henning Schulzrinne	Internet server selection mechanisms attempt to optimize, subject to a variety of constraints, the distribution of client requests to a geographically and topologically diverse pool of servers. Research on server selection has thus far focused primarily on techniques for choosing a server from a group administered by single entity, like a content distribution network provider. In a federated, multi-provider computing system, however, selection must occur over distributed server sets deployed by the participating providers, without the benefit of the full information available in the single-provider case. Intelligent server set selection algorithms will require a model of the expected performance clients would receive from a candidate server set.In this paper, we study whether the complex policies and dynamics of intelligent server selection can be effectively modeled in order to predict client performance for server sets. We introduce a novel server set distance metric, and use it in a measurement study of several million server selection transactions to develop simple models of existing server selection schemes. We then evaluate these models in terms of their ability to accurately predict performance for a second, larger set of distributed clients. We show that our models are able to predict performance within 20ms for over 90% of the observed samples. Our analysis demonstrates that although existing deployments use a variety of complex and dynamic server selection criteria, most of which are proprietary, these schemes can be modeled with surprising accuracy.	Modeling redirection in geographically diverse server sets	NA:NA:NA	2018
TBA	NA	Session details: Open hypermedia and the web	NA	2018
Nikos Karousos:Ippokratis Pandis:Siegfried Reich:Manolis Tzagarakis	Hypermedia systems and more specifically open hypermedia systems (OHS) provide a rich set of implementations of different hypertext flavors such as navigational hypertext, spatial hypertext or taxonomic hypertext. Additionally, these systems offer component-based modular architectures and address interoperability between hypertext domains. Despite multiple efforts of integrating Web clients, a widespread adoption of OHS technology by Web developers has not taken place. In this paper it is argued that Web Services - which offer a component model for Web applications - can be integrated in OHSs. An architectural integration is proposed, a step-by-step process is outlined and an example of integration is provided. This very approach is aimed to benefit both worlds: the Web community with new rich hypermedia functionality that extends the current navigational hypermedia, and the OHS community by opening its tools and platforms to the many developer groups of the Web community.	Offering open hypermedia services to the WWW: a step-by-step approach for developers	NA:NA:NA:NA	2018
Bent G. Christensen:Frank Allan Hansen:Niels Olof Bouvin	This paper evaluates the XLink format in comparison with other linking formats. The comparison is based on Xspect, an implementation of XLink. Xspect handles transformation between an open hypermedia format (OHIF) and XLink, and the paper discusses this isomorphic transformation and generalises it to include another open hypermedia format, FOHM. The Xspect system, based on XSLT and Javascript, provides users with an interface to browse and merge linkbases. Xspect supports navigational hypermedia in the form of links inserted on the fly into Web pages, as well as guided tours presented as SVG. Xspect has two implementations: one server-side and one running on the client. Both implementation provide the user with an interface for the creation of annotations. The main result of the paper is a critique of XLink. XLink is shown to be a format well suited for navigational hypermedia, but lacking in more advanced constructs. More problematic are the issues regarding large-scale use, such as evaluating validity and credibility of linkbases, and ensuring general support for a format as flexible as XLink.	Xspect: bridging open hypermedia and XLink	NA:NA:NA	2018
Laurent Mignet:Denilson Barbosa:Pierangelo Veltri	Although originally designed for large-scale electronic publishing, XML plays an increasingly important role in the exchange of data on the Web. In fact, it is expected that XML will become the lingua franca of the Web, eventually replacing HTML. Not surprisingly, there has been a great deal of interest on XML both in industry and in academia. Nevertheless, to date no comprehensive study on the XML Web (i.e., the subset of the Web made of XML documents only) nor on its contents has been made. This paper is the first attempt at describing the XML Web and the documents contained in it. Our results are drawn from a sample of a repository of the publicly available XML documents on the Web, consisting of about 200,000 documents. Our results show that, despite its short history, XML already permeates the Web, both in terms of generic domains and geographically. Also, our results about the contents of the XML Web provide valuable input for the design of algorithms, tools and systems that use XML in one form or another.	The XML web: a first study	NA:NA:NA	2018
Oren Etzioni	NA	Session details: Data mining	NA	2018
Bhushan Mandhani:Sachindra Joshi:Krishna Kummamuru	This paper proposes an algorithm to hierarchically cluster documents. Each cluster is actually a cluster of documents and an associated cluster of words, thus a document-word co-cluster. Note that, the vector model for documents creates the document-word matrix, of which every co-cluster is a submatrix. One would intuitively expect a submatrix made up of high values to be a good document cluster, with the corresponding word cluster containing its most distinctive features. Our algorithm looks to exploit this. We have defined matrix density, and our algorithm basically uses matrix density considerations in its working.The algorithm is a partitional-agglomerative algorithm. The partitioning step involves the identification of dense submatrices so that the respective row sets partition the row set of the complete matrix. The hierarchical agglomerative step involves merging the most "similar" submatrices until we are down to the required number of clusters (if we want a flat clustering) or until we have just the single complete matrix left (if we are interested in a hierarchical arrangement of documents). It also generates apt labels for each cluster or hierarchy node. The similarity measure between clusters that we use here for the merging cleverly uses the fact that the clusters here are co-clusters, and is a key point of difference from existing agglomerative algorithms. We will refer to the proposed algorithm as RPSA (Rowset Partitioning and Submatrix Agglomeration). We have compared it as a clustering algorithm with Spherical K-Means and Spectral Graph Partitioning. We have also evaluated some hierarchies generated by the algorithm.	A matrix density based algorithm to hierarchically co-cluster documents and words	NA:NA:NA	2018
Kushal Dave:Steve Lawrence:David M. Pennock	The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful.	Mining the peanut gallery: opinion extraction and semantic classification of product reviews	NA:NA:NA	2018
Rakesh Agrawal:Sridhar Rajagopalan:Ramakrishnan Srikant:Yirong Xu	Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. We investigate the feasibility of applying link-based methods in new applications domains. The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. This social behavior gives rise to a network in which the vertices are individuals and the links represent "responded-to" relationships. An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. This behavior is in sharp contrast to the WWW link graph, where linkage is an indicator of agreement or common interest. By analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. In contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text.	Mining newsgroups using networks arising from social behavior	NA:NA:NA:NA	2018
Rudi Studer	NA	Session details: Scaling up the semantic web	NA	2018
Wolfgang Nejdl:Martin Wolpers:Wolf Siberski:Christoph Schmitz:Mario Schlosser:Ingo Brunkhorst:Alexander Löser	RDF-based P2P networks have a number of advantages compared with simpler P2P networks such as Napster, Gnutella or with approaches based on distributed indices such as CAN and CHORD. RDF-based P2P networks allow complex and extendable descriptions of resources instead of fixed and limited ones, and they provide complex query facilities against these metadata instead of simple keyword-based searches.In previous papers, we have described the Edutella infrastructure and different kinds of Edutella peers implementing such an RDF-based P2P network. In this paper we will discuss these RDF-based P2P networks as a specific example of a new type of P2P networks, schema-based P2P networks, and describe the use of super-peer based topologies for these networks. Super-peer based networks can provide better scalability than broadcast based networks, and do provide perfect support for inhomogeneous schema-based networks, which support different metadata schemas and ontologies (crucial for the Semantic Web). Furthermore, as we will show in this paper, they are able to support sophisticated routing and clustering strategies based on the metadata schemas, attributes and ontologies used. Especially helpful in this context is the RDF functionality to uniquely identify schemas, attributes and ontologies. The resulting routing indices can be built using dynamic frequency counting algorithms and support local mediation and transformation rules, and we will sketch some first ideas for implementing these advanced functionalities as well.	Super-peer-based routing and clustering strategies for RDF-based peer-to-peer networks	NA:NA:NA:NA:NA:NA:NA	2018
Vassilis Christophides:Dimitris Plexousakis:Michel Scholl:Sotirios Tourtounis	This paper focuses on the optimization of the navigation through voluminous subsumption hierarchies of topics employed by Portal Catalogs like Netscape Open Directory (ODP). We advocate for the use of labeling schemes for modeling these hierarchies in order to efficiently answer queries such as subsumption check, descendants, ancestors or nearest common ancestor, which usually require costly transitive closure computations. We first give a qualitative comparison of three main families of schemes, namely bit vector, prefix and interval based schemes. We then show that two labeling schemes are good candidates for an efficient implementation of label querying using standard relational DBMS, namely, the Dewey Prefix scheme [6] and an Interval scheme by Agrawal, Borgida and Jagadish [1]. We compare their storage and query evaluation performance for the 16 ODP hierarchies using the PostgreSQL engine.	On labeling schemes for the semantic web	NA:NA:NA:NA	2018
Alon Y. Halevy:Zachary G. Ives:Peter Mork:Igor Tatarinov	The Semantic Web envisions a World Wide Web in which data is described with rich semantics and applications can pose complex queries. To this point, researchers have defined new languages for specifying meanings for concepts and developed techniques for reasoning about them, using RDF as the data model. To flourish, the Semantic Web needs to be able to accommodate the huge amounts of existing data and the applications operating on them. To achieve this, we are faced with two problems. First, most of the world's data is available not in RDF but in XML; XML and the applications consuming it rely not only on the domain structure of the data, but also on its document structure. Hence, to provide interoperability between such sources, we must map between both their domain structures and their document structures. Second, data management practitioners often prefer to exchange data through local point-to-point data translations, rather than mapping to common mediated schemas or ontologies.This paper describes the Piazza system, which addresses these challenges. Piazza offers a language for mediating between data sources on the Semantic Web, which maps both the domain structure and document structure. Piazza also enables interoperation of XML data with RDF data that is accompanied by rich OWL ontologies. Mappings in Piazza are provided at a local scale between small sets of nodes, and our query answering algorithm is able to chain sets mappings together to obtain relevant data from across the Piazza network. We also describe an implemented scenario in Piazza and the lessons we learned from it.	Piazza: data management infrastructure for semantic web applications	NA:NA:NA:NA	2018
Taher Haveliwala	NA	Session details: Dynamic services and analysis	NA	2018
Ravi Kumar:Jasmine Novak:Prabhakar Raghavan:Andrew Tomkins	We propose two new tools to address the evolution of hyperlinked corpora. First, we define time graphs to extend the traditional notion of an evolving directed graph, capturing link creation as a point phenomenon in time. Second, we develop definitions and algorithms for time-dense community tracking, to crystallize the notion of community evolution. We develop these tools in the context of Blogspace , the space of weblogs (or blogs). Our study involves approximately 750K links among 25K blogs. We create a time graph on these blogs by an automatic analysis of their internal time stamps. We then study the evolution of connected component structure and microscopic community structure in this time graph. We show that Blogspace underwent a transition behavior around the end of 2001, and has been rapidly expanding over the past year, not just in metrics of scale, but also in metrics of community structure and connectedness. This expansion shows no sign of abating, although measures of connectedness must plateau within two years. By randomizing link destinations in Blogspace, but retaining sources and timestamps, we introduce a concept of randomized Blogspace . Herein, we observe similar evolution of a giant component, but no corresponding increase in community structure. Having demonstrated the formation of micro-communities over time, we then turn to the ongoing activity within active communities. We extend recent work of Kleinberg [11] to discover dense periods of "bursty" intra-community link creation.	On the bursty evolution of blogspace	NA:NA:NA:NA	2018
Mayank Bawa:Roberto J. Bayardo, Jr.:Sridhar Rajagopalan:Eugene J. Shekita	Personal webservers have proven to be a popular means of sharing files and peer collaboration. Unfortunately, the transient availability and rapidly evolving content on such hosts render centralized, crawl-based search indices stale and incomplete. To address this problem, we propose YouSearch, a distributed search application for personal webservers operating within a shared context (e.g., a corporate intranet). With YouSearch, search results are always fast, fresh and complete -- properties we show arise from an architecture that exploits both the extensive distributed resources available at the peer webservers in addition to a centralized repository of summarized network state. YouSearch extends the concept of a shared context within web communities by enabling peers to aggregate into groups and users to search over specific groups. In this paper, we describe the challenges, design, implementation and experiences with a successful intranet deployment of YouSearch.	Make it fresh, make it quick: searching a network of personal webservers	NA:NA:NA:NA	2018
Wen-Syan Li:Oliver Po:Wang-Pin Hsiung:K. Selçuk Candan:Divyakant Agrawal	Wide-area database replication technologies and the availability of content delivery networks allow Web applications to be hosted and served from powerful data centers. This form of application support requires a complete Web application suite to be distributed along with the database replicas. A major advantage of this approach is that dynamic content is served from locations closer to users, leading into reduced network latency and fast response times. However, this is achieved at the expense of overheads due to (a) invalidation of cached dynamic content in the edge caches and (b) synchronization of database replicas in the data center. These have adverse effects on the freshness of delivered content. In this paper, we propose a freshness-driven adaptive dynamic content caching, which monitors the system status and adjusts caching policies to provide content freshness guarantees. The proposed technique has been intensively evaluated to validate its effectiveness. The experimental results show that the freshness-driven adaptive dynamic content caching technique consistently provides good content freshness. Furthermore, even a Web site that enables dynamic content caching can further benefit from our solution, which improves content freshness up to 7 times, especially under heavy user request traffic and long network latency conditions. Our approach also provides better scalability and significantly reduced response times up to 70% in the experiments.	Engineering and hosting adaptive freshness-sensitive web applications on data centers	NA:NA:NA:NA:NA	2018
Balachander Krishnamurthy	NA	Session details: CDNs and caching	NA	2018
Mikhail Mikhailov:Craig E. Wills	The problem of Web cache consistency continues to be an important one. Current Web caches use heuristic-based policies for determining the freshness of cached objects, often forcing content providers to unnecessarily mark their content as uncacheable simply to retain control over it. Server-driven invalidation has been proposed as a mechanism for providing strong cache consistency for Web objects, but it requires servers to maintain per-client state even for infrequently changing objects. We propose an alternative approach to strong cache consistency, called MONARCH, which does not require servers to maintain per-client state. In this work we focus on a new approach for evaluation of MONARCH in comparison with current practice and other cache consistency policies. This approach uses snapshots of content collected from real Web sites as input to a simulator. Results of the evaluation show MONARCH generates little more request traffic than an optimal cache coherency policy.	Evaluating a new approach to strong web cache consistency with snapshots of collected content	NA:NA	2018
Purushottam Kulkarni:Prashant Shenoy:Weibo Gong	Since CDN simulations are known to be highly memory-intensive, in this paper, we argue the need for reducing the memory requirements of such simulations. We propose a novel memory-efficient data structure that stores cache state for a small subset of popular objects accurately and uses approximations for storing the state for the remaining objects. Since popular objects receive a large fraction of the requests while less frequently accessed objects consume much of the memory space, this approach yields large memory savings and reduces errors. We use bloom filters to store approximate state and show that careful choice of parameters can substantially reduce the probability of errors due to approximations. We implement our techniques into a user library for constructing proxy caches in CDN simulators. Our experimental results show up to an order of magnitude reduction in memory requirements of CDN simulations, while incurring a 5-10% error.	Scalable techniques for memory-efficient CDN simulations	NA:NA:NA	2018
Sean C. Rhea:Kevin Liang:Eric Brewer	Despite traditional web caching techniques, redundant data is often transferred over HTTP links. These redundant transfers result from both resource modification and aliasing. Resource modification causes the data represented by a single URI to change; often, in transferring the new data, some old data is retransmitted. Aliasing, in contrast, occurs when the same data is named by multiple URIs, often in the context of dynamic or advertising content. Traditional web caching techniques index data by its name and thus often fail to recognize and take advantage of aliasing.Despite traditional web caching techniques, redundant data is often transferred over HTTP links. These redundant transfers result from both resource modification and aliasing. Resource modification causes the data represented by a single URI to change; often, in transferring the new data, some old data is retransmitted. Aliasing, in contrast, occurs when the same data is named by multiple URIs, often in the context of dynamic or advertising content. Traditional web caching techniques index data by its name and thus often fail to recognize and take advantage of aliasing.	Value-based web caching	NA:NA:NA	2018
Yao-Wen Huang	NA	Session details: Protocols	NA	2018
Rakesh Agrawal:Jerry Kiernan:Ramakrishnan Srikant:Yirong Xu	The Platform for Privacy Preferences (P3P) is the most significant effort currently underway to enable web users to gain control over their private information. The designers of P3P simultaneously designed a preference language called APPEL to allow users to express their privacy preferences, thus enabling automatic matching of privacy preferences against P3P policies. Unfortunately subtle interactions between P3P and APPEL result in serious problems when using APPEL: Users can only directly specify what is unacceptable in a policy, not what is acceptable; simple preferences are hard to express; and writing APPEL preferences is error prone. We show that these problems follow from a fundamental design choice made by APPEL, and cannot be solved without completely redesigning the language. Therefore we explore alternatives to APPEL that can overcome these problems. In particular, we show that XPath serves quite nicely as a preference language and solves all the above problems. We identify the minimal subset of XPath that is needed, thus allowing matching programs to potentially use a smaller memory footprint. We also give an APPEL to XPath translator that shows that XPath is as expressive as APPEL.	An XPath-based preference language for P3P	NA:NA:NA:NA	2018
Sepandar D. Kamvar:Mario T. Schlosser:Hector Garcia-Molina	Peer-to-peer file-sharing networks are currently receiving much attention as a means of sharing and distributing information. However, as recent experience shows, the anonymous, open nature of these networks offers an almost ideal environment for the spread of self-replicating inauthentic files.We describe an algorithm to decrease the number of downloads of inauthentic files in a peer-to-peer file-sharing network that assigns each peer a unique global trust value, based on the peer's history of uploads. We present a distributed and secure method to compute global trust values, based on Power iteration. By having peers use these global trust values to choose the peers from whom they download, the network effectively identifies malicious peers and isolates them from the network.In simulations, this reputation system, called EigenTrust, has been shown to significantly decrease the number of inauthentic files on the network, even under a variety of conditions where malicious peers cooperate in an attempt to deliberately subvert the system.	The Eigentrust algorithm for reputation management in P2P networks	NA:NA:NA	2018
Chun Zeng:Chun-Xiao Xing:Li-Zhu Zhou	Collaborative filtering has been very successful in both research and applications such as information filtering and E-commerce. The k-Nearest Neighbor (KNN) method is a popular way for its realization. Its key technique is to find k nearest neighbors for a given user to predict his interests. However, this method suffers from two fundamental problems: sparsity and scalability. In this paper, we present our solutions for these two problems. We adopt two techniques: a matrix conversion method for similarity measure and an instance selection method. And then we present an improved collaborative filtering algorithm based on these two methods. In contrast with existing collaborative algorithms, our method shows its satisfactory accuracy and performance.	Similarity measure and instance selection for collaborative filtering	NA:NA:NA	2018
Ricardo Baeza-Yates	NA	Session details: Web crawling and measurement	NA	2018
Sandeep Pandey:Krithi Ramamritham:Soumen Chakrabarti	Continuous queries are queries for which responses given to users must be continuously updated, as the sources of interest get updated. Such queries occur, for instance, during on-line decision making, e.g., traffic flow control, weather monitoring, etc. The problem of keeping the responses current reduces to the problem of deciding how often to visit a source to determine if and how it has been modified, in order to update earlier responses accordingly. On the surface, this seems to be similar to the crawling problem since crawlers attempt to keep indexes up-to-date as pages change and users pose search queries. We show that this is not the case, both due to the inherent differences between the nature of the two problems as well as the performance metric. We propose, develop and evaluate a novel multi-phase (Continuous Adaptive Monitoring) (CAM) solution to the problem of maintaining the currency of query results. Some of the important phases are: The tracking phase, in which changes, to an initially identified set of relevant pages, are tracked. From the observed change characteristics of these pages, a probabilistic model of their change behavior is formulated and weights are assigned to pages to denote their importance for the current queries. During the next phase, the resource allocation phase, based on these statistics, resources, needed to continuously monitor these pages for changes, are allocated. Given these resource allocations, the scheduling phase produces an optimal achievable schedule for the monitoring tasks. An experimental evaluation of our approach compared to prior approaches for crawling dynamic web pages shows the effectiveness of CAM for monitoring dynamic changes. For example, by monitoring just 5% of the page changes, CAM is able to return 90% of the changed information to the users. The experiments also produce some interesting observations pertaining to the differences between the two problems of crawling--to build an index--and the problem of change tracking--to respond to continuous queries.	Monitoring the dynamic web to respond to continuous queries	NA:NA:NA	2018
Dennis Fetterly:Mark Manasse:Marc Najork:Janet Wiener	How fast does the web change? Does most of the content remain unchanged once it has been authored, or are the documents continuously updated? Do pages change a little or a lot? Is the extent of change correlated to any other property of the page? All of these questions are of interest to those who mine the web, including all the popular search engines, but few studies have been performed to date to answer them.One notable exception is a study by Cho and Garcia-Molina, who crawled a set of 720,000 pages on a daily basis over four months, and counted pages as having changed if their MD5 checksum changed. They found that 40% of all web pages in their set changed within a week, and 23% of those pages that fell into the .com domain changed daily.This paper expands on Cho and Garcia-Molina's study, both in terms of coverage and in terms of sensitivity to change. We crawled a set of 150,836,209 HTML pages once every week, over a span of 11 weeks. For each page, we recorded a checksum of the page, and a feature vector of the words on the page, plus various other data such as the page length, the HTTP status code, etc. Moreover, we pseudo-randomly selected 0.1% of all of our URLs, and saved the full text of each download of the corresponding pages.After completion of the crawl, we analyzed the degree of change of each page, and investigated which factors are correlated with change intensity. We found that the average degree of change varies widely across top-level domains, and that larger pages change more often and more severely than smaller ones.This paper describes the crawl and the data transformations we performed on the logs, and presents some statistical observations on the degree of change of different classes of pages.	A large-scale study of the evolution of web pages	NA:NA:NA:NA	2018
Andrei Z. Broder:Marc Najork:Janet L. Wiener	Crawling the web is deceptively simple: the basic algorithm is (a) Fetch a page (b) Parse it to extract all linked URLs (c) For all the URLs not seen before, repeat (a)-(c). However, the size of the web (estimated at over 4 billion pages) and its rate of change (estimated at 7% per week) move this plan from a trivial programming exercise to a serious algorithmic and system design challenge. Indeed, these two factors alone imply that for a reasonably fresh and complete crawl of the web, step (a) must be executed about a thousand times per second, and thus the membership test (c) must be done well over ten thousand times per second against a set too large to store in main memory. This requires a distributed architecture, which further complicates the membership test.A crucial way to speed up the test is to cache, that is, to store in main memory a (dynamic) subset of the "seen" URLs. The main goal of this paper is to carefully investigate several URL caching techniques for web crawling. We consider both practical algorithms: random replacement, static cache, LRU, and CLOCK, and theoretical limits: clairvoyant caching and infinite cache. We performed about 1,800 simulations using these algorithms with various cache sizes, using actual log data extracted from a massive 33 day web crawl that issued over one billion HTTP requests.Our main conclusion is that caching is very effective - in our setup, a cache of roughly 50,000 entries can achieve a hit rate of almost 80%. Interestingly, this cache size falls at a critical point: a substantially smaller cache is much less effective while a substantially larger cache brings little additional benefit. We conjecture that such critical points are inherent to our problem and venture an explanation for this phenomenon.	Efficient URL caching for world wide web crawling	NA:NA:NA	2018
Peter Patel-Schneider	NA	Session details: Using the semantic web	NA	2018
Kemafor Anyanwu:Amit Sheth	This paper presents the notion of Semantic Associations as complex relationships between resource entities. These relationships capture both a connectivity of entities as well as similarity of entities based on a specific notion of similarity called r-isomorphism. It formalizes these notions for the RDF data model, by introducing a notion of a Property Sequence as a type. In the context of a graph model such as that for RDF, Semantic Associations amount to specific certain graph signatures. Specifically, they refer to sequences (i.e. directed paths) here called Property Sequences, between entities, networks of Property Sequences (i.e. undirected paths), or subgraphs of r-isomorphic Property Sequences.The ability to query about the existence of such relationships is fundamental to tasks in analytical domains such as national security and business intelligence, where tasks often focus on finding complex yet meaningful and obscured relationships between entities. However, support for such queries is lacking in contemporary query systems, including those for RDF.	Ρ-Queries: enabling querying for semantic associations on the semantic web	NA:NA	2018
R. Guha:Rob McCool:Eric Miller	Activities such as Web Services and the Semantic Web are working to create a web of distributed machine understandable data. In this paper we present an application called 'Semantic Search' which is built on these supporting technologies and is designed to improve traditional web searching. We provide an overview of TAP, the application framework upon which the Semantic Search is built. We describe two implemented Semantic Search systems which, based on the denotation of the search query, augment traditional search results with relevant data aggregated from distributed sources. We also discuss some general issues related to searching and the Semantic Web and outline how an understanding of the semantics of the search terms can be used to provide better results.	Semantic search	NA:NA:NA	2018
Nicholas Gibbins:Stephen Harris:Nigel Shadbolt	The Web Services world consists of loosely-coupled distributed systems which adapt to ad-hoc changes by the use of service descriptions that enable opportunistic service discovery. At present, these service descriptions are semantically impoverished, being concerned with describing the functional signature of the services rather than characterising their meaning. In the Semantic Web community, the DAML Services effort attempts to rectify this by providing a more expressive way of describing Web services using ontologies. However, this approach does not separate the domain-neutral communicative intent of a message (considered in terms of speech acts) from its domain-specific content, unlike similar developments from the multi-agent systems community.In this paper, we describe our experiences of designing and building an ontologically motivated Web Services system for situational awareness and information triage in a simulated humanitarian aid scenario. In particular, we discuss the merits of using techniques from the multi-agent systems community for separating the intentional force of messages from their content, and the implementation of these techniques within the DAML Services model.	Agent-based semantic web services	NA:NA:NA	2018
Aya Soffer	NA	Session details: Improving the browsing experience	NA	2018
Alistair Coles:Eric Deliot:Tom Melamed:Kevin Lansard	As users acquire or gain access to an increasingly diverse range of web access clients, web applications are adapting their user interfaces to support multiple modalities on multiple client types. User experiences can be enhanced by clients with differing capabilities combining to provide a distributed user interface to applications. Indeed, users will be frustrated if their interaction with applications is limited to one client at a time.This paper discusses the requirements for coordinating web interaction across an aggregation of clients. We present a framework for multi-device browsing that provides both coordinated navigation between web resources and coordinated interaction between variants, or representations, of those resources once instantiated in the clients. The framework protects the application from some of the complexities of client aggregation.We show how a small number of enhancements to the XForms and XML Events vocabularies can facilitate coordination between clients and provide an appropriate level of control to applications. We also describe a novel proxy which consolidates HTTP requests from aggregations of clients and reduces the burden that multi-client browsing places on the application.	A framework for coordinated multi-modal browsing with multiple clients	NA:NA:NA:NA	2018
Akiyo Nadamoto:Katsumi Tanaka	In this paper, we propose a new type of Web browser, called the Comparative Web Browser(CWB), which concurrently presents multiple Web pages in a way that enables the content of the Web pages to be automatically synchronized. The ability to view multiple Web pages at one time is useful when we wish to make a comparison on the Web, such as when we compare similar products or news articles from different newspapers. The CWB is characterized by (1) automatic content-based retrieval of passages from another Web page based on a passage of the Web page the user is reading, and (2) automatic transformation of a user's behavior (scrolling, clicking, or moving backward or forward) on a Web page into a series of behaviors on the other Web pages. The CWB tries to concurrently present "similar" passages from different Web pages, and for this purpose our CWB automatically navigates Web pages that contain passages similar to those of the initial Web page. Furthermore, we propose an enhancement to the CWB, which enables it to use linkage information to find related documents based on link structure.	A comparative web browser (CWB) for browsing and comparing web pages	NA:NA	2018
Luis Gravano	NA	Session details: Search engineering 1	NA	2018
Alexandros Ntoulas:Junghoo Cho:Christopher Olston	We seek to gain improved insight into how Web search engines shouldcope with the evolving Web, in an attempt to provide users with themost up-to-date results possible. For this purpose we collectedweekly snapshots of some 150 Web sites over the course of one year,and measured the evolution of content and link structure. Our measurements focus on aspects of potential interest to search engine designers: the evolution of link structure over time, the rate ofcreation of new pages and new distinct content on the Web, and the rate of change of the content of existing pages under search-centric measures of degree of change.Our findings indicate a rapid turnover rate of Web pages, i.e.,high rates of birth and death, coupled with an even higher rate ofturnover in the hyperlinks that connect them. For pages that persistover time we found that, perhaps surprisingly, the degree of contentshift as measured using TF.IDF cosine distance does not appear to beconsistently correlated with the frequency of contentupdating. Despite this apparent non-correlation, the rate of content shift of a given page is likely to remain consistent over time. That is, pages that change a great deal in one week will likely change by a similarly large degree in the following week. Conversely, pages that experience little change will continue to experience little change. We conclude the paper with a discussion of the potential implications ofour results for the design of effective Web search engines.	What's new on the web?: the evolution of the web from a search engine perspective	NA:NA:NA	2018
Daniel E. Rose:Danny Levinson	Previous work on understanding user web search behavior has focused on how people search and what they are searching for, but not why they are searching. In this paper, we describe a framework for understanding the underlying goals of user searches, and our experience in using the framework to manually classify queries from a web search engine. Our analysis suggests that so-called navigational" searches are less prevalent than generally believed while a previously unexplored "resource-seeking" goal may account for a large fraction of web searches. We also illustrate how this knowledge of user search goals might be used to improve future web search engines.	Understanding user goals in web search	NA:NA	2018
Junghoo Cho:Sourashis Roy	Recent studies show that a majority of Web page accesses are referred by search engines. In this paper we study the widespread use of Web search engines and its impact on the ecology of the Web. In particular, we study how much impact search engines have on the popularity evolution of Web pages. For example, given that search engines return currently popular" pages at the top of search results, are we somehow penalizing newly created pages that are not very well known yet? Are popular pages getting even more popular and new pages completely ignored? We first show that this unfortunate trend indeed exists on the Web through an experimental study based on real Web data. We then analytically estimate how much longer it takes for a new page to attract a large number of Web users when search engines return only popular pages at the top of search results. Our result shows that search engines can have an immensely worrisome impact on the discovery of new Web pages.	Impact of search engines on page popularity	NA:NA	2018
Patrick MacDaniel	NA	Session details: Security and privacy	NA	2018
Jasmine Novak:Prabhakar Raghavan:Andrew Tomkins	It is increasingly common for users to interact with the web using a number of different aliases. This trend is a double-edged sword. On one hand, it is a fundamental building block in approaches to online privacy. On the other hand, there are economic and social consequences to allowing each user an arbitrary number of free aliases. Thus, there is great interest in understanding the fundamental issues in obscuring the identities behind aliases.However, most work in the area has focused on linking aliases through analysis of lower-level properties of interactions such as network routes. We show that aliases that actively post text on the web can be linked together through analysis of that text. We study a large number of users posting on bulletin boards, and develop algorithms to anti-alias those users: we can with a high degree of success identify when two aliases belong to the same individual.Our results show that such techniques are surprisingly effective, leading us to conclude that guaranteeing privacy among aliases that post actively requires mechanisms that do not yet exist.	Anti-aliasing on the web	NA:NA:NA	2018
Yao-Wen Huang:Fang Yu:Christian Hang:Chung-Hung Tsai:Der-Tsai Lee:Sy-Yen Kuo	Security remains a major roadblock to universal acceptance of the Web for many kinds of transactions, especially since the recent sharp increase in remotely exploitable vulnerabilities have been attributed to Web application bugs. Many verification tools are discovering previously unknown vulnerabilities in legacy C programs, raising hopes that the same success can be achieved with Web applications. In this paper, we describe a sound and holistic approach to ensuring Web application security. Viewing Web application vulnerabilities as a secure information flow problem, we created a lattice-based static analysis algorithm derived from type systems and typestate, and addressed its soundness. During the analysis, sections of code considered vulnerable are instrumented with runtime guards, thus securing Web applications in the absence of user intervention. With sufficient annotations, runtime overhead can be reduced to zero. We also created a tool named.WebSSARI (Web application Security by Static Analysis and Runtime Inspection) to test our algorithm, and used it to verify 230 open-source Web application projects on SourceForge.net, which were selected to represent projects of different maturity, popularity, and scale. 69 contained vulnerabilities. After notifying the developers, 38 acknowledged our findings and stated their plans to provide patches. Our statistics also show that static analysis reduced potential runtime overhead by 98.4%.	Securing web application code by static analysis and runtime protection	NA:NA:NA:NA:NA:NA	2018
Halvard Skogsrud:Boualem Benatallah:Fabio Casati	A scalable approach to trust negotiation is required in Web service environments that have large and dynamic requester populations. We introduce Trust-Serv, a model-driven trust negotiation framework for Web services. The framework employs a model for trust negotiation that is based on state machines, extended with security abstractions. Our policy model supports lifecycle management, an important trait in the dynamic environments that characterize Web services. In particular, we provide a set of change operations to modify policies, and migration strategies that permit ongoing negotiations to be migrated to new policies without being disrupted. Experimental results show the performance benefit of these strategies. The proposed approach has been implemented as a container-centric mechanism that is transparent to the Web services and to the developers of Web services, simplifying Web service development and management as well as enabling scalable deployments.	Trust-serv: model-driven lifecycle management of trust negotiation policies for web services	NA:NA:NA	2018
Bay-Wei Chang	NA	Session details: Usability and accessibility	NA	2018
Natasa Milic-Frayling:Rachel Jones:Kerry Rodden:Gavin Smyth:Alan Blackwell:Ralph Sommerer	This paper presents the design and user evaluation of SmartBack, a feature that complements the standard Back button by enabling users to jump directly to key pages in their navigation session, making common navigation activities more efficient. Defining key pages was informed by the findings of a user study that involved detailed monitoring of Web usage and analysis of Web browsing in terms of navigation trails. The pages accessible through SmartBack are determined automatically based on the structure of the user's navigation trails or page association with specific user's activities, such as search or browsing bookmarked sites. We discuss implementation decisions and present results of a usability study in which we deployed the SmartBack prototype and monitored usage for a month in both corporate and home settings. The results show that the feature brings qualitative improvement to the browsing experience of individuals who use it.	Smartback: supporting users in back navigation	NA:NA:NA:NA:NA:NA	2018
John T. Richards:Vicki L. Hanson	Web accessibility is an important goal. However, most approaches to its attainment are based on unrealistic economic models in which Web content developers are required to spend too much for which they receive too little. We believe this situation is due, in part, to the overly narrow definitions given both to those who stand to benefit from enhanced access to the Web and what is meant by this enhanced access. In this paper, we take a broader view, discussing a complementary approach that costs developers less and provides greater advantages to a larger community of users. While we have quite specific aims in our technical work, we hope it can also serve as an example of how the technical conversation regarding Web accessibility can move beyond the narrow confines of limited adaptations for small populations.	Web accessibility: a broader view	NA:NA	2018
I. V. Ramakrishnan:Amanda Stent:Guizhen Yang	In this paper we present HearSay, a system for browsing hypertext Web documents via audio. The HearSay system is based on our novel approach to automatically creating audio browsable content from hypertext Web documents. It combines two key technologies: (1) automatic partitioning of Web documents through tightly coupled structural and semantic analysis, which transforms raw HTML documents into semantic structures so as to facilitate audio browsing; and (2) VoiceXML, an already standardized technology which we adopt to represent voice dialogs automatically created from the XML output of partitioning. This paper describes the software components of HearSay and presents an initial system evaluation.	Hearsay: enabling audio browsing on hypertext content	NA:NA:NA	2018
Roberto Bayardo	NA	Session details: Information extraction	NA	2018
Hang Cui:Min-Yen Kan:Tat-Seng Chua	Breaking news often contains timely definitions and descriptions of current terms, organizations and personalities. We utilize such web sources to construct definitions for such terms. Previous work has identified definitions using hand-crafted rules or supervised learning that constructs rigid, hard text patterns. In contrast, we demonstrate a new approach that uses flexible, soft matching patterns to characterize definition sentences. Our soft patterns are able to effectively accommodate the diversity of definition sentence structure exhibited in news. We use pseudo-relevance feedback to automatically label sentences for use in soft pattern generation. The application of our unsupervised method significantly improves baseline systems on both the standardized TREC corpus as well as crawled online news articles by 27% and 30%, respectively, in terms of F measure. When applied to a state-of-art definition generation system recently fielded in the TREC 2003 definitional question answering task, it improves the performance by 14%.	Unsupervised learning of soft patterns for generating definitions from online news	NA:NA:NA	2018
Oren Etzioni:Michael Cafarella:Doug Downey:Stanley Kok:Ana-Maria Popescu:Tal Shaked:Stephen Soderland:Daniel S. Weld:Alexander Yates	Manually querying search engines in order to accumulate a large bodyof factual information is a tedious, error-prone process of piecemealsearch. Search engines retrieve and rank potentially relevantdocuments for human perusal, but do not extract facts, assessconfidence, or fuse information from multiple documents. This paperintroduces KnowItAll, a system that aims to automate the tedious process ofextracting large collections of facts from the web in an autonomous,domain-independent, and scalable manner.The paper describes preliminary experiments in which an instance of KnowItAll, running for four days on a single machine, was able to automatically extract 54,753 facts. KnowItAll associates a probability with each fact enabling it to trade off precision and recall. The paper analyzes KnowItAll's architecture and reports on lessons learned for the design of large-scale information extraction systems.	Web-scale information extraction in knowitall: (preliminary results)	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Ganesh Ramakrishnan:Soumen Chakrabarti:Deepa Paranjpe:Pushpak Bhattacharya	We present a question answering (QA) system which learns how to detect and rank answer passages by analyzing questions and their answers (QA pairs) provided as training data. We built our system in only a few person-months using off-the-shelf components: a part-of-speech tagger, a shallow parser, a lexical network, and a few well-known supervised learning algorithms. In contrast, many of the top TREC QA systems are large group efforts, using customized ontologies, question classifiers, and highly tuned ranking functions. Our ease of deployment arises from using generic, trainable algorithms that exploit simple feature extractors on QA pairs. With TREC QA data, our system achieves mean reciprocal rank (MRR) that compares favorably with the best scores in recent years, and generalizes from one corpus to another. Our key technique is to recover, from the question, fragments of what might have been posed as a structured query, had a suitable schema been available. comprises selectors: tokens that are likely to appear (almost) unchanged in an answer passage. The other fragment contains question tokens which give clues about the answer type, and are expected to be replaced in the answer passage by tokens which specialize or instantiate the desired answer type. Selectors are like constants in where-clauses in relational queries, and answer types are like column names. We present new algorithms for locating selectors and answer type clues and using them in scoring passages with respect to a question.	Is question answering an acquired skill?	NA:NA:NA:NA	2018
Fred Douglis	NA	Session details: Mobility	NA	2018
Pablo Rodriguez:Sarit Mukherjee:Sampath Ramgarajan	Recent observations through experiments that we have performed incurrent third generation wireless networks have revealed that the achieved throughput over wireless links varies widely depending on the application. In particular, the throughput achieved by file transfer application (FTP) and web browsing application (HTTP) are quite different. The throughput achieved over a HTTP session is much lower than that achieved over an FTP session. The reason for the lower HTTP throughput is that the HTTP protocol is affected by the large Round-Trip Time (RTT) across Wireless links. HTTP transfers require multiple TCP connections and DNS lookups before a HTTP page can be displayed. Each TCP connection requires several RTTs to fully open the TCP send window and each DNS lookup requires several RTTs before resolving the domain name to IP mapping. These TCP/DNS RTTs significantly degrade the performance of HTTP over wireless links. To overcome these problems, we have developed session level optimization techniques to enhance HTTP download mechanisms. These techniques (a) minimize the number of DNS lookups over the wireless link and (b) minimize the number of TCP connections opened by the browser. These optimizations bridge the mismatch caused by wireless links between application-level protocols (such as HTTP) and transport-level protocols (such asTCP). Our solutions do not require any client-side software and can be deployed transparently on a service provider network toprovide 30-50% decrease in end-to-end user perceived latency and 50-100% increase in data throughput across wireless links for HTTP sessions.	Session level techniques for improving web browsing performance on wireless links	NA:NA:NA	2018
Dong Zhou:Nayeem Islam:Ali Ismael	An increasingly large amount of Web applications employ service
objects such as Servlets to generate dynamic and personalized
content. Existing caching infrastructures are not well suited for
caching such content in mobile environments because of
disconnection and weak connection. One possible approach to this
problem is to replicate Web-related application logic to client
devices. The challenges to this approach are to deal with client
devices that exhibit huge divergence in resource availabilities, to
support applications that have different data sharing and coherency
requirements, and to accommodate the same application under
different deployment environments.
The Replet system targets these challenges. It uses client,
server and application capability and preference information (CPI)
to direct the replication of service objects to client devices:
from the selection of a device for replication and populating the
device with client-specific data, to choosing an appropriate
replica to serve a given request and maintaining the desired state
consistency among replicas. The Replet system exploits on-device
replication to enable client-, server- and application-specific
cost metrics for replica invocation and synchronization. We have
implemented a prototype in the context of Servlet-based Web
applications. Our experiment and simulation results demonstrate the
viability and significant benefits of CPI-driven on-device service
object replication.	Flexible on-device service object replication with replets	NA:NA:NA	2018
Albert M. Lai:Jason Nieh:Bhagyashree Bohra:Vijayarka Nandikonda:Abhishek P. Surana:Suchita Varshneya	Web applications are becoming increasingly popular for mobile wireless PDAs. However, web browsing on these systems can be quite slow. An alternative approach is handheld thin-client computing, in which the web browser and associated application logic run on a server, which then sends simple screen updates to thePDA for display. To assess the viability of this thin-client approach, we compare the web browsing performance of thin clients against fat clients that run the web browser locally on a PDA. Our results show that thin clients can provide better web browsing performance compared to fat clients, both in terms of speed and ability to correctly display web content. Surprisingly, thin clients are faster even when having to send more data over the network. We characterize and analyze different design choices in various thin-client systems and explain why these approaches can yield superior web browsing performance on mobile wireless PDAs.	Improving web browsing performance on wireless pdas using thin-client computing	NA:NA:NA:NA:NA:NA	2018
Bebo White	NA	Session details: XML	NA	2018
Quanzhong Li:Michelle Y. Kim:Edward So:Steve Wood	XML has become one of the core technologies for contemporary business applications, especially web-based applications. To facilitate processing of diverse XML data, we propose an extensible, integrated XML processing architecture, the XML Virtual Machine (XVM), which connects XML data with their behaviors. At the same time, the XVM is also a framework for developing and deploying XML-based applications. Using component-based techniques, the XVM supports arbitrary granularity and provides a high degree of modularity and reusability. XVM components are dynamically loaded and composed during XML data processing. Using the XVM, both client-side and server-side XML applications can be developed and deployed in an integrated way. We also present an XML application container built on top of the XVM along with several sample applications to demonstrate the applicability of the XVM framework.	XVM: a bridge between xml data and its behavior	NA:NA:NA:NA	2018
Claudio Sacerdoti Coen:Paolo Marinelli:Fabio Vitali	In the past few years, a number of constraint languages for XML documents has been proposed. They are cumulatively called schema languages or validation languages and they comprise, among others, DTD, XML Schema, RELAX NG, Schematron, DSD, xlinkit. One major point of discrimination among schema languages is the support of co-constraints, or co-occurrence constraints, e.g., requiring that attribute A is present if and only if attribute B is (or is not) presentin the same element. Although there is no way in XML Schema to express these requirements, they are in fact frequently used in many XML document types, usually only expressed in plain human-readable text, and validated by means of special code modules by the relevant applications. In this paper we propose SchemaPath, a light extension of XML Schema to handle conditional constraints on XML documents. Two new constructs have been added to XML Schema: conditions -- based on XPath patterns -- on type assignments for elements and attributes; and a new simple type, xsd:error, for the direct expression of negative constraints (e.g. it is prohibited for attribute A to be present if attribute B is also present). A proof-of-concept implementation is provided. A Web interface is publicly accessible for experiments and assessments of the real expressiveness of the proposed extension.	Schemapath, a minimal extension to xml schema for conditional constraints	NA:NA:NA	2018
Martin Bernauer:Gerti Kappel:Gerhard Kramler	Recently, active behavior has received attention in the XML field to automatically react to occurred events. Aside from proprietary approaches for enriching XML with active behavior, the W3C standardized the Document Object Model (DOM) Event Module for the detection of events in XML documents. When using any of these approaches, however, it is often impossible to decide which event to react upon because not a single event but a combination of multiple events, i.e., a composite event determines a situation to react upon. The paper presents the first approach for detecting composite events in XML documents by addressing the peculiarities of XML events which are caused by their hierarchical order in addition to their temporal order. It also provides for the detection of satisfied multiplicity constraints defined by XML schemas. Thereby the approach enables applications operating on XML documents to react to composite events which have richer semantics.	Composite events for xml	NA:NA:NA	2018
Bing Liu	NA	Session details: Learning classifiers	NA	2018
Chien-Chung Huang:Shui-Lung Chuang:Lee-Feng Chien	Many Web information services utilize techniques of information extraction(IE) to collect important facts from the Web. To create more advanced services, one possible method is to discover thematic information from the collected facts through text classification. However, most conventional text classification techniques rely on manual-labelled corpora and are thus ill-suited to cooperate with Web information services with open domains. In this work, we present a system named LiveClassifier that can automatically train classifiersthrough Web corpora based on user-defined topic hierarchies. Due to its flexibility and convenience, LiveClassifier can be easily adapted for various purposes. New Web information services can be created to fully exploit it; human users can use it to create classifiers for their personal applications. The effectiveness of classifiers created by LiveClassifier is well supportedby empirical evidence.	Liveclassifier: creating hierarchical text classifiers through web corpora	NA:NA:NA	2018
L. K. Shih:D. R. Karger	We propose new features and algorithms for automating Web-page classification tasks such as content recommendation and ad blocking. We show that the automated classification of Web pages can be much improved if, instead of looking at their textual content, we consider each links's URL and the visual placement of those links on a referring page. These features are unusual: rather than being scalar measurements like word counts they are tree structured---describing the position of the item in a tree. We develop a model and algorithm for machine learning using such tree-structured features. We apply our methods in automated tools for recognizing and blocking Web advertisements and for recommending "interesting" news stories to a reader. Experiments show that our algorithms are both faster and more accurate than those based on the text content of Web documents.	Using urls and table layout for web classification tasks	NA:NA	2018
Ruihua Song:Haifeng Liu:Ji-Rong Wen:Wei-Ying Ma	Previous work shows that a web page can be partitioned into multiple segments or blocks, and often the importance of those blocks in a page is not equivalent. Also, it has been proven that differentiating noisy or unimportant blocks from pages can facilitate web mining, search and accessibility. However, no uniform approach and model has been presented to measure the importance of different segments in web pages. Through a user study, we found that people do have a consistent view about the importance of blocks in web pages. In this paper, we investigate how to find a model to automatically assign importance values to blocks in a web page. We define the block importance estimation as a learning problem. First, we use a vision-based page segmentation algorithm to partition a web page into semantic blocks with a hierarchical structure. Then spatial features (such as position and size) and content features (such as the number of images and links) are extracted to construct a feature vector for each block. Based on these features, learning algorithms are used to train a model to assign importance to different segments in the web page. In our experiments, the best model can achieve the performance with Micro-F1 79% and Micro-Accuracy 85.9%, which is quite close to a person's view.	Learning block importance models for web pages	NA:NA:NA:NA	2018
Andreas Paepcke	NA	Session details: Web site engineering	NA	2018
Michael Narayan:Christopher Williams:Saverio Perugini:Naren Ramakrishnan	Multimodal interfaces are becoming increasingly ubiquitous with the advent of mobile devices, accessibility considerations, and novel software technologies that combine diverse interaction media. In addition to improving access and delivery capabilities, such interfaces enable flexible and personalized dialogs with websites, much like a conversation between humans. In this paper, we present a software framework for multimodal web interaction management that supports mixed-initiative dialogs between users and websites. A mixed-initiative dialog is one where the user and the website take turns changing the flow of interaction. The framework supports the functional specification and realization of such dialogs using staging transformations -- a theory for representing and reasoning about dialogs based on partial input. It supports multiple interaction interfaces, and offers sessioning, caching, and co-ordination functions through the use of an interaction manager. Two case studies are presented to illustrate the promise of this approach.	Staging transformations for multimodal web interaction management	NA:NA:NA:NA	2018
Terence John Parr	The mantra of every experienced web application developer is the same: thou shalt separate business logic from display. Ironically, almost all template engines allow violation of this separation principle, which is the very impetus for HTML template engine development. This situation is due mostly to a lack of formal definition of separation and fear that enforcing separation emasculates a template's power. I show that not only is strict separation a worthy design principle, but that we can enforce separation while providing a potent template engine. I demonstrate my StringTemplate engine, used to build jGuru.com and other commercial sites, at work solving some nontrivial generational tasks.My goal is to formalize the study of template engines, thus, providing a common nomenclature, a means of classifying template generational power, and a way to leverage interesting results from formal language theory. I classify three types of restricted templates analogous to Chomsky's type 1..3 grammar classes and formally define separation including the rules that embody separation.Because this paper provides a clear definition of model-view separation, template engine designers may no longer blindly claim enforcement of separation. Moreover, given theoretical arguments and empirical evidence, programmers no longer have an excuse to entangle model and view.	Enforcing strict model-view separation in template engines	NA	2018
Fernando Bellas:Daniel Fernández:Abel Muiño	There exist many portal servers that support the construction of "My" portals that is portals that allow the user to have one or more personal pages composed of a number of personalizable services. The main drawback of current portal servers is their lack of generality and adaptability. This paper presents the design of MyPersonalizer a J2EE-based framework for engineering My portals. The framework is structured according to the Model-View-Controller and Layers architectural patterns providing generic adaptable model and controller layers that implement the typical use cases of a My portal. MyPersonalizer allows for a good separation of roles in the development team: graphical designers (without programming skills) develop the portal view by writing JSP pages while software engineers implement service plugins and specify framework configuration.	A flexible framework for engineering "my" portals	NA:NA:NA	2018
Peter Patel-Schneider	NA	Session details: Semantic interfaces and OWL tools	NA	2018
Luke McDowell:Oren Etzioni:Alon Halevy:Henry Levy	This paper investigates how the vision of the Semantic Web can be carried overto the realm of email. We introduce a general notion of semantice mail, in which an email message consists of an RDF query or update coupled with corresponding explanatory text. Semantic email opens the door to a wide range of automated, email-mediated applications with formally guaranteed properties. In particular, this paper introduces a broad class of semantic email processes. For example consider the process of sending an email to a program committee asking who will attend the PC dinner automatically collecting the responses and tallying them up. We define bothlogical and decision-theoretic models where an email process ismodeled as a set of updates to a data set on which we specify goals via certain constraints or utilities. We then describe a set ofinference problems that arise while trying to satisfy these goals and analyze their computational tractability. In particular weshow that for the logical model it is possible to automatically infer which email responses are acceptable w.r.t. a set ofconstraints in polynomial time and for the decision-theoreticmodel it is possible to compute the optimal message-handling policy in polynomial time. Finally we discuss our publicly available implementation of semantic email and outline research challenges inthis realm.	Semantic email	NA:NA:NA:NA	2018
D. A. Quan:R. Karger	Two important architectural choices underlie the success of the Web: numerous, independently operated servers speak a common protocol, and a single type of client the Web browser provides point-and-click access to the content and services on these decentralized servers. However, because HTML marries content and presentation into a single representation, end users are often stuck with inappropriate choices made by the Web site designer of how to work with and view the content. RDF metadata on the Semantic Web does not have this limitation: users can gain direct access to information and control over how it is presented. This principle forms the basis for our Semantic Web browser an end user application that automatically locates metadata and assembles point-and-click interfaces from a combination of relevant information, ontological specifications, and presentation knowledge, all described in RDF and retrieved dynamically from the Semantic Web. Because data and services are accessed directly through a standalone client and not through a central point of access (e.g., a portal), new content and services can be consumed as soon as they become available. In this way we take advantage of an important sociological force that encourages the production of new Semantic Web content while remaining faithful to the decentralized nature of the Web.	How to make a semantic web browser	NA:NA	2018
Sean K. Bechhofer:Jeremy J. Carroll	The Web Ontology Language (OWL) defines three classes of documents: Lite, DL, and Full. All RDF/XML documents are OWL Full documents, some OWL Full documents are also OWL DL documents, and some OWL DL documents are also OWL Lite documents. This paper discusses parsing and species recognition -- that is the process of determining whether a given document falls into the OWL Lite, DL or Full class. Wedescribe two alternative approaches to this task, one based on abstract syntax trees, the other on RDF triples, and compare their key characteristics.	Parsing owl dl: trees or triples?	NA:NA	2018
Irwin King	NA	Session details: Server performance and scalability	NA	2018
Sameh Elnikety:Erich Nahum:John Tracey:Willy Zwaenepoel	This paper presents a method for admission control and request scheduling for multiply-tiered e-commerce Web sites, achieving both stable behavior during overload and improved response times. Our method externally observes execution costs of requests online, distinguishing different request types, and performs overload protection and preferential scheduling using relatively simple measurements and a straight forward control mechanism. Unlike previous proposals, which require extensive changes to the server or operating system, our method requires no modifications to the host O.S., Web server, application server or database. Since our method is external, it can be implemented in a proxy. We present such an implementation, called Gatekeeper, using it with standard software components on the Linux operating system. We evaluate the proxy using the industry standard TPC-W workload generator in a typical three-tiered e-commerce environment. We show consistent performance during overload and throughput increases of up to 10 percent. Response time improves by up to a factor of 14, with only a 15 percent penalty to large jobs.	A method for transparent admission control and request scheduling in e-commerce web sites	NA:NA:NA:NA	2018
Bowei Xi:Zhen Liu:Mukund Raghavachari:Cathy H. Xia:Li Zhang	The overwhelming success of the Web as a mechanism for facilitating information retrieval and for conducting business transactions has ledto an increase in the deployment of complex enterprise applications. These applications typically run on Web Application Servers, which assume the burden of managing many tasks, such as concurrency, memory management, database access, etc., required by these applications. The performance of an Application Server depends heavily on appropriate configuration. Configuration is a difficult and error-prone task dueto the large number of configuration parameters and complex interactions between them. We formulate the problem of finding an optimal configuration for a given application as a black-box optimization problem. We propose a smart hill-climbing algorithm using ideas of importance sampling and Latin Hypercube Sampling (LHS). The algorithm is efficient in both searching and random sampling. It consists of estimating a local function, and then, hill-climbing in the steepest descent direction. The algorithm also learns from past searches and restarts in a smart and selective fashion using the idea of importance sampling. We have carried out extensive experiments with an on-line brokerage application running in a WebSphere environment. Empirical results demonstrate that our algorithm is more efficient than and superior to traditional heuristic methods.	A smart hill-climbing algorithm for application server configuration	NA:NA:NA:NA:NA	2018
Wen-Syan Li:Wang-Pin Hsiung:Oliver Po:Koji Hino:Kasim Selcuk Candan:Divyakant Agrawal	For most Web-based applications, contents are created dynamically based on the current state of a business, such as product prices and inventory, stored in database systems. These applications demand personalized content and track user behavior while maintaining application integrity. Many of such practices are not compatible with Web acceleration solutions. Consequently, although many web acceleration solutions have shown promising performance improvement and scalability, architecting and engineering distributed enterprise Web applications to utilize available content delivery networks remains a challenge. In this paper, we examine the challenge to accelerate J2EE-based enterprise web applications. We list obstacles and recommend some practices to transform typical database-driven J2EE applications to cache friendly Web applications where Web acceleration solutions can be applied. Furthermore, such transformation should be done without modification to the underlying application business logic and without sacrificing functions that are essential to e-commerce. We take the J2EE reference software, the Java PetStore, as a case study. By using the proposed guideline, we are able to cache more than 90% of the content in the PetStore and scale up the Web site more than 20 times.	Challenges and practices in deploying web acceleration solutions for distributed enterprise systems	NA:NA:NA:NA:NA:NA	2018
Junghoo Cho	NA	Session details: Link analysis	NA	2018
Nadav Eiron:Kevin S. McCurley:John A. Tomlin	The celebrated PageRank algorithm has proved to be a very effective paradigm for ranking results of web search algorithms. In this paper we refine this basic paradigm to take into account several evolving prominent features of the web, and propose several algorithmic innovations. First, we analyze features of the rapidly growing "frontier" of the web, namely the part of the web that crawlers are unable to cover for one reason or another. We analyze the effect of these pages and find it to be significant. We suggest ways to improve the quality of ranking by modeling the growing presence of "link rot" on the web as more sites and pages fall out of maintenance. Finally we suggest new methods of ranking that are motivated by the hierarchical structure of the web, are more efficient than PageRank, and may be more resistant to direct manipulation.	Ranking the web frontier	NA:NA:NA	2018
Wensi Xi:Benyu Zhang:Zheng Chen:Yizhou Lu:Shuicheng Yan:Wei-Ying Ma:Edward Allan Fox	Web link analysis has proven to be a significant enhancement for quality based web search. Most existing links can be classified into two categories: intra-type links (e.g., web hyperlinks), which represent the relationship of data objects within a homogeneous data type (web pages), and inter-type links (e.g., user browsing log) which represent the relationship of data objects across different data types (users and web pages). Unfortunately, most link analysis research only considers one type of link. In this paper, we propose a unified link analysis framework, called "link fusion", which considers both the inter- and intra- type link structure among multiple-type inter-related data objects and brings order to objects in each data type at the same time. The PageRank and HITS algorithms are shown to be special cases of our unified link analysis framework. Experiments on an instantiation of the framework that makes use of the user data and web pages extracted from a proxy log show that our proposed algorithm could improve the search effectiveness over the HITS and DirectHit algorithms by 24.6% and 38.2% respectively.	Link fusion: a unified link analysis framework for multi-type interrelated data objects	NA:NA:NA:NA:NA:NA:NA	2018
Ziv Bar-Yossef:Andrei Z. Broder:Ravi Kumar:Andrew Tomkins	The rapid growth of the web has been noted and tracked extensively. Recent studies have however documented the dual phenomenon: web pages have small half lives, and thus the web exhibits rapid death as well. Consequently, page creators are faced with an increasingly burdensome task of keeping links up-to-date, and many are falling behind. In addition to just individual pages, collections of pages or even entire neighborhoods of the web exhibit significant decay, rendering them less effective as information resources. Such neighborhoods are identified only by frustrated searchers, seeking a way out of these stale neighborhoods, back to more up-to-date sections of the web; measuring the decay of a page purely on the basis of dead links on the page is too naive to reflect this frustration. In this paper we formalize a strong notion of a decay measure and present algorithms for computing it efficiently. We explore this measure by presenting a number of validations, and use it to identify interesting artifacts on today's web. We then describe a number of applications of such a measure to search engines, web page maintainers, ontologists, and individual users.	Sic transit gloria telae: towards an understanding of the web's decay	NA:NA:NA:NA	2018
Jason Nieh	NA	Session details: Optimizing encoding	NA	2018
Xinyi Yin:Wee Sun Lee	Delivering web pages to mobile phones or personal digital assistants has become possible with the latest wireless technology. However, mobile devices have very small screen sizes and memory capacities. Converting web pages for delivery to a mobile device is an exciting new problem. In this paper, we propose to use a ranking algorithm similar to Google's PageRank algorithm to rank the content objects within a web page. This allows the extraction of only important parts of web pages for delivery to mobile devices. Experiments show that the new method is effective. In experiments on pages from randomly selected websites, the system needed to extract and deliver only 39% of the objects in a web page in order to provide 85% of a viewer's desired viewing content. This provides significant savings in the wireless traffic and downloading time while providing a satisfactory reading experience on the mobile device.	Using link analysis to improve layout on mobile devices	NA:NA	2018
R. J. Bayardo:D. Gruhl:V. Josifovski:J. Myllymaki	This paper provides an objective evaluation of the performance impacts of binary XML encodings, using a fast stream-based XQuery processor as our representative application. Instead of proposing one binary format and comparing it against standard XML parsers, we investigate the individual effects of several binary encoding techniques that are shared by many proposals. Our goal is to provide a deeper understanding of the performance impacts of binary XML encodings in order to clarify the ongoing and often contentious debate over their merits, particularly in the domain of high performance XML stream processing.	An evaluation of binary xml encoding optimizations for fast stream based xml processing	NA:NA:NA:NA	2018
Jacqueline Spiesser:Les Kitchen	Automatically generated HTML, as produced by WYSIWYG programs, typically contains much repetitive and unnecessary markup. Thispaper identifies aspects of such HTML that may be altered whileleaving a semantically equivalent document, and proposes techniques to achieve optimizing modifications. These techniques include attribute re-arrangement via dynamic programming, the use of style classes, and dead-coderemoval. These techniques produce documents as small as 33% of original size. The size decreases obtained are still significant when the techniques are used in combination with conventional text-based compression.	Optimization of html automatically generated by wysiwyg programs	NA:NA	2018
Amit Sheth	NA	Session details: Semantic web applications	NA	2018
Timothy J. Miles-Board:Christopher P. Bailey:Wendy Hall:Leslie A. Carr	A problem facing many textbook authors (including one of the authors of this paper) is the inevitable delay between new advances in the subject area and their incorporation in a new (paper) edition of the textbook. This means that some textbooks are quickly considered out of date, particularly in active technological areas such as the Web, even though the ideas presented in the textbook are still valid and important to the community. This paper describes our approach to building a companion website for the textbook Hypermedia and the Web: An Engineering Approach. We use Bloom's taxonomy of educational objectives to critically evaluate a number of authoring and presentation techniques used in existing companion websites, and adapt these techniques to create our own companion website using Semantic Web technologies in order to overcome the identified weaknesses. Finally, we discuss a potential model of future companion websites, in the context of an e-publishing, e-commerce Semantic Web services scenario.	Building a companion website in the semantic web	NA:NA:NA:NA	2018
Cristiano Rocha:Daniel Schwabe:Marcus Poggi Aragao	This paper presents a search architecture that combines classical search techniques with spread activation techniques applied to a semantic model of a given domain. Given an ontology, weights are assigned to links based on certain properties of the ontology, so that they measure the strength of the relation. Spread activation techniques are used to find related concepts in the ontology given an initial set of concepts and corresponding initial activation values. These initial values are obtained from the results of classical search applied to the data associated with the concepts in the ontology. Two test cases were implemented, with very positive results. It was also observed that the proposed hybrid spread activation, combining the symbolic and the sub-symbolic approaches, achieved better results when compared to each of the approaches alone.	A hybrid approach for searching in the semantic web	NA:NA:NA	2018
m. c. schraefel:Nigel R. Shadbolt:Nicholas Gibbins:Stephen Harris:Hugh Glaser	We present a Semantic Web application that we callCS AKTive Space. The application exploits a wide range of semantically heterogeneousand distributed content relating to Computer Science research in theUK. This content is gathered on a continuous basis using a variety of methods including harvesting and scraping as well as adopting a range models for content acquisition. The content currently comprises aroundten million RDF triples and we have developed storage, retrieval andmaintenance methods to support its management. The content is mediated through an ontology constructed for the application domainand incorporates components from other published ontologies. CS AKTive Spacesupports the exploration of patterns and implications inherent in the content and exploits a variety of visualisations and multi dimensional representations. Knowledge services supported in the applicationinclude investigating communities of practice: who is working, researching or publishing with whom. This work illustrates a number ofsubstantial challenges for the Semantic Web. These include problems of referential integrity, tractable inference and interaction support. Wereview our approaches to these issues and discuss relevant related work.	CS AKTive space: representing computer science in the semantic web	NA:NA:NA:NA:NA	2018
David Pennock	NA	Session details: Reputation networks	NA	2018
Shyong K. Lam:John Riedl	Recommender systems have emerged in the past several years as an effective way to help people cope with the problem of information overload. One application in which they have become particularly common is in e-commerce, where recommendation of items can often help a customer find what she is interested in and, therefore can help drive sales. Unscrupulous producers in the never-ending quest for market penetration may find it profitable to shill recommender systems by lying to the systems in order to have their products recommended more often than those of their competitors. This paper explores four open questions that may affect the effectiveness of such shilling attacks: which recommender algorithm is being used, whether the application is producing recommendations or predictions, how detectable the attacks are by the operator of the system, and what the properties are of the items being attacked. The questions are explored experimentally on a large data set of movie ratings. Taken together, the results of the paper suggest that new ways must be used to evaluate and detect shilling attacks on recommender systems.	Shilling recommender systems for fun and profit	NA:NA	2018
R. Guha:Ravi Kumar:Prabhakar Raghavan:Andrew Tomkins	A (directed) network of people connected by ratings or trust scores, and a model for propagating those trust scores, is a fundamental building block in many of today's most successful e-commerce and recommendation systems. We develop a framework of trust propagation schemes, each of which may be appropriate in certain circumstances, and evaluate the schemes on a large trust network consisting of 800K trust scores expressed among 130K people. We show that a small number of expressed trusts/distrust per individual allows us to predict trust between any two people in the system with high accuracy. Our work appears to be the first to incorporate distrust in a computational trust propagation setting.	Propagation of trust and distrust	NA:NA:NA:NA	2018
Rodrigo B. Almeida:Virgilio A. F. Almeida	Current search technologies work in a "one size fits all" fashion. Therefore, the answer to a query is independent of specific user information need. In this paper we describe a novel ranking technique for personalized search servicesthat combines content-based and community-based evidences. The community-based information is used in order to provide context for queries andis influenced by the current interaction of the user with the service. Ouralgorithm is evaluated using data derived from an actual service available on the Web an online bookstore. We show that the quality of content-based ranking strategies can be improved by the use of communityinformation as another evidential source of relevance. In our experiments the improvements reach up to 48% in terms of average precision.	A community-aware search engine	NA:NA	2018
Corey Anderson	NA	Session details: Versioning and fragmentation	NA	2018
Curtis E. Dyreson:Hui-ling Lin:Yingxia Wang	This paper presents a transaction-time HTTP server, called TTApache that supports document versioning. A document often consists of a main file formatted in HTML or XML and several included files such as images and stylesheets. A change to any of the files associated with a document creates a new version of that document. To construct a document version history, snapshots of the document's files are obtained over time. Transaction times are associated with each file version to record the version's lifetime. The transaction time is the system time of the edit that created the version. Accounting for transaction time is essential to supporting audit queries that delve into past document versions and differential queries that pinpoint differences between two versions. TTApache performs automatic versioning when a document is read thereby removing the burden of versioning from document authors. Since some versions may be created but never read, TTApache distinguishes between known and assumed versions of a document. TTApache has a simple query language to retrieve desired versions. A browser can request a specific version, or the entire history of a document. Queries can also rewrite links and references to point to current or past versions. Over time, the version history of a document continually grows. To free space, some versions can be vacuumed. Vacuuming a version however changes the semantics of requests for that version. This paper presents several policies for vacuuming versions and strategies for accounting for vacuumed versions in queries.	Managing versions of web documents in a transaction-time web server	NA:NA:NA	2018
Tien Nhut Nguyen:Ethan Vincent Munson:Cheng Thao	Researchers in Web engineering have regularly noted that existing Web application development environments provide little support for managing the evolution of Web applications. Key limitations of Web development environments include line-oriented change models that inadequately represent Web document semantics and in ability to model changes to link structure or the set of objects making up the Webapplication. Developers may find it difficult to grasp how theoverall structure of the Web application has changed over time and may respond by using ad hoc solutions that lead to problems of maintain ability, quality and reliability. Web applications are software artifacts, and as such, can benefit from advanced version control and software configuration management (SCM)technologies from software engineering. We have modified an integrated development environment to manage the evolution and maintenance of Web applications. The resulting environment is distinguished by itsfine-grained version control framework, fine-grained Web contentchange management, and product versioning configuration management, in which a Web project can be organized at the logical level and itsstructure and components are versioned in a fine-grained manner aswell. This paper describes the motivation for this environment as well as its user interfaces, features, and implementation.	Fine-grained, structured configuration management for web projects	NA:NA:NA	2018
Lakshmish Ramaswamy:Arun Iyengar:Ling Liu:Fred Douglis	Dividing web pages into fragments has been shown to provide significant benefits for both content generation and caching. In order for a web site to use fragment-based content generation, however, good methods are needed for dividing web pages into fragments. Manual fragmentation of web pages is expensive, error prone, and unscalable. This paper proposes a novel scheme to automatically detect and flag fragments that are cost-effective cache units in web sites serving dynamic content. We consider the fragments to be interesting if they are shared among multiple documents or they have different lifetime or personalization characteristics. Our approach has three unique features. First, we propose a hierarchical and fragment-aware model of the dynamic web pages and a data structure that is compact and effective for fragment detection. Second, we present an efficient algorithm to detect maximal fragments that are shared among multiple documents. Third, we develop a practical algorithm that effectively detects fragments based on their lifetime and personalization characteristics. We evaluate the proposed scheme through a series of experiments, showing the benefits and costs of the algorithms. We also study the impact of adopting the fragments detected by our system on disk space utilization and network bandwidth consumption.	Automatic detection of fragments in dynamically generated web pages	NA:NA:NA:NA	2018
Carole Goble	NA	Session details: Semantic annotation and integration	NA	2018
Jim Blythe:Yolanda Gil	For the manual semantic markup of documents to become wide-spread, usersmust be able to express annotations that conform to ontologies (orschemas) that have shared meaning. However, a typical user is unlikelyto be familiar with the details of the terms as defined by the ontology authors. In addition, the idea to be expressed may not fit perfectly within a pre-defined ontology. The ideal tool should help users find apartial formalization that closely follows the ontology where possiblebut deviates from the formal representation where needed. We describe animplemented approach to help users create semi-structured semantic annotations for a document according to an extensible OWL ontology. In our approach, users enter a short sentence in free text to describe allor part of a document, and the system presents a set of potential paraphrases of the sentence that are generated from valid expressions inthe ontology, from which the user chooses the closest match. We use a combination of off-the-shelf parsing tools and breadth-first search of expressions in the ontology to help users create valid annotations starting from free text. The user can also define new terms to augmentthe ontology, so the potential matches can improve over time.	Incremental formalization of document annotations through ontology-based paraphrasing	NA:NA	2018
Philipp Cimiano:Siegfried Handschuh:Steffen Staab	The success of the Semantic Web depends on the availability of ontologies as well as on the proliferation of web pages annotated with metadata conforming to these ontologies. Thus, a crucial question is where to acquire these metadata from. In this paper wepropose PANKOW (Pattern-based Annotation through Knowledge on theWeb), a method which employs an unsupervised, pattern-based approach to categorize instances with regard to an ontology. The approach is evaluated against the manual annotations of two human subjects. The approach is implemented in OntoMat, an annotation tool for the Semantic Web and shows very promising results.	Towards the self-annotating web	NA:NA:NA	2018
Dell Zhang:Wee Sun Lee	We address the problem of integrating objects from a source taxonomy into a master taxonomy. This problem is not only currently pervasive on the web, but also important to the emerging semantic web. A straightforward approach to automating this process would be to train a classifier for each category in the master taxonomy, and then classify objects from the source taxonomy into these categories. In this paper we attempt to use a powerful classification method, Support Vector Machine (SVM), to attack this problem. Our key insight is that the availability of the source taxonomy data could be helpful to build better classifiers in this scenario, therefore it would be beneficial to do transductive learning rather than inductive learning, i.e., learning to optimize classification performance on a particular set of test examples. Noticing that the categorizations of the master and source taxonomies often have some semantic overlap, we propose a method, Cluster Shrinkage (CS), to further enhance the classification by exploiting such implicit knowledge. Our experiments with real-world web data show substantial improvements in the performance of taxonomy integration.	Web taxonomy integration using support vector machines	NA:NA	2018
Krishna Bharat	NA	Session details: Mining new media	NA	2018
Evgeniy Gabrilovich:Susan Dumais:Eric Horvitz	We present a principled methodology for filtering news stories by formal measures of information novelty, and show how the techniques can be usedto custom-tailor news feeds based on information that a user has already reviewed. We review methods for analyzing novelty and then describe Newsjunkie, a system that personalizes news for users by identifying the novelty of stories in the context of stories they have already reviewed. Newsjunkie employs novelty-analysis algorithms that represent articles as words and named entities. The algorithms analyze inter-andintra-document dynamics by considering how information evolves over timefrom article to article, as well as within individual articles. We review the results of a user study undertaken to gauge the value of the approachover legacy time-based review of newsfeeds, and also to compare the performance of alternate distance metrics that are used to estimate the dissimilarity between candidate new articles and sets of previously reviewed articles.	Newsjunkie: providing personalized newsfeeds via analysis of information novelty	NA:NA:NA	2018
Daniel Gruhl:R. Guha:David Liben-Nowell:Andrew Tomkins	We study the dynamics of information propagation in environments of low-overhead personal publishing, using a large collection of weblogs over time as our example domain. We characterize and model this collection at two levels. First, we present a macroscopic characterization of topic propagation through our corpus, formalizing the notion of long-running "chatter" topics consisting recursively of "spike" topics generated by outside world events, or more rarely, by resonances within the community. Second, we present a microscopic characterization of propagation from individual to individual, drawing on the theory of infectious diseases to model the flow. We propose, validate, and employ an algorithm to induce the underlying propagation network from a sequence of posts, and report on the results.	Information diffusion through blogspace	NA:NA:NA:NA	2018
D. C. Reis:P. B. Golgher:A. S. Silva:A. F. Laender	The Web poses itself as the largest data repository ever available in the history of humankind. Major efforts have been made in order to provide efficient access to relevant information within this huge repository of data. Although several techniques have been developed to the problem of Web data extraction, their use is still not spread, mostly because of the need for high human intervention and the low quality of the extraction results.In this paper, we present a domain-oriented approach to Web data extraction and discuss its application to automatically extracting news from Web sites. Our approach is based on a highly efficient tree structure analysis that produces very effective results. We have tested our approach with several important Brazilian on-line news sites and achieved very precise results, correctly extracting 87.71% of the news in a set of 4088 pages distributed among 35 different sites.	Automatic web news extraction using tree edit distance	NA:NA:NA:NA	2018
Alec Wolman	NA	Session details: Workload analysis	NA	2018
Subhabrata Sen:Oliver Spatscheck:Dongmei Wang	The ability to accurately identify the network traffic associated with different P2P applications is important to a broad range of network operations including application-specific traffic engineering, capacity planning, provisioning, service differentiation,etc. However, traditional traffic to higher-level application mapping techniques such as default server TCP or UDP network-port baseddisambiguation is highly inaccurate for some P2P applications.In this paper, we provide an efficient approach for identifying the P2P application traffic through application level signatures. We firstidentify the application level signatures by examining some available documentations, and packet-level traces. We then utilize the identified signatures to develop online filters that can efficiently and accurately track the P2P traffic even on high-speed network links.We examine the performance of our application-level identification approach using five popular P2P protocols. Our measurements show thatour technique achieves less than 5% false positive and false negative ratios in most cases. We also show that our approach only requires the examination of the very first few packets (less than 10packets) to identify a P2P connection, which makes our approach highly scalable. Our technique can significantly improve the P2P traffic volume estimates over what pure network port based approaches provide. For instance, we were able to identify 3 times as much traffic for the popular Kazaa P2P protocol, compared to the traditional port-based approach.	Accurate, scalable in-network identification of p2p traffic using application signatures	NA:NA:NA	2018
L. Bent:M. Rabinovich:G. M. Voelker:Z. Xiao	This paper presents a systematic study of the properties of a large number of Web sites hosted by a major ISP. To our knowledge, ours is the first comprehensive study of a large server farm that contains thousands of commercial Web sites. We also perform a simulation analysis to estimate potential performance benefits of content delivery networks (CDNs) for these Web sites. We make several interesting observations about the current usage of Web technologies and Web site performance characteristics. First, compared with previous client workload studies, the Web server farm workload contains a much higher degree of uncacheable responses and responses that require mandatory cache validations. A significant reason for this is that cookie use is prevalent among our population, especially among more popular sites. However, we found an indication of wide-spread indiscriminate usage of cookies, which unnecessarily impedes the use of many content delivery optimizations. We also found that most Web sites do not utilize the cache-control features ofthe HTTP 1.1 protocol, resulting in suboptimal performance. Moreover, the implicit expiration time in client caches for responses is constrained by the maximum values allowed in the Squid proxy. Finally, our simulation results indicate that most Web sites benefit from the use of a CDN. The amount of the benefit depends on site popularity, and, somewhat surprisingly, a CDN may increase the peak to average request ratio at the origin server because the CDN can decrease the average request rate more than the peak request rate.	Characterization of a large web site population with implications for content delivery	NA:NA:NA:NA	2018
Cristiano P. Costa:Italo S. Cunha:Alex Borges:Claudiney V. Ramos:Marcus M. Rocha:Jussara M. Almeida:Berthier Ribeiro-Neto	This paper provides an extensive analysis of pre-stored streaming media workloads, focusing on the client interactive behavior. We analyze four workloads that fall into three different domains, namely, education, entertainment video and entertainment audio. Our main goals are: (a) to identify qualitative similarities and differences in the typical client behavior for the three workload classes and (b) to provide data for generating realistic synthetic workloads.	Analyzing client interactivity in streaming media	NA:NA:NA:NA:NA:NA:NA	2018
Steffen Staab	NA	Session details: Semantic web services	NA	2018
Monika Solanki:Antonio Cau:Hussein Zedan	Current ontological specifications for semantically describing properties of Web services are limited to their static interface description. Normally for proving properties of service compositions, mapping input/output parameters and specifying the pre/post conditions are found to be sufficient. However these properties are assertions only on the initial and final states of the service respectively. They do not help in specifying/verifying ongoing behaviour of an individual service or a composed system. We propose a framework for enriching semantic service descriptions with two compositional assertions: assumption and commitment that facilitate reasoning about service composition and verification of their integration. The technique is based on Interval Temporal Logic(ITL): a sound formalism for specifying and proving temporal properties of systems. Our approach utilizes the recently proposed Semantic Web Rule Language.	Augmenting semantic web service descriptions with compositional specification	NA:NA:NA	2018
Abhijit A. Patil:Swapna A. Oundhakar:Amit P. Sheth:Kunal Verma	The World Wide Web is emerging not only as an infrastructure for data, but also for a broader variety of resources that are increasingly being made available as Web services. Relevant current standards like UDDI, WSDL, and SOAP are in their fledgling years and form the basis of making Web services a workable and broadly adopted technology. However, realizing the fuller scope of the promise of Web services and associated service oriented architecture will requite further technological advances in the areas of service interoperation, service discovery, service composition, and process orchestration. Semantics, especially as supported by the use of ontologies, and related Semantic Web technologies, are likely to provide better qualitative and scalable solutions to these requirements. Just as semantic annotation of data in the Semantic Web is the first critical step to better search, integration and analytics over heterogeneous data, semantic annotation of Web services is an equally critical first step to achieving the above promise. Our approach is to work with existing Web services technologies and combine them with ideas from the Semantic Web to create a better framework for Web service discovery and composition. In this paper we present MWSAF (METEOR-S Web Service Annotation Framework), a framework for semi-automatically marking up Web service descriptions with ontologies. We have developed algorithms to match and annotate WSDL files with relevant ontologies. We use domain ontologies to categorize Web services into domains. An empirical study of our approach is presented to help evaluate its performance.	Meteor-s web service annotation framework	NA:NA:NA:NA	2018
Peter Mika:Daniel Oberle:Aldo Gangemi:Marta Sabou	Clarity in semantics and a rich formalization of this semantics are important requirements for ontologies designed to be deployed in large-scale, open, distributed systems such as the envisioned Semantic Web This is especially important for the description of Web Services, which should enable complex tasks involving multiple agents. As one of the first initiatives of the Semantic Webcommunity for describing Web Services, OWL-S attracts a lot of interest even though it is still under development. We identify problematic aspects of OWL-S and suggest enhancements through alignment to a foundational ontology. Another contribution of ourwork is the Core Ontology of Services that tries to fill the epistemological gap between the foundational ontology and OWL-S. It can be reused to align other Web Service description languages as well. Finally, we demonstrate the applicability of our work byaligning OWL-S' standard example called CongoBuy.	Foundations for service ontologies: aligning OWL-S to dolce	NA:NA:NA:NA	2018
Nick Koudas	NA	Session details: Search engineering 2	NA	2018
Mike Perkowitz:Matthai Philipose:Kenneth Fishkin:Donald J. Patterson	The ability to determine what day-to-day activity (such as cooking pasta, taking a pill, or watching a video) a person is performing is of interest in many application domains. A system that can do this requires models of the activities of interest, but model construction does not scale well: humans must specify low-level details, such as segmentation and feature selection of sensor data, and high-level structure, such as spatio-temporal relations between states of the model, for each and every activity. As a result, previous practical activity recognition systems have been content to model a tiny fraction of the thousands of human activities that are potentially useful to detect. In this paper, we present an approach to sensing and modeling activities that scales to a much larger class of activities than before. We show how a new class of sensors, based on Radio Frequency Identification (RFID) tags, can directly yield semantic terms that describe the state of the physical world. These sensors allow us to formulate activity models by translating labeled activities, such as 'cooking pasta', into probabilistic collections of object terms, such as 'pot'. Given this view of activity models as text translations, we show how to mine definitions of activities in an unsupervised manner from the web. We have used our technique to mine definitions for over 20,000 activities. We experimentally validate our approach using data gathered from actual human activity as well as simulated data.	Mining models of human activities from the web	NA:NA:NA:NA	2018
S. Amer-Yahia:C. Botev:J. Shanmugasundaram	One of the key benefits of XML is its ability to represent a mix of structured and unstructured (text) data. Although current XML query languages such as XPath and XQuery can express rich queries over structured data, they can only express very rudimentary queries over text data. We thus propose TeXQuery, which is a powerful full-text search extension to XQuery. TeXQuery provides a rich set of fully composable full-text search primitives,such as Boolean connectives, phrase matching, proximity distance, stemming and thesauri. TeXQuery also enables users to seamlessly query over both structured and text data by embedding TeXQuery primitives in XQuery, and vice versa. Finally, TeXQuery supports a flexible scoring construct that can be used toscore query results based on full-text predicates. TeXQuery is the precursor ofthe full-text language extensions to XPath 2.0 and XQuery 1.0 currently being developed by the W3C.	Texquery: a full-text search extension to xquery	NA:NA:NA	2018
P. Boldi:S. Vigna	Studying web graphs is often difficult due to their large size. Recently,several proposals have been published about various techniques that allow tostore a web graph in memory in a limited space, exploiting the inner redundancies of the web. The WebGraph framework is a suite of codes, algorithms and tools that aims at making it easy to manipulate large web graphs. This papers presents the compression techniques used in WebGraph, which are centred around referentiation and intervalisation (which in turn are dual to each other). WebGraph can compress the WebBase graph (118 Mnodes, 1 Glinks)in as little as 3.08 bits per link, and its transposed version in as littleas 2.89 bits per link.	The webgraph framework I: compression techniques	NA:NA	2018
Martin Gaedke	NA	Session details: Infastructure for implementation	NA	2018
Nicola Onose:Jerome Simeon	XML messaging is at the heart of Web services, providing the flexibility required for their deployment, composition, and maintenance. Yet, current approaches to Web services development hide the messaging layer behind Java or C# APIs, preventing the application to get direct access to the underlying XML information. To address this problem, we advocate the use of a native XML language, namely XQuery, as an integral part of the Web services development infrastructure. The main contribution of the paper is a binding between WSDL, the Web Services Description Language, and XQuery. The approach enables the use of XQuery for both Web services deployment and composition. We present a simple command-line tool that can be used to automatically deploy a Web service from a given XQuery module, and extend the XQuery language itself with a statement for accessing one or more Web services. The binding provides tight-coupling between WSDL and XQuery, yielding additional benefits, notably: the ability to use WSDL as an interface language for XQuery, and the ability to perform static typing on XQuery programs that include Web service calls. Last but not least, the proposal requires only minimal changes to the existing infrastructure. We report on our experience implementing this approach in the Galax XQuery processor.	XQuery at your web service	NA:NA	2018
Bita Shadgar:Ian Holyer	The ability of the Web to share data regardless of geographical location raises a new issue called remote authoring. With the Internet and Web browsers being independent of hardware, it becomes possible to build Web-enabled database applications. Many approaches are provided to integrate databases into the Web environment, which use the Web's protocol i.e. HTTP to transfer the data between clients and servers. However, those methods are affected by the HTTP shortfalls with regard to remote authoring. This paper introduces and discusses a new methodology for remote authoring of databases, which is based on the WebDAV protocol. It is a seamless and effective methodology for accessing and authoring databases, particularly in that it naturally benefits from the WebDAV advantages such as metadata and access control. These features establish a standard way of accessing database metadata, and increase the database security, while speeding up the database connection.	Adapting databases and WebDAV protocol	NA:NA	2018
Xiang Fu:Tevfik Bultan:Jianwen Su	This paper presents a set of tools and techniques for analyzing interactions of composite web services which are specified in BPEL and communicate through asynchronous XML messages. We model the interactions of composite web services as conversations, the global sequence of messages exchanged by the web services. As opposed to earlier work, our tool-set handles rich data manipulation via XPath expressions. This allows us to verify designs at a more detailed level and check properties about message content. We present a framework where BPEL specifications of web services are translated to an intermediate representation, followed by the translation of the intermediate representation to a verification language. As an intermediate representation we use guarded automata augmented with unbounded queues for incoming messages, where the guards are expressed as XPath expressions. As the target verification language we use Promela, input language of the model checker SPIN. Since SPIN model checker is a finite-state verification tool we can only achieve partial verification by fixing the sizes of the input queues in the translation. We propose the concept of synchronizability to address this problem. We show that if a composite web service is synchronizable, then its conversation set remains same when asynchronous communication is replaced with synchronous communication. We give a set of sufficient conditions that guarantee synchronizability and that can be checked statically. Based on our synchronizability results, we show that a large class of composite web services with unbounded input queues can be completely verified using a finite state model checker such as SPIN.	Analysis of interacting BPEL web services	NA:NA:NA	2018
Frank van Harmelen	NA	Session details: Distributed semantic query	NA	2018
Heiner Stuckenschmidt:Richard Vdovjak:Geert-Jan Houben:Jeen Broekstra	A technical infrastructure for storing, querying and managing RDFdata is a key element in the current semantic web development. Systems like Jena, Sesame or the ICS-FORTH RDF Suite are widelyused for building semantic web applications. Currently, none ofthese systems supports the integrated querying of distributed RDF repositories. We consider this a major shortcoming since the semanticweb is distributed by nature. In this paper we present an architecture for querying distributed RDF repositories by extending the existing Sesame system. We discuss the implications of our architectureand propose an index structure as well as algorithms forquery processing and optimization in such a distributed context.	Index structures and algorithms for querying distributed RDF repositories	NA:NA:NA:NA	2018
Christoph Tempich:Steffen Staab:Adrian Wranik	In peer-to-peer networks, finding the appropriate answer for an information request, such as the answer to a query for RDF(S) data, depends on selecting the right peer in the network. We hereinvestigate how social metaphors can be exploited effectively andefficiently to solve this task. To this end, we define a method for query routing, REMINDIN', that lets <em>(i)</em> peers observewhich queries are successfully answered by other peers,<em>(ii)</em>, memorizes this observation, and, <em>(iii)</em>,subsequently uses this information in order to select peers to forward requests to.REMINDIN' has been implemented for the SWAP peer-to-peer platformas well as for a simulation environment. We have used the simulation environment in order to investigate how successfulvariations of REMINDIN' are and how they compare to baseline strategies in terms of number of messages forwarded in the networkand statements appropriately retrieved.	Remindin': semantic query routing in peer-to-peer networks based on social metaphors	NA:NA:NA	2018
Min Cai:Martin Frank	Centralized Resource Description Framework (RDF) repositories have limitations both in their failure tolerance and in their scalability. Existing Peer-to-Peer (P2P) RDF repositories either cannot guarantee to find query results, even if these results exist in the network, or require up-front definition of RDF schemas and designation of super peers. We present a scalable distributed RDF repository (RDFPeers) that stores each triple at three places in a multi-attribute addressable network by applying globally known hash functions to its subject predicate and object. Thus all nodes know which node is responsible for storing triple values they are looking for and both exact-match and range queries can be efficiently routed to those nodes. RDFPeers has no single point of failure nor elevated peers and does not require the prior definition of RDF schemas. Queries are guaranteed to find matched triples in the network if the triples exist. In RDFPeers both the number of neighbors per node and the number of routing hops for inserting RDF triples and for resolving most queries are logarithmic to the number of nodes in the network. We further performed experiments that show that the triple-storing load in RDFPeers differs by less than an order of magnitude between the most and the least loaded nodes for real-world RDF data.	RDFPeers: a scalable distributed RDF repository based on a structured peer-to-peer network	NA:NA	2018
Andrei Broder	NA	Session details: Query result processing	NA	2018
Krishna Kummamuru:Rohit Lotlikar:Shourya Roy:Karan Singal:Raghu Krishnapuram	Organizing Web search results into a hierarchy of topics and sub-topics facilitates browsing the collection and locating results of interest. In this paper, we propose a new hierarchical monothetic clustering algorithm to build a topic hierarchy for a collection of search results retrieved in response to a query. At every level of the hierarchy, the new algorithm progressively identifies topics in a way that maximizes the coverage while maintaining distinctiveness of the topics. We refer the proposed algorithm to as DisCover. Evaluating the quality of a topic hierarchy is a non-trivial task, the ultimate test being user judgment. We use several objective measures such as coverage and reach time for an empirical comparison of the proposed algorithm with two other monothetic clustering algorithms to demonstrate its superiority. Even though our algorithm is slightly more computationally intensive than one of the algorithms, it generates better hierarchies. Our user studies also show that the proposed algorithm is superior to the other algorithms as a summarizing and browsing tool.	A hierarchical monothetic document clustering algorithm for summarization and browsing search results	NA:NA:NA:NA:NA	2018
Reiner Kraft:Jason Zien	When searching large hypertext document collections, it is often possible that there are too many results available for ambiguous queries. Query refinement is an interactive process of query modification that can be used to narrow down the scope of search results. We propose a new method for automatically generating refinements or related terms to queries by mining anchor text for a large hypertext document collection. We show that the usage of anchor text as a basis for query refinement produces high quality refinement suggestions that are significantly better in terms of perceived usefulness compared to refinements that are derived using the document content. Furthermore, our study suggests that anchor text refinements can also be used to augment traditional query refinement algorithms based on query logs, since they typically differ in coverage and produce different refinements. Our results are based on experiments on an anchor text collection of a large corporate intranet.	Mining anchor text for query refinement	NA:NA	2018
Kazunari Sugiyama:Kenji Hatano:Masatoshi Yoshikawa	Web search engines help users find useful information on the World Wide Web (WWW). However, when the same query is submitted by different users, typical search engines return the same result regardless of who submitted the query. Generally, each user has different information needs for his/her query. Therefore, the search result should be adapted to users with different information needs. In this paper, we first propose several approaches to adapting search results according to each user's need for relevant information without any user effort, and then verify the effectiveness of our proposed approaches. Experimental results show that search systems that adapt to each user's preferences can be achieved by constructing user profiles based on modified collaborative filtering with detailed analysis of user's browsing history in one day.	Adaptive web search based on user profile constructed without any effort from users	NA:NA:NA	2018
Daniel Schwabe	NA	Session details: Web site analysis and customization	NA	2018
Thierry Despeyroux	As Web sites are now ordinary products, it is necessary to explicit the notion of quality of a Web site. The quality of a site may belinked to the easiness of accessibility and also to other criteria such as the fact that the site is up to date and coherent. This last quality is difficult to insure because sites may be updated very frequently, may have many authors, may be partially generated and inthis context proof-reading is very difficult. The same piece of information may be found in different occurrences, but also in data ormeta-data, leading to the need for consistency checking. In this paper we make a parallel between programs and Web sites. We present some examples of semantic constraints that one would like to specify (constraints between the meaning of categories and sub-categories in a thematic directory, consistency between the organization chart and the rest of the site in an academic site). We present quickly the Natural Semantics a way to specify the semantics of programming languages that inspires ourworks. Natural Semantics itself comes from both an operational semantics and from logic programming and its implementation uses Prolog. Then we propose a specification language for semantic constraints in Web sites that, in conjunction with the well known "make" program, permits to generate some site verification tools by compiling the specification into Prolog code. We apply our method to alarge XML document which is the scientific part of our instituteactivity report, tracking errors or inconsistencies and alsoconstructing some indicators that can be used by the management of theinstitute.	Practical semantic analysis of web sites and documents	NA	2018
Eugene Hung:Joseph Pasquale	ReAgents are remotely executing agents that customize Web browsing for non-standard clients. A reAgent is essentially a one-shot" mobile agent that acts as an extension of a client dynamically launched by the client to run on its behalf at a remote more advantageous location. ReAgents simplify the use of mobile agent technology by transparently handling data migration and run-time network communications and provide a general interface for programmers to more easily implement their application-specific customizing logic. This is made possible by the identification of useful remote behaviors i.e. common patterns of actions that exploit the ability to process and communicate remotely. Examples of such behaviors are transformers monitors cachers and collators. In this paper we identify a set ofuseful reAgent behaviors for interacting with Web services via astandard browser describe how to program and use reAgents and show that the overhead of using reAgents is low and outweighed by its benefits.	Web customization using behavior-based remote executing agents	NA:NA	2018
Jeremy Carroll	NA	Session details: Semantic web foundations	NA	2018
Bernardo Cuenca Grau	In the semantic Web architecture, Web ontology languages arebuilt on top of RDF(S). However, serious difficulties have arisen when trying to layer expressive ontology languages, like OWL, on top of RDF-Schema. Although these problems can be avoided, OWL (andthe whole semantic Web architecture) becomes much more complex than it should be. In this paper, a possible simplification of thesemantic Web architecture is suggested, which has several import antadvantages with respect to the layering currently accepted by the W3C Ontology Working Group.	A possible simplification of the semantic web architecture	NA	2018
J. S. Dong:C. H. Lee:H. B. Lee:Y. F. Li:H. Wang	The understanding of Semantic Web documents is built upon ontologies that define concepts and relationships of data. Hence, the correctness of ontologies is vital. Ontology reasoners such as RACER and FaCT have been developed to reason ontologies with a high degree of automation. However, complex ontology-related properties may not be expressible within the current web ontology languages, consequently they may not be checkable by RACER and FaCT. We propose to use the software engineering techniques and tools, i.e., Z/EVES and Alloy Analyzer, to complement the ontology tools for checking Semantic Web documents.In this approach, Z/EVES is first applied to remove trivial syntax and type errors of the ontologies. Next, RACER is used to identify any ontological inconsistencies, whose origins can be traced by Alloy Analyzer. Finally Z/EVES is used again to express complex ontology-related properties and reveal errors beyond the modeling capabilities of the current web ontology languages. We have successfully applied this approach to checking a set of military plan ontologies.	A combined approach to checking web ontologies	NA:NA:NA:NA:NA	2018
Tim Berners-Lee	The key property of the WWWW is its universality: One must be able to access it whatever the hardware device, software platform, and network one is using, and despite the disabilities one might have, and whether oner is in a "developed" or "developing" country; it must support information of any language, culture, quality, medium, and field without discrimination so that a hypertext link can go anywhere; it must support information intended for people, and that intended for machine processing. The Web architecture incorporated various choices which support these axes of universality.Currently the architecture and the principles are being exploited in the recent Mobile Web initiative in W3C to promote content which can be accessed optimally from conventional computers and mobile devices. New exciting areas arise every few months as possible Semantic Web flagship applications. As new areas burst forth, the fundamental principles remain important and are extended and adjusted. At the same time, the principles of openness and consensus among international stakeholders which the WWW consortium employs for new technology are adjusted, but ever-important.	WWW at 15 years: looking forward	NA	2018
Bing Liu	NA	Session details: Usage analysis	NA	2018
Steve Chien:Nicole Immorlica	We investigate the idea of finding semantically related search engine queries based on their temporal correlation; in other words, we infer that two queries are related if their popularities behave similarly over time. To this end, we first define a new measure of the temporal correlation of two queries based on the correlation coefficient of their frequency functions. We then conduct extensive experiments using our measure on two massive query streams from the MSN search engine, revealing that this technique can discover a wide range of semantically similar queries. Finally, we develop a method of efficiently finding the highest correlated queries for a given input query using far less space and time than the naive approach, making real-time implementation possible.	Semantic similarity between search engine queries using temporal correlation	NA:NA	2018
Ahmed Metwally:Divyakant Agrawal:Amr El Abbadi	We consider the problem of finding duplicates in data streams. Duplicate detection in data streams is utilized in various applications including fraud detection. We develop a solution based on Bloom Filters [9], and discuss the space and time requirements for running the proposed algorithm in both the contexts of sliding, and landmark stream windows. We run a comprehensive set of experiments, using both real and synthetic click streams, to evaluate the performance of the proposed solution. The results demonstrate that the proposed solution yields extremely low error rates.	Duplicate detection in click streams	NA:NA:NA	2018
Cai-Nicolas Ziegler:Sean M. McNee:Joseph A. Konstan:Georg Lausen	In this work we present topic diversification, a novel method designed to balance and diversify personalized recommendation lists in order to reflect the user's complete spectrum of interests. Though being detrimental to average accuracy, we show that our method improves user satisfaction with recommendation lists, in particular for lists generated using the common item-based collaborative filtering algorithm.Our work builds upon prior research on recommender systems, looking at properties of recommendation lists as entities in their own right rather than specifically focusing on the accuracy of individual recommendations. We introduce the intra-list similarity metric to assess the topical diversity of recommendation lists and the topic diversification approach for decreasing the intra-list similarity. We evaluate our method using book recommendation data, including offline analysis on 361, !, 349 ratings and an online study involving more than 2, !, 100 subjects.	Improving recommendation lists through topic diversification	NA:NA:NA:NA	2018
Michael Rabinovich	NA	Session details: Wide-area architecture and protocols	NA	2018
Swaminathan Sivasubramanian:Gustavo Alonso:Guillaume Pierre:Maarten van Steen	We present GlobeDB, a system for hosting Web applications that performs autonomic replication of application data. GlobeDB offers data-intensive Web applications the benefits of low access latencies and reduced update traffic. The major distinction in our system compared to existing edge computing infrastructures is that the process of distribution and replication of application data is handled by the system automatically with very little manual administration. We show that significant performance gains can be obtained this way. Performance evaluations with the TPC-W benchmark over an emulated wide-area network show that GlobeDB reduces latencies by a factor of 4 compared to non-replicated systems and reduces update traffic by a factor of 6 compared to fully replicated systems.	GlobeDB: autonomic data replication for web applications	NA:NA:NA:NA	2018
Utku Irmak:Torsten Suel	While overall bandwidth in the internet has grown rapidly over the last few years, and an increasing number of clients enjoy broadband connectivity, many others still access the internet over much slower dialup or wireless links. To address this issue, a number of techniques for optimized delivery of web and multimedia content over slow links have been proposed, including protocol optimizations, caching, compression, and multimedia transcoding, and several large ISPs have recently begun to widely promote dialup acceleration services based on such techniques. A recent paper by Rhea, Liang, and Brewer proposed an elegant technique called value-based caching that caches substrings of files, rather than entire files, and thus avoids repeated transmission of substrings common to several pages or page versions.We propose and study a hierarchical substring caching technique that provides significant savings over this basic approach. We describe several additional techniques for minimizing overheads and perform an evaluation on a large set of real web access traces that we collected. In the second part of our work, we compare our approach to a widely studied alternative approach based on delta compression, and show how to integrate the two for best overall performance. The studied techniques are typically employed in a client-proxy environment, with each proxy serving a large number of clients, and an important aspect is how to conserve resources on the proxy while exploiting the significant memory and CPU power available on current clients.	Hierarchical substring caching for efficient content distribution to low-bandwidth clients	NA:NA	2018
Rajeev Gupta:Ashish Puri:Krithi Ramamritham	Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making. Typically a user desires to obtain the value of some function over distributed data items, for example, to determine when and whether (a) the traffic entering a highway from multiple feed roads will result in congestion in a thoroughfare or (b) the value of a stock portfolio exceeds a threshold. Using the standard Web infrastructure for these applications will increase the reach of the underlying information. But, since these queries involve data from multiple sources, with sources supporting standard HTTP (pull-based) interfaces, special query processing techniques are needed. Also, these applications often have the flexibility to tolerate some incoherency, i.e., some differences between the results reported to the user and that produced from the virtual database made up of the distributed data sources.In this paper, we develop and evaluate client-pull-based techniques for refreshing data so that the results of the queries over distributed data can be correctly reported, conforming to the limited incoherency acceptable to the users.We model as well as estimate the dynamics of the data items using a probabilistic approach based on Markov Chains. Depending on the dynamics of data we adapt the data refresh times to deliver query results with the desired coherency. The commonality of data needs of multiple queries is exploited to further reduce refresh overheads. Effectiveness of our approach is demonstrated using live sources of dynamic data: the number of refreshes it requires is (a) an order of magnitude less than what we would need if every potential update is pulled from the sources, and (b) comparable to the number of messages needed by an ideal algorithm, one that knows how to optimally refresh the data from distributed data sources. Our evaluations also bring out a very practical and attractive tradeoff property of pull based approaches, e.g., a small increase in tolerable incoherency leads to a large decrease in message overheads.	Executing incoherency bounded continuous queries at web data aggregators	NA:NA:NA	2018
Shinichi Morishita	NA	Session details: Data extraction	NA	2018
Hongkun Zhao:Weiyi Meng:Zonghuan Wu:Vijay Raghavan:Clement Yu	When a query is submitted to a search engine, the search engine returns a dynamically generated result page containing the result records, each of which usually consists of a link to and/or snippet of a retrieved Web page. In addition, such a result page often also contains information irrelevant to the query, such as information related to the hosting site of the search engine and advertisements. In this paper, we present a technique for automatically producing wrappers that can be used to extract search result records from dynamically generated result pages returned by search engines. Automatic search result record extraction is very important for many applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling. The novel aspect of the proposed technique is that it utilizes both the visual content features on the result page as displayed on a browser and the HTML tag structures of the HTML source file of the result page. Experimental results indicate that this technique can achieve very high extraction accuracy.	Fully automatic wrapper generation for search engines	NA:NA:NA:NA:NA	2018
Yanhong Zhai:Bing Liu	This paper studies the problem of extracting data from a Web page that contains several structured data records. The objective is to segment these data records, extract data items/fields from them and put the data in a database table. This problem has been studied by several researchers. However, existing methods still have some serious limitations. The first class of methods is based on machine learning, which requires human labeling of many examples from each Web site that one is interested in extracting data from. The process is time consuming due to the large number of sites and pages on the Web. The second class of algorithms is based on automatic pattern discovery. These methods are either inaccurate or make many assumptions. This paper proposes a new method to perform the task automatically. It consists of two steps, (1) identifying individual data records in a page, and (2) aligning and extracting data items from the identified data records. For step 1, we propose a method based on visual information to segment data records, which is more accurate than existing methods. For step 2, we propose a novel partial alignment technique based on tree matching. Partial alignment means that we align only those data fields in a pair of data records that can be aligned (or matched) with certainty, and make no commitment on the rest of the data fields. This approach enables very accurate alignment of multiple data records. Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records, align and extract data from them very accurately.	Web data extraction based on partial tree alignment	NA:NA	2018
Andrew Hogue:David Karger	We describe Thresher, a system that lets non-technical users teach their browsers how to extract semantic web content from HTML documents on the World Wide Web. Users specify examples of semantic content by highlighting them in a web browser and describing their meaning. We then use the tree edit distance between the DOM subtrees of these examples to create a general pattern, or wrapper, for the content, and allow the user to bind RDF classes and predicates to the nodes of these wrappers. By overlaying matches to these patterns on standard documents inside the Haystack semantic web browser, we enable a rich semantic interaction with existing web pages, "unwrapping" semantic data buried in the pages' HTML. By allowing end-users to create, modify, and utilize their own patterns, we hope to speed adoption and use of the Semantic Web and its applications.	Thresher: automating the unwrapping of semantic content from the World Wide Web	NA:NA	2018
Eric A. Brewer	Moore's Law and the wave of technologies it enabled have led to tremendous improvements in productivity and the quality of life in the industrialized world. Yet, technology has had almost no effect on the four billion people that make less US$2000/day. In this talk I argue that the decreasing costs of computing and wireless networking make this the right time to spread the benefits of technology, and that the biggest missing piece is a lack of focus on the problems that matter, including health, education, and government. After covering some example applications that have shown very high impact, I take an early look at the research agenda for developing regions. Finally, I examine some of the pragmatic issues required to make progress on these very challenging problems. My goal is to convince high-tech researchers that technology for developing regions is an important and viable research topic.	The case for technology for developing regions	NA	2018
Andrew Tomkins	NA	Session details: Semantic querying	NA	2018
Gianna M. Del Corso:Antonio Gullí:Francesco Romani	According to a recent survey made by Nielsen NetRatings, searching on news articles is one of the most important activity online. Indeed, Google, Yahoo, MSN and many others have proposed commercial search engines for indexing news feeds. Despite this commercial interest, no academic research has focused on ranking a stream of news articles and a set of news sources. In this paper, we introduce this problem by proposing a ranking framework which models: (1) the process of generation of a stream of news articles, (2) the news articles clustering by topics, and (3) the evolution of news story over the time. The ranking algorithm proposed ranks news information, finding the most authoritative news sources and identifying the most interesting events in the different categories to which news article belongs. All these ranking measures take in account the time and can be obtained without a predefined sliding window of observation over the stream. The complexity of our algorithm is linear in the number of pieces of news still under consideration at the time of a new posting. This allow a continuous on-line process of ranking. Our ranking framework is validated on a collection of more than 300,000 pieces of news, produced in two months by more then 2000 news sources belonging to 13 different categories (World, U.S, Europe, Sports, Business, etc). This collection is extracted from the index of comeToMyHead, an academic news search engine available online.	Ranking a stream of news	NA:NA:NA	2018
Ana G. Maguitman:Filippo Menczer:Heather Roinestad:Alessandro Vespignani	Automatic extraction of semantic information from text and links in Web pages is key to improving the quality of search results. However, the assessment of automatic semantic measures is limited by the coverage of user studies, which do not scale with the size, heterogeneity, and growth of the Web. Here we propose to leverage human-generated metadata --- namely topical directories --- to measure semantic relationships among massive numbers of pairs of Web pages or topics. The Open Directory Project classifies millions of URLs in a topical ontology, providing a rich source from which semantic relationships between Web pages can be derived. While semantic similarity measures based on taxonomies (trees) are well studied, the design of well-founded similarity measures for objects stored in the nodes of arbitrary ontologies (graphs) is an open problem. This paper defines an information-theoretic measure of semantic similarity that exploits both the hierarchical and non-hierarchical structure of an ontology. An experimental study shows that this measure improves significantly on the traditional taxonomy-based approach. This novel measure allows us to address the general question of how text and link analyses can be combined to derive measures of relevance that are in good agreement with semantic similarity. Surprisingly, the traditional use of text similarity turns out to be ineffective for relevance ranking.	Algorithmic detection of semantic similarity	NA:NA:NA:NA	2018
Kemafor Anyanwu:Angela Maduko:Amit Sheth	While the idea that querying mechanisms for complex relationships (otherwise known as Semantic Associations) should be integral to Semantic Web search technologies has recently gained some ground, the issue of how search results will be ranked remains largely unaddressed. Since it is expected that the number of relationships between entities in a knowledge base will be much larger than the number of entities themselves, the likelihood that Semantic Association searches would result in an overwhelming number of results for users is increased, therefore elevating the need for appropriate ranking schemes. Furthermore, it is unlikely that ranking schemes for ranking entities (documents, resources, etc.) may be applied to complex structures such as Semantic Associations.In this paper, we present an approach that ranks results based on how predictable a result might be for users. It is based on a relevance model SemRank, which is a rich blend of semantic and information-theoretic techniques with heuristics that supports the novel idea of modulative searches, where users may vary their search modes to effect changes in the ordering of results depending on their need. We also present the infrastructure used in the SSARK system to support the computation of SemRank values for resulting Semantic Associations and their ordering.	SemRank: ranking complex relationship search results on the semantic web	NA:NA:NA	2018
Vikas Agarwal:Koustuv Dasgupta:Neeran Karnik:Arun Kumar:Ashish Kundu:Sumit Mittal:Biplav Srivastava	The demand for quickly delivering new applications is increasingly becoming a business imperative today. Application development is often done in an ad hoc manner, without standard frameworks or libraries, thus resulting in poor reuse of software assets. Web services have received much interest in industry due to their potential in facilitating seamless business-to-business or enterprise application integration. A web services composition tool can help automate the process, from creating business process functionality, to developing executable workflows, to deploying them on an execution environment. However, we find that the main approaches taken thus far to standardize and compose web services are piecemeal and insufficient. The business world has adopted a (distributed) programming approach in which web service instances are described using WSDL, composed into flows with a language like BPEL and invoked with the SOAP protocol. Academia has propounded the AI approach of formally representing web service capabilities in ontologies, and reasoning about their composition using goal-oriented inferencing techniques from planning. We present the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the above approaches. We describe a prototype service creation environment along with a use-case scenario, and demonstrate how it can significantly speed up the time-to-market for new services.	A service creation environment based on end to end composition of Web services	NA:NA:NA:NA:NA:NA:NA	2018
Sami Bhiri:Olivier Perrin:Claude Godart	The recent evolution of Internet, driven by the Web services technology, is extending the role of the Web from a support of information interaction to a middleware for B2B interactions.Indeed, the Web services technology allows enterprises to outsource parts of their business processes using Web services. And it also provides the opportunity to dynamically offer new value-added services through the composition of pre-existing Web services.In spite of the growing interest in Web services, current technologies are found lacking efficient transactional support for composite Web services (CSs).In this paper, we propose a transactional approach to ensure the failure atomicity, of a CS, required by partners. We use the Accepted Termination States (ATS) property as a mean to express the required failure atomicity.Partners specify their CS, mainly its control flow, and the required ATS. Then, we use a set of transactional rules to assist designers to compose a valid CS with regards to the specified ATS.	Ensuring required failure atomicity of composite Web services	NA:NA:NA	2018
Dirk Beyer:Arindam Chakrabarti:Thomas A. Henzinger	We present a language for specifying web service interfaces. A web service interface puts three kinds of constraints on the users of the service. First, the interface specifies the methods that can be called by a client, together with types of input and output parameters; these are called signature constraints. Second, the interface may specify propositional constraints on method calls and output values that may occur in a web service conversation; these are called consistency constraints. Third, the interface may specify temporal constraints on the ordering of method calls; these are called protocol constraints. The interfaces can be used to check, first, if two or more web services are compatible, and second, if a web service A can be safely substituted for a web service B. The algorithm for compatibility checking verifies that two or more interfaces fulfill each others' constraints. The algorithm for substitutivity checking verifies that service A demands fewer and fulfills more constraints than service B.	Web service interfaces	NA:NA:NA	2018
Geert-Jan Houben	NA	Session details: Web application design	NA	2018
Ivan Kurtev:Klaas van den Berg	We present an approach in which the semantics of an XML language is defined by means of a transformation from an XML document model (an XML schema) to an application specific model. The application specific model implements the intended behavior of documents written in the language. A transformation is specified in a model transformation language used in the Model Driven Architecture (MDA) approach for software development. Our approach provides a better separation of three concerns found in XML applications: syntax, syntax processing logic and intended meaning of the syntax. It frees the developer of low-level syntactical details and improves the adaptability and reusability of XML applications. Declarative transformation rules and the explicit application model provide a finer control over the application parts affected by adaptations. Transformation rules and the application model for an XML language may be composed with the corresponding rules and application models defined for other XML languages. In that way we achieve reuse and composition of XML applications.	Building adaptable and reusable XML applications with model transformations	NA:NA	2018
Marco Brambilla:Stefano Ceri:Sara Comai:Christina Tziviskou	As the Web becomes a platform for implementing B2B applications, the need arises of Web conceptual models for describing Web oriented workflow applications implementing business processes. In this context, new problems about process correctness arise, due to the loose control of Web applications upon the behavior of their Web clients. Indeed, incoherent user's behavior can lead to inconsistent processes.This paper presents a high level approach to the management of exceptions that occur during the execution of processes on the Web. We present a classification of exceptions that can be raised inside workflow-driven Web applications, and recovery policies to retrieve coherent status and data after an exception. We devise these concepts at high level and then we exploit them using a Web modeling language (WebML) that in turn provides development facilities like automatic code generation, validation of hypertext models, and so on. An industrial implementation experience is briefly presented too.	Exception handling in workflow-driven Web applications	NA:NA:NA:NA	2018
Henning Qin Jehøj:Niels Olof Bouvin:Kaj Grønbæk	WebDAV needs awareness support in order to be a full-fledged collaboration system, This paper introduces AwareDAV, a new WebDAV extension framework enabling shared awareness through event notification. By extending the WebDAV protocol with seven new request-methods and an extensible XML based event subscription scheme, AwareDAV supports fine grained event subscriptions over a range of transport mechanisms and enables a wide range of collaboration scenarios. This paper describes the design of AwareDAV, its API, experiences with its initial implementation, as well as a comparison with Microsoft Exchange and WebDAV-notify.	AwareDAV: a generic WebDAV notification framework and implementation	NA:NA:NA	2018
Jim Hendler	NA	Session details: Semantic Web	NA	2018
Marta Sabou:Chris Wroe:Carole Goble:Gilad Mishne	The reasoning tasks that can be performed with semantic web service descriptions depend on the quality of the domain ontologies used to create these descriptions. However, building such domain ontologies is a time consuming and difficult task.We describe an automatic extraction method that learns domain ontologies for web service descriptions from textual documentations attached to web services. We conducted our experiments in the field of bioinformatics by learning an ontology from the documentation of the web services used in myGrid, a project that supports biology experiments on the Grid. Based on the evaluation of the extracted ontology in the context of the project, we conclude that the proposed extraction method is a helpful tool to support the process of building domain ontologies for web service descriptions.	Learning domain ontologies for Web service descriptions: an experiment in bioinformatics	NA:NA:NA:NA	2018
Lloyd Rutledge:Jacco van Ossenbruggen:Lynda Hardman	This paper discusses generating document structure from annotated media repositories in a domain-independent manner. This approaches the vision of a universal RDF browser. We start by applying the search-and-browse paradigm established for the WWW to RDF presentation. Furthermore, this paper adds to this paradigm the clustering-based derivation of document structure from search returns, providing simple but domain-independent hypermedia generation from RDF stores. While such generated presentations hardly meet the standards of those written by humans, they provide quick access to media repositories when the required document has not yet been written. The resulting system allows a user to specify a topic for which it generates a hypermedia document providing guided navigation through virtually any RDF repository. The impact for content providers is that as soon as one adds new media items and their annotations to a repository, they become immediately available for automatic integration into subsequently requested presentations.	Making RDF presentable: integrated global and local semantic Web browsing	NA:NA:NA	2018
Jonathan Trevor	NA	Session details: Applications	NA	2018
Paolo Avesani:Marco Cova	The interoperability among distributed and autonomous systems is the ultimate challenge facing the semantic web. Heterogeneity of data representation is the main source of problems. This paper proposes an innovative solution that combines lexical approaches and language games. The benefits for distributed annotation systems on the web are twofold: firstly, it will reduce the complexity of the semantic problem by moving the focus from the full-featured ontology level to the simpler lexicon level; secondly, it will avoid the drawback of a centralized third party mediator that may become a single point of failure.The main contributions of this work are concerned with: providing a proof of concept that language games can be an effective solution to creating and managing a distributed process of agreement on a shared lexicon,describing a fully distributed service oriented architecture for language games,providing empirical evidence on a real world case study in the domain of ski mountaineering.	Shared lexicon for distributed annotations on the Web	NA:NA	2018
Richard Cardone:Danny Soroker:Alpana Tiwari	The difficulty of developing and deploying commercial web applications increases as the number of technologies they use increases and as the interactions between these technologies become more complex. This paper describes a way to avoid this increasing complexity by re-examining the basic requirements of web applications. Our approach is to first separate client concerns from server concerns, and then to reduce the interaction between client and server to its most elemental: parameter passing. We define a simplified programming model for form-based web applications and we use XForms and a subset of J2EE as enabling technologies. We describe our implementation of an MVC-based application builder for this model, which automatically generates the code needed to marshal input and output data between clients and servers. This marshalling uses type checking and other forms of validation on both clients and servers. We also show how our programming model and application builder support the customization of web applications for different execution targets, including, for example, different client devices.	Using XForms to simplify Web programming	NA:NA:NA	2018
Mike Dowman:Valentin Tablan:Hamish Cunningham:Borislav Popov	The Rich News system, that can automatically annotate radio and television news with the aid of resources retrieved from the World Wide Web, is described. Automatic speech recognition gives a temporally precise but conceptually inaccurate annotation model. Information extraction from related web news sites gives the opposite: conceptual accuracy but no temporal data. Our approach combines the two for temporally accurate conceptual semantic annotation of broadcast news. First low quality transcripts of the broadcasts are produced using speech recognition, and these are then automatically divided into sections corresponding to individual news stories. A key phrases extraction component finds key phrases for each story and uses these to search for web pages reporting the same event. The text and meta-data of the web pages is then used to create index documents for the stories in the original broadcasts, which are semantically annotated using the KIM knowledge management platform. A web interface then allows conceptual search and browsing of news stories, and playing of the parts of the media files corresponding to each news story. The use of material from the World Wide Web allows much higher quality textual descriptions and semantic annotations to be produced than would have been possible using the ASR transcript directly. The semantic annotations can form a part of the Semantic Web, and an evaluation shows that the system operates with high precision, and with a moderate level of recall.	Web-assisted annotation, semantic indexing and search of television and radio news	NA:NA:NA:NA	2018
Frank McSherry	NA	Session details: Indexing and querying	NA	2018
Edleno S. de Moura:Célia F. dos Santos:Daniel R. Fernandes:Altigran S. Silva:Pavel Calado:Mario A. Nascimento	The unarguably fast, and continuous, growth of the volume of indexed (and indexable) documents on the Web poses a great challenge for search engines. This is true regarding not only search effectiveness but also time and space efficiency. In this paper we present an index pruning technique targeted for search engines that addresses the latter issue without disconsidering the former. To this effect, we adopt a new pruning strategy capable of greatly reducing the size of search engine indices. Experiments using a real search engine show that our technique can reduce the indices' storage costs by up to 60% over traditional lossless compression methods, while keeping the loss in retrieval precision to a minimum. When compared to the indices size with no compression at all, the compression rate is higher than 88%, i.e., less than one eighth of the original size. More importantly, our results indicate that, due to the reduction in storage overhead, query processing time can be reduced to nearly 65% of the original time, with no loss in average precision. The new method yields significative improvements when compared against the best known static pruning method for search engine indices. In addition, since our technique is orthogonal to the underlying search algorithms, it can be adopted by virtually any search engine.	Improving Web search efficiency via a locality based static pruning method	NA:NA:NA:NA:NA:NA	2018
Aris Anagnostopoulos:Andrei Z. Broder:David Carmel	We consider the problem of efficiently sampling Web search engine query results. In turn, using a small random sample instead of the full set of results leads to efficient approximate algorithms for several applications, such as: Determining the set of categories in a given taxonomy spanned by the search results;Finding the range of metadata values associated to the result set in order to enable "multi-faceted search;"Estimating the size of the result set;Data mining associations to the query terms.We present and analyze an efficient algorithm for obtaining uniform random samples applicable to any search engine based on posting lists and document-at-a-time evaluation. (To our knowledge, all popular Web search engines, e.g. Google, Inktomi, AltaVista, AllTheWeb, belong to this class.)Furthermore, our algorithm can be modified to follow the modern object-oriented approach whereby posting lists are viewed as streams equipped with a next method, and the next method for Boolean and other complex queries is built from the next method for primitive terms. In our case we show how to construct a basic next(p) method that samples term posting lists with probability p, and show how to construct next(p) methods for Boolean operators (AND, OR, WAND) from primitive methods.Finally, we test the efficiency and quality of our approach on both synthetic and real-world data.	Sampling search-engine results	NA:NA:NA	2018
Xiaohui Long:Torsten Suel	Large web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level.We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.	Three-level caching for efficient query processing in large Web search engines	NA:NA	2018
Yuji Inoue	This talk presents NTT's approach for realizing a Human-Centered Network. Last November, we announced the NTT Group's Medium-Term Management Strategy, which consists of three management objectives: (1) building the ubiquitous broadband market and helping achieve the e-Japan Strategy and the u-Japan Initiative; (2) building a safe, secure, and convenient communications network environment and broadband access infrastructure, while achieving a seamless migration from the legacy telephone network to the next generation network; and (3) striving to increase corporate value and achieve sustainable growth. Since the management strategy takes account of Japan's future social issues such as declining birthrate and aging population, the need to reduce the environmental load, etc, we believe that the R&D activities directed towards accomplishing these objectives consequently lead to the realization of a Human-Centered Network.	Innovation for a human-centered network: NTT's R&D activities for achieving the NTT group's medium-term management strategy	NA	2018
Jim Webber	NA	Session details: XML query and programming languages	NA	2018
Arnaud Sahuguet:Bogdan Alexe	This paper describes XSQirrel, a new XML query language that transforms a document into a sub-document, i.e. a tree where the root-to-leaf paths are a subset of the root-to-leaf paths from the original document.We show that this type of queries is extremely useful for various applications (e.g. web services) and that the currently existing query languages are poorly equipped to express, reason and evaluate such queries. In particular, we emphasize the need to be able to compose such queries. We present the XSQirrel language with its syntax, semantics and two language specific operators, union and composition.For the evaluation of the language, we leverage well established query technologies by translating XSQirrel expressions into XPath programs, XQuery queries or XSLT stylesheets.We provide some experimental results that compare our various evaluation strategies. We also show the runtime benefits of query composition over sequential evaluation.	Sub-document queries over XML with XSQirrel	NA:NA	2018
Matthew Harren:Mukund Raghavachari:Oded Shmueli:Michael G. Burke:Rajesh Bordawekar:Igor Pechtchanski:Vivek Sarkar	The increased importance of XML as a data representation format has led to several proposals for facilitating the development of applications that operate on XML data. These proposals range from runtime API-based interfaces to XML-based programming languages. The subject of this paper is XJ, a research language that proposes novel mechanisms for the integration of XML as a first-class construct into Java™. The design goals of XJ distinguish it from past work on integrating XML support into programming languages --- specifically, the XJ design adheres to the XML Schema and XPath standards. Moreover, it supports in-place updates of XML data thereby keeping with the imperative nature of Java. We have built a prototype compiler for XJ, and our preliminary experiments demonstrate that the performance of XJ programs can approach that of traditional low-level API-based interfaces, while providing a higher level of abstraction.	XJ: facilitating XML processing in Java	NA:NA:NA:NA:NA:NA:NA	2018
Li Chen:Elke A. Rundensteiner	Semantic caching is an important technology for improving the response time of future user queries specified over remote servers. This paper deals with the fundamental query containment problem in an XQuery-based semantic caching system. To our best knowledge, the impact of subtle differences in XQuery semantics caused by different ways of specifying variables on query containment has not yet been studied. We introduce the concept of variable binding dependencies for representing the hierarchical element dependencies preserved by an XQuery. We analyze the problem of XQuery containment in the presence of such dependencies. We propose a containment mapping technique for nested XQuery in presence of variable binding dependencies. The implication of the nested block structure on XQuery containment is also considered. We mention the performance gains achieved by a semantic caching system we build based on the proposed technique.	XQuery containment in presence of variable binding dependencies	NA:NA	2018
Lora Aroyo	NA	Session details: Web-based educational applications	NA	2018
Christina Brodersen:Bent G. Christensen:Kaj Grønbæk:Christian Dindler:Balasuthas Sundararajah	This paper describes the eBag infrastructure, which is a generic infrastructure inspired from work with school children who could benefit from a electronic schoolbag for collaborative handling of their digital material. The eBag infrastructure is utilizing the Context-aware HyCon framework and collaborative web services based on WebDAV. A ubiquitous login and logout mechanism has been built based on BlueTooth sensor networks. The eBag infrastructure has been tried out in field tests with school kids. In this paper we discuss experiences and design issues for ubiquitous Web integration in interactive school environments with multiple interactive whiteboards and workstations. This includes proposals for specialized and adaptive XLink structures for organizing school materials as well as issues in login/logout based on proximity of different display surfaces.	eBag: a ubiquitous Web infrastructure for nomadic learning	NA:NA:NA:NA:NA	2018
Dimitris Kotzinos:Sofia Pediaditaki:Apostolos Apostolidis:Nikolaos Athanasis:Vassilis Christophides	Online Curriculum Portals aim to support networks of instructors and learners by providing a space of convergence for enhancing peer-to-peer learning interactions among individuals of an educational institution. To this end, effective, open and scalable e-learning systems are required to acquire, store, and share knowledge under the form of learning objects (LO). In this paper, we are interested in exploiting the semantic relationships that characterize these LOs (e.g., prerequisite, part-of or see-also) in order to capture and access individual and group knowledge in conjunction with the learning processes supported by educational institutions. To achieve this functionality, Semantic Web (e.g., RDF/s) and declarative query languages (e.g., RQL) are employed to represent LOs and their relationships (e.g., LOM), as well as, to support navigation at the conceptual e-learning Portal space. In this way, different LOs could be presented to the same learners, according to the traversed schema navigation paths (i.e., learning paths). Using the Apache Jetspeed framework we are able to generate and assemble at run-time portlets (i.e., pluggable web components) for visualizing personalized views as dynamic web pages. Last but not least, both learners and instructors can employ the same Portal GUI for updating semantically described LOs and thus support an open-ended continuum of learning. To the best of our knowledge, the work presented in this paper is the first Online Curriculum Portal platform supporting the aforementioned functionality.	Online curriculum on the semantic Web: the CSD-UoC portal for peer-to-peer e-learning	NA:NA:NA:NA:NA	2018
Mark K. Singley:Richard B. Lam	Whereas schools typically record mounds of data regarding student performance, attendance, and other behaviors over the course of a school year, rarely is that data consulted and used to inform day-to-day instructional practice in the classroom. As teachers come under increasing pressure to ensure success for all of their students, we are attempting to provide tools to help teachers make sense of what is happening in their classrooms and take appropriate proactive and/or remedial action. One such tool is a Web service we've dubbed the Classroom Sentinel. The Classroom Sentinel mines electronic gradebook and other student information system data sources to detect critical teaching and learning patterns and bring those patterns to the attention of the teacher in the form of timely alerts. In this paper, we introduce the notion of classroom patterns, present some examples, and describe a framework for alert generation and delivery.	The classroom sentinel: supporting data-driven decision-making in the classroom	NA:NA	2018
Jong Wook Kim:K. Selçuk Candan:Mehmet E. Dönderler	Message hierarchies in web discussion boards grow with new postings. Threads of messages evolve as new postings focus within or diverge from the original themes of the threads. Thus, just by investigating the subject headings or contents of earlier postings in a message thread, one may not be able to guess the contents of the later postings. The resulting navigation problem is further compounded for blind users who need the help of a screen reader program that can provide only a linear representation of the content. We see that, in order to overcome the navigation obstacle for blind as well as sighted users, it is essential to develop techniques that help identify how the content of a discussion board grows through generalizations and specializations of topics. This knowledge can be used in segmenting the content in coherent units and guiding the users through segments relevant to their navigational goals. Our experimental results showed that the segmentation algorithm described in this paper provides up to 80-85% success rate in labeling messages. The algorithm is being deployed in a software system to reduce the navigational load of blind students in accessing web-based electronic course materials; however, we note that the techniques are equally applicable for developing web indexing and summarization tools for users with sight.	Topic segmentation of message hierarchies for indexing and navigation support	NA:NA:NA	2018
Philipp Cimiano:Günter Ladwig:Steffen Staab	Without the proliferation of formal semantic annotations, the Semantic Web is certainly doomed to failure. In earlier work we presented a new paradigm to avoid this: the 'Self Annotating Web', in which globally available knowledge is used to annotate resources such as web pages. In particular, we presented a concrete method instantiating this paradigm, called PANKOW (Pattern-based ANnotation through Knowledge On the Web). In PANKOW, a named entity to be annotated is put into several linguistic patterns that convey competing semantic meanings. The patterns that are matched most often on the Web indicate the meaning of the named entity --- leading to automatic or semi-automatic annotation.In this paper we present C-PANKOW (Context-driven PANKOW), which alleviates several shortcomings of PANKOW. First, by downloading abstracts and processing them off-line, we avoid the generation of large number of linguistic patterns and correspondingly large number of Google queries.Second, by linguistically analyzing and normalizing the downloaded abstracts, we increase the coverage of our pattern matching mechanism and overcome several limitations of the earlier pattern generation process. Third, we use the annotation context in order to distinguish the significance of a pattern match for the given annotation task. Our experiments show that C-PANKOW inherits all the advantages of PANKOW (no training required etc.), but in addition it is far more efficient and effective.	Gimme' the context: context-driven automatic semantic annotation with C-PANKOW	NA:NA:NA	2018
Bing Liu:Minqing Hu:Junsheng Cheng	The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly.	Opinion observer: analyzing and comparing opinions on the Web	NA:NA:NA	2018
Lorrie Faith Cranor	Internet users now rely on a whole arsenal of tools to protect their security and privacy. Experts recommend that computer users install personal firewalls, anti-virus software, spyware blockers, spam filters, cookie managers, and a variety of other tools to keep themselves safe. Users are told to pick hard-to-guess passwords, use a different password at every Web site, and not to write any of their passwords down. They are told to read privacy policies before providing personal information to Web sites, look for lock icons before typing in a credit card number, refrain from opening email attachments from people they don't know, and even to think twice about opening email attachments from people they do know. With so many do's and don'ts, it is not surprising that much of this advice is ignored. In this talk I will highlight usability problems that make it difficult for people to protect their privacy and security on the Web, and I will discuss a number of approaches to addressing these problems.	Towards usable Web privacy and security	NA	2018
Piero Fraternali	NA	Session details: Web engineering with semantic annotation	NA	2018
Peter Plessers:Sven Casteleyn:Yeliz Yesilada:Olga De Troyer:Robert Stevens:Simon Harper:Carole Goble	Currently, the vast majority of web sites do not support accessibility for visually impaired users. Usually, these users have to rely on screen readers: applications that sequentially read the content of a web page in audio. Unfortunately, screen readers are not able to detect the meaning of the different page objects, and thus the implicit semantic knowledge conveyed in the presentation of the page is lost. One approach described in literature to tackle this problem, is the Dante approach, which allows semantic annotation of web pages to provide screen readers with extra (semantic) knowledge to better facilitate the audio presentation of a web page. Until now, such annotations were done manually, and failed for dynamic pages. In this paper, we combine the Dante approach with a web design method, WSDM, to fully automate the generation of the semantic annotation for visually impaired users. To do so, the semantic knowledge gathered during the design process is exploited, and the annotations are generated as a by-product of the design process, requiring no extra effort from the designer.	Accessibility: a Web engineering approach	NA:NA:NA:NA:NA:NA:NA	2018
Kumiko Tanaka-Ishii:Hiroshi Nakagawa	We present a usage consultation tool, based on Internet searching, for language learners. When a user enters a string of words for which he wants to find usages, the system sends this string as a query to a search engine and obtains search results about the string. The usages are extracted by performing statistical analysis on snippets and then fed back to the user.Unlike existing tools, this usage consultation tool is multi-lingual, so that usages can be obtained even in a language for which there are no well-established analytical methods. Our evaluation has revealed that usages can be obtained more effectively than by only using a search engine directly. Also, we have found that the resulting usage does not depend on the search engine for a prominent usage when the amount of data downloaded from the search engine is increased.	A multilingual usage consultation tool based on internet searching: more than a search engine, less than QA	NA:NA	2018
Oscar Díaz:Jon Iturrioz:Arantza Irastorza	Portlets (i.e. multi-step, user-facing applications to be syndicated within a portal) are currently supported by most portal frameworks. However, there is not yet a definitive answer to portlet interoperation whereby data flows smoothly from one portlet to a neighbouring one. Both data-based and API-based approaches exhibit some drawbacks in either the limitation of the sharing scope or the standardization effort required. We argue that these limitations can be overcome by using deep annotation techniques. By providing additional markup about the background services, deep annotation strives to interact with these underlying services rather than with the HTML surface that conveys the markup. In this way, the portlet producer can extend a portlet markup, a fragment, with data about the processes whose rendering this fragment supports. Then, the portlet consumer (e.g. a portal) can use deep annotation to map an output process in fragment A to an input process in fragment B. This mapping results in fragment B having its input form (or other "input" widget) filled up. We consider deep annotation as particularly valid for portlet interoperation due to the controlled and cooperative environment that characterizes the portal setting.	Improving portlet interoperability through deep annotation	NA:NA:NA	2018
Brian Davidson	NA	Session details: User-focused search and crawling	NA	2018
Jian-Tao Sun:Hua-Jun Zeng:Huan Liu:Yuchang Lu:Zheng Chen	As the competition of Web search market increases, there is a high demand for personalized Web search to conduct retrieval incorporating Web users' information needs. This paper focuses on utilizing clickthrough data to improve Web search. Since millions of searches are conducted everyday, a search engine accumulates a large volume of clickthrough data, which records who submits queries and which pages he/she clicks on. The clickthrough data is highly sparse and contains different types of objects (user, query and Web page), and the relationships among these objects are also very complicated. By performing analysis on these data, we attempt to discover Web users' interests and the patterns that users locate information. In this paper, a novel approach CubeSVD is proposed to improve Web search. The clickthrough data is represented by a 3-order tensor, on which we perform 3-mode analysis using the higher-order singular value decomposition technique to automatically capture the latent factors that govern the relations among these multi-type objects: users, queries and Web pages. A tensor reconstructed based on the CubeSVD analysis reflects both the observed interactions among these objects and the implicit associations among them. Therefore, Web search activities can be carried out based on CubeSVD analysis. Experimental evaluations using a real-world data set collected from an MSN search engine show that CubeSVD achieves encouraging search results in comparison with some standard methods.	CubeSVD: a novel approach to personalized Web search	NA:NA:NA:NA:NA	2018
Uichin Lee:Zhenyu Liu:Junghoo Cho	There has been recent interests in studying the "goal" behind a user's Web query, so that this goal can be used to improve the quality of a search engine's results. Previous studies have mainly focused on using manual query-log investigation to identify Web query goals. In this paper we study whether and how we can automate this goal-identification process. We first present our results from a human subject study that strongly indicate the feasibility of automatic query-goal identification. We then propose two types of features for the goal-identification task: user-click behavior and anchor-link distribution. Our experimental evaluation shows that by combining these features we can correctly identify the goals for 90% of the queries studied.	Automatic identification of user goals in Web search	NA:NA:NA	2018
Sandeep Pandey:Christopher Olston	Search engines are the primary gateways of information access on the Web today. Behind the scenes, search engines crawl the Web to populate a local indexed repository of Web pages, used to answer user search queries. In an aggregate sense, the Web is very dynamic, causing any repository of Web pages to become out of date over time, which in turn causes query answer quality to degrade. Given the considerable size, dynamicity, and degree of autonomy of the Web as a whole, it is not feasible for a search engine to maintain its repository exactly synchronized with the Web.In this paper we study how to schedule Web pages for selective (re)downloading into a search engine repository. The scheduling objective is to maximize the quality of the user experience for those who query the search engine. We begin with a quantitative characterization of the way in which the discrepancy between the content of the repository and the current content of the live Web impacts the quality of the user experience. This characterization leads to a user-centric metric of the quality of a search engine's local repository. We use this metric to derive a policy for scheduling Web page (re)downloading that is driven by search engine usage and free of exterior tuning parameters. We then focus on the important subproblem of scheduling refreshing of Web pages already present in the repository, and show how to compute the priorities efficiently. We provide extensive empirical comparisons of our user-centric method against prior Web page refresh strategies, using real Web data. Our results demonstrate that our method requires far fewer resources to maintain same search engine quality level for users, leaving substantially more resources available for incorporating new Web pages into the search repository.	User-centric Web crawling	NA:NA	2018
Mark Manasse	NA	Session details: Trustworthy Web sites	NA	2018
Guilin Wang	A fair contract signing protocol allows two potentially mistrusted parities to exchange their commitments (i.e., digital signatures) to an agreed contract over the Internet in a fair way, so that either each of them obtains the other's signature, or neither party does. Based on the RSA signature scheme, a new digital contract signing protocol is proposed in this paper. Like the existing RSA-based solutions for the same problem, our protocol is not only fair, but also optimistic, since the third trusted party is involved only in the situations where one party is cheating or the communication channel is interrupted. Furthermore, the proposed protocol satisfies a new property, i.e., it is abuse-free. That is, if the protocol is executed unsuccessfully, none of the two parties can show the validity of intermediate results to others. Technical details are provided to analyze the security and performance of the proposed protocol. In summary, we present the first abuse-free fair contract signing protocol based on the RSA signature, and show that it is both secure and efficient.	An abuse-free fair contract signing protocol based on the RSA signature	NA	2018
Mudhakar Srivatsa:Li Xiong:Ling Liu	Reputation systems have been popular in estimating the trustworthiness and predicting the future behavior of nodes in a large-scale distributed system where nodes may transact with one another without prior knowledge or experience. One of the fundamental challenges in distributed reputation management is to understand vulnerabilities and develop mechanisms that can minimize the potential damages to a system by malicious nodes. In this paper, we identify three vulnerabilities that are detrimental to decentralized reputation management and propose TrustGuard - a safeguard framework for providing a highly dependable and yet efficient reputation system. First, we provide a dependable trust model and a set of formal methods to handle strategic malicious nodes that continuously change their behavior to gain unfair advantages in the system. Second, a transaction based reputation system must cope with the vulnerability that malicious nodes may misuse the system by flooding feedbacks with fake transactions. Third, but not least, we identify the importance of filtering out dishonest feedbacks when computing reputation-based trust of a node, including the feedbacks filed by malicious nodes through collusion. Our experiments show that, comparing with existing reputation systems, our framework is highly dependable and effective in countering malicious nodes regarding strategic oscillating behavior, flooding malevolent feedbacks with fake transactions, and dishonest feedbacks.	TrustGuard: countering vulnerabilities in reputation management for decentralized overlay networks	NA:NA:NA	2018
Yasuhiko Minamide	Server-side programming is one of the key technologies that support today's WWW environment. It makes it possible to generate Web pages dynamically according to a user's request and to customize pages for each user. However, the flexibility obtained by server-side programming makes it much harder to guarantee validity and security of dynamically generated pages.To check statically the properties of Web pages generated dynamically by a server-side program, we develop a static program analysis that approximates the string output of a program with a context-free grammar. The approximation obtained by the analyzer can be used to check various properties of a server-side program and the pages it generates.To demonstrate the effectiveness of the analysis, we have implemented a string analyzer for the server-side scripting language PHP. The analyzer is successfully applied to publicly available PHP programs to detect cross-site scripting vulnerabilities and to validate pages they generate dynamically.	Static approximation of dynamically generated Web pages	NA	2018
Junghoo Cho	NA	Session details: Semantic search	NA	2018
Michael J. Cafarella:Oren Etzioni	Many modern natural language-processing applications utilize search engines to locate large numbers of Web documents or to compute statistics over the Web corpus. Yet Web search engines are designed and optimized for simple human queries---they are not well suited to support such applications. As a result, these applications are forced to issue millions of successive queries resulting in unnecessary search engine load and in slow applications with limited scalability.In response, this paper introduces the Bindings Engine (BE), which supports queries containing typed variables and  string-processing functions. For example, in response to the query  "powerful ‹noun›" BE will return all the nouns in its index that immediately follow the word "powerful", sorted by frequency. In response to the query  "Cities such as ProperNoun(Head(‹NounPhrase›))", BE will return a list of proper nouns likely to be city names.BE's novel  neighborhood index enables it to do so with O(k) random disk seeks and O(k) serial disk reads, where k is the number of non-variable terms in its query. As a result, BE can yield several orders of magnitude speedup for large-scale language-processing applications. The main cost is a modest increase in space to store the index. We report on experiments validating these claims, and analyze how BE's space-time tradeoff scales with the size of its index and the number of variable types. Finally, we describe how a BE-based application extracts thousands of facts from the Web at interactive speeds in response to simple user queries.	A search engine for natural language applications	NA:NA	2018
Lei Zhang:Yong Yu:Jian Zhou:ChenXi Lin:Yin Yang	Semantic Portal is the next generation of web portals that are powered by Semantic Web technologies for improved information sharing and exchange for a community of users. Current methods of searching in Semantic Portals are limited to keyword-based search using information retrieval (IR) techniques, ontology-based formal query and reasoning, or a simple combination of the two. In this paper, we propose an enhanced model that tightly integrates IR with formal query and reasoning to fully utilize both textual and semantic information for searching in Semantic Portals. The model extends the search capabilities of existing methods and can answer more complex search requests. The ideas in a fuzzy description logic (DL) IR model and a formal DL query method are employed and combined in our model. Based on the model, a semantic search service is implemented and evaluated. The evaluation shows very large improvements over existing methods.	An enhanced model for searching in semantic portals	NA:NA:NA:NA:NA	2018
Ron Bekkerman:Andrew McCallum	Say you are looking for information about a particular person. A search engine returns many pages for that person's name but which pages are about the person you care about, and which are about other people who happen to have the same name? Furthermore, if we are looking for multiple people who are related in some way, how can we best leverage this social network? This paper presents two unsupervised frameworks for solving this problem: one based on link structure of the Web pages, another using Agglomerative/Conglomerative Double Clustering (A/CDC)---an application of a recently introduced multi-way distributional clustering method. To evaluate our methods, we collected and hand-labeled a dataset of over 1000 Web pages retrieved from Google queries on 12 personal names appearing together in someones in an email folder. On this dataset our methods outperform traditional agglomerative clustering by more than 20%, achieving over 80% F-measure.	Disambiguating Web appearances of people in a social network	NA:NA	2018
J. Alex Halderman:Brent Waters:Edward W. Felten	Computer users are asked to generate, keep secret, and recall an increasing number of passwords for uses including host accounts, email servers, e-commerce sites, and online financial services. Unfortunately, the password entropy that users can comfortably memorize seems insufficient to store unique, secure passwords for all these accounts, and it is likely to remain constant as the number of passwords (and the adversary's computational power) increases into the future. In this paper, we propose a technique that uses a strengthened cryptographic hash function to compute secure passwords for arbitrarily many accounts while requiring the user to memorize only a single short password. This mechanism functions entirely on the client; no server-side changes are needed. Unlike previous approaches, our design is both highly resistant to brute force attacks and nearly stateless, allowing users to retrieve their passwords from any location so long as they can execute our program and remember a short secret. This combination of security and convenience will, we believe, entice users to adopt our scheme. We discuss the construction of our algorithm in detail, compare its strengths and weaknesses to those of related approaches, and present Password Multiplier, an implementation in the form of an extension to the Mozilla Firefox web browser.	A convenient method for securely managing passwords	NA:NA:NA	2018
Stephen E. Levy:Carl Gutwin	Website privacy policies state the ways that a site will use personal identifiable information (PII) that is collected from fields and forms in web-based transactions. Since these policies can be complex, machine-readable versions have been developed that allow automatic comparison of a site's privacy policy with a user's privacy preferences. However, it is still difficult for users to determine the cause and origin of conformance conflicts, because current standards operate at the page level - they can only say that there is a conflict on the page, not where the conflict occurs or what causes it. In this paper we describe fine-grained policy anchors, an extension to the way a website implements the Platform for Privacy Preferences (P3P), that solves this problem. Fine grained policy anchors enable field-level comparisons of policy and preference, field-specific conformance displays, and faster access to additional conformance information. We built a prototype user agent based on these extensions and tested it with representative users. We found that fine-grained anchors do help users understand how privacy policy relates to their privacy preferences, and where and why conformance conflicts occur.	Improving understanding of website privacy policies with fine-grained policy anchors	NA:NA	2018
Haidong Xia:José Carlos Brustoloni	Existing Web browsers handle security errors in a manner that often confuses users. In particular, when a user visits a secure site whose certificate the browser cannot verify, the browser typically allows the user to view and install the certificate and connect to the site despite the verification failure. However, few users understand the risk of man-in-the-middle attacks and the principles behind certificate-based authentication. We propose context-sensitive certificate verification (CSCV), whereby the browser interrogates the user about the context in which a certificate verification error occurs. Considering the context, the browser then guides the user in handling and possibly overcoming the security error. We also propose specific password warnings (SPW) when users are about to send passwords in a form vulnerable to eavesdropping. We performed user studies to evaluate CSCV and SPW. Our results suggest that CSCV and SPW can greatly improve Web browsing security and are easy to use even without training. Moreover, CSCV had greater impact than did staged security training.	Hardening Web browsers against man-in-the-middle and eavesdropping attacks	NA:NA	2018
Prashant Shenoy	NA	Session details: Measurements and analysis	NA	2018
Balachander Krishnamurthy:Harsha V. Madhyastha:Oliver Spatscheck	Web performance measurements and availability tests have been carried out using a variety of infrastructures over the last several years. Disruptions in the Internet can lead to Web sites being unavailable or increase user-perceived latency. The unavailability could be due to DNS, failures in segments of the physical network cutting off thousands of users, or attacks. Prompt reactions to network-wide events can be facilitated by local or remote measurement and monitoring. Better yet, a distributed set of intercommunicating measurement and monitoring entities that react to events dynamically could go a long way to handle disruptions.We have designed and built ATMEN, a triggered measurement infrastructure to communicate and coordinate across various administrative entities. ATMEN nodes can trigger new measurements, query ongoing passive measurements or historical measurements stored on remote nodes, and coordinate the responses to make local decisions. ATMEN reduces wasted measurements by judiciously reusing measurements along three axes: spatial, temporal, and application.We describe the use of ATMEN for key Web applications such as performance based ranking of popular Web sites and availability of DNS servers on which most Web transactions are dependent. The evaluation of ATMEN is done using multiple network monitoring entities called Gigascopes installed across the USA, measurement data of a popular network application involving millions of users distributed across the Internet, and scores of clients to aid in gathering measurement information upon demand. Our results show that such a system can be built in a scalable fashion.	ATMEN: a triggered network measurement infrastructure	NA:NA:NA	2018
Mark Meiss:Filippo Menczer:Alessandro Vespignani	We offer the first large-scale analysis of Web traffic based on network flow data. Using data collected on the Internet2 network, we constructed a weighted bipartite client-server host graph containing more than 18 x 106 vertices and 68 x 106 edges valued by relative traffic flows. When considered as a traffic map of the World-Wide Web, the generated graph provides valuable information on the statistical patterns that characterize the global information flow on the Web. Statistical analysis shows that client-server connections and traffic flows exhibit heavy-tailed probability distributions lacking any typical scale. In particular, the absence of an intrinsic average in some of the distributions implies the absence of a prototypical scale appropriate for server design, Web-centric network design, or traffic modeling. The inspection of the amount of traffic handled by clients and servers and their number of connections highlights non-trivial correlations between information flow and patterns of connectivity as well as the presence of anomalous statistical patterns related to the behavior of users on the Web. The results presented here may impact considerably the modeling, scalability analysis, and behavioral study of Web applications.	On the lack of typical behavior in the global Web traffic network	NA:NA:NA	2018
Lei Guo:Songqing Chen:Zhen Xiao:Xiaodong Zhang	In this paper, we study the media workload collected from a large number of commercial Web sites hosted by a major ISP and that collected from a large group of home users connected to the Internet via a well-known cable company. Some of our key findings are: (1) Surprisingly, the majority of media contents are still delivered via downloading from Web servers. (2) A substantial percentage of media downloading connections are aborted before completion due to the long waiting time. (3) A hybrid approach, pseudo streaming, is used by clients to imitate real streaming. (4) The mismatch between the downloading rate and the client playback speed in pseudo streaming is common, which either causes frequent playback delays to the clients, or unnecessary traffic to the Internet. (5) Compared with streaming, downloading and pseudo streaming are neither bandwidth efficient nor performance effective. To address this problem, we propose the design of AutoStream, an innovative system that can provide additional previewing and streaming services automatically for media objects hosted on standard Web sites in server farms at the client's will.	Analysis of multimedia workloads with implications for internet streaming	NA:NA:NA:NA	2018
Rob Glaser	NA	Real and the future of digital media	NA	2018
P. A. Bonatti:P. Festa	While many works have been devoted to service matchmaking and modeling nonfunctional properties, the problem of matching service requests to offers in an optimal way has not yet been extensively studied. In this paper we formalize three kinds of optimal service selection problems, based on different criteria. Then we study their complexity and implement solutions. We prove that one-time costs make the optimal selection problem computationally hard; in the absence of these costs the problem can be solved in polynomial time. We designed and implemented both exact and heuristic (suboptimal) algorithms for the hard case, and carried out a preliminary experimental evaluation with interesting results.	On optimal service selection	NA:NA	2018
Milenko Petrovic:Haifeng Liu:Hans-Arno Jacobsen	RDF is increasingly being used to represent metadata. RDF Site Summary (RSS) is an application of RDF on the Web that has considerably grown in popularity. However, the way RSS systems operate today does not scale well. In this paper we introduce G-ToPSS, a scalable publish/subscribe system for selective information dissemination. G-ToPSS is particularly well suited for applications that deal with large-volume content distribution from diverse sources. RSS is an instance of the content distribution problem. G-ToPSS allows use of ontology as a way to provide additional information about the data. Furthermore, in this paper we show how G-ToPSS can support RDFS class taxonomies. We have implemented and experimentally evaluated G-ToPSS and we provide results in the paper demonstrating its scalability compared to alternatives.	G-ToPSS: fast filtering of graph-based metadata	NA:NA:NA	2018
Kris Cardinaels:Michael Meire:Erik Duval	In this paper, we focus on the development of a framework for automatic metadata generation. The first step towards this framework is the definition of an Application Programmer Interface (API), which we call the Simple Indexing Interface (SII). The second step is the definition of a framework for implementation of the SII. Both steps are presented in some detail in this paper. We also report on empirical evaluation of the metadata that the SII and supporting framework generated in a real-life context.	Automating metadata generation: the simple indexing interface	NA:NA:NA	2018
Torsten Suel	NA	Session details: Link-based ranking	NA	2018
Paolo Boldi:Massimo Santini:Sebastiano Vigna	PageRank is defined as the stationary state of a Markov chain. The chain is obtained by perturbing the transition matrix induced by a web graph with a damping factor α that spreads uniformly part of the rank. The choice of α is eminently empirical, and in most cases the original suggestion α = 0.85 by Brin and Page is still used. Recently, however, the behaviour of PageRank with respect to changes in α was discovered to be useful in link-spam detection[21]. Moreover, an analytical justification of the value chosen for α is still missing. In this paper, we give the first mathematical analysis of PageRank when α changes. In particular, we show that, contrarily to popular belief, for real-world graphs values of α close to 1 do not give a more meaningful ranking. Then, we give closed-form formulae for PageRank derivatives of any order, and an extension of the Power Method that approximates them with convergence O (tk αt) for the k-th derivative. Finally, we show a tight connection between iterated computation and analytical behaviour by proving that the k-th iteration of the Power Method gives exactly the PageRank value obtained using a Maclaurin polynomial of degree k. The latter result paves the way towards the application of analytical methods to the study of PageRank.	PageRank as a function of the damping factor	NA:NA:NA	2018
Zaiqing Nie:Yuanzhi Zhang:Ji-Rong Wen:Wei-Ying Ma	In contrast with the current Web search methods that essentially do document-level ranking and retrieval, we are exploring a new paradigm to enable Web search at the object level. We collect Web information for objects relevant for a specific application domain and rank these objects in terms of their relevance and popularity to answer user queries. Traditional PageRank model is no longer valid for object popularity calculation because of the existence of heterogeneous relationships between objects. This paper introduces PopRank, a domain-independent object-level link analysis model to rank the objects within a specific domain. Specifically we assign a popularity propagation factor to each type of object relationship, study how different popularity propagation factors for these heterogeneous relationships could affect the popularity ranking, and propose efficient approaches to automatically decide these factors. Our experiments are done using 1 million CS papers, and the experimental results show that PopRank can achieve significantly better ranking results than naively applying PageRank on the object graph.	Object-level ranking: bringing order to Web objects	NA:NA:NA:NA	2018
Frank McSherry	In this note we consider a simple reformulation of the traditional power iteration algorithm for computing the stationary distribution of a Markov chain. Rather than communicate their current probability values to their neighbors at each step, nodes instead communicate only changes in probability value. This reformulation enables a large degree of flexibility in the manner in which nodes update their values, leading to an array of optimizations and features, including faster convergence, efficient incremental updating, and a robust distributed implementation.While the spirit of many of these optimizations appear in previous literature, we observe several cases where this unification simplifies previous work, removing technical complications and extending their range of applicability. We implement and measure the performance of several optimizations on a sizable (34M node) web subgraph, seeing significant composite performance gains, especially for the case of incremental recomputation after changes to the web graph.	A uniform approach to accelerated PageRank computation	NA	2018
Yoelle Maarek	NA	Session details: Improving the browsing experience	NA	2018
Anne Aula:Natalie Jhaveri:Mika Käki	Experienced web users have strategies for information search and re-access that are not directly supported by web browsers or search engines. We studied how prevalent these strategies are and whether even experienced users have problems with searching and re-accessing information. With this aim, we conducted a survey with 236 experienced web users. The results showed that this group has frequently used key strategies (e.g., using several browser windows in parallel) that they find important, whereas some of the strategies that have been suggested in previous studies are clearly less important for them (e.g., including URLs on a webpage). In some aspects, such as query formulation, this group resembles less experienced web users. For instance, we found that most of the respondents had misconceptions about how their search engine handles queries, as well as other problems with information search and re-access. In addition to presenting the prevalence of the strategies and rationales for their use, we present concrete designs solutions and ideas for making the key strategies also available to less experienced users.	Information search and re-access strategies of experienced web users	NA:NA:NA	2018
Saikat Mukherjee:I. V. Ramakrishnan	Focused Web browsing activities such as periodically looking up headline news, weather reports, etc., which require only selective fragments of particular Web pages, can be made more efficient for users of limited-display-size handheld mobile devices by delivering only the target fragments. Semantic bookmarks provide a robust conceptual framework for recording and retrieving such targeted content not only from the specific pages used in creating the bookmarks but also from any user-specified page with similar content semantics. This paper describes a technique for realizing semantic bookmarks by coupling machine learning with Web page segmentation to create a statistical model of the bookmarked content. These models are used to identify and retrieve the bookmarked content from Web pages that share a common content domain. In contrast to ontology-based approaches where semantic bookmarks are limited to available concepts in the ontology, the learning-based approach allows users to bookmark ad-hoc personalized semantic concepts to effectively target content that fits the limited display of handhelds. User evaluation measuring the effectiveness of a prototype implementation of learning-based semantic bookmarking at reducing browsing fatigue in handhelds is provided.	Browsing fatigue in handhelds: semantic bookmarking spells relief	NA:NA	2018
Shaya Potter:Jason Nieh	We present WebPod, a portable system that enables mobile users to use the same persistent, personalized web browsing session on any Internet-enabled device. No matter what computer is being used, WebPod provides a consistent browsing session, maintaining all of a user's plugins, bookmarks, browser web content, open browser windows, and browser configuration options and preferences. This is achieved by leveraging rapid improvements in capacity, cost, and size of portable storage devices. WebPod provides a virtualization and checkpoint/restart mechanism that decouples the browsing environment from the host, enabling web browsing sessions to be suspended to portable storage, carried around, and resumed from the storage device on another computer. WebPod virtualization also isolates web browsing sessions from the host, protecting the browsing privacy of the user and preventing malicious web content from damaging the host. We have implemented a Linux WebPod prototype and demonstrate its ability to quickly suspend and resume web browsing sessions, enabling a seamless web browsing experience for mobile users as they move among computers.	WebPod: persistent Web browsing sessions with pocketable storage devices	NA:NA	2018
Steffen Staab	NA	Session details: Semantic Web foundations	NA	2018
Jeremy J. Carroll:Christian Bizer:Pat Hayes:Patrick Stickler	The Semantic Web consists of many RDF graphs nameable by URIs. This paper extends the syntax and semantics of RDF to cover such Named Graphs. This enables RDF statements that describe graphs, which is beneficial in many Semantic Web application areas. As a case study, we explore the application area of Semantic Web publishing: Named Graphs allow publishers to communicate assertional intent, and to sign their graphs; information consumers can evaluate specific graphs using task-specific trust policies, and act on information from those Named Graphs that they accept. Graphs are trusted depending on: their content; information about the graph; and the task the user is performing. The extension of RDF to Named Graphs provides a formally defined framework to be a foundation for the Semantic Web trust layer.	Named graphs, provenance and trust	NA:NA:NA:NA	2018
Jos de Bruijn:Rubén Lara:Axel Polleres:Dieter Fensel	The Semantic Web languages RDFS and OWL have been around for some time now. However, the presence of these languages has not brought the breakthrough of the Semantic Web the creators of the languages had hoped for. OWL has a number of problems in the area of interoperability and usability in the context of many practical application scenarios which impede the connection to the Software Engineering and Database communities. In this paper we present OWL Flight, which is loosely based on OWL, but the semantics is grounded in Logic Programming rather than Description Logics, and it borrows the constraint-based modeling style common in databases. This results in different types of modeling primitives and enforces a different style of ontology modeling. We analyze the modeling paradigms of OWL DL and OWL Flight, as well as reasoning tasks supported by both languages. We argue that different applications on the Semantic Web require different styles of modeling and thus both types of languages are required for the Semantic Web.	OWL DL vs. OWL flight: conceptual modeling and reasoning for the semantic Web	NA:NA:NA:NA	2018
Bijan Parsia:Evren Sirin:Aditya Kalyanpur	As an increasingly large number of OWL ontologies become available on the Semantic Web and the descriptions in the ontologies become more complicated, finding the cause of errors becomes an extremely hard task even for experts. Existing ontology development environments provide some limited support, in conjunction with a reasoner, for detecting and diagnosing errors in OWL ontologies. Typically these are restricted to the mere detection of, for example, unsatisfiable concepts. We have integrated a number of simple debugging cues generated from our description logic reasoner, Pellet, in our hypertextual ontology development environment, Swoop. These cues, in conjunction with extensive undo/redo and Annotea based collaboration support in Swoop, significantly improve the OWL debugging experience, and point the way to more general improvements in the presentation of an ontology to new users.	Debugging OWL ontologies	NA:NA:NA	2018
Marc Najork	NA	Session details: Link-based similarity	NA	2018
Dániel Fogaras:Balázs Rácz	To exploit the similarity information hidden in the hyperlink structure of the web, this paper introduces algorithms scalable to graphs with billions of vertices on a distributed architecture. The similarity of multi-step neighborhoods of vertices are numerically evaluated by similarity functions including SimRank [20], a recursive refinement of cocitation; PSimRank, a novel variant with better theoretical characteristics; and the Jaccard coefficient, extended to multi-step neighborhoods. Our methods are presented in a general framework of Monte Carlo similarity search algorithms that precompute an index database of random fingerprints, and at query time, similarities are estimated from the fingerprints. The performance and quality of the methods were tested on the Stanford Webbase [19] graph of 80M pages by comparing our scores to similarities extracted from the ODP directory [26]. Our experimental results suggest that the hyperlink structure of vertices within four to five steps provide more adequate information for similarity search than single-step neighborhoods.	Scaling link-based similarity search	NA:NA	2018
Mayank Bawa:Tyson Condie:Prasanna Ganesan	We consider the problem of indexing high-dimensional data for answering (approximate) similarity-search queries. Similarity indexes prove to be important in a wide variety of settings: Web search engines desire fast, parallel, main-memory-based indexes for similarity search on text data; database systems desire disk-based similarity indexes for high-dimensional data, including text and images; peer-to-peer systems desire distributed similarity indexes with low communication cost. We propose an indexing scheme called LSH Forest which is applicable in all the above contexts. Our index uses the well-known technique of locality-sensitive hashing (LSH), but improves upon previous designs by (a) eliminating the different data-dependent parameters for which LSH must be constantly hand-tuned, and (b) improving on LSH's performance guarantees for skewed data distributions while retaining the same storage and query overhead. We show how to construct this index in main memory, on disk, in parallel systems, and in peer-to-peer systems. We evaluate the design with experiments on multiple text corpora and demonstrate both the self-tuning nature and the superior performance of LSH Forest.	LSH forest: self-tuning indexes for similarity search	NA:NA:NA	2018
Hidehiko Ino:Mineichi Kudo:Atsuyoshi Nakamura	We introduce a stricter Web community definition to overcome boundary ambiguity of a Web community defined by Flake, Lawrence and Giles [2], and consider the problem of finding communities that satisfy our definition. We discuss how to find such communities and hardness of this problem.We also propose Web page partitioning by equivalence relation defined using the class of communities of our definition. Though the problem of efficiently finding all communities of our definition is NP-complete, we propose an efficient method of finding a subclass of communities among the sets partitioned by each of n-1 cuts represented by a Gomory-Hu tree [10], and partitioning a Web graph by equivalence relation defined using the subclass.According to our preliminary experiments, partitioning by our method divided the pages retrieved by keyword search into several different categories to some extent.	Partitioning of Web graphs by community topology	NA:NA:NA	2018
Murata Makoto	NA	Session details: XML parsing and stylesheets	NA	2018
Makoto Onizuka:Fong Yee Chan:Ryusuke Michigami:Takashi Honishi	This paper proposes an incremental maintenance algorithm that efficiently updates the materialized XPath/XSLT views defined using XPath expressions in XP([],*,//,vars). The algorithm consists of two processes. 1) The dynamic execution flow of an XSLT program is stored as an XT (XML Transformation) tree during the full transformation. 2) In response to a source XML data update, the impacted portions of the XT-tree are identified and maintained by partially re-evaluating the XSLT program. This paper discusses the XPath/XSLT features of incremental view maintenance for subtree insertion/deletion and applies them to the maintenance algorithm. Experiments show that the incremental maintenance algorithm outperforms full XML transformation algorithms by factors of up to 500.	Incremental maintenance for materialized XPath/XSLT views	NA:NA:NA:NA	2018
Achille Fokoue:Kristoffer Rose:Jérôme Siméon:Lionel Villard	As XQuery is gathering momentum as the standard query language for XML, there is a growing interest in using it as an integral part of the XML application development infrastructure. In that context, one question which is often raised is how well XQuery interoperates with other XML languages, and notably with XSLT. XQuery 1.0 [16] and XSLT 2.0 [7] share a lot in common: they share XPath 2.0 as a common sub-language and have the same expressiveness. However, they are based on fairly different programming paradigms. While XSLT has adopted a highly declarative template based approach, XQuery relies on a simpler, and more operational, functional approach.In this paper, we present an approach to compile XSLT 2.0 into XQuery 1.0, and a working implementation of that approach. The compilation rules explain how XSLT's template-based approach can be implemented using the functional approach of XQuery and underpins the tight connection between the two languages. The resulting compiler can be used to migrate a XSLT code base to XQuery, or to enable the use of XQuery runtimes (e.g., as will soon be provided by most relational database management systems) for XSLT users. We also identify a number of areas where compatibility between the two languages could be improved. Finally, we show experiments on actual XSLT stylesheets, demonstrating the applicability of the approach in practice.	Compiling XSLT 2.0 into XQuery 1.0	NA:NA:NA:NA	2018
Toshiro Takase:Hisashi MIYASHITA:Toyotaro Suzumura:Michiaki Tatsubori	XML (Extensible Markup Language) processing can incur significant runtime overhead in XML-based infrastructural middleware such as Web service application servers. This paper proposes a novel mechanism for efficiently processing similar XML documents. Given a new XML document as a byte sequence, the XML parser proposed in this paper normally avoids syntactic analysis but simply matches the document with previously processed ones, reusing those results. Our parser is adaptive since it partially parses and then remembers XML document fragments that it has not met before. Moreover, it processes safely since its partial parsing correctly checks the well-formedness of documents. Our implementation of the proposed parser complies with the JSR 63 standard of the Java API for XML Processing (JAXP) 1.1 specification. We evaluated Deltarser performance with messages using Google Web services. Comparing to Piccolo (and Apache Xerces), it effectively parses 35% (106%) faster in a server-side use-case scenario, and 73% (126%) faster in a client-side use-case scenario.	An adaptive, fast, and safe XML parser based on byte sequences memorization	NA:NA:NA:NA	2018
François Bry:Frank-André Ries:Stephanie Spranger	Data referring to cultural calendars such as the widespread Gregorian dates but also dates after the Chinese, Hebrew, or Islamic calendars as well as data referring to professional calendars like fiscal years or teaching terms are omnipresent on the Web. Formalisms such as XML Schema have acknowledged this by offering a rather extensive set of Gregorian dates and times as basic data types. This article introduces into CaTTS, the <u>C</u>alendar <u>a</u>nd <u>T</u>ime <u>T</u>ype <u>S</u>ystem. CaTTS goes far beyond predefined date and time types after the Gregorian calendar as supported by XML Schema. CaTTS first gives rise to  declaratively specify more or less complex cultural or professional calendars including specificities such as leap seconds, leap years, and time zones. CaTTS further offers a tool for the static type checking (of data typed after calendar(s) defined in CaTTS). CaTTS finally offers a language for declaratively expressing and a solver for efficiently solving temporal constraints (referring to calendar(s) expressed in CaTTS). CaTTS complements data modeling and reasoning methods designed for generic Semantic Web applications such as RDF or OWL with methods specific to the particular application domain of calendars and time.	CaTTS: calendar types and constraints for Web applications	NA:NA:NA	2018
Geert Jan Bex:Wim Martens:Frank Neven:Thomas Schwentick	On an abstract level, XML Schema increases the limited expressive power of Document Type Definitions (DTDs) by extending them with a recursive typing mechanism. However, an investigation of the XML Schema Definitions (XSDs) occurring in practice reveals that the vast majority of them are structurally equivalent to DTDs. This might be due to the complexity of the XML Schema specification and the difficulty to understand the effect of constraints on typing and validation of schemas. To shed some light on the actual expressive power of XSDs this paper studies the impact of the Element Declarations Consistent (EDC) and the Unique Particle Attribution (UPA) rule. An equivalent formalism based on contextual patterns rather than on recursive types is proposed which might serve as a light-weight front end for XML Schema. Finally, the effect of EDC and UPA on the way XML documents can be typed is discussed. It is argued that a cleaner, more robust, stronger but equally efficient class is obtained by replacing EDC and UPA with the notion of 1-pass preorder typing: schemas that allow to determine the type of an element of a streaming document when its opening tag is met. This notion can be defined in terms of restrained competition regular expressions and there is again an equivalent syntactical formalism based on contextual patterns.	Expressiveness of XSDs: from practice to theory, there and back again	NA:NA:NA:NA	2018
Gerald Reif:Harald Gall:Mehdi Jazayeri	The success of the Semantic Web crucially depends on the existence of Web pages that provide machine-understandable meta-data. This meta-data is typically added in the semantic annotation process which is currently not part of the Web engineering process. Web engineering, however, proposes methodologies to design, implement and maintain Web applications but lack the generation of meta-data. In this paper we introduce a technique to extend existing Web engineering methodologies to develop semantically annotated Web pages. The novelty of this approach is the definition of a mapping from XML Schema to ontologies, called WEESA, that can be used to automatically generate RDF meta-data from XML content documents. We further show how we integrated the WEESA mapping into an Apache Cocoon transformer to easily extend XML based Web applications to semantically annotated Web application.	WEESA: Web engineering for semantic Web applications	NA:NA:NA	2018
Gyu Sang Choi:Jin-Ha Kim:Deniz Ersoz:Chita R. Das	Design of high performance Web servers has become a recent research thrust to meet the increasing demand of network-based services. In this paper, we propose a new Web server architecture, called multi-threaded PIPELINED Web server, suitable for Symmetric Multi-Processor (SMP) or System-on-Chip (SoC) architectures. The proposed PIPELINED model consists of multiple thread pools, where each thread pool consists of five basic threads and two helper threads. The main advantages of the proposed model are global information sharing by the threads, minimal synchronization overhead due to less number of threads, and non-blocking I/O operations, possible with the helper threads.We have conducted an in-depth performance analysis of the proposed server model along with four prior Web server models (Multi-Process (MP), Multi-Thread (MT), Single-Process Event-Driven (SPED) and Asynchronous Multi-Process Event-Driven (AMPED)) via simulation using six Web server workloads. The experiments are conducted to investigate the impact of various factors such as the memory size, disk speed and numbers of clients. The simulation results indicate that the proposed PIPELINED Web server architecture shows the best performance across all system and workload parameters compared to the MP, MT, SPED and AMPED models. Although the MT and AMPED models show competitive performance with less number of processors, the advantage of the PIPELINED model becomes obvious as the number of processors or clients in an SMP/SoC machine increases. The MP model shows the worst performance in most of the cases. The results indicate that the proposed server architecture can be used in future large-scale SMP/SoC machines to boost system performance.	A multi-threaded PIPELINED Web server architecture for SMP/SoC machines	NA:NA:NA:NA	2018
Bhuvan Urgaonkar:Prashant Shenoy	In this paper we present the Cataclysm server platform for handling extreme overloads in hosted Internet applications. The primary contribution of our work is to develop a low overhead, highly scalable admission control technique for Internet applications. Cataclysm provides several desirable features, such as guarantees on response time by conducting accurate size-based admission control, revenue maximization at multiple time-scales via preferential admission of important requests and dynamic capacity provisioning, and the ability to be operational even under extreme overloads. Cataclysm can transparently trade-off the accuracy of its decision making with the intensity of the workload allowing it to handle incoming rates of several tens of thousands of requests/second. We implement a prototype Cataclysm hosting platform on a Linux cluster and demonstrate the benefits of our integrated approach using a variety of workloads.	Cataclysm: policing extreme overloads in internet applications	NA:NA	2018
David Brown	From his twin perspectives as a career-long telecommunications engineer and Chairman of one of the UK's largest electronics companies, Sir David Brown will reflect on whether and when the New Economy, seemingly so long coming, will finally arrive. He will begin by exploring how the prospect of everything being digital; everyone having broadband; and intelligence being everywhere is changing our understanding of mobility. Then he will comment on the economic effects of that changed understanding under three headings - the macroeconomy, microeconomy and socioeconomy - before suggesting the criteria we might use to decide when the New Economy has arrived.	The new economy: an engineer's perspective	NA	2018
Paolo Bouquet	NA	Session details: Ontologies	NA	2018
Peter F. Patel-Schneider:Ian Horrocks	Classical logics and Datalog-related logics have both been proposed as underlying formalisms for the Semantic Web. Although these two different formalism groups have some commonalities, and look similar in the context of expressively-impoverished languages like RDF, their differences become apparent at more expressive language levels. After considering some of these differences, we argue that, although some of the characteristics of Datalog have their utility, the open environment of the Semantic Web is better served by standard logics.	Position paper: a comparison of two modelling paradigms in the Semantic Web	NA:NA	2018
Julian Seidenberg:Alan Rector	Ontologies are at the heart of the semantic web. They define the concepts and relationships that make global interoperability possible. However, as these ontologies grow in size they become more and more difficult to create, use, understand, maintain, transform and classify. We present and evaluate several algorithms for extracting relevant segments out of large description logic ontologies for the purposes of increasing tractability for both humans and computers. The segments are not mere fragments, but stand alone as ontologies in their own right. This technique takes advantage of the detailed semantics captured within an OWL ontology to produce highly relevant segments. The research was evaluated using the GALEN ontology of medical terms and procedures.	Web ontology segmentation: analysis, classification and use	NA:NA	2018
Yuzhong Qu:Wei Hu:Gong Cheng	On the investigation of linguistic techniques used in ontology matching, we propose a new idea of virtual documents to pursue a cost-effective approach to linguistic matching in this paper. Basically, as a collection of weighted words, the virtual document of a URIref declared in an ontology contains not only the local descriptions but also the neighboring information to reflect the intended meaning of the URIref. Document similarity can be computed by traditional vector space techniques, and then be used in the similarity-based approaches to ontology matching. In particular, the RDF graph structure is exploited to define the description formulations and the neighboring operations. Experimental results show that linguistic matching based on the virtual documents is dominant in average F-Measure as compared to other three approaches. It is also demonstrated by our experiments that the virtual documents approach is cost-effective as compared to other linguistic matching approaches.	Constructing virtual documents for ontology matching	NA:NA:NA	2018
Eric Brewer	NA	Session details: Adapitivity & mobility	NA	2018
Shumeet Baluja	Fitting enough information from webpages to make browsing on small screens compelling is a challenging task. One approach is to present the user with a thumbnail image of the full web page and allow the user to simply press a single key to zoom into a region (which may then be transcoded into wml/xhtml, summarized, etc). However, if regions for zooming are presented naively, this yields a frustrating experience because of the number of coherent regions, sentences, images, and words that may be inadvertently separated. Here, we cast the web page segmentation problem into a machine learning framework, where we re-examine this task through the lens of entropy reduction and decision tree learning. This yields an efficient and effective page segmentation algorithm. We demonstrate how simple techniques from computer vision can be used to fine-tune the results. The resulting segmentation keeps coherent regions together when tested on a broad set of complex webpages.	Browsing on small screens: recasting web-page segmentation into an efficient machine learning framework	NA	2018
Takuya Maekawa:Takahiro Hara:Shojiro Nishio	It is difficult for users of mobile devices such as cellular phones equipped with a small screen and a poor input interface to browse Web pages designed for desktop PCs with large displays. Many studies and commercial products have tried to solve this problem. Web pages include images that have various roles such as site menus, line headers for itemization, and page titles. However, most studies of mobile Web browsing haven't paid much attention to the roles of Web images. In this paper, we define eleven Web image categories according to their roles and use these categories for proper Web image handling. We manually categorized 3,901 Web images collected from forty Web sites and extracted image features of each category according to the classification. By making use of the extracted features, we devised an automatic Web image classification method. Furthermore, we evaluated the automatic classification of real Web pages and achieved up to 83.1% classification accuracy. We also implemented an automatic Web page scrolling system as an application of our automatic image classification method.	Image classification for mobile web browsing	NA:NA:NA	2018
Cristina Hava Muntean:Jennifer McManis	New communication technologies can enable Web users to access personalised information "anytime, anywhereö. However, the network environments allowing this "anytime, anywhereö access may have widely varying performance characteristics such as bandwidth, level of congestion, mobility support, and cost of transmission. It is unrealistic to expect that the quality of delivery of the same content can be maintained in this variable environment, but rather an effort must be made to fit the content served to the current delivery conditions, thus ensuring high Quality of Experience (QoE) to the users. This paper introduces an end-user QoE-aware adaptive hypermedia framework that extends the adaptation functionality of adaptive hypermedia systems with a fine-grained content-based adaptation mechanism. The proposed mechanism attempts to take into account multiple factors affecting QoE in relation to the delivery of Web content. Various simulation tests investigate the performance improvements provided by this mechanism, in a home-like, low bit rate operational environment, in terms of access time per page, aggregate access time per browsing session and quantity of transmitted information.	Fine grained content-based adaptation mechanism for providing high end-user quality of experience with adaptive hypermedia systems	NA:NA	2018
Ziv Bar-Yossef	NA	Session details: Fighting search spam	NA	2018
Baoning Wu:Vinay Goel:Brian D. Davison	Web spam is behavior that attempts to deceive search engine ranking algorithms. TrustRank is a recent algorithm that can combat web spam. However, TrustRank is vulnerable in the sense that the seed set used by TrustRank may not be sufficiently representative to cover well the different topics on the Web. Also, for a given seed set, TrustRank has a bias towards larger communities. We propose the use of topical information to partition the seed set and calculate trust scores for each topic separately to address the above issues. A combination of these trust scores for a page is used to determine its ranking. Experimental results on two large datasets show that our Topical TrustRank has a better performance than TrustRank in demoting spam sites or pages. Compared to TrustRank, our best technique can decrease spam from the top ranked sites by as much as 43.1%.	Topical TrustRank: using topicality to combat web spam	NA:NA:NA	2018
André Luiz da Costa Carvalho:Paul - Alexandru Chirita:Edleno Silva de Moura:Pável Calado:Wolfgang Nejdl	The currently booming search engine industry has determined many online organizations to attempt to artificially increase their ranking in order to attract more visitors to their web sites. At the same time, the growth of the web has also inherently generated several navigational hyperlink structures that have a negative impact on the importance measures employed by current search engines. In this paper we propose and evaluate algorithms for identifying all these noisy links on the web graph, may them be spam or simple relationships between real world entities represented by sites, replication of content, etc. Unlike prior work, we target a different type of noisy link structures, residing at the site level, instead of the page level. We thus investigate and annihilate site level mutual reinforcement relationships, abnormal support coming from one site towards another, as well as complex link alliances between web sites. Our experiments with the link database of the TodoBR search engine show a very strong increase in the quality of the output rankings after having applied our techniques.	Site level noise removal for search engines	NA:NA:NA:NA:NA	2018
Alexandros Ntoulas:Marc Najork:Mark Manasse:Dennis Fetterly	In this paper, we continue our investigations of "web spam": the injection of artificially-created pages into the web in order to influence the results from search engines, to drive traffic to certain pages for fun or profit. This paper considers some previously-undescribed techniques for automatically detecting spam pages, examines the effectiveness of these techniques in isolation and when aggregated using classification algorithms. When combined, our heuristics correctly identify 2,037 (86.2%) of the 2,364 spam pages (13.8%) in our judged collection of 17,168 pages, while misidentifying 526 spam and non-spam pages (3.1%).	Detecting spam web pages through content analysis	NA:NA:NA:NA	2018
Monika Solanki	NA	Session details: XML	NA	2018
Margaret G. Kostoulas:Morris Matsa:Noah Mendelsohn:Eric Perkins:Abraham Heifets:Martha Mercaldi	This paper describes an experimental system in which customized high performance XML parsers are prepared using parser generation and compilation techniques. Parsing is integrated with Schema-based validation and deserialization, and the resulting validating processors are shown to be as fast as or in many cases significantly faster than traditional nonvalidating parsers. High performance is achieved by integration across layers of software that are traditionally separate, by avoiding unnecessary data copying and transformation, and by careful attention to detail in the generated code. The effect of API design on XML performance is also briefly discussed..	XML screamer: an integrated approach to high performance XML parsing, validation and deserialization	NA:NA:NA:NA:NA:NA	2018
Shuohao Zhang:Curtis Dyreson	Path expressions are the principal means of locating data in a hierarchical model. But path expressions are brittle because they often depend on the structure of data and break if the data is structured differently. The structure of data could be unfamiliar to a user, may differ within a data collection, or may change over time as the schema evolves. This paper proposes a novel construct that locates related nodes in an instance of an XML data model, independent of a specific structure. It can augment many XPath expressions and can be seamlessly incorporated in XQuery or XSLT.	Symmetrically exploiting XML	NA:NA	2018
Kentaro Toyama	NA	Session details: Developing regions & peer-to-peer	NA	2018
Seung Jun:Mustaque Ahamad	As most blogs and traditional media support RSS or Atom feeds, the news feed technology becomes increasingly prevalent. Taking advantage of ubiquitous news feeds, we design FeedEx, a news feed exchange system. Forming a distribution overlay network, nodes in FeedEx not only fetch feed documents from the servers but also exchange them with neighbors. Among many benefits of collaborative feed exchange, we focus on the low-overhead, scalable delivery mechanism that increases the availability of news feeds. Our design of FeedEx is incentive-compatible so that nodes are encouraged into cooperating rather than free riding. In addition, for a better design of FeedEx, we analyze the data collected from 245 feeds for 10 days and present relevant statistics about news feed publishing, including the distributions of feed size, entry lifetime, and publishing rate.Our experimental evaluation using 189 PlanetLab machines, which fetch from real-world feed servers, shows that FeedEx is an efficient system in many respects. Even when a node fetches feed documents as infrequently as every 16 hours, it captures more than 90% of the total entries published, and those captured entries are available within 22 minutes on average after published at the servers. By contrast, stand-alone applications in the same condition show 36% of entry coverage and 5.7 hours of time lag. The efficient delivery of FeedEx is achieved with low communication overhead as each node receives only 0.9 document exchange calls and 6.3 document checking calls per minute on average.	FeedEx: collaborative exchange of news feeds	NA:NA	2018
Krishna Bharat	NA	Session details: Browsers	NA	2018
Kirstie Hawkey:Kori M. Inkpen	This research examines the privacy comfort levels of participants if others can view traces of their web browsing activity. During a week-long field study, participants used an electronic diary daily to annotate each web page visited with a privacy level. Content categories were used by participants to theoretically specify their privacy comfort for each category and by researchers to partition participants' actual browsing. The content categories were clustered into groups based on the dominant privacy levels applied to the pages. Inconsistencies between participants in their privacy ratings of categories suggest that a general privacy management scheme is inappropriate. Participants' consistency within categories suggests that a personalized scheme may be feasible; however a more fine-grained approach to classification is required to improve results for sites that tend to be general, of multiple task purposes, or dynamic in content.	Examining the content and privacy of web browsing incidental information	NA:NA	2018
Harald Weinreich:Hartmut Obendorf:Eelco Herder:Matthias Mayer	This paper presents results of a long-term client-side Web usage study, updating previous studies that range in age from five to ten years. We focus on three aspects of Web navigation: changes in the distribution of navigation actions, speed of navigation and within-page navigation."Navigation actionsö corresponding to users' individual page requests are discussed by type. We reconfirm links to be the most important navigation element, while backtracking has lost more than half of its previously reported share and form submission has become far more common. Changes of the Web and the browser interfaces are candidates for causing these changes.Analyzing the time users stayed on pages, we confirm Web navigation to be a rapidly interactive activity. A breakdown of page characteristics shows that users often do not take the time to read the available text or consider all links. The performance of the Web is analyzed and reassessed against the resulting requirements.Finally, habits of within-page navigation are presented. Although most selected hyperlinks are located in the top left corner of the = screen, in nearly a quarter of all cases people choose links that require scrolling. We analyzed the available browser real estate to gain insights for the design of non-scrolling Web pages.	Off the beaten tracks: exploring three aspects of web navigation	NA:NA:NA:NA	2018
Joeng Kim:Ricardo A. Baratto:Jason Nieh	Although web applications are gaining popularity on mobile wireless PDAs, web browsers on these systems can be quite slow and often lack adequate functionality to access many web sites. We have developed pTHINC, a PDA thin-client solution that leverages more powerful servers to run full-function web browsers and other application logic, then sends simple screen updates to the PDA for display. pTHINC uses server-side screen scaling to provide high-fidelity display and seamless mobility across a broad range of different clients and screen sizes, including both portrait and landscape viewing modes. pTHINC also leverages existing PDA control buttons to improve system usability and maximize available screen resolution for application display. We have implemented pTHINC on Windows Mobile and evaluated its performance on mobile wireless devices. Our results compared to local PDA web browsers and other thin-client approaches demonstrate that pTHINC provides superior web browsing performance and is the only PDA thin client that effectively supports crucial browser helper applications such as video playback.	pTHINC: a thin-client architecture for mobile wireless web	NA:NA:NA	2018
Bertram Ludaescher	NA	Session details: E-communities	NA	2018
K. Faith Lawrence:m. c. schraefel	In this paper we consider the types of community networks that are most often codified within the Semantic Web. We propose the recognition of a new structure which fulfils the definition of community used outside the Semantic Web. We argue that the properties inherent in a community allow additional processing to be done with the described relationships existing between entities within the community network. Taking an existing online community as a case study we describe the ontologies and applications that we developed to support this community in the Semantic Web environment and discuss what lessons can be learnt from this exercise and applied in more general settings.	Bringing communities to the semantic web and the semantic web to communities	NA:NA	2018
Vladimir Soroka:Sheizaf Rafaeli	The asymmetry of activity in virtual communities is of great interest. While participation in the activities of virtual communities is crucial for a community's survival and development, many people prefer lurking, that is passive attention over active participation. Lurking can be measured and perhaps affected by both dispositional and situational variables. This work investigates the concept of cultural capital as situational antecedent of lurking and de-lurking (the decision to start posting after a certain amount of lurking time). Cultural capital is defined as the knowledge that enables an individual to interpret various cultural codes. The main hypothesis states that a user's cultural capital affects her level of activity in a community and her decision to de-lurk and cease to exist in very active communities because of information overload. This hypothesis is analyzed by mathematically defining a social communication network (SCN) of activities in authenticated discussion forums. We validate this model by examining the SCN using data collected in a sample of 636 online forums in Open University in Israel and 2 work based communities from IBM. The hypotheses verified here make it clear that fostering receptive participation may be as important and constructive as encouraging active contributions in online communities.	Invisible participants: how cultural capital relates to lurking behavior	NA:NA	2018
Ding Zhou:Eren Manavoglu:Jia Li:C. Lee Giles:Hongyuan Zha	The increasing amount of communication between individuals in e-formats (e.g. email, Instant messaging and the Web) has motivated computational research in social network analysis (SNA). Previous work in SNA has emphasized the social network (SN) topology measured by communication frequencies while ignoring the semantic information in SNs. In this paper, we propose two generative Bayesian models for semantic community discovery in SNs, combining probabilistic modeling with community detection in SNs. To simulate the generative models, an EnF-Gibbs sampling algorithm is proposed to address the efficiency and performance problems of traditional methods. Experimental studies on Enron email corpus show that our approach successfully detects the communities of individuals and in addition provides semantic topic descriptions of these communities.	Probabilistic models for discovering e-communities	NA:NA:NA:NA:NA	2018
Yoelle Maarek	NA	Session details: User interfaces: semantic tagging	NA	2018
Daniel Gruhl:Daniel N. Meredith:Jan H. Pieper:Alex Cozzi:Stephen Dill	Popularity based search engines have served to stagnate information retrieval from the web. Developed to deal with the very real problem of degrading quality within keyword based search they have had the unintended side effect of creating "icebergs" around topics, where only a small minority of the information is above the popularity water-line. This problem is especially pronounced with emerging information--new sites are often hidden until they become popular enough to be considered above the water-line. In domains new to a user this is often helpful--they can focus on popular sites first. Unfortunately it is not the best tool for a professional seeking to keep up-to-date with a topic as it emerges and evolves.We present a tool focused on this audience--a system that addresses the very large scale information gathering, filtering and routing, and presentation problems associated with creating a useful incremental stream of information from the web as a whole. Utilizing the WebFountain platform as the primary data engine and Really Simple Syndication (RSS) as the delivery mechanism, our "Daily Deltas" (Delta) application is able to provide an informative feed of relevant content directly to a user. Individuals receive a personalized, incremental feed of pages related to their topic allowing them to track their interests independent of the overall popularity of the topic.	The web beyond popularity: a really simple system for web scale RSS	NA:NA:NA:NA:NA	2018
Micah Dubinko:Ravi Kumar:Joseph Magnani:Jasmine Novak:Prabhakar Raghavan:Andrew Tomkins	We consider the problem of visualizing the evolution of tags within the Flickr (flickr.com) online image sharing community. Any user of the Flickr service may append a tag to any photo in the system. Over the past year, users have on average added over a million tags each week. Understanding the evolution of these tags over time is therefore a challenging task. We present a new approach based on a characterization of the most interesting tags associated with a sliding interval of time. An animation provided via Flash in a web browser allows the user to observe and interact with the interesting tags as they evolve over time.New algorithms and data structures are required to support the efficient generation of this visualization. We combine a novel solution to an interval covering problem with extensions to previous work on score aggregation in order to create an efficient backend system capable of producing visualizations at arbitrary scales on this large dataset in real time.	Visualizing tags over time	NA:NA:NA:NA:NA:NA	2018
Richard Atterer:Monika Wnuk:Albrecht Schmidt	In this paper, we investigate how detailed tracking of user interaction can be monitored using standard web technologies. Our motivation is to enable implicit interaction and to ease usability evaluation of web applications outside the lab. To obtain meaningful statements on how users interact with a web application, the collected information needs to be more detailed and fine-grained than that provided by classical log files. We focus on tasks such as classifying the user with regard to computer usage proficiency or making a detailed assessment of how long it took users to fill in fields of a form. Additionally, it is important in the context of our work that usage tracking should not alter the user's experience and that it should work with existing server and browser setups. We present an implementation for detailed tracking of user actions on web pages. An HTTP proxy modifies HTML pages by adding JavaScript code before delivering them to the client. This JavaScript tracking code collects data about mouse movements, keyboard input and more. We demonstrate the usefulness of our approach in a case study.	Knowing the user's every move: user activity tracking for website usability evaluation and implicit interaction	NA:NA:NA	2018
Dmitri Pavlov	NA	Session details: Mining the web	NA	2018
Wen-tau Yih:Joshua Goodman:Vitor R. Carvalho	A large and growing number of web pages display contextual advertising based on keywords automatically extracted from the text of the page, and this is a substantial source of revenue supporting the web today. Despite the importance of this area, little formal, published research exists. We describe a system that learns how to extract keywords from web pages for advertisement targeting. The system uses a number of features, such as term frequency of each potential keyword, inverse document frequency, presence in meta-data, and how often the term occurs in search query logs. The system is trained with a set of example pages that have been hand-labeled with "relevant" keywords. Based on this training, it can then extract new keywords from previously unseen pages. Accuracy is substantially better than several baseline systems.	Finding advertising keywords on web pages	NA:NA:NA	2018
Reid Andersen:Kevin J. Lang	Expanding a seed set into a larger community is a common procedure in link-based analysis. We show how to adapt recent results from theoretical computer science to expand a seed set into a community with small conductance and a strong relationship to the seed, while examining only a small neighborhood of the entire graph. We extend existing results to give theoretical guarantees that apply to a variety of seed sets from specified communities. We also describe simple and flexible heuristics for applying these methods in practice, and present early experiments showing that these methods compare favorably with existing approaches.	Communities from seed sets	NA:NA	2018
Masashi Toyoda:Masaru Kitsuregawa	Identifying and tracking new information on the Web is important in sociology, marketing, and survey research, since new trends might be apparent in the new information. Such changes can be observed by crawling the Web periodically. In practice, however, it is impossible to crawl the entire expanding Web repeatedly. This means that the novelty of a page remains unknown, even if that page did not exist in previous snapshots. In this paper, we propose a novelty measure for estimating the certainty that a newly crawled page appeared between the previous and current crawls. Using this novelty measure, new pages can be extracted from a series of unstable snapshots for further analysis and mining to identify new trends on the Web. We evaluated the precision, recall, and miss rate of the novelty measure using our Japanese web archive, and applied it to a Web archive search engine.	What's really new on the web?: identifying new pages from a series of unstable web snapshots	NA:NA	2018
Mary Ann Davidson	Information technology has become "infrastructure technology," as most sectors of critical infrastructure rest on an IT backbone. Yet IT systems are not yet designed to be as safe, secure and reliable as physical infrastructure. Improving the security worthiness of commercial software requires a significant change in the development and product delivery process across the board. The security worthiness of all commercial software - from all vendors - demands that assurance became a critical focus for both providers and customers of IT. During Oracle's long history of building and delivering secure software, we continue to invest heavily in building security into each component of the product lifecycle. This is also an "organic" process which is regularly being enhanced to improve overall security practices. Our efforts have evolved from a formal development process to now additionally include secure coding standards, intensive developer training, innovative "bug finding" tools and working with leading vendors to "raise the bar" for all of industry as it pertains to security.	A case for software assurance	NA	2018
Tony Hey	The Internet was the inspiration of J.C.R.Licklider when he was at the Advanced Research Projects Agency in the 1960's. In those pre-Moore's Law days, Licklider imagined a future in which researchers could access and use computers and data from anywhere in the world. Today, as everyone knows, the killer applications for the Internet were email in the 1970's and the World Wide Web in the 1990's which was developed initially as a collaboration tool for the particle physics academic community. In the future, frontier research in many fields will increasingly require the collaboration of globally distributed groups of researchers needing access to distributed computing, data resources and support for remote access to expensive, multi-national specialized facilities such as telescopes and accelerators or specialist data archives. In the context of science and engineering, this is the 'e-Science' agenda. Robust middleware services deployed on top of research networks will constitute a powerful 'Cyberinfrastructure' for collaborative science and engineering.This talk will review the elements of this vision and describe the present status of efforts to build such an internet-scale distributed infrastructure based on Web Services. The goal is to provide robust middleware components that will allow scientists and engineers to routinely construct the inter-organizational 'Virtual Organizations'. Given the present state of Web Services, we argue for the need to define such Virtual Organization 'Grid' services on well-established Web Service specifications that are widely supported by the IT industry. Only industry can provide the necessary tooling and development environments to enable widespread adoption of such Grid services. Extensions to these basic Grid services can be added as more Web Services mature and the research community has had the opportunity to experiment with new services providing potentially useful new functionalities. The new Cyberinfrastructure will be of relevance to more than just the research community: it will impact both the e-learning and digital library communities allow the creation of scientific 'mash-ups' of services giving significant added value.	'e-science and cyberinfrastructure: a middleware perspective'	NA	2018
Harry Halpin	NA	Session details: Correctness & security	NA	2018
Stefan Kals:Engin Kirda:Christopher Kruegel:Nenad Jovanovic	As the popularity of the web increases and web applications become tools of everyday use, the role of web security has been gaining importance as well. The last years have shown a significant increase in the number of web-based attacks. For example, there has been extensive press coverage of recent security incidences involving the loss of sensitive credit card information belonging to millions of customers.Many web application security vulnerabilities result from generic input validation problems. Examples of such vulnerabilities are SQL injection and Cross-Site Scripting (XSS). Although the majority of web vulnerabilities are easy to understand and to avoid, many web developers are, unfortunately, not security-aware. As a result, there exist many web sites on the Internet that are vulnerable.This paper demonstrates how easy it is for attackers to automatically discover and exploit application-level vulnerabilities in a large number of web applications. To this end, we developed SecuBat, a generic and modular web vulnerability scanner that, similar to a port scanner, automatically analyzes web sites with the aim of finding exploitable SQL injection and XSS vulnerabilities. Using SecuBat, we were able to find many potentially vulnerable web sites. To verify the accuracy of SecuBat, we picked one hundred interesting web sites from the potential victim list for further analysis and confirmed exploitable flaws in the identified web pages. Among our victims were well-known global companies and a finance ministry. Of course, we notified the administrators of vulnerable sites about potential security problems. More than fifty responded to request additional information or to report that the security hole was closed.	SecuBat: a web vulnerability scanner	NA:NA:NA:NA	2018
Massimo Mecella:Mourad Ouzzani:Federica Paci:Elisa Bertino	Service Oriented Computing is emerging as the main approach to build distributed enterprise applications on the Web. The widespread use of Web services is hindered by the lack of adequate security and privacy support. In this paper, we present a novel framework for enforcing access control in conversation-based Web services. Our approach takes into account the conversational nature of Web services. This is in contrast with existing approaches to access control enforcement that assume a Web service as a set of independent operations. Furthermore, our approach achieves a tradeoff between the need to protect Web service's access control policies and the need to disclose to clients the portion of access control policies related to the conversations they are interested in. This is important to avoid situations where the client cannot progress in the conversation due to the lack of required security requirements. We introduce the concept of k-trustworthiness that defines the conversations for which a client can provide credentials maximizing the likelihood that it will eventually hit a final state.	Access control enforcement for conversation-based web services	NA:NA:NA:NA	2018
Raman Kazhamiakin:Marco Pistore:Luca Santuari	In this paper we describe an approach for the verification of Web service compositions defined by sets of BPEL processes. The key aspect of such a verification is the model adopted for representing the communications among the services participating in the composition. Indeed, these communications are asynchronous and buffered in the existing execution frameworks, while most verification approaches assume a synchronous communication model for efficiency reasons. In our approach, we develop a parametric model for describing Web service compositions, which allows us to capture a hierarchy of communication models, ranging from synchronous communications to asynchronous communications with complex buffer structures. Moreover, we develop a technique to associate with a Web service composition the most adequate communication model, i.e., the simplest model that is sufficient to capture all the behaviors of the composition. This way, we can provide an accurate model of a wider class of service composition scenarios, while preserving as much as possible an efficient performance in verification.	Analysis of communication models in web service compositions	NA:NA:NA	2018
David Lowe	NA	Session details: Search engine engineering	NA	2018
Taro Tezuka:Takeshi Kurashima:Katsumi Tanaka	Integration of Web search with geographic information has recently attracted much attention. There are a number of local Web search systems enabling users to find location-specific Web content. In this paper, however, we point out that this integration is still at a superficial level. Most local Web search systems today only link local Web content to a map interface. They are extensions of a conventional stand-alone geographic information system (GIS), applied to a Web-based client-server architecture. In this paper, we discuss the directions available for tighter integration of Web search with a GIS, in terms of extraction, knowledge discovery, and presentation. We also describe implementations to support our argument that the integration must go beyond the simple map-and hyperlink architecture.	Toward tighter integration of web search with a geographic information system	NA:NA:NA	2018
Weizheng Gao:Hyun Chul Lee:Yingbo Miao	A collaborative crawler is a group of crawling nodes, in which each crawling node is responsible for a specific portion of the web. We study the problem of collecting geographi-cally-aware pages using collaborative crawling strategies. We first propose several collaborative crawling strategies for the geographically focused crawling, whose goal is to collect web pages about specified geographic locations, by considering features like URL address of page, content of page, extended anchor text of link, and others. Later, we propose various evaluation criteria to qualify the performance of such crawling strategies. Finally, we experimentally study our crawling strategies by crawling the real web data showing that some of our crawling strategies greatly outperform the simple URL-hash based partition collaborative crawling, in which the crawling assignments are determined according to the hash-value computation over URLs. More precisely, features like URL address of page and extended anchor text of link are shown to yield the best overall performance for the geographically focused crawling.	Geographically focused collaborative crawling	NA:NA:NA	2018
Tamás Sarlós:Adrás A. Benczúr:Károly Csalogány:Dániel Fogaras:Balázs Rácz	Personalized PageRank expresses link-based page quality around user selected pages. The only previous personalized PageRank algorithm that can serve on-line queries for an unrestricted choice of pages on large graphs is our Monte Carlo algorithm [WAW 2004]. In this paper we achieve unrestricted personalization by combining rounding and randomized sketching techniques in the dynamic programming algorithm of Jeh and Widom [WWW 2003]. We evaluate the precision of approximation experimentally on large scale real-world data and find significant improvement over previous results. As a key theoretical contribution we show that our algorithms use an optimal amount of space by also improving earlier asymptotic worst-case lower bounds. Our lower bounds and algorithms apply to the SimRank as well; of independent interest is the reduction of the SimRank computation to personalized PageRank.	To randomize or not to randomize: space optimal summaries for hyperlink analysis	NA:NA:NA:NA:NA	2018
Wolfgana Nejdl	NA	Session details: E-learning & scientific applications	NA	2018
Mingyu Feng:Neil T. Heffernan:Kenneth R. Koedinger	Secondary teachers across the country are being asked to use formative assessment data to inform their classroom instruction. At the same time, critics of No Child Left Behind are calling the bill "No Child Left Untestedö emphasizing the negative side of assessment, in that every hour spent assessing students is an hour lost from instruction. Or does it have to be? What if we better integrated assessment into the classroom, and we allowed students to learn during the test? Maybe we could even provide tutoring on the steps of solving problems. Our hypothesis is that we can achieve more accurate assessment by not only using data on whether students get test items right or wrong, but by also using data on the effort required for students to learn how to solve a test item. We provide evidence for this hypothesis using data collected with our E-ASSISTment system by more than 600 students over the course of the 2004-2005 school year. We also show that we can track student knowledge over time using modern longitudinal data analysis techniques. In a separate paper [9], we report on the ASSISTment system's architecture and scalability, while this paper is focused on how we can reliably assess student learning.	Addressing the testing challenge with a web-based e-assessment system that tutors as it assesses	NA:NA:NA	2018
Satya S. Sahoo:Christopher Thomas:Amit Sheth:William S. York:Samir Tartir	High throughput glycoproteomics, similar to genomics and proteomics, involves extremely large volumes of distributed, heterogeneous data as a basis for identification and quantification of a structurally diverse collection of biomolecules. The ability to share, compare, query for and most critically correlate datasets using the native biological relationships are some of the challenges being faced by glycobiology researchers. As a solution for these challenges, we are building a semantic structure, using a suite of ontologies, which supports management of data and information at each step of the experimental lifecycle. This framework will enable researchers to leverage the large scale of glycoproteomics data to their benefit.In this paper, we focus on the design of these biological ontology schemas with an emphasis on relationships between biological concepts, on the use of novel approaches to populate these complex ontologies including integrating extremely large datasets ( 500MB) as part of the instance base and on the evaluation of ontologies using OntoQA [38] metrics. The application of these ontologies in providing informatics solutions, for high throughput glycoproteomics experimental domain, is also discussed. We present our experience as a use case of developing two ontologies in one domain, to be part of a set of use cases, which are used in the development of an emergent framework for building and deploying biological ontologies.	Knowledge modeling and its application in life sciences: a tale of two ontologies	NA:NA:NA:NA:NA	2018
Elizabeth Brown:Tim Brailsford:Tony Fisher:Adam Moore:Helen Ashman	The mechanisms for personalisation used in web applications are currently the subject of much debate amongst researchers from many diverse subject areas. One of the most contemporary ideas for user modelling in web applications is that of cognitive styles, where a user's psychological preferences are assessed stored in a database and then used to provide personalised content and/or links. We describe user trials of a case study that utilises visual-verbal preferences in an adaptive web-based educational system (AWBES). Students in this trial were assessed by the Felder-Solomon Inventory of Learning Styles (ILS) instrument, and their preferences were used as a means of content personalisation.Contrary to previous findings by other researchers, we found no significant differences in performance between matched and mismatched students. Conclusions are drawn about the value and validity of using cognitive styles as a way of modelling user preferences in educational web applications.	Reappraising cognitive styles in adaptive web applications	NA:NA:NA:NA:NA	2018
Mustaque Ahamad	NA	Session details: High availability & performance	NA	2018
Balachander Krishnamurthy:Craig E. Wills	Web pages include extraneous material that may be viewed as undesirable by a user. Increasingly many Web sites also require users to register to access either all or portions of the site. Such tension between content owners and users has resulted in a "cat and mouse" game between content provided and how users access it.We carried out a measurement-based study to understand the nature of extraneous content and its impact on performance as perceived by users. We characterize how this content is distributed and the effectiveness of blocking mechanisms to stop it as well as countermeasures taken by content owners to negate such mechanisms. We also examine sites that require some form of registration to control access and the attempts made to circumvent it.Results from our study show that extraneous content exists on a majority of popular pages and that a 25-30% reduction in downloaded objects and bytes with corresponding latency reduction can be attained by blocking such content. The top ten advertisement delivering companies delivered 40% of all URLs matched as ads in our study. Both the server name and the remainder of the URL are important in matching a URL as an ad. A majority of popular sites require some form of registration and for such sites users can obtain an account from a shared public database. We discuss future measures and countermeasures on the part of each side.	Cat and mouse: content delivery tradeoffs in web access	NA:NA	2018
Patrick Reynolds:Janet L. Wiener:Jeffrey C. Mogul:Marcos K. Aguilera:Amin Vahdat	Wide-area distributed applications are challenging to debug, optimize, and maintain. We present Wide-Area Project 5 (WAP5), which aims to make these tasks easier by exposing the causal structure of communication within an application and by exposing delays that imply bottlenecks. These bottlenecks might not otherwise be obvious, with or without the application's source code. Previous research projects have presented algorithms to reconstruct application structure and the corresponding timing information from black-box message traces of local-area systems. In this paper we present (1) a new algorithm for reconstructing application structure in both local- and wide-area distributed systems, (2) an infrastructure for gathering application traces in PlanetLab, and (3) our experiences tracing and analyzing three systems: CoDeeN and Coral, two content-distribution networks in PlanetLab; and Slurpee, an enterprise-scale incident-monitoring system.	WAP5: black-box performance debugging for wide-area systems	NA:NA:NA:NA:NA	2018
Jorge Salas:Francisco Perez-Sorrosal:Marta Patiño-Martínez:Ricardo Jiménez-Peris	Due to the rapid acceptance of web services and its fast spreading, a number of mission-critical systems will be deployed as web services in next years. The availability of those systems must be guaranteed in case of failures and network disconnections. An example of web services for which availability will be a crucial issue are those belonging to coordination web service infrastructure, such as web services for transactional coordination (e.g., WS-CAF and WS-Transaction). These services should remain available despite site and connectivity failures to enable business interactions on a 24x7 basis. Some of the common techniques for attaining availability consist in the use of a clustering approach. However, in an Internet setting a domain can get partitioned from the network due to a link overload or some other connectivity problems. The unavailability of a coordination service impacts the availability of all the partners in the business process. That is, coordination services are an example of critical components that need higher provisions for availability. In this paper, we address this problem by providing an infrastructure, WS-Replication, for WAN replication of web services. The infrastructure is based on a group communication web service, WS-Multicast, that respects the web service autonomy. The transport of WS-Multicast is based on SOAP and relies exclusively on web service technology for interaction across organizations. We have replicated WS-CAF using our WS-Replication framework and evaluated its performance.	WS-replication: a framework for highly available web services	NA:NA:NA:NA	2018
Susan Dumais	NA	Session details: Web mining with search engines	NA	2018
Ziv Bar-Yossef:Maxim Gurevich	We revisit a problem introduced by Bharat and Broder almost a decade ago: how to sample random pages from a search engine's index using only the search engine's public interface? Such a primitive is particularly useful in creating objective benchmarks for search engines.The technique of Bharat and Broder suffers from two well recorded biases: it favors long documents and highly ranked documents. In this paper we introduce two novel sampling techniques: a lexicon-based technique and a random walk technique. Our methods produce biased sample documents, but each sample is accompanied by a corresponding "weight", which represents the probability of this document to be selected in the sample. The samples, in conjunction with the weights, are then used to simulate near-uniform samples. To this end, we resort to three well known Monte Carlo simulation methods: rejection sampling, importance sampling and the Metropolis-Hastings algorithm.We analyze our methods rigorously and prove that under plausible assumptions, our techniques are guaranteed to produce near-uniform samples from the search engine's index. Experiments on a corpus of 2.4 million documents substantiate our analytical findings and show that our algorithms do not have significant bias towards long or highly ranked documents. We use our algorithms to collect fresh data about the relative sizes of Google, MSN Search, and Yahoo!.	Random sampling from a search engine's index	NA:NA	2018
Mehran Sahami:Timothy D. Heilman	Determining the similarity of short text snippets, such as search queries, works poorly with traditional document similarity measures (e.g., cosine), since there are often few, if any, terms in common between two short text snippets. We address this problem by introducing a novel method for measuring the similarity between short text snippets (even those without any overlapping terms) by leveraging web search results to provide greater context for the short texts. In this paper, we define such a similarity kernel function, mathematically analyze some of its properties, and provide examples of its efficacy. We also show the use of this kernel function in a large-scale system for suggesting related queries to search engine users.	A web-based kernel function for measuring the similarity of short text snippets	NA:NA	2018
Rosie Jones:Benjamin Rey:Omid Madani:Wiley Greiner	We introduce the notion of query substitution, that is, generating a new query to replace a user's original search query. Our technique uses modifications based on typical substitutions web searchers make to their queries. In this way the new query is strongly related to the original query, containing terms closely related to all of the original terms. This contrasts with query expansion through pseudo-relevance feedback, which is costly and can lead to query drift. This also contrasts with query relaxation through boolean or TFIDF retrieval, which reduces the specificity of the query. We define a scale for evaluating query substitution, and show that our method performs well at generating new queries related to the original queries. We build a model for selecting between candidates, by using a number of features relating the query-candidate pair, and by fitting the model to human judgments of relevance of query suggestions. This further improves the quality of the candidates generated. Experiments show that our techniques significantly increase coverage and effectiveness in the setting of sponsored search.	Generating query substitutions	NA:NA:NA:NA	2018
Carole Goble	NA	Session details: Social networks	NA	2018
Yutaka Matsuo:Junichiro Mori:Masahiro Hamasaki:Keisuke Ishida:Takuichi Nishimura:Hideaki Takeda:Koiti Hasida:Mitsuru Ishizuka	Social networks play important roles in the Semantic Web: knowledge management, information retrieval, ubiquitous computing, and so on. We propose a social network extraction system called POLYPHONET, which employs several advanced techniques to extract relations of persons, detect groups of persons, and obtain keywords for a person. Search engines, especially Google, are used to measure co-occurrence of information and obtain Web documents.Several studies have used search engines to extract social networks from the Web, but our research advances the following points: First, we reduce the related methods into simple pseudocodes using Google so that we can build up integrated systems. Second, we develop several new algorithms for social networking mining such as those to classify relations into categories, to make extraction scalable, and to obtain and utilize person-to-word relations. Third, every module is implemented in POLYPHONET, which has been used at four academic conferences, each with more than 500 participants. We overview that system. Finally, a novel architecture called Super Social Network Mining is proposed; it utilizes simple modules using Google and is characterized by scalability and Relate-Identify processes: Identification of each entity and extraction of relations are repeated to obtain a more precise social network.	POLYPHONET: an advanced social network extraction system from the web	NA:NA:NA:NA:NA:NA:NA:NA	2018
Boanerges Aleman-Meza:Meenakshi Nagarajan:Cartic Ramakrishnan:Li Ding:Pranam Kolari:Amit P. Sheth:I. Budak Arpinar:Anupam Joshi:Tim Finin	In this paper, we describe a Semantic Web application that detects Conflict of Interest (COI) relationships among potential reviewers and authors of scientific papers. This application discovers various 'semantic associations' between the reviewers and authors in a populated ontology to determine a degree of Conflict of Interest. This ontology was created by integrating entities and relationships from two social networks, namely "knows," from a FOAF (Friend-of-a-Friend) social network and "co-author," from the underlying co-authorship network of the DBLP bibliography. We describe our experiences developing this application in the context of a class of Semantic Web applications, which have important research and engineering challenges in common. In addition, we present an evaluation of our approach for real-life COI detection.	Semantic analytics on social networks: experiences in addressing the problem of conflict of interest detection	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Xian Wu:Lei Zhang:Yong Yu	In order to obtain a machine understandable semantics for web resources, research on the Semantic Web tries to annotate web resources with concepts and relations from explicitly defined formal ontologies. This kind of formal annotation is usually done manually or semi-automatically. In this paper, we explore a complement approach that focuses on the "social annotations of the web" which are annotations manually made by normal web users without a pre-defined formal ontology. Compared to the formal annotations, although social annotations are coarse-grained, informal and vague, they are also more accessible to more people and better reflect the web resources' meaning from the users' point of views during their actual usage of the web resources. Using a social bookmark service as an example, we show how emergent semantics [2] can be statistically derived from the social annotations. Furthermore, we apply the derived emergent semantics to discover and search shared web bookmarks. The initial evaluation on our implementation shows that our method can effectively discover semantically related web bookmarks that current social bookmark service can not discover easily.	Exploring social annotations for the semantic web	NA:NA:NA	2018
Martin Gaedke	NA	Session details: Web engineering: validation	NA	2018
Jirka Kosek:Petr Nálevka	To maintain interoperability in the Web environment it is necessary to comply with Web standards. Current specifications of HTML and XHTML languages define conformance conditions both in specification prose and in a formalized way utilizing DTD. Unfortunately DTD is a very limited schema language and can not express many constraints that are specified in the free text parts of the specification. This means that a page which validates against DTD is not necessarily conforming to the specification. In this article we analyze features of modern schema languages that can improve validation of Web pages by covering more (X)HTML language constraints then DTD. Our schemas use combination of RELAX NG and Schematron to check not only the structure of the Web pages, but also datatypes of attributes and elements, more complex relations between elements and some WCAG checkpoints. A modular approach for schema composition is presented together with usage examples, including sample schemas for various compound documents (e.g. XHTML combined with MathML and SVG).The second part of this article contains description of Relaxed validator application we have developed. Relaxed is an extensible and powerful validation engine offering a convenient Web interface, a Web-service API, Java API and command-line interface. Combined with our RELAX NG + Schematron schemas, Relaxed offers very valuable validation results that surpass W3C validator in many aspects.	Relaxed: on the way towards true validation of compound documents	NA:NA	2018
Tien N. Nguyen	During a lifecycle of a large-scale Web application, Web developers produce a wide variety of inter-related Web objects. Following good Web engineering practice, developers often create them based on a Web application development method, which requires certain logical models for the development and maintenance process. Web development is dynamic, thus, those logical models as well as Web artifacts evolve over time. However, the task of managing their evolution is still very inefficient because design decisions in models are not directly accessible in existing file-based software configuration management repositories. Key limitations of existing Web version control tools include their inadequacy in representing semantics of design models and inability to manage the evolution of model-based objects and their logical connections to Web documents. This paper presents a framework that allows developers to manage versions and configurations of models and to capture changes to model-to-model relations among Web objects. Model-based objects, Web documents, and relations are directly represented and versioned in a structure-oriented manner.	Model-based version and configuration management for a web engineering lifecycle	NA	2018
Zan Sun:Jalal Mahmud:Saikat Mukherjee:I. V. Ramakrishnan	Online transactions (e.g., buying a book on the Web) typically involve a number of steps spanning several pages. Conducting such transactions under constrained interaction modalities as exemplified by small screen handhelds or interactive speech interfaces - the primary mode of communication for visually impaired individuals - is a strenuous, fatigue-inducing activity. But usually one needs to browse only a small fragment of a Web page to perform a transactional step such as a form fillout, selecting an item from a search results list, etc. We exploit this observation to develop an automata-based process model that delivers only the "relevant" page fragments at each transactional step, thereby reducing information overload on such narrow interaction bandwidths. We realize this model by coupling techniques from content analysis of Web documents, automata learning and statistical classification. The process model and associated techniques have been incorporated into Guide-O, a prototype system that facilitates online transactions using speech/keyboard interface (Guide-O-Speech), or with limited-display size handhelds (Guide-O-Mobile). Performance of Guide-O and its user experience are reported.	Model-directed web transactions under constrained modalities	NA:NA:NA:NA	2018
Ravi Kumar	NA	Session details: New search paradigms	NA	2018
Beverly Yang:Glen Jeh	Major search engines currently use the history of a user's actions (e.g., queries, clicks) to personalize search results. In this paper, we present a new personalized service, query-specific web recommendations (QSRs), that retroactively answers queries from a user's history as new results arise. The QSR system addresses two important subproblems with applications beyond the system itself: (1) Automatic identification of queries in a user's history that represent standing interests and unfulfilled needs. (2) Effective detection of interesting new results to these queries. We develop a variety of heuristics and algorithms to address these problems, and evaluate them through a study of Google history users. Our results strongly motivate the need for automatic detection of standing interests from a user's history, and identifies the algorithms that are most useful in doing so. Our results also identify the algorithms, some which are counter-intuitive, that are most useful in identifying interesting new results for past queries, allowing us to achieve very high precision over our data set.	Retroactive answering of search queries	NA:NA	2018
Jian-Tao Sun:Xuanhui Wang:Dou Shen:Hua-Jun Zeng:Zheng Chen	In this paper, we define and study a novel search problem: Comparative Web Search (CWS). The task of CWS is to seek relevant and comparative information from the Web to help users conduct comparisons among a set of topics. A system called CWS is developed to effectively facilitate Web users' comparison needs. Given a set of queries, which represent the topics that a user wants to compare, the system is characterized by: (1) automatic retrieval and ranking of Web pages by incorporating both their relevance to the queries and the comparative contents they contain; (2) automatic clustering of the comparative contents into semantically meaningful themes; (3) extraction of representative keyphrases to summarize the commonness and differences of the comparative contents in each theme. We developed a novel interface which supports two types of view modes: a pair-view which displays the result in the page level, and a cluster-view which organizes the comparative pages into the themes and displays the extracted phrases to facilitate users' comparison. Experiment results show the CWS system is effective and efficient.	CWS: a comparative web search system	NA:NA:NA:NA:NA	2018
Reiner Kraft:Chi Chao Chang:Farzin Maghoul:Ravi Kumar	Contextual search refers to proactively capturing the information need of a user by automatically augmenting the user query with information extracted from the search context; for example, by using terms from the web page the user is currently browsing or a file the user is currently editing.We present three different algorithms to implement contextual search for the Web. The first, it query rewriting (QR), augments each query with appropriate terms from the search context and uses an off-the-shelf web search engine to answer this augmented query. The second,  rank-biasing (RB), generates a representation of the context and answers queries using a custom-built search engine that exploits this representation. The third, iterative filtering meta-search (IFM), generates multiple subqueries based on the user query and appropriate terms from the search context, uses an off-the-shelf search engine to answer these subqueries, and re-ranks the results of the subqueries using rank aggregation methods.We extensively evaluate the three methods using 200 contexts and over 24,000 human relevance judgments of search results. We show that while QR works surprisingly well, the relevance and recall can be improved using RB and substantially more using IFM. Thus, QR, RB, and IFM represent a cost-effective design spectrum for contextual search.	Searching with context	NA:NA:NA:NA	2018
Richard Granger	Richard Granger will be providing an update on the deployment of information technology at a national scale in the NHS in England. Particular topics that will be covered include variability of performance and user organizations and suppliers. Access/channel strategies for NHS users and members of the public. Take-up rates for new technologies including internet adoption. Data on number of users and transactions to date will also be provided.	Keynote talk	NA	2018
Daniel Weitzner	Across the World Wide Web there is government censorship and monitoring of political messages and "morally-corrupting" material. Google have been in the news recently for capitulating to the Chinese government's demands to ban certain kinds of content, and also for refusing topass logs of browsing habits to the US government (while Microsoft and Yahoo complied wth the request). How can the Web survive as a unified, global information environment in the face of government censorship? Can governments and the private sector come to an agreement on international legal standards for the free flow of information and privacy.	Broken links on the web: local laws and the global free flow of information	NA	2018
Peter Patel-Schneider	NA	Session details: Semantic web: ontology construction	NA	2018
Harith Alani	One of the main hurdles towards a wide endorsement of ontologies is the high cost of constructing them. Reuse of existing ontologies offers a much cheaper alternative than building new ones from scratch, yet tools to support such reuse are still in their infancy. However, more ontologies are becoming available on the web, and online libraries for storing and indexing ontologies are increasing in number and demand. Search engines have also started to appear, to facilitate search and retrieval of online ontologies. This paper presents a fresh view on constructing ontologies automatically, by identifying, ranking, and merging fragments of online ontologies.	Position paper: ontology construction from online ontologies	NA	2018
Mustafa Jarrar	In this paper, we first introduce the notion of gloss for ontology engineering purposes. We propose that each vocabulary in an ontology should have a gloss. A gloss basically is an informal description of the meaning of a vocabulary that is supposed to render factual and critical knowledge to understanding a concept, but that is unreasonable or very difficult to formalize and/or articulate formally. We present a set of guidelines on what should and should not be provided in a gloss. Second, we propose to incorporate linguistic resources in the ontology engineering process. We clarify the importance of using lexical resources as a "consensus reference" in ontology engineering, and so enabling the adoption of the glosses found in these resources. A linguistic resource (i.e. its list of terms and their definitions) shall be seen as a shared vocabulary space for ontologies. We present an ontology engineering software tool (called DogmaModeler), and illustrate its support of reusing of WordNet's terms and glosses in ontology modeling.	Position paper: towards the notion of gloss, and the adoption of linguistic resources in formal ontology engineering	NA	2018
Paolo Bouquet:Luciano Serafini:Stefano Zanobini:Simone Sceffer	In most web sites, web-based applications (such as web portals, e-marketplaces, search engines), and in the file systems of personal computers, a wide variety of schemas (such as taxonomies, directory trees, thesauri, Entity-Relationship schemas, RDF Schemas) are published which (i) convey a clear meaning to humans (e.g. help in the navigation of large collections of documents), but (ii) convey only a small fraction (if any) of their meaning to machines, as their intended meaning is not formally/explicitly represented. In this paper we present a general methodology for automatically eliciting and representing the intended meaning of these structures, and for making this meaning available in domains like information integration and interoperability, web service discovery and composition, peer-to-peer knowledge management, and semantic browsers. We also present an implementation (called CtxMatch2) of how such a method can be used for semantic interoperability.	Bootstrapping semantics on the web: meaning elicitation from schemas	NA:NA:NA:NA	2018
Ari Juels	NA	Session details: Security, privacy & ethics	NA	2018
Markus Jakobsson:Jacob Ratkiewicz	We study how to design experiments to measure the success rates of phishing attacks that are ethical and accurate, which are two requirements of contradictory forces. Namely, an ethical experiment must not expose the participants to any risk; it should be possible to locally verify by the participants or representatives thereof that this was the case. At the same time, an experiment is accurate if it is possible to argue why its success rate is not an upper or lower bound of that of a real attack -- this may be difficult if the ethics considerations make the user perception of the experiment different from the user perception of the attack. We introduce several experimental techniques allowing us to achieve a balance between these two requirements, and demonstrate how to apply these, using a context aware phishing experiment on a popular online auction site which we call "rOnl". Our experiments exhibit a measured average yield of 11% per collection of unique users. This study was authorized by the Human Subjects Committee at Indiana University (Study #05-10306).	Designing ethical phishing experiments: a study of (ROT13) rOnl query features	NA:NA	2018
Markus Jakobsson:Sid Stamm	We describe the detrimental effects of browser cache/history sniffing in the context of phishing attacks, and detail an approach that neutralizes the threat by means of URL personalization; we report on an implementation performing such personalization on the fly, and analyze the costs of and security properties of our proposed solution.	Invasive browser sniffing and countermeasures	NA:NA	2018
Mehran Sahami	NA	Session details: Data mining	NA	2018
Qiaozhu Mei:Chao Liu:Hang Su:ChengXiang Zhai	Mining subtopics from weblogs and analyzing their spatiotemporal patterns have applications in multiple domains. In this paper, we define the novel problem of mining spatiotemporal theme patterns from weblogs and propose a novel probabilistic approach to model the subtopic themes and spatiotemporal theme patterns simultaneously. The proposed model discovers spatiotemporal theme patterns by (1) extracting common themes from weblogs; (2) generating theme life cycles for each given location; and (3) generating theme snapshots for each given time period. Evolution of patterns can be discovered by comparative analysis of theme life cycles and theme snapshots. Experiments on three different data sets show that the proposed approach can discover interesting spatiotemporal theme patterns effectively. The proposed probabilistic model is general and can be used for spatiotemporal text mining on any domain with time and location information.	A probabilistic approach to spatiotemporal theme pattern mining on weblogs	NA:NA:NA:NA	2018
Qiankun Zhao:Steven C. H. Hoi:Tie-Yan Liu:Sourav S. Bhowmick:Michael R. Lyu:Wei-Ying Ma	It has become a promising direction to measure similarity of Web search queries by mining the increasing amount of click-through data logged by Web search engines, which record the interactions between users and the search engines. Most existing approaches employ the click-through data for similarity measure of queries with little consideration of the temporal factor, while the click-through data is often dynamic and contains rich temporal information. In this paper we present a new framework of time-dependent query semantic similarity model on exploiting the temporal characteristics of historical click-through data. The intuition is that more accurate semantic similarity values between queries can be obtained by taking into account the timestamps of the log data. With a set of user-defined calendar schema and calendar patterns, our time-dependent query similarity model is constructed using the marginalized kernel technique, which can exploit both explicit similarity and implicit semantics from the click-through data effectively. Experimental results on a large set of click-through data acquired from a commercial search engine show that our time-dependent query similarity model is more accurate than the existing approaches. Moreover, we observe that our time-dependent query similarity model can, to some extent, reflect real-world semantics such as real-world events that are happening over time.	Time-dependent semantic similarity measure of queries using historical click-through data	NA:NA:NA:NA:NA:NA	2018
Utku Irmak:Torsten Suel	While much of the data on the web is unstructured in nature, there is also a significant amount of embedded structured data, such as product information on e-commerce sites or stock data on financial sites. A large amount of research has focused on the problem of generating wrappers, i.e., software tools that allow easy and robust extraction of structured data from text and HTML sources. In many applications, such as comparison shopping, data has to be extracted from many different sources, making manual coding of a wrapper for each source impractical. On the other hand, fully automatic approaches are often not reliable enough, resulting in low quality of the extracted data.We describe a complete system for semi-automatic wrapper generation that can be trained on different data sources in a simple interactive manner. Our goal is to minimize the amount of user effort for training reliable wrappers through design of a suitable training interface that is implemented based on a powerful underlying extraction language and a set of training and ranking algorithms. Our experiments show that our system achieves reliable extraction with a very small amount of user effort.	Interactive wrapper generation with minimal user effort	NA:NA	2018
Sean Bechhofer	NA	Session details: Semi-structured semantic data	NA	2018
Yolanda Gil:Donovan Artz	Trust is an integral part of the Semantic Web architecture. While most prior work focuses on entity-centered issues such as authentication and reputation, it does not model the content, i.e. the nature and use of the information being exchanged. This paper discusses content trust as an aggregate of other trust measures that have been previously studied. The paper introduces several factors that users consider in deciding whether to trust the content provided by a Web resource. Many of these factors are hard to capture in practice, since they would require a large amount of user input. Our goal is to discern which of these factors could be captured in practice with minimal user interaction in order to maximize the system's trust estimates. The paper also describes a simulation environment that we have designed to study alternative models of content trust.	Towards content trust of web resources	NA:NA	2018
Anupriya Ankolekar:Katia Sycara:James Herbsleb:Robert Kraut:Chris Welty	The Web plays a critical role in hosting Web communities, their content and interactions. A prime example is the open source software (OSS) community, whose members, including software developers and users, interact almost exclusively over the Web, constantly generating, sharing and refining content in the form of software code through active interaction over the Web on code design and bug resolution processes. The Semantic Web is an envisaged extension of the current Web, in which content is given a well-defined meaning, through the specification of metadata and ontologies, increasing the utility of the content and enabling information from heterogeneous sources to be integrated. We developed a prototype Semantic Web system for OSS communities, Dhruv. Dhruv provides an enhanced semantic interface to bug resolution messages and recommends related software objects and artifacts. Dhruv uses an integrated model of the OpenACS community, the software, and the Web interactions, which is semi-automatically populated from the existing artifacts of the community.	Supporting online problem-solving communities with the semantic web	NA:NA:NA:NA:NA	2018
Max Völkel:Markus Krötzsch:Denny Vrandecic:Heiko Haller:Rudi Studer	Wikipedia is the world's largest collaboratively edited source of encyclopaedic knowledge. But in spite of its utility, its contents are barely machine-interpretable. Structural knowledge, e.,g. about how concepts are interrelated, can neither be formally stated nor automatically processed. Also the wealth of numerical data is only available as plain text and thus can not be processed by its actual meaning.We provide an extension to be integrated in Wikipedia, that allows the typing of links between articles and the specification of typed data inside the articles in an easy-to-use manner.Enabling even casual users to participate in the creation of an open semantic knowledge base, Wikipedia has the chance to become a resource of semantic statements, hitherto unknown regarding size, scope, openness, and internationalisation. These semantic enhancements bring to Wikipedia benefits of today's semantic technologies: more specific ways of searching and browsing. Also, the RDF export, that gives direct access to the formalised knowledge, opens Wikipedia up to a wide range of external applications, that will be able to use it as a background knowledge base.In this paper, we present the design, implementation, and possible uses of this extension.	Semantic Wikipedia	NA:NA:NA:NA:NA	2018
Craig Wills	NA	Session details: Performance, reliability & scalability	NA	2018
A. Karve:T. Kimbrel:G. Pacifici:M. Spreitzer:M. Steinder:M. Sviridenko:A. Tantawi	We introduce and evaluate a middleware clustering technology capable of allocating resources to web applications through dynamic application instance placement. We define application instance placement as the problem of placing application instances on a given set of server machines to adjust the amount of resources available to applications in response to varying resource demands of application clusters. The objective is to maximize the amount of demand that may be satisfied using a configured placement. To limit the disturbance to the system caused by starting and stopping application instances, the placement algorithm attempts to minimize the number of placement changes. It also strives to keep resource utilization balanced across all server machines. Two types of resources are managed, one load-dependent and one load-independent. When putting the chosen placement in effect our controller schedules placement changes in a manner that limits the disruption to the system.	Dynamic placement for clustered web applications	NA:NA:NA:NA:NA:NA:NA	2018
Jingyu Zhou:Tao Yang	Internet traffic is bursty and network servers are often overloaded with surprising events or abnormal client request patterns. This paper studies a load shedding mechanism called selective early request termination (SERT) for network services that use threads to handle multiple incoming requests continuously and concurrently. Our investigation with applications from Ask.com shows that during overloaded situations, a relatively small percentage of long requests that require excessive computing resource can dramatically affect other short requests and reduce the overall system throughput. By actively detecting and aborting overdue long requests, services can perform significantly better to achieve QoS objectives compared to a purely admission based approach. We have proposed a termination scheme that monitors running time of requests, accounts for their resource usage, adaptively adjusts the selection threshold, and performs a safe termination for a class of requests. This paper presents the design and implementation of this scheme and describes experimental results to validate the proposed approach.	Selective early request termination for busy internet services	NA:NA	2018
Preethi Natarajan:Janardhan R. Iyengar:Paul D. Amer:Randall Stewart	We propose using the Stream Control Transmission Protocol (SCTP), a recent IETF transport layer protocol, for reliable web transport. Although TCP has traditionally been used, we argue that SCTP better matches the needs of HTTP-based network applications. This position paper discusses SCTP features that address: (i) head-of-line blocking within a single TCP connection, (ii) vulnerability to network failures, and (iii) vulnerability to denial-of-service SYN attacks. We discuss our experience in modifying the Apache server and the Firefox browser to benefit from SCTP, and demonstrate our HTTP over SCTP design via simple experiments. We also discuss the benefits of using SCTP in other web domains through two example scenarios ? multiplexing user requests, and multiplexing resource access. Finally, we highlight several SCTP features that will be valuable to the design and implementation of current HTTP-based client-server applications.	SCTP: an innovative transport layer protocol for the web	NA:NA:NA:NA	2018
Soumen Chakrabati	NA	Session details: Data mining classification	NA	2018
Christopher H. Brooks:Nancy Montanez	Tags have recently become popular as a means of annotating and organizing Web pages and blog entries. Advocates of tagging argue that the use of tags produces a 'folksonomy', a system in which the meaning of a tag is determined by its use among the community as a whole. We analyze the effectiveness of tags for classifying blog entries by gathering the top 350 tags from Technorati and measuring the similarity of all articles that share a tag. We find that tags are useful for grouping articles into broad categories, but less effective in indicating the particular content of an article. We then show that automatically extracting words deemed to be highly relevant can produce a more focused categorization of articles. We also show that clustering algorithms can be used to reconstruct a topical hierarchy among tags, and suggest that these approaches may be used to address some of the weaknesses in current tagging systems.	Improved annotation of the blogosphere via autotagging and hierarchical clustering	NA:NA	2018
Steven C. H. Hoi:Rong Jin:Michael R. Lyu	Large-scale text categorization is an important research topic for Web data mining. One of the challenges in large-scale text categorization is how to reduce the human efforts in labeling text documents for building reliable classification models. In the past, there have been many studies on applying active learning methods to automatic text categorization, which try to select the most informative documents for labeling manually. Most of these studies focused on selecting a single unlabeled document in each iteration. As a result, the text categorization model has to be retrained after each labeled document is solicited. In this paper, we present a novel active learning algorithm that selects a batch of text documents for labeling manually in each iteration. The key of the batch mode active learning is how to reduce the redundancy among the selected examples such that each example provides unique information for model updating. To this end, we use the Fisher information matrix as the measurement of model uncertainty and choose the set of documents to effectively maximize the Fisher information of a classification model. Extensive experiments with three different datasets have shown that our algorithm is more effective than the state-of-the-art active learning techniques for text categorization and can be a promising tool toward large-scale text categorization for World Wide Web documents.	Large-scale text categorization by batch mode active learning	NA:NA:NA	2018
Dou Shen:Jian-Tao Sun:Qiang Yang:Zheng Chen	It is well known that Web-page classification can be enhanced by using hyperlinks that provide linkages between Web pages. However, in the Web space, hyperlinks are usually sparse, noisy and thus in many situations can only provide limited help in classification. In this paper, we extend the concept of linkages from explicit hyperlinks to implicit links built between Web pages. By observing that people who search the Web with the same queries often click on different, but related documents together, we draw implicit links between Web pages that are clicked after the same queries. Those pages are implicitly linked. We provide an approach for automatically building the implicit links between Web pages using Web query logs, together with a thorough comparison between the uses of implicit and explicit links in Web page classification. Our experimental results on a large dataset confirm that the use of the implicit links is better than using explicit links in classification performance, with an increase of more than 10.5% in terms of the Macro-F1 measurement.	A comparison of implicit and explicit links for web page classification	NA:NA:NA:NA	2018
Mark Manasse	NA	Session details: E-commerce & e-government	NA	2018
John Debenham:Simeon Simoff	Fully automated trading, such as e-procurement, using the Internet is virtually unheard of today. Three core technologies are needed to fully automate the trading process: data mining, intelligent trading agents and virtual institutions in which informed trading agents can trade securely both with each other and with human agents in a natural way. This paper describes a demonstrable prototype e-trading system that integrates these three technologies and is available on the World Wide Web. This is part of a larger project that aims to make informed automated trading a reality.	An e-market framework for informed trading	NA:NA	2018
Ian Pascal Volz	The music industry's business model is to produce stars. In order to do so, musicians producing music that fits into well defined clusters of factors explaining the demand of the majority of music consumers are disproportionately promoted. This leads to a limitation of available diversity and therefore of a limitation of the end user's benefit from listening to music. This paper analyses online music consumer's needs and preferences. These factors are used in order to explain the demand for stars and the impact of different online music services on promoting a more diverse music market.	The impact of online music services on the demand for stars in the music industry	NA	2018
Vaclav Petricek:Tobias Escher:Ingemar J. Cox:Helen Margetts	In this paper we describe preliminary work that examines whether statistical properties of the structure of websites can be an informative measure of their quality. We aim to develop a new method for evaluating e-government. E-government websites are evaluated regularly by consulting companies, international organizations and academic researchers using a variety of subjective measures. We aim to improve on these evaluations using a range of techniques from webmetric and social network analysis. To pilot our methodology, we examine the structure of government audit office sites in Canada, the USA, the UK, New Zealand and the Czech Republic.We report experimental values for a variety of characteristics, including the connected components, the average distance between nodes, the distribution of paths lengths, and the indegree and outdegree. These measures are expected to correlate with (i) the navigability of a website and (ii) with its "nodalityö which is a combination of hubness and authority. Comparison of websites based on these characteristics raised a number of issues, related to the proportion of non-hyperlinked content (e.g. pdf and doc files) within a site, and both the very significant differences in the size of the websites and their respective national populations. Methods to account for these issues are proposed and discussed.There appears to be some correlation between the values measured and the league tables reported in the literature. However, this multi dimensional analysis provides a richer source of evaluative techniques than previous work. Our analysis indicates that the US and Canada provide better navigability, much better than the UK; however, the UK site is shown to have the strongest "nodalityö on the Web.	The web structure of e-government - developing a methodology for quantitative evaluation	NA:NA:NA:NA	2018
Massimo Mecella	NA	Session details: XML & web services	NA	2018
Harry Halpin:Henry S. Thompson	We present a paradigm for uniting the diverse strands of XML-based Web technologies by allowing them to be incorporated within a single document. This overcomes the distinction between programs and data to make XML truly "self-describing." A proposal for a lightweight yet powerful functional XML vocabulary called "Semantic fXML" is detailed, based on the well-understood functional programming paradigm and resembling the embedding of Lisp directly in XML. Infosets are made "dynamic," since documents can now directly embed local processes or Web Services into their Infoset. An optional typing regime for info-sets is provided by Semantic Web ontologies. By regarding Web Services as functions and the Semantic Web as providing types, and tying it all together within a single XML vocabulary, the Web can compute. In this light, the real Web 2.0 can be considered the transformation of the Web from a universal information space to a universal computation space.	One document to bind them: combining XML, web services, and the semantic web	NA:NA	2018
Monika Solanki:Antonio Cau:Hussein Zedan	A Service oriented system emerges from composition of services. Dynamically composed reactive Web services form a special class of service oriented system, where the delays associated with communication, unreliability and unavailability of services, and competition for resources from multiple service requesters are dominant concerns. As complexity of services increase, an abstract design language for the specification of services and interaction between them is desired. In this paper, we present ASDL (Abstract Service Design Language), a wide spectrum language for modelling Web services. We initially provide an informal description of our computational model for service oriented systems. We then present ASDL along with its specification oriented semantics defined in Interval Temporal Logic (ITL): a sound formalism for specifying and reasoning about temporal properties of systems. The objective of ASDL is to provide a notation for the design of service composition and interaction protocols at an abstract level.	ASDL: a wide spectrum language for designing web services	NA:NA:NA	2018
Nicole Oldham:Kunal Verma:Amit Sheth:Farshad Hakimpour	In a dynamic service oriented environment it is desirable for service consumers and providers to offer and obtain guarantees regarding their capabilities and requirements. WS-Agreement defines a language and protocol for establishing agreements between two parties. The agreements are complex and expressive to the extent that the manual matching of these agreements would be expensive both in time and resources. It is essential to develop a method for matching agreements automatically. This work presents the framework and implementation of an innovative tool for the matching providers and consumers based on WS-Agreements. The approach utilizes Semantic Web technologies to achieve rich and accurate matches. A key feature is the novel and flexible approach for achieving user personalized matches.	Semantic WS-agreement partner selection	NA:NA:NA:NA	2018
Nick Craswell	NA	Session details: Improved search ranking	NA	2018
Matthew Richardson:Amit Prakash:Eric Brill	Since the publication of Brin and Page's paper on PageRank, many in the Web community have depended on PageRank for the static (query-independent) ordering of Web pages. We show that we can significantly outperform PageRank using features that are independent of the link structure of the Web. We gain a further boost in accuracy by using data on the frequency at which users visit Web pages. We use RankNet, a ranking machine learning algorithm, to combine these and other static features based on anchor text and domain characteristics. The resulting model achieves a static ranking pairwise accuracy of 67.3% (vs. 56.7% for PageRank or 50% for random).	Beyond PageRank: machine learning for static ranking	NA:NA:NA	2018
Soumen Chakrabarti:Kriti Puniyani:Sujatha Das	We introduce a new, powerful class of text proximity queries: find an instance of a given "answer type" (person, place, distance) near "selector" tokens matching given literals or satisfying given ground predicates. An example query is type=distance NEAR Hamburg Munich. Nearness is defined as a flexible, trainable parameterized aggregation function of the selectors, their frequency in the corpus, and their distance from the candidate answer. Such queries provide a key data reduction step for information extraction, data integration, question answering, and other text-processing applications. We describe the architecture of a next-generation information retrieval engine for such applications, and investigate two key technical problems faced in building it. First, we propose a new algorithm that estimates a scoring function from past logs of queries and answer spans. Plugging the scoring function into the query processor gives high accuracy: typically, an answer is found at rank 2-4. Second, we exploit the skew in the distribution over types seen in query logs to optimize the space required by the new index structures required by our system. Extensive performance studies with a 10GB, 2-million document TREC corpus and several hundred TREC queries show both the accuracy and the efficiency of our system. From an initial 4.3GB index using 18,000 types from WordNet, we can discard 88% of the space, while inflating query times by a factor of only 1.9. Our final index overhead is only 20% of the total index space needed.	Optimizing scoring functions and indexes for proximity search in type-annotated corpora	NA:NA:NA	2018
Feng Qiu:Junghoo Cho	One hundred users, one hundred needs. As more and more topics are being discussed on the web and our vocabulary remains relatively stable, it is increasingly difficult to let the search engine know what we want. Coping with ambiguous queries has long been an important part of the research on Information Retrieval, but still remains a challenging task. Personalized search has recently got significant attention in addressing this challenge in the web search community, based on the premise that a user's general preference may help the search engine disambiguate the true intention of a query. However, studies have shown that users are reluctant to provide any explicit input on their personal preference. In this paper, we study how a search engine can learn a user's preference automatically based on her past click history and how it can use the user preference to personalize search results. Our experiments show that users' preferences can be learned accurately even from little click-history data and personalized search based on user preference yields significant improvements over the best existing ranking mechanism in the literature.	Automatic identification of user interest for personalized search	NA:NA	2018
Collin Jackson:Andrew Bortz:Dan Boneh:John C. Mitchell	Through a variety of means, including a range of browser cache methods and inspecting the color of a visited hyperlink, client-side browser state can be exploited to track users against their wishes. This tracking is possible because persistent, client-side browser state is not properly partitioned on per-site basis in current browsers. We address this problem by refining the general notion of a "same-origin" policy and implementing two browser extensions that enforce this policy on the browser cache and visited links.We also analyze various degrees of cooperation between sites to track users, and show that even if long-term browser state is properly partitioned, it is still possible for sites to use modern web features to bounce users between sites and invisibly engage in cross-domain tracking of their visitors. Cooperative privacy attacks are an unavoidable consequence of all persistent browser state that affects the behavior of the browser, and disabling or frequently expiring this state is the only way to achieve true privacy against colluding parties.	Protecting browser state from web privacy attacks	NA:NA:NA:NA	2018
Ron Brachman:Dan Connolly:Rohit Khare:Frank Smadja:Frank van Harmelen	It is a truism that as the Web grows in size and scope, it becomes harder to find what we want, to identify like-minded people and communities, to find the best ads to offer, and to have applications work together smoothly. Services don't interoperate; queries yield long lists of results, most of which seem to miss the point. If the Web were a person, we would expect richer and more successful interactions with it - interactions that were, quite literally, more meaningful. That's because in human discourse, it is shared meaning that gives us real communication. Yet with the current Web, meaning cannot be found.Much recent work has aspired to change this, both for human-machine interchange and machine-machine synchronization. Certainly the "semantic web" looks to add meaning to our current simplistic matching of mere strings of characters against mere "bags" of words. But can we legislate meaning from on high? Isn't meaning organic and determined by use, a moving and context-dependent target? But if meaning is an evolving organic soup, how are humans able to get anything done with one another? Don't we love to "define our terms"? But then again, is real definition even possible?These questions have daunted philosophers for years, and we probably won't solve them here. But we'll try to understand what's at the root of our own current religious debate: should meaning on the Web be evolutionary, driven organically through the bottom-up human assignment of tags? Or does it need to be carefully crafted and managed by a higher authority, using structured representations with defined semantics? Without picket signs or violence (we hope), our panelists will explore the two extreme ends of the spectrum - and several points in between.	Meaning on the web: evolution vs intelligent design?	NA:NA:NA:NA:NA	2018
Arnaud Sahuguet:Stefan Brands:Kim Cameron:Cahill Conor:Aude Pichelin:Fulup Ar Foll:Mike Neuenschwander	Since the early days of the Web, identity management has been a big issue. As the famous cartoon from the New Yorker reminds us, "on the internet, nobody knows you are a dog". This was true back in July 1993. This is true today. For the last few years, numerous initiatives have emerged to tackle this issue: Microsoft Passport, Liberty Alliance, 3GPP GUP, Shibboleth, to name a few. Major investments are being made in this area and this is foreseen as a multi-billion dollar market. Yet, as of this writing, there is still no widespread identity management infrastructure in place ready to be used by the general public on converged networks.The goal of this panel is to do a reality check and try to answer the following five questions: What is identity management? Who needs identity management and why? What will the identity management ecosystem look like? What's agreed upon? What's next?	Identity management on converged networks: a reality check	NA:NA:NA:NA:NA:NA:NA	2018
Rachna Dhamija:Peter Cassidy:Phillip Hallam-Baker:Markus Jacobsson	In the last few years, Internet users have seen the rapid expansion of "phishing", the use of spoofed e-mails and fraudulent websites designed to trick users into divulging sensitive data. More recently, we have seen the growth of "pharming", the use of malware or DNS-based attacks to misdirect users to rogue websites. In this panel, we will examine the state of the art in anti-phishing solutions and explore promising directions for future research.	Phoiling phishing	NA:NA:NA:NA	2018
Nigel Shadbolt:Tim Berners-Lee:Jim Hendler:Claire Hart:Richard Benjamins	The World Wide Web has been revolutionary in terms of impact, scale and outreach. At every level society has been changed in some way by the Web. This Panel will consider likely developments in this extraordinary human construct as we attempt to realise the Next Wave of the Web - a Semantic Web.Nigel Shadbolt will Chair a discussion that will focus on the prospects for the Semantic Web, its likely form and the challenges it faces. Can we achieve the necessary agreements on shared meaning for the Semantic Web? Can we achieve a critical mass of semantically annotated data and content? How are we to trust such content? Do the scientific and commercial drivers really demand a Semantic Web? How will the move to a mobile and ubiquitous Web affect the Semantic Web? How does Web 2.0 relate to the Semantic Web?	The next wave of the web	NA:NA:NA:NA:NA	2018
Monika Solanki	NA	Session details: XML	NA	2018
P. Ferragina:F. Luccio:G. Manzini:S. Muthukrishnan	XML is fast becoming the standard format to store, exchange and publish over the web, and is getting embedded in applications. Two challenges in handling XML are its size (the XML representation of a document is significantly larger than its native state) and the complexity of its search (XML search involves path and content searches on labeled tree structures). We address the basic problems of compression, navigation and searching of XML documents. In particular, we adopt recently proposed theoretical algorithms [11] for succinct tree representations to design and implement a compressed index for XML, called XBZIPiNDEX, in which the XML document is maintained in a highly compressed format, and both navigation and searching can be done uncompressing only a tiny fraction of the data. This solution relies on compressing and indexing two arrays derived from the XML data. With detailed experiments we compare this with other compressed XML indexing and searching engines to show that XBZIPiNDEX has compression ratio up to 35% better than the ones achievable by those other tools, and its time performance on some path and content search operations is order of magnitudes faster: few milliseconds over hundreds of MBs of XML files versus tens of seconds, on standard XML data sources.	Compressing and searching XML data via two zips	NA:NA:NA:NA	2018
Kentaro Toyama	NA	Session details: Developing regions & peer-to-peer	NA	2018
Nilesh Mishra:Kameswari Chebrolu:Bhaskaran Raman:Abhinav Pathak	In bridging the digital divide, two important criteria are cost-effectiveness, and power optimization. While 802.11 is cost-effective and is being used in several installations in the developing world, typical system configurations are not really power efficient. In this paper, we propose a novel "Wake-on-WLAN" mechanism for coarse-grained, on-demand power on/off of the networking equipment at a remote site. The novelty also lies in our implementation of a prototype system using low-power 802.15.4-based sensor motes. We describe the prototype, as well as its evaluation on field in a WiFi testbed. Preliminary estimates indicate that the proposed mechanism can save significant power in typical rural networking settings.	Wake-on-WLAN	NA:NA:NA:NA	2018
Bowei Du:Michael Demmer:Eric Brewer	In this paper we present an analysis of HTTP traffic captured from Internet cafés and kiosks from two different developing countries -- Cambodia and Ghana. This paper has two main contributions. The first contribution is a analysis of the characteristics of the web trace, including the distribution and classification of the web objects requested by the users. We outline notable features of the data set which effect the performance of the web for users in developing regions. Using the trace data, we also perform several simulation analyses of cache performance, including both traditional caching and more novel off-line caching proposals. The second contribution is a set of suggestions on mechanisms to improve the user experience of the web in these regions. These mechanisms include both applications of well-known research techniques as well as offering some less well-studied suggestions based on intermittent connectivity.	Analysis of WWW traffic in Cambodia and Ghana	NA:NA:NA	2018
Kentaro Toyama	NA	Session details: Developing regions 2	NA	2018
Joyojeet Pal:Udai Singh Pawar:Eric A. Brewer:Kentaro Toyama	Computer-aided learning is fast gaining traction in developing regions as a means to augment classroom instruction. Reasons for using computer-aided learning range from supplementing teacher shortages to starting underprivileged children off in technology, and funding for such initiatives range from state education funds to international agencies and private groups interested in child development. The interaction of children with computers is seen at various levels, from unsupervised self-guided learning at public booths without specific curriculum to highly regulated in-class computer applications with modules designed to go with school curriculum. Such learning is used at various levels from children as young as 5 year-old to high-schoolers. This paper uses field observations of primary school children in India using computer-aided learning modules, and finds patterns by which children who perform better in classroom activities seat themselves in front of computer monitors, and control the mouse, in cases where children are required to share computer resources. We find that in such circumstances, there emerges a pattern of learning, unique to multi-user environments - wherein certain children tend to learn better because of their control of the mouse. This research also shows that while computer aided learning software for children is primarily designed for single-users, the implementation realities of resource-strapped learning environments in developing regions presents a strong case for multi-user design.	The case for multi-user design for computer aided learning in developing regions	NA:NA:NA:NA	2018
Tapan S. Parikh:Edward D. Lazowska	Implementing successful rural computing applications requires addressing a number of significant challenges. Recent advances in mobile phone computing capabilities make this device a likely candidate to address the client hardware constraints. Long battery life, wireless connectivity, solid-state memory, low price and immediate utility all make it better suited to rural conditions than a PC. However, current mobile software platforms are not as appropriate. Web-based mobile applications are hard to use, do not take advantage of the mobile phone's media capabilities and require an online connection. Custom mobile applications are difficult to develop and distribute. To address these limitations we present CAM - a new framework for developing and deploying mobile computing applications in the rural developing world. CAM applications are accessed by capturing barcodes using the mobile phone camera, or entering numeric strings with the keypad. Supporting minimal navigation, direct linkage to paper practices and offline multi-media interaction, CAM is uniquely adapted to rural device, user and infrastructure constraints. To illustrate the breadth of the framework, we list a number of CAM-based applications that we have implemented or are planning. These include processing microfinance loans, facilitating rural supply chains, documenting grassroots innovation and accessing electronic medical histories.	Designing an architecture for delivering mobile information services to the rural developing world	NA:NA	2018
Prasad Pingali:Jagadeesh Jagarlamudi:Vasudeva Varma	Today web search engines provide the easiest way to reach information on the web. In this scenario, more than 95% of Indian language content on the web is not searchable due to multiple encodings of web pages.Most of these encodings are proprietary and hence need some kind of standardization for making the content accessible via a search engine. In this paper we present a search engine called WebKhoj which is capable of searching multi-script and multi-encoded Indian language content on the web. We describe a language focused crawler and the transcoding processes involved to achieve accessibility of Indian langauge content. In the end we report some of the experiments that were conducted along with results on Indian language web content.	WebKhoj: Indian language IR from multiple character encodings	NA:NA:NA	2018
Marc Najork	NA	Session details: Industrial practice & experience	NA	2018
Pavel A. Dmitriev:Nadav Eiron:Marcus Fontoura:Eugene Shekita	A major difference between corporate intranets and the Internet is that in intranets the barrier for users to create web pages is much higher. This limits the amount and quality of anchor text, one of the major factors used by Internet search engines, making intranet search more difficult. The social phenomenon at play also means that spam is relatively rare. Both on the Internet and in intranets, users are often willing to cooperate with the search engine in improving the search experience. These characteristics naturally lead to considering using user feedback to improve search quality in intranets. In this paper we show how a particular form of feedback, namely user annotations, can be used to improve the quality of intranet search. An annotation is a short description of the contents of a web page, which can be considered a substitute for anchor text. We propose two ways to obtain user annotations, using explicit and implicit feedback, and show how they can be integrated into a search engine. Preliminary experiments on the IBM intranet demonstrate that using annotations improves the search quality.	Using annotations in enterprise search	NA:NA:NA:NA	2018
Baoning Wu:Brian D. Davison	By supplying different versions of a web page to search engines and to browsers, a content provider attempts to cloak the real content from the view of the search engine. Semantic cloaking refers to differences in meaning between pages which have the effect of deceiving search engine ranking algorithms. In this paper, we propose an automated two-step method to detect semantic cloaking pages based on different copies of the same page downloaded by a web crawler and a web browser. The first step is a filtering step, which generates a candidate list of semantic cloaking pages. In the second step, a classifier is used to detect semantic cloaking pages from the candidates generated by the filtering step. Experiments on manually labeled data sets show that we can generate a classifier with a precision of 93% and a recall of 85%. We apply our approach to links from the dmoz Open Directory Project and estimate that more than 50,000 of these pages employ semantic cloaking.	Detecting semantic cloaking on the web	NA:NA	2018
Honghua (Kathy) Dai:Lingzhi Zhao:Zaiqing Nie:Ji-Rong Wen:Lee Wang:Ying Li	Understanding goals and preferences behind a user's online activities can greatly help information providers, such as search engine and E-Commerce web sites, to personalize contents and thus improve user satisfaction. Understanding a user's intention could also provide other business advantages to information providers. For example, information providers can decide whether to display commercial content based on user's intent to purchase. Previous work on Web search defines three major types of user search goals for search queries: navigational, informational and transactional or resource [1][7]. In this paper, we focus our attention on capturing commercial intention from search queries and Web pages, i.e., when a user submits the query or browse a Web page, whether he/she is about to commit or in the middle of a commercial activity, such as purchase, auction, selling, paid service, etc. We call the commercial intentions behind a user's online activities as OCI (Online Commercial Intention). We also propose the notion of "Commercial Activity Phase" (CAP), which identifies in which phase a user is in his/her commercial activities: Research or Commit. We present the framework of building machine learning models to learn OCI based on any Web page content. Based on that framework, we build models to detect OCI from search queries and Web pages. We train machine learning models from two types of data sources for a given search query: content of algorithmic search result page(s) and contents of top sites returned by a search engine. Our experiments show that the model based on the first data source achieved better performance. We also discover that frequent queries are more likely to have commercial intention. Finally we propose our future work in learning richer commercial intention behind users' online activities.	Detecting online commercial intention (OCI)	NA:NA:NA:NA:NA:NA	2018
Martin Halvey:Mark T. Keane:Barry Smyth	Many systems use past behavior, preferences and environmental factors to attempt to predict user navigation on the Internet. However we believe that many of these models have shortcomings, in that they do not take into account that users may have many different sets of preferences. Here we investigate an environmental factor, namely time, in making predictions about user navigation. We present methods for creating temporal rules that describe user navigation patterns. We also show the benefit of using these rules to predict user navigation and also show the benefits of these models over traditional methods. An analysis is carried out on a sample of usage logs for Wireless Application Protocol (WAP) browsing, and the results of this analysis verify our hypothesis.	Temporal rules for mobile web personalization	NA:NA:NA	2018
Ganesan Velayathan:Seiji Yamada	This paper describes our efforts to factor in a user's browsing behavior to automatically evaluate web pages that the user shows interest in, based on user browsing behaviors while browsing. To evaluate a webpage automatically, we have developed a client-side logging tool: the GINIS Framework. We do not focus just on clicking, scrolling, navigation, or duration of visit alone, but we propose integrating these patterns of interaction to recognize and evaluate a user's response to a given web page.	Behavior-based web page evaluation	NA:NA	2018
Melanie Kellar:Carolyn Watters	The automatic identification of a user's task has the potential to improve information filtering systems that rely on implicit measures of interest and whose effectiveness may be dependant upon the task at hand. Knowledge of a user's current task type would allow information filtering systems to apply the most useful measures of user interest. We recently conducted a field study in which we logged all participants' interactions with their web browsers and asked participants to categorize their web usage according to a high-level task schema. Using the data collected during this study, we have conducted a preliminary exploration of the usefulness of logged web browser interactions to predict users' tasks. The results of this initial analysis suggest that individual models of users' web browser interactions may be useful in predicting task type.	Using web browser interactions to predict task	NA:NA	2018
Masahiro Hamasaki:Yutaka Matsuo:Keisuke Ishida:Tom Hope:Takuichi Nishimura:Hideaki Takeda	A social network can become bases for information infrastructure in the future. It is important to extract social networks that are not biased. Providing a simple means for users to register their social relation is also important. We propose a method that combines various approaches to extract social networks. Especially, three kinds of networks are extracted; user-registered Know link network, Web-mined Web link network, and face-to-face Touch link network. In this paper, the combination of social network extraction for communities is described, and the analysis on the extracted social networks is shown.	An integrated method for social network extraction	NA:NA:NA:NA:NA:NA	2018
Marta Gatius:Meritxell González:Sheyla Militello:Pablo Hernández	In this paper, we describe how domain ontologies are used in a dialogue system guiding the user to access web public administration contents. The current implementation of the system supports speech (through the telephone) and text mode in different languages (English, Spanish, Catalan and Italian).	Integrating semantic web and language technologies to improve the online public administrations services	NA:NA:NA:NA	2018
Cristiano Maciel:Ana Cristina Bicharra Garcia	Electronic democracy should provide information and service for the citizens on the Internet, allowing room for debate, participation and electronic voting. The languages being adopted by mass communication means, especially Reality Shows, are efficient and encourage public participation in decision-making. This paper discusses a citizen-government interaction language intended to facilitate citizen participation in the government's decisions. An e-Democracy Model for people participation through web-based technologies is conceived. This model specifies the syntax of an Democracy Interaction Language, a DemIL. Such language incorporates characteristics of Reality Show Formats, and it is the back-end of a web-interface project in the domain researched. The study of case Participative Budget of Brazil represents the language proposed.	DemIL: an online interaction language between citizen and government	NA:NA	2018
Osamu Segawa	We have developed a system that allows users to add annotations immediately onto a Web page they are viewing, and share the information via a network. A novel feature of our method is that P2P nodes in the system determine their roles autonomously, and share the annotation data. Our method is based on P2P; however, P2P nodes in the system change their roles and data transfer procedures, depending on their network topology or the status of other nodes.Our method is robust to node or network problems, and has flexible scalability.	Web annotation sharing using P2P	NA	2018
Alexander Jaffe:Mor Naaman:Tamir Tassa:Marc Davis	We describe a framework for automatically selecting a summary set of photographs from a large collection of geo-referenced photos. The summary algorithm is based on spatial patterns in photo sets, but can be expanded to support social, temporal, as well as textual-topical factors of the photo set. The summary set can be biased by the user, the content of the user's query, and the context in which the query is made. An initial evaluation on a set of geo-referenced photos shows that our algorithm performs well, producing results that are highly rated by users.	Generating summaries for large collections of geo-referenced photographs	NA:NA:NA:NA	2018
Lloyd Rutledge:Lora Aroyo:Natalia Stash	Currently, there is an increasing effort to provide various personalized services on museum web sites. This paper presents an approach for determining user interests in a museum collection with the help of an interactive dialog. It uses a semantically annotated collection of the Rijksmuseum Amsterdam to elicit specific user's interests in artists, periods, genres and themes and uses these values to recommend relevant artefacts and related concepts from the museum collection. In the presented prototype, we show how constructing a user profile and applying recommender strategies in this way enable dynamical generation personalized museum tours for different users.	Determining user interests about museum collections	NA:NA:NA	2018
Omar Alonso:Sandeepan Banerjee:Mark Drake	It is well understood that the key for successful Semantic Web applications depends on the availability of machine understandable meta-data. We describe the Information Grid, a practical approach to the Semantic Web, and show a prototype implementation. Information grid resources span all the data in the organization and all the metadata required to make it meaningful. The final goal is to let organizations view their assets in a smooth continuum from the Internet to the Intranet, with uniform semantically rich access.	GIO: a semantic web application using the information grid framework	NA:NA:NA	2018
Andreas Harth:Sebastian Ryszard Kruk:Stefan Decker	In this poster we discuss a graphical notation for representing queries for semistructured data. We try to strike a balance between expressiveness of the query language and simplicity and understandability of the graphical notation. We present the primitives of the notation by means of examples.	Graphical representation of RDF queries	NA:NA:NA	2018
Philipp Cimiano:Peter Haase:York Sure:Johanna Völker:Yimin Wang	In this poster we present an approach to query answering over knowledge sources that makes use of different ontology management components within an application scenario of the BT Digital Library. The novelty of the approach lies in the combination of different semantic technologies providing a clear benefit for the application scenario considered.	Question answering on top of the BT digital library	NA:NA:NA:NA:NA	2018
Kaspar Giger:Erik Wilde	Locating files based on file system structure, file properties, and maybe even file contents is a core task of the user interface of operating systems. By adapting XPath's power to the environment of a Unix shell, it is possible to greatly increase the expressive power of the command line language. We present a concept for integrating an XPath view of the file system into a shell, the emphXPath Shell (XPsh), which can be used to find files based on file attributes and contents in a very flexible way. The syntax of the command line language is backwards compatible with traditional shells, and the new XPath-based expressions can be easily mastered with a little bit of XPath knowledge.	XPath filename expansion in a Unix shell	NA:NA	2018
Rohit Khare:Tantek Çelik	Microformats are a clever adaptation of semantic XHTML that makes it easier to publish, index, and extract semi-structured information such as tags, calendar entries, contact information, and reviews on the Web. This makes it a pragmatic path towards achieving the vision set forth for the Semantic Web.Even though it sidesteps the existing "technology stack" of RDF, ontologies, and Artificial Intelligence-inspired processing tools, various microformats have emerged that parallel the goals of several well-known Semantic Web projects. This poster compares their prospects to the Semantic Web according to Rogers' Diffusion of Innovation model.	Microformats: a pragmatic path to the semantic web	NA:NA	2018
Asunción Gómez-Pérez:Rafael González-Cabero	In this paper, we describe SGSDesigner, the ODESGS Environment user interface. ODESGS Environment (the realization of the ODESGS Framework [1]) is an environment for supporting both a) the annotation of pre-existing Grid Services (GSs) and b) the design of new complex Semantic Grid Services (SGSs) in a (semi) automatic way.	SGSDesigner: a graphical interface for annotating and designing semantic grid services	NA:NA	2018
Rizza Camus Caminero:Pavol Zavarsky:Yoshiki Mikami	As part of the Language Observatory Project [4], we have been crawling all the web space since 2004. We have collected terabytes of data mostly from Asian and African ccTLDs. In this paper, we present results of the current status of the African web and compare it with its status in 2004 and 2002. This paper focuses on the accessibility of the web pages, the web tree growth, web technology, privacy protection, and web interconnection.	Status of the African Web	NA:NA:NA	2018
Ann Chapman:Brian Kelly:Liddy Nevile:Andy Heath	This paper describes personalization metadata standards that can be used to enable individuals to access and use resources based on a user's particular requirements. The paper describes two approaches which are being developed in the library and Web worlds and highlights some of the potential challenges which will need to be addressed in order to maximise interoperability. The paper concludes by arguing the need for greater dialogue across these two communities.	Personalization and accessibility: integration of library and web approaches	NA:NA:NA:NA	2018
Patrizia Andronico:Marina Buzzi:Barbara Leporini:Carlos Castillo	We present the results of a research project focus on improving the usability of web search tools for blind users who interact via screen reader and voice synthesizer. In the first stage of our study, we proposed eight specific guidelines for simplifying this interaction with search engines. Next, we evaluated these criteria by applying them to Google UIs, re-implementing the simple search and the result page. Finally, we prepared the environment for a remote test with 12 totally blind users. The results highlight how Google interfaces could be improved in order to simplify interaction for the blind.	Testing google interfaces modified for the blind	NA:NA:NA:NA	2018
Suhit Gupta:Hila Becker:Gail Kaiser:Salvatore Stolfo	The content of a webpage is usually contained within a small body of text and images, or perhaps several articles on the same page; however, the content may be lost in the clutter, particularly hurting users browsing on small cell phone and PDA screens and visually impaired users relying on speed rendering of web pages. Using the genre of a web page, we have created a solution, Crunch that automatically identifies clutter and removes it, thus leaving a clean content-full page. In order to evaluate the improvement in the applications for this technology, we identified a number of experiments. In this paper, we have those experiments, the associated results and their evaluation.	Verifying genre-based clustering approach to content extraction	NA:NA:NA:NA	2018
Adam Jatowt:Yukiko Kawai:Satoshi Nakamura:Yutaka Kidawara:Katsumi Tanaka	We describe a browser for the past web. It can retrieve data from multiple past web resources and features a passive browsing style based on change detection and presentation. The browser shows past pages one by one along a time line. The parts that were changed between consecutive page versions are animated to reflect their deletion or insertion, thereby drawing the user's attention to them. The browser enables automatic skipping of changeless periods and filtered browsing based on user specified query.	A browser for browsing the past web	NA:NA:NA:NA:NA	2018
Natarajan Kannan:Toufeeq Hussain	This paper provides a novel approach to use URI fragment identifiers to enable HTTP clients to address and process content, independent of its original representation.	Live URLs: breathing life into URLs	NA:NA	2018
Erik Wilde	Namespaces are a central building block of XML technologies today, they provide the identification mechanism for many XML-related vocabularies. Despite their ubiquity, there is no established mechanism for describing namespaces, and in particular for describing the dependencies of namespaces. We propose a simple model for describing namespaces and their dependencies. Using these descriptions, it is possible to compile directories of namespaces providing searchable and browsable namespace descriptions.	Structuring namespace descriptions	NA	2018
Huajing Li:Isaac Councill:Wang-Chien Lee:C. Lee Giles	CiteSeer is a scientific literature digital library and search engine which automatically crawls and indexes scientific documents in the field of computer and information science. After serving as a public search engine for nearly ten years, CiteSeer is starting to have scaling problems for handling of more documents, adding new feature and more users. Its monolithic architecture design prevents it from effectively making use of new web technologies and providing new services. After analyzing the current system problems, we propose a new architecture and data model, CiteSeerx. CiteSeerx that will overcome the existing problems as well as provide scalability and better performance plus new services and system features.	CiteSeerx: an architecture and web service design for an academic document search engine	NA:NA:NA:NA	2018
Erik Wilde	There are principal differences between the relational model and XML's tree model. This causes problems in all cases where information from these two worlds has to be brought together. Using a few rules for mapping the incompatible aspects of the two models, it becomes easier to process data in systems which need to work with relational and tree data. The most important requirement for a good mapping is that the conceptual model is available and can thus be used for making mapping decisions.	Tables and trees don't mix (very well)	NA	2018
Marek Kowalkiewicz:Maria E. Orlowska:Tomasz Kaczmarek:Witold Abramowicz	We present an empirical evaluation and comparison of two content extraction methods in HTML: absolute XPath expressions and relative XPath expressions. We argue that the relative XPath expressions, although not widely used, should be used in preference to absolute XPath expressions in extracting content from human-created Web documents. Evaluation of robustness covers four thousand queries executed on several hundred webpages. We show that in referencing parts of real world dynamic HTML documents, relative XPath expressions are on average significantly more robust than absolute XPath ones.	Robust web content extraction	NA:NA:NA:NA	2018
Demetrius Arraes Nunes:Daniel Schwabe	There have been several authoring methods proposed in the literature that are model based, essentially following the Model Driven Design philosophy. While useful, such methods need an effective way to allow the application designer to somehow synthesize the actual running application from the specification. In this paper, we describe HyperDE, an environment that combines Model Driven Design and Domain Specific Languages to enable rapid prototyping of Web applications.	Rapid prototyping of web applications combining domain specific languages and model driven design	NA:NA	2018
Jie Liu:Liang Feng:Yunpeng Xing	An important issue arising from large scale data integration is how to efficiently select the top-K ranking answers from multiple sources while minimizing the transmission cost. This paper resolves this issue by proposing an efficient pruning-based approach to answer top-K join queries. The total amount of transmitted data can be greatly reduced by pruning tuples that can not produce the desired join results with a rank value greater than or equal to the rank value generated so far.	A pruning-based approach for supporting Top-K join queries	NA:NA:NA	2018
Martin Nussbaumer:Patrick Freudenstein:Martin Gaedke	Strong user involvement and clear business objectives, both relying on efficient communication between the developers and the business, are key factors for a project's success. Domain-Specific Languages (DSLs) being simple, highly-focused and tailored to a clear problem domain are a promising alternative to heavy-weight modeling approaches in the field of Web Engineering. Thus, they enable stakeholders to validate, modify and even develop parts of a distributed Web-based solution.	Towards DSL-based web engineering	NA:NA:NA	2018
Johannes Meinecke:Martin Gaedke:Frederic Majer:Alexander Brändle	Today, the Web is increasingly used as a platform for distributed services, which transcend organizational boundaries to form federated applications. Consequently, there is a growing interest in the architectural aspect of Web-based systems, i.e. the composition of the overall solution into individual Web applications and Web services from different parties. The design and evolution of federated systems calls for models that give an overview of the structural as well as trust-specific composition and reflect the technical details of the various accesses. We introduce the WebComposition Architecture Model (WAM) as an overall modeling approach tailored to aspects of highly distributed systems with federation as an integral factor.	Capturing the essentials of federated systems	NA:NA:NA:NA	2018
Sven Casteleyn:Zoltán Fiala:Geert-Jan Houben:Kees van der Sluijs	The evolution of the Web requires to consider an increasing number of context-dependency issues. Therefore, in our research we focus on how to extend a Web application with additional adaptation concerns without having to redesign the entire application. Based on a generic transcoding tool we illustrate here how we can add adaptation functionality to an existing Web application. Furthermore, we consider how an aspect-oriented approach can support the high-level specification of such additional concerns in the design of the Web application.	From adaptation engineering to aspect-oriented context-dependency	NA:NA:NA:NA	2018
Stefano Ferretti:Marco Roccetti:Johannes Andrich	The union of Interactive Digital TV (IDTV) and Web promotes the development of new interactive multimedia services, enjoyable while watching TV even on the new handheld digital TV receivers. Yet, several design constraints complicate the deployment of this new pattern of services. Indeed, for a suitable presentation on a TV set, Web contents must be structured in such a way that they can be effectively displayed on TV screens via low-end Set Top Boxes (STBs). Moreover, usable interfaces for IDTV platforms are needed which ensure a smooth access to contents. Our claim is that the distribution of Web contents over the IDTV broadcast channels may bring IDTV to a new life. A failure of this attempt may put IDTV on a progressive track towards irrelevance. We propose a system for the distribution of Web contents towards IDTV under the Digital Video Broadcasting - Multimedia Home Platform (DVB-MHP) standard. Our system is able to automatically transcode Web contents and ensure a proper visualization on IDTV. The system is endowed with a client application which permits to easily browse contents on the TV via a remote control. Real assessments have confirmed the effectiveness for such an automatic online service able to reconfigure Web contents for an appropriate distribution and presentation on IDTV.	Living the TV revolution: unite MHP to the web or face IDTV irrelevance!	NA:NA:NA	2018
Tamir Hassan:Robert Baumgartner	Wrapping is the process of navigating a data source, semi-automatically extracting data and transforming it into a form suitable for data processing applications. There are currently a number of established products on the market for wrapping data from web pages. One such approach is Lixto [1], a product of research performed at our institute.Our work is concerned with extending the wrapping functionality of Lixto to PDF documents. As the PDF format is relatively unstructured, this is a challenging task. We have developed a method to segment the page into blocks, which are represented as nodes in a relational graph. This paper describes our current research in the use of relational matching techniques on this graph to locate wrapping instances.	Using graph matching techniques to wrap data from PDF documents	NA:NA	2018
Ajay Chakravarthy:Vitaveska Lanfranchi:Fabio Ciravegna	Nowadays a large and growing percentage of information is stored in various multimedia formats. In order for multimedia information to be efficiently utilised by users, it is very important to add suitable metadata. In this paper we will present AKTiveMedia, a tool for enriching multimedia documents with semantic information.	Requirements for multimedia document enrichment	NA:NA:NA	2018
Rui Lopes:Luís Carriço:Carlos Duarte	We present DiTaBBu, Digital Talking Books Builder, a framework for automatic production of time-based hypermedia for the Web, focusing on the Digital Talking Books domain. Delivering Digital Talking Books collections to a wide range of users is an expensive task, as it must take into account each user profile's different needs, therefore authoring should be dismissed in favour of automation. With DiTaBBu, we enable automated content delivery in several playback platforms, targeted to specific user needs, featuring powerful navigation capabilities over the content. DiTaBBu can also be used as testbed for prototyping novel capabilities, through its flexible extension mechanisms.	DiTaBBu: automating the production of time-based hypermedia content	NA:NA:NA	2018
Alessandro Bozzon:Sara Comai:Piero Fraternali:Giovanni Toffetti Carughi	This work addresses conceptual modeling and automatic code generation for Rich Internet Applications, a variant of Web-based systems bridging the gap between desktop and Web interfaces. The approach we propose is a first step towards a full integration of RIA paradigms into the Web development process, enabling the specification of complex Web solutions mixing HTTP+HTML and Rich Internet Applications, using a single modeling language and tool.	Capturing RIA concepts in a web modeling language	NA:NA:NA:NA	2018
Hsin Chia Fu:Yeong Y. Xu:C. L. Tseng	In this paper, we present a system we have developed for automatic TV News video indexing that successfully combines results from the fields of speaker verification, acoustic analysis, very large vocabulary video OCR, content based sampling of video, information retrieval, dialogue systems, and ASF media delivery over IP. The prototype of TV news content processing Web was completed in July 2003. Since then, the system has been up running continuously. Up to the date when this message is written (March 27, 2006), the system records and analyzes the prime time evening news program in Taiwan every day of these years, except a few power failure shutdown. The TV news web is at http://140.113.216.64/NewsQuery/main.as	Generation of multimedia TV news contents for WWW	NA:NA:NA	2018
Hisashi Miyamori:Mitsuru Minakuchi:Zoran Stejic:Qiang Ma:Tadashi Araki:Katsumi Tanaka	A search engine that can handle TV programs and Web content in an integrated way is proposed. Conventional search engines have been able to handle Web content and/or data stored in a PC desktop as target information. In the future, however, the target information is expected to be stored in various places such as in hard-disk (HD)/DVD recorders, digital cameras, mobile devices, and even in real space as ubiquitous content, and a search engine that can search across such heterogeneous resources will become essential. Therefore, as a first step towards developing such next-generation search engine, a prototype search system for Web and TV programs is developed that performs integrated search of those content, and that allows chain search where related content can be accessed from each search result. The integrated search is achieved by generating integrated indices for Web and TV content based on vector space model and by computing similarity between the query and all the content described by the indices. The chain search of related content is done by computing similarity between the selected result and all other content based on the integrated indices. Also, the zoom-based display of the search results enables to control media transition and level of details of the contents to acquire information efficiently. In this paper, testing of a prototype of the integrated search engine validated the approach taken by the proposed method.	Proposal of integrated search engine of web and TV contents	NA:NA:NA:NA:NA:NA	2018
Brian Shields:Owen Molloy:Gerard Lyons:Jim Duggan	Semantic Web technologies are bring increasingly employed to solve knowledge management issues in traditional Web technologies. This paper follows that trend and proposes using Semantic rule languages to construct rules for defining access control rules for Web Services. Using these rules, a system will be able to manage access to Web Services and also the information accessed via these services.	Using semantic rules to determine access control for web services	NA:NA:NA:NA	2018
Domenico Rotiroti	In this paper we present a way to integrate web proxies with smart card based authentication systems.	Strong authentication in web proxies	NA	2018
Anthony Y. Fu:Wan Zhang:Xiaotie Deng:Liu Wenyin	A severe potential security problem in utilization of Unicode on the Web is identified, which is resulted from the fact that there are many similar characters in the Universal Character Set (UCS). The foundation of our solution relies on evaluating the similarity of characters in UCS. We develop a solution based on the renowned Kernel Density Estimation (KDE) method to establish such a Unicode Similarity List (UC-SimList).	Safeguard against unicode attacks: generation and applications of UC-simlist	NA:NA:NA:NA	2018
Gennaro Iaccarino:Delfina Malandrino:Marco Del Percio:Vittorio Scarano	NA	Efficient edge-services for colorblind users	NA:NA:NA:NA	2018
Lo Ka Kan:Xiang Peng:Irwin King	In the spread of internet, internet-based information service business has started to become profitable. One of the key technologies is personalization. Successful internet information services must realize personalized information delivery, by which the users can automatically receive highly tuned information according to their personal needs and preferences. In order to realize such personalized information services, we have developed an automatic user preference capture and an automatic information clipping function based on a Personalized Information Access technique. In this paper, those techniques will be demonstrated by showing a deployed personalized webpage service application.	A user profile-based approach for personal information access: shaping your information portfolio	NA:NA:NA	2018
Keiji Yanai:Kobus Barnard	We propose measuring "visualness" of concepts with images on the Web, that is, what extent concepts have visual characteristics. This is a new application of "Web image mining". To know which concept has visually discriminative power is important for image recognition, since not all concepts are related to visual contents. Mining image data on the Web with our method enables it. Our method performs probabilistic region selection for images and computes an entropy measure which represents "visualness" of concepts. In the experiments, we collected about forty thousand images from the Web for 150 concepts. We examined which concepts are suitable for annotation of image contents.	Finding visual concepts by web image mining	NA:NA	2018
Gilad Mishne:Maarten de Rijke	We use a combination of text analysis and external knowledge sources to estimate the commercial taste of bloggers from their text; our methods are evaluated using product wishlists found in the blogs. Initial results are promising, showing that valuable insights can be mined from blogs, not just at the aggregate but also at the individual blog level.	Deriving wishlists from blogs show us your blog, and we'll tell you what books to buy	NA:NA	2018
Ricardo Baeza-Yates:Carlos Castillo	We report on observations on Web characterization studies that suggest that the amount of Web links among sites under different country-code top-level domains is related to the amount of trade between the corresponding countries.	Relationship between web links and trade	NA:NA	2018
Angelo Dalli	Previous work on spatio-temporal analysis of news items and other documents has largely focused on broad categorization of small text collections by region or country. A system for large-scale spatio-temporal analysis of online news media and blogs is presented, together with an analysis of global news media coverage over a nine year period. We demonstrate the benefits of using a hierarchical geospatial database to disambiguate between geographical named entities, and provide results for an extremely fine-grained analysis of news items. Aggregate maps of media attention for particular places around the world are compared with geographical and socio-economic data. Our analysis suggests that GDP per capita is the best indicator for media attention.	System for spatio-temporal analysis of online news and blogs	NA	2018
Michael Maslov:Alexander Golovko:Ilya Segalovich:Pavel Braslavski	In this poster, we present a method for extracting queries related to real-life events, or news-related queries, from large web query logs. The method employs query frequencies and search over a collection of recent news. News-related queries can be helpful for disambiguating user information needs, as well as for effective online news processing. The performed evaluation proves that the method yields good precision.	Extracting news-related queries from web query log	NA:NA:NA:NA	2018
Bernhard Krüpl:Marcus Herzog	In the AllRight project, we are developing an algorithm for unsupervised table detection and segmentation that uses the visual rendition of a Web page rather than the HTML code. Our algorithm works bottom-up by grouping word bounding boxes into larger groups and uses a set of heuristics. It has already been implemented and a preliminary evaluation on about 6000 Web documents has been carried out.	Visually guided bottom-up table detection and segmentation in web documents	NA:NA	2018
Hanene Azzag:David Ratsimba:David Da Costa:Gilles Venturini:Christiane Guinot	The aim of web pages visualization is to present in a very informative and interactive way a set of web documents to the user in order to let him or her navigate through these documents. In the web context, this may correspond to several user's tasks: displaying the results of a search engine, or visualizing a graph of pages such as a hypertext or a surf map. In addition to web pages visualization, web pages clustering also greatly improves the amount of information presented to the user by highlighting the similarities between the documents [6]. In this paper we explore the use of a cellular automata (CA) to generate such maps of web pages.	Generating maps of web pages using cellular automata	NA:NA:NA:NA:NA	2018
Klaus Berberich:Srikanta Bedathur:Michalis Vazirgiannis:Gerhard Weikum	Ranking methods like PageRank assess the importance of Web pages based on the current state of the rapidly evolving Web graph. The dynamics of the resulting importance scores, however, have not been considered yet, although they provide the key to an understanding of the Zeitgeist on the Web. This paper proposes the BuzzRank method that quantifies trends in time series of importance scores and is based on a relevant growth model of importance scores. We experimentally demonstrate the usefulness of BuzzRank on a bibliographic dataset.	BuzzRank … and the trend is your friend	NA:NA:NA:NA	2018
András A. Benczúr:István Bíró:Károly Csalogány:Máté Uher	In this short note we demonstrate the applicability of hyperlink downweighting by means of language model disagreement. The method filters out hyperlinks with no relevance to the target page without the need of white and blacklists or human interaction. We fight various forms of nepotism such as common maintainers, ads, link exchanges or misused affiliate programs. Our method is tested on a 31 M page crawl of the .de domain with a manually classified 1000-page random sample.	Detecting nepotistic links by language model disagreement	NA:NA:NA:NA	2018
Luca Becchetti:Carlos Castillo	We show that the empirical distribution of the PageRank values in a large set of Web pages does not follow a power-law except for some particular choices of the damping factor. We argue that for a graph with an in-degree distribution following a power-law with exponent between 2.1 and 2.2, choosing a damping factor around 0.85 for PageRank yields a power-law distribution of its values. We suggest that power-law distributions of PageRank in Web graphs have been observed because the typical damping factor used in practice is between 0.85 and 0.90.	The distribution of pageRank follows a power-law only for particular values of the damping factor	NA:NA	2018
Xiaodong Shi:Christopher C. Yang	In this work we propose a method that retrieves a list of related queries given an initial input query. The related queries are based on the query log of previously issued queries by human users, which can be discovered using our improved association rule mining model. Users can use the suggested related queries to tune or redirect the search process. Our method not only discovers the related queries, but also ranks them according to the degree of their relatedness. Unlike many other rival techniques, it exploits only limited query log information and performs relatively better on queries in all frequency divisions.	Mining related queries from search engine query logs	NA:NA	2018
Christopher C. Yang:Xiaodong Shi	In this paper, we propose an approach to automatically mine event evolution graphs from newswires on the Web. Event evolution graph is a directed graph in which the vertices and edges denote news events and the evolutions between events respectively, in a news affair. Our model utilizes the content similarity between events and incorporates temporal proximity and document distributional proximity as decaying functions. Our approach is effective in presenting the inside developments of news affairs along the timeline, which can facilitate users' information browsing tasks.	Discovering event evolution graphs from newswires	NA:NA	2018
Jian-Tao Sun:Xuanhui Wang:Dou Shen:Hua-Jun Zeng:Zheng Chen	This paper is to investigate the group behavior patterns of search activities based on Web search history data, i.e., clickthrough data, to boost search performance. We propose a Collaborative Web Search (CWS) framework based on the probabilistic modeling of the co-occurrence relationship among the heterogeneous web objects: users, queries, and Web pages. The CWS framework consists of two steps: (1) a cube-clustering approach is put forward to estimate the semantic cluster structures of the Web objects; (2) Web search activities are conducted by leveraging the probabilistic relations among the estimated cluster structures. Experiments on a real-world clickthrough data set validate the effectiveness of our CWS approach.	Mining clickthrough data for collaborative web search	NA:NA:NA:NA:NA	2018
Blaž Fortuna:Marko Grobelnik:Dunja Mladenič	In this paper we describe a solution for incorporating background knowledge into the OntoGen system for semi-automatic ontology construction. This makes it easier for different users to construct different and more personalized ontologies for the same domain. To achieve this we introduce a word weighting schema to be used in the document representation. The weighting schema is learned based on the background knowledge provided by user. It is than used by OntoGen's machine learning and text mining algorithms.	Background knowledge for ontology construction	NA:NA:NA	2018
Tao Jiang:Ah-Hwee Tan	In this paper, we present a novel frequent generalized pattern mining algorithm, called GP-Close, for mining generalized associations from RDF metadata. To solve the over-generalization problem encountered by existing methods, GP-Close employs the notion of emphgeneralization closure for systematic over-generalization reduction.	Mining RDF metadata for generalized association rules: knowledge discovery in the semantic web era	NA:NA	2018
Gilad Mishne	This paper describes AutoTag, a tool which suggests tags for weblog posts using collaborative filtering methods. An evaluation of AutoTag on a large collection of posts shows good accuracy; coupled with the blogger's final quality control, AutoTag assists both in simplifying the tagging process and in improving its quality.	AutoTag: a collaborative approach to automated tag assignment for weblog posts	NA	2018
Erik Wilde	XML is the predominant format for representing structured information inside documents, but it stops at the level of files. This makes it hard to use XML-oriented tools to process information which is scattered over multiple documents within a file system. File System XML (FSX) and its content integration provides a unified view of file system structure and content. FSX's adaptors map file contents to XML, which means that any file format can be integrated with an XML view in the integrated view of the file system.	Merging trees: file system and content integration	NA	2018
Barbara Poblete:Ricardo Baeza-Yates	We present a novel model for validating and improving the content and structure organization of a website. This model studies the website as a graph and evaluates its interconnectivity in relation to the similarity of its documents. The aim of this model is to provide a simple way for improving the overall structure, contents and interconnectivity of a website. This model has been implemented as a prototype and applied to several websites, showing very interesting results. Our model is complementary to other methods of website personalization and improvement.	A content and structure website mining model	NA:NA	2018
Hua-Fu Li:Man-Kwan Shan:Suh-Yin Lee	In this paper, we proposed an online algorithm, called FQT-Stream (Frequent Query Trees of Streams), to mine the set of all frequent tree patterns over a continuous XML data stream. A new numbering method is proposed to represent the tree structure of a XML query tree. An effective sub-tree numeration approach is developed to extract the essential information from the XML data stream. The extracted information is stored in an effective summary data structure. Frequent query trees are mined from the current summary data structure by a depth-first-search manner.	Online mining of frequent query trees over XML data streams	NA:NA:NA	2018
Xiaojun Wan:Jianwu Yang	This paper proposes a novel approach to measuring XML document similarity by taking into account the semantics between XML elements. The motivation of the proposed approach is to overcome the problems of "under-contributionö and "over-contributionö existing in previous work. The element semantics are learned in an unsupervised way and the Proportional Transportation Similarity is proposed to evaluate XML document similarity by modeling the similarity calculation as a transportation problem. Experiments of clustering are performed on three ACM SIGMOD data sets and results show the favorable performance of the proposed approach.	Using proportional transportation similarity with learned element semantics for XML document clustering	NA:NA	2018
Rahman AliMohammadzadeh:Sadegh Soltan:Masoud Rahgozar	Compared with traditional association rule mining in the structured world (e.g. Relational Databases), mining from XML data is confronted with more challenges due to the inherent flexibilities of XML in both structure and semantics. The major challenges include 1) a more complicated hierarchical data structure; 2) an ordered data context; and 3) a much bigger size for each data element. In order to make XML-enabled association rule mining truly practical and computationally tractable, we propose a practical model for mining association rules from XML documents and demonstrate the usability and effectiveness of model through a set of experiments on real-life data.	Template guided association rule mining from XML documents	NA:NA:NA	2018
Alexei Pyalling:Michael Maslov:Pavel Braslavski	The poster describes a fast, simple, yet accurate method to associate large amounts of web resources stored in a search engine database with geographic locations. The method uses location-by-IP data, domain names, and content-related features: ZIP and area codes. The novelty of the approach lies in building location-by-IP database by using continuous IP blocks method. Another contribution is domain name analysis. The method uses search engine infrastructure and makes it possible to effectively associate large amounts of search engine data with geography on a regular basis. Experiments ran on Yandex search engine index; evaluation has proved the efficacy of the approach.	Automatic geotagging of Russian web sites	NA:NA:NA	2018
Esteban Meneses:Oldemar Rodríguez-Rojas	Web Clustering is useful for several activities in the WWW, from automatically building web directories to improve retrieval performance. Nevertheless, due to the huge size of the web, a linear mechanism must be employed to cluster web documents. The k-means is one classic algorithm used in this problem. We present a variant of the vector model to be used with the k-means algorithm. Our representation uses symbolic objects for clustering web documents. Some experiments were done with positive results and future work is optimistic.	Using symbolic objects to cluster web documents	NA:NA	2018
Wolfgang Gatterbauer	Information on the Web is not only abundant but also redundant. This redundancy of information has an important consequence on the relation between the recall of an information gathering system and its capacity to harvest the core information of a certain domain of knowledge. This paper provides a new idea for estimating the necessary Web coverage of a knowledge acquisition system in order to achieve a certain desired coverage of the contained core information.	Estimating required recall for successful knowledge acquisition from the web	NA	2018
Narichika Hamaguchi:Mamoru Doke:Masaki Hayashi:Nobuyuki Yagi	A video blogging system has been developed for easily producing your own video programs that can be made available to the public in much the same way that blogs are created. The user merely types a program script on a webpage, the same as creating a blog, selects a direction style, and pastes in some additional material content to create a CG-based video program that can be openly distributed to the general public. The script, direction style, and material content are automatically combined to create a movie file on the server side. The movie file can then be accessed by referring to an RSS feed and viewed on the screens of various devices.	Text-based video blogging	NA:NA:NA:NA	2018
Byeong Man Kim:Qing Li:Adele E. Howe	In this paper, we propose a decentralized collaborative filtering (CF) approach based on P2P overlay network for the autonomous agents' environment. Experiments show that our approach is more scalable than traditional centralized CF filtering systems and alleviates the sparsity problem in distributed CF.	A decentralized CF approach based on cooperative agents	NA:NA:NA	2018
Doug Warner:Stephen D. Durbin:J. Neal Richter:Zuzana Gedeon	Adaptive web sites have been proposed to enhance ease of navigation and information retrieval. A variety of approaches are described in the literature, but consideration of interface presentation issues and realistic user studies are generally lacking. We report here a large-scale study of sites with dynamic information collections and user interests, where adaptation is based on an Ant Colony Optimization technique. We find that most users were able to locate information effectively without needing to perform explicit searches. The behavior of users who did search was similar to that on Internet search engines. Simulations based on site and user models give insight into the adaptive behavior and correspond to observations.	Adaptive web sites: user studies and simulation	NA:NA:NA:NA	2018
Sylvia C. Wong:Richard M. Crowder:Gary B. Wills	Increasingly, manufacturing companies are shifting their focus from selling products to providing services. As a result, when designing new products, engineers must increasingly consider the life cycle costs in addition to any design requirements. To identify possible areas of concern, designers are required to consult existing maintenance information from identical products. However, in a large engineering company, the amount of information available is significant and in wide range of formats. This paper presents a prototype knowledge desktop suitable for the design engineer. The Engineering Knowledge Desktop analyses and suggests relevant information from ontologically marked-up heterogeneous web resources. It is designed using a Service-Oriented Architecture, with an ontology to mediate between Web Services. It has been delivered to the user community for evaluation.	On a service-oriented approach for an engineering knowledge desktop	NA:NA:NA	2018
Sidek H. A. Aziz:Aida Suraya:M. Yunus:Kamariah A. Bakar:Hamidah B. Meseran	This paper reports the design and development of the e-SPRINT, Learning Management System, which has been derived from Sistem Pengurusan Rangkaian Integrasi Notakuliah dalam Talian - mod Elektronik) and currently being implemented at Universiti Putra Malaysia (UPM). The e-SPRINT was developed by utilizing PERL (Practical Extraction and Report Language) and was supported by standard database in Linux/UNIX environment operating system. The system is currently being used to supplement and complement part of the classroom-based teaching. This paper covers the architecture and features of the e-SPRINT system which consists of five main modules. Some general issues and challenges of such e-learning initiatives implementation will also be discussed.	Design and development of learning management system at universiti Putra Malaysia: a case study of e-SPRINT	NA:NA:NA:NA:NA	2018
M. Rey-López:A. Fernández-Vilas:R. Díaz-Redondo:J. Pazos-Arias	Content personalization is a very important aspect in the field of e-learning, although current standards do not fully support it. In this paper, we outline an extension to the ADL SCORM (Sharable Content Object Reference Model) standard in an effort to permit a suitable adaptivity based on user's characteristics. Applying this extension, we can create adaptable courses, which should be personalized before shown to the student.	Providing SCORM with adaptivity	NA:NA:NA:NA	2018
Alessandro Campi:Paola Spoletini	The need of formal verification is a problem that involves all the fields in which sensible data are managed. In this context the verification of data streams became a fundamental task. The purpose of this paper is to present a framework, based on the model checker SPIN, for the verification of data streams.The proposed method uses a linear temporal logic, called TRIO, to describe data constraints and properties. Constraints are automatically translated into Promela, the input language of the model checker SPIN in order to verify them.	A framework for XML data streams history checking and monitoring	NA:NA	2018
Koji Yamamoto:Daisuke Katagami:Katsumi Nitta:Akira Aiba:Hitoshi Kuwata	We propose a method for estimating the credibility of the posted information from users. The system displays these information on the map. Since posted information can include subjective information from various perspectives, we can't trust all of the postings as they are. We propose and integrate factors of the user's geographic posting tendency and votes by other users.	The credibility of the posted information in a recommendation system based on a map	NA:NA:NA:NA:NA	2018
Maureen Pennock:Brian Kelly	In this paper, we propose the use of records management principles to identify and manage Web site resources with enduring value as records. Current Web archiving activities, collaborative or organisational, whilst extremely valuable in their own right, often do not and cannot incorporate requirements for proper records management. Material collected under such initiatives therefore may not be reliable or authentic from a legal or archival perspective, with insufficient metadata collected about the object during its active life, and valuable materials destroyed whilst ephemeral items are maintained. Education, training, and collaboration between stakeholders are integral to avoiding these risks and successfully preserving valuable Web-based materials.	Archiving web site resources: a records management view	NA:NA	2018
Katsuko T. Nakahira:Tetsuya Hoshino:Yoshiki Mikami	The ccTLD (country code Top Level Domain) in a URL does not necessarily point to the geographic location of the server concerned. The authors have surveyed sample servers belonging to 60 ccTLDs in Africa, with regard to the number of hops required to reach the target site from Japan, the response time, and the NIC registration information of each domain. The survey has revealed the geographical distribution of server sites as well as their connection environments. It has been found that the percentage of offshore (out of home country) servers is as high as 80% and more than half of these are located in Europe. Offshore servers not only provide little benefit to the people of the country to which each ccTLD rightly belongs but their existence also heightens the risk of a country being unable to control them with its own policies and regulations. Offshore servers constitute a significant aspect of the digital divide problem.	Geographic locations of web servers	NA:NA:NA	2018
Rahul Tongia	I present analysis examining some of the causes of poor connectivity in developing countries. Based on a techno-economic analysis and design, I show that technical limitations per se are not the bottleneck for widespread connectivity; rather, design, policy, and regulatory challenges dominate.	Why is connectivity in developing regions expensive: policy challenges more than technical limitations?	NA	2018
Tak Pang Lau:Irwin King	Readability assessment is a method to measure the difficulty of a piece of text material, and it is widely used in educational field to assist instructors to prepare appropriate materials for students. In this paper, we investigate the applications of readability assessment in Web development, such that users can retrieve information which is appropriate to their levels. We propose a bilingual (English and Chinese)assessment scheme for Web page and Web site readability based on textual features, and conduct a series of experiments with real Web data to evaluate our scheme. Experimental results show that, apart from just indicating the readability level, the estimated score acts as a good heuristic to figure out pages with low textual content. Furthermore, we can obtain the overall content distribution in a Web site by studying the variation of its readability.	Bilingual web page and site readability assessment	NA:NA	2018
Daisuke Morikawa:Masaru Honjo:Satoshi Nishiyama:Masayoshi Ohashi	NA	Mobile web publishing and surfing based on environmental sensing data	NA:NA:NA:NA	2018
Malcolm Attard:Matthew Montebello	In the very near future complete households will be entirely networked as a de facto standard. In this poster we briefly describe our work in the area of domotics, where personalization, semantics and agent technology come together. We illustrate a home system oriented ontology and an intelligent agent based framework for the rapid development of home control and automation. The ever changing nature of the home, places the user in a position were he needs to be involved and become, through DoNet, a part of an ongoing home system optimization process.	DoNet: a semantic domotic framework	NA:NA	2018
John Garofalakis:Theofanis - Aristofanis Michail:Athanasios Plessas	A map is one of the most useful media in disseminating spatial information. As mobile devices are becoming increasingly powerful and ubiquitous, new possibilities to access map information are created. However, mobile devices still face severe constraints that limit the possibilities that a mobile map application may offer. We present the m-CHARTIS system, a device independent mobile map application that enables mobile users to access map information from their device.	Web based device independent mobile map applications.: the m-CHARTIS system	NA:NA:NA	2018
David Webster:Weihong Huang:Darren Mundy:Paul Warren	How can we solve the problem of information overload in news syndication? This poster outlines the path from keyword-based body text matching to distance-measurable taxonomic tag matching, on to context scale and practical uses.	Context-orientated news riltering for web 2.0 and beyond	NA:NA:NA:NA	2018
Hai Jin:Xiaomin Ning:Hanhua Chen	This paper proposes a semantic overlay based on the small world phenomenon that facilitates efficient search for information retrieval in unstructured P2P systems. In the semantic overlay, each node maintains a number of short-range links which are semantically similar to each other, together with a small collection of long-range links that help increasing recall rate of information retrieval and reduce network traffic as well. Experimental results show that our model can improve performance by 150% compared to Gnutella and by up to 60% compared to the Interest-based model - a similar shortcut-based search technique.	Efficient search for peer-to-peer information retrieval using semantic small world	NA:NA:NA	2018
Jie Liu:Liang Feng:Chao He	An important issue arising from Peer-to-Peer applications is how to accurately and efficiently retrieve a set of K best matching data objects from different sources while minimizing the number of objects that have to be accessed. This paper resolves this issue by organizing peers in a Semantic Link Network Overlay, where semantic links are established to denote the semantic relationship between peers' data schemas. A query request will be routed to appropriate peers according to the semantic link type and a lower bound of rank function. Optimization strategies are proposed to reduce the total amount of data transmitted.	Semantic link based top-K join queries in P2P networks	NA:NA:NA	2018
Asunción Gómez-Pérez:Fernando Ortiz-Rodriguez:Boris Villazón-Terrazas	In this paper, we present EgoIR, an approach for retrieving legal information based on ontologies; this approach has been developed with Legal Ontologies to be deployed within the e-government context.	Ontology-based legal information retrieval to improve the information access in e-government	NA:NA:NA	2018
Raul Palma:Peter Haase:Asunción Gómez-Pérez	In this paper, we present Oyster, a Peer-to-Peer system for exchanging ontology metadata among communities in the Semantic Web. Oyster exploits semantic web techniques in data representation, query formulation and query result presentation to provide an online solution for sharing ontologies, thus assisting researchers in re-using existing ontologies.	Oyster: sharing and re-using ontologies in a peer-to-peer community	NA:NA:NA	2018
Márcio L. A. Vidal:Altigran S. da Silva:Edleno S. de Moura:João M. B. Cavalcanti	We present GoGetIt!, a tool for generating structure-driven crawlers that requires a minimum effort from the users. The tool takes as input a sample page and an entry point to a Web site and generates a structure-driven crawler based on navigation patterns, sequences of patterns for the links a crawler has to follow to reach the pages structurally similar to the sample page. In the experiments we have performed, structure-driven crawlers generated by GoGetIt! were able to collect all pages that match the samples given, including those pages added after their generation.	GoGetIt!: a tool for generating structure-driven web crawlers	NA:NA:NA:NA	2018
George Ferizis:Peter Bailey	Classification of documents by genre is typically done either using linguistic analysis or term frequency based techniques. The former provides better classification accuracy than the latter but at the cost of two orders of magnitude more computation time. While term frequency analysis requires much less computational resources than linguistic analysis,it returns poor classification accuracy when the genres are not sufficiently distinct. A method that removes or approximates the expensive portions of linguistic analysis is presented.The accuracy and computation time of this method then compared with both linguistic analysis and term frequency analysis. The results in this paper show that this method can significantly reduce the computation of both time of linguistic analysis and term frequency analysis, while retaining an accuracy that is higher than that of term frequency analysis.	Towards practical genre classification of web documents	NA:NA	2018
Uri Schonfeld:Ziv Bar-Yossef:Idit Keidar	We consider the problem of dust: Different URLs with Similar Text. Such duplicate URLs are prevalent in web sites, as web server software often uses aliases and redirections, translates URLs to some canonical form, and dynamically generates the same page from various different URL requests. We present a novel algorithm, DustBuster, for uncovering dust; that is, for discovering rules for transforming a given URL to others that are likely to have similar content. DustBuster is able to detect dust effectively from previous crawl logs or web server logs, without examining page contents. Verifying these rules via sampling requires fetching few actual web pages. Search engines can benefit from this information to increase the effectiveness of crawling, reduce indexing overhead as well as improve the quality of popularity statistics such as PageRank.	Do not crawl in the DUST: different URLs with similar text	NA:NA:NA	2018
Ying Zhou:Joseph Davis	Weblog has quickly evolved into a new information and knowledge dissemination channel. Yet it is not easy to discover weblog communities through keyword search. The main contribution of this paper is the study of weblog communities from the perspective of social network analysis. We proposed a new way of collecting and preparing data for weblog community discovery. The data collection stage focuses on gaining knowledge of the strength of social ties between weblogs. The strength of social ties and the clustering feature of social network guided the discovery of weblog communities.	Community discovery and analysis in blogspace	NA:NA	2018
Zhenjiang Lin:Michael R. Lyu:Irwin King	To find similar web pages to a query page on the Web, this paper introduces a novel link-based similarity measure, called PageSim. Contrast to SimRank, a recursive refinement of cocitation, PageSim can measure similarity between any two web pages, whereas SimRank cannot in some cases. We give some intuitions to the PageSim model, and outline the model with mathematical definitions. Finally, we give an example to illustrate its effectiveness.	PageSim: a novel link-based measure of web page aimilarity	NA:NA:NA	2018
Naoki Yoshinag:Kentaro Torisaw	This paper presents a method for finding a specification page on the web for a given object (e.g."Titanic ö)and its class label (e.g."film ö). A specification page for an object is a web page which gives concise attribute-value information about the object (e.g."director ö-"James Cameron öfor "Titanic ö). A simple unsupervised method using layout and symbolic decoration cues was applied to a large number of web pages to acquire the class attributes. We used these acquired attributes to select a representative specification page for a given object from the web pages retrieved by a normal search engine. Experimental results revealed that our method greatly outperformed the normal search engine in terms of specification retrieval.	Finding specification pages according to attributes	NA:NA	2018
Amit C. Awekar:Pabitra Mitra:Jaewoo Kang	We address the problem of answering broad-topic queries on the World Wide Web. We present a link based analysis algorithm SelHITS, which is an improvement over Kleinberg's HITS [2] algorithm. We introduce the concept of virtual links to exploit the latent information in the hyperlinked environment. We propose a novel approach to calculate hub and authority values. We also present a selective expansion method which avoids topic drift and provides results consistent with only one interpretation of the query, even if the query is ambiguous. Initial experimental evaluation and user feedback show that our algorithm indeed distills the most important and relevant pages for broad-topic queries. We also infer that there exists a uniform notion of quality of search results within users.	Selective hypertext induced topic search	NA:NA:NA	2018
Marco Furini:Marco Aragone	The high availability of video streams is making necessary mechanisms for indexing such contents in the Web world. In this paper we focus on news programs and we propose a mechanism that integrates low and high level video features to provide a high level semantic description. A color/luminance analysis is coupled with audio analysis to provide a better identification of all the video segments that compose the video stream. Each video segment is subject to speech detection and is described through MPEG7 so that the resulting metadata description can be used to index the video stream. An experimental evaluation shows the benefits of integrating audio and video analysis.	An audio/video analysis mechanism for web indexing	NA:NA	2018
Christos Doulkeridis:Kjetil Nørvåg:Michalis Vazirgiannis	Peer-to-peer (P2P) Web search has gained a lot of interest lately, due to the salient characteristics of P2P systems, namely scalability, fault-tolerance and load-balancing. However, the lack of global knowledge in a vast and dynamically evolving environment like the Web presents a grand challenge for organizing content and providing efficient searching. Semantic overlay networks (SONs) have been proposed as an approach to reduce cost and increase quality of results, and in this paper we present an unsupervised approach for distributed and decentralized SON construction, aiming to support efficient search mechanisms in unstructured P2P systems.	The SOWES approach to P2P web search using semantic overlays	NA:NA:NA	2018
Shao-Chi Wang:Yuzuru Tanaka	The contribution of this paper includes three folders: (1) To introduce a topic-oriented query expansion model based on the Information Bottleneck theory that classify terms into distinct topical clusters in order to find out candidate terms for the query expansion. (2) To define a term-term similarity matrix that is available to improve the term ambiguous problem. (3) To propose two measures, intracluster and intercluster similarities, that are based on proximity between the topics represented by two clusters in order to evaluate the retrieval effectiveness. Results of several evaluation experiments in Web search exhibit the average intracluster similarity was improved for the gain of 79.1% while the average intercluster similarity was decreased for the loss of 36.0%.	Topic-oriented query expansion for web search	NA:NA	2018
Maeve O'Brien:Mark T. Keane:Barry Smyth	Search engine results are usually presented in some form of text summary (e.g., document title, some snippets of the page's content, a URL, etc). Based on the information contained within these summaries users make relevance judgments about what links best suit their information needs. Current research suggests that these relevance judgments are in the service of some search strategy. In this paper, we model two different search strategies (the comparison and threshold strategies) and determine how well they fit data gathered from an experiment on user search within a simulated Google environment.	Predictive modeling of first-click behavior in web-search	NA:NA:NA	2018
Srisupa Palakvangsa-Na-Ayudhya:John A. Keane	We created a proximity measure, called Proximity Within Paragraph (PWP), which is based on the concept of value assignment to queried words, grouped by associated ideas within paragraphs. Based on the WT10G dataset, a test system comprising three test sets and fifty queries were constructed to evaluate the effectiveness of PWP by comparing it with the existing method: Minimum Distance Between Queried Pairs. A further experiment combines the scores obtained from both methods and the results suggest that the combination can significantly improve the effectiveness.	Proximity within paragraph: a measure to enhance document retrieval performance	NA:NA	2018
Krisztian Balog:Maarten de Rijke	We present methods for finding experts (and their contact details) using e-mail messages. We locate messages on a topic, and then find the associated experts. Our approach is unsupervised: both the list of potential experts and their personal details are obtained automatically from e-mail message headers and signatures, respectively. Evaluation is done using the e-mail lists in the W3C corpus.	Finding experts and their eetails in e-mail corpora	NA:NA	2018
Utku Irmak:Svilen Mihaylov:Torsten Suel:Samrat Ganguly:Rauf Izmailov	Current web search engines are retrospective in that they limit users to searches against already existing pages. Prospective search engines, on the other hand, allow users to upload queries that will be applied to newly discovered pages in the future. We study and compare algorithms for efficiently matching large numbers of simple keyword queries against a stream of newly discovered pages.	Efficient query subscription processing for prospective search engines	NA:NA:NA:NA:NA	2018
Zhiyong Zhang:Olfa Nasraoui	This paper presents a simple and intuitive method for mining search engine query logs to get fast query recommendations on a large scale industrial strength search engine. In order to get a more comprehensive solution, we combine two methods together. On the one hand, we study and model search engine users' sequential search behavior, and interpret this consecutive search behavior as client-side query refinement, that should form the basis for the search engine's own query refinement process. On the other hand, we combine this method with a traditional content based similarity method to compensate for the high sparsity of real query log data, and more specifically, the shortness of most query sessions. To evaluate our method, we use one hundred day worth query logs from SINA' search engine to do off-line mining. Then we analyze three independent editors evaluations on a query test set. Based on their judgement, our method was found to be effective for finding related queries, despite its simplicity. In addition to the subjective editors' rating, we also perform tests based on actual anonymous user search sessions.	Mining search engine query logs for query recommendation	NA:NA	2018
Iván Gonzlez:Adam Marcus:Daniel N. Meredith:Linda A. Nguyen	The web crawler space is often delimited into two general areas: full-web crawling and focused crawling. We present netSifter, a crawler system which integrates features from these two areas to provide an effective mechanism for web-scale crawling. netSifter utilizes a combination of page-level analytics and heuristics which are applied to a sample of web pages from a given website. These algorithms score individual web pages to determine the general utility of the overall website. In doing so, netSifter can formulate an in-depth opinion of a website (and the entirety of its web pages) with a relative minimum of work. netSifter is then able to bias the future efforts of its crawl towards higher quality websites, and away from the myriad of low quality websites and crawler traps that litter the World Wide Web.	Effective web-scale crawling through website analysis	NA:NA:NA:NA	2018
Antonio Badia:Tulay Muezzinoglu:Olfa Nasraoui	NA	Focused crawling: experiences in a real world project	NA:NA:NA	2018
Xin-Jing Wang:Lei Zhang:Feng Jing:Wei-Ying Ma	In this paper, we present a novel solution to the image annotation problem which annotates images using search and data mining technologies. An accurate keyword is required to initialize this process, and then leveraging a large-scale image database, it 1) searches for semantically and visually similar images, 2) and mines annotations from them. A notable advantage of this approach is that it enables unlimited vocabulary, while it is not possible for all existing approaches. Experimental results on real web images show the effectiveness and efficiency of the proposed algorithm.	Image annotation using search and mining technologies	NA:NA:NA:NA	2018
P. Sinclair:P. Lewis:K. Martinez:M. Addis:D. Prideaux	In this paper, we describe research into the use of ontologies to integrate access to cultural heritage and photographic archives. The use of the CIDOC CRM and CRM Core ontologies are described together with the metadata mapping methodology. A system integrating data from four content providers will be demonstrated.	Semantic web integration of cultural heritage sources	NA:NA:NA:NA:NA	2018
Oscar Corcho:Angel López-Cima:Asunción Gómez-Pérez	We describe the architecture of the ODESeW 2.0 Semantic Web application development platform, which has been used to generate the internal and external Web sites of several R&D projects.	The ODESeW 2.0 semantic web application framework	NA:NA:NA	2018
Bruce G. Robertson	This poster presents ongoing efforts to enrich the RDF-based semantic Web with the tools of the Historical Event Markup and Linking Project (Heml). An experimental RDF vocabulary for Heml data is illustrated, as well as its use in storing and querying encoded historical events. Finally, the practical use of Heml-RDF is illustrated with a toolkit for the Piggy Bank semantic browser plugin.	Visualizing an historical semantic web with Heml	NA	2018
Benedikt Linse:Andreas Schroeder	Applications and services that access Web data are becoming increasingly more useful and wide-spread. Current main-stream Web query languages such as XQuery, XSLT, or SPARQL, however, focus only on one of the different data formats available on the Web. In contrast, Xcerpt is a emphversatile semi-structured query language, i.e., a query language able to access all kinds of Web data such as XML and RDF in the same language reusing common concepts and language constructs. To integrate heterogeneous data and as a foundation for Semantic Web reasoning, Xcerpt also provides rules. Xcerpt has a visual companion language, visXcerpt, that is conceived as a mere rendering of the (textual) query language Xcerpt using a slightly extended CSS. Both languages are demonstrated along a realistic use case integrating XML and RDF data highlighting interesting and unique features. Novel language constructs and optimization techniques are currently under investigation in the Xcerpt project ([email protected] urlhttp://xcerpt.org/).	Beyond XML and RDF: the versatile web query language xcerpt	NA:NA	2018
Armin Haller:Eyal Oren:Paavo Kotinurmi	In this paper we introduce our multi metamodel process ontology (m3po), which is based on various existing reference models and languages from the workflow and choreography domain. This ontology allows the extraction of arbitrary choreography interface descriptions from arbitrary internal workflow models. We also report on an initial validation: we translate an IBM Websphere MQ Workflow model into the m3po ontology and then extract an Abstract BPEL model from the ontology.	An ontology for internal and external business processes	NA:NA:NA	2018
Sudhir Agarwal:Anupriya Ankolekar	NA	Automatic matchmaking of web services	NA:NA	2018
Paavo Kotinurmi:Tomas Vitvar	The use of Semantic Web Service (SWS) technologies have been suggested to enable more dynamic B2B integration of heterogeneous systems and partners. We present how we add semantics to RosettaNet specifications to enable the WSMX SWS environment to automate mediation of messages. The benefits of applying SWS technologies include flexibility in accepting heterogeneity in B2B integrations.	Adding semantics to rosettaNet specifications	NA:NA	2018
Tomoyuki Nanno:Manabu Okumura	We present a system to automatically generate RSS feeds from HTML documents that consist of time-series items with date expressions, e.g., archives of weblogs, BBSs, chats, mailing lists, site update descriptions, and event announcements. Our system extracts date expressions, performs structure analysis of a HTML document, and detects or generates titles from the document.	HTML2RSS: automatic generation of RSS feed based on structure analysis of HTML document	NA:NA	2018
Zhang Kuo:Wu Gang:Li JuanZi	Addressed in this paper is the issue of semantic relationship extraction from semi-structured documents. Many research efforts have been made so far on the semantic information extraction. However, much of the previous work focuses on detecting `isolated' semantic information by making use of linguistic analysis or linkage information in web pages and limited research has been done on extracting semantic relationship from the semi-structured documents. In this paper, we propose a method for semantic relationship extraction by using the logical information in the semi-structured document (semi-structured document usually has various types of structure information, e.g. a semi-structured document may be hierarchical laid out). To the best of our knowledge, extracting semantic relationships by using logical information has not been investigated previously. A probabilistic approach has been proposed in the paper. Features used in the probabilistic model have been defined.	Logical structure based semantic relationship extraction from semi-structured documents	NA:NA:NA	2018
Jeff Z. Pan:Ian Horrocks	This paper proposes OWL FA, a decidable extension of OWL DL with the metamodeling architecture of RDFS(FA). It shows that the knowledge base satisfiability problem of OWL FA can be reduced to that of OWL DL, and compares the FA semantics with the recently proposed contextual semantics and Hilog semantics for OWL.	OWL FA: a metamodeling extension of OWL D	NA:NA	2018
Xing Jiang:Ah-Hwee Tan	Domain ontology has been used in many Semantic Web applications. However, few applications explore the use of ontology for personalized services. This paper proposes an ontology based user model consisting of both concepts and semantic relations to represent users' interests. Specifically, we adopt a statistical approach to learning a semantic-based user ontology model from domain ontology and a spreading activation procedure for inferencing in the user ontology model. We apply the methods of learning and exploiting user ontology to a semantic search engine for finding academic publications. Our experimental results support the efficacy of user ontology and spreading activation theory (SAT) for providing personalized semantic services.	Learning and inferencing in user ontology for personalized semantic web services	NA:NA	2018
Jesús Barrasa Rodriguez:Asunción Gómez-Pérez	In this poster, we describe a framework composed of the R2O mapping language and the ODEMapster processor to upgrade relational legacy data to the Semantic Web. The framework is based on the declarative description of mappings between relational and ontology elements and the exploitation of such mapping descriptions by a generic processor capable of performing both massive and query driven data upgrade.	Upgrading relational legacy data to the semantic web	NA:NA	2018
Eyal Oren:John G. Breslin:Stefan Decker	Wikis are popular collaborative hypertext authoring environments, but they neither support structured access nor information reuse. Adding semantic annotations helps to address these limitations. We present an architecture for Semantic Wikis and discuss design decisions including structured access, views, and annotation language. We present our prototype SemperWiki that implements this architecture.	How semantics make better wikis	NA:NA:NA	2018
Cynthia Sims Parr:Andriy Parafiynyk:Joel Sachs:Li Ding:Sandor Dornbush:Tim Finin:David Wang:Allan Hollander	We describe ELVIS (the Ecosystem Location Visualization and Information System), a suite of tools for constructing food webs for a given location. We express both ELVIS input and output data in OWL, thereby enabling its integration with other semantic web resources. In particular, we describe using a Triple Shop application to answer SPARQL queries from a collection of semantic web documents. This is an end-to-end case study of the semantic web's utility for ecological and environmental research.	Integrating ecoinformatics resources on the semantic web	NA:NA:NA:NA:NA:NA:NA:NA	2018
Tomoyuki Nanno:Manabu Okumura	We present a system to automatically generate RSS feeds from HTML documents that consist of time-series items with date expressions, e.g., archives of weblogs, BBSs, chats, mailing lists, site update descriptions, and event announcements. Our system extracts date expressions, performs structure analysis of a HTML document, and detects or generates titles from the document.	HTML2RSS: automatic generation of RSS feed based on structure analysis of HTML document	NA:NA	2018
Andrei Arion:Angela Bonifati:Ioana Manolescu:Andrea Pugliese	NA	Path summaries and path partitioning in modern XML databases	NA:NA:NA:NA	2018
Mirella M. Moro:Zografoula Vagena:Vassilis J. Tsotras	Structural summaries are data structures that preserve all structural features of XML documents in a compact form. We investigate the applicability of the most popular summaries as textitaccess methods within XML query processing. In this context, issues like space and false positives introduced by the summaries need to be examined. Our evaluation reveals that the additional space required by the more precise structures is usually small and justified by the considerable performance gains that they achieve.	Evaluating structural summaries as access methods for XML	NA:NA:NA	2018
Jie Han:Dingyi Han:Chenxi Lin:Hua-Jun Zeng:Zheng Chen:Yong Yu	The emergence of personalized homepage services, e.g. personalized Google Homepage and Microsoft Windows Live, has enabled Web users to select Web contents of interest and to aggregate them in a single Web page. The web contents are often predefined content blocks provided by the service providers. However, it involves intensive manual efforts to define the content blocks and maintain the information in it. In this paper, we propose a novel personalized homepage system, called .Homepage Live., to allow end users to use drag-and-drop actions to collect their favorite Web content blocks from existing Web pages and organize them in a single page. Moreover, Homepage Live automatically traces the changes of blocks with the evolvement of the container pages by measuring the tree edit distance of the selected blocks. By exploiting the immutable elements of Web pages, the tracing algorithm performance is significantly improved. The experimental results demonstrate the effectiveness and efficiency of our algorithm.	Homepage live: automatic block tracing for web personalization	NA:NA:NA:NA:NA:NA	2018
Jae-wook Ahn:Peter Brusilovsky:Jonathan Grady:Daqing He:Sue Yeon Syn	Over the last five years, a range of projects have focused on progressively more elaborated techniques for adaptive news delivery. However, the adaptation process in these systems has become more complicated and thus less transparent to the users. In this paper, we concentrate on the application of open user models in adding transparency and controllability to adaptive news systems. We present a personalized news system, YourNews, which allows users to view and edit their interest profiles, and report a user study on the system. Our results confirm that users prefer transparency and control in their systems, and generate more trust to such systems. However, similar to previous studies, our study demonstrate that this ability to edit user profiles may also harm the system.s performance and has to be used with caution.	Open user profiles for adaptive news systems: help or harm?	NA:NA:NA:NA:NA	2018
Ryen W. White:Steven M. Drucker	Understanding the extent to which people's search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help World Wide Web users search more effectively. In this paper we describe a longitudinal log-based study that investigated variability in people.s interaction behavior when engaged in search-related activities on the Web.allWe analyze the search interactions of more than two thousand volunteer users over a five-month period, with the aim of characterizing differences in their interaction styles.allThe findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they submit.allOur findings also suggest two classes of extreme user. navigators and explorers. whose search interaction is highly consistent or highly variable. Lessons learned from these users can inform the design of tools to support effective Web-search interactions for everyone.	Investigating behavioral variability in web search	NA:NA	2018
Jalal U. Mahmud:Yevgen Borodin:I. V. Ramakrishnan	Web sites are designed for graphical mode of interaction. Sighted users can "cut to the chase" and quickly identify relevant information in Web pages. On the contrary, individuals with visual disabilities have to use screen-readers tobrowse the Web. As screen-readers process pages sequentially and read through everything, Web browsing can become strenuous and time-consuming. Although, the use ofshortcuts and searching offers some improvements, the problem still remains. In this paper, we address the problemof information overload in non-visual Web access using thenotion of context. Our prototype system, CSurf, embodyingour approach, provides the usual features of a screen-reader.However, when a user follows a link, CSurf captures thecontext of the link using a simple topic-boundary detectiontechnique, and uses it to identify relevant information onthe next page with the help of a Support Vector Machine, astatistical machine-learning model. Then, CSurf reads the Web page starting from the most relevant section, identifiedby the model. We conducted a series experiments to evaluate the performance of CSurf against the state-of-the-artscreen-reader, JAWS. Our results show that the use of context can potentially save browsing time and substantiallyimprove browsing experience of visually disabled people.	Csurf: a context-driven non-visual web-browser	NA:NA:NA	2018
Yih-Farn Robin Chen:Giuseppe Di Fabbrizio:David Gibbon:Rittwik Jana:Serban Jora:Bernard Renger:Bin Wei	The Web is rapidly moving towards a platform for mass collaboration in content production and consumption. Fresh content on a variety of topics, people, and places is being created and made available on the Web at breathtaking speed. Navigating the content effectively not only requires techniques such as aggregating various RSS-enabled feeds, but it also demands a new browsing paradigm. In this paper, we present novel geospatial and temporal browsing techniques that provide users with the capability of aggregating and navigating RSS-enabled content in a timely, personalized and automatic manner. In particular, we describe a system called GeoTracker that utilizes both a geospatial representation and a temporal (chronological) presentation to help users spot the most relevant updates quickly. Within the context of this work, we provide a middleware engine that supports intelligent aggregation and dissemination of RSS feeds with personalization to desktops and mobile devices. We study the navigation capabilities of this system on two kinds of data sets, namely, 2006 World Cup soccer data collected over two months and breaking news items that occur every day. We also demonstrate that the application of such technologies to the video search results returned by YouTube and Google greatly enhances a user.s ability in locating and browsing videos based on his or her geographical interests. Finally, we demonstrate that the location inference performance of GeoTracker compares well against machine learning techniques used in the natural language processing/information retrieval community. Despite its algorithm simplicity, it preserves high recall percentages.	Geotracker: geospatial and temporal RSS navigation	NA:NA:NA:NA:NA:NA:NA	2018
Anthony Tomasic:Isaac Simmons:John Zimmerman	Users in an organization frequently request help by sending request messages to assistants that express information intent: an intention to update data in an information system. Human assistants spend a significant amount of time and effort processing these requests. For example, human resource assistants process requests to update personnel records, and executive assistants process requests to schedule conference rooms or to make travel reservations. To process the intent of a request, assistants read the request and then locate, complete, and submit a form that corresponds to the expressed intent. Automatically or semi-automatically processing the intent expressed in a request on behalf of an assistant would ease the mundane and repetitive nature of this kind of work.For a well-understood domain, a straightforward application of natural language processing techniques can be used to build an intelligent form interface to semi-automatically process information intent request messages. However, high performance parsers are based on machine learning algorithms that require a large corpus of examples that have been labeled by an expert. The generation of a labeled corpus of requests is a major barrier to the construction of a parser. In this paper, we investigate the construction of a natural language processing system and an intelligent form system that observes an assistant processing requests. The intelligent form system then generates a labeled training corpus by interpreting the observations. This paper reports on the measurement of the performance of the machine learning algorithms based on real data. The combination of observations, machine learning and interaction design produces an effective intelligent form interface based on natural language processing.	Learning information intent via observation	NA:NA:NA	2018
Deepayan Chakrabarti:Ravi Kumar:Kunal Punera	We develop a novel framework for the page-level template detection problem. Our framework is built on two main ideas. The first is theautomatic generation of training data for a classifier that, given apage, assigns a templateness score to every DOM node of the page. The second is the global smoothing of these per-node classifier scores bysolving a regularized isotonic regression problem; the latter follows from a simple yet powerful abstraction of templateness on a page. Our extensive experiments on human-labeled test data show that our approachdetects templates effectively.	Page-level template detection via isotonic smoothing	NA:NA:NA	2018
Wolfgang Gatterbauer:Paul Bohunsky:Marcus Herzog:Bernhard Krüpl:Bernhard Pollak	Traditionally, information extraction from web tables has focused on small, more or less homogeneous corpora, often based on assumptions about the use of <table> tags. A multitude of different HTML implementations of web tables make these approaches difficult to scale. In this paper, we approach the problem of domain-independent information extraction from web tables by shifting our attention from the tree-based representation of webpages to a variation of the two-dimensional visual box model used by web browsers to display the information on the screen. The there by obtained topological and style information allows us to fill the gap created by missing domain-specific knowledge about content and table templates. We believe that, in a future step, this approach can become the basis for a new way of large-scale knowledge acquisition from the current "Visual Web.	Towards domain-independent information extraction from web tables	NA:NA:NA:NA:NA	2018
Zaiqing Nie:Yunxiao Ma:Shuming Shi:Ji-Rong Wen:Wei-Ying Ma	The primary function of current Web search engines is essentially relevance ranking at the document level. However, myriad structured information about real-world objects is embedded in static Web pages and online Web databases. Document-level information retrieval can unfortunately lead to highly inaccurate relevance ranking in answering object-oriented queries. In this paper, we propose a paradigm shift to enable searching at the object level. In traditional information retrieval models, documents are taken as the retrieval units and the content of a document is considered reliable. However, this reliability assumption is no longer valid in the object retrieval context when multiple copies of information about the same object typically exist. These copies may be inconsistent because of diversity of Web site qualities and the limited performance of current information extraction techniques. If we simply combine the noisy and inaccurate attribute information extracted from different sources, we may not be able to achieve satisfactory retrieval performance. In this paper, we propose several language models for Web object retrieval, namely an unstructured object retrieval model, a structured object retrieval model, and a hybrid model with both structured and unstructured retrieval features. We test these models on a paper search engine and compare their performances. We conclude that the hybrid model is the superior by taking into account the extraction errors at varying levels.	Web object retrieval	NA:NA:NA:NA:NA	2018
Giuseppe Carenini:Raymond T. Ng:Xiaodong Zhou	Accessing an ever increasing number of emails, possibly on small mobile devices, has become a major problem for many users. Email summarization is a promising way to solve this problem. In this paper, we propose a new framework for email summarization. One novelty is to use a fragment quotation graph to try to capture an email conversation. The second novelty is to use clue words to measure the importance of sentences in conversation summarization. Based on clue words and their scores, we propose a method called CWS, which is capable of producing a summary of any length as requested by the user. We provide a comprehensive comparison of CWS with various existing methods on the Enron data set. Preliminary results suggest that CWS provides better summaries than existing methods.	Summarizing email conversations with clue words	NA:NA:NA	2018
Marius Paşca	As part of a large effort to acquire large repositories of facts from unstructured text on the Web, a seed-based framework for textual information extraction allows for weakly supervised extraction of class attributes (e.g., side effects and generic equivalent for drugs) from anonymized query logs. The extraction is guided by a small set of seed attributes, without any need for handcrafted extraction patterns or further domain-specific knowledge. The attributes of classes pertaining to various domains of interest to Web search users have accuracy levels significantly exceeding current state of the art. Inherently noisy search queries are shown to be a highly valuable, albeit unexplored, resource for Web-based information extraction, in particular for the task of class attribute extraction.	Organizing and searching the world wide web of facts -- step two: harnessing the wisdom of the crowds	NA	2018
Ziv Bar-Yossef:Idit Keidar:Uri Schonfeld	We consider the problem of DUST: Different URLs with Similar Text. Such duplicate URLs are prevalent in web sites, as web server software often uses aliases and redirections, and dynamically generates the same page from various different URLrequests. We present a novel algorithm, DustBuster, for uncovering DUST; that is, for discovering rules that transform a given URL to others that are likely to have similar content. DustBuster mines DUST effectively from previous crawl logs or web server logs, without examining page contents. Verifying these rules via sampling requires fetching few actual web pages. Search engines can benefit from information about DUST to increase the effectiveness of crawling, reduce indexing overhead, and improve the quality of popularity statistics such as PageRank.	Do not crawl in the dust: different urls with similar text	NA:NA:NA	2018
Hung Chim:Xiaotie Deng	In this paper, we propose a new similarity measure to compute the pairwise similarity of text-based documents based on suffix tree document model. By applying the new suffix tree similarity measure in Group-average Agglomerative Hierarchical Clustering (GAHC) algorithm, we developed a new suffix tree document clustering algorithm (NSTC). Experimental results on two standard document clustering benchmark corpus OHSUMED and RCV1 indicate that the new clustering algorithm is a very effective document clustering algorithm. Comparing with the results of traditional word term weight tf-idf similarity measure in the same GAHC algorithm, NSTC achieved an improvement of 51% on the average of F-measure score. Furthermore, we apply the new clustering algorithm in analyzing the Web documents in online forum communities. A topic oriented clustering algorithm is developed to help people in assessing, classifying and searching the the Web documents in a large forum community.	A new suffix tree similarity measure for document clustering	NA:NA	2018
Roberto J. Bayardo:Yiming Ma:Ramakrishnan Srikant	Given a large collection of sparse vector data in a high dimensional space, we investigate the problem of finding all pairs of vectors whose similarity score (as determined by a function such as cosine distance) is above a given threshold. We propose a simple algorithm based on novel indexing and optimization strategies that solves this problem without relying on approximation methods or extensive parameter tuning. We show the approach efficiently handles a variety of datasets across a wide setting of similarity thresholds, with large speedups over previous state-of-the-art approaches.	Scaling up all pairs similarity search	NA:NA:NA	2018
Gurmeet Singh Manku:Arvind Jain:Anish Das Sarma	Near-duplicate web documents are abundant. Two such documents differ from each other in a very small portion that displays advertisements, for example. Such differences are irrelevant for web search. So the quality of a web crawler increases if it can assess whether a newly crawled web page is a near-duplicate of a previously crawled web page or not. In the course of developing a near-duplicate detection system for a multi-billion page repository, we make two research contributions. First, we demonstrate that Charikar's fingerprinting technique is appropriate for this goal. Second, we present an algorithmic technique for identifying existing f-bit fingerprints that differ from a given fingerprint in at most k bit-positions, for small k. Our technique is useful for both online queries (single fingerprints) and all batch queries (multiple fingerprints). Experimental evaluation over real data confirms the practicality of our design.	Detecting near-duplicates for web crawling	NA:NA:NA	2018
Jian Hu:Hua-Jun Zeng:Hua Li:Cheng Niu:Zheng Chen	Demographic information plays an important role in personalized web applications. However, it is usually not easy to obtain this kind of personal data such as age and gender. In this paper, we made a first approach to predict users' gender and age from their Web browsing behaviors, in which the Webpage view information is treated as a hidden variable to propagate demographic information between different users. There are three main steps in our approach: First, learning from the Webpage click-though data, Webpages are associated with users' (known) age and gender tendency through a discriminative model; Second, users' (unknown) age and gender are predicted from the demographic information of the associated Webpages through a Bayesian framework; Third, based on the fact that Webpages visited by similar users may be associated with similar demographic tendency, and users with similar demographic information would visit similar Webpages, a smoothing component is employed to overcome the data sparseness of web click-though log. Experiments are conducted on a real web click-through log to demonstrate the effectiveness of the proposed approach. The experimental results show that the proposed algorithm can achieve up to 30.4% improvements on gender prediction and 50.3% on age prediction in terms of macro F1, compared to baseline algorithms.	Demographic prediction based on user's browsing behavior	NA:NA:NA:NA:NA	2018
Eytan Adar:Daniel S. Weld:Brian N. Bershad:Steven S. Gribble	The aggregation and comparison of behavioral patterns on the WWW represent a tremendous opportunity for understanding past behaviors and predicting future behaviors. In this paper, we take a first step at achieving this goal. We present a large scale study correlating the behaviors of Internet users on multiple systems ranging in size from 27 million queries to 14 million blog posts to 20,000 news articles. We formalize a model for events in these time-varying datasets and study their correlation. We have created an interface for analyzing the datasets, which includes a novel visual artifact, the DTWRadar, for summarizing differences between time series. Using our tool we identify a number of behavioral properties that allow us to understand the predictive power of patterns of use.	Why we search: visualizing and predicting user behavior	NA:NA:NA:NA	2018
Qiaozhu Mei:Xu Ling:Matthew Wondra:Hang Su:ChengXiang Zhai	In this paper, we define the problem of topic-sentiment analysis on Weblogs and propose a novel probabilistic model to capture the mixture of topics and sentiments simultaneously. The proposed Topic-Sentiment Mixture (TSM) model can reveal the latent topical facets in a Weblog collection, the subtopics in the results of an ad hoc query, and their associated sentiments. It could also provide general sentiment models that are applicable to any ad hoc topics. With a specifically designed HMM structure, the sentiment models and topic models estimated with TSM can be utilized to extract topic life cycles and sentiment dynamics. Empirical experiments on different Weblog datasets show that this approach is effective for modeling the topic facets and sentiments and extracting their dynamics from Weblog collections. The TSM model is quite general; it can be applied to any text collections with a mixture of topics and sentiments, thus has many potential applications, such as search result summarization, opinion tracking, and user behavior prediction.	Topic sentiment mixture: modeling facets and opinions in weblogs	NA:NA:NA:NA:NA	2018
Lars Backstrom:Cynthia Dwork:Jon Kleinberg	In a social network, nodes correspond topeople or other social entities, and edges correspond to social links between them. In an effort to preserve privacy, the practice of anonymization replaces names with meaningless unique identifiers. We describe a family of attacks such that even from a single anonymized copy of a social network, it is possible for an adversary to learn whether edges exist or not between specific targeted pairs of nodes.	Wherefore art thou r3579x?: anonymized social networks, hidden patterns, and structural steganography	NA:NA:NA	2018
Xiaodan Song:Yun Chi:Koji Hino:Belle L. Tseng	Information flows in a network where individuals influence each other. The diffusion rate captures how efficiently the information can diffuse among the users in the network. We propose an information flow model that leverages diffusion rates for: (1) prediction . identify where information should flow to, and (2) ranking . identify who will most quickly receive the information. For prediction, we measure how likely information will propagate from a specific sender to a specific receiver during a certain time period. Accordingly a rate-based recommendation algorithm is proposed that predicts who will most likely receive the information during a limited time period. For ranking, we estimate the expected time for information diffusion to reach a specific user in a network. Subsequently, a DiffusionRank algorithm is proposed that ranks users based on how quickly information will flow to them. Experiments on two datasets demonstrate the effectiveness of the proposed algorithms to both improve the recommendation performance and rank users by the efficiency of information flow.	Information flow modeling based on diffusion rate for prediction and ranking	NA:NA:NA:NA	2018
Shashank Pandit:Duen Horng Chau:Samuel Wang:Christos Faloutsos	Given a large online network of online auction users and their histories of transactions, how can we spot anomalies and auction fraud? This paper describes the design and implementation of NetProbe, a system that we propose for solving this problem. NetProbe models auction users and transactions as a Markov Random Field tuned to detect the suspicious patterns that fraudsters create, and employs a Belief Propagation mechanism to detect likely fraudsters. Our experiments show that NetProbe is both efficient and effective for fraud detection. We report experiments on synthetic graphs with as many as 7,000 nodes and 30,000 edges, where NetProbe was able to spot fraudulent nodes with over 90% precision and recall, within a matter of seconds. We also report experiments on a real dataset crawled from eBay, with nearly 700,000 transactions between more than 66,000users, where NetProbe was highly effective at unearthing hidden networks of fraudsters, within a realistic response time of about 6 minutes. For scenarios where the underlying data is dynamic in nature, we propose IncrementalNetProbe, which is an approximate, but fast, variant of NetProbe. Our experiments prove that Incremental NetProbe executes nearly doubly fast as compared to NetProbe, while retaining over 99% of its accuracy.	Netprobe: a fast and scalable system for fraud detection in online auction networks	NA:NA:NA:NA	2018
Harry Halpin:Valentin Robu:Hana Shepherd	The debate within the Web community over the optimal means by which to organize information often pits formalized classifications against distributed collaborative tagging systems. A number of questions remain unanswered, however, regarding the nature of collaborative tagging systems including whether coherent categorization schemes can emerge from unsupervised tagging by users. This paper uses data from the social bookmarking site delicio. us to examine the dynamics of collaborative tagging systems. In particular, we examine whether the distribution of the frequency of use of tags for "popular" sites with a long history (many tags and many users) can be described by a power law distribution, often characteristic of what are considered complex systems. We produce a generative model of collaborative tagging in order to understand the basic dynamics behind tagging, including how a power law distribution of tags could arise. We empirically examine the tagging history of sites in order to determine how this distribution arises over time and to determine the patterns prior to a stable distribution. Lastly, by focusing on the high-frequency tags of a site where the distribution of tags is a stabilized power law, we show how tag co-occurrence networks for a sample domain of tags can be used to analyze the meaning of particular tags given their relationship to other tags.	The complex dynamics of collaborative tagging	NA:NA:NA	2018
Jun Zhang:Mark S. Ackerman:Lada Adamic	Web-based communities have become important places for people to seek and share expertise. We find that networks in these communities typically differ in their topology from other online networks such as the World Wide Web. Systems targeted to augment web-based communities by automatically identifying users with expertise, for example, need to adapt to the underlying interaction dynamics. In this study, we analyze the Java Forum, a large online help-seeking community, using social network analysis methods. We test a set of network-based ranking algorithms, including PageRank and HITS, on this large size social network in order to identify users with high expertise. We then use simulations to identify a small number of simple simulation rules governing the question-answer dynamic in the network. These simple rules not only replicate the structural characteristics and algorithm performance on the empirically observed Java Forum, but also allow us to evaluate how other algorithms may perform in communities with different characteristics. We believe this approach will be fruitful for practical algorithm design and implementation for online expertise-sharing communities.	Expertise networks in online communities: structure and algorithms	NA:NA:NA	2018
Qi Su:Dmitry Pavlov:Jyh-Herng Chow:Wendell C. Baker	Enterprise and web data processing and content aggregation systems often require extensive use of human-reviewed data (e.g. for training and monitoring machine learning-based applications). Today these needs are often met by in-house efforts or out-sourced offshore contracting. Emerging applications attempt to provide automated collection of human-reviewed data at Internet-scale. We conduct extensive experiments to study the effectiveness of one such application. We also study the feasibility of using Yahoo! Answers, a general question-answering forum, for human-reviewed data collection.	Internet-scale collection of human-reviewed data	NA:NA:NA:NA	2018
Ahmed Metwally:Divyakant Agrawal:Amr El Abbadi	Click fraud is jeopardizing the industry of Internet advertising. Internet advertising is crucial for the thriving of the entire Internet, since it allows producers to advertise their products, and hence contributes to the well being of e-commerce. Moreover, advertising supports the intellectual value of the Internet by covering the running expenses of publishing content. Some content publishers are dishonest, and use automation to generate traffic to defraud the advertisers. Similarly, some advertisers automate clicks on the advertisements of their competitors to deplete their competitors' advertising budgets. This paper describes the advertising network model, and focuses on the most sophisticated type of fraud, which involves coalitions among fraudsters. We build on several published theoretical results to devise the Similarity-Seeker algorithm that discovers coalitions made by pairs of fraudsters. We then generalize the solution to coalitions of arbitrary sizes. Before deploying our system on a real network, we conducted comprehensive experiments on data samples for proof of concept. The results were very accurate. We detected several coalitions, formed using various techniques, and spanning numerous sites. This reveals the generality of our model and approach.	Detectives: detecting coalition hit inflation attacks in advertising networks streams	NA:NA:NA	2018
Bingjun Sun:Qingzhao Tan:Prasenjit Mitra:C. Lee Giles	Often scientists seek to search for articles on the Web related to a particular chemical. When a scientist searches for a chemical formula using a search engine today, she gets articles where the exact keyword string expressing the chemical formula is found. Searching for the exact occurrence of keywords during searching results in two problems for this domain: a) if the author searches for CH4 and the article has H4C, the article is not returned, and b) ambiguous searches like "He" return all documents where Helium is mentioned as well as documents where the pronoun "he" occurs. To remedy these deficiencies, we propose a chemical formula search engine. To build a chemical formula search engine, we must solve the following problems: 1) extract chemical formulae from text documents, 2) index chemical formulae, and 3) designranking functions for the chemical formulae. Furthermore, query models are introduced for formula search, and for each a scoring scheme based on features of partial formulae is proposed tomeasure the relevance of chemical formulae and queries. We evaluate algorithms for identifying chemical formulae in documents using classification methods based on Support Vector Machines(SVM), and a probabilistic model based on conditional random fields (CRF). Different methods for SVM and CRF to tune the trade-off between recall and precision forim balanced data are proposed to improve the overall performance. A feature selection method based on frequency and discrimination isused to remove uninformative and redundant features. Experiments show that our approaches to chemical formula extraction work well, especially after trade-off tuning. The results also demonstrate that feature selection can reduce the index size without changing ranked query results much.	Extraction and search of chemical formulae in text documents on the web	NA:NA:NA:NA	2018
B. Thomas Adler:Luca de Alfaro	We present a content-driven reputation system for Wikipedia authors. In our system, authors gain reputation when the edits they perform to Wikipedia articles are preserved by subsequent authors, and they lose reputation when their edits are rolled back or undone in short order. Thus, author reputation is computed solely on the basis of content evolution; user-to-user comments or ratings are not used. The author reputation we compute could be used to flag new contributions from low-reputation authors, or it could be used to allow only authors with high reputation to contribute to controversialor critical pages. A reputation system for the Wikipedia could also provide an incentive for high-quality contributions. We have implemented the proposed system, and we have used it to analyze the entire Italian and French Wikipedias, consisting of a total of 691, 551 pages and 5, 587, 523 revisions. Our results show that our notion of reputation has good predictive value: changes performed by low-reputation authors have a significantly larger than average probability of having poor quality, as judged by human observers, and of being later undone, as measured by our algorithms.	A content-driven reputation system for the wikipedia	NA:NA	2018
Abhinandan S. Das:Mayur Datar:Ashutosh Garg:Shyam Rajaram	Several approaches to collaborative filtering have been studied but seldom have studies been reported for large (several millionusers and items) and dynamic (the underlying item set is continually changing) settings. In this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of Google News. We generate recommendations using three approaches: collaborative filtering using MinHash clustering, Probabilistic Latent Semantic Indexing (PLSI), and covisitation counts. We combine recommendations from different algorithms using a linear model. Our approach is content agnostic and consequently domain independent, making it easily adaptable for other applications and languages with minimal effort. This paper will describe our algorithms and system setup in detail, and report results of running the recommendations engine on Google News.	Google news personalization: scalable online collaborative filtering	NA:NA:NA:NA	2018
Xiaochuan Ni:Gui-Rong Xue:Xiao Ling:Yong Yu:Qiang Yang	Weblogs have become a prevalent source of information for people to express themselves. In general, there are two genres of contents in weblogs. The first kind is about the webloggers' personal feelings, thoughts or emotions. We call this kind of weblogs affective articles. The second kind of weblogs is about technologies and different kinds of informative news. In this paper, we present a machine learning method for classifying informative and affective articles among weblogs. We consider this problem as a binary classification problem. By using machine learning approaches, we achieve about 92% on information retrieval performance measures including precision, recall and F1. We set up three studies on the applications of above classification approach in both research and industrial fields. The above classification approach is used to improve the performance of classification of emotions from weblog articles. We also develop an intent-driven weblog-search engine based on the classification techniques to improve the satisfaction of Web users. Finally, our approach is applied to search for weblogs with a great deal of informative articles.	Exploring in the weblog space by detecting informative and affective articles	NA:NA:NA:NA:NA	2018
Yi-Min Wang:Ming Ma:Yuan Niu:Hao Chen	Spammers use questionable search engine optimization (SEO) techniques to promote their spam links into top search results. In this paper, we focus on one prevalent type of spam - redirection spam - where one can identify spam pages by the third-party domains that these pages redirect traffic to. We propose a five-layer, double-funnel model for describing end-to-end redirection spam, present a methodology for analyzing the layers, and identify prominent domains on each layer using two sets of commercial keywords. one targeting spammers and the other targeting advertisers. The methodology and findings are useful for search engines to strengthen their ranking algorithms against spam, for legitimate website owners to locate and remove spam doorway pages, and for legitimate advertisers to identify unscrupulous syndicators who serve ads on spam pages.	Spam double-funnel: connecting web spammers with advertisers	NA:NA:NA:NA	2018
Tobias Groothuyse:Swaminathan Sivasubramanian:Guillaume Pierre	Generic database replication algorithms do not scale linearly in throughput as all update, deletion and insertion (UDI) queries must be applied to every database replica. The throughput is therefore limited to the point where the number of UDI queries alone is sufficient to overload one server. In such scenarios, partial replication of a database can help, as UDI queries are executed only by a subset of all servers. In this paper we propose GlobeTP, a system that employs partial replication to improve database throughput. GlobeTP exploits the fact that a Web application's query workload is composed of a small set of read and write templates. Using knowledge of these templates and their respective execution costs, GlobeTP provides database table placements that produce significant improvements in database throughput. We demonstrate the efficiency of this technique using two different industry standard benchmarks. In our experiments, GlobeTP increases the throughput by 57% to 150% compared to full replication, while using identical hardware configuration. Furthermore, adding a single query cache improves the throughput by another 30% to 60%.	Globetp: template-based database replication for scalable web applications	NA:NA:NA	2018
Niraj Tolia:M. Satyanarayanan	With the growing use of dynamic web content generated from relational databases, traditional caching solutions for through put and latency improvements are ineffective. We describe a middleware layer called Ganesh that reduces the volume of data transmitted without semantic interpretation of queries or results. It achieves this reduction through the use of cryptographic hashing to detect similarities with previous results. These benefits do not require any compromise of the strict consistency semantics provided by the back-end database. Further, Ganesh does not require modifications to applications, web servers, or database servers, and works with closed-source applications and databases. Using two bench marks representative of dynamic web sites, measurements of our prototype show that it can increase end-to-end throughput by as much as two fold for non-data intensive applications and by as much as ten fold for dataintensive ones.	Consistency-preserving caching of dynamic database content	NA:NA	2018
Rajeev Gupta:Krithi Ramamritham	Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making. Typically a user desires to obtain the value of some aggregation function over distributed data items, for example, to know (a) the average of temperatures sensed by a set of sensors (b) the value of index of mid-cap stocks. In these queries a client specifies a coherency requirement as part of the query. In this paper we present a low-cost, scalable technique to answer continuous aggregation queries using a content distribution network of dynamic data items. In such a network of data aggregators, each data aggregator serves a set of data items at specific coherencies. Just as various fragments of a dynamic web-page are served by one or more nodes of a content distribution network, our technique involves decomposing a client query into sub-queries and executing sub-queries on judiciously chosen data aggregators with their individual sub-query incoherency bounds. We provide a technique of getting the optimal query plan (i.e., set of sub-queries and their chosen data aggregators) which satisfies client query.s coherency requirement with least cost, measured in terms of the number of refresh messages sent from aggregators to the client. For estimating query execution cost, we build a continuous query cost model which can be used to estimate the number of messages required to satisfy the client specified incoherency bound. Performance results using real-world traces show that our cost based query planning leads to queries being executed using less than one third the number of messages required by existing schemes.	Optimized query planning of continuous aggregation queries in dynamic data dissemination networks	NA:NA	2018
Chunqiang Tang:Malgorzata Steinder:Michael Spreitzer:Giovanni Pacifici	Given a set of machines and a set of Web applications with dynamically changing demands, an online application placement controller decides how many instances to run for each application and where to put them, while observing all kinds of resource constraints. This NP hard problem has real usage in commercial middleware products. Existing approximation algorithms for this problem can scale to at most a few hundred machines, and may produce placement solutions that are far from optimal when system resources are tight. In this paper, we propose a new algorithm that can produce within 30seconds high-quality solutions for hard placement problems with thousands of machines and thousands of applications. This scalability is crucial for dynamic resource provisioning in large-scale enterprise data centers. Our algorithm allows multiple applications to share a single machine, and strivesto maximize the total satisfied application demand, to minimize the number of application starts and stops, and to balance the load across machines. Compared with existing state-of-the-art algorithms, for systems with 100 machines or less, our algorithm is up to 134 times faster, reduces application starts and stops by up to 97%, and produces placement solutions that satisfy up to 25% more application demands. Our algorithm has been implemented and adopted in a leading commercial middleware product for managing the performance of Web applications.	A scalable application placement controller for enterprise data centers	NA:NA:NA:NA	2018
Fan Yang:Nitin Gupta:Nicholas Gerner:Xin Qi:Alan Demers:Johannes Gehrke:Jayavel Shanmugasundaram	Data-driven web applications are usually structured in three tiers with different programming models at each tier. This division forces developers to manually partition application functionality across the tiers, resulting in complex logic, suboptimal partitioning, and expensive re-partitioning of applications. In this paper, we introduce a unified platform for automatic partitioning of data-driven web applications. Our approach is based on Hilda[41, 46], a high-level declarative programming language with a unified data and programming model for all the layers of the application. Based on run-time properties of the application, Hilda's run time system automatically partitions the application between the tiers to improve response time while adhering to memory and/ or processing constraints at the clients. We evaluate our methodology with traces from a real application and with TPC-W, and our results show that automatic partitioning outperforms manual partitioning without the associated development overhead.	A unified platform for data driven web applications with automatic client-server partitioning	NA:NA:NA:NA:NA:NA:NA	2018
Hussein A. Alzoubi:Michael Rabinovich:Oliver Spatscheck	This paper presents the architecture and the preliminary evaluation ofa request routing DNS server that decouples server selectionfrom the rest of DNS functionality. Our DNS server, which we refer toas MyXDNS, exposes well-defined APIs for uploading an externallycomputed server selection policy and for interacting with an external networkproximity service. With MyXDNS, researchers can explore their ownnetwork proximity metrics and request routing algorithms withouthaving to worry about DNS internals. Furthermore, MyXDNS is based onopen-source MyDNS and is available to public. Stress-testing of MyXDNSindicated that it achieves its flexibility at an acceptable cost: asingle MyXDNS running on a low-level server can process 3000 req/secwith sub-millisecond response even in the presence of continuousupdates to server selection policy.	MyXDNS: a resquest routing dns server with decoupled server selection	NA:NA:NA	2018
Gen Hattori:Keiichiro Hoashi:Kazunori Matsumoto:Fumiaki Sugaya	The demand of browsing information from general Web pages using a mobile phone is increasing. However, since the majority of Web pages on the Internet are optimized for browsing from PCs, it is difficult for mobile phone users to obtain sufficient information from the Web. Therefore, a method to reconstruct PC-optimized Web pages for mobile phone users is essential. An example approach is to segment the Web page based on its structure, and utilize the hierarchy of the content element to regenerate a page suitable for mobile phone browsing. In our previous work, we have examined a robust automatic Web page segmentation scheme which uses the distance between content elements based on the relative HTML tag hierarchy, i.e., the number and depth of HTML tags in Web pages. However, this scheme has a problem that the content-distance based on the order of HTML tags does not always correspond to the intuitional distance between content elements on the actual layout of a Web page. In this paper, we propose a hybrid segmentation method which segments Web pages based on both the content-distance calculated by the previous scheme, and a novel approach which utilizes Web page layout information. Experiments conducted to evaluate the accuracy of Web page segmentation results prove that the proposed method can segment Web pages more accurately than conventional methods. Furthermore, implementation and evaluation of our system on the mobile phone prove that our method can realize superior usability compared to commercial Web browsers.	Robust web page segmentation for mobile terminal using content-distances and page layout information	NA:NA:NA:NA	2018
Gabriel Ghinita:Panos Kalnis:Spiros Skiadopoulos	Nowadays, mobile users with global positioning devices canaccess Location Based Services (LBS) and query about pointsof interest in their proximity. For such applications to succeed,privacy and confidentiality are essential. Encryptionalone is not adequate; although it safeguards the systemagainst eavesdroppers, the queries themselves may disclosethe location and identity of the user. Recently, there havebeen proposed centralized architectures based on K-anonymity,which utilize an intermediate anonymizer between themobile users and the LBS. However, the anonymizer mustbe updated continuously with the current locations of allusers. Moreover, the complete knowledge of the entire systemposes a security threat, if the anonymizer is compromised.In this paper we address two issues: (i) We show thatexisting approaches may fail to provide spatial anonymityfor some distributions of user locations and describe a noveltechnique which solves this problem. (ii) We propose Prive,a decentralized architecture for preserving the anonymityof users issuing spatial queries to LBS. Mobile users self-organizeinto an overlay network with good fault toleranceand load balancing properties. Prive avoids the bottleneckcaused by centralized techniques both in terms of anonymizationand location updates. Moreover, the system state isdistributed in numerous users, rendering Prive resilient toattacks. Extensive experimental studies suggest that Priveis applicable to real-life scenarios with large populations ofmobile users.	PRIVE: anonymous location-based queries in distributed mobile systems	NA:NA:NA	2018
Rainer Simon:Peter Fröhlich	In this paper we present an application framework that leverages geospatial content on the World Wide Web by enabling innovative modes of interaction and novel types of user interfaces on advanced mobile phones and PDAs. We discuss the current development steps involved in building mobile geospatial Web applications and derive three technological pre-requisites for our framework: spatial query operations based on visibility and field of view, a 2.5D environment model, and a presentationindependent data exchange format for geospatial query results. We propose the Local Visibility Model as a suitable XML-based candidate and present a prototype implementation.	A mobile application framework for the geospatial web	NA:NA	2018
Shashank Pandit:Christopher Olston	Users searching for information in hypermedia environments often perform querying followed by manual navigation. Yet, the conventional text/hypertext retrieval paradigm does not explicity take post-query navigation into account. This paper proposes a new retrieval paradigm, called navigation-aided retrieval (NAR), which treats both querying and navigation as first-class activities. In the NAR paradigm, querying is seen as a means to identify starting points for navigation, and navigation is guided based on information supplied in the query. NAR is a generalization of the conventional probabilistic information retrieval paradigm, which implicitly assumes no navigation takes place. This paper presents a formal model for navigation-aided retrieval, and reports empirical results that point to the real-world applicability of the model. The experiments were performed over a large Web corpus provided by TREC, using human judgments on a new rating scale developed for navigation-aided retrieval. In the case of ambiguous queries, the new retrieval model identifies good starting points for post-query navigation. For less ambiguous queries that need not be paired with navigation, the output closely matches that of a conventional retrieval system.	Navigationaided retrieval	NA:NA	2018
Ziv Bar-Yossef:Maxim Gurevich	We address the problem of measuring global quality met-rics of search engines, like corpus size, index freshness, anddensity of duplicates in the corpus. The recently proposedestimators for such metrics [2, 6] suffer from significant biasand/or poor performance, due to inaccurate approximationof the so called .document degrees..We present two new estimators that are able to overcomethe bias introduced by approximate degrees. Our estimatorsare based on a careful implementation of an approximateimportance sampling procedure. Comprehensive theoreti-cal and empirical analysis of the estimators demonstratesthat they have essentially no bias even in situations wheredocument degrees are poorly approximated.Building on an idea from [6], we discuss Rao Blackwelliza-tion as a generic method for reducing variance in searchengine estimators. We show that Rao-Blackwellizing ourestimators results in significant performance improvements,while not compromising accuracy.	Efficient search engine measurements	NA:NA	2018
Jiangong Zhang:Torsten Suel	Current web search engines focus on searching only themost recentsnapshot of the web. In some cases, however, it would be desirableto search over collections that include many different crawls andversions of each page. One important example of such a collectionis the Internet Archive, though there are many others. Sincethe data size of such an archive is multiple times that of a singlesnapshot, this presents us with significant performance challenges.Current engines use various techniques for index compression andoptimized query execution, but these techniques do not exploit thesignificant similarities between different versions of a page, or betweendifferent pages.In this paper, we propose a general framework for indexing andquery processing of archival collections and, more generally, anycollections with a sufficient amount of redundancy. Our approachresults in significant reductions in index size and query processingcosts on such collections, and it is orthogonal to and can be combinedwith the existing techniques. It also supports highly efficientupdates, both locally and over a network. Within this framework,we describe and evaluate different implementations that trade offindex size versus CPU cost and other factors, and discuss applicationsranging from archival web search to local search of web sites,email archives, or file systems. We present experimental resultsbased on search engine query log and a large collection consistingof multiple crawls.	Efficient search in large textual collections with redundancy	NA:NA	2018
Anirban Dasgupta:Arpita Ghosh:Ravi Kumar:Christopher Olston:Sandeep Pandey:Andrew Tomkins	Previous studies have highlighted the high arrival rate of new contenton the web. We study the extent to which this new content can beefficiently discovered by a crawler. Our study has two parts. First,we study the inherent difficulty of the discovery problem using amaximum cover formulation, under an assumption of perfect estimates oflikely sources of links to new content. Second, we relax thisassumption and study a more realistic setting in which algorithms mustuse historical statistics to estimate which pages are most likely toyield links to new content. We recommend a simple algorithm thatperforms comparably to all approaches we consider.We measure the emphoverhead of discovering new content, defined asthe average number of fetches required to discover one new page. Weshow first that with perfect foreknowledge of where to explore forlinks to new content, it is possible to discover 90% of all newcontent with under 3% overhead, and 100% of new content with 9%overhead. But actual algorithms, which do not have access to perfectforeknowledge, face a more difficult task: one quarter of new contentis simply not amenable to efficient discovery. Of the remaining threequarters, 80% of new content during a given week may be discoveredwith 160% overhead if content is recrawled fully on a monthly basis.	The discoverability of the web	NA:NA:NA:NA:NA:NA	2018
Luciano Barbosa:Juliana Freire	We address the problem of identifying the domain of onlinedatabases. More precisely, given a set F of Web forms automaticallygathered by a focused crawler and an online databasedomain D, our goal is to select from F only the formsthat are entry points to databases in D. Having a set ofWebforms that serve as entry points to similar online databasesis a requirement for many applications and techniques thataim to extract and integrate hidden-Web information, suchas meta-searchers, online database directories, hidden-Webcrawlers, and form-schema matching and merging.We propose a new strategy that automatically and accuratelyclassifies online databases based on features that canbe easily extracted from Web forms. By judiciously partitioningthe space of form features, this strategy allows theuse of simpler classifiers that can be constructed using learningtechniques that are better suited for the features of eachpartition. Experiments using real Web data in a representativeset of domains show that the use of different classifiersleads to high accuracy, precision and recall. This indicatesthat our modular classifier composition provides an effectiveand scalable solution for classifying online databases.	Combining classifiers to identify online databases	NA:NA	2018
Luciano Barbosa:Juliana Freire	In this paper we describe new adaptive crawling strategies to efficiently locate the entry points to hidden-Web sources. The fact that hidden-Web sources are very sparsely distributedmakes the problem of locating them especially challenging. We deal with this problem by using the contents ofpages to focus the crawl on a topic; by prioritizing promisinglinks within the topic; and by also following links that may not lead to immediate benefit. We propose a new frameworkwhereby crawlers automatically learn patterns of promisinglinks and adapt their focus as the crawl progresses, thus greatly reducing the amount of required manual setup andtuning. Our experiments over real Web pages in a representativeset of domains indicate that online learning leadsto significant gains in harvest rates' the adaptive crawlers retrieve up to three times as many forms as crawlers thatuse a fixed focus strategy.	An adaptive crawler for locating hidden-Web entry points	NA:NA	2018
Toufik Bennouas:Fabien de Montgolfier	This paper proposes a random Web crawl model. A Web crawl is a (biased and partial) image of the Web. This paper deals with the hyperlink structure, i.e. a Web crawl is a graph, whose vertices are the pages and whose edges are the hypertextual links. Of course a Web crawl has a very special structure; we recall some known results about it. We then propose a model generating similar structures. Our model simply simulates a crawling, i.e. builds and crawls the graph at the same time. The graphs generated have lot of known properties of Web crawls. Our model is simpler than most random Web graph models, but captures the sames properties. Notice that it models the crawling process instead of the page writing process of Web graph models.	Random web crawls	NA:NA	2018
Yon Dourisboure:Filippo Geraci:Marco Pellegrini	The World Wide Web (WWW) is rapidly becoming important for society as a medium for sharing data, information and services, and there is a growing interest in tools for understanding collective behaviors and emerging phenomena in the WWW. In this paper we focus on the problem of searching and classifying communities in the web. Loosely speaking a community is a group of pages related to a common interest. More formally communities have been associated in the computer science literature with the existence of a locally dense sub-graph of the web-graph (where web pages are nodes and hyper-links are arcs of the web-graph). The core of our contribution is a new scalable algorithm for finding relatively dense subgraphs in massive graphs. We apply our algorithm on web-graphs built on three publicly available large crawls of the web (with raw sizes up to 120M nodes and 1G arcs). The effectiveness of our algorithm in finding dense subgraphs is demonstrated experimentally by embedding artificial communities in the web-graph and counting how many of these are blindly found. Effectiveness increases with the size and density of the communities: it is close to 100% for communities of a thirty nodes or more (even at low density). It is still about 80% even for communities of twenty nodes with density over 50% of the arcs present. At the lower extremes the algorithm catches 35% of dense communities made of ten nodes. We complete our Community Watch system by clustering the communities found in the web-graph into homogeneous groups by topic and labelling each group by representative keywords.	Extraction and classification of dense communities in the web	NA:NA:NA	2018
Jure Leskovec:Susan Dumais:Eric Horvitz	Graphical relationships among Web pages have been exploited inmethods for ranking search results. To date, specific graphicalproperties have been used in these analyses. We introduce a WebProjection methodology that generalizes prior efforts of graphicalrelationships of the web in several ways. With the approach, wecreate subgraphs by projecting sets of pages and domains onto thelarger web graph, and then use machine learning to constructpredictive models that consider graphical properties as evidence. Wedescribe the method and then present experiments that illustrate theconstruction of predictive models of search result quality and userquery reformulation.	Web projections: learning from contextual subgraphs of the web	NA:NA:NA	2018
Yu-Ting Liu:Tie-Yan Liu:Tao Qin:Zhi-Ming Ma:Hang Li	This paper is concerned with rank aggregation, the task of combining the ranking results of individual rankers at meta-search. Previously, rank aggregation was performed mainly by means of unsupervised learning. To further enhance ranking accuracies, we propose employing supervised learning to perform the task, using labeled data. We refer to the approach as Supervised Rank Aggregation. We set up a general framework for conducting Supervised Rank Aggregation, in which learning is formalized an optimization which minimizes disagreements between ranking results and the labeled data. As case study, we focus on Markov Chain based rank aggregation in this paper. The optimization for Markov Chain based methods is not a convex optimization problem, however, and thus is hard to solve. We prove that we can transform the optimization problem into that of Semidefinite Programming and solve it efficiently. Experimental results on meta-searches show that Supervised Rank Aggregation can significantly outperform existing unsupervised methods.	Supervised rank aggregation	NA:NA:NA:NA:NA	2018
Huaiyu Zhu:Sriram Raghavan:Shivakumar Vaithyanathan:Alexander Löser	Despite the success of web search engines, search over large enterprise intranets still suffers from poor result quality. Earlier work [6] that compared intranets and the Internet from the view point of keyword search has pointed to several reasons why the search problem is quite different in these two domains. In this paper, we address the problem of providing high quality answers to navigational queries in the intranet (e.g., queries intended to find product or personal home pages, service pages, etc.). Our approach is based on offline identification of navigational pages, intelligent generation of term-variants to associate with each page, and the construction of separate indices exclusively devoted to answering navigational queries. Using a testbed of 5.5M pages from the IBM intranet, we present evaluation results that demonstrate that for navigational queries, our approach of using custom indices produces results of significantly higher precision than those produced by a general purpose search algorithm.	Navigating the intranet with high precision	NA:NA:NA:NA	2018
Shenghua Bao:Guirong Xue:Xiaoyuan Wu:Yong Yu:Ben Fei:Zhong Su	This paper explores the use of social annotations to improve websearch. Nowadays, many services, e.g. del.icio.us, have been developed for web users to organize and share their favorite webpages on line by using social annotations. We observe that the social annotations can benefit web search in two aspects: 1) the annotations are usually good summaries of corresponding webpages; 2) the count of annotations indicates the popularity of webpages. Two novel algorithms are proposed to incorporate the above information into page ranking: 1) SocialSimRank (SSR)calculates the similarity between social annotations and webqueries; 2) SocialPageRank (SPR) captures the popularity of webpages. Preliminary experimental results show that SSR can find the latent semantic association between queries and annotations, while SPR successfully measures the quality (popularity) of a webpage from the web users' perspective. We further evaluate the proposed methods empirically with 50 manually constructed queries and 3000 auto-generated queries on a dataset crawledfrom delicious. Experiments show that both SSR and SPRbenefit web search significantly.	Optimizing web search using social annotations	NA:NA:NA:NA:NA:NA	2018
Kamal Ali:Mark Scarr	Metrics such as click counts are vital to online businesses but their measurement has been problematic due to inclusion of high variance robot traffic. We posit that by applying statistical methods more rigorous than have been employed to date that we can build a robust model of thedistribution of clicks following which we can set probabilistically sound thresholds to address outliers and robots. Prior research in this domain has used inappropriate statistical methodology to model distributions and current industrial practice eschews this research for conservative ad-hoc click-level thresholds. Prevailing belief is that such distributions are scale-free power law distributions but using more rigorous statistical methods we find the best description of the data is instead provided by a scale-sensitive Zipf-Mandelbrot mixture distribution. Our results are based on ten data sets from various verticals in the Yahoo domain. Since mixture models can overfit the data we take care to use the BIC log-likelihood method which penalizes overly complex models. Using a mixture model in the web activity domain makes sense because there are likely multiple classes of users. In particular, we have noticed that there is a significantly large set of "users" that visit the Yahoo portal exactly once a day. We surmise these may be robots testing internet connectivity by pinging the Yahoo main website. Backing up our quantitative analysis is graphical analysis in which empirical distributions are plotted against heoretical distributions in log-log space using robust cumulative distribution plots. This methodology has two advantages: plotting in log-log space allows one to visually differentiate the various exponential distributions and secondly, cumulative plots are much more robust to outliers. We plan to use the results of this work for applications for robot removal from web metrics business intelligence systems.	Robust methodologies for modeling web click distributions	NA:NA	2018
Matthew Richardson:Ewa Dominowska:Robert Ragno	Search engine advertising has become a significant element of the Web browsing experience. Choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad. This ranking has a strong impact on the revenue the search engine receives from the ads. Further, showing the user an ad that they prefer to click on improves user satisfaction. For these reasons, it is important to be able to accurately estimate the click-through rate of ads in the system. For ads that have been displayed repeatedly, this is empirically measurable, but for new ads, other means must be used. We show that we can use features of ads, terms, and advertisers to learn a model that accurately predicts the click-though rate for new ads. We also show that using our model improves the convergence and performance of an advertising system. As a result, our model increases both revenue and user satisfaction.	Predicting clicks: estimating the click-through rate for new ads	NA:NA:NA	2018
Christian Borgs:Jennifer Chayes:Nicole Immorlica:Kamal Jain:Omid Etesami:Mohammad Mahdian	We consider the problem of online keyword advertising auctions among multiple bidders with limited budgets, and study a natural bidding heuristic in which advertisers attempt to optimize their utility by equalizing their return-on-investment across all keywords. We show that existing auction mechanisms combined with this heuristic can experience cycling (as has been observed in many current systems), and therefore propose a modified class of mechanisms with small random perturbations. This perturbation is reminiscent of the small time-dependent perturbations employed in the dynamical systems literature to convert many types of chaos into attracting motions. We show that the perturbed mechanism provably converges in the case of first-price auctions and experimentally converges in the case of second-price auctions. Moreover, the point of convergence has a natural economic interpretation as the unique market equilibrium in the case of first-price mechanisms. In the case of second-price auctions, we conjecture that it converges to the "supply-aware" market equilibrium. Thus, our results can be alternatively described as a tâtonnement process for convergence to market equilibriumin which prices are adjusted on the side of the buyers rather than the sellers. We also observe that perturbation in mechanism design is useful in a broader context: In general, it can allow bidders to "share" a particular item, leading to stable allocations and pricing for the bidders, and improved revenue for the auctioneer.	Dynamics of bid optimization in online advertisement auctions	NA:NA:NA:NA:NA:NA	2018
Jiahui Liu:Earl Wagner:Larry Birnbaum	Comparing and contrasting is an important strategy people employ to understand new situations and create solutions for new problems. Similar events can provide hints for problem solving, as well as larger contexts for understanding the specific circumstances of an event. Lessons can leaned from past experience, insights can be gained about the new situation from familiar examples, and trends can be discovered among similar events. As the largest knowledge base for human beings, the Web provides both an opportunity and a challenge to discover comparable cases in order to facilitate situation analysis and problem solving. In this paper, we present Compare & Contrast, a system that uses the Web to discover comparable cases for news stories, documents about similar situations but involving distinct entities. The system analyzes a news story given by the user and builds a model of the story. With the story model, the system dynamically discovers entities comparable to the main entity in the original story and uses these comparable entities as seeds to retrieve web pages about comparable cases. The system is domain independent, does not require any domain-specific knowledge engineering efforts, and deals with the complexity of unstructured text and noise on the web in a robust way. We evaluated the system with an experiment on a collection of news articles and a user study.	Compare&contrast: using the web to discover comparable cases for news stories	NA:NA:NA	2018
Dirk Kukulenz:Alexandros Ntoulas	Search queries applied to extract relevant information from the World Wide Web over a period of time may be denoted as continuous search queries. The improvement of continuous search queries may concern not only the quality of retrieved results but also the freshness of results, i.e. the time between the availability of a respective data object on the Web and the notification of a user by the search engine. In some cases a user should be notified immediately since the value of the respective information decreases quickly, as e.g. news about companies that affect the value of respective stocks, or sales offers for products that may no longer be available after a short period of time. In the document filtering literature, the optimization of such queries is usually based on threshold classification. Documents above a quality threshold are returned to a user. The threshold is tuned in order to optimize the quality of retrieved results. The disadvantage of such approaches is that the amount of information returned to a user may hardly be controlled without further user-interaction. In this paper, we consider the optimization of bounded continuous search queries where only the estimated best k elements are returned to a user. We present a new optimization method for bounded continuous search queries based on the optimal stopping theory and compare the new method to methods currently applied by Web search systems. The new method provides results of significantly higher quality for the cases where very fresh results have to be delivered.	Answering bounded continuous search queries in the world wide web	NA:NA	2018
Gang Luo:Chunqiang Tang:Ying-li Tian	Finding relationships between entities on the Web, e.g., the connections between different places or the commonalities of people, is a novel and challenging problem. Existing Web search engines excel in keyword matching and document ranking, but they cannot well handle many relationship queries. This paper proposes a new method for answering relationship queries on two entities. Our method first respectively retrieves the top Web pages for either entity from a Web search engine. It then matches these Web pages and generates an ordered list of Web page pairs. Each Web page pair consists of one Web page for either entity. The top ranked Web page pairs are likely to contain the relationships between the two entities. One main challenge in the ranking process is to effectively filter out the large amount of noise in the Web pages without losing much useful information. To achieve this, our method assigns appropriate weights to terms in Web pages and intelligently identifies the potential connecting terms that capture the relationships between the two entities. Only those top potential connecting terms with large weights are used to rank Web page pairs. Finally, the top ranked Web page pairs are presented to the searcher. For each such pair, the query terms and the top potential connecting terms are properly highlighted so that the relationships between the two entities can be easily identified. We implemented a prototype on top of the Google search engine and evaluated it under a wide variety of query scenarios. The experimental results show that our method is effective at finding important relationships with low overhead.	Answering relationship queries on the web	NA:NA:NA	2018
Soumen Chakrabarti	Extractors and taggers turn unstructured text into entity-relation(ER) graphs where nodes are entities (email, paper, person,conference, company) and edges are relations (wrote, cited,works-for). Typed proximity search of the form <B>type=personNEAR company~"IBM", paper~"XML"</B> is an increasingly usefulsearch paradigm in ER graphs. Proximity search implementations either perform a Pagerank-like computation at query time, which is slow, or precompute, store and combine per-word Pageranks, which can be very expensive in terms of preprocessing time and space. We present HubRank, a new system for fast, dynamic, space-efficient proximity searches in ER graphs. During preprocessing, HubRank computesand indexes certain "sketchy" random walk fingerprints for a small fraction of nodes, carefully chosen using query log statistics. At query time, a small "active" subgraph is identified, bordered bynodes with indexed fingerprints. These fingerprints are adaptively loaded to various resolutions to form approximate personalized Pagerank vectors (PPVs). PPVs at remaining active nodes are now computed iteratively. We report on experiments with CiteSeer's ER graph and millions of real Cite Seer queries. Some representative numbers follow. On our testbed, HubRank preprocesses and indexes 52 times faster than whole-vocabulary PPV computation. A text index occupies 56 MB. Whole-vocabulary PPVs would consume 102GB. If PPVs are truncated to 56 MB, precision compared to true Pagerank drops to 0.55; incontrast, HubRank has precision 0.91 at 63MB. HubRank's average querytime is 200-300 milliseconds; query-time Pagerank computation takes 11 seconds on average.	Dynamic personalized pagerank in entity-relation graphs	NA	2018
Zhicheng Dou:Ruihua Song:Ji-Rong Wen	Although personalized search has been proposed for many years and many personalization strategies have been investigated, it is still unclear whether personalization is consistently effective on different queries for different users, and under different search contexts. In this paper, we study this problem and get some preliminary conclusions. We present a large-scale evaluation framework for personalized search based on query logs, and then evaluate five personalized search strategies (including two click-based and three profile-based ones) using 12-day MSN query logs. By analyzing the results, we reveal that personalized search has significant improvement over common web search on some queries but it also has little effect on other queries (e.g., queries with small click entropy). It even harms search accuracy under some situations. Furthermore, we show that straightforward click-based personalization strategies perform consistently and considerably well, while profile-based ones are unstable in our experiments. We also reveal that both long-term and short-term contexts are very important in improving search performance for profile-based personalized search strategies.	A large-scale evaluation and analysis of personalized search strategies	NA:NA:NA	2018
Yabo Xu:Ke Wang:Benyu Zhang:Zheng Chen	Personalized web search is a promising way to improve search quality by customizing search results for people with individual information goals. However, users are uncomfortable with exposing private preference information to search engines. On the other hand, privacy is not absolute, and often can be compromised if there is a gain in service or profitability to the user. Thus, a balance must be struck between search quality and privacy protection. This paper presents a scalable way for users to automatically build rich user profiles. These profiles summarize a user.s interests into a hierarchical organization according to specific interests. Two parameters for specifying privacy requirements are proposed to help the user to choose the content and degree of detail of the profile information that is exposed to the search engine. Experiments showed that the user profile improved search quality when compared to standard MSN rankings. More importantly, results verified our hypothesis that a significant improvement on search quality can be achieved by only sharing some higher-level user profile information, which is potentially less sensitive than detailed personal information.	Privacy-enhancing personalized web search	NA:NA:NA:NA	2018
Trevor Jim:Nikhil Swamy:Michael Hicks	Web sites that accept and display content such as wiki articles or comments typically filter the content to prevent injected script code from running in browsers that view the site. The diversity of browser rendering algorithms and the desire to allow rich content make filtering quite difficult, however, and attacks such as the Samy and Yamanner worms have exploited filtering weaknesses. This paper proposes a simple alternative mechanism for preventing script injection called Browser-Enforced Embedded Policies (BEEP). The idea is that a web site can embed a policy in its pages that specifies which scripts are allowed to run. The browser, which knows exactly when it will run a script, can enforce this policy perfectly. We have added BEEP support to several browsers, and built tools to simplify adding policies to web applications. We found that supporting BEEP in browsers requires only small and localized modifications, modifying web applications requires minimal effort, and enforcing policies is generally lightweight.	Defeating script injection attacks with browser-enforced embedded policies	NA:NA:NA	2018
Collin Jackson:Helen J. Wang	Combining data and code from third-party sources has enabled a new wave of web mashups that add creativity and functionality to web applications. However, browsers are poorly designed to pass data between domains, often forcing web developers to abandon security in the name of functionality. To address this deficiency, we developed Subspace, a cross-domain communication mechanism that allows efficient communication across domains without sacrificing security. Our prototype requires only a small JavaScript library, and works across all major browsers. We believe Subspace can serve as a new secure communication primitive for web mashups.	Subspace: secure cross-domain communication for web mashups	NA:NA	2018
Andrew Bortz:Dan Boneh	We show that the time web sites take to respond to HTTP requests can leak private information, using two different types of attacks. The first, direct timing, directly measures response times from a web site to expose private information such as validity of an username at a secured site or the number of private photos in a publicly viewable gallery. The second, cross-site timing, enables a malicious web site to obtain information from the user's perspective at another site. For example, a malicious site can learn if the user is currently logged in at a victim site and, in some cases, the number of objects in the user's shopping cart. Our experiments suggest that these timing vulnerabilities are wide-spread. We explain in detail how and why these attacks work, and discuss methods for writing web application code that resists these attacks.	Exposing private information by timing web applications	NA:NA	2018
Ravi Kumar:Jasmine Novak:Bo Pang:Andrew Tomkins	In this paper we study the privacy preservation properties of aspecific technique for query log anonymization: token-based hashing. In this approach, each query is tokenized, and then a secure hash function is applied to each token. We show that statistical techniques may be applied to partially compromise the anonymization. We then analyze the specific risks that arise from these partial compromises, focused on revelation of identity from unambiguous names, addresses, and so forth, and the revelation of facts associated with an identity that are deemed to be highly sensitive. Our goal in this work is two fold: to show that token-based hashing is unsuitable for anonymization, and to present a concrete analysis of specific techniques that may be effective in breaching privacy, against which other anonymization schemes should be measured.	On anonymizing query logs via token-based hashing	NA:NA:NA:NA	2018
Yue Zhang:Jason I. Hong:Lorrie F. Cranor	Phishing is a significant problem involving fraudulent email and web sites that trick unsuspecting users into revealing private information. In this paper, we present the design, implementation, and evaluation of CANTINA, a novel, content-based approach to detecting phishing web sites, based on the TF-IDF information retrieval algorithm. We also discuss the design and evaluation of several heuristics we developed to reduce false positives. Our experiments show that CANTINA is good at detecting phishing sites, correctly labeling approximately 95% of phishing sites.	Cantina: a content-based approach to detecting phishing web sites	NA:NA:NA	2018
Ian Fette:Norman Sadeh:Anthony Tomasic	Each month, more attacks are launched with the aim of making web users believe that they are communicating with a trusted entity for the purpose of stealing account information, logon credentials, and identity information in general. This attack method, commonly known as "phishing," is most commonly initiated by sending out emails with links to spoofed websites that harvest information. We present a method for detecting these attacks, which in its most general form is an application of machine learning on a feature set designed to highlight user-targeted deception in electronic communication. This method is applicable, with slight modification, to detection of phishing websites, or the emails used to direct victims to these sites. We evaluate this method on a set of approximately 860 such phishing emails, and 6950 non-phishing emails, and correctly identify over 96% of the phishing emails while only mis-classifying on the order of 0.1% of the legitimate emails. We conclude with thoughts on the future for such techniques to specifically identify deception, specifically with respect to the evolutionary nature of the attacks and information available.	Learning to detect phishing emails	NA:NA:NA	2018
Dinei Florencio:Cormac Herley	We report the results of a large scale study of password use andpassword re-use habits. The study involved half a million users over athree month period. A client component on users' machines recorded a variety of password strength, usage and frequency metrics. This allows us to measure or estimate such quantities as the average number of passwords and average number of accounts each user has, how many passwords she types per day, how often passwords are shared among sites, and how often they are forgotten. We get extremely detailed data on password strength, the types and lengths of passwords chosen, and how they vary by site. The data is the first large scale study of its kind, and yields numerous other insights into the role the passwords play in users' online experience.	A large-scale study of web password habits	NA:NA	2018
Evan Martin:Tao Xie	To increase confidence in the correctness of specified policies, policy developers can conduct policy testing by supplying typical test inputs (requests) and subsequently checking test outputs (responses) against expected ones. Unfortunately, manual testing is tedious and few tools exist for automated testing of access control policies. We present a fault model for access control policies and a framework to explore it. The framework includes mutation operators used to implement the fault model, mutant generation, equivalent-mutant detection, and mutant-killing determination. This framework allows us to investigate our fault model, evaluate coverage criteria for test generation and selection, and determine a relationship between structural coverage and fault-detection effectiveness. We have implemented the framework and applied it to various policies written in XACML. Our experimental results offer valuable insights into choosing mutation operators in mutation testing and choosing coverage criteria in test generation and selection.	A fault model and mutation testing of access control policies	NA:NA	2018
Vladimir Kolovski:James Hendler:Bijan Parsia	XACML has emerged as a popular access control language on the Web, but because of its rich expressiveness, it has proved difficult to analyze in an automated fashion. In this paper, we present a formalization of XACML using description logics (DL), which are a decidable fragment of First-Order logic. This formalization allows us to cover a more expressive subset of XACML than propositional logic-based analysis tools, and in addition we provide a new analysis service (policy redundancy). Also, mapping XACML to description logics allows us to use off-the-shelf DL reasoners for analysis tasks such as policy comparison, verification and querying. We provide empirical evaluation of a policy analysis tool that was implemented on top of open source DL reasoner Pellet.	Analyzing web access control policies	NA:NA:NA	2018
Jay A. McCarthy:Shriram Krishnamurthi:Joshua D. Guttman:John D. Ramsdell	Cryptographic protocols are useful for trust engineering in Web transactions. The Cryptographic Protocol Programming Language (CPPL) provides a model wherein trust management annotations are attached to protocol actions, and are used to constrain the behavior of a protocol participant to be compatible with its own trust policy. The first implementation of CPPL generated stand-alone, single-session servers, making it unsuitable for deploying protocols on the Web. We describe a new compiler that uses a constraint-based analysis to produce multi-session server programs. The resulting programs run without persistent TCP connections for deployment on traditional Web servers. Most importantly, the compiler preserves existing proofs about the protocols. We present an enhanced version of the CPPL language, discuss the generation and use of constraints, show their use in the compiler, formalize the preservation of properties, present subtleties, and outline implementation details.	Compiling cryptographic protocols for deployment on the web	NA:NA:NA:NA	2018
Fabian M. Suchanek:Gjergji Kasneci:Gerhard Weikum	We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.	Yago: a core of semantic knowledge	NA:NA:NA	2018
Xiang Zhang:Gong Cheng:Yuzhong Qu	Ontology summarization is very important to quick understanding and selection of ontologies. In this paper, we study extractive summarization of ontology. We propose a notion of RDF sentence as the basic unit of summarization. An RDF Sentence Graph is proposed to characterize the links between RDF sentences derived from a given ontology. The salience of each RDF sentence is assessed in terms of its "centrality" in the graph. We propose to summarize an ontology by extracting a set of salient RDF sentences according to a re-ranking strategy. We compare several measurements in assessing the salience of RDF sentences and give an overall evaluation of experiment results, which shows that our approach to ontology summarization is feasible.	Ontology summarization based on rdf sentence graph	NA:NA:NA	2018
Bernardo Cuenca Grau:Ian Horrocks:Yevgeny Kazakov:Ulrike Sattler	The ability to extract meaningful fragments from an ontology is key for ontology re-use. We propose a definition of a module that guarantees to completely capture the meaning of a given set of terms, i.e., to include all axioms relevant to the meaning of these terms, and study the problem of extracting minimal modules. We show that the problem of determining whether a subset of an ontology is a module for a given vocabulary is undecidable even for rather restricted sub-languages of OWL DL. Hence we propose two "approximations", i.e., alternative definitions of modules for a vocabulary that still provide the above guarantee, but that are possibly too strict, and that may thus result in larger modules: the first approximation is semantic and can be computed using existing DL reasoners; the second is syntactic, and can be computed in polynomial time. Finally, we report on an empirical evaluation of our syntactic approximation which demonstrates that the modules we extract are surprisingly small.	Just the right amount: extracting modules from ontologies	NA:NA:NA:NA	2018
Christian Halaschek-Wiener:James Hendler	Syndication systems on the Web have attracted vast amounts of attention in recent years. As technologies have emerged and matured, there has been a transition to more expressive syndication approaches; that is, subscribers and publishers are provided with more expressive means of describing their interests and published content, enabling more accurate information filtering. In this paper, we formalize a syndication architecture that utilizes expressive Web ontologies and logic-based reasoning for selective content dissemination. This provides finer grained control for filtering and automated reasoning for discovering implicit subscription matches, both of which are not achievable in less expressive approaches. We then address one of the main limitations with such a syndication approach, namely matching newly published information with subscription requests in an efficient and practical manner. To this end, we investigate continuous query answering for a large subset of the Web Ontology Language (OWL); specifically, we formally define continuous queries for OWL knowledge bases and present a novel algorithm for continuous query answering in a large subset of this language. Lastly, an evaluation of the query approach is shown, demonstrating its effectiveness for syndication purposes.	Toward expressive syndication on the web	NA:NA	2018
David F. Huynh:David R. Karger:Robert C. Miller	The early Web was hailed for giving individuals the same publishing power as large content providers. But over time, large content providers learned to exploit the structure in their data, leveraging databases and server side technologies to provide rich browsing and visualization. Individual authors fall behind once more: neither old-fashioned static pages nor domain-specific publishing frameworks supporting limited customization can match custom database-backed web applications. In this paper, we propose Exhibit, a lightweight framework for publishing structured data on standard web servers that requires no installation, database administration, or programming. Exhibit lets authors with relatively limited skills-those same enthusiasts who could write HTML pages for the early Web-publish richly interactive pages that exploit the structure of their data for better browsing and visualization. Such structured publishing in turn makes that data more useful to all of its consumers: individual readers get more powerful interfaces, mashup creators can more easily repurpose the data, and Semantic Web enthusiasts can feed the data to the nascent Semantic Web.	Exhibit: lightweight structured data publishing	NA:NA:NA	2018
Jean-Sébastien Brunner:Li Ma:Chen Wang:Lei Zhang:Daniel C. Wolfson:Yue Pan:Kavitha Srinivas	Master data refers to core business entities a company uses repeatedly across many business processes and systems (such as lists or hierarchies of customers, suppliers, accounts, products, or organizational units). Product information is the most important kind of master data and product information management (PIM) is becoming critical for modern enterprises because it provides a rich business context for various applications. Existing PIM systems are less flexible and scalable for on-demand business, as well as too weak to completely capture and use the semantics of master data. This paper explores how to use semantic web technologies to enhance a collaborative PIM system by simplifying modeling and representation while preserving enough dynamic flexibility. Furthermore, we build a semantic PIM system using one of the state-of-art ontology repositories and summarize the challenges we encountered based on our experimental results, especially on performance and scalability. We believe that our study and experiences are valuable for both semantic web community and master data management community.	Explorations in the use of semantic web technologies for product information management	NA:NA:NA:NA:NA:NA:NA	2018
NA	NA	Measuring semantic similarity between words using web search engines	NA	2018
Risto Gligorov:Warner ten Kate:Zharko Aleksovski:Frank van Harmelen	Discovering mappings between concept hierarchies is widely regarded as one of the hardest and most urgent problems facing the Semantic Web. The problem is even harder in domains where concepts are inherently vague and ill-defined, and cannot be given a crisp definition. A notion of approximate concept mapping is required in such domains, but until now, no such notion is vailable. The first contribution of this paper is a definition for approximate mappings between concepts. Roughly, a mapping between two concepts is decomposed into a number of submappings, and a sloppiness value determines the fraction of these submappings that can be ignored when establishing the mapping. A potential problem of such a definition is that with an increasing sloppiness value, it will gradually allow mappings between any two arbitrary concepts. To improve on this trivial behaviour, we need to design a heuristic weighting which minimises the sloppiness required to conclude desirable matches, but at the same time maximises the sloppiness required to conclude undesirable matches. The second contribution of this paper is to show that a Google based similarity measure has exactly these desirable properties. We establish these results by experimental validation in the domain of musical genres. We show that this domain does suffer from ill-defined concepts. We take two real-life genre hierarchies from the Web, we compute approximate mappings between them at varying levels of sloppiness, and we validate our results against a handcrafted Gold Standard. Our method makes use of the huge amount of knowledge that is implicit in the current Web, and exploits this knowledge as a heuristic for establishing approximate mappings between ill-defined concepts.	Using Google distance to weight approximate ontology matches	NA:NA:NA:NA	2018
Yaoyong Li:Kalina Bontcheva	Recent work on ontology-based Information Extraction (IE) has tried to make use of knowledge from the target ontology in order to improve semantic annotation results. However, very few approaches exploit the ontology structure itself, and those that do so, have some limitations. This paper introduces a hierarchical learning approach for IE, which uses the target ontology as an essential part of the extraction process, by taking into account the relations between concepts. The approach is evaluated on the largest available semantically annotated corpus. The results demonstrate clearly the benefits of using knowledge from the ontology as input to the information extraction process. We also demonstrate the advantages of our approach over other state-of-the-art learning systems on a commonly used benchmark dataset.	Hierarchical, perceptron-like learning for ontology-based information extraction	NA:NA	2018
Axel Polleres	As the data and ontology layers of the Semantic Web stack have achieved a certain level of maturity in standard recommendations such as RDF and OWL, the current focus lies on two related aspects. On the one hand, the definition of a suitable query language for RDF, SPARQL, is close to recommendation status within the W3C. The establishment of the rules layer on top of the existing stack on the other hand marks the next step to be taken, where languages with their roots in Logic Programming and Deductive Databases are receiving considerable attention. The purpose of this paper is threefold. First, we discuss the formal semantics of SPARQLextending recent results in several ways. Second, weprovide translations from SPARQL to Datalog with negation as failure. Third, we propose some useful and easy to implement extensions of SPARQL, based on this translation. As it turns out, the combination serves for direct implementations of SPARQL on top of existing rules engines as well as a basis for more general rules and query languages on top of RDF.	From SPARQL to rules (and back)	NA	2018
Kemafor Anyanwu:Angela Maduko:Amit Sheth	Many applications in analytical domains often have the need to "connect the dots" i.e., query about the structure of data. In bioinformatics for example, it is typical to want to query about interactions between proteins. The aim of such queries is to "extract" relationships between entities i.e. paths from a data graph. Often, such queries will specify certain constraints that qualifying results must satisfy e.g. paths involving a set of mandatory nodes. Unfortunately, most present day Semantic Web query languages including the current draft of the anticipated recommendation SPARQL, lack the ability to express queries about arbitrary path structures in data. In addition, many systems that support some limited form of path queries rely on main memory graph algorithms limiting their applicability to very large scale graphs. In this paper, we present an approach for supporting Path Extraction queries. Our proposal comprises (i) a query language SPARQ2L which extends SPARQL with path variables and path variable constraint expressions, and (ii) a novel query evaluation framework based on efficient algebraic techniques for solving path problems which allows for path queries to be efficiently evaluated on disk resident RDF graphs. The effectiveness of our proposal is demonstrated by a performance evaluation of our approach on both real world based and synthetic dataset.	SPARQ2L: towards support for subgraph extraction queries in rdf databases	NA:NA:NA	2018
Boris Motik:Ian Horrocks:Ulrike Sattler	Schema statements in OWL are interpreted quite differently from analogous statements in relational databases. If these statements are meant to be interpreted as integrity constraints (ICs), OWL's interpretation may seem confusing and/or inappropriate. Therefore, we propose an extension of OWL with ICs that captures the intuition behind ICs in relational databases. We discuss the algorithms for checking IC satisfaction for different types of knowledge bases, and show that, if the constraints are satisfied, we can disregard them while answering a broad range of positive queries.	Bridging the gap between OWL and relational databases	NA:NA:NA	2018
Eyal Oren:Renaud Delbru:Sebastian Gerke:Armin Haller:Stefan Decker	Object-oriented programming is the current mainstream programming paradigm but existing RDF APIs are mostly triple-oriented. Traditional techniques for bridging a similar gap between relational databases and object-oriented programs cannot be applied directly given the different nature of Semantic Web data, for example in the semantics of class membership, inheritance relations, and object conformance to schemas. We present ActiveRDF, an object-oriented API for managing RDF data that offers full manipulation and querying of RDF data, does not rely on a schema and fully conforms to RDF(S) semantics. ActiveRDF can be used with different RDF data stores: adapters have been implemented to generic SPARQL endpoints, Sesame, Jena, Redland and YARS and new adapters can be added easily. In addition, integration with the popular Ruby on Rails framework enables rapid development of Semantic Web applications.	ActiveRDF: object-oriented semantic web programming	NA:NA:NA:NA:NA	2018
Anupriya Ankolekar:Markus Krötzsch:Thanh Tran:Denny Vrandecic	A common perception is that there are two competing visions for the future evolution of the Web: the Semantic Web and Web 2.0. A closer look, though, reveals that the core technologies and concerns of these two approaches are complementary and that each field can and must draw from the other's strengths. We believe that future web applications will retain the Web 2.0 focus on community and usability, while drawing on Semantic Web infrastructure to facilitate mashup-like information sharing. However, there are several open issues that must be addressed before such applications can become commonplace. In this paper, we outline a semantic weblogs scenario that illustrates the potential for combining Web 2.0 and Semantic Web technologies, while highlighting the unresolved issues that impede its realization. Nevertheless, we believe that the scenario can be realized in the short-term. We point to recent progress made in resolving each of the issues as well as future research directions for each of the communities.	The two cultures: mashing up web 2.0 and the semantic web	NA:NA:NA:NA	2018
Yong-Yeol Ahn:Seungyeop Han:Haewoon Kwak:Sue Moon:Hawoong Jeong	Social networking services are a fast-growing business in the Internet. However, it is unknown if online relationships and their growth patterns are the same as in real-life social networks. In this paper, we compare the structures of three online social networking services: Cyworld, MySpace, and orkut, each with more than 10 million users, respectively. We have access to complete data of Cyworld's ilchon (friend) relationships and analyze its degree distribution, clustering property, degree correlation, and evolution over time. We also use Cyworld data to evaluate the validity of snowball sampling method, which we use to crawl and obtain partial network topologies of MySpace and orkut. Cyworld, the oldest of the three, demonstrates a changing scaling behavior over time in degree distribution. The latest Cyworld data's degree distribution exhibits a multi-scaling behavior, while those of MySpace and orkut have simple scaling behaviors with different exponents. Very interestingly, each of the two e ponents corresponds to the different segments in Cyworld's degree distribution. Certain online social networking services encourage online activities that cannot be easily copied in real life; we show that they deviate from close-knit online social networks which show a similar degree correlation pattern to real-life social networks.	Analysis of topological characteristics of huge online social networking services	NA:NA:NA:NA:NA	2018
Paul - Alexandru Chirita:Stefania Costache:Wolfgang Nejdl:Siegfried Handschuh	The success of the Semantic Web depends on the availability of Web pages annotated with metadata. Free form metadata or tags, as used in social bookmarking and folksonomies, have become more and more popular and successful. Such tags are relevant keywords associated with or assigned to a piece of information (e.g., a Web page), describing the item and enabling keyword-based classification. In this paper we propose P-TAG, a method which automatically generates personalized tags for Web pages. Upon browsing a Web page, P-TAG produces keywords relevant both to its textual content, but also to the data residing on the surfer's Desktop, thus expressing a personalized viewpoint. Empirical evaluations with several algorithms pursuing this approach showed very promising results. We are therefore very confident that such a user oriented automatic tagging approach can provide large scale personalized metadata annotations as an important step towards realizing the Semantic Web.	P-TAG: large scale automatic generation of personalized annotation tags for the web	NA:NA:NA:NA	2018
Sarita Seshagiri:Sagar Aman:Dhaval Joshi	This paper is based on our exploratory study of a South Indian village in Chamrajanagar district of Karnataka. The study was to understand the rural communication environment and villagers. communication preferences. We examined people's lifestyle, working conditions and their communication eco-system. Our study revealed that villagers, unlike urban inhabitants, interacted with people outside the village only for specific, rather than casual purposes. Another interesting aspect of rural communication was the marginal use of the postal system and the ubiquitous use of pay phone, apart from word of mouth and face-to-face interactions. In fact, personal (face-to-face) interaction was usually preferred among villages in this region, over other kinds of communication, despite infrastructural constraints like poor transport services. We also observed that communication frequency increased when status quo changed to one that required immediate attention. During the analysis we identified certain social, economic and cultural communication gaps (or problems). However, these problems were clear opportunities to connect the unconnected rural users, by deploying new communication systems and features. Here, we have highlighted some of our findings and possible design avenues based on these findings.	Connecting the "bottom of the pyramid": an exploratory case study of india's rural communication environment	NA:NA:NA	2018
Beth E. Kolko:Emma J. Rose:Erica J. Johnson	In this paper, we describe several findings from a multi-year, multi-method study of how information and communication technologies have been adopted and adapted in Central Asia. We have found that mobile phone usage is outpacing the rate of Internet adoption, that access to the Internet is primarily through public access sites carrying with it issues regarding privacy and surveillance, that people rely on their social networks as information sources, that public institutions tend to be fairly weak as citizen resources, and that information seeking and communication are conflated in people's usage patterns with different technologies. In addition, in the developed world social networking software has grown rapidly and shown itself to have significant potential for mobilizing a population. Based on the collection of findings from Central Asia and observing patterns of technology usage in other parts of the world, our research leads to the conclusion that exploring mobile social software holds significant potential as an ICT that meshes well with preexisting patterns of communication and information seeking and also leverages the most predominant pattern of technology adoption. Many of the findings from this research echo results from studies in other geographic areas, and so we anticipate that much of this research will be relevant to developing regions generally.	Communication as information-seeking: the case for mobile social software for developing regions	NA:NA:NA	2018
Indrani Medhi:Archana Prasad:Kentaro Toyama	We present research leading toward an understanding of the optimal audio-visual representation for illustrating concepts for illiterate and semi-literate users of computers. In our user study, which to our knowledge is the first of its kind, we presented to 200 illiterate subjects each of 13 different health symptoms in one representation randomly selected among the following ten: text, static drawings, staticphotographs, hand-drawn animations, and video, each with and without voice annotation. The goal was to see how comprehensible these representation types were for an illiterate audience. We used a methodology for generating each of the representations tested in a way that fairly stacks one representational type against the others. Our main results are that (1) voice annotation generally helps in speed of comprehension, but bimodal audio-visual information can be confusing for the target population; (2) richer information is not necessarily better understood overall; (3) the relative value of dynamic imagery versus static imagery depends on various factors. Analysis of these statistically significant results and additional detailed results are also provided.	Optimal audio-visual representations for illiterate users of computers	NA:NA:NA	2018
Jeffrey Erman:Anirban Mahanti:Martin Arlitt:Carey Williamson	Traffic classification is the ability to identify and categorize network traffic by application type. In this paper, we consider the problem of traffic classification in the network core.Classification at the core is challenging because only partial information about the flows and their contributors is available. We address this problem by developing a framework that can classify a flow using only unidirectional flow information. We evaluated this approach using recent packet traces that we collected and pre-classified to establish a "base truth". From our evaluation, we find that flow statistics for the server-to-client direction of a TCP connection provide greater classification accuracy than the flow statistics for the client-to-server direction. Because collection of the server-to-client flow statistics may not always be feasible, we developed and validated an algorithm that can estimate the missing statistics froma unidirectional packet trace.	Identifying and discriminating between web and peer-to-peer traffic in the network core	NA:NA:NA:NA	2018
Sayandeep Sen:Bhaskaran Raman	Several research efforts as well as deployments have chosen IEEE802.11 as a low-cost, long-distance access technology to bridge the digital divide. In this paper, we consider the important issue of planning such networks to the minimize system cost. This is a non-trivial task since it involves several sets of variables: the network topology, tower heights, antenna types to be used and the irorientations, and radio transmit powers. The task is further complicated due to the presence of network performance constraints, and the inter-dependence among the variables. Our first contribution in this paper is the formulation of this problem in terms of the variables, constraints and the optimization criterion. Our second contribution is in identifying the dependencies among the variables and breaking-down the problem into four tractable sub-parts. In this process, we extensively use domain knowledge to strike a balance between tractability and practicality. We have evaluated the proposed algorithms using random input sets as well as real-life instances with success. We have been able to show detailed planning of network topology, required tower heights, antenna types, and transmit powers for the Ashwini project, a long distance WiFi network under deployment in Andhra Pradesh, India, In this case, we are able to achieve within 2% additional cost of a lower bound estimate.	Long distance wireless mesh network planning: problem formulation and solution	NA:NA	2018
Siddhartha Annapureddy:Saikat Guha:Christos Gkantsidis:Dinan Gunawardena:Pablo Rodriguez Rodriguez	Peer-to-peer technologies are increasingly becoming the medium of choice for deliveringmedia content, both professional and home-grown, to large user populations. Indeed, current P2P swarming systems have been shown to be very efficient for large-scale content distribution with few server resources.However, such systems have been designed for generic file distribution and provide a limited user experience for viewing media content.For example, users need to wait to download the full video before they can start watching it.In general, the main challenge resides in designing systems that ensure that users can start watching a movie at any point in time, with small start-up times and sustainable playback rates. In this work, we address the issues of providing a Video-on-Demand (VoD) using P2P mesh-based networks. We show that providing high quality VoD using P2P is feasible using a combination of techniquesincluding (a) network coding, (b) optimized resource allocation across different parts of the video, and (c) overlay topology management algorithms.Our evaluation also shows that systems that do not use these techniques and do not optimize all of those dimensions can significantly under-utilize the network resources and result in poor VoD performance.Our results are based on simulations and results from a prototype implementation.	Is high-quality vod feasible using P2P swarming?	NA:NA:NA:NA:NA	2018
Oscar Díaz:Salvador Trujillo:Sandy Pérez	Portlets strive to play at the front end the same role that Web services currently enjoy at the back end, namely, enablers of application assembly through reusable services. However, it is well-known in the component community that, the larger the component, the more reduced the reuse. Hence, the coarse-grained nature of portlets (they encapsulate also the presentation layer) can jeopardize this vision of portlets as reusable services. To avoid this situation, this work proposes a perspective shift in portlet development by introducing the notion of Consumer Profile. While the user profile characterizes the end user (e.g. age, name, etc), the Consumer Profile captures the idiosyncrasies of the organization through which the portlet is being delivered (e.g. the portal owner) as far as the portlet functionality is concerned. The user profile can be dynamic and hence, requires the portlet to be customized at runtime. By contrast, the Consumer Profile is known at registration time, and it is not always appropriate/possible to consider it at runtime. Rather, it is better to customize the code at development time, and produce an organization-specific portlet which built-in, custom functionality. In this scenario, we no longer have a portlet but a family of portlets, and the portlet provider becomes the "assembly line" of this family. This work promotes this vision by introducing an organization-aware, WSRPcompliant architecture that let portlet consumers registry and handle "family portlets" in the same way that "traditional portlets". In so doing, portlets are nearer to become truly reusable services.	Turning portlets into services: the consumer profile	NA:NA:NA	2018
Jin Yu:Boualem Benatallah:Regis Saint-Paul:Fabio Casati:Florian Daniel:Maristella Matera	The development of user interfaces (UIs) is one of the most time-consuming aspects in software development. In this context, the lack of proper reuse mechanisms for UIs is increasingly becoming manifest, especially as software development is more and more moving toward composite applications. In this paper we propose a framework for the integration of stand-alone modules or applications, where integration occurs at the presentation layer. Hence, the final goal is to reduce the effort required for UI development by maximizing reus. The design of the framework is inspired by lessons learned from application integration, appropriately modified to account for the specificity of the UI integration problem. We provide an abstract component model to specify characteristics and behaviors of presentation components and propose an event-based composition model to specify the composition logic. Components and composition are described by means of a simple XML-based language, which is interpreted by a runtime middleware for the execution of the resulting composite application. A proof-of-concept prototype allows us to show that the proposed component model can also easily be applied to existing presentation components, built with different languages and/or component technologies.	A framework for rapid integration of presentation components	NA:NA:NA:NA:NA:NA	2018
Farooque Azam:Zhang Li:Rashid Ahmad	Requirement engineering (RE) is emerging as an increasingly important discipline for supporting Web application development, as these are designed to satisfy diverse stakeholder needs, additional functional, information, multimedia and usability requirements as compared to traditional software applications. Moreover, when considering innovative e-commerce applications, value-based RE is an extremely relevant methodology which exploits the concept of economic value during the RE activity. In contrast, most of the methodologies proposed for the development of Web applications, primarily focus on the system design, and paying less attention to the RE, and specifically to value-based RE. Focusing this aspect, the paper presents integration of value-based RE models to WebML models using our recently proposed VIP Business Modeling Framework [1]. We also analyze the framework's potential in linking other modeling approaches, and argue about its significant integration potential with various E-R/OO-based, process aware Web modeling approaches.	Integrating value-based requirement engineering models to webml using vip business modeling framework	NA:NA:NA	2018
Rui Li:Shenghua Bao:Yong Yu:Ben Fei:Zhong Su	This paper is concerned with the problem of browsing social annotations. Today, a lot of services (e.g., Del.icio.us, Filckr) have been provided for helping users to manage and share their favorite URLs and photos based on social annotations. Due to the exponential increasing of the social annotations, more and more users, however, are facing the problem how to effectively find desired resources from large annotation data. Existing methods such as tag cloud and annotation matching work well only on small annotation sets. Thus, an effective approach for browsing large scale annotation sets and the associated resources is in great demand by both ordinary users and service providers. In this paper, we propose a novel algorithm, namely Effective Large Scale Annotation Browser (ELSABer), to browse large-scale social annotation data. ELSABer helps the users browse huge number of annotations in a semantic, hierarchical and efficient way. More specifically, ELSABer has the following features: 1) the semantic relations between annotations are explored for browsing of similar resources; 2) the hierarchical relations between annotations are constructed for browsing in a top-down fashion; 3) the distribution of social annotations is studied for efficient browsing. By incorporating the personal and time information, ELSABer can be further extended for personalized and time-related browsing. A prototype system is implemented and shows promising results.	Towards effective browsing of large scale social annotations	NA:NA:NA:NA:NA	2018
Sandeep Lingam:Sebastian Elbaum	Web authoring environments enable end-users to create applications that integrate information from other web sources. Users can create web sites that include built-in components to dynamically incorporate, for example, weather information, stock-quotes, or the latest news from different web sources. Recent surveys conducted among end-users have indicated an increasing interest in creating such applications. Unfortunately, web authoring environments do not provide support beyond a limited set of built-in components. This work addresses this limitation by providing end-user support for "clipping" information from a target web site to incorporate it into the end-user site. The support consists of a mechanism to identify the target clipping with multiple markers to increase robustness, and a dynamic assessment of the retrieved information to quantify its reliability. The clipping approach has been integrated as a feature into a popular web authoring tool on which we present the results of two preliminary studies.	Supporting end-users in the creation of dependable web clips	NA:NA	2018
Emilia Mendes:Sergio Di Martino:Filomena Ferrucci:Carmine Gravino	Previous studies comparing the prediction accuracy of effort models built using Web cross- and single-company data sets have been inconclusive, and as such replicated studies are necessary to determine under what circumstances a company can place reliance on a cross-company effort model. This paper therefore replicates a previous study by investigating how successful a cross-company effort model is: i) to estimate effort for Web projects that belong to a single company and were not used to build the cross-company model; ii) compared to a single-company effort model. Our single-company data set had data on 15 Web projects from a single company and our cross-company data set had data on 68 Web projects from 25 different companies. The effort estimates used in our analysis were obtained by means of two effort estimation techniques, namely forward stepwise regression and case-based reasoning. Our results were similar to those from the replicated study, showing that predictions based on the single-company model were significantly more accurate than those based on the cross-company model.	Effort estimation: how valuable is it for a web company to use a cross-company data set, compared to using its own single-company data set?	NA:NA:NA:NA	2018
Zongyan Qiu:Xiangpeng Zhao:Chao Cai:Hongli Yang	With the growth of interest on the web services, people pay increasinglyattention to the choreography, that is, to describe collaborations ofparticipants in accomplishing a common business goal from a globalviewpoint. In this paper, based on a simple choreography language and arole-oriented process language, we study some fundamental issues relatedto choreography, especially those related to implementation, includingsemantics, projection and natural projection, dominant role in choices anditerations, etc. We propose the concept of dominant role and somenovel languages structures related to it. The study reveals some cluesabout the language, the semantics, the specification and theimplementation of choreography.	Towards the theoretical foundation of choreography	NA:NA:NA:NA	2018
Daniel James Goodman	The workflow language Martlet described in this paper implements a new programming model that allows users to write parallel programs and analyse distributed data without having to be aware of the details of the parallelisation. Martlet abstracts the parallelisation of the computation and the splitting of the data through the inclusion of constructs inspired by functional programming. These allow programs to be written as an abstract description that can be adjusted automatically at runtime to match the data set and available resources. Using this model it is possible to write programs to perform complex calculations across a distributed data set such as Singular Value Decomposition or Least Squares problems, as well as creating an intuitive way of working with distributed system. Having described and evaluated Martlet against other functional languages for parallel computation, this paper goes on to look at how Martlet might develop. In doing so it covers both possible additions to the language itself, and the use of JIT compilers to increase the range of platforms it is capable of running on.	Introduction and evaluation of Martlet: a scientific workflow language for abstracted parallelisation	NA	2018
Hamid Reza Motahari Nezhad:Boualem Benatallah:Axel Martens:Francisco Curbera:Fabio Casati	In today's Web, many functionality-wise similar Web services are offered through heterogeneous interfaces (operation definitions) and business protocols (ordering constraints defined on legal operation invocation sequences). The typical approach to enable interoperation in such a heterogeneous setting is through developing adapters. There have been approaches for classifying possible mismatches between service interfaces and business protocols to facilitate adapter development. However, the hard job is that of identifying, given two service specifications, the actual mismatches between their interfaces and business protocols. In this paper we present novel techniques and a tool that provides semi-automated support for identifying and resolution of mismatches between service interfaces and protocols, and for generating adapter specification. We make the following main contributions: (i) we identify mismatches between service interfaces, which leads to finding mismatches of type of signature, merge/split, and extra/missing messages; (ii) we identify all ordering mismatches between service protocols and generate a tree, called mismatch tree, for mismatches that require developers' input for their resolution. In addition, we provide semi-automated support in analyzing the mismatch tree to help in resolving such mismatches. We have implemented the approach in a tool inside IBM WID (WebSphere Integration Developer). Our experiments with some real-world case studies show the viability of the proposed approach. The methods and tool are significant in that they considerably simplify the problem of adapting services so that interoperation is possible.	Semi-automated adaptation of service interactions	NA:NA:NA:NA:NA	2018
Radu Jurca:Boi Faltings:Walter Binder	Service-level agreements (SLAs) establish a contract between service providersand clients concerning Quality of Service (QoS) parameters. Without properpenalties, service providers have strong incentives to deviate from theadvertised QoS, causing losses to the clients. Reliable QoS monitoring (andproper penalties computed on the basis of delivered QoS) are thereforeessential for the trustworthiness of a service-oriented environment. In thispaper, we present a novel QoS monitoring mechanism based on quality ratings from theclients. A reputation mechanism collects the ratings and computes theactual quality delivered to the clients. The mechanism provides incentives forthe clients to report honestly, and pays special attention to minimizing costand overhead1.	Reliable QoS monitoring based on client feedback	NA:NA:NA	2018
Steffen Lamparter:Anupriya Ankolekar:Rudi Studer:Stephan Grimm	A key challenge for dynamic Web service selection is that Web services are typically highly configurable and service requesters often have dynamic preferences on service configurations. Current approaches, such as WS-Agreement, describe Web services by enumerating the various possible service configurations, an inefficient approach when dealing with numerous service attributes with large value spaces. We model Web service configurations and associated prices and preferences more compactly using utility function policies, which also allows us to draw from multi-attribute decision theory methods to develop an algorithm for optimal service selection. In this paper, we present an OWL ontology for the specification of configurable Web service offers and requests, and a flexible and extensible framework for optimal service selection that combines declarative logic-based matching rules with optimization methods, such as linear programming. Assuming additive price/preference functions, experimental results indicate that our algorithm introduces an overhead of only around 2 sec.~compared to random service selection, while giving optimal results. The overhead, as percentage of total time, decreases as the number of offers and configurations increase.	Preference-based selection of highly configurable web services	NA:NA:NA:NA	2018
John Harney:Prashant Doshi	Web processes must often operate in volatile environments where the quality of service parameters of the participating service providers change during the life time of the process. In order to remain optimal, the Web process must adapt to these changes. Adaptation requires knowledge about the parameter changes of each of the service providers and using this knowledge to determine whether the Web process should make a different more optimal decision. Previously, we defined a mechanism called the value of changed information which measures the impact of expected changes in the service parameters on the Web process, thereby offering a way to query and incorporate those changes that are useful and cost-efficient. However, computing the value of changed information incurs a substantial computational overhead. In this paper, we use service expiration times obtained from pre-defined service level agreements to reduce the computational overhead of adaptation. We formalize the intuition that services whose parameters have not expired need not be considered for querying for revised information. Using two realistic scenarios, we illustrate our approach and demonstrate the associated computational savings.	Speeding up adaptation of web service compositions using expiration times	NA:NA	2018
Ulrich Küster:Birgitta König-Ries:Mirco Stern:Michael Klein	Automated matching of semantic service descriptions is the key to automatic service discovery and binding. But when trying to find a match for a certain request it may often happen, that the request cannot be serviced by a single offer but could be handled by combining existing offers. In this case automatic service composition is needed. Although automatic composition is an active field of research it is mainly viewed as a planning problem and treated separatedly from service discovery. In this paper we argue that an integrated approach to the problem is better than seperating these issues as is usually done. We propose an approach that integrates service composition into service discovery and matchmaking to match service requests that ask for multiple connected effects, discuss general issues involved in describing and matching such services and present an efficient algorithm implementing our ideas.	DIANE: an integrated approach to automated service discovery, matchmaking and composition	NA:NA:NA:NA	2018
Chong Sun:Chee-Yong Chan:Amit K. Goenka	Keyword search for smallest lowest common ancestors (SLCAs)in XML data has recently been proposed as a meaningful way to identify interesting data nodes inXML data where their subtrees contain an input set of keywords. In this paper, we generalize this useful search paradigm to support keyword search beyond the traditional AND semantics to include both AND and OR boolean operators as well. We first analyze properties of the LCA computation and propose improved algorithms to solve the traditional keyword search problem (with only AND semantics). We then extend our approach to handle general keyword search involving combinations of AND and OR boolean operators. The effectiveness of our new algorithms is demonstrated with a comprehensive experimental performance study.	Multiway SLCA-based keyword search in XML data	NA:NA:NA	2018
Viraj Kumar:P. Madhusudan:Mahesh Viswanathan	We propose the study of visibly pushdown automata (VPA) for processing XML documents. VPAs are pushdown automata where the input determines the stack operation, and XML documents are naturally visibly pushdown with the VPA pushing onto the stack on open-tags and popping the stack on close-tags. In this paper we demonstrate the power and ease visibly pushdown automata give in the design of streaming algorithms for XML documents. We study the problems of type-checking streaming XML documents against SDTD schemas, and the problem of typing tags in a streaming XML document according to an SDTD schema. For the latter problem, we consider both pre-order typing and post-order typing of a document, which dynamically determines types at open-tags and close-tags respectively as soon as they are met. We also generalize the problems of pre-order and post-order typing to prefix querying. We show that a deterministic VPA yields an algorithm to the problem of answering in one pass the set of all answers to any query that has the property that a node satisfying the query is determined solely by the prefix leading to the node. All the streaming algorithms we develop in this paper are based on the construction of deterministic VPAs, and hence, for any fixed problem, the algorithms process each element of the input in constant time, and use space (d), where d is the depth of the document.	Visibly pushdown automata for streaming XML	NA:NA:NA	2018
Haifeng Jiang:Howard Ho:Lucian Popa:Wook-Shin Han	Clio is an existing schema-mapping tool that provides user-friendly means to manage and facilitate the complex task of transformation and integration of heterogeneous data such as XML over the Web or in XML databases. By means of mappings from source to target schemas, Clio can help users conveniently establish the precise semantics of data transformation and integration. In this paper we study the problem of how to efficiently implement such data transformation (i.e., generating target data from the source data based on schema mappings). We present a three-phase framework for high-performance XML-to-XML transformation based on schema mappings, and discuss methodologies and algorithms for implementing these phases. In particular, we elaborate on novel techniques such as streamed extraction of mapped source values and scalable disk-based merging of overlapping data (including duplicate elimination). We compare our transformation framework with alternative methods such as using XQuery or SQL/XML provided by current commercial databases. The results demonstrate that the three-phase framework (although as simple as it is) is highly scalable and outperforms the alternative methods by orders of magnitude.	Mapping-driven XML transformation	NA:NA:NA:NA	2018
Raymond K. Wong:Franky Lam:William M. Shui	As XML database sizes grow, the amount of space used for storing the data and auxiliary data structures becomes a major factor in query and update performance. This paper presents a new storage scheme for XML data that supports all navigational operations in near constant time. In addition to supporting efficient queries, the space requirement of the proposed scheme is within a constant factor of the information theoretic minimum, while insertions and deletions can be performed in near constant time as well. As a result, the proposed structure features a small memory footprint that increases cache locality, whilst still supporting standard APIs, such as DOM, and necessary database operations, such as queries and updates, efficiently. Analysis and experiments show that the proposed structure is space and time efficient.	Querying and maintaining a compact XML storage	NA:NA:NA	2018
Solmaz Kolahi:Leonid Libkin	Design principles for XML schemas that eliminate redundancies and avoid update anomalies have been studied recently. Several normal forms, generalizing those for relational databases, have been proposed. All of them, however, are based on the assumption of anative XML storage, while in practice most of XML data is stored inrelational databases. In this paper we study XML design and normalization for relational storage of XML documents. To be able to relate and compare XML and relational designs, we use an information-theoretic framework that measures information content in relations and documents, with higher values corresponding to lower levels of redundancy. We show that most common relational storage schemes preserve the notion of being well-designed (i.e., anomalies- and redundancy-free). Thus,existing XML normal forms guarantee well-designed relational storagesas well. We further show that if this perfect option is not achievable, then a slight restriction on XML constraints guarantees a "second-best" relational design, according to possible values of the information-theoretic measure. We finally consider an edge-based relational representation of XML documents, and show that while it has similar information-theoretic properties with other relational representations, it can behave significantly worse in terms of enforcing integrity constraints.	XML design for relational storage	NA:NA	2018
Morris Matsa:Eric Perkins:Abraham Heifets:Margaret Gaitatzes Kostoulas:Daniel Silva:Noah Mendelsohn:Michelle Leger	XML delivers key advantages in interoperability due to its flexibility, expressiveness, and platform-neutrality. As XML has become a performance-critical aspect of the next generation of business computing infrastructure, however, it has become increasingly clear that XML parsing often carries a heavy performance penalty, and that current, widely-used parsing technologies are unable to meet the performance demands of an XML-based computing infrastructure. Several efforts have been made to address this performance gap through the use of grammar-based parser generation. While the performance of generated parsers has been significantly improved, adoption of the technology has been hindered by the complexity of compiling and deploying the generated parsers. Through careful analysis of the operations required for parsing and validation, we have devised a set of specialized byte codes, designed for the task of XML parsing and validation. These byte codes are designed to engender the benefits of fine-grained composition of parsing and validation that make existing compiled parsers fast, while being coarse-grained enough to minimize interpreter overhead. This technique of using an interpretive,validating parser balances the need for performance against the requirements of simple tooling and robust scalable infrastructure. Our approach is demonstrated with a specialized schema compiler, used to generate byte codes which in turn drive an interpretive parser. With almost as little tooling and deployment complexity as a traditional interpretive parser, the byte code-driven parser usually demonstrates performance within 20% of the fastest fully compiled solutions.	A high-performance interpretive approach to schema-directed parsing	NA:NA:NA:NA:NA:NA:NA	2018
Soumya Roy:Shantanu Biswas	Indian business clusters have contributed immensely to the country's industrial output, poverty alleviation and employment generation. However, with recent globalization these clusters can loose out to international competitors if they do not continuously innovate and take advantage of the new opportunities that are available through economic liberalization. In this paper, we discuss how information and communication technologies (ICT) can help in improving the productivity and growth of these clusters.	Collaborative ICT for Indian business clusters	NA:NA	2018
Saurabh Sahni:Krithi Ramamritham	With the explosive growth and spread of Internet, web access from mobile and rural users has become significant. But these users face problems of low bandwidth and intermittent Internet connectivity. To make the benefits of the Internet reach the common man in developing countries, accessibility and availability of the information has to be improved. aAQUA is an online multilingual, multimedia agricultural portal for disseminating information from and to rural communities. Considering resource constrained rural environments, we have designed and implemented an offline solution which provides an online experience to users in disconnected mode. Our solution is based on heterogeneous database synchronization which involves only a small synchronization payload ensuring an efficient use of available bandwidth. Offline aAQUA has been deployed in the field and systematic studies of our solution show that user experience has improved tremendously not only in disconnected mode but also in connected mode.	Delay tolerant applications for low bandwidth and intermittently connected users: the aAQUA experience	NA:NA	2018
Lan Nie:Baoning Wu:Brian D. Davison	This work proposes a novel cautious surfer to incorporate trust into the process of calculating authority for web pages. We evaluate a total of sixty queries over two large, real-world datasets to demonstrate that incorporating trust can improve PageRank's performance.	A cautious surfer for PageRank	NA:NA:NA	2018
Levent Bolelli:Seyda Ertekin:Ding Zhou:C. Lee Giles	Traditional clustering algorithms work on "flat" data, making the assumption that the data instances can only be represented by a set of homogeneous and uniform features. Many real world data, however, is heterogeneous in nature, comprising of multiple types of interrelated components. We present a clustering algorithm, K-SVMeans, that integrates the well known K-Means clustering with the highly popular Support Vector Machines(SVM) in order to utilize the richness of data. Our experimental results on authorship analysis of scientific publications show that K-SVMeans achieves better clustering performance than homogeneous data clustering.	A clustering method for web data with multi-type interrelated components	NA:NA:NA:NA	2018
Yang Sun:Ziming Zhuang:C. Lee Giles	Search engines largely rely on Web robots to collect information from the Web. Due to the unregulated open-access nature of the Web, robot activities are extremely diverse. Such crawling activities can be regulated from the server side by deploying the Robots Exclusion Protocol in a file called robots.txt. Although it is not an enforcement standard, ethical robots (and many commercial) will follow the rules specified in robots.txt. With our focused crawler, we investigate 7,593 websites from education, government, news, and business domains. Five crawls have been conducted in succession to study the temporal changes. Through statistical analysis of the data, we present a survey of the usage of Web robots rules at the Web scale. The results also show that the usage of robots.txt has increased over time.	A large-scale study of robots.txt	NA:NA:NA	2018
Tony Abou-Assaleh:Tapajyoti Das:Weizheng Gao:Yingbo Miao:Philip O'Brien:Zhen Zhen	This paper introduces a novel link-based ranking algorithm based on a model of focused Web surfers. FocusedRank is described and compared to implementations of PageRank and Topic-Sensitive PageRank. We report a user study that measures the relevance and precision of each approach. FocusedRank gives superior relevancy over PageRank, while significantly reducing the computational complexity compared to the Topic-Senstivice PageRank.	A link-based ranking scheme for focused search	NA:NA:NA:NA:NA:NA	2018
Nan Liu:Christopher C. Yang	Hierarchical models are commonly used to organize a Website's content. A Website's content structure can be represented by a topic hierarchy, a directed tree rooted at a Website's homepage in which the vertices and edges correspond to Web pages and hyperlinks. In this work, we propose a new method for constructing the topic hierarchy of a Website. We model the Website's link structure using weighted directed graph, in which the edge weights are computed using a classifier that predicts if an edge connects a pair of nodes representing a topic and a sub-topic. We then pose the problem of building the topic hierarchy as finding the shortest-path tree and directed minimum spanning tree in the weighted graph. We've done extensive experiments using real Websites and obtained very promising results.	A link classification based approach to website topic hierarchy generation	NA:NA	2018
Xin-Jing Wang:Yong Qin:Wen Liu	In this paper, we propose a novel Chinese word segmentation method which leverages the huge deposit of Web documents and search technology. It simultaneously solves ambiguous phrase boundary resolution and unknown word identification problems. Evaluations prove its effectiveness.	A search-based Chinese word segmentation method	NA:NA:NA	2018
Amruta Joshi:Ravi Kumar:Benjamin Reed:Andrew Tomkins	We present a family of measures of proximity of an arbitrary node in a directed graph to a pre-specified subset of nodes, called the anchor. Our measures are based on three different propagation schemesand two different uses of the connectivity structure of the graph. We consider a web-specific application of the above measures with two disjoint anchors - good and bad web pages - and study the accuracy of these measures in this context.	Anchor-based proximity measures	NA:NA:NA:NA	2018
Yiqun Liu:Yupeng Fu:Min Zhang:Shaoping Ma:Liyun Ru	Performance evaluation is an important issue in Web search engine researches. Traditional evaluation methods rely on much human efforts and are therefore quite time-consuming. With click-through data analysis, we proposed an automatic search engine performance evaluation method. This method generates navigational type query topics and answers automatically based on search users. querying and clicking behavior. Experimental results based on a commercial Chinese search engine's user logs show that the automatically method gets a similar evaluation result with traditional assessor-based ones.	Automatic search engine performance evaluation with click-through data analysis	NA:NA:NA:NA:NA	2018
Ying Liu:Kun Bai:Prasenjit Mitra:C. Lee Giles	Tables are ubiquitous. Unfortunately, no search engine supportstable search. In this paper, we propose a novel table specificsearching engine, TableSeer, to facilitate the table extracting, indexing, searching, and sharing. In addition, wepropose an extensive set of medium-independent metadata to precisely present tables. Given a query, TableSeer ranks the returned results using an innovative ranking algorithm - TableRank with a tailored vector space model and a novel term weightingscheme. Experimental results show that TableSeer outperforms existing search engines on table search. In addition, incorporating multiple weighting factors can significantly improve the ranking results.	Automatic searching of tables in digital libraries	NA:NA:NA:NA	2018
Keke Cai:Jiajun Bu:Chun Chen:Kangmiao Liu:Wei Chen	This paper makes an intensive investigation of the application of Bayesian network in sentence retrieval and introduces three Bayesian network based sentence retrieval models with or without consideration of term relationships. Term relationships in this paper are considered from two perspectives: relationships between pairs of terms and relationships between terms and term sets. Experiments have proven the efficiency of Bayesian network in the application of sentence retrieval. Particularly, retrieval result with consideration of the second kind of term relationship performs better in improving retrieval precision.	Bayesian network based sentence retrieval model	NA:NA:NA:NA:NA	2018
Bernard J. Jansen:Mimi Zhang:Ying Zhang	We investigate the effect of search engine brand (i.e., the identifying name or logo that distinguishes a product from its competitors) on evaluation of system performance. This research is motivated by the large amount of search traffic directed to a handful of Web search engines, even though most are of equal technical quality with similar interfaces. We conducted a laboratory study with 32 participants to measure the effect of four search engine brands while controlling for the quality of search engine results. There was a 25% difference between the most highly rated search engine and the lowest using average relevance ratings, even though search engine results were identical in both content and presentation. Qualitative analysis suggests branding affects user views of popularity, trust and specialization. We discuss implications for search engine marketing and the design of search engine quality studies.	Brand awareness and the evaluation of search results	NA:NA:NA	2018
Yizhou Sun:Kunqing Xie:Ning Liu:Shuicheng Yan:Benyu Zhang:Zheng Chen	In this paper, we study a new problem of mining causal relation of queries in search engine query logs. Causal relation between two queries means event on one query is the causation of some event on the other. We first detect events in query logs by efficient statistical frequency threshold. Then the causal relation of queries is mined by the geometric features of the events. Finally the Granger Causality Test (GCT) is utilized to further re-rank the causal relation of queries according to their GCT coefficients. In addition, we develop a 2-dimensional visualization tool to display the detected relationship of events in a more intuitive way. The experimental results on the MSN search engine query logs demonstrate that our approach can accurately detect the events in temporal query logs and the causal relation of queries is detected effectively.	Causal relation of queries from temporal logs	NA:NA:NA:NA:NA:NA	2018
Christoph Lindemann:Lars Littig	In this paper, we present a novel method for the classification of Web sites. This method exploits both structure and content of Web sites in order to discern their functionality. It allows for distinguishing between eight of the most relevant functional classes of Web sites. We show that a pre-classification of Web sites utilizing structural properties considerably improves a subsequent textual classification with standard techniques. We evaluate this approach on a dataset comprising more than 16,000 Web sites with about 20 million crawled and 100 million known Web pages. Our approach achieves an accuracy of 92% for the coarse-grained classification of these Web sites.	Classifying web sites	NA:NA	2018
Klaus Berberich:Srikanta Bedathur:Gerhard Weikum:Michalis Vazirgiannis	PageRank is the best known technique for link-based importance ranking. The computed importance scores, however, are not directly comparable across different snapshots of an evolving graph. We present an efficiently computable normalization for PageRank scores that makes them comparable across graphs. Furthermore, we show that the normalized PageRank scores are robust to non-local changes in the graph, unlike the standard PageRank measure.	Comparing apples and oranges: normalized pagerank for evolving graphs	NA:NA:NA:NA	2018
Qingzhao Tan:Ziming Zhuang:Prasenjit Mitra:C. Lee Giles	Due to resource constraints, Web archiving systems and search engines usually have difficulties keeping the entire local repository synchronized with the Web. We advance the state-of-art of the sampling-based synchronization techniques by answering a challenging question: Given a sampled webpage and its change status, which other webpages are also likely to change? We present a study of various downloading granularities and policies, and propose an adaptive model based on the update history and the popularity of the webpages. We run extensive experiments on a large dataset of approximately 300,000 webpages to demonstrate that it is most likely to find more updated webpages in the current or upper directories of the changed samples. Moreover, the adaptive strategies outperform the non-adaptive one in terms of detecting important changes.	Designing efficient sampling techniques to detect webpage updates	NA:NA:NA:NA	2018
Bernard J. Jansen:Danielle L. Booth:Amanda Spink	Determining the user intent of Web searches is a difficult problem due to the sparse data available concerning the searcher. In this paper, we examine a method to determine the user intent underlying Web search engine queries. We qualitatively analyze samples of queries from seven transaction logs from three different Web search engines containing more than five million queries. From this analysis, we identified characteristics of user queries based on three broad classifications of user intent. The classifications of informational, navigational, and transactional represent the type of content destination the searcher desired as expressed by their query. We implemented our classification algorithm and automatically classified a separate Web search engine transaction log of over a million queries submitted by several hundred thousand users. Our findings show that more than 80% of Web queries are informational in nature, with about 10% each being navigational and transactional. In order to validate the accuracy of our algorithm, we manually coded 400 queries and compared the classification to the results from our algorithm. This comparison showed that our automatic classification has an accuracy of 74%. Of the remaining 25% of the queries, the user intent is generally vague or multi-faceted, pointing to the need to for probabilistic classification. We illustrate how knowledge of searcher intent might be used to enhance future Web search engines.	Determining the user intent of web search engine queries	NA:NA:NA	2018
Takashi Tashiro:Takanori Ueda:Taisuke Hori:Yu Hirate:Hayato Yamana	In this paper, we propose a new system extracting potentially copyright infringement texts from the Web, called EPCI. EPCI extracts them in the following way: (1) generating a set of queries based on a given copyright reserved seed-text, (2) putting every query to search engine API, (3) gathering the search result Web pages from high ranking until the similarity between the given seed-text and the search result pages becomes less than a given threshold value, and (4) merging all the gathered pages, then re-ranking them in the order of their similarity. Our experimental result using 40 seed-texts shows that EPCI is able to extract 132 potentially copyright infringement Web pages per a given copyright reserved seed-text with 94% precision in average.	EPCI: extracting potentially copyright infringement texts from the web	NA:NA:NA:NA:NA	2018
Xiang Peng:Irwin King	The Biased Minimax Probability Machine (BMPM) constructs a classifier which deals with the imbalanced learning tasks. In this paper, we propose a Second Order Cone Programming (SOCP) based algorithm to train the model. We outline the theoretical derivatives of the biased classification model, and address the text classification tasks where negative training documents significantly outnumber the positive ones using the proposed strategy. We evaluated the learning scheme in comparison with traditional solutions on three different datasets. Empirical results have shown that our method is more effective and robust to handle imbalanced text classification problems.	Efficient training on biased minimax probability machine for imbalanced text classification	NA:NA	2018
Valentin Jijkoun:Maarten Marx:Maarten de Rijke:Frank van Waveren	The Netherlands had parliamentary elections on November 22, 2006. We built a system which helped voters to make an informed choice among the many participating parties. One of the most important pieces of information in the Dutch election and subsequent coalition government formation is the party program, a text document with an average length of 45 pages. Our system provides the voter with focused access to party programs, enabling her to make a topic-wise comparison of parties' viewpoints. We complemented this type of access ("What do the parties promise?") with access to news ("What happens around these topics?") and blogs ("What do people say about them?"). We describe the system, including design technical details, and user statistics.	Electoral search using the VerkiezingsKijker: an experience report	NA:NA:NA:NA	2018
Keke Cai:Chun Chen:Jiajun Bu:Peng Huang:Zhiming Kang	A number of existing information retrieval systems propose the notion of query context to combine the knowledge of query and user into retrieval to reveal the most exact description of user's information needs. In this paper we interpret query context as a document consisting of sentences related to the current query. This kind of query context is used to re-estimate the relevance probabilities of top-ranked documents and then re-rank top-ranked documents. The experiments show that the proposed context-based approach for information retrieval can greatly improved relevance of search results.	Exploration of query context for information retrieval	NA:NA:NA:NA:NA	2018
Qingyang Xu:Wanli Zuo	This paper reports a new general framework of focused web crawling based on "relational subgroup discovery". Predicates are used explicitly to represent the relevance clues of those unvisited pages in the crawl frontier, and then first-order classification rules are induced using subgroup discovery technique. The learned relational rules with sufficient support and confidence will guide the crawling process afterwards. We present the many interesting features of our proposed first-order focused crawler, together with preliminary promising experimental results.	First-order focused crawling	NA:NA	2018
Ye Wang:Zhihua Geng:Sheng Huang:Xiaoling Wang:Aoying Zhou	Given a document repository, search engine is very helpful to retrieve information. Currently, vertical search is a hot topic, and Google Scholar [4] is an example for academic search. However, most vertical search engines only return the flat ranked list without an efficient result exhibition for given users. We study this problem and designed a vertical search engine prototype Dolphin, where the flexible user-oriented templates can be defined and the survey-like results are presented according to the template.	Academic web search engine: generating a survey automatically	NA:NA:NA:NA:NA	2018
Yang Song:Jian Huang:Isaac G. Councill:Jia Li:C. Lee Giles	Name ambiguity is a special case of identity uncertainty where one person can be referenced by multiple name variations in different situations or evenshare the same name with other people. In this paper, we present an efficient framework by using two novel topic-based models, extended from Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA). Our models explicitly introduce a new variable for persons and learn the distribution of topics with regard to persons and words. Experiments indicate that our approach consistently outperforms other unsupervised methods including spectral and DBSCAN clustering. Scalability is addressed by disambiguating authors in over 750,000 papers from the entire CiteSeer dataset.	Generative models for name disambiguation	NA:NA:NA:NA:NA	2018
Kumar Chellapilla:Anton Mityagin:Denis Charles	A minimal perfect function maps a static set of n keys on to the range of integers {0,1,2,...,n - 1}. We present a scalable high performance algorithm based on random graphs for constructing minimal perfect hash functions (MPHFs). For a set of n keys, our algorithm outputs a description of h in expected time O(n). The evaluation of h(x) requires three memory accesses for any key x and the description of h takes up 0.89n bytes (7.13n bits). This is the best (most space efficient) known result to date. Using a simple heuristic and Huffman coding, the space requirement is further reduced to 0.79n bytes (6.86n bits). We present a high performance architecture that is easy to parallelize and scales well to very large data sets encountered in internet search applications. Experimental results on a one billion URL dataset obtained from Live Search crawl data, show that the proposed algorithm (a)finds an MPHF for one billion URLs in less than 4 minutes, and (b) requires only 6.86 bits/key for the description of h.	GigaHash: scalable minimal perfect hashing for billions of urls	NA:NA:NA	2018
Gjergji Kasneci:Fabain M. Suchanek:Maya Ramanath:Gerhard Weikum	Current keyword-oriented search engines for theWorld WideWeb do not allow specifying the semantics of queries. We address this limitation with NAGA1, a new semantic search engine. NAGA builds on a large semantic knowledge base of binary relationships (facts) derived from the Web. NAGA provides a simple, yet expressive query language to query this knowledge base. The results are then ranked with an intuitive scoring mechanism. We show the effectiveness and utility of NAGA by comparing its output with that of Googleon some interesting queries.	How NAGA uncoils: searching with entities and relations	NA:NA:NA:NA	2018
Ruihua Song:Zhenxiao Luo:Ji-Rong Wen:Yong Yu:Hsiao-Wuen Hon	It is widely believed that some queries submitted to search engines are by nature ambiguous (e.g., java, apple). However, few studies have investigated the questions of "how many queries are ambiguous?" and "how can we automatically identify an ambiguous query?" This paper deals with these issues. First, we construct the taxonomy of query ambiguity, and ask human annotators to manually classify queries based upon it. From manually labeled results, we find that query ambiguity is to some extent predictable. We then use a supervised learning approach to automatically classify queries as being ambiguous or not. Experimental results show that we can correctly identify 87% of labeled queries. Finally, we estimate that about 16% of queries in a real search log are ambiguous.	Identifying ambiguous queries in web search	NA:NA:NA:NA:NA	2018
Zenglin Xu:Irwin King:Michael R. Lyu	Web pages are more than text and they contain much contextual and structural information, e.g., the title, the meta data, the anchor text,etc., each of which can be seen as a data source or are presentation. Due to the different dimensionality and different representing forms of these heterogeneous data sources, simply putting them together would not greatly enhance the classification performance. We observe that via a kernel function, different dimensions and types of data sources can be represented into acommon format of kernel matrix, which can be seen as a generalized similarity measure between a pair of web pages. In this sense, a kernel learning approach is employed to fuse these heterogeneous data sources. The experimental results on a collection of the ODP database validate the advantages of the proposed method over traditional methods based on any single data source and the uniformly weighted combination of them.	Web page classification with heterogeneous data fusion	NA:NA:NA	2018
Xiaojun Wan:Jianwu Yang	Many text documents on the Web are not originally created but forwarded or copied from other source documents. The phenomenon of document forwarding or transmission between various web sites is denoted as Web information diffusion. This paper focuses on mining information diffusion processes for specific topics on the Web. A novel system called LIDPW is proposed to address this problem using matching learning techniques. The source site and source document of each document are identified and the diffusion process composed of a sequence of diffusion relationships is visually presented to users. The effectiveness of LIDPW is validated on a real data set. A preliminary user study is performed and the results show that LIDPW does benefit users to monitor the information diffusion process of a specific topic, and aid them to discover the diffusion start and diffusion center of the topic.	Learning information diffusion process on the web	NA:NA	2018
Gang Luo:Chunqiang Tang:Hao Yang:Xing Wei	People are thirsty for medical information. Existing Web search engines cannot handle medical search well because they do not consider its special requirements. Often a medical information searcher is uncertain about his exact questions and unfamiliar with medical terminology. Therefore, he prefers to pose long queries, describing his symptoms and situation in plain English, and receive comprehensive, relevant information from search results. This paper presents MedSearch, a specialized medical Web search engine, to address these challenges. MedSearch can assist ordinary Internet users to search for medical information, by accepting queries of extended length, providing diversified search results, and suggesting related medical phrases.	MedSearch: a specialized search engine for medical information	NA:NA:NA:NA	2018
Jinlin Chen:Terry Cook	Finding Contiguous Sequential Patterns (CSP) is an important problem in Web usage mining. In this paper we propose a new data structure, UpDown Tree, for CSP mining. An UpDown Tree combines suffix tree and prefix tree for efficient storage of all the sequences that contain a given item. The special structure of UpDown Tree ensures efficient detection of CSPs. Experiments show that UpDown Tree improves CSP mining in terms of both time and memory usage comparing to previous approaches.	Mining contiguous sequential patterns from web logs	NA:NA	2018
Ioannis Anagnostopoulos	In this paper, we describe a capture-recapture experiment conducted on Google's and MSN's cached directories. The anticipated outcome of this work was to monitor evolution rates in these web search services as well as measure their ability to index and maintain fresh and up-to-date results in their cached directories.	Monitoring the evolution of cached content in Google and MSN	NA	2018
Neel Sundaresan:Kavita Ganesan:Roopnath Grandhi	Search engines provide a small window to the vast repository of data they index and against which they search. They try their best to return the documents that are of relevance to the user but often a large number of results may be returned. Users struggle to manage this vast result set looking for the items of interest. Clustering search results is one way of alleviating this navigational pain. In this paper we describe a clustering system that enables clustering search results in an online marketplace search system.	Multi-factor clustering for a marketplace search interface	NA:NA:NA	2018
Sara Cohen:Carmel Domshlak:Naama Zwerdling	This paper addresses the desktop search problem by considering varioustechniques for ranking results of a search query over thefile system. First, basic ranking techniques, which are based ona single file feature (e.g., file name, file content, access date, etc.)are considered. Next, two learning-based ranking schemes are presented, and are shown to be significantly more effective than the basic ranking methods. Finally, a novel ranking technique, based on query selectiveness is considered,for use during the cold-start period of the system. This method isalso shown to be empirically effective, even though it does notinvolve any learning.	On ranking techniques for desktop search	NA:NA:NA	2018
Gleb Skobeltsyn:Toan Luu:Karl Aberer:Martin Rajman:Ivana Podnar Zarko	We describe a query-driven indexing framework for scalable text retrieval over structured P2P networks. To cope with the bandwidth consumption problem that has been identified as the major obstacle for full-text retrieval in P2P networks, we truncate posting lists associated with indexing features to a constant size storing only top-k ranked document references. To compensate for the loss of information caused by the truncation, we extend the set of indexing features with carefully chosen term sets. Indexing term sets are selected based on the query statistics extracted from query logs, thus we index only such combinations that are a) frequently present in user queries and b) non-redundant w.r.t the rest of the index. The distributed index is compact and efficient as it constantly evolves adapting to the current query popularity distribution. Moreover, it is possible to control the tradeoff between the storage/bandwidth requirements and the quality of query answering by tuning the indexing parameters. Our theoretical analysis and experimental results indicate that we can indeed achieve scalable P2P text retrieval for very large document collections and deliver good retrieval performance.	Query-driven indexing for peer-to-peer text retrieval	NA:NA:NA:NA:NA	2018
Xuefeng He:Jun Yan:Jinwen Ma:Ning Liu:Zheng Chen	In this paper, we show that most multiple term queries include more than one topic and users usually reformulate their queries by topics instead of terms. In order to provide empirical evidence on user's reformulation behavior and to help search engines better handle the query reformulation problem, we focus on detecting internal topics in the original query and analyzing users. reformulation to those topics. Particularly, we utilize the Interaction Information (II) to measure the degree of one sub-query being a topic based on the local search results. The experimental results on query log show that: most users reformulate query at the topical level; and our proposed II-based algorithm is a good method to detect topics from original queries.	Query topic detection for reformulation	NA:NA:NA:NA:NA	2018
Nitin Jindal:Bing Liu	It is now a common practice for e-commerce Web sites to enable their customers to write reviews of products that they have purchased. Such reviews provide valuable sources of information on these products. They are used by potential customers to find opinions of existing users before deciding to purchase a product. They are also used by product manufacturers to identify problems of their products and to find competitive intelligence information about their competitors. Unfortunately, this importance of reviews also gives good incentive for spam, which contains false positive or malicious negative opinions. In this paper, we make an attempt to study review spam and spam detection. To the best of our knowledge, there is still no reported study on this problem.	Review spam detection	NA:NA	2018
Xiaoping Sun	This paper presents a structured P2P overlay SCAN that augments CAN overlay with long links based on Kleinberg's small-world model in a d-dimensional Cartesian space. The construction of long links does not require the estimate of network size. Queries in multi-dimensional data space can achieve O(log n) hops by equipping each node with O(log n) long links and O(d)short links.	SCAN: a small-world structured p2p overlay for multi-dimensional queries	NA	2018
Xiaoping Sun:Xue Chen	This paper presents SRing, a structured non DHT P2P overlay that efficiently supports exact and range queries on multiple attribute values. In SRing, all attribute values are interpreted as strings formed by a base alphabet and are published in the lexicographical order. Two virtual rings are built: N-ring is built in a skip-list way for range partition and queries; D-ring is built in a small-world way for the construction of N-ring. A leave-and-join based load balancing method is used to balance range overload in the network with heterogeneous nodes.	SRing: a structured non dht p2p overlay supporting string range queries	NA:NA	2018
Yang Sok Kim:Byeong Ho Kang:Paul Compton:Hiroshi Motoda	In this paper we analyze the Web coverage of three search engines, Google, Yahoo and MSN. We conducted a 15 month study collecting 15,770 Web content or information pages linked from 260 Australian federal and local government Web pages. The key feature of this domain is that new information pages are constantly added but the 260 web pages tend to provide links only to the more recently added information pages. Search engines list only some of the information pages and their coverage varies from month to month. Meta-search engines do little to improve coverage of information pages, because the problem is not the size of web coverage, but the frequency with which information is updated. We conclude that organizations such as governments which post important information on the Web cannot rely on all relevant pages being found with conventional search engines, and need to consider other strategies to ensure important information can be found.	Search engine retrieval of changing information	NA:NA:NA:NA	2018
Frank McCown:Michael L. Nelson	Researchers of commercial search engines often collect datausing the application programming interface (API) or by"scraping" results from the web user interface (WUI), butanecdotal evidence suggests the interfaces produce differentresults. We provide the first in-depth quantitative analysisof the results produced by the Google, MSN and Yahoo APIand WUI interfaces. After submitting a variety of queriesto the interfaces for 5 months, we found significant discrepanciesin several categories. Our findings suggest that theAPI indexes are not older, but they are probably smaller for Google and Yahoo. Researchers may use our findings tobetter understand the differences between the interfaces andchoose the best API for their particular types of queries.	Search engines and their public interfaces: which apis are the most synchronized?	NA:NA	2018
Mukesh Dalal	We present a new approach for propagating spam scores in web graphs, in order to combat link spam. The resulting spam rating is then used for propagating popularity scores like PageRank. Both propagations work even in presence of censure links that represent distrust. Initial testing using a C++ prototype on small examples show more reasonable results than other published approaches.	Spam and popularity ratings for combating link spam	NA	2018
Daniel E. Rose:David Orr:Raj Gopal Prasad Kantamneni	We conducted a series of experiments in which surveyed web search users answered questions about the quality of search results on the basis of the result summaries. Summaries shown to different groups of users were editorially constructed so that they differed in only one attribute, such as length. Some attributes had no effect on users' quality judgments, while in other cases, changing an attribute had a "halo effect" which caused seemingly unrelated dimensions of result quality to be rated higher by users.	Summary attributes and perceived search quality	NA:NA:NA	2018
Byron Y-L Kuo:Thomas Hentrich:Benjamin M . Good:Mark D. Wilkinson	In this paper, we describe an application, PubCloud that uses tagclouds for the summarization of results from queries over thePubMed database of biomedical literature. PubCloud responds toqueries of this database with tag clouds generated from wordsextracted from the abstracts returned by the query. The results ofa user study comparing the PubCloud tag-cloud summarization ofquery results with the standard result list provided by PubMedindicated that the tag cloud interface is advantageous in presenting descriptive information and in reducing user frustrationbut that it is less effective at the task of enabling users to discoverrelations between concepts.	Tag clouds for summarizing web search results	NA:NA:NA:NA	2018
Zhenglu Yang:Lin Li:Botao Wang:Masaru Kitsuregawa	In recent years, there has been a prevalence of search engines being employed to find useful information in the Web as they efficiently explore hyperlinks between web pages which define a natural graph structure that yields a good ranking. Unfortunately, current search engines cannot effectively rank those relational data, which exists on dynamic websites supported by online databases. In this study, to rank such structured data (i.e., find the "best" items), we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data. Efficient querying strategies and updating scheme are devised to facilitate the ranking process. Extensive experiments illustrate the effectiveness and efficiency of our methods. As such, we believe the work in this poster can be complementary to traditional search engines.	Towards efficient dominant relationship exploration of the product items on the web	NA:NA:NA:NA	2018
Bernard J. Jansen:Brian Smith:Danielle L. Booth	Investigating whether one can view Web searching as a learning process, we examined the searching characteristics of 41 participants engaged in 246 searching tasks. We classified the searching tasks according an updated version of Bloom.s taxonomy, a six level categorization of cognitive learning. Results show that Applying takes the most searching effort as measured by queries per session and specific topics searched per sessions. The lower level categories of Remembering and Understanding exhibit searching characteristics similar to the higher order learning of Evaluating and Creating. It appears that searchers rely primarily on their internal knowledge for Evaluating and Creating, using searching primarily as fact checking and verification. Implications are that the commonly held notion that Web searchers have simple information needs may not be correct. We discuss the implications for Web searching, including designing interfaces to support exploration.	Understanding web search via a learning paradigm	NA:NA:NA	2018
Jinlin Chen:Terry Cook	Sequential patterns of d-gaps exist pervasively in inverted lists of Web document collection indices due to the cluster property. In this paper the information of d-gap sequential patterns is used as a new dimension for improving inverted index compression. We first detect d-gap sequential patterns using a novel data structure, UpDown Tree. Based on the detected patterns, we further substitute each pattern with its pattern Id in the inverted lists that contain it. The resulted inverted lists are then coded with an existing coding scheme. Experiments show that this approach can effectively improve the compression ratio of existing codes.	Using d-gap patterns for index compression	NA:NA	2018
Christian Kohlschütter:Paul-Alexandru Chirita:Wolfgang Nejdl	PageRank is known to be an efficient metric for computing general document importance in the Web. While commonly used as a one-size-fits-all measure, the ability to produce topically biased ranks has not yet been fully explored in detail. In particular, it was still unclear to what granularity of "topic" the computation of biased page ranks makes sense. In this paper we present the results of a thorough quantitative and qualitative analysis of biasing PageRank on Open Directory categories. We show that the MAP quality of Biased PageRank generally increases with the ODP level up to a certain point, thus sustaining the usage of more specialized categories to bias PageRank on, in order to improve topic specific search.	Utility analysis for topically biased PageRank	NA:NA:NA	2018
Nikolai Buzikashvili	The results of the Web query log analysis may be significantly shifted depending on the fraction of agents (non-human clients), which are not excluded from the log. To detect and exclude agents the Web log studies use threshold values for a number of requests submitted by a client during the observation period. However, different studies use different observation periods, and a threshold assigned to one period is usually incomparable with the threshold assigned to the other period. We propose the uniform method equally working on the different observation periods. The method bases on the sliding window technique: a threshold is assigned to the sliding window rather than to the whole observation period. Besides, we determine the sub-optimal values of the parameters of the method: a window size and a threshold and recommend 5-7 unique queries as an upper bound of the threshold for 1-hour sliding window.	Sliding window technique for the web log analysis	NA	2018
ChangHee Lee:Heejo Lee	In this paper, we present a password stretching method using user specific salts. Our scheme takes similar time to stretch a password as recent password stretching algorithms, but the complexity of a pre-computation attack increases by 10^8 times and the storage required to store the pre-computation result increases by 10^8 times.	A password stretching method using user specific salts	NA:NA	2018
Timothy W. van der Horst:Kent E. Seamons	Automated email-based password reestablishment (EBPR) is an efficient, cost-effective means to deal with forgotten passwords. In this technique, email providers authenticate users on behalf of web sites. This method works because web sites trust email providers to deliver messages to their intended recipients. Simple Authentication for the Web (SAW) improves upon this basic approach to user authentication to create an alternative to password-based logins. SAW: 1) Removes the setup and management costs of passwords at sites that accept the risks of EBPR; 2) Provides single sign-on without a specialized identity provider; 3) Thwarts all passive attacks.	Simple authentication for the web	NA:NA	2018
Malena Mesarina:Venugopal Srinivasmurthy:Nic Lyons:Craig Sayers	The unification of Semantic Web query languages under the SPARQL standard and the development of commercial-quality implementations are encouraging industries to use semantic technologies for managing information. Current implementations, however, lack the performance monitoring and management services that the industry expects. In this paper, we present a performance and management framework interface to a generic SPARQL web server. We leverage existing standards for instrumentation to make the system ready-to-manage through existing monitoring applications, and we provide a performance framework which has the distinct feature of providing measurement results through the same SPARQL interface used to query data, eliminating the need for special interfaces.	A management and performance framework for semantic web servers	NA:NA:NA:NA	2018
Jiangang Ma:Jinli Cao:Yanchun Zhang	Service discovery is one of challenging issues in Service-Oriented computing. Currently, most of the existing service discovering and matching approaches are based on keywords-based strategy. However, this method is inefficient and time-consuming. In this paper, we present a novel approach for discovering web services. Based on the current dominating mechanisms of discovering and describing Web Services with UDDI and WSDL, the proposed approach utilizes Probabilistic Latent Semantic Analysis (PLSA) to capture semantic concepts hidden behind words in the query and advertisements in services so that services matching is expected to carry out at concept level. We also present related algorithms and preliminary experiments to evaluate the effectiveness of our approach.	A probabilistic semantic approach for discovering web services	NA:NA:NA	2018
Satoshi Sekine:Hisami Suzuki	We present a method for acquiring ontological knowledge using search query logs. We first use query logs to identify important contexts associated with terms belonging to a semantic category; we then use these contexts to harvest new words belonging to this category. Our evaluation on selected categories indicates that the method works very well to help harvesting terms, achieving 85% to 95% accuracy in categorizing newly acquired terms.	Acquiring ontological knowledge from query logs	NA:NA	2018
Meenakshi Nagarajan:Amit Sheth:Marcos Aguilera:Kimberly Keeton:Arif Merchant:Mustafa Uysal	In this paper we extend the state-of-the-art in utilizing background knowledge for supervised classification by exploiting the semantic relationships between terms explicated in Ontologies. Preliminary evaluations indicate that the new approach generally improves precision and recall, more so for hard to classify cases and reveals patterns indicating the usefulness of such background knowledge.	Altering document term vectors for classification: ontologies as expectations of co-occurrence	NA:NA:NA:NA:NA:NA	2018
Melike Sah:Wendy Hall	This paper presents a semantic portal, SEMPort, which provides better user support with personalized views, semantic navigation, ontology-based search and three different kinds of semantic hyperlinks. Distributed content editing and provision is supplied for the maintenance of the contents in real-time. As a case study, SEMPort is tested on the Course Modules Web Page (CMWP) of the School of Electronics and Computer Science (ECS).	Building and managing personalized semantic portals	NA:NA	2018
Xiaonan Lu:James Z. Wang:Prasenjit Mitra:C. Lee Giles	Figures in digital documents contain important information. Current digital libraries do not summarize and index information available within figures for document retrieval. We present our system on automatic categorization of figures and extraction of data from 2-D plots. A machine-learning based method is used to categorize figures into a set of predefined types based on image features. An automated algorithm is designed to extract data values from solid line curves in 2-D plots. The semantic type of figures and extracted data values from 2-D plots can be integrated with textual information within documents to provide more effective document retrieval services for digital library users. Experimental evaluation has demonstrated that our system can produce results suitable for real-world use.	Deriving knowledge from figures for digital libraries	NA:NA:NA:NA	2018
Joo Seong Jeon:Gi Jeong Lee	This paper describes the development of a semantic web and ontology based local search system that can be used in wireless mobile communication services.	Development of a semantic web based mobile local search system	NA:NA	2018
Angela Maduko:Kemafor Anyanwu:Amit Sheth:Paul Schliekelman	Most RDF query languages allow for graph structure search through a conjunction of triples which is typically processed using join operations. A key factor in optimizing joins is determining the join order which depends on the expected cardinality of intermediate results. This work proposes a pattern-based summarization framework for estimating the cardinality of RDF graph patterns. We present experiments on real world and synthetic datasets which confirm the feasibility of our approach.	Estimating the cardinality of RDF graph patterns	NA:NA:NA:NA	2018
Federico Michele Facca:Marco Brambilla	Available methodologies for developing Sematic Web applications do not fully exploit the whole potential deriving from interaction with ontological data sources. Here we introduce an extension of the WebML modeling framework to fulfill most of the design requirements emerging for the new area of Semantic Web. We generalize the development process to support Semantic Web applications and we introduce a set of new primitives for ontology importing and querying.	Extending WebML towards semantic web	NA:NA	2018
Qiankun Zhao:Prasenjit Mitra:C. Lee Giles	In this paper, we propose a novel approach of image annotation byconstructing a hierarchical mapping between low-level visualfeatures and text features utilizing the relations within and acrossboth visual features and text features. Moreover, we propose a novelannotation strategy that maximizes both the accuracy and thediversity of the generated annotation by generalizing or specifyingthe annotation in the corresponding annotation hierarchy.Experiments with 4500 scientific images from Royal Society ofChemistry journals show that the proposed annotation approachproduces satisfactory results at different levels of annotations.	Image annotation by hierarchical mapping of features	NA:NA:NA	2018
Christopher C. Yang:Jianfeng Lin	Documents in the Web are often organized using category trees by information providers (e.g. CNN, BBC) or search engines (e.g. Google, Yahoo!). Such category trees are commonly known as Web directories. The category tree structures from different internet content providers may be similar to some extent but are usually not exactly the same. As a result, it is desirable to integrate these category trees together so that web users only need to browse through a unified category tree to extract information from multiple providers. In this paper, we address this problem by capturing structural information of multiple category trees, which are embedded with the knowledge of professional in organizing the documents. Our experiments with real Web data show that the proposed technique is promising.	Integrating web directories by learning their structures	NA:NA	2018
Hui Guo:Anca Andreea Ivan:Rama Akkiraju:Richard Goodwin	This paper presents a novel technique that significantly improves the quality of semantic Web service matching by (1) automatically generating ontologies based on Web service descriptions and (2) using these ontologies to guide the mapping between Web services.	Learning ontologies to improve the quality of automatic web service matching	NA:NA:NA:NA	2018
Benjamin M. Good:Mark D. Wilkinson	We describe an approach designed to reduce the costs of ontology development through the use of untrained, volunteer knowledge engineers. Results are provided from an experiment in which volunteers were asked to judge the correctness of automatically inferred subsumption relationships in the biomedical domain. The experiment indicated that volunteers can be recruited fairly easily but that their attention is difficult to hold, that most do not understand the subsumption relationship without training, and that incorporating learned estimates of trust into voting systems is beneficial to aggregate performance.	Ontology engineering using volunteer labor	NA:NA	2018
Christina Tziviskou:Marco Brambilla	Enriching Web applications with personalized data is of major interest for facilitating the user access to the published contents, and therefore, for guaranteeing successful user navigation. We propose a conceptual model for extracting personalized recommendations based on user profiling, ontological domain models, and semantic reasoning. The approach offers a high-level representation of the designed application based on a domain-specific metamodel for Web applications called WebML.	Semantic personalization of web portal contents	NA:NA	2018
Johan Bollen:Marko A. Rodriguez:Herbert Van de Sompel:Lyudmila L. Balakireva:Aric Hagberg	Scholarly entities, such as articles, journals, authors and institutions, are now mostly ranked according to expert opinion and citation data. The Andrew W. Mellon Foundation funded MESUR project at the Los Alamos National Laboratory is developing metrics of scholarly impact that can rank a wide range of scholarly entities on the basis of their usage. The MESUR project starts with the creation of a semantic network model of the scholarly community that integrates bibliographic, citation, and usage data collected from publishers and repositories world-wide. It is estimated that this scholarly semantic network will include approximately 50 million articles, 1 million authors, 10,000 journals and conference proceedings, 500 million citations, and 1 billion usage-related events; the largest scholarly semantic network ever created. The developed scholarly semantic network will then serve as a standardized platform for the definition and validation of new metrics of scholarly impact. This poster describes the MESUR project's data aggregation and processing techniques including the OWL scholarly ontology that was developed to model the scholarly communication process.	The largest scholarly semantic network...ever.	NA:NA:NA:NA:NA	2018
Yu Jianjun:Guo Shengmin:Su Hao:Zhang Hui:Xu Ke	This paper describes a kernel based Web Services (abbrevi-ated as service) matching mechanism for service discoveryand integration. The matching mechanism tries to exploitthe latent semantics by the structure of services. Using textual similarity and n-spectrum kernel values as features of low-level and mid-level, we build up a model to estimate thefunctional similarity between services, whose parameters arelearned by a Ranking-SVM. The experiment results showedthat several metrics for the retrieval of services have beenimproved by our approach.	A kernel based structure matching for web services search	NA:NA:NA:NA:NA	2018
Qiudan Li:Chunheng Wang:Guanggang Geng:Ruwei Dai	With the rapid growth of wireless technologies and handheld devices, m-commerce is becoming a promising research area. Personalization is especially important to the success of m-commerce. This paper proposes a novel collaborative filtering-based framework for personalized services in m-commerce. The framework extends our previous work by using Online Analytical Processing (OLAP) to represent the relations among user, content and context information, and adopting a multi-dimensional collaborative filtering model to perform inference. It provides a powerful and well-founded mechanism to personalization for m-commerce. We implemented it in an existing m-commerce platform, and experimental results demonstrate its feasibility and correctness.	A novel collaborative filtering-based framework for personalized services in m-commerce	NA:NA:NA:NA	2018
Xuanzhe Liu:Li Zhou:Gang Huang:Hong Mei	In current web service discovery and subscription, consumers must pay too much time on manually selection and cannot easily benefit from the wide QoS spectrum brought by the proliferating services. In our approach, we introduce the service pool as a "virtual service" grouping function identical services together and dispatching consumer requests to the proper service in terms of QoS requirements.	Towards service pool based approach for services discovery and subscription	NA:NA:NA:NA	2018
Eyhab Al-Masri:Qusay H. Mahmoud	As Web services proliferate, size and magnitude of UDDI Business Registries (UBRs) are likely to increase. The ability to discover Web services of interest then across multiple UBRs becomes a major challenge specially when using primitive search methods provided by existing UDDI APIs. Clients do not have the time to endlessly search accessible UBRs for finding appropriate services particularly when operating via mobile devices. Finding services of interest should be time effective and highly productive. This paper addresses issues relating to the efficient access and discovery of Web services across multiple UBRs and introduces a novel exploration engine, the Web Service Crawler Engine (WSCE). WSCE is capable of crawling multiple UBRs, and enables for the establishment of a centralized Web services repository that can be used for discovering Web services much more efficiently. The paper presents experimental validation, results, and analysis of the proposed ideas.	Crawling multiple UDDI business registries	NA:NA	2018
Eyhab Al-Masri:Qusay H. Mahmoud	Major research challenges in discovering Web services include, provisioning of services across multiple or heterogeneous registries, differentiating between services that share similar functionalities, improving end-to-end Quality of Service (QoS), and enabling clients to customize the discovery process. Proliferation and interoperability of this multitude of Web services have lead to the emergence of new standards on how services can be published, discovered, or used (i.e. UDDI, WSDL, SOAP). Such standards can potentially provide many of these features and much more, however, there are technical challenges associated with existing standards. One of these challenges is the client.s ability to control the discovery process across accessible service registries for finding services of interest. This work proposes a solution to this problem and introduces the Web Service Relevancy Function (WsRF) used for measuring the relevancy ranking of a particular Web service based on QoS metrics and client preferences. We present experimental validation, results, and analysis of the presented ideas.	Discovering the best web service	NA:NA	2018
Huaigu Wu:Yuri Natchetoi	The goal of this poster is to describe our implementation of a newarchitecture enabling efficient integration between mobile phoneapplications and Web Services. Using this architecture, we haveimplemented a mobile shopping assistant described further. In orderto build this architecture, we designed an innovative XMLcompression mechanism to facilitate data exchange between mobilephones and Web Services. We also designed a smart connection managerto control asynchronous communication for all possible channels of amobile phone. In addition, we used diverse input modes in order toextend users' access to Web Services.	Mobile shopping assistant: integration of mobile applications and web services	NA:NA	2018
Zhongnan Shen:Jianwen Su	We develop a framework to compose services through discovery and orchestration for a given goal service. Tightening techniques are used in composition algorithms to achieve "completeness".	On automated composition for web services	NA:NA	2018
Ismail Ari:Jun Li:Riddhiman Ghosh:Mohamed Dekhil	It is extremely hard for a global organization with services over multiple channels to capture a consistent and unified view of its data, services, and interactions. While SOA and web services are addressing integration and interoperability problems, it is painful for an operational organization with legacy systems to quickly switch to service-based methods. We need methods to combine advantages of traditional (i.e. web, desktop, or mobile) application development environments and service-based deployments.In this paper, we focus on the design and implementation of session management as a core service to support business processes and go beyond application-specific sessions and web sessions. We develop local session components for different platforms and complement them with a remote "session service" that is independent of applications and platforms. We aim to close the gap between the two worlds by combining their performance, availability and interoperability advantages.	Providing session management as core business service	NA:NA:NA:NA	2018
Michael E. Ruth:Shengru Tu	This paper reports a safe regression test selection (RTS) approach that is designed for verifying Web services in an end-to-end manner. The Safe RTS technique has been integrated into a systematic method that monitors distributed code modifications and automates the RTS and RT processes.	Towards automating regression test selection for web services	NA:NA	2018
Takuya Maekawa:Yutaka Yanagisawa:Takeshi Okadome	The environment generated media (EGM) are defined here as being generated from a massive amount of and/or incomprehensible environmental data by compressing them into averages or representative values and/or by converting them into such user-friendly media as text, figures, charts, and animations. As an application of EGM, an object-participation-type weblog is introduced, where anthropomorphic indoor objects with sensor nodes post weblog entries and comments about what happened to them in a sensor networked environment.	Towards environment generated media: object-participation-type weblog in home sensor network	NA:NA:NA	2018
Nilesh Bansal:Nick Koudas	We present BlogScope (www.blogscope.net), a system for analyzing the Blogosphere. BlogScope is an information discovery and text analysis system that offers a set of unique features. Such features include, spatio-temporal analysis of blogs, flexible navigation of the Blogosphere through information bursts, keyword correlations and burst synopsis, as well as enhanced ranking functions for improved query answer relevance. We describe the system, its design and the features of the current version of BlogScope.	BlogScope: spatio-temporal analysis of the blogosphere	NA:NA	2018
Juanzi Li:Jie Tang:Jing Zhang:Qiong Luo:Yunhao Liu:Mingcai Hong	In this paper, we present the design and implementation of our expertise oriented search system, EOS http://www.arnetminer.net. EOS is a researcher social network system. It has gathered information about a half-million computer science researchers from the Web and constructed a social network among the researchers through their co-authorship. In particular, the relationship in the social network information is used in both ranking experts for a given topic and searching for associations between researchers. Our experimental results demonstrate that the proposed methods for expert finding and association search in a social network are both more effective and efficient than the baseline methods.	EOS: expertise oriented search using social networks	NA:NA:NA:NA:NA:NA	2018
Martin J. Halvey:Mark T. Keane	It is now feasible to view media at home as easily as text-based pages were viewed when the World Wide Web (WWW) first emerged. This development has supported media sharing and search services providing hosting, indexing and access to large, online media repositories. Many of these sharing services also have a social aspect to them. This paper provides an initial analysis of the social interactions on a video sharing and search service (www.youtube.com). Results show that many users do not form social networks in the online community and a very small number do not appear to contribute to the wider community. However, it does seem those people who do use the available tools have much a greater tendency to form social connections.	Exploring social dynamics in online media sharing	NA:NA	2018
Ken Wakita:Toshiyuki Tsurumi	Community analysis algorithm proposed by Clauset, Newman, and Moore (CNM algorithm) finds community structure in social networks. Unfortunately, CNM algorithm does not scale well and its use is practically limited to networks whose sizes are up to 500,000 nodes. We show that this inefficiency is caused from merging communities in unbalanced manner and that a simple heuristics that attempts to merge community structures in a balanced manner can dramatically improve community structure analysis. The proposed techniques are tested using data sets obtained from existing social networking service that hosts 5.5 million users. We have tested three three variations of the heuristics. The fastest method processes a SNS friendship network with 1 million users in 5 minutes (70 times faster than CNM) and another friendship network with 4 million users in 35 minutes, respectively. Another one processes a network with 500,000 nodes in 50 minutes (7 times faster than CNM), finds community structures that has improved modularity, and scales to a network with 5.5 million.	Finding community structure in mega-scale social networks: [extended abstract]	NA:NA	2018
Yun-Maw Cheng:Tzu-Chuan Chou:Wai Yu:Li-Chieh Chen:Ching-Long Yeh:Meng-Chang Chen	Recent trend in the development of mobile devices, wireless communications, sensor technologies, weblogs, and peer-to-peer communications have prompted a new design opportunity for enhancing social interactions. This paper introduces our preliminary experiences in designing a prototype utilizing the aforementioned technologies to share life experience. Users equipped with camera phones coupled with short-range communication technology, such as RFID, can capture life experience and share it as weblogs to other people. However, in reality, this is easier said than done. The success of weblogs relies on the active participation and willingness of people to contribute. To encourage active participations, a ranking system, AgreeRank, is specifically developed to get them motivated.	Life is sharable: mechanisms to support and sustain blogging life experience	NA:NA:NA:NA:NA:NA	2018
Wei Wei:Jimmy Lee:Irwin King	Learning Villages (LV) is an E-learning platform for people's online discussions and frequently citing postings of one another. In this paper, we propose a novel method to rank credit authors in the LV system. We first propose a k-EACM graph to describe the article citation structure in the LV system. And then we build a weighted graph model k-UCM graph to reveal the implicit relationship between authors hidden behind the citations among their articles. Furthermore, we design a graph-based ranking algorithm, the Credit Author Ranking (CAR) algorithm, which can be applied to rank nodes in a graph with negative edges. Finally, we perform experimental evaluations by simulations. The results of evaluations illustrate that the proposed method works pretty well on ranking the credibility of users in the LV system.	Measuring credibility of users in an e-learning environment	NA:NA:NA	2018
Tomoharu Iwata:Kazumi Saito:Takeshi Yamada	We propose a model for user purchase behavior in online stores that provide recommendation services. We model the purchase probability given recommendations for each user based on the maximum entropy principle using features that deal with recommendations and user interests. The proposed model enable us to measure the effect of recommendations on user purchase behavior, and the effect can be used to evaluate recommender systems. We show the validity of our model using the log data of an online cartoon distribution service, and measure the recommendation effects for evaluating the recommender system.	Modeling user behavior in recommender systems based on maximum entropy	NA:NA:NA	2018
Duen Horng Chau:Shashank Pandit:Samuel Wang:Christos Faloutsos	Given a huge online social network, how do we retrieve information from it through crawling? Even better, how do we improve the crawling performance by using parallel crawlers that work independently? In this paper, we present the framework of parallel crawlers for online social networks, utilizing a centralized queue. To show how this works in practice, we describe our implementation of the crawlers for an online auction website. The crawlers work independently, therefore the failing of one crawler does not affect the others at all. The framework ensures that no redundant crawling would occur. Using the crawlers that we built, we visited a total of approximately 11 million auction users, about 66,000 of which were completely crawled.	Parallel crawling for online social networks	NA:NA:NA:NA	2018
Mukesh Dalal	This paper presents Adaptive Web Search (AWS), a novel search technique that combines personalized, social, and real-time collaborative search. Preliminary empirical results from a small sample suggest that an AWS prototype built on WAMP platform using Yahoo! Web Search API generates more relevant results and allows faster discovery of information.	Personalized social & real-time collaborative search	NA	2018
Tye Rattenbury:Nathan Good:Mor Naaman	We address the problem of extracting semantics of tags -- short, unstructured text-labels assigned to resources on the Web -- based on each tag's metadata patterns. In particular, we describe an approach for extracting place and event semantics for tags that are assigned to photos on Flickr, a popular photo sharing website supporting time and location (latitude/longitude) metadata. The approach can be generalized to other domains where text terms can be extracted and associated with metadata patterns, such as geo-annotated web pages.	Towards extracting flickr tag semantics	NA:NA:NA	2018
Marius Pasca	In a new model for answer retrieval, document collections are distilled offline into large repositories of facts. Each fact constitutes a potential direct answer to questions seeking a particular kind of entity or relation, such as questions asking about the date of particular events. Question answering becomes equivalent to online fact retrieval, which greatly simplifies the de-facto system architecture.	A no-frills architecture for lightweight answer retrieval	NA	2018
Shrirang Sudhir Shirodkar:Varsha Apte	We present a load generator and performance measurement tool AutoPerf which requires minimal input and configuration from the user, and produces a comprehensive capacity analysis as well as server-side resource usage profile of a Web-based distributed system, in an automated fashion. The tool requires only the workload and deployment description of the distributed system, and automatically sets typical parameters that load generator programs need, such as maximum number of users to be emulated, number of users for each experiment, warm-up time, etc. The tool also does all the co-ordination required to generate a critical type of measure, namely, resource usage per transaction or per user for each software server. This is a necessary input for creating a performance model of a software system.	AutoPerf: an automated load generator and performance measurement tool for multi-tier software systems	NA:NA	2018
Johannes Meinecke:Frederic Majer:Martin Gaedke	The success of many innovative Web applications is not based on the content they produce -- but on how they combine and link existing content. Older Web Engineering methods lack flexibility in a sense that they rely strongly on a-priori knowledge of existing content structures and do not take into account initially unknown content sources. We propose the adoption of principles that are also found in Component-based Software Engineering, to assemble highly extensible solutions from reusable artifacts. The main contribution of our work is a support system, consisting of a central service that manages n:m relationships between arbitrary Web resources, and of Web application components that realize navigation, presentation, and interaction for the linked content.	Construction by linking: the linkbase method	NA:NA:NA	2018
Keiji Yanai	We propose a new system to mine visual knowledge on the Web.There are huge image data as well as text data on the Web. However, mining image data from the Web is paid less attention than mining text data, since treating semantics of images are much more difficult. In this paper, we propose introducing a latest image recognition technique, which is the bag-of-keypoints representation,into Web image-gathering task. By the experiments we show theproposed system outperforms our previous systems and Google Imagesearch greatly.	Image collector III: a web image-gathering system with bag-of-keypoints	NA	2018
Ling Chen:Sourav Bhowmick:Wolfgang Nejdl	Mirroring Web sites is a well-known technique commonly used in the Web community. A mirror site should be updated frequently to ensure that it reflects the content of the original site. Existing mirroring tools apply page-level strategies to check each page of a site, which is inefficient and expensive. In this paper, we propose a novel site-level mirror maintenance strategy. Our approach studies the evolution of Web directorystructures and mines association rules between ancestor-descendant Web directories. Discovered rules indicate the evolution correlations between Web directories. Thus, when maintaining the mirror of a Web site (directory), we can optimally skipsubdirectories which are negatively correlated with it in undergoing significant changes. The preliminary experimental results show that our approach improves the efficiency of the mirror maintenance process significantly while sacrificing slightly in keeping the "freshness" of the mirrors.	Mirror site maintenance based on evolution associations of web directories	NA:NA:NA	2018
Hanane Azzag:Julien Lavergne:Christiane Guinot:Gilles Venturini	We present an incremental algorithm for building a neighborhood graph from a set of documents. This algorithm is based on a population of artificial agents that imitate the way real ants build structures with self-assembly behaviors. We show that our method outperforms standard algorithms for building such neighborhood graphs (up to 2230 times faster on the tested databases with equal quality) and how the user may interactively explore the graph.	On building graphs of documents with artificial ants	NA:NA:NA:NA	2018
Aidan Hogan:Andreas Harth:Jürgen Umrich:Stefan Decker	Current search engines do not fully leverage semantically rich datasets, or specialise in indexing just one domain-specific dataset.We present a search engine that uses the RDF data model to enable interactive query answering over richly structured and interlinked data collected from many disparate sources on the Web.	Towards a scalable search and query engine for the web	NA:NA:NA:NA	2018
Walter Dees:Paul Shrubsole	In a world where all devices will be interconnected, the boundaries between the different devices will start to disappear. Devices will be able to access each other's applications; sessions can be suspended on one device and resumed on another device; devices can serve as each other's input and output device, and all devices will be able to connect to the Internet. This will give true mobility to the user as he/she will not be restricted to the time and location where he/she accesses an application. Of course, we need a variety of different mechanisms and technologies to enable this, such as: Remote rendering of UIs on other devices in the network. Infrastructure for discovering client and servers in a network. Mechanisms to exchange capability information between devices, and to adapt the UI based on these capabilities. Mechanisms to deal with session migration.Support for a wide range of consumer devices, ranging from mobile phones to high-end TVs. This requires technologies that cross different domains, i.e. the PC domain, mobile domain, and TV domain. Several major companies within these different domains have decided to work together on these issues. One of the results is a framework for remote user interfaces for both UPnP" networks and the Internet. This framework is called Web4CE (a.k.a. CEA-2014) [1], and has been accepted as the baseline remote user interface technology within the Digital Living Network Alliance (DLNA) [2], which is a large industry-wide effort for creating true interoperability between network-enabled devices.This paper provides a short overview of the Web4CE framework, and some of the use cases that it enables.	Web4CE: accessing web-based applications on consumer devices	NA:NA	2018
Marwan Sabbouh:Jeff Higginson:Salim Semy:Danny Gagne	The Web Mashup Scripting Language (WMSL) enables an end-user (you) working from his browser, e.g. not needing any other infrastructure, to quickly write mashups that integrate any two, or more, web services on the Web. The end-user accomplishes this by writing a web page that combines HTML, metadata in the form of mapping relations, and small piece of code, or script. The mapping relations enable not only the discovery and retrieval of the WMSL pages, but also affect a new programming paradigm that abstracts many programming complexities from the script writer. Furthermore, the WMSL Web pages or scripts that disparate end-users (you) write, can be harvested by Crawlers to automatically generate the concepts needed to build lightweight ontologies containing local semantics of a web service and its data model, to extend context ontologies or middle ontologies, and to develop links, or mappings, between these ontologies. This enables an open-source model of building ontologies based on the WMSL Web page or scripts that end users (you) write.	Web mashup scripting language	NA:NA:NA:NA	2018
Richard A. Frost:Xiaoli Ma:Y. Shi	A SpeechWeb is a collection of hyperlinked applications, which are accessed remotely by speech browsers running on end-user devices. Links are activated through spoken commands. Despite the fact that protocols and technologies for creating and deploying speech applications have been readily available for several years, we have not seen the development of a Public-Domain SpeechWeb. In this paper, we show how freely available software and commonly used communication protocols can be used to change this situation.	A browser for a public-domain SpeechWeb	NA:NA:NA	2018
Xin Li:Jun Yan:Zhihong Deng:Lei Ji:Weiguo Fan:Benyu Zhang:Zheng Chen	In recent years, different commercial Weblog subscribing systems have been proposed to return stories from users. subscribed feeds. In this paper, we propose a novel clustering-based RSS aggregator called as RSS Clusgator System (RCS) for Weblog reading. Note that an RSS feed may have several different topics. A user may only be interested in a subset of these topics. In addition there could be many different stories from multiple RSS feeds, which discuss similar topic from different perspectives. A user may be interested in this topic but do not know how to collect all feeds related to this topic. In contrast to many previous works, we cluster all stories in RSS feeds into hierarchical structure to better serve the readers. Through this way, users can easily find all their interested stories. To make the system current, we propose a flexible time window for incremental clustering. RCS utilizes both link information and content information for efficient clustering. Experiments show the effectiveness of RCS.	A novel clustering-based RSS aggregator	NA:NA:NA:NA:NA:NA:NA	2018
Michal Tvarozek:Maria Bielikova	Open information spaces have several unique characteristics such as their changeability, large size, complexity and diverse user base. These result in novel challenges during user navigation, information retrieval and data visualization in open information spaces.We propose a method of navigation in open information spaces based on an enhanced faceted browser with support for dynamic facet generation and adaptation based on user characteristics.	Adaptive faceted browser for navigation in open information spaces	NA:NA	2018
Martin J. Halvey:Mark T. Keane	With the growth of social bookmarking a new approach for metadata creation called tagging has emerged. In this paper we evaluate the use of tag presentation techniques. The main goal of our evaluation is to investigate the effect of some of the different properties that can be utilized in presenting tags e.g. alphabetization, using larger fonts etc. We show that a number of these factors can affect the ease with which users can find tags and use the tools for presenting tags to users.	An assessment of tag presentation techniques	NA:NA	2018
Marta Gatius:Meritxell González:Elisabet Comelles	In this paper we propose the integration of intelligent components technologies (natural language and discourse management) in voice web interfaces to make them smarter. We describe how we have integrated reusable components of dialogue management and language processing in a multilingual voice system to improve its friendliness and portability. The dialogue management component deals with complex dialogue phenomena, such as user-initiative dialogues, and follows the information state-based theory. The resulting dialogue system supports friendly communication (through the telephone and the web) in several languages: English, Spanish, Catalan and Italian. The dialogue system has been adapted to guide the users to access online public administration services.	An information state-based dialogue manager for making voice web smarter	NA:NA:NA	2018
Ganesan Velayathan:Seiji Yamada	This paper describes our efforts to investigate factors in user's browsing behavior to automatically evaluate web pages that the user shows interest in. To evaluate web pages automatically, we developed a client-side logging/analyzing tool: the GINIS Framework. This work focuses primarily on client-side user behavior using a customized web browser and AJAX technologies. First, GINIS unobtrusively gathers logs of user behavior through the user.s natural interaction with the web browser. Then it analyses the logs and extracts effective rules to evaluate web pages using C4.5 machine learning system. Eventually, GINIS becomes able to automatically evaluate web pages using these learned rules.	Behavior based web page evaluation	NA:NA	2018
Leo Spalteholz:Kin Fun Li:Nigel Livingston	For many users with a disability it can be difficult or impossible to use a computer mouse to navigate the web. An alternative way to select elements on a web page is the label typing approach, in which users select elements by typing part of the label. In most cases, these labels are specified by the page authors, but some selectable elements do not have an obvious textual description, thus requiring that a label be generated. The set of element labels on a web page must be both efficient to select by text input and meaningful to the user. This paper discusses our approach to this problem, using page structural analysis and user history to determine important elements of a page, and then matching this information with the efficiency model of the input device.	Generating efficient labels to facilitate web accessibility	NA:NA:NA	2018
Kehinde Alabi	This paper describes a comprehensive method for presenting mathematical equations and expressions using only pure HTML and CSS. This method renders the equations portable and editable and contrasts with previous procedures that represent equations as whole graphic objects. Methods for generating and documenting the equations using HTML and JavaScript are also described such that the equations can be interpreted and converted to or from other formats such as LaTex, MATHML, or linear representation.	Generation, documentation and presentation of mathematical equations and symbolic scientific expressions using pure HTML and CSS	NA	2018
Yih-Farn Chen:Giuseppe Di Fabbrizio:David Gibbon:Rittwik Jana:Serban Jora:Bernard Renger:Bin Wei	The Web is rapidly moving towards a platform for mass collaboration in content production and consumption from three screens: computers, mobile phones, and TVs. While there has been a surge of interests in making Web content accessible from mobile devices, there is a significant lack of progress when it comes to making the web experience suitable for viewing on a television. Towards this end, we describe a novel concept, namely GeoTV, where we explore a framework by which web content can be presented or pushed in a meaningful manner to create an entertainment experience for the TV audience. Fresh content on a variety of topics, people, and places is being created and made available on the Web at breathtaking speed. Navigating fresh content effectively on TV demands a new browsing paradigm that requires fewer mouse clicks or user interactions from the remote control. Novel geospatial and temporal browsing techniques are provided in GeoTV that allow users the capability of aggregating and navigating RSS-enabled content in a timely, personalized and automatic manner for viewing in an IPTV environment. This poster is an extension of our previous work on GeoTracker that utilizes both a geospatial representation and a temporal (chronological) presentation to help users spot the most relevant updates quickly within the context of a Web-enabled environment. We demonstrate 1) the usability of such a tool that greatly enhances a user.s ability in locating and browsing videos based on his or her geographical interests and 2) various innovative interface designs for showing RSS-enabled information in an IPTV environment.	GeoTV: navigating geocoded rss to create an iptv experience	NA:NA:NA:NA:NA:NA:NA	2018
Shane Ahern:Simon King:Mor Naaman:Rahul Nair	The availability of map interfaces and location-aware devices makes a growing amount of unstructured, geo-referenced information available on the Web. In particular, over twelve million geo-referenced photos are now available on Flickr, a popular photo-sharing website. We show a method to analyze the Flickr data and generate aggregate knowledge in the form of "representative tags" for arbitrary areas in the world. We display these tags on a map interface in an interactive web application along with images associated with each tag. We then use the implicit feedback of the aggregate user interactions with the tags and images to learn which images best describe the area shown on the map.	Summarization of online image collections via implicit feedback	NA:NA:NA:NA	2018
Tetsushi Morita:Tetsuo Hidaka:Akimichi Tanaka:Yasuhisa Kato	We propose a system for reminding a user of information obtained through a web browsing experience. The system extracts keywords from the content of the web page currently being viewed and retrieves the context of past web browsing related tothe keywords. We define the context as a sequence of web browsing when many web pages related to the keyword were viewed intensively because we assume that a lot of information connected to the current content was obtained in the sequence.The information is not only what pages you viewed but also how you found those pages and what knowledge you acquired from them. Specifically, when you browse web pages, this system automatically displays a list of the contexts judged to be important in relation to the current web page. If you select the context, details of the context are shown graphically with marksindicating characteristic activities.	System for reminding a user of information obtainedthrough a web browsing experience	NA:NA:NA:NA	2018
David Gotz	The World Wide Web is a powerful platform for a wide range of information tasks. Dramatic advances in technology, such as improved search capabilities and the AJAX application model, have enabled entirely new web-based applications and usage patterns, making many tasks easier to perform than ever before. However, few tools have been developed to assist with sensemaking tasks: complex research behaviors in which users gather and comprehend information from many sources to answer potentially vague, non-procedural questions. Sensemaking tasks are common and include, for example, researching vacation destinations or deciding how to invest. This paper presents the ScratchPad, an extension to the standard browser interface that is designed to capture, organize, and exploit the information discovered while performing a sensemaking task.	The ScratchPad: sensemaking support for the web	NA	2018
Chong Huang:Yonghong Tian:Zhi Zhou:Tiejun Huang	Generally speaking, digital libraries have multiple granularities of semantic units: book, chapter, page, paragraph and word. However, there are two limitations of current eBook retrieval systems: (1) the granularity of retrievable units is either too big or too small, scales such as chapters, paragraphs are ignored; (2) the retrieval results should be grouped by facets to facilitate user's browsing and exploration. To overcome these limitations, we propose a multi-granularity multi-facet eBook retrieval approach.	Towards multi-granularity multi-facet e-book retrieval	NA:NA:NA:NA	2018
M. S. Ali:Mariano P. Consens:Flavio Rizzolo	NA	Visualizing structural patterns in web collections	NA:NA:NA	2018
Justin Park:Denilson Barbosa	We describe an adaptive method for extracting records from web pages. Our algorithm combines a weighted tree matching metric with clustering for obtaining data extraction patterns.We compare our method experimentally to the state-of-the-art, and show that our approach is very competitive for rigidly-structured records (such as product descriptions) and far superior for loosely-structured records (such as entrieson blogs).	Adaptive record extraction from web pages	NA:NA	2018
Jianhua Feng:Na Ta:Yong Zhang:Guoliang Li	In XML databases, materializing queries and their results into views in a semantic cache can improve the performance of query evaluation by reducing computational complexity and I/O cost. Although there are a number of proposals of semantic cache for XML queries, the issues of fast cache lookup and compensation query construction could be further studied. In this paper, based on sequential XPath queries, we propose fastCLU, a fast Cache LookUp algorithm and effiCQ, an efficient Compensation Query constructing algorithm to solve these two problems. Experimental results show that our algorithms outperform previous algorithms and can achieve good performance of query evaluation.	Exploit sequencing views in semantic cache to accelerate xpath query evaluation	NA:NA:NA:NA	2018
Felix Michel:Erik Wilde	XML Schema documents are defined using an XML syntax, which means that the idea of generating schema documentation through standard XML technologies is intriguing. We present X2Doc, a framework for generating schema-documentation solely through XSLT. The framework uses SCX, an XML syntax for XML Schema components, as intermediate format and produces XML-based output formats. Using a modular set of XSLT stylesheets, X2Doc is highly configurable and carefully crafted towards extensibility. This proves especially useful for composite schemas, where additional schema information like Schematron rules are embedded into XML Schemas.	Extensible schema documentation with XSLT 2.0	NA:NA	2018
Mirella M. Moro:Susan Malaika:Lipyeow Lim	In XML databases, new schema versions may be released as frequently as once every two weeks. This poster describes a taxonomy of changes for XML schema evolution. It examines the impact of those changes on schema validation and query evaluation. Based on that study, it proposes guidelines for XML schema evolution and for writing queries in such a way that they continue to operate as expected across evolving schemas.	Preserving XML queries during schema evolution	NA:NA:NA	2018
Erik Wilde:Felix Michel	XML is increasingly being used as a typed data format, and therefore it becomes more important to gain access to the type system; very often this is an XML Schema. The XML Schema Path Language (SPath) presented in this paper provides access to XML Schema components by extending the well-known XPath language to also include the domain of XML Schemas. Using SPath, XML developers gain access to XML Schemas and thus can more easily develop software which is type- or schema-aware, and thus more robust.	SPath: a path language for XML schema	NA:NA	2018
Katsuko T. Nakahira:Masashi Matsui:Yoshiki Mikami	Since conventional historical records have been written assuming human readers, they are not well-suited for computers to collect and process automatically. If computers could understand descriptions in historical records and process them automatically, it would be easy to analyze them from different perspectives. In this paper, we review a number of existing frameworks used to describe historical events, and make a comparative assessment of these frameworks in terms of usability, based on "deep cases" of Fillmore's core grammar. Based on this assessment, we propose a new description framework, and have created a microformat vocabulary set suitable for that framework.	The use of XML to express a historical knowledge base	NA:NA:NA	2018
Yuan Kui Shen:David R. Karger	In this paper, we describe a system that can extract recordstructures from web pages with no direct human supervision.Records are commonly occurring HTML-embedded data tuples that describe people, offered courses, products,company profiles, etc. We present a simplified frameworkfor studying the problem of unsupervised record extraction. one which separates the algorithms from the feature engineering.Our system, U-REST formalizes an approach tothe problem of unsupervised record extraction using a simple two-stage machine learning framework. The first stage involves clustering, where structurally similar regions are discovered, and the second stage involves classification, where discovered groupings (clusters of regions) are ranked by their likelihood of being records. In our work, we describe, and summarize the results of an extensive survey of features for both stages. We conclude by comparing U-REST to related systems. The results of our empirical evaluation show encouraging improvements in extraction accuracy.	U-REST: an unsupervised record extraction system	NA:NA	2018
Nikolay Anisimov:Brian Galvin:Herbert Ristock	In this paper, we consider a way to represent contact center applications as a set of multiple XML documents written in different markups including VoiceXML and CCXML. Applications can comprise a dialog with IVR, call routing and agent scripting functionalities. We also consider ways how such applications can be executed in run-time contact center environment.	XML-based multimodal interaction framework for contact center applications	NA:NA:NA	2018
Xing Xie	NA	Session details: Browsers and UI	NA	2018
Jae-wook Ahn:Peter Brusilovsky:Daqing He:Jonathan Grady:Qi Li	Personalized Web search has emerged as one of the hottest topics for both the Web industry and academic researchers. However, the majority of studies on personalized search focused on a rather simple type of search, which leaves an important research topic - the personalization in exploratory searches - as an under-studied area. In this paper, we present a study of personalization in task-based information exploration using a system called TaskSieve. TaskSieve is a Web search system that utilizes a relevance feedback based profile, called a "task model", for personalization. Its innovations include flexible and user controlled integration of queries and task models, task-infused text snippet generation, and on-screen visualization of task models. Through an empirical study using human subjects conducting task-based exploration searches, we demonstrate that TaskSieve pushes significantly more relevant documents to the top of search result lists as compared to a traditional search system. TaskSieve helps users select significantly more accurate information for their tasks, allows the users to do so with higher productivity, and is viewed more favorably by subjects under several usability related characteristics.	Personalized web exploration with task models	NA:NA:NA:NA:NA	2018
Yeliz Yesilada:Caroline Jay:Robert Stevens:Simon Harper	This paper presents an eye-tracking study that examines how people use the visual elements of Web pages to complete certain tasks. Whilst these elements are available to play their role in these tasks for sighted users, it is not the case for visually disabled users. This lack of access to some visual elements of a page means that visually disabled users are hindered in accomplishing these tasks. Our previous work has introduced a framework that identifies these elements and then reengineers Web pages such that these elements can play their intended roles in an audio, as well as visual presentation. To further improve our understanding of how these elements are used and to validate our framework, we track the eye movements of sighted users performing a number of different tasks. The resulting gaze data show that there is a strong relationship between the aspects of a page that receive visual attention and the objects identified by our framework. The study also shows some limitations, as well as yielding information to address these short-comings. Perhaps the most important result is the support provided for a particular kind of object called a Way Edge - the visual construct used to group content into sections. There is a significant effect of Way Edges on the distribution of attention across tasks. This is a result that not only provides strong evidence for the utility of re-engineering, but also has consequences for our understanding of how people allocate attention to different parts of a page. We speculate that the phenomenon of 'Banner Blindness' owes as much to Way Edges, as it does to colour and font size.	Validating the use and role of visual elements of web pages in navigation with an eye-tracking study	NA:NA:NA:NA	2018
Zhiwei Li:Shuming Shi:Lei Zhang	Current web search engines return result pages containing mostly text summary even though the matched web pages may contain informative pictures. A text excerpt (i.e. snippet) is generated by selecting keywords around the matched query terms for each returned page to provide context for user's relevance judgment. However, in many scenarios, we found that the pictures in web pages, if selected properly, could be added into search result pages and provide richer contextual description because a picture is worth a thousand words. Such new summary is named as image excerpts. By well designed user study, we demonstrate image excerpts can help users make much quicker relevance judgment of search results for a wide range of query types. To implement this idea, we propose a practicable approach to automatically generate image excerpts in the result pages by considering the dominance of each picture in each web page and the relevance of the picture to the query. We also outline an efficient way to incorporate image excerpts in web search engines. Web search engines can adopt our approach by slightly modifying their index and inserting a few low cost operations in their workflow. Our experiments on a large web dataset indicate the performance of the proposed approach is very promising.	Improving relevance judgment of web search results with image excerpts	NA:NA:NA	2018
Leo Spalteholz:Kin Fun Li:Nigel Livingston:Foad Hamidi	For many users with a physical or motor disability, using a computer mouse or other pointing device to navigate the web is cumbersome or impossible due to problems with pointing accuracy. At the same time, web accessibility using a keyboard in major browsers is rudimentary, requiring many key presses to select links or other elements. We introduce KeySurf, a character controlled web navigation system which addresses this situation by presenting an interface which allows a user to activate any web page element with only two or three keystrokes. Through an implementation of a user-centric incremental search algorithm, elements are matched according to user expectation as characters are entered into the interface. We show how our interface can be integrated with a speech recognition input, as well as with specialized on-screen keyboards for people with disabilities. Using the user's browsing history, we improve the efficiency of the selection process and find potentially interesting page links for the user within the current web page. We present the results from a pilot study evaluating the performance of various components of our system.	Keysurf: a character controlled browser for people with physical disabilities	NA:NA:NA:NA	2018
Masaru Kitsuregawa	NA	Session details: Data mining: log analysis	NA	2018
Barbara Poblete:Ricardo Baeza-Yates	In this paper we present a new document representation model based on implicit user feedback obtained from search engine queries. The main objective of this model is to achieve better results in non-supervised tasks, such as clustering and labeling, through the incorporation of usage data obtained from search engine queries. This type of model allows us to discover the motivations of users when visiting a certain document. The terms used in queries can provide a better choice of features, from the user's point of view, for summarizing the Web pages that were clicked from these queries. In this work we extend and formalize as "query model" an existing but not very well known idea of "query view" for document representation. Furthermore, we create a novel model based on "frequent query patterns" called the "query-set model". Our evaluation shows that both "query-based" models outperform the vector-space model when used for clustering and labeling documents in a website. In our experiments, the query-set model reduces by more than 90% the number of features needed to represent a set of documents and improves by over 90% the quality of the results. We believe that this can be explained because our model chooses better features and provides more accurate labels according to the user's expectations.	Query-sets: using implicit feedback and query patterns to organize web documents	NA:NA	2018
Mikhail Bilenko:Ryen W. White	The paper proposes identifying relevant information sources from the history of combined searching and browsing behavior of many Web users. While it has been previously shown that user interactions with search engines can be employed to improve document ranking, browsing behavior that occurs beyond search result pages has been largely overlooked in prior work. The paper demonstrates that users' post-search browsing activity strongly reflects implicit endorsement of visited pages, which allows estimating topical relevance of Web resources by mining large-scale datasets of search trails. We present heuristic and probabilistic algorithms that rely on such datasets for suggesting authoritative websites for search queries. Experimental evaluation shows that exploiting complete post-search browsing trails outperforms alternatives in isolation (e.g., clickthrough logs), and yields accuracy improvements when employed as a feature in learning to rank for Web search.	Mining the search trails of surfing crowds: identifying relevant websites from user activity	NA:NA	2018
Ariel Fuxman:Panayiotis Tsaparas:Kannan Achan:Rakesh Agrawal	In the sponsored search model, search engines are paid by businesses that are interested in displaying ads for their site alongside the search results. Businesses bid for keywords, and their ad is displayed when the keyword is queried to the search engine. An important problem in this process is 'keyword generation': given a business that is interested in launching a campaign, suggest keywords that are related to that campaign. We address this problem by making use of the query logs of the search engine. We identify queries related to a campaign by exploiting the associations between queries and URLs as they are captured by the user's clicks. These queries form good keyword suggestions since they capture the "wisdom of the crowd" as to what is related to a site. We formulate the problem as a semi-supervised learning problem, and propose algorithms within the Markov Random Field model. We perform experiments with real query logs, and we demonstrate that our algorithms scale to large query logs and produce meaningful results.	Using the wisdom of the crowds for keyword generation	NA:NA:NA:NA	2018
Alex Ntoulas	NA	Session details: Data mining: learning	NA	2018
Xiaoxun Zhang:Xueying Wang:Honglei Guo:Zhili Guo:Xian Wu:Zhong Su	This paper is concerned with the problem of Imbalanced Classification (IC) in web mining, which often arises on the web due to the "Matthew Effect". As web IC applications usually need to provide online service for user and deal with large volume of data, classification speed emerges as an important issue to be addressed. In face detection, Asymmetric Cascade is used to speed up imbalanced classification by building a cascade structure of simple classifiers, but it often causes a loss of classification accuracy due to the iterative feature addition in its learning procedure. In this paper, we adopt the idea of cascade classifier in imbalanced web mining for fast classification and propose a novel asymmetric cascade learning method called FloatCascade to improve the accuracy. To the end, FloatCascade selects fewer yet more effective features at each stage of the cascade classifier. In addition, a decision-tree scheme is adopted to enhance feature diversity and discrimination capability for FloatCascade learning. We evaluate FloatCascade through two typical IC applications in web mining: web page categorization and citation matching. Experimental results demonstrate the effectiveness and efficiency of FloatCascade comparing to the state-of-the-art IC methods like Asymmetric Cascade, Asymmetric AdaBoost and Weighted SVM.	Floatcascade learning for fast imbalanced web mining	NA:NA:NA:NA:NA:NA	2018
Yunbo Cao:Huizhong Duan:Chin-Yew Lin:Yong Yu:Hsiao-Wuen Hon	The paper is concerned with the problem of question recommendation. Specifically, given a question as query, we are to retrieve and rank other questions according to their likelihood of being good recommendations of the queried question. A good recommendation provides alternative aspects around users' interest. We tackle the problem of question recommendation in two steps: first represent questions as graphs of topic terms, and then rank recommendations on the basis of the graphs. We formalize both steps as the tree-cutting problems and then employ the MDL (Minimum Description Length) for selecting the best cuts. Experiments have been conducted with the real questions posted at Yahoo! Answers. The questions are about two domains, 'travel' and 'computers & internet'. Experimental results indicate that the use of the MDL-based tree cut model can significantly outperform the baseline methods of word-based VSM or phrase-based VSM. The results also show that the use of the MDL-based tree cut model is essential to our approach.	Recommending questions using the mdl-based tree cut model	NA:NA:NA:NA:NA	2018
Xuan-Hieu Phan:Le-Minh Nguyen:Susumu Horiguchi	This paper presents a general framework for building classifiers that deal with short and sparse text & Web segments by making the most of hidden topics discovered from large-scale data collections. The main motivation of this work is that many classification tasks working with short segments of text & Web, such as search snippets, forum & chat messages, blog & news feeds, product reviews, and book & movie summaries, fail to achieve high accuracy due to the data sparseness. We, therefore, come up with an idea of gaining external knowledge to make the data more related as well as expand the coverage of classifiers to handle future data better. The underlying idea of the framework is that for each classification task, we collect a large-scale external data collection called "universal dataset", and then build a classifier on both a (small) set of labeled training data and a rich set of hidden topics discovered from that data collection. The framework is general enough to be applied to different data domains and genres ranging from Web search results to medical text. We did a careful evaluation on several hundred megabytes of Wikipedia (30M words) and MEDLINE (18M words) with two tasks: "Web search domain disambiguation" and "disease categorization for medical text", and achieved significant quality enhancement.	Learning to classify short and sparse text & web with hidden topics from large-scale data collections	NA:NA:NA	2018
Kunal Punera	NA	Session details: Data mining: modeling	NA	2018
Qiaozhu Mei:Deng Cai:Duo Zhang:ChengXiang Zhai	In this paper, we formally define the problem of topic modeling with network structure (TMN). We propose a novel solution to this problem, which regularizes a statistical topic model with a harmonic regularizer based on a graph structure in the data. The proposed method bridges topic modeling and social network analysis, which leverages the power of both statistical topic models and discrete regularization. The output of this model well summarizes topics in text, maps a topic on the network, and discovers topical communities. With concrete selection of a topic model and a graph-based regularizer, our model can be applied to text mining problems such as author-topic analysis, community discovery, and spatial text mining. Empirical experiments on two different genres of data show that our approach is effective, which improves text-oriented methods as well as network-oriented methods. The proposed model is general; it can be applied to any text collections with a mixture of topics and an associated network structure.	Topic modeling with network regularization	NA:NA:NA:NA	2018
Ivan Titov:Ryan McDonald	In this paper we present a novel framework for extracting the ratable aspects of objects from online user reviews. Extracting such aspects is an important challenge in automatically mining product opinions from the web and in generating opinion-based summaries of user reviews [18, 19, 7, 12, 27, 36, 21]. Our models are based on extensions to standard topic modeling methods such as LDA and PLSA to induce multi-grain topics. We argue that multi-grain models are more appropriate for our task since standard models tend to produce topics that correspond to global properties of objects (e.g., the brand of a product type) rather than the aspects of an object that tend to be rated by a user. The models we present not only extract ratable aspects, but also cluster them into coherent topics, e.g., 'waitress' and 'bartender' are part of the same topic 'staff' for restaurants. This differentiates it from much of the previous work which extracts aspects through term frequency analysis with minimal clustering. We evaluate the multi-grain models both qualitatively and quantitatively to show that they improve significantly upon standard topic models.	Modeling online reviews with multi-grain topic models	NA:NA	2018
Yue Lu:Chengxiang Zhai	Web 2.0 technology has enabled more and more people to freely express their opinions on the Web, making the Web an extremely valuable source for mining user opinions about all kinds of topics. In this paper we study how to automatically integrate opinions expressed in a well-written expert review with lots of opinions scattering in various sources such as blogspaces and forums. We formally define this new integration problem and propose to use semi-supervised topic models to solve the problem in a principled way. Experiments on integrating opinions about two quite different topics (a product and a political figure) show that the proposed method is effective for both topics and can generate useful aligned integrated opinion summaries. The proposed method is quite general. It can be used to integrate a well written review with opinions in an arbitrary text collection about any topic to potentially support many interesting applications in multiple domains.	Opinion integration through semi-supervised topic modeling	NA:NA	2018
Ravi Kumar	NA	Session details: Data mining: algorithms	NA	2018
Chuan Xiao:Wei Wang:Xuemin Lin:Jeffrey Xu Yu	With the increasing amount of data and the need to integrate data from multiple data sources, a challenging issue is to find near duplicate records efficiently. In this paper, we focus on efficient algorithms to find pairs of records such that their similarities are above a given threshold. Several existing algorithms rely on the prefix filtering principle to avoid computing similarity values for all possible pairs of records. We propose new filtering techniques by exploiting the ordering information; they are integrated into the existing methods and drastically reduce the candidate sizes and hence improve the efficiency. Experimental results show that our proposed algorithms can achieve up to 2.6x - 5x speed-up over previous algorithms on several real datasets and provide alternative solutions to the near duplicate Web page detection problem.	Efficient similarity joins for near duplicate detection	NA:NA:NA:NA	2018
Ding Zhou:Shenghuo Zhu:Kai Yu:Xiaodan Song:Belle L. Tseng:Hongyuan Zha:C. Lee Giles	The Web offers rich relational data with different semantics. In this paper, we address the problem of document recommendation in a digital library, where the documents in question are networked by citations and are associated with other entities by various relations. Due to the sparsity of a single graph and noise in graph construction, we propose a new method for combining multiple graphs to measure document similarities, where different factorization strategies are used based on the nature of different graphs. In particular, the new method seeks a single low-dimensional embedding of documents that captures their relative similarities in a latent space. Based on the obtained embedding, a new recommendation framework is developed using semi-supervised learning on graphs. In addition, we address the scalability issue and propose an incremental algorithm. The new incremental method significantly improves the efficiency by calculating the embedding for new incoming documents only. The new batch and incremental methods are evaluated on two real world datasets prepared from CiteSeer. Experiments demonstrate significant quality improvement for our batch method and significant efficiency improvement with tolerable quality loss for our incremental method.	Learning multiple graphs for document recommendations	NA:NA:NA:NA:NA:NA:NA	2018
Kunal Punera:Joydeep Ghosh	Hierarchical topic taxonomies have proliferated on the World Wide Web [5, 18], and exploiting the output space decompositions they induce in automated classification systems is an active area of research. In many domains, classifiers learned on a hierarchy of classes have been shown to outperform those learned on a flat set of classes. In this paper we argue that the hierarchical arrangement of classes leads to intuitive relationships between the corresponding classifiers' output scores, and that enforcing these relationships as a post-processing step after classification can improve its accuracy. We formulate the task of smoothing classifier outputs as a regularized isotonic tree regression problem, and present a dynamic programming based method that solves it optimally. This new problem generalizes the classic isotonic tree regression problem, and both, the new formulation and algorithm, might be of independent interest. In our empirical analysis of two real-world text classification scenarios, we show that our approach to smoothing classifier outputs results in improved classification accuracy.	Enhanced hierarchical classification via isotonic smoothing	NA:NA	2018
Amin Saberi	NA	Session details: Internet monetization: online advertising	NA	2018
Arpita Ghosh:Mohammad Mahdian	Most models for online advertising assume that an advertiser's value from winning an ad auction, which depends on the clickthrough rate or conversion rate of the advertisement, is independent of other advertisements served alongside it in the same session. This ignores an important 'externality effect': as the advertising audience has a limited attention span, a high-quality ad on a page can detract attention from other ads on the same page. That is, the utility to a winner in such an auction also depends on the set of other winners. In this paper, we introduce the problem of modeling externalities in online advertising, and study the winner determination problem in these models. Our models are based on choice models on the audience side. We show that in the most general case, the winner determination problem is hard even to approximate. However, we give an approximation algorithm for this problem with an approximation factor that is logarithmic in the ratio of the maximum to the minimum bid. Furthermore, we show that there are some interesting special cases, such as the case where the audience preferences are single peaked, where the problem can be solved exactly in polynomial time. For all these algorithms, we prove that the winner determination algorithm can be combined with VCG-style payments to yield truthful mechanisms.	Externalities in online advertising	NA:NA	2018
Ureil Feige:Nicole Immorlica:Vahab Mirrokni:Hamid Nazerzadeh	Most current banner advertising is sold through negotiation thereby incurring large transaction costs and possibly suboptimal allocations. We propose a new automated system for selling banner advertising. In this system, each advertiser specifies a collection of host webpages which are relevant to his product, a desired total quantity of impressions on these pages, and a maximum per-impression price. The system selects a subset of advertisers as 'winners' and maps each winner to a set of impressions on pages within his desired collection. The distinguishing feature of our system as opposed to current combinatorial allocation mechanisms is that, mimicking the current negotiation system, we guarantee that winners receive at least as many advertising opportunities as they requested or else receive ample compensation in the form of a monetary payment by the host. Such guarantees are essential in markets like banner advertising where a major goal of the advertising campaign is developing brand recognition. As we show, the problem of selecting a feasible subset of advertisers with maximum total value is inapproximable. We thus present two greedy heuristics and discuss theoretical techniques to measure their performances. Our first algorithm iteratively selects advertisers and corresponding sets of impressions which contribute maximum marginal per-impression profit to the current solution. We prove a bi-criteria approximation for this algorithm, showing that it generates approximately as much value as the optimum algorithm on a slightly harder problem. However, this algorithm might perform poorly on instances in which the value of the optimum solution is quite large, a clearly undesirable failure mode. Hence, we present an adaptive greedy algorithm which again iteratively selects advertisers with maximum marginal per-impression profit, but additionally reassigns impressions at each iteration. For this algorithm, we prove a structural approximation result, a newly defined framework for evaluating heuristics [10]. We thereby prove that this algorithm has a better performance guarantee than the simple greedy algorithm.	A combinatorial allocation mechanism with penalties for banner advertising	NA:NA:NA:NA	2018
Hamid Nazerzadeh:Amin Saberi:Rakesh Vohra	We study the Cost-Per-Action or Cost-Per-Acquisition (CPA) charging scheme in online advertising. In this scheme, instead of paying per click, the advertisers pay only when a user takes a specific action (e.g. fills out a form) or completes a transaction on their websites. We focus on designing efficient and incentive compatible mechanisms that use this charging scheme. We describe a mechanism based on a sampling-based learning algorithm that under suitable assumptions is asymptotically individually rational, asymptotically Bayesian incentive compatible and asymptotically ex-ante efficient. In particular, we demonstrate our mechanism for the case where the utility functions of the advertisers are independent and identically-distributed random variables as well as the case where they evolve like independent reflected Brownian motions.	Dynamic cost-per-action mechanisms and applications to online advertising	NA:NA:NA	2018
Nicole Immorlica	NA	Session details: Internet monetization: recommendation and security	NA	2018
Jason Hartline:Vahab Mirrokni:Mukund Sundararajan	We discuss the use of social networks in implementing viral marketing strategies. While influence maximization has been studied in this context (see Chapter 24 of [10]), we study revenue maximization, arguably, a more natural objective. In our model, a buyer's decision to buy an item is influenced by the set of other buyers that own the item and the price at which the item is offered. We focus on algorithmic question of finding revenue maximizing marketing strategies. When the buyers are completely symmetric, we can find the optimal marketing strategy in polynomial time. In the general case, motivated by hardness results, we investigate approximation algorithms for this problem. We identify a family of strategies called influence-and-exploit strategies that are based on the following idea: Initially influence the population by giving the item for free to carefully a chosen set of buyers. Then extract revenue from the remaining buyers using a 'greedy' pricing strategy. We first argue why such strategies are reasonable and then show how to use recently developed set-function maximization techniques to find the right set of buyers to influence.	Optimal marketing strategies over social networks	NA:NA:NA	2018
Reid Andersen:Christian Borgs:Jennifer Chayes:Uriel Feige:Abraham Flaxman:Adam Kalai:Vahab Mirrokni:Moshe Tennenholtz	High-quality, personalized recommendations are a key feature in many online systems. Since these systems often have explicit knowledge of social network structures, the recommendations may incorporate this information. This paper focuses on networks that represent trust and recommendation systems that incorporate these trust relationships. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the opinions of other users in the trust network. In analogy to prior work on voting and ranking systems, we use the axiomatic approach from the theory of social choice. We develop a set of five natural axioms that a trust-based recommendation system might be expected to satisfy. Then, we show that no system can simultaneously satisfy all the axioms. However, for any subset of four of the five axioms we exhibit a recommendation system that satisfies those axioms. Next we consider various ways of weakening the axioms, one of which leads to a unique recommendation system based on random walks. We consider other recommendation systems, including systems based on personalized PageRank, majority of majorities, and minimum cuts, and search for alternative axiomatizations that uniquely characterize these systems. Finally, we determine which of these systems are incentive compatible, meaning that groups of agents interested in manipulating recommendations can not induce others to share their opinion by lying about their votes or modifying their trust links. This is an important property for systems deployed in a monetized environment.	Trust-based recommendation systems: an axiomatic approach	NA:NA:NA:NA:NA:NA:NA:NA	2018
Jens Grossklags:Nicolas Christin:John Chuang	Despite general awareness of the importance of keeping one's system secure, and widespread availability of consumer security technologies, actual investment in security remains highly variable across the Internet population, allowing attacks such as distributed denial-of-service (DDoS) and spam distribution to continue unabated. By modeling security investment decision-making in established (e.g., weakest-link, best-shot) and novel games (e.g., weakest-target), and allowing expenditures in self-protection versus self-insurance technologies, we can examine how incentives may shift between investment in a public good (protection) and a private good (insurance), subject to factors such as network size, type of attack, loss probability, loss magnitude, and cost of technology. We can also characterize Nash equilibria and social optima for different classes of attacks and defenses. In the weakest-target game, an interesting result is that, for almost all parameter settings, more effort is exerted at Nash equilibrium than at the social optimum. We may attribute this to the "strategic uncertainty" of players seeking to self-protect at just slightly above the lowest protection level.	Secure or insure?: a game-theoretic analysis of information security games	NA:NA:NA	2018
Ying Li	NA	Session details: Internet monetization: sponsored Search	NA	2018
Anindya Ghose:Sha Yang	The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines. Firms across different industries have are beginning to adopt this as the primary form of online advertising. This process works on an auction mechanism in which advertisers bid for different keywords, and final rank for a given keyword is allocated by the search engine. But how different are firm's actual bids from their optimal bids? Moreover, what are other ways in which firms can potentially benefit from sponsored search advertising? Based on the model and estimates from prior work [10], we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords. Further, we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories, and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits. We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement. We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for. Latency (the time it takes for consumer to place a purchase order after clicking on the advertisement) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet.	Analyzing search engine advertising: firm behavior and cross-selling in electronic markets	NA:NA	2018
Massimiliano Ciaramita:Vanessa Murdock:Vassilis Plachouras	Sponsored search is one of the enabling technologies for today's Web search engines. It corresponds to matching and showing ads related to the user query on the search engine results page. Users are likely to click on topically related ads and the advertisers pay only when a user clicks on their ad. Hence, it is important to be able to predict if an ad is likely to be clicked, and maximize the number of clicks. We investigate the sponsored search problem from a machine learning perspective with respect to three main sub-problems: how to use click data for training and evaluation, which learning framework is more suitable for the task, and which features are useful for existing models. We perform a large scale evaluation based on data from a commercial Web search engine. Results show that it is possible to learn and evaluate directly and exclusively on click data encoding pairwise preferences following simple and conservative assumptions. We find that online multilayer perceptron learning, based on a small set of features representing content similarity of different kinds, significantly outperforms an information retrieval baseline and other learning models, providing a suitable framework for the sponsored search task.	Online learning from click data for sponsored search	NA:NA:NA	2018
Wang-Chien Lee	NA	Session details: Mobility	NA	2018
Bhuvan Bamba:Ling Liu:Peter Pesti:Ting Wang	This paper presents PrivacyGrid - a framework for supporting anonymous location-based queries in mobile information delivery systems. The PrivacyGrid framework offers three unique capabilities. First, it provides a location privacy protection preference profile model, called location P3P, which allows mobile users to explicitly define their preferred location privacy requirements in terms of both location hiding measures (e.g., location k-anonymity and location l-diversity) and location service quality measures (e.g., maximum spatial resolution and maximum temporal resolution). Second, it provides fast and effective location cloaking algorithms for location k-anonymity and location l-diversity in a mobile environment. We develop dynamic bottom-up and top-down grid cloaking algorithms with the goal of achieving high anonymization success rate and efficiency in terms of both time complexity and maintenance cost. A hybrid approach that carefully combines the strengths of both bottom-up and top-down cloaking approaches to further reduce the average anonymization time is also developed. Last but not the least, PrivacyGrid incorporates temporal cloaking into the location cloaking process to further increase the success rate of location anonymization. We also discuss PrivacyGrid mechanisms for supporting anonymous location queries. Experimental evaluation shows that the PrivacyGrid approach can provide close to optimal location k-anonymity as defined by per user location P3P without introducing significant performance penalties.	Supporting anonymous location queries in mobile environments with privacygrid	NA:NA:NA:NA	2018
Yu Zheng:Like Liu:Longhao Wang:Xing Xie	Geographic information has spawned many novel Web applications where global positioning system (GPS) plays important roles in bridging the applications and end users. Learning knowledge from users' raw GPS data can provide rich context information for both geographic and mobile applications. However, so far, raw GPS data are still used directly without much understanding. In this paper, an approach based on supervised learning is proposed to automatically infer transportation mode from raw GPS data. The transportation mode, such as walking, driving, etc., implied in a user's GPS data can provide us valuable knowledge to understand the user. It also enables context-aware computing based on user's present transportation mode and design of an innovative user interface for Web users. Our approach consists of three parts: a change point-based segmentation method, an inference model and a post-processing algorithm based on conditional probability. The change point-based segmentation method was compared with two baselines including uniform duration based and uniform length based methods. Meanwhile, four different inference models including Decision Tree, Bayesian Net, Support Vector Machine (SVM) and Conditional Random Field (CRF) are studied in the experiments. We evaluated the approach using the GPS data collected by 45 users over six months period. As a result, beyond other two segmentation methods, the change point based method achieved a higher degree of accuracy in predicting transportation modes and detecting transitions between them. Decision Tree outperformed other inference models over the change point based segmentation method.	Learning transportation mode from raw gps data for geographic applications on the web	NA:NA:NA:NA	2018
Jeonghee Yi:Farzin Maghoul:Jan Pedersen	In this paper we study the characteristics of search queries submitted from mobile devices using various Yahoo! one-Search applications during a 2 months period in the second half of 2007, and report the query patterns derived from 20 million English sample queries submitted by users in US, Canada, Europe, and Asia. We examine the query distribution and topical categories the queries belong to in order to find new trends. We compare and contrast the search patterns between US vs international queries, and between queries from various search interfaces (XHTML/WAP, java widgets, and SMS). We also compare our results with previous studies wherever possible, either to confirm previous findings, or to find interesting differences in the query distribution and pattern.	Deciphering mobile search patterns: a study of Yahoo! mobile search queries	NA:NA:NA	2018
Anirban Mahanti	NA	Session details: Performance and scalability	NA	2018
Zhou Wei:Jiang Dejun:Guillaume Pierre:Chi-Hung Chi:Maarten van Steen	Many techniques have been proposed to scale web applications. However, the data interdependencies between the database queries and transactions issued by the applications limit their efficiency. We claim that major scalability improvements can be gained by restructuring the web application data into multiple independent data services with exclusive access to their private data store. While this restructuring does not provide performance gains by itself, the implied simplification of each database workload allows a much more efficient use of classical techniques. We illustrate the data denormalization process on three benchmark applications: TPC-W, RUBiS and RUBBoS. We deploy the resulting service-oriented implementation of TPC-W across an 85-node cluster and show that restructuring its data can provide at least an order of magnitude improvement in the maximum sustainable throughput compared to master-slave database replication, while preserving strong consistency and transactional properties.	Service-oriented data denormalization for scalable web applications	NA:NA:NA:NA:NA	2018
Hussein A. Alzoubi:Seungjoon Lee:Michael Rabinovich:Oliver Spatscheck:Jacobus Van der Merwe	Because it is an integral part of the Internet routing apparatus, and because it allows multiple instances of the same service to be "naturally" discovered, IP Anycast has many attractive features for any service that involve the replication of multiple instances across the Internet. While briefly considered as an enabler when content distribution networks (CDNs) first emerged, the use of IP Anycast was deemed infeasible in that environment. The main reasons for this decision were the lack of load awareness of IP Anycast and unwanted side effects of Internet routing changes on the IP Anycast mechanism. Prompted by recent developments in route control technology, as well as a better understanding of the behavior of IP Anycast in operational settings, we revisit this decision and propose a load-aware IP Anycast CDN architecture that addresses these concerns while benefiting from inherent IP Anycast features. Our architecture makes use of route control mechanisms to take server and network load into account to realize load-aware Anycast. We show that the resulting redirection requirements can be formulated as a Generalized Assignment Problem and present practical algorithms that address these requirements while at the same time limiting session disruptions that plague regular IP Anycast. We evaluate our algorithms through trace based simulation using traces obtained from an operation CDN network.	Anycast CDNS revisited	NA:NA:NA:NA:NA	2018
Naimul Basher:Aniket Mahanti:Anirban Mahanti:Carey Williamson:Martin Arlitt	Peer-to-Peer (P2P) applications continue to grow in popularity, and have reportedly overtaken Web applications as the single largest contributor to Internet traffic. Using traces collected from a large edge network, we conduct an extensive analysis of P2P traffic, compare P2P traffic with Web traffic, and discuss the implications of increased P2P traffic. In addition to studying the aggregate P2P traffic, we also analyze and compare the two main constituents of P2P traffic in our data, namely BitTorrent and Gnutella. The results presented in the paper may be used for generating synthetic workloads, gaining insights into the functioning of P2P applications, and developing network management strategies. For example, our results suggest that new models are necessary for Internet traffic. As a first step, we present flow-level distributional models for Web and P2P traffic that may be used in network simulation and emulation experiments.	A comparative analysis of web and peer-to-peer traffic	NA:NA:NA:NA:NA	2018
Edward Chang	NA	Session details: Rich media	NA	2018
Lyndon S. Kennedy:Mor Naaman	Can we leverage the community-contributed collections of rich media on the web to automatically generate representative and diverse views of the world's landmarks? We use a combination of context- and content-based tools to generate representative sets of images for location-driven features and landmarks, a common search task. To do that, we using location and other metadata, as well as tags associated with images, and the images' visual features. We present an approach to extracting tags that represent landmarks. We show how to use unsupervised methods to extract representative views and images for each landmark. This approach can potentially scale to provide better search and representation for landmarks, worldwide. We evaluate the system in the context of image search using a real-life dataset of 110,000 images from the San Francisco area.	Generating diverse and representative image search results for landmarks	NA:NA	2018
Yushi Jing:Shumeet Baluja	In this paper, we cast the image-ranking problem into the task of identifying "authority" nodes on an inferred visual similarity graph and propose an algorithm to analyze the visual link structure that can be created among a group of images. Through an iterative procedure based on the PageRank computation, a numerical weight is assigned to each image; this measures its relative importance to the other images being considered. The incorporation of visual signals in this process differs from the majority of large-scale commercial-search engines in use today. Commercial search-engines often solely rely on the text clues of the pages in which images are embedded to rank images, and often entirely ignore the content of the images themselves as a ranking signal. To quantify the performance of our approach in a real-world system, we conducted a series of experiments based on the task of retrieving images for 2000 of the most popular products queries. Our experimental results show significant improvement, in terms of user satisfaction and relevancy, in comparison to the most recent Google Image Search results.	Pagerank for product image search	NA:NA	2018
Manjeet Rege:Ming Dong:Jing Hua	With the explosive growth of Web and the recent development in digital media technology, the number of images on the Web has grown tremendously. Consequently, Web image clustering has emerged as an important application. Some of the initial efforts along this direction revolved around clustering Web images based on the visual features of images or textual features by making use of the text surrounding the images. However, not much work has been done in using multimodal information for clustering Web images. In this paper, we propose a graph theoretical framework for simultaneously integrating visual and textual features for efficient Web image clustering. Specifically, we model visual features, images and words from surrounding text using a tripartite graph. Partitioning this graph leads to clustering of the Web images. Although, graph partitioning approach has been adopted before, the main contribution of this work lies in a new algorithm that we propose - Consistent Isoperimetric High-order Co-clustering (CIHC), for partitioning the tripartite graph. Computationally, CIHC is very quick as it requires a simple solution to a sparse system of linear equations. Our theoretical analysis and extensive experiments performed on real Web images demonstrate the performance of CIHC in terms of the quality, efficiency and scalability in partitioning the visual feature-image-word tripartite graph.	Graph theoretical framework for simultaneously integrating visual and textual features for efficient web image clustering	NA:NA:NA	2018
Börkur Sigurbjörnsson:Roelof van Zwol	Online photo services such as Flickr and Zooomr allow users to share their photos with family, friends, and the online community at large. An important facet of these services is that users manually annotate their photos using so called tags, which describe the contents of the photo or provide additional contextual and semantical information. In this paper we investigate how we can assist users in the tagging phase. The contribution of our research is twofold. We analyse a representative snapshot of Flickr and present the results by means of a tag characterisation focussing on how users tags photos and what information is contained in the tagging. Based on this analysis, we present and evaluate tag recommendation strategies to support the user in the photo annotation task by recommending a set of tags that can be added to the photo. The results of the empirical evaluation show that we can effectively recommend relevant tags for a variety of photos with different levels of exhaustiveness of original tagging.	Flickr tag recommendation based on collective knowledge	NA:NA	2018
Nick Craswell	NA	Session details: Search: query analysis	NA	2018
Atsushi Fujii	Several types of queries are widely used on the World Wide Web and the expected retrieval method can vary depending on the query type. We propose a method for classifying queries into informational and navigational types. Because terms in navigational queries often appear in anchor text for links to other pages, we analyze the distribution of query terms in anchor texts on the Web for query classification purposes. While content-based retrieval is effective for informational queries, anchor-based retrieval is effective for navigational queries. Our retrieval system combines the results obtained with the content-based and anchor-based retrieval methods, in which the weight for each retrieval result is determined automatically depending on the result of the query classification. We also propose a method for improving anchor-based retrieval. Our retrieval method, which computes the probability that a document is retrieved in response to the given query, identifies synonyms of query terms in the anchor texts on the Web and uses these synonyms for smoothing purposes in the probability estimation. We use the NTCIR test collections and show the effectiveness of individual methods and the entire Web retrieval system experimentally.	Modeling anchor text and classifying queries to enhance web document retrieval	NA	2018
Bin Tan:Fuchun Peng	In this paper, we propose a novel unsupervised approach to query segmentation, an important task in Web search. We use a generative query model to recover a query's underlying concepts that compose its original segmented form. The model's parameters are estimated using an expectation-maximization (EM) algorithm, optimizing the minimum description length objective function on a partial corpus that is specific to the query. To augment this unsupervised learning, we incorporate evidence from Wikipedia. Experiments show that our approach dramatically improves performance over the traditional approach that is based on mutual information, and produces comparable results with a supervised method. In particular, the basic generative language model contributes a 7.4% improvement over the mutual information based method (measured by segment F1 on the Intersection test set). EM optimization further improves the performance by 14.3%. Additional knowledge from Wikipedia provides another improvement of 24.3%, adding up to a total of 46% improvement (from 0.530 to 0.774).	Unsupervised query segmentation using generative language models and wikipedia	NA:NA	2018
Lars Backstrom:Jon Kleinberg:Ravi Kumar:Jasmine Novak	Local aspects of Web search - associating Web content and queries with geography - is a topic of growing interest. However, the underlying question of how spatial variation is manifested in search queries is still not well understood. Here we develop a probabilistic framework for quantifying such spatial variation; on complete Yahoo! query logs, we find that our model is able to localize large classes of queries to within a few miles of their natural centers based only on the distribution of activity for the query. Our model provides not only an estimate of a query's geographic center, but also a measure of its spatial dispersion, indicating whether it has highly local interest or broader regional or national appeal. We also show how variations on our model can track geographically shifting topics over time, annotate a map with each location's "distinctive queries", and delineate the "spheres of influence" for competing queries in the same general domain.	Spatial variation in search engine queries	NA:NA:NA:NA	2018
Hongyuan Zha	NA	Session details: Search: corpus characterization and Search Perform	NA	2018
Ricardo Baeza-Yates:Álvaro Pereira:Nivio Ziviani	This paper presents an extensive study about the evolution of textual content on the Web, which shows how some new pages are created from scratch while others are created using already existing content. We show that a significant fraction of the Web is a byproduct of the latter case. We introduce the concept of Web genealogical tree, in which every page in a Web snapshot is classified into a component. We study in detail these components, characterizing the copies and identifying the relation between a source of content and a search engine, by comparing page relevance measures, documents returned by real queries performed in the past, and click-through data. We observe that sources of copies are more frequently returned by queries and more clicked than other documents.	Genealogical trees on the web: a search engine user perspective	NA:NA:NA	2018
Deepayan Chakrabarti:Ravi Kumar:Kunal Punera	We consider the problem of segmenting a webpage into visually and semantically cohesive pieces. Our approach is based on formulating an appropriate optimization problem on weighted graphs, where the weights capture if two nodes in the DOM tree should be placed together or apart in the segmentation; we present a learning framework to learn these weights from manually labeled data in a principled manner. Our work is a significant departure from previous heuristic and rule-based solutions to the segmentation problem. The results of our empirical analysis bring out interesting aspects of our framework, including variants of the optimization problem and the role of learning.	A graph-theoretic approach to webpage segmentation	NA:NA:NA	2018
Jiangong Zhang:Xiaohui Long:Torsten Suel	Due to the rapid growth in the size of the web, web search engines are facing enormous performance challenges. The larger engines in particular have to be able to process tens of thousands of queries per second on tens of billions of documents, making query throughput a critical issue. To satisfy this heavy workload, search engines use a variety of performance optimizations including index compression, caching, and early termination. We focus on two techniques, inverted index compression and index caching, which play a crucial rule in web search engines as well as other high-performance information retrieval systems. We perform a comparison and evaluation of several inverted list compression algorithms, including new variants of existing algorithms that have not been studied before. We then evaluate different inverted list caching policies on large query traces, and finally study the possible performance benefits of combining compression and caching. The overall goal of this paper is to provide an updated discussion and evaluation of these two techniques, and to show how to select the best set of approaches and settings depending on parameter such as disk speed and main memory cache size.	Performance of compressed inverted list caching in search engines	NA:NA:NA	2018
Ricardo Baeza-Yate	NA	Session details: Search: ranking and retrieval enhancement	NA	2018
Rong Jin:Hamed Valizadegan:Hang Li	We consider the problem of ranking refinement, i.e., to improve the accuracy of an existing ranking function with a small set of labeled instances. We are, particularly, interested in learning a better ranking function using two complementary sources of information, ranking information given by the existing ranking function (i.e., a base ranker) and that obtained from users' feedbacks. This problem is very important in information retrieval where the feedback is gradually collected. The key challenge in combining the two sources of information arises from the fact that the ranking information presented by the base ranker tends to be imperfect and the ranking information obtained from users' feedbacks tends to be noisy. We present a novel boosting framework for ranking refinement that can effectively leverage the uses of the two sources of information. Our empirical study shows that the proposed algorithm is effective for ranking refinement, and furthermore significantly outperforms the baseline algorithms that incorporate the outputs from the base ranker as an additional feature.	Ranking refinement and its application to information retrieval	NA:NA:NA	2018
Tao Qin:Tie-Yan Liu:Xu-Dong Zhang:De-Sheng Wang:Wen-Ying Xiong:Hang Li	Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects. The technology has been successfully applied to web search, and is becoming one of the key machineries for building search engines. Existing approaches to learning to rank, however, did not consider the cases in which there exists relationship between the objects to be ranked, despite of the fact that such situations are very common in practice. For example, in web search, given a query certain relationships usually exist among the the retrieved documents, e.g., URL hierarchy, similarity, etc., and sometimes it is necessary to utilize the information in ranking of the documents. This paper addresses the issue and formulates it as a novel learning problem, referred to as, 'learning to rank relational objects'. In the new learning task, the ranking model is defined as a function of not only the contents (features) of objects but also the relations between objects. The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined. It formalizes the learning task as an optimization problem in the setting. The paper then proposes a new method to perform the optimization task, particularly an implementation based on SVM. Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks (Pseudo Relevance Feedback and Topic Distillation) in web search, indicating that the proposed method can indeed make effective use of relation information and content information in ranking.	Learning to rank relational objects and its application to web search	NA:NA:NA:NA:NA:NA	2018
Deepayan Chakrabarti:Deepak Agarwal:Vanja Josifovski	Contextual advertising supports much of the Web's ecosystem today. User experience and revenue (shared by the site publisher and the ad network) depend on the relevance of the displayed ads to the page content. As with other document retrieval systems, relevance is provided by scoring the match between individual ads (documents) and the content of the page where the ads are shown (query). In this paper we show how this match can be improved significantly by augmenting the ad-page scoring function with extra parameters from a logistic regression model on the words in the pages and ads. A key property of the proposed model is that it can be mapped to standard cosine similarity matching and is suitable for efficient and scalable implementation over inverted indexes. The model parameter values are learnt from logs containing ad impressions and clicks, with shrinkage estimators being used to combat sparsity. To scale our computations to train on an extremely large training corpus consisting of several gigabytes of data, we parallelize our fitting algorithm in a Hadoop framework [10]. Experimental evaluation is provided showing improved click prediction over a holdout set of impression and click events from a large scale real-world ad placement engine. Our best model achieves a 25% lift in precision relative to a traditional information retrieval model which is based on cosine similarity, for recalling 10% of the clicks in our test data.	Contextual advertising by combining relevance with click feedback	NA:NA:NA	2018
Hang Li	NA	Session details: Search: crawlers	NA	2018
Hsin-Tsang Lee:Derek Leonard:Xiaoming Wang:Dmitri Loguinov	This paper shares our experience in designing a web crawler that can download billions of pages using a single-server implementation and models its performance. We show that with the quadratically increasing complexity of verifying URL uniqueness, BFS crawl order, and fixed per-host rate-limiting, current crawling algorithms cannot effectively cope with the sheer volume of URLs generated in large crawls, highly-branching spam, legitimate multi-million-page blog sites, and infinite loops created by server-side scripts. We offer a set of techniques for dealing with these issues and test their performance in an implementation we call IRLbot. In our recent experiment that lasted 41 days, IRLbot running on a single server successfully crawled 6.3 billion valid HTML pages ($7.6$ billion connection requests) and sustained an average download rate of 319 mb/s (1,789 pages/s). Unlike our prior experiments with algorithms proposed in related work, this version of IRLbot did not experience any bottlenecks and successfully handled content from over 117 million hosts, parsed out 394 billion links, and discovered a subset of the web graph with 41 billion unique nodes.	IRLbot: scaling to 6 billion pages and beyond	NA:NA:NA:NA	2018
Christopher Olston:Sandeep Pandey	It is crucial for a web crawler to distinguish between ephemeral and persistent content. Ephemeral content (e.g., quote of the day) is usually not worth crawling, because by the time it reaches the index it is no longer representative of the web page from which it was acquired. On the other hand, content that persists across multiple page updates (e.g., recent blog postings) may be worth acquiring, because it matches the page's true content for a sustained period of time. In this paper we characterize the longevity of information found on the web, via both empirical measurements and a generative model that coincides with these measurements. We then develop new recrawl scheduling policies that take longevity into account. As we show via experiments over real web data, our policies obtain better freshness at lower cost, compared with previous approaches.	Recrawl scheduling based on information longevity	NA:NA	2018
Rui Cai:Jiang-Ming Yang:Wei Lai:Yida Wang:Lei Zhang	We study in this paper the Web forum crawling problem, which is a very fundamental step in many Web applications, such as search engine and Web data mining. As a typical user-created content (UCC), Web forum has become an important resource on the Web due to its rich information contributed by millions of Internet users every day. However, Web forum crawling is not a trivial problem due to the in-depth link structures, the large amount of duplicate pages, as well as many invalid pages caused by login failure issues. In this paper, we propose and build a prototype of an intelligent forum crawler, iRobot, which has intelligence to understand the content and the structure of a forum site, and then decide how to choose traversal paths among different kinds of pages. To do this, we first randomly sample (download) a few pages from the target forum site, and introduce the page content layout as the characteristics to group those pre-sampled pages and re-construct the forum's sitemap. After that, we select an optimal crawling path which only traverses informative pages and skips invalid and duplicate ones. The extensive experimental results on several forums show the performance of our system in the following aspects: 1) Effectiveness - Compared to a generic crawler, iRobot significantly decreases the duplicate and invalid pages; 2) Efficiency - With a small cost of pre-sampling a few pages for learning the necessary knowledge, iRobot saves substantial network bandwidth and storage as it only fetches informative pages from a forum site; and 3) Long threads that are divided into multiple pages can be re-concatenated and archived as a whole thread, which is of great help for further indexing and data mining.	iRobot: an intelligent crawler for web forums	NA:NA:NA:NA:NA	2018
Ronny Lempel	NA	Session details: Search: applications	NA	2018
Canhui Wang:Min Zhang:Shaoping Ma:Liyun Ru	In many cases, rather than a keyword search, people intend to see what is going on through the Internet. Then the integrated comprehensive information on news topics is necessary, which we called news issues, including the background, history, current progress, different opinions and discussions, etc. Traditionally, news issues are manually generated by website editors. It is quite a time-consuming hard work, and hence real-time update is difficult to perform. In this paper, a three-step automatic online algorithm for news issue construction is proposed. The first step is a topic detection process, in which newly appearing stories are clustered into new topic candidates. The second step is a topic tracking process, where those candidates are compared with previous topics, either merged into old ones or generating a new one. In the final step, news issues are constructed by the combination of related topics and updated by the insertion of new topics. An automatic online news issue construction process under practical Web circumstances is simulated to perform news issue construction experiments. F-measure of the best results is either above (topic detection) or close to (topic detection and tracking) 90%. Four news issue construction results are successfully generated in different time granularities: one meets the needs like "what's new", and the other three will answer questions like "what's hot" or "what's going on". Through the proposed algorithm, news issues can be effectively and automatically constructed with real-time update, and lots of human efforts will be released from tedious manual work.	Automatic online news issue construction in web environment	NA:NA:NA:NA	2018
Jiang Bian:Yandong Liu:Eugene Agichtein:Hongyuan Zha	Community Question Answering has emerged as a popular and effective paradigm for a wide range of information needs. For example, to find out an obscure piece of trivia, it is now possible and even very effective to post a question on a popular community QA site such as Yahoo! Answers, and to rely on other users to provide answers, often within minutes. The importance of such community QA sites is magnified as they create archives of millions of questions and hundreds of millions of answers, many of which are invaluable for the information needs of other searchers. However, to make this immense body of knowledge accessible, effective answer retrieval is required. In particular, as any user can contribute an answer to a question, the majority of the content reflects personal, often unsubstantiated opinions. A ranking that combines both relevance and quality is required to make such archives usable for factual information retrieval. This task is challenging, as the structure and the contents of community QA archives differ significantly from the web setting. To address this problem we present a general ranking framework for factual information retrieval from social media. Results of a large scale evaluation demonstrate that our method is highly effective at retrieving well-formed, factual answers to questions, as evaluated on a standard factoid QA benchmark. We also show that our learning framework can be tuned with the minimum of manual labeling. Finally, we provide result analysis to gain deeper understanding of which features are significant for social media search and retrieval. Our system can be used as a crucial building block for combining results from a variety of social media content with general web search results, and to better integrate social media content for effective information access.	Finding the right facts in the crowd: factoid question answering over social media	NA:NA:NA:NA	2018
Jonathan Koren:Yi Zhang:Xue Liu	Faceted search is becoming a popular method to allow users to interactively search and navigate complex information spaces. A faceted search system presents users with key-value metadata that is used for query refinement. While popular in e-commerce and digital libraries, not much research has been conducted on which metadata to present to a user in order to improve the search experience. Nor are there repeatable benchmarks for evaluating a faceted search engine. This paper proposes the use of collaborative filtering and personalization to customize the search interface to each user's behavior. This paper also proposes a utility based framework to evaluate the faceted interface. In order to demonstrate these ideas and better understand personalized faceted search, several faceted search algorithms are proposed and evaluated using the novel evaluation methodology.	Personalized interactive faceted search	NA:NA:NA	2018
Collin Jackson	NA	Session details: Security I: misc	NA	2018
Mohammad Mannan:Paul C. van Oorschot	Publishing personal content on the web is gaining increased popularity with dramatic growth in social networking websites, and availability of cheap personal domain names and hosting services. Although the Internet enables easy publishing of any content intended to be generally accessible, restricting personal content to a selected group of contacts is more difficult. Social networking websites partially enable users to restrict access to a selected group of users of the same network by explicitly creating a "friends' list." While this limited restriction supports users' privacy on those (few) selected websites, personal websites must still largely be protected manually by sharing passwords or obscure links. Our focus is the general problem of privacy-enabled web content sharing from any user-chosen web server. By leveraging the existing "circle of trust" in popular Instant Messaging (IM) networks, we propose a scheme called IM-based Privacy-Enhanced Content Sharing (IMPECS) for personal web content sharing. IMPECS enables a publishing user's personal data to be accessible only to her IM contacts. A user can put her personal web page on any web server she wants (vs. being restricted to a specific social networking website), and maintain privacy of her content without requiring site-specific passwords. Our prototype of IMPECS required only minor modifications to an IM server, and PHP scripts on a web server. The general idea behind IMPECS extends beyond IM and IM circles of trust; any equivalent scheme, (ideally) containing pre-arranged groups, could similarly be leveraged.	Privacy-enhanced sharing of personal content on the web	NA:NA	2018
Bhaskar Mehta:Saurabh Nangia:Manish Gupta:Wolfgang Nejdl	Email spam is a much studied topic, but even though current email spam detecting software has been gaining a competitive edge against text based email spam, new advances in spam generation have posed a new challenge: image-based spam. Image based spam is email which includes embedded images containing the spam messages, but in binary format. In this paper, we study the characteristics of image spam to propose two solutions for detecting image-based spam, while drawing a comparison with the existing techniques. The first solution, which uses the visual features for classification, offers an accuracy of about 98%, i.e. an improvement of at least 6% compared to existing solutions. SVMs (Support Vector Machines) are used to train classifiers using judiciously decided color, texture and shape features. The second solution offers a novel approach for near duplication detection in images. It involves clustering of image GMMs (Gaussian Mixture Models) based on the Agglomerative Information Bottleneck (AIB) principle, using Jensen-Shannon divergence (JS) as the distance measure.	Detecting image spam using visual features and near duplicate detection	NA:NA:NA:NA	2018
Dachuan Yu:Ajay Chander:Hiroshi Inamura:Igor Serikov	It is notoriously difficult to program a solid web application. Besides addressing web interactions, state maintenance, and whimsical user navigation behaviors, programmers must also avoid a minefield of security vulnerabilities. The problem is twofold. First, we lack a clear understanding of the new computation model underlying web applications. Second, we lack proper abstractions for hiding common and subtle coding details that are orthogonal to the business functionalities of specific web applications. This paper addresses both issues. First, we present a language BASS for declarative server-side scripting. BASS allows programmers to work in an ideal world, using new abstractions to tackle common but problematic aspects of web programming. The meta properties of BASS provide useful security guarantees. Second, we present a language MOSS reflecting realistic web programming concepts and scenarios, thus articulating the computation model behind web programming. Finally, we present a translation from BASS to MOSS, demonstrating how the ideal programming model and security guarantees of BASS can be implemented in practice.	Better abstractions for secure server-side scripting	NA:NA:NA:NA	2018
Mary Ellen Zurko	NA	Session details: Security II: web client security	NA	2018
Ben Adida	Typical web sessions can be hijacked easily by a network eavesdropper in attacks that have come to be designated "sidejacking." The rise of ubiquitous wireless networks, often unprotected at the transport layer, has significantly aggravated this problem. While SSL can protect against eavesdropping, its usability disadvantages often make it unsuitable when the data is not considered highly confidential. Most web-based email services, for example, use SSL only on their login page and are thus vulnerable to sidejacking. We propose SessionLock, a simple approach to securing web sessions against eavesdropping without extending the use of SSL. SessionLock is easily implemented by web developers using only JavaScript and simple server-side logic. Its performance impact is negligible, and all major web browsers are supported. Interestingly, it is particularly easy to implement on single-page AJAX web applications, e.g. Gmail or Yahoo mail, with approximately 200 lines of JavaScript and 60 lines of server-side verification code.	Sessionlock: securing web sessions against eavesdropping	NA	2018
Collin Jackson:Adam Barth	As wireless networks proliferate, web browsers operate in an increasingly hostile network environment. The HTTPS protocol has the potential to protect web users from network attackers, but real-world deployments must cope with misconfigured servers, causing imperfect web sites and users to compromise browsing sessions inadvertently. ForceHTTPS is a simple browser security mechanism that web sites or users can use to opt in to stricter error processing, improving the security of HTTPS by preventing network attacks that leverage the browser's lax error processing. By augmenting the browser with a database of custom URL rewrite rules, ForceHTTPS allows sophisticated users to transparently retrofit security onto some insecure sites that support HTTPS. We provide a prototype implementation of ForceHTTPS as a Firefox browser extension.	Forcehttps: protecting high-security web sites from network attacks	NA:NA	2018
Frederik De Keukelaere:Sumeer Bhola:Michael Steiner:Suresh Chari:Sachiko Yoshihama	Mashup applications mix and merge content (data and code) from multiple content providers in a user's browser, to provide high-value web applications that can rival the user experience provided by desktop applications. Current browser security models were not designed to support such applications and they are therefore implemented with insecure workarounds. In this paper, we present a secure component model, where components are provided by different trust domains, and can interact using a communication abstraction that allows ease of specification of a security policy. We have developed an implementation of this model that works currently in all major browsers, and addresses challenges of communication integrity and frame-phishing. An evaluation of the performance of our implementation shows that this approach is not just feasible but also practical.	SMash: secure component model for cross-domain mashups on unmodified browsers	NA:NA:NA:NA:NA	2018
Rui Guo:Bin B. Zhu:Min FENG:Aimin PAN:Bosheng ZHOU	In this paper, client-site Web mashups are studied from component-oriented perspective, and CompoWeb, a component-oriented Web architecture, is proposed. In CompoWeb, a Web application is decomposed into Web components called gadgets. A gadget is an abstraction of functional or logical Web component. It is isolated from other gadgets for security and reliability. Contract-based channels are the only way to interact with each other. An abstraction of contract-based channels supported or required by a gadget is also presented. It enables binding of gadgets at deployment, and promotes interchangeable gadgets. Unlike the model of a normal function call where the function logic is executed in caller's context, CompoWeb ensures that the function logic is executed in callee's context so that both the caller and callee are protected. Implementation of a prototype CompoWeb system and its performance are also presented.	Compoweb: a component-oriented web architecture	NA:NA:NA:NA:NA	2018
R V Guha	NA	Session details: Semantic web I	NA	2018
Boris Motik:Bernardo Cuenca Grau:Ulrike Sattler	Applications of semantic technologies often require the representation of and reasoning with structured objects - that is, objects composed of parts connected in complex ways. Although OWL is a general and powerful language, its class descriptions and axioms cannot be used to describe arbitrarily connected structures. An OWL representation of structured objects can thus be underconstrained, which reduces the inferences that can be drawn and causes performance problems in reasoning. To address these problems, we extend OWL with description graphs, which allow for the description of structured objects in a simple and precise way. To represent conditional aspects of the domain, we also allow for SWRL-like rules over description graphs. Based on an observation about the nature of structured objects, we ensure decidability of our formalism. We also present a hypertableau-based decision procedure, which we implemented in the HermiT reasoner. To evaluate its performance, we have extracted description graphs from the GALEN and FMA ontologies, classified them successfully, and even detected a modeling error in GALEN.	Structured objects in owl: representation and reasoning	NA:NA:NA	2018
Jianfeng Du:Yi-Dong Shen	Ontology population is prone to cause inconsistency because the populating process is imprecise or the populated data may conflict with the original data. By assuming that the intensional part of the populated DL-based ontology is fixed and each removable ABox assertion is given a removal cost, we repair the ontology by deleting a subset of removable ABox assertions in which the sum of removal costs is minimum. We call such subset a minimum cost diagnosis. We show that, unless P=NP, the problem of finding a minimum cost diagnosis for a DL-Lite ontology is insolvable in PTIME w.r.t. data complexity. In spite of that, we present a feasible computational method for more general (i.e. SHIQ) ontologies. It transforms a SHIQ ontology to a set of disjoint propositional programs, thus reducing the original problem into a set of independent subproblems. Each such subproblem computes an optimal model and is solvable in logarithmic calls to a SAT solver. Experimental results show that the method can handle moderately complex ontologies with over thousands of ABox assertions, where all ABox assertions can be assumed removable.	Computing minimum cost diagnoses to repair populated DL-based ontologies	NA:NA	2018
Jeff Z. Pan:Giorgos Stamou:Giorgos Stoilos:Stuart Taylor:Edward Thomas	Fuzzy ontologies are envisioned to be useful in the Semantic Web. Existing fuzzy ontology reasoners are not scalable enough to handle the scale of data that the Web provides. In this paper, we propose a framework of fuzzy query languages for fuzzy ontologies, and present query answering algorithms for these query languages over fuzzy DL-Lite ontologies. Moreover, this paper reports on implementation of our approach in the fuzzy DL-Lite query engine in the ONTOSEARCH2 system and preliminary, but encouraging, benchmarking results. To the best of our knowledge, this is the first ever scalable query engine for fuzzy ontologies.	Scalable querying services over fuzzy ontologies	NA:NA:NA:NA:NA	2018
Juanzi Li	NA	Session details: Semantic web II	NA	2018
Simon Schenk:Steffen Staab	Easy reuse and integration of declaratively described information in a distributed setting is one of the main motivations for building the Semantic Web. Despite of this claim, reuse and recombination of RDF data today is mostly done using data replication and procedural code. A simple declarative mechanism for reusing and combining RDF data would help users to generate content for the semantic web. Having such a mechanism, the Semantic Web could better benefit from user generated content, as it is broadly present in the so called Web 2.0, but also from better linkage of existing content. We propose Networked Graphs, which allow users to define RDF graphs both, by extensionally listing content, but also by using views on other graphs. These views can be used to include parts of other graphs, to transform data before including it and to denote rules. The relationships between graphs are described declaratively using SPARQL queries and an extension of the SPARQL semantics. Networked Graphs are easily exchangeable between and interpretable on different computers. Using existing protocols, Networked Graphss can be evaluated in a distributed setting.	Networked graphs: a declarative mechanism for SPARQL rules, SPARQL views and RDF data integration on the web	NA:NA	2018
Markus Stocker:Andy Seaborne:Abraham Bernstein:Christoph Kiefer:Dave Reynolds	In this paper, we formalize the problem of Basic Graph Pattern (BGP) optimization for SPARQL queries and main memory graph implementations of RDF data. We define and analyze the characteristics of heuristics for selectivity-based static BGP optimization. The heuristics range from simple triple pattern variable counting to more sophisticated selectivity estimation techniques. Customized summary statistics for RDF data enable the selectivity estimation of joined triple patterns and the development of efficient heuristics. Using the Lehigh University Benchmark (LUBM), we evaluate the performance of the heuristics for the queries provided by the LUBM and discuss some of them in more details.	SPARQL basic graph pattern optimization using selectivity estimation	NA:NA:NA:NA:NA	2018
Andrea Pugliese:Octavian Udrea:V. S. Subrahmanian	The World Wide Web Consortium's RDF standard primarily consists of (subject, property, object) triples that specify the value that a given subject has for a given property. However, it is frequently the case that even for a fixed subject and property, the value varies with time. As a consequence, efforts have been made to annotate RDF triples with "valid time" intervals. However, to date, no proposals exist for efficient indexing of such temporal RDF databases. It is clearly beneficial to store RDF data in a relational DB - however, standard relational indexes are inadequately equipped to handle RDF's graph structure. In this paper, we propose the tGRIN index structure that builds a specialized index for temporal RDF that is physically stored in an RDBMS. Past efforts to store RDF in relational stores include Jena2 from HP, Sesame from OpenRDF.org, and 3store from the University of Southampton. We show that even when these efforts are augmented with well known temporal indexes like R+ trees, SR-trees, ST-index, and MAP21, the tGRIN index exhibits superior performance. In terms of index build time, tGRIN takes two thirds or less of the time used by any other system, and it uses a comparable amount of memory and less disk space than Jena, Sesame and 3store. More importantly, tGRIN can answer queries three to six times faster for average query graph patterns and five to ten times faster for complex queries than these systems.	Scaling RDF with Time	NA:NA:NA	2018
Riichiro Mizoguchi	NA	Session details: Semantic web III	NA	2018
Angelo Di Iorio:Fabio Vitali:Stefano Zacchiroli	Wiki content templating enables reuse of content structures among wiki pages. In this paper we present a thorough study of this widespread feature, showing how its two state of the art models (functional and creational templating) are sub-optimal. We then propose a third, better, model called lightly constrained (LC) templating and show its implementation in the Moin wiki engine. We also show how LC templating implementations are the appropriate technologies to push forward semantically rich web pages on the lines of (lowercase) semantic web and microformats.	Wiki content templating	NA:NA:NA	2018
Bernhard Schueler:Sergej Sizov:Steffen Staab:Duc Thanh Tran	The Semantic Web is based on accessing and reusing RDF data from many different sources, which one may assign different levels of authority and credibility. Existing Semantic Web query languages, like SPARQL, have targeted the retrieval, combination and reuse of facts, but have so far ignored all aspects of meta knowledge, such as origins, authorship, recency or certainty of data, to name but a few. In this paper, we present an original, generic, formalized and implemented approach for managing many dimensions of meta knowledge, like source, authorship, certainty and others. The approach re-uses existing RDF modeling possibilities in order to represent meta knowledge. Then, it extends SPARQL query processing in such a way that given a SPARQL query for data, one may request meta knowledge without modifying the original query. Thus, our approach achieves highly flexible and automatically coordinated querying for data and meta knowledge, while completely separating the two areas of concern.	Querying for meta knowledge	NA:NA:NA:NA	2018
Fei Wu:Daniel S. Weld	The combined efforts of human volunteers have recently extracted numerous facts from Wikipedia, storing them as machine-harvestable object-attribute-value triples in Wikipedia infoboxes. Machine learning systems, such as Kylin, use these infoboxes as training data, accurately extracting even more semantic knowledge from natural language text. But in order to realize the full power of this information, it must be situated in a cleanly-structured ontology. This paper introduces KOG, an autonomous system for refining Wikipedia's infobox-class ontology towards this end. We cast the problem of ontology refinement as a machine learning problem and solve it using both SVMs and a more powerful joint-inference approach expressed in Markov Logic Networks. We present experiments demonstrating the superiority of the joint-inference approach and evaluating other aspects of our system. Using these techniques, we build a rich ontology, integrating Wikipedia's infobox-class schemata with WordNet. We demonstrate how the resulting ontology may be used to enhance Wikipedia with improved query processing and other features.	Automatically refining the wikipedia infobox ontology	NA:NA	2018
Yutaka Matsuo	NA	Session details: Social networks: analysis of social networks & O	NA	2018
Vicenç Gómez:Andreas Kaltenbrunner:Vicente López	We analyze the social network emerging from the user comment activity on the website Slashdot. The network presents common features of traditional social networks such as a giant component, small average path length and high clustering, but differs from them showing moderate reciprocity and neutral assortativity by degree. Using Kolmogorov-Smirnov statistical tests, we show that the degree distributions are better explained by log-normal instead of power-law distributions. We also study the structure of discussion threads using an intuitive radial tree representation. Threads show strong heterogeneity and self-similarity throughout the different nesting levels of a conversation. We use these results to propose a simple measure to evaluate the degree of controversy provoked by a post.	Statistical analysis of the social network and discussion threads in slashdot	NA:NA:NA	2018
Parag Singla:Matthew Richardson	Characterizing the relationship that exists between a person's social group and his/her personal behavior has been a long standing goal of social network analysts. In this paper, we apply data mining techniques to study this relationship for a population of over 10 million people, by turning to online sources of data. The analysis reveals that people who chat with each other (using instant messaging) are more likely to share interests (their Web searches are the same or topically similar). The more time they spend talking, the stronger this relationship is. People who chat with each other are also more likely to share other personal characteristics, such as their age and location (and, they are likely to be of opposite gender). Similar findings hold for people who do not necessarily talk to each other but do have a friend in common. Our analysis is based on a well-defined mathematical formulation of the problem, and is the largest such study we are aware of.	Yes, there is a correlation: - from social networks to personal behavior on the web	NA:NA	2018
Lada A. Adamic:Jun Zhang:Eytan Bakshy:Mark S. Ackerman	Yahoo Answers (YA) is a large and diverse question-answer forum, acting not only as a medium for sharing technical knowledge, but as a place where one can seek advice, gather opinions, and satisfy one's curiosity about a countless number of things. In this paper, we seek to understand YA's knowledge sharing and activity. We analyze the forum categories and cluster them according to content characteristics and patterns of interaction among the users. While interactions in some categories resemble expertise sharing forums, others incorporate discussion, everyday advice, and support. With such a diversity of categories in which one can participate, we find that some users focus narrowly on specific topics, while others participate across categories. This not only allows us to map related categories, but to characterize the entropy of the users' interests. We find that lower entropy correlates with receiving higher answer ratings, but only for categories where factual expertise is primarily sought after. We combine both user attributes and answer characteristics to predict, within a given category, whether a particular answer will be chosen as the best answer by the asker.	Knowledge sharing and yahoo answers: everyone knows something	NA:NA:NA:NA	2018
Matthew Hurst	NA	Session details: Social networks: discovery & evolution of commun	NA	2018
Xin Li:Lei Guo:Yihong Eric Zhao	The success and popularity of social network systems, such as del.icio.us, Facebook, MySpace, and YouTube, have generated many interesting and challenging problems to the research community. Among others, discovering social interests shared by groups of users is very important because it helps to connect people with common interests and encourages people to contribute and share more contents. The main challenge to solving this problem comes from the difficulty of detecting and representing the interest of the users. The existing approaches are all based on the online connections of users and so unable to identify the common interest of users who have no online connections. In this paper, we propose a novel social interest discovery approach based on user-generated tags. Our approach is motivated by the key observation that in a social network, human users tend to use descriptive tags to annotate the contents that they are interested in. Our analysis on a large amount of real-world traces reveals that in general, user-generated tags are consistent with the web content they are attached to, while more concise and closer to the understanding and judgments of human users about the content. Thus, patterns of frequent co-occurrences of user tags can be used to characterize and capture topics of user interests. We have developed an Internet Social Interest Discovery system, ISID, to discover the common user interests and cluster users and their saved URLs by different interest topics. Our evaluation shows that ISID can effectively cluster similar documents by interest topics and discover user communities with common interests no matter if they have any online connections.	Tag-based social interest discovery	NA:NA:NA	2018
Yu-Ru Lin:Yun Chi:Shenghuo Zhu:Hari Sundaram:Belle L. Tseng	We discover communities from social network data, and analyze the community evolution. These communities are inherent characteristics of human interaction in online social networks, as well as paper citation networks. Also, communities may evolve over time, due to changes to individuals' roles and social status in the network as well as changes to individuals' research interests. We present an innovative algorithm that deviates from the traditional two-step approach to analyze community evolutions. In the traditional approach, communities are first detected for each time slice, and then compared to determine correspondences. We argue that this approach is inappropriate in applications with noisy data. In this paper, we propose FacetNet for analyzing communities and their evolutions through a robust unified process. In this novel framework, communities not only generate evolutions, they also are regularized by the temporal smoothness of evolutions. As a result, this framework will discover communities that jointly maximize the fit to the observed data and the temporal evolution. Our approach relies on formulating the problem in terms of non-negative matrix factorization, where communities and their evolutions are factorized in a unified way. Then we develop an iterative algorithm, with proven low time complexity, which is guaranteed to converge to an optimal solution. We perform extensive experimental studies, on both synthetic datasets and real datasets, to demonstrate that our method discovers meaningful communities and provides additional insights not directly obtainable from traditional methods.	Facetnet: a framework for analyzing communities and their evolutions in dynamic networks	NA:NA:NA:NA:NA	2018
Jure Leskovec:Kevin J. Lang:Anirban Dasgupta:Michael W. Mahoney	A large body of work has been devoted to identifying community structure in networks. A community is often though of as a set of nodes that has more connections between its members than to the remainder of the network. In this paper, we characterize as a function of size the statistical and structural properties of such sets of nodes. We define the network community profile plot, which characterizes the "best" possible community - according to the conductance measure - over a wide range of size scales, and we study over 70 large sparse real-world networks taken from a wide range of application domains. Our results suggest a significantly more refined picture of community structure in large real-world networks than has been appreciated previously. Our most striking finding is that in nearly every network dataset we examined, we observe tight but almost trivial communities at very small scales, and at larger size scales, the best possible communities gradually "blend in" with the rest of the network and thus become less "community-like." This behavior is not explained, even at a qualitative level, by any of the commonly-used network generation models. Moreover, this behavior is exactly the opposite of what one would expect based on experience with and intuition from expander graphs, from graphs that are well-embeddable in a low-dimensional structure, and from small social networks that have served as testbeds of community detection algorithms. We have found, however, that a generative model, in which new edges are added via an iterative "forest fire" burning process, is able to produce graphs exhibiting a network community structure similar to our observations.	Statistical properties of community structure in large social and information networks	NA:NA:NA:NA	2018
Lada Adamic	NA	Session details: Social networks: applications and infrastructures	NA	2018
Carsten Ullrich:Kerstin Borau:Heng Luo:Xiaohong Tan:Liping Shen:Ruimin Shen	The term "Web 2.0" is used to describe applications that distinguish themselves from previous generations of software by a number of principles. Existing work shows that Web 2.0 applications can be successfully exploited for technology-enhance learning. However, in-depth analyses of the relationship between Web 2.0 technology on the one hand and teaching and learning on the other hand are still rare. In this article, we will analyze the technological principles of the Web 2.0 and describe their pedagogical implications on learning. We will furthermore show that Web 2.0 is not only well suited for learning but also for research on learning: the wealth of services that is available and their openness regarding API and data allow to assemble prototypes of technology-supported learning applications in amazingly small amount of time. These prototypes can be used to evaluate research hypotheses quickly. We will present two example prototypes and discuss the lessons we learned from building and using these prototypes.	Why web 2.0 is good for learning and for research: principles and prototypes	NA:NA:NA:NA:NA:NA	2018
Ding Zhou:Jiang Bian:Shuyi Zheng:Hongyuan Zha:C. Lee Giles	Social annotation has gained increasing popularity in many Web-based applications, leading to an emerging research area in text analysis and information retrieval. This paper is concerned with developing probabilistic models and computational algorithms for social annotations. We propose a unified framework to combine the modeling of social annotations with the language modeling-based methods for information retrieval. The proposed approach consists of two steps: (1) discovering topics in the contents and annotations of documents while categorizing the users by domains; and (2) enhancing document and query language models by incorporating user domain interests as well as topical background models. In particular, we propose a new general generative model for social annotations, which is then simplified to a computationally tractable hierarchical Bayesian network. Then we apply smoothing techniques in a risk minimization framework to incorporate the topical information to language models. Experiments are carried out on a real-world annotation data set sampled from del.icio.us. Our results demonstrate significant improvements over the traditional approaches.	Exploring social annotations for information retrieval	NA:NA:NA:NA:NA	2018
Jiangming Yang:Haixun Wang:Ning Gu:Yiming Liu:Chunsong Wang:Qiwei Zhang	Online collaboration and sharing is the central theme of many web-based services that create the so-called Web 2.0 phenomena. Using the Internet as a computing platform, many Web 2.0 applications set up mirror sites to provide large-scale availability and to achieve load balance. However, in the age of Web 2.0, where every user is also a writer and publisher, the deployment of mirror sites makes consistency maintenance a Web scale problem. Traditional concurrency control methods (e.g. two phase lock, serialization, etc.) are not up to the task for several reasons. First, large network latency between mirror sites will make two phase locking a throughput bottleneck. Second, locking will block a large portion of concurrent operations, which makes it impossible to provide large-scale availability. On the other hand, most Web 2.0 operations do not need strict serializability - it is not the intention of a user who is correcting a typo in a shared document to block another who is adding a comment, as long as consistency can still be achieved. Thus, in order to enable maximal online collaboration and sharing, we need a lock-free mechanism that can maintain consistency among mirror sites on the Web. In this paper, we propose a flexible and efficient method to achieve consistency maintenance in the Web 2.0 world. Our experiments show its good performance improvement compared with existing methods based on distributed lock.	Lock-free consistency control for web 2.0 applications	NA:NA:NA:NA:NA:NA	2018
Daniel Schwabe	NA	Session details: Web engineering -- applications	NA	2018
Bingjun Sun:Prasenjit Mitra:C. Lee Giles	Current search engines do not support user searches for chemical entities (chemical names and formulae) beyond simple keyword searches. Usually a chemical molecule can be represented in multiple textual ways. A simple keyword search would retrieve only the exact match and not the others. We show how to build a search engine that enables searches for chemical entities and demonstrate empirically that it improves the relevance of returned documents. Our search engine first extracts chemical entities from text, performs novel indexing suitable for chemical names and formulae, and supports different query models that a scientist may require. We propose a model of hierarchical conditional random fields for chemical formula tagging that considers long-term dependencies at the sentence level. To substring searches of chemical names, a search engine must index substrings of chemical names. Indexing all possible sub-sequences is not feasible in practice. We propose an algorithm for independent frequent subsequence mining to discover sub-terms of chemical names with their probabilities. We then propose an unsupervised hierarchical text segmentation (HTS) method to represent a sequence with a tree structure based on discovered independent frequent subsequences, so that sub-terms on the HTS tree should be indexed. Query models with corresponding ranking functions are introduced for chemical name searches. Experiments show that our approaches to chemical entity tagging perform well. Furthermore, we show that index pruning can reduce the index size and query time without changing the returned ranked results significantly. Finally, experiments show that our approaches out-perform traditional methods for document search with ambiguous chemical terms.	Mining, indexing, and searching for textual chemical molecule information on the web	NA:NA:NA	2018
Davide Bolchini:Franca Garzotto:Paolo Paolini	An infosuasive web application is mainly intended to be at the same time informative and persuasive, i.e., it aims at supporting knowledge needs and it has also the (declared or not declared) goal of influencing user's opinions, attitudes and behaviors. Most web applications, in fact, are infosuasive (except those whose aim is mainly operational). In this paper, we investigate the complex set of elements that informs the very early design of infosuasive web applications. We propose a conceptual framework aimed at supporting the actors involved in this process to integrate their different viewpoints, to organize the variety of issues that need to be analyzed, to find a direction in the numerous design options, and to represent the results of this activity in an effective way. Our approach is value-driven since it is centered around the concept of communication value, regarded as a vehicle to fulfill communication goals on specific communication targets. We place the analysis of these aspects in the wider context of web requirements analysis, highlighting their relationships with business values analysis and user needs analysis. We pinpoint how values and communication goals impact on various design dimensions of infosuasive web application - contents, information architecture, interaction, operations, and lay-out. Our approach is multidisciplinary, and was inspired to goal-based and value-based requirements engineering (often used in web engineering), to brand design (often used in marketing), and to value-centered design "frameworks"(as proposed by the HCI community). A case study exemplifies our methodological proposal, discussing a large project in which we are currently involved.	Value-driven design for "infosuasive" web applications	NA:NA:NA	2018
Roxana Geambasu:Cherie Cheung:Alexander Moshchuk:Steven D. Gribble:Henry M. Levy	The migration from desktop applications to Web-based services is scattering personal data across a myriad of Web sites, such as Google, Flickr, YouTube, and Amazon S3. This dispersal poses new challenges for users, making it more difficult for them to: (1) organize, search, and archive their data, much of which is now hosted by Web sites; (2) create heterogeneous, multi-Web-service object collections and share them in a protected way; and (3) manipulate their data with standard applications or scripts. In this paper, we show that a Web-service interface supporting standardized naming, protection, and object-access services can solve these problems and can greatly simplify the creation of a new generation of object-management services for the Web. We describe the implementation of Menagerie, a proof-of-concept prototype that provides these services for Web-based applications. At a high level, Menagerie creates an integrated file and object system from heterogeneous, personal Web-service objects dispersed across the Internet. We present several object-management applications we developed on Menagerie to show the practicality and benefits of our approach.	Organizing and sharing distributed personal web-service data	NA:NA:NA:NA:NA	2018
Martin Gaedke	NA	Session details: Web engineering -- web service composition	NA	2018
Nalaka Gooneratne:Zahir Tari	Service discovery employs matching techniques to select services by comparing their descriptions against user constraints. Semantic-based matching approaches achieve higher recall than syntactic-based ones (as they employ ontological reasoning mechanisms to match syntactically heterogeneous descriptions). However, semantic-based approaches still have problems (e.g. lack of scalability as an exhaustive search is often performed to located services conforming to constraints). This paper proposes two approaches that deal with the problem of scalability/performance for composite service location. First, services are indexed based on the values they assign to their restricted attributes (the attributes restricted by a given constraint). Then, services that assign "conforming values" to those attributes are combined to form composite services. The first proposed approach extends a local optimisation technique to perform the latter, since identifying such values is NP-hard. However, this approach returns false negatives since the local optimisation technique does not consider all the values. Hence, a second approach that derives conforming values using domain rules is defined. The used rules are returned with each composite service so that a user can understand the context in which it is retrieved. Results obtained from the experiments that varied the number of available services demonstrate that the performance of the local optimisation-based approach is 76% better than existing semantic-based approaches and recall is 98% higher than syntactic-based approaches.	Matching independent global constraints for composite web services	NA:NA	2018
Anton V. Riabov:Eric Boillet:Mark D. Feblowitz:Zhen Liu:Anand Ranganathan	With the emergence of Yahoo Pipes and several similar services, data mashup tools have started to gain interest of business users. Making these tools simple and accessible ton users with no or little programming experience has become a pressing issue. In this paper we introduce MARIO (Mashup Automation with Runtime Orchestration and Invocation), a new tool that radically simplifies data mashup composition. We have developed an intelligent automatic composition engine in MARIO together with a simple user interface using an intuitive "wishful search" abstraction. It thus allows users to explore the space of potentially composable data mashups and preview composition results as they iteratively refine their "wishes", i.e. mashup composition goals. It also lets users discover and make use of system capabilities without having to understand the capabilities of individual components, and instantly reflects changes made to the components by presenting an aggregate view of changed capabilities of the entire system. We describe our experience with using MARIO to compose flows of Yahoo Pipes components.	Wishful search: interactive composition of data mashups	NA:NA:NA:NA:NA	2018
Dieter König:Niels Lohmann:Simon Moser:Christian Stahl:Karsten Wolf	WS-BPEL defines a standard for executable processes. Executable processes are business processes which can be automated through an IT infrastructure. The WS-BPEL specification also introduces the concept of abstract processes: In contrast to their executable siblings, abstract processes are not executable and can have parts where business logic is disguised. Nevertheless, the WS-BPEL specification introduces a notion of compatibility between such an under-specified abstract process and a fully specified executable one. Basically, this compatibility notion defines a set of syntactical rules that can be augmented or restricted by profiles. So far, there exist two of such profiles: the Abstract Process Profile for Observable Behavior and the Abstract Process Profile for Templates. None of these profiles defines a concept of behavioral equivalence. Therefore, both profiles are too strict with respect to the rules they impose when deciding whether an executable process is compatible to an abstract one. In this paper, we propose a novel profile that extends the existing Abstract Process Profile for Observable Behavior by defining a behavioral relationship. We also show that our novel profile allows for more flexibility when deciding whether an executable and an abstract process are compatible.	Extending the compatibility notion for abstract WS-BPEL processes	NA:NA:NA:NA:NA	2018
Geert-Jan Hoube	NA	Session details: Web engineering -- web service deployment	NA	2018
Eyhab Al-Masri:Qusay H. Mahmoud	Searching for Web service access points is no longer attached to service registries as Web search engines have become a new major source for discovering Web services. In this work, we conduct a thorough analytical investigation on the plurality of Web service interfaces that exist on the Web today. Using our Web Service Crawler Engine (WSCE), we collect metadata service information on retrieved interfaces through accessible UBRs, service portals and search engines. We use this data to determine Web service statistics and distribution based on object sizes, types of technologies employed, and the number of functioning services. This statistical data can be used to help determine the current status of Web services. We determine an intriguing result that 63% of the available Web services on the Web are considered to be active. We further use our findings to provide insights on improving the service retrieval process.	Investigating web services on the world wide web	NA:NA	2018
Cesare Pautasso:Olaf Zimmermann:Frank Leymann	Recent technology trends in the Web Services (WS) domain indicate that a solution eliminating the presumed complexity of the WS-* standards may be in sight: advocates of REpresentational State Transfer (REST) have come to believe that their ideas explaining why the World Wide Web works are just as applicable to solve enterprise application integration problems and to simplify the plumbing required to build service-oriented architectures. In this paper we objectify the WS-* vs. REST debate by giving a quantitative technical comparison based on architectural principles and decisions. We show that the two approaches differ in the number of architectural decisions that must be made and in the number of available alternatives. This discrepancy between freedom-from-choice and freedom-of-choice explains the complexity difference perceived. However, we also show that there are significant differences in the consequences of certain decisions in terms of resulting development and maintenance costs. Our comparison helps technical decision makers to assess the two integration styles and technologies more objectively and select the one that best fits their needs: REST is well suited for basic, ad hoc integration scenarios, WS-* is more flexible and addresses advanced quality of service requirements commonly occurring in enterprise computing.	Restful web services vs. "big"' web services: making the right architectural decision	NA:NA:NA	2018
Oliver Moser:Florian Rosenberg:Schahram Dustdar	Web service processes currently lack monitoring and dynamic (runtime) adaptation mechanisms. In highly dynamic processes, services frequently need to be exchanged due to a variety of reasons. In this paper we present VieDAME, a system which allows monitoring of BPEL processes according to Quality of Service (QoS) attributes and replacement of existing partner services based on various (pluggable) replacement strategies. The chosen replacement services can be syntactically or semantically equivalent to the BPEL interface. Services can be automatically replaced during runtime without any downtime of the overall system. We implemented our solution with an aspect-oriented approach by intercepting SOAP messages and allow services to be exchanged during runtime with little performance penalty costs, as shown in our experiments, thereby making our approach suitable for high-availability BPEL environments.	Non-intrusive monitoring and service adaptation for WS-BPEL	NA:NA:NA	2018
Philip Bohannon	NA	Session details: XML I	NA	2018
Geert Jan Bex:Wouter Gelade:Frank Neven:Stijn Vansummeren	Inferring an appropriate DTD or XML Schema Definition (XSD) for a given collection of XML documents essentially reduces to learning deterministic regular expressions from sets of positive example words. Unfortunately, there is no algorithm capable of learning the complete class of deterministic regular expressions from positive examples only, as we will show. The regular expressions occurring in practical DTDs and XSDs, however, are such that every alphabet symbol occurs only a small number of times. As such, in practice it suffices to learn the subclass of regular expressions in which each alphabet symbol occurs at most k times, for some small k. We refer to such expressions as k-occurrence regular expressions (k-OREs for short). Motivated by this observation, we provide a probabilistic algorithm that learns k-OREs for increasing values of k, and selects the one that best describes the sample based on a Minimum Description Length argument. The effectiveness of the method is empirically validated both on real world and synthetic data. Furthermore, the method is shown to be conservative over the simpler classes of expressions considered in previous work.	Learning deterministic regular expressions for the inference of schemas from XML data	NA:NA:NA:NA	2018
Xiaoying Wu:Stefanos Souldatos:Dimitri Theodoratos:Theodore Dalamagas:Timos Sellis	Finding the occurrences of structural patterns in XML data is a key operation in XML query processing. Existing algorithms for this operation focus almost exclusively on path-patterns or tree-patterns. Requirements in flexible querying of XML data have motivated recently the introduction of query languages that allow a partial specification of path-patterns in a query. In this paper, we focus on the efficient evaluation of partial path queries, a generalization of path pattern queries. Our approach explicitly deals with repeated labels (that is, multiple occurrences of the same label in a query). We show that partial path queries can be represented as rooted dags for which a topological ordering of the nodes exists. We present three algorithms for the efficient evaluation of these queries under the indexed streaming evaluation model. The first one exploits a structural summary of data to generate a set of path-patterns that together are equivalent to a partial path query. To evaluate these path-patterns, we extend PathStack so that it can work on path-patterns with repeated labels. The second one extracts a spanning tree from the query dag, uses a stack-based algorithm to find the matches of the root-to-leaf paths in the tree, and merge-joins the matches to compute the answer. Finally, the third one exploits multiple pointers of stack entries and a topological ordering of the query dag to apply a stack-based holistic technique. An analysis of the algorithms and extensive experimental evaluation shows that the holistic algorithm outperforms the other ones.	Efficient evaluation of generalized path pattern queries on XML data	NA:NA:NA:NA:NA	2018
Ramadhana Bramandia:Byron Choi:Wee Keong Ng	Recent interests on XML, Semantic Web, and Web ontology, among other topics, have sparked a renewed interest on graph-structured databases. A fundamental query on graphs is the reachability test of nodes. Recently, 2-hop labeling has been proposed to index large collections of XML and/or graphs for efficient reachability tests. However, there has been few work on updates of 2-hop labeling. This is compounded by the fact that Web data changes over time. In response to these, this paper studies the incremental maintenance of 2-hop labeling. We identify the main reason for the inefficiency of updates of existing 2-hop labels. We propose two updatable 2-hop labelings, hybrids of 2-hop labeling, and their incremental maintenance algorithms. The proposed 2-hop labeling is derived from graph connectivities, as opposed to set cover which is used by all previous work. Our experimental evaluation illustrates the space efficiency and update performance of various kinds of 2-hop labeling. The main conclusion is that there is a natural way to spare some index size for update performance in 2-hop labeling.	On incremental maintenance of 2-hop labeling of graphs	NA:NA:NA	2018
Geert-Jan Bex	NA	Session details: XML II	NA	2018
Mingzhu Wei:Elke A. Rundensteiner:Murali Mani	Because of the high volume and unpredictable arrival rate, stream processing systems may not always be able to keep up with the input data streams - resulting in buffer overflow and uncontrolled loss of data. Load shedding, the prevalent strategy for solving this overflow problem, has so far only been considered for relational stream processing, but not for XML. Shedding applied to XML stream processing brings new opportunities and challenges due to complex nested nature of XML structures. In this paper, we tackle this unsolved XML shedding problem using a three-pronged approach. First, we develop an XQuery preference model that enables users to specify the relative importance of preserving different subpatterns in the XML result structure. This transforms shedding into the problem of rewriting the user query into shed queries that return approximate query answers with utility as measured by the given user preference model. Second, we develop a cost model to compare the performance of alternate shed queries. Third, we develop two shedding algorithms, OptShed and FastShed. OptShed guarantees to find an optimal solution however at the cost of exponential complexity. FastShed, as confirmed by our experiments, achieves a close-to-optimal result in a wide range of test cases. Finally we describe the in-automaton shedding mechanism for XQuery stream engines. The experiments show that our proposed utility-driven shedding solutions consistently achieve higher utility results compared to the existing relational shedding techniques.	Utility-driven load shedding for xml stream processing	NA:NA:NA	2018
Iris Miliaraki:Zoi Kaoudi:Manolis Koubarakis	We present a novel approach for filtering XML documents using nondeterministic finite automata and distributed hash tables. Our approach differs architecturally from recent proposals that deal with distributed XML filtering; they assume an XML broker architecture, whereas our solution is built on top of distributed hash tables. The essence of our work is a distributed implementation of YFilter, a state-of-the-art automata-based XML filtering system on top of Chord. We experimentally evaluate our approach and demonstrate that our algorithms can scale to millions of XPath queries under various filtering scenarios, and also exhibit very good load balancing properties.	Xml data dissemination using automata on top of structured overlay networks	NA:NA:NA	2018
Sumit Mittal:Dipanjan Chakraborty:Sunil Goyal:Sougata Mukherjea	With Telecom market reaching saturation in many geographies and revenues from voice calls decreasing, Telecom operators are trying to identify new sources of revenue. For this purpose, these operators can take advantage of their core functionalities like Location, Call Control, etc. by exposing them as services to be composed by developers with third party offerings available over the Web. To hide the complexity of underlying Telecom protocols from application developers, the operators are steadily adopting Service Oriented Architecture (SOA) and reference standards like Parlay-X and IMS. However, a number of challenges still remain in rapid utilization of Telecom functionalities for creating new applications - existence of multiple protocols, different classes of developers, and the need to coordinate and manage usage of these functionalities. In this paper, we present SewNet, a framework for creating applications exploiting Telecom functionality exposed over a (converged) IP network. More specifically, SewNet a) provides an abstraction model for encapsulating invocation, coordination and enrichment of the Telecom functionalities, b) renders a service creation environment on top of this model, and c) caters to various different categories of developers. With the help of two use-case scenarios, we demonstrate how SewNet can create services utilizing rich Telecom functionality.	Sewnet -: a framework for creating services utilizing telecom functionality	NA:NA:NA:NA	2018
Jayavel Shanmugasundaram	NA	Session details: Industrial track session	NA	2018
Narayanan Sadagopan:Jie Li	Millions of users retrieve information from the Internet using search engines. Mining these user sessions can provide valuable information about the quality of user experience and the perceived quality of search results. Often search engines rely on accurate estimates of Click Through Rate (CTR) to evaluate the quality of user experience. The vast heterogeneity in the user population and presence of automated software programs (bots) can result in high variance in the estimates of CTR. To improve the estimation accuracy of user experience metrics like CTR, we argue that it is important to identify typical and atypical user sessions in clickstreams. Our approach to identify these sessions is based on detecting outliers using Mahalanobis distance in the user session space. Our user session model incorporates several key clickstream characteristics including a novel conformance score obtained by Markov Chain analysis. Editorial results show that our approach of identifying typical and atypical sessions has a precision of about 89%. Filtering out these atypical sessions reduces the uncertainty (95% confidence interval) of the mean CTR by about 40%. These results demonstrate that our approach of identifying typical and atypical user sessions is extremely valuable for cleaning "noisy" user session data for increased accuracy in evaluating user experience.	Characterizing typical and atypical user sessions in clickstreams	NA:NA	2018
Shumeet Baluja:Rohan Seth:D. Sivakumar:Yushi Jing:Jay Yagnik:Shankar Kumar:Deepak Ravichandran:Mohamed Aly	The rapid growth of the number of videos in YouTube provides enormous potential for users to find content of interest to them. Unfortunately, given the difficulty of searching videos, the size of the video repository also makes the discovery of new content a daunting task. In this paper, we present a novel method based upon the analysis of the entire user-video graph to provide personalized video suggestions for users. The resulting algorithm, termed Adsorption, provides a simple method to efficiently propagate preference information through a variety of graphs. We extensively test the results of the recommendations on a three month snapshot of live data from YouTube.	Video suggestion and discovery for youtube: taking random walks through the view graph	NA:NA:NA:NA:NA:NA:NA:NA	2018
Yanqing Cui:Virpi Roto	This paper describes a series of user studies on how people use the Web via mobile devices. The data primarily comes from contextual inquiries with 47 participants between 2004 and 2007, and is complemented with a phone log analysis of 577 panelists in 2007. We report four key contextual factors in using the Web on mobile devices and propose mobile Web activity taxonomy. The framework contains three user activity categories identical to previous stationary Web studies: information seeking, communication, and transaction, and a new category: personal space extension. The new category refers to the practice that people put their content on the Web for personal access, therefore extending their personal information space.	How people use the web on mobile devices	NA:NA	2018
Jure Leskovec:Eric Horvitz	We present a study of anonymized data capturing a month of high-level communication activities within the whole of the Microsoft Messenger instant-messaging system. We examine characteristics and patterns that emerge from the collective dynamics of large numbers of people, rather than the actions and characteristics of individuals. The dataset contains summary properties of 30 billion conversations among 240 million people. From the data, we construct a communication graph with 180 million nodes and 1.3 billion undirected edges, creating the largest social network constructed and analyzed to date. We report on multiple aspects of the dataset and synthesized graph. We find that the graph is well-connected and robust to node removal. We investigate on a planetary-scale the oft-cited report that people are separated by "six degrees of separation" and find that the average path length among Messenger users is 6.6. We find that people tend to communicate more with each other when they have similar age, language, and location, and that cross-gender conversations are both more frequent and of longer duration than conversations with the same gender.	Planetary-scale views on a large instant-messaging network	NA:NA	2018
Wenyan Hu:Alvaro Bolivar	Online auctions have become a pervasive transaction mechanism for e-commerce. As the largest online marketplace in the world, eBay is an attractive case study that enables the study of online auctions utilizing data involving real people and transactions. In this paper, we present a detailed investigation and analysis of multiple online auction properties including: consumer surplus, sniping, bidding strategy and their cross-relationships. Our goal is to evaluate the theoretical foundations of online auctions and discover patterns and behaviors hidden due to the lack of real and extensive transaction data. Among our findings, we uncover an important correlation among sniping and high surplus ratios, which implies the uncertainty of true value in a competitive environment. The key issue is the wrong assumption that bidders' valuations are independent from each other, which leads to inefficient auctions. In order to address the inefficiencies of current online formats we introduce a declining price auction model customized for online transactions. Conceptually, this model ought to deal with the complexities of competition in an online environment while maximizing social welfare.	Online auctions efficiency: a survey of ebay auctions	NA:NA	2018
Lakshminarayanan Subramanian	NA	Session details: Technology for developing regions	NA	2018
Arun Kumar:Nitendra Rajput:Sheetal Agarwal:Dipanjan Chakraborty:Amit Anit Nanavati	Various sectors in developing countries are typically dominated by the presence of a large number of small and micro-businesses that operate in an informal, unorganized manner. Many of these are single person run micro-businesses and cannot afford to buy and maintain their own IT infrastructure. For others, easy availability of cheap labour provides a convenient alternative even though it results in inefficiency, as little or no records are maintained, and only manual, paper-based processes are followed. This results in high response times for customers, no formal accountability and higher charges. For the businesses this translates to lower earnings and losses due to inefficiencies. In this paper, we look at few such micro-business segments and explore their current models of operation, while identifying existing inefficiencies and pain points. We build upon the findings and propose an approach for delivering benefits of IT solutions to such micro-business segments. Finally, we present technology that realizes the proposed approach in the specific context of two such segments.	Organizing the unorganized - employing IT to empower the under-privileged	NA:NA:NA:NA:NA	2018
Bowei Du:Eric A. Brewer	Wikis have proven to be a valuable tool for collaboration and content generation on the web. Simple semantics and ease-of-use make wiki systems well suited for meeting many emerging region needs in the areas of education, collaboration and local content generation. Despite their usefulness, current wiki software does not work well in the network environments found in emerging regions. For example, it is common to have long-lasting network partitions due to cost, power and poor connectivity. Network partitions make a traditional centralized wiki architecture unusable due to the unavailability of the central server. Existing solutions towards addressing connectivity problems include web-caching proxies and snapshot distribution. While proxies and snapshots allow wiki data to be read while disconnected, they prevent users from contributing updates back to the wiki. In this paper we detail the design and implementation of DTWiki, a wiki system which explicitly addresses the problem of operating a wiki system in an intermittent environment. The DTWiki system is able to cope with long-lasting partitions and bad connectivity while providing the functionality of popular wiki software such as MediaWiki and TWiki.	Dtwiki: a disconnection and intermittency tolerant wiki	NA:NA	2018
Yao-Jen Chang:Hsin-Yu Hsu:Tsen-Yung Wang	The study follows action science approach to the problem of nonprofit housing services. After 4 months of action science-based activities, such as organized participant observations, in-depth interviews, field work, and focus group studies, the main findings are (1) Web 2.0 suits nonprofit organizations better than traditional webs in terms of maintenance cost and usability, (2) mapping tools make better GUI with respect to web-based housing services, and (3) context-aware personalization can translate to better user experiences as an RDFS-based working prototype has been built and tested. A user survey shows high level user satisfaction. Although the case study was carried in a nonprofit housing organization, the practices in action research approach can be applied to other NPOs as well.	Action science approach to nonprofit housing services using web 2.0 mapping tools	NA:NA:NA	2018
Zaiqing Nie	NA	Session details: WWW in China: mining the chinese web	NA	2018
Qi Su:Xinying Xu:Honglei Guo:Zhili Guo:Xian Wu:Xiaoxun Zhang:Bin Swen:Zhong Su	The boom of product review websites, blogs and forums on the web has attracted many research efforts on opinion mining. Recently, there was a growing interest in the finer-grained opinion mining, which detects opinions on different review features as opposed to the whole review level. The researches on feature-level opinion mining mainly rely on identifying the explicit relatedness between product feature words and opinion words in reviews. However, the sentiment relatedness between the two objects is usually complicated. For many cases, product feature words are implied by the opinion words in reviews. The detection of such hidden sentiment association is still a big challenge in opinion mining. Especially, it is an even harder task of feature-level opinion mining on Chinese reviews due to the nature of Chinese language. In this paper, we propose a novel mutual reinforcement approach to deal with the feature-level opinion mining problem. More specially, 1) the approach clusters product features and opinion words simultaneously and iteratively by fusing both their content information and sentiment link information. 2) under the same framework, based on the product feature categories and opinion word groups, we construct the sentiment association set between the two groups of data objects by identifying their strongest n sentiment links. Moreover, knowledge from multi-source is incorporated to enhance clustering in the procedure. Based on the pre-constructed association set, our approach can largely predict opinions relating to different product features, even for the case without the explicit appearance of product feature words in reviews. Thus it provides a more accurate opinion evaluation. The experimental results demonstrate that our method outperforms the state-of-art algorithms.	Hidden sentiment association in chinese web opinion mining	NA:NA:NA:NA:NA:NA:NA:NA	2018
Xiao Ling:Gui-Rong Xue:Wenyuan Dai:Yun Jiang:Qiang Yang:Yong Yu	As the World Wide Web in China grows rapidly, mining knowledge in Chinese Web pages becomes more and more important. Mining Web information usually relies on the machine learning techniques which require a large amount of labeled data to train credible models. Although the number of Chinese Web pages increases quite fast, it still lacks Chinese labeled data. However, there are relatively sufficient English labeled Web pages. These labeled data, though in different linguistic representations, share a substantial amount of semantic information with Chinese ones, and can be utilized to help classify Chinese Web pages. In this paper, we propose an information bottleneck based approach to address this cross-language classification problem. Our algorithm first translates all the Chinese Web pages to English. Then, all the Web pages, including Chinese and English ones, are encoded through an information bottleneck which can allow only limited information to pass. Therefore, in order to retain as much useful information as possible, the common part between Chinese and English Web pages is inclined to be encoded to the same code (i.e. class label), which makes the cross-language classification accurate. We evaluated our approach using the Web pages collected from Open Directory Project (ODP). The experimental results show that our method significantly improves several existing supervised and semi-supervised classifiers.	Can chinese web pages be classified with english data source?	NA:NA:NA:NA:NA:NA	2018
Liping Wang:Qing Li:Na Li:Guozhu Dong:Yu Yang	Improving the precision of information retrieval has been a challenging issue on Chinese Web. As exemplified by Chinese recipes on the Web, it is not easy/natural for people to use keywords (e.g. recipe names) to search recipes, since the names can be literally so abstract that they do not bear much, if any, information on the underlying ingredients or cooking methods. In this paper, we investigate the underlying features of Chinese recipes, and based on workflow-like cooking procedures, we model recipes as graphs. We further propose a novel similarity measurement based on the frequent patterns, and devise an effective filtering algorithm to prune unrelated data so as to support efficient on-line searching. Benefiting from the characteristics of graphs, frequent common patterns can be mined from a cooking graph database. So in our prototype system called RecipeView, we extend the subgraph mining algorithm FSG to cooking graphs and combine it with our proposed similarity measurement, resulting in an approach that well caters for specific users' needs. Our initial experimental studies show that the filtering algorithm can efficiently prune unrelated cooking graphs without affecting the retrieval performance and the similarity measurement gets a relatively higher precision/recall against its counterparts	Substructure similarity measurement in chinese recipes	NA:NA:NA:NA:NA	2018
Qing Li	NA	Session details: WWW in China: chinese web innovations	NA	2018
Hanhua Chen:Hai Jin:Jiliang Wang:Lei Chen:Yunhao Liu:Lionel M. Ni	Current search mechanisms of DHT-based P2P systems can well handle a single keyword search problem. Other than single keyword search, multi-keyword search is quite popular and useful in many real applications. Simply using the solution for single keyword search will require distributed intersection/union operations in wide area networks, leading to unacceptable traffic cost. As it is well known that Bloom Filter (BF) is effective in reducing traffic, we would like to use BF encoding to handle multi-keyword search. Applying BF is not difficult, but how to get optimal results is not trivial. In this study we show, through mathematical proof, that the optimal setting of BF in terms of traffic cost is determined by the global statistical information of keywords, not the minimized false positive rate as claimed by previous methods. Through extensive experiments, we demonstrate how to obtain optimal settings. We further argue that the intersection order between sets is important for multi-keyword search. Thus, we design optimal order strategies based on BF for both "and" and "or" queries. To better evaluate the performance of this design, we conduct extensive simulations on TREC WT10G test collection and the query log of a commercial search engine. Results show that our design significantly reduces the search traffic of existing approach by 73%.	Efficient multi-keyword search over p2p web	NA:NA:NA:NA:NA:NA	2018
Conglei Yao:Yongjian Yu:Sicong Shou:Xiaoming Li	Popular entities often have thousands of instances on the Web. In this paper, we focus on the case where they are presented in table-like format, namely appearing with their attribute names. It is observed that, on one hand, for the same entity, different web pages often incorporate different attributes; on the other, for the same attribute, different web pages often use different attribute names (labels). Therefore, it is imaginably difficult to produce a global attribute schema for all the web entities of a given entity type based on their web instances, although the global attribute schema is usually highly desired in web entity instances integration and web object extraction. To this end, we propose a novel framework of automatically learning a global attribute schema for all web entities of one specific entity type. Under this framework, an iterative instances extraction procedure is first employed to extract sufficient web entity instances to discover enough attribute labels. Next, based on the labels, entity instances, and related web pages, a maximum entropy-based schema discovery approach is adopted to learn the global attribute schema for the target entity type. Experimental results on the Chinese Web achieve weighted average Fscores of 0.7122 and 0.7123 on two global attribute schemas for person-type and movie-type web entities, respectively. These results show that our framework is general, efficient and effective.	Towards a global schema for web entities	NA:NA:NA:NA	2018
Lu Liu:Lifeng Sun:Yong Rui:Yao Shi:Shiqiang Yang	Automatic topic discovery and tracking on web-shared videos can greatly benefit both web service providers and end users. Most of current solutions of topic detection and tracking were done on news and cannot be directly applied on web videos, because the semantic information of web videos is much less than that of news videos. In this paper, we propose a bipartite graph model to address this issue. The bipartite graph represents the correlation between web videos and their keywords, and automatic topic discovery is achieved through two steps - coarse topic filtering and fine topic re-ranking. First, a weight-updating co-clustering algorithm is employed to filter out topic candidates at a coarse level. Then the videos on each topic are re-ranked by analyzing the link structures of the corresponding bipartite graph. After the topics are discovered, the interesting ones can also be tracked over a period of time using the same bipartite graph model. The key is to propagate the relevant scores and keywords from the videos of interests to other relevant ones through the bipartite graph links. Experimental results on real web videos from YouKu, a YouTube counterpart in China, demonstrate the effectiveness of the proposed methods. We report very promising results.	Web video topic discovery and tracking via bipartite graph reinforcement model	NA:NA:NA:NA:NA	2018
ByungIn Yoo:JongHo Lea:YeunBae Kim	In this paper, we present a new web browsing system, Seamless Browser, for fast link traversal on a large screen like TV In navigating web, users mainly suffer from cognitive overhead of determining whether or not to follow links. This overhead can be reduced by providing preview information of the destination of links, and also by providing semantic cues on the nearest location in relation to the anchor. In order to reduce disorientation and annoyance from the preview information, we propose that users will focus on the small area nearside around a pointer, and a small number of hyperlink previews in that focused area will appear differently depending on the distances between the pointer and the hyperlinks: the nearer the distance is, the richer the content of the information scent is. We also propose that users can navigate the link paths by controlling the pointer and the zooming interface, so that users may go backward and forward seamlessly along several possible link paths. We found that combining the pointer and a zoom significantly improved performance for navigational tasks.	The seamless browser: enhancing the speed of web browsing by zooming and preview thumbnails	NA:NA:NA	2018
Jiahui Liu:Larry Birnbaum	The web has become an important medium for news delivery and consumption. Fresh content about a variety of topics and events is constantly being created and published on the web by many sources. As intuitively understood by readers, and studied in journalism, news articles produced by different social groups present different attitudes towards and interpretations of the same news issues. In this paper, we propose a new paradigm for aggregating news articles according to the news sources related to the stakeholders of the news issues. We implement this paradigm in a prototype system called LocalSavvy. The system provides users the capability to aggregate and browse various local views about the news issues in which they are interested.	What do they think?: aggregating local views about news events and topics	NA:NA	2018
Michal Tvarozek:Maria Bielikova	Effective access to and navigation in information stored in deep Web ontological repositories or relational databases has yet to be realized due to issues with usability of user interfaces and the overall scope and complexity of information as well as the nature of exploratory user tasks. We propose the integration and adaptation of novel navigation and visualization approaches to faceted browsing such as visual depiction of facets and restrictions, visual navigation in (clusters of) search results and graph like exploration of individual search results' properties.	Personalized view-based search and visualization as a means for deep/semantic web data access	NA:NA	2018
Xiao Wu:Jintao Li:Yongdong Zhang:Sheng Tang:Shi-Yong Neo	In this paper, we highlight the use of multimedia technology in generating intrinsic summaries of tourism related information. The system utilizes an automated process to gather, filter and classify information on various tourist spots on the Web. The end result present to the user is a personalized multimedia summary generated with respect to users queries filled with text, image, video and real-time news made retrievable for mobile devices. Preliminary experiments demonstrate the superiority of our presentation scheme to traditional methods.	Personalized multimedia web summarizer for tourist	NA:NA:NA:NA:NA	2018
Maayan Zhitomirsky-Geffet:Judit Bar-Ilan:Yitzchak Miller:Snunith Shoham	The research objective of this work is to develop a general framework that incorporates collaborative social tagging with a novel ontology scheme conveying multiple perspectives. We propose a framework where multiple users tag the same object (an image in our case), and an ontology is extended based on these tags while being tolerant about different points of view. We are not aware of any other work that attempted to devise such an environment and to study its dynamics. The proposed framework characterizes the underlying processes for controlled collaborative development of a multi-perspective ontology and its application to improve image annotation, searching and browsing. Our case study experiment with a set of selected annotated images indicates the soundness of the proposed ontological model.	A generic framework for collaborative multi-perspective ontology acquisition	NA:NA:NA:NA	2018
Pavel Dmitriev	This paper considers the problem of identifying on the Web compound documents (cDocs) -- groups of web pages that in aggregate constitute semantically coherent information entities. Examples of cDocs are a news article consisting of several html pages, or a set of pages describing specifications, price, and reviews of a digital camera. Being able to identify cDocs would be useful in many applications including web and intranet search, user navigation, automated collection generation, and information extraction. In the past, several heuristic approaches have been proposed to identify cDocs [1][5]. However, heuristics fail to capture the variety of types, styles and goals of information on the web, and do not account for the fact that the definition of a cDoc often depends on the context. This paper presents an experimental evaluation of three machine learning-based algorithms for cDoc discovery. These algorithms are responsive to the varying structure of cDocs and adaptive to their application-specific nature. Based on our previous work [4], this paper proposes a different scenario for discovering cDocs, and compares in this new setting the local machine learned clustering algorithm from [4] to a global purely graph based approach [3] and a Conditional Markov Network approach previously applied to noun coreference task [6]. The results show that the approach of [4] outperforms the other algorithms, suggesting that global relational characteristics of web sites are too noisy for cDoc identification purposes.	As we may perceive: finding the boundaries of compound documents on the web	NA	2018
Max Braun:Klaas Dellschaft:Thomas Franz:Dominik Hering:Peter Jungen:Hagen Metzler:Eugen Müller:Alexander Rostilov:Carsten Saathoff	NA	Personalized search and exploration with mytag	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
David Bodoff:Sheizaf Rafaeli	All information exchange on the Internet -- whether through full text, controlled vocabularies, ontologies, or other mechanisms -- ultimately requires that that an information provider and seeker use the same word or symbol. In this paper, we investigate what happens when both searchers and authors are dynamically choosing terms to match the other side. With each side trying to anticipate the other, does a terminological convention ever emerge, or do searchers and providers continue to miss potential partners through mis-match of terms? We use a game-theoretic setup to frame questions, and learning theory to make predictions about whether and which term will emerge as a convention.	Emergence of terminological conventions as an author-searcher coordination game	NA:NA	2018
Gang Wu:Juanzi Li:Kehong Wang	To manage the increasing amount of RDF data, an RDF repository should provide not only necessary scalability and efficiency, but also sufficient inference capabilities. In this paper, we propose a native RDF repository, System, to pursue a better tradeoff among the above requirements. System takes the hypergraph representation for RDF as the data model for its persistent storage, which effectively avoids the costs of data model transformation when accessing RDF data. In addition, a set of efficient semantic query processing techniques are designed. The results of performance evaluation on the LUBM benchmark show that System has a better combined metric value than the other comparable systems.	System  II: a hypergraph based native rdf repository	NA:NA:NA	2018
Guoliang Li:Jianhua Feng:Jianyong Wang:Feng Lin:Lizhu Zhou	This paper proposes several vectorial operators for processing XML twig queries, which are easy to be performed and inherently efficient for both Ancestor-Descendant (A-D) and Parent-Child (P-C) relationships. We develop optimizations on the vectorial operators to improve the efficiency of answering twig queries in holistic. We propose an algorithm to answer GTP queries based on our vectorial operators.	Efficient vectorial operators for processing xml twig queries	NA:NA:NA:NA:NA	2018
Yiqun Liu:Min Zhang:Shaoping Ma:Liyun Ru	Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam detection techniques are usually designed for specific known types of Web spam and are incapable and inefficient for recently-appeared spam. With user behavior analyses into Web access logs, we propose a spam page detection algorithm based on Bayes learning. Preliminary experiments on Web access data collected by a commercial Web site (containing over 2.74 billion user clicks in 2 months) show the effectiveness of the proposed detection framework and algorithm.	User behavior oriented web spam detection	NA:NA:NA:NA	2018
Souvik Debnath:Niloy Ganguly:Pabitra Mitra	We propose a hybridization of collaborative filtering and content based recommendation system. Attributes used for content based recommendations are assigned weights depending on their importance to users. The weight values are estimated from a set of linear regression equations obtained from a social network graph which captures human judgment about similarity of items.	Feature weighting in content based recommendation system using social network analysis	NA:NA:NA	2018
Kazunari Ishida	This paper reports the estimated number of spam blogs in order to assess their current state in the blogosphere. To extract spam blogs, I developed a traversal method among co-citation clusters of blogs from a spam seed. Spam seeds were collected in terms of high out-degree and spam keyword. According to the experiment, a mixed seed set composed of high out-degree and spam keyword seeds is more effective than individual seed sets in terms of F-Measure. In conclusion, mixed seeds from different methods are effective in improving the F-Measure results of spam extraction with co-citation clusters.	Extracting spam blogs with co-citation clusters	NA	2018
Guoliang Li:Jianhua Feng:Jianyong Wang:Bei Yu:Yukai He	In this paper, we study the problem of keyword proximity search over XML documents and leverage the efficiency and effectiveness. We take the disjunctive semantics among input keywords into consideration and identify meaningful compact connected trees as the answers of keyword proximity queries. We introduce the notions of Compact Lowest Common Ancestor (CLCA) and Maximal CLCA (MCLCA) and propose Compact Connected Trees (CCTrees) and Maximal CCTrees (MCCTrees) to efficiently and effectively answer keyword queries. We propose a novel ranking mechanism, RACE, to Rank compAct Connected trEes, by taking into consideration both the structural similarity and the textual similarity. Our extensive experimental study shows that our method achieves both high search efficiency and effectiveness, and outperforms existing approaches significantly.	Race: finding and ranking compact connected trees for keyword proximity search over xml documents	NA:NA:NA:NA:NA	2018
Hongyu Zhang	Semantic web ontology languages, such as OWL, have been widely used for knowledge representation. Through empirical analysis of real-world ontologies we discover that, like many natural and social phenomenon, the semantic web ontology is also "scale-free".	The scale-free nature of semantic web ontology	NA	2018
Zhiyuan Liu:Maosong Sun	This paper presents a new query recommendation method that generates recommended query list by mining large-scale user logs. Starting from the user logs of click-through data, we construct a bipartite network where the nodes on one side correspond to unique queries, on the other side to unique URLs. Inspired by the bipartite network based resource allocation method, we try to extract the hidden information from the Query-URL bipartite network. The recommended queries generated by the method are asymmetrical which means two related queries may have different strength to recommend each other. To evaluate the method, we use one week user logs from Chinese search engine Sogou. The method is not only `content ignorant', but also can be easily implemented in a paralleled manner, which is feasible for commercial search engines to handle large scale user logs.	Asymmetrical query recommendation method based on bipartite network resource allocation	NA:NA	2018
Chuancong Gao:Jianyong Wang:Yukai He:Lizhu Zhou	Sequential pattern mining has raised great interest in data mining research field in recent years. However, to our best knowledge, no existing work studies the problem of frequent sequence generator mining. In this paper we present a novel algorithm, FEAT (abbr. Frequent sEquence generATor miner), to perform this task. Experimental results show that FEAT is more efficient than traditional sequential pattern mining algorithms but generates more concise result set, and is very effective for classifying Web product reviews.	Efficient mining of frequent sequence generators	NA:NA:NA:NA	2018
Ying Yan:Chen Wang:Aoying Zhou:Weining Qian:Li Ma:Yue Pan	Efficiently querying RDF data is being an important factor in applying Semantic Web technologies to real-world applications. In this context, many efforts have been made to store and query RDF data in relational database using particular schemas. In this paper, we propose a new scheme to store, index, and query RDF data in triple stores. Graph feature of RDF data is taken into considerations which might help reduce the join costs on the vertical database structure. We would partition RDF triples into overlapped groups, store them in a triple table with one more column of group identity, and build up a signature tree to index them. Based on this infrastructure, a complex RDF query is decomposed into multiple pieces of sub-queries which could be easily filtered into some RDF groups using signature tree index, and finally is evaluated with a composed and optimized SQL with specific constraints. We compare the performance of our method with prior art on typical queries over a large scaled LUBM and UOBM benchmark data (more than 10 million triples). For some extreme cases, they can promote 3 to 4 orders of magnitude.	Efficiently querying rdf data in triple stores	NA:NA:NA:NA:NA:NA	2018
Jyh-Ren Shieh:Yang-Ting Yeh:Chih-Hung Lin:Ching-Yung Lin:Ja-Ling Wu	In this paper, we propose a Collaborative Knowledge Semantic Graphs Image Search (CKSGIS) system. It provides a novel way to conduct image search by utilizing the collaborative nature in Wikipedia and by performing network analysis to form semantic graphs for search-term expansion. The collaborative article editing process used by Wikipedia's contributors is formalized as bipartite graphs that are folded into networks between terms. When a user types in a search term, CKSGIS automatically retrieves an interactive semantic graph of related terms that allow users to easily find related images not limited to a specific search term. Interactive semantic graph then serve as an interface to retrieve images through existing commercial search engines. This method significantly saves users' time by avoiding multiple search keywords that are usually required in generic search engines. It benefits both naïve users who do not possess a large vocabulary and professionals who look for images on a regular basis. In our experiments, 85% of the participants favored CKSGIS system rather than commercial search engines.	Collaborative knowledge semantic graph image search	NA:NA:NA:NA:NA	2018
Hai Liu:Qing Li:Naijie Gu:An Liu	In this paper, we incorporate concrete domain and action theory into a very expressive Description Logic (DL), called ALCQO. Notably, this extension can significantly augment the expressive power for modeling and reasoning about dynamic aspects of services contracting. Meanwhile, the original nature and advantages of classical DLs are also preserved to the extent possible.	A logical framework for modeling and reasoning about semantic web services contract	NA:NA:NA:NA	2018
Yuan Ni:Chee-Yong Chan	A lot of recent research has focused on the content-based dissemination of XML data. However, due to the heterogeneous data schemas used by different data publishers even for data in the same domain, an important challenge is how to efficiently and effectively disseminate relevant data to subscribers whose subscriptions might be specified based on schemas that are different from those used by the data publishers. This paper examines the options to resolve this schema heterogeneity problem in XML data dissemination, and proposes a novel paradigm that is based on data rewriting. Our experimental results demonstrate the effectiveness of the data rewriting paradigm and identifies the tradeoffs of the various approaches	Dissemination of heterogeneous xml data	NA:NA	2018
Guoliang Li:Jianhua Feng:Jianyong Wang:Xiaoming Song:Lizhu Zhou	This paper studies the problem of unified ranked retrieval of heterogeneous XML documents and Web data. We propose an effective search engine called Sailer to adaptively and versatilely answer keyword queries over the heterogenous data. We model the Web pages and XML documents as graphs. We propose the concept of pivotal trees to effectively answer keyword queries and present an effective method to identify the top-k pivotal trees with the highest ranks from the graphs. Moreover, we propose effective indexes to facilitate the effective unified ranked retrieval. We have conducted an extensive experimental study using real datasets, and the experimental results show that Sailer achieves both high search efficiency and accuracy, and outperforms the existing approaches significantly.	Sailer: an effective search engine for unified retrieval of heterogeneous xml and web documents	NA:NA:NA:NA:NA	2018
Nikhil Garg:Ingmar Weber	We present a system for personalized tag suggestion for Flickr: While the user is entering/selecting new tags for a particular picture, the system is suggesting related tags to her, based on the tags that she or other people have used in the past along with (some of) the tags already entered. The suggested tags are dynamically updated with every additional tag entered/selected. We describe three algorithms which can be applied to this problem. In experiments, our best-performing method yields an improvement in precision of 10-15% over a baseline method very similar to the system currently used by Flickr. Our system is accessible at http://ltaa5.epfl.ch/flickr-tags/. To the best of our knowledge, this is the first study on tag suggestion in a setting where (i) no full text information is available, such as for blogs, (ii) no item has been tagged by more than one person, such as for social bookmarking sites, and (iii) suggestions are dynamically updated, requiring efficient yet effective algorithms.	Personalized tag suggestion for flickr	NA:NA	2018
Qiancheng Jiang:Lei Zhang:Yizhen Zhu:Yan Zhang	Seed selection is of significant importance for the biased PageRank algorithms such as TrustRank to combat link spamming. Previous work usually uses a small seed set, which has a big problem that the top ranking results have a strong bias towards seeds. In this paper, we analyze the relationship between the result bias and the number of seeds. Furthermore, we experimentally show that an automatically selected large seed set can work better than a carefully selected small seed set.	Larger is better: seed selection in link-based anti-spamming algorithms	NA:NA:NA:NA	2018
Ling Chen:Yiqun Hu:Wolfgang Nejdl	Although most of existing research usually detects events by analyzing the content or structural information of Web documents, a recent direction is to study the usage data. In this paper, we focus on detecting events from Web click-through data generated by Web search engines. We propose a novel approach which effectively detects events from click-through data based on robust subspace analysis. We first transform click-through data to the 2D polar space. Next, an algorithm based on Generalized Principal Component Analysis (GPCA) is used to estimate subspaces of transformed data such that each subspace contains query sessions of similar topics. Then, we prune uninteresting subspaces which do not contain query sessions corresponding to real events by considering both the semantic certainty and the temporal certainty of query sessions in each subspace. Finally, various events are detected from interesting subspaces by utilizing a nonparametric clustering technique. Compared with existing approaches, our experimental results based on real-life click-through data have shown that the proposed approach is more accurate in detecting real events and more effective in determining the number of events.	Using subspace analysis for event detection from web click-through data	NA:NA:NA	2018
Patrick Freudenstein:Martin Nussbaumer:Florian Allerding:Martin Gaedke	Complex dialogs with comprehensive underlying data models are gaining increasing importance in today's Web applications. This in turn accelerates the need for highly dynamic dialogs offering guidance to the users and thus reducing cognitive overload. Beyond that, requirements from the fields of aesthetics, Web accessibility, platform-independence, and Web service integration arise. To this end, we present an evolutionary, extensible approach for the model-driven construction of advanced dialogs. It is based on a Domain-specific Language (DSL) focusing on simplicity and fostering collaboration with stakeholders.	A domain-specific language for the model-driven construction of advanced web-based dialogs	NA:NA:NA:NA	2018
Javier Artiles:Satoshi Sekine:Julio Gonzalo	This paper presents the motivation, resources and results for the first Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. Also, we will describe a survey and proposal for a new task, "attribute extraction", which is planned for inclusion in the second evaluation, planned for autumn, 2008.	Web people search: results of the first evaluation and the plan for the second	NA:NA:NA	2018
Wen-Huang Cheng:David Gotz	Sensemaking tasks require users to perform complex research behaviors to gather and comprehend information from many sources. Such tasks are common and include, for example, researching vacation destinations or deciding how to invest. In this paper, we present an algorithm and interface that provides context-based page unit recommendation to assist in connection discovery during sensemaking tasks. We exploit the natural note-taking activity common to sensemaking behavior as the basis for a task-specific context model. Each web page visited by a user is dynamically analyzed to determine the most relevant content fragments which are then recommended to the user. Our initial evaluations indicate that our approach improves user performance.	Context-based page unit recommendation for web-basedsensemaking tasks	NA:NA	2018
Michalis Vazirgiannis:Dimitris Drosos:Pierre Senellart:Akrivi Vlachou	In this paper we propose a method for predicting the ranking position of a Web page. Assuming a set of successive past top-k rankings, we study the evolution of Web pages in terms of ranking trend sequences used for Markov Models training, which are in turn used to predict future rankings. The predictions are highly accurate for all experimental setups and similarity measures.	Web page rank prediction with markov models	NA:NA:NA:NA	2018
Louenas Hamdi:Huaigu Wu:Serhan Dagtas:Abdel Benharref	This paper provides a brief description of a research project on using Ajax to enhance mobile Web applications for enterprise use. The project, known as MobileWeaver Ajax Framework, leverages enterprise SOA (Service Oriented Architecture) and the latest web technologies on mobile devices.	Ajax for mobility: mobileweaver ajax framework	NA:NA:NA:NA	2018
Xiaojun Wan	WebPMI is a popular web-based association measure to evaluate the semantic similarity between two queries (i.e. words or entities) by leveraging search results returned by search engines. This paper proposes a novel measure named CM-PMI to evaluate query similarity at a finer granularity than WebPMI, under the assumption that a query is usually associated with more than one aspect and two queries are deemed semantically related if their associated aspect sets are highly consistent with each other. CM-PMI first extracts contextual labels from search results to represent the aspects of a query, and then uses the optimal matching method to assess the consistency between the aspects of two queries. Experimental results on the benchmark Miller Charles' dataset demonstrate the good effectiveness of the proposed CM-PMI measure. Moreover, we further fuse WebPMI and CM-PMI to obtain improved results.	Cm-pmi: improved web-based association measure with contextual label matching	NA	2018
Jiaqian Zheng:Jing Yao:Junyu Niu	It is a kind of privacy infraction in personalized web service if the user profile submitted to one web site transferred to another site without user permission. That can cause the second web site easily re-identify to whom these personal data belong, no matter whether the transfer is under control or by hacking. This paper presents a portable solution for users to bind their sensitive web data under the appointed domain. Such data, including query logs, user accounts, click stream etc, could be used to identify the sensitive information of the particular user. By our domain stretching de-identification method, if personal data leak from domain A to B, the web user could still not be identified even though he logins to sites under domain B using the same name and password. In the experiment implemented by javascript, we show the flexibility and efficiency of our de-identification approach.	Web user de-identification in personalization	NA:NA:NA	2018
Liang Xie:Hui Song:Trent Jaeger:Sencun Zhu	Cell phones are increasingly becoming attractive targets of various worms, which cause the leakage of user privacy, extra service charges and depletion of battery power. In this work, we study propagation of cell-phone worms, which exploit Multimedia Messaging Service (MMS) and/or Bluetooth for spreading. We then propose a systematic countermeasure against the worms. At the terminal level, we adopt Graphic Turing test and identity-based signature to block unauthorized messages from leaving compromised phones; at the network level, we propose a push-based automated patching scheme for cleansing compromised phones. Through experiments on phone devices and a wide variety of networks, we show that cellular systems taking advantage of our defense can achieve a low infection rate (e.g., less than 3% within 30 hours) even under severe attacks.	A systematic approach for cell-phone worm containment	NA:NA:NA:NA	2018
Zhaohui Wu:Tong Yu:Huajun Chen:Xiaohong Jiang:Yi Feng:Yuxin Mao:Heng Wang:Jingming Tang:Chunying Zhou	We conduct the first systematical adoption of the Semantic Web solution in the integration, management, and utilization of TCM information and knowledge resources. As the results, the largest TCM Semantic Web ontology is engineered as the uniform knowledge representation mechanism; the ontology-based query and search engine is deployed, mapping legacy and heterogeneous relational databases to the Semantic Web layer for query and search across database boundaries; the first global herb-drug interaction network is mapped through semantic integration, and the semantic graph mining methodology is implemented for discovering and interpreting interesting patterns from this network. The platform and underlying methodology are proved effective in TCM-related drug usage, discovery, and safety analysis.	Information retrieval and knowledge discovery on the semantic web of traditional chinese medicine	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Ko Fujimura:Shigeru Fujimura:Tatsushi Matsubayashi:Takeshi Yamada:Hidenori Okuda	This paper proposes a new method for displaying large-scale tag clouds. We use a topographical image that helps users to grasp the relationship among tags intuitively as a background to the tag clouds. We apply this interface to a blog navigation system and show that the proposed method enables users to find the desired tags easily even if the tag clouds are very large, 5,000 and above tags. Our approach is also effective for understanding the overall structure of a large amount of tagged documents.	Topigraphy: visualization for large-scale tag clouds	NA:NA:NA:NA:NA	2018
Yuko Sakurai:Atsushi Iwasaki:Yasumasa Saito:Makoto Yokoo	We propose a keyword auction protocol called the Generalized Second Price with an Exclusive Right (GSP-ExR). In existing keyword auctions, the number of displayed advertisements is determined in advance. Thus, we consider adjusting the number of advertisements dynamically based on bids. In the GSP-ExR, the number of slots can be either 1 or K. When K slots are displayed, the protocol is identical to the GSP. If the value per click of the highest ranked bidder is large enough, then this bidder can exclusively display her advertisement by paying a premium. Thus, this pricing scheme is relatively simple and seller revenue is at least as good as the GSP. Also, in the GSP-ExR, the highest ranked bidder has no incentive to change the number of slots by over/under-bidding as long as she retains the top position.	Gsp-exr: gsp protocol with an exclusive right for keyword auctions	NA:NA:NA:NA	2018
Alex Penev:Raymond K. Wong	Social tagging describes a community of users labeling web content with tags. It is a simple activity that enriches our knowledge about resources on the web. For a computer to help users search the tagged repository, it must know when tags are good or bad. We describe TagScore, a scoring function that rates the goodness of tags. The tags and their ratings give us a succinct synopsis for a page. We `find similar' pages in Del.icio.us by comparing synopses. Our approach gives good correlation to the full cosine similarity but is hundreds of times faster.	Finding similar pages in a social tagging repository	NA:NA	2018
Kangpyo Lee:Hyunwoo Kim:Chungsu Jang:Hyoung-Joo Kim	In this paper, targeting del.icio.us tag data, we propose a method, FolksoViz, for deriving subsumption relationships between tags by using Wikipedia texts, and visualizing a folksonomy. To fulfill this method, we propose a statistical model for deriving subsumption relationships based on the frequency of each tag on the Wikipedia texts, as well as the TSD (Tag Sense Disambiguation) method for mapping each tag to a corresponding Wikipedia text. The derived subsumption pairs are visualized effectively on the screen. The experiment shows that the FolksoViz manages to find the correct subsumption pairs with high accuracy.	Folksoviz: a subsumption-based folksonomy visualization using wikipedia texts	NA:NA:NA:NA	2018
Joshua E. Blumenstock	Wikipedia, "the free encyclopedia", now contains over two million English articles, and is widely regarded as a high-quality, authoritative encyclopedia. Some Wikipedia articles, however, are of questionable quality, and it is not always apparent to the visitor which articles are good and which are bad. We propose a simple metric -- word count -- for measuring article quality. In spite of its striking simplicity, we show that this metric significantly outperforms the more complex methods described in related work.	Size matters: word count as a measure of quality on wikipedia	NA	2018
Xiaozhu Kang:Hui Zhang:Guofei Jiang:Haifeng Chen:Xiaoqiao Meng:Kenji Yoshihira	In this paper we measured and analyzed the workload on Yahoo! Video, the 2nd largest U.S. video sharing site, to understand its nature and the impact on online video data center design. We discovered interesting statistical properties on both static and temporal dimensions of the workload including file duration and popularity distributions, arrival rate dynamics and predictability, and workload stationarity and burstiness. Complemented with queueing-theoretic techniques, we further extended our understanding on the measurement data with a virtual design on the workload and capacity management components of a data center assuming the same workload as measured, which reveals key results regarding the impact of Service Level Agreements (SLAs) and workload scheduling schemes on the design and operations of such large-scale video distribution systems.	Understanding internet video sharing site workload: a view from data center design	NA:NA:NA:NA:NA:NA	2018
Nan Di:Conglei Yao:Mengcheng Duan:Jonathan Zhu:Xiaoming Li	As opposed to representing a document as a "bag of words" in most information retrieval applications, we propose a model of representing a web page as sets of named entities of multiple types. Specifically, four types of named entities are extracted, namely person, geographic location, organization, and time. Moreover, the relations among these entities are also extracted, weighted, classified and marked by labels. On top of this model, some interesting applications are demonstrated. In particular, we introduce a notion of person-activity, which contains four different elements: person, location, time and activity. With this notion and based on a reasonably large set of web pages, we are able to show how one person's activities can be attributed by time and location, which gives a good idea of the mobility of the person under question.	Representing a web page as sets of named entities of multiple types: a model and some preliminary applications	NA:NA:NA:NA:NA	2018
Gong Cheng:Weiyi Ge:Yuzhong Qu	As of today, the amount of data on the Semantic Web has grown considerably. The services for searching and browsing entities on the Semantic Web are in demand. To provide such services, we developed the Falcons system. In this poster, we present the features of the Falcons system.	Falcons: searching and browsing entities on the semantic web	NA:NA:NA	2018
Ann Hsieh:Todd Hausman:Nerija Titus	Modern Day Marco Polos, young influencers in emerging markets who have more access to technology than their peers, often act as the gateway to new websites and technology in their respective countries. However, as they influence their peers through account gifting, they are often met with barriers such as language.	Influencers and their barriers to technology	NA:NA:NA	2018
Florin Dragan:Georges Gardarin:Laurent Yeh	In this poster, we present the P2P XML data localization layer of XLive, an XQuery mediation system developed at University of Versailles [2]. A major challenge in the evaluation of XQuery over a P2P network in the context of multiple XML sources is to abstract from structural heterogeneity. Most existing approaches have mainly exploited varied types of 1:1 semantic mappings between peer schemas and/or ontologies. Complex query algorithms are required to exploit these semantic links between peers. In contrast, our approach focuses on a simple semantic layer. It is built on a Chord [4] DHT for indexing semantic descriptions of mediated data sources. A mapping process transforms an XML view of a mediator (each peer is equipped with a mediator) to a semantic form that is published into the P2P network and stored in the DHT. For a given user query, a search in the DHT retrieves all relevant data sources able to contribute to the result elaboration. Then, every peer that contains relevant data is directly queried to collect data for elaborating the final answer.	A semantic layer for publishing and localizing xml data for a p2p xquery mediator	NA:NA:NA	2018
Danushka Bollegala:Taiki Honma:Yutaka Matsuo:Mitsuru Ishizuka	We propose a novel approach to find aliases of a given name from the web. We exploit a set of known names and their aliases as training data and extract lexical patterns that convey information related to aliases of names from text snippets returned by a web search engine. The patterns are then used to find candidate aliases of a given name. We use anchor texts and hyperlinks to design a word co-occurrence model and define numerous ranking scores to evaluate the association between a name and its candidate aliases. The proposed method outperforms numerous baselines and previous work on alias extraction on a dataset of personal names, achieving a statistically significant mean reciprocal rank of 0.6718. Moreover, the aliases extracted using the proposed method improve recall by 20% in a relation-detection task.	Mining for personal name aliases on the web	NA:NA:NA:NA	2018
Kengo Fujioka:Yukio Uematsu:Makoto Onizuka	We developed the HS-bitmap index for efficient information retrieval. The HS-bitmap index is a hierarchical document-term matrix: the original document-term matrix is called the leaf matrix and an upper matrix is the summary of its lower matrix. Our experiment results show the HS-bitmap index performs better than the inverted index with a minor space overhead.	Application of bitmap index to information retrieval	NA:NA:NA	2018
Xiaoyan Li:Lidan Shou:Gang Chen:Xiaolong Zhang:Tianlei Hu:Jinxiang Dong	We propose a novel iterative searching and refining prototype for tagged images. This prototype, named PivotBrowser, captures semantically similar tag sets in a structure called pivot. By constructing a pivot for a textual query, PivotBrowser first selects candidate images possibly relevant to the query. The tags contained in these candidate images are then selected in terms of their tag relevances to the pivot. The shortlisted tags are clustered and one of the tag clusters is used to select the results from the candidate images. Ranking of the images in each partition is based on their relevance to the tag cluster. With the guidance of the tag clusters presented, a user is able to perform searching and iterative query refinement.	Pivotbrowser: a tag-space image searching prototype	NA:NA:NA:NA:NA:NA	2018
Yana Volkovich:Nelly Litvak:Bert Zwart	We analyze dependencies in power law graph data (Web sample, Wikipedia sample and a preferential attachment graph) using statistical inference for multivariate regular variation. The well developed theory of regular variation is widely applied in extreme value theory, telecommunications and mathematical finance, and it provides a natural mathematical formalism for analyzing dependencies between variables with power laws. However, most of the proposed methods have never been used in the Web graph data mining. The present work fills this gap. The new insights this yields are striking: the three above-mentioned data sets are shown to have a totally different dependence structure between different graph parameters, such as in-degree and PageRank.	Measuring extremal dependencies in web graphs	NA:NA:NA	2018
Ranbir S. Sanasam:Hema A. Murthy:Timothy A. Gonsalves	Most of the search engine optimization techniques attempt to predict users interest by learning from the past information collected from different sources. But, a user's current interest often depends on many factors which are not captured in the past information. In this paper, we attempt to identify user's current interest in real time from the information provided by the user in the current query session. By identifying user's interest in real time, the engine could adapt differently to different users in real time. Experimental verification indicates that our approach is encouraging for short queries	Determining user's interest in real time	NA:NA:NA	2018
Nan Hu:Ling Liu:Bin Chen:Jialie Shen	This paper investigates the strategic decisions of online vendors for offering different mechanism, such as sampling and online reviews of information products, to increase their online sales. Focusing on measuring the effectiveness of electronic market design (offering reviews, sampling, or both), our study shows that online markets behavior as communication markets, and consumers learn product quality information both passively (reading online reviews) and actively but subjectively (listening to music sampling). Using data from Amazon, first we show that sampling along is a strong product quality signal that reduces the product uncertainty after controlling for halo effect. In general, products with sampling option enjoy a higher conversion rate (which leads to better sales) than those without sampling because sampling decreases the uncertainty of consuming experience goods. Second, the impact of online reviews on sales conversion rate is lower for experience goods with a sampling option than those without. Third, when the uncertainty of the societal reviews is higher, sampling plays a more important role because it mitigates such uncertainty introduced by online reviews.	How to influence my customers?: the impact of electronic market design	NA:NA:NA:NA	2018
Guanggang Geng:Chunheng Wang:Qiudan Li	Web spam detection has become one of the top challenges for the Internet search industry. Instead of using some heuristic rules, we propose a feature re-extraction strategy to optimize the detection result. Based on the predicted spamicity obtained by the preliminary detection, through the host level web graph, three types of features are extracted. Experiments on WEBSPAM-UK2006 benchmark show that with this strategy, the performance of web spam detection can be improved evidently.	Improving web spam detection with re-extracted features	NA:NA:NA	2018
Sheetal Agarwal:Arun Kumar:Amit Anil Nanavati:Nitendra Rajput	As the number of telephony voice applications grow, there will be a need for a browser to surf the Web of interconnected voice applications (called as VoiceSites). These VoiceSites are accessed through a telephone over an audio channel. We present the concept and architecture of T-Web Browser, a World Wide Telecom Web browser that enables browsing the Web of voice applications through an ordinary phone. This browser will support rich browsing features such as history and bookmarking.	The world wide telecom web browser	NA:NA:NA:NA	2018
Sheetal Agarwal:Arun Kumar:Amit Anil Nanavati:Nitendra Rajput	One of the several initiatives to bridge the digital divide in developing countries has been the deployment of information kiosks or knowledge centers in villages in rural parts of the country. These kiosks provide services ranging from email, chat and browsing to distance education programs, agricultural services and eGovernance services. A kiosk typically comprises of a computer with printer, web cam, multimedia system and Internet connectivity and is owned by a local entrepreneur. Moving away from the PC based kiosk model, we present an alternative platform to create and host such information kiosks in the telephony network. We call these as VoiKiosks and they are accessible through voice interaction over an ordinary phone call.	VoiKiosk: increasing reachability of kiosks in developing regions	NA:NA:NA:NA	2018
Ce Zhang:Yu-Jing Wang:Bin Cui:Gao Cong	This paper presents a new method of calculating the semantic similarity between two articles based on WordNet. To further improve the performance of the proposed method, we build a new Compact Concept Ontology (CCO) from WordNet by combining the words with similar semantic meanings. The experimental results show that our approach significantly outperforms a recent proposal of computing semantic similarity, and demonstrate the superiority of the proposed CCO method.	Semantic similarity based on compact concept ontology	NA:NA:NA:NA	2018
Oscar Díaz:Cristóbal Arellano:Jon Iturrioz	Client scripting permits end users to customize content, layout or style of their favourite websites. But current scripting suffers from a tight coupling with the website. If the page changes, all the scripting can fall apart. The problem is that websites are reckoned to evolve frequently, and this can jeopardize all the scripting efforts. To avoid this situation, this work enriches websites with a "modding interface" in an attempt to decouple layman's script from website upgrades. From the website viewpoint, this interface ensures safe scripting, i.e. scripts that do not break the page. From a scripter perspective, this interface limits tuning but increases change resilience. The approach tries to find a balance between openness (scripter free inspection) and modularity (scripter isolation from website design decisions) that permits scripting to scale up as a mature software practice. The approach is realized for Greasemonkey scripts.	Layman tuning of websites: facing change resilience	NA:NA:NA	2018
Yuan Cheng:Guang Qiu:Jiajun Bu:Kangmiao Liu:Ye Han:Can Wang:Chun Chen	Blogs have been expanded at an incredible speed in recent years. Plentiful personal information makes blogs a popular way mining user profiles. In this paper, we propose a novel bloggers' interests modeling approach based on forgetting mechanism. A new forgetting function is introduced to track interest drift. Based on that, the Short Term Interest Models (STIM) and Long Term Interest Models (LTIM) are constructed to describe bloggers' short-term and long-term interests. The experiments show that both models can identify bloggers' preferences well respectively.	Model bloggers' interests based on forgetting mechanism	NA:NA:NA:NA:NA:NA:NA	2018
Geetha Manjunath:R Badrinath:Craig Sayers:Venugopal K. S	Supporting fast access to large RDF stores has been one of key challenges for enabling use of the Semantic Web in real-life applications, more so in sensor-based systems where large amounts of historic data needs to be stored. We propose a semantics-based temporal view mechanism that enables faster access to time-varying data by caching into memory only the required subset of RDF triples. We describe our experience of implementing such a framework in the context of a wide area network monitoring system. Our preliminary results show that our solution significantly improves client access time and scales well for moderate data sets.	Temporal views over rdf data	NA:NA:NA:NA	2018
Jonathan J. H. Zhu:Tao Meng:Zhengmao Xie:Geng Li:Xiaoming Li	The shape of the Web in terms of its graphical structure has been a widely interested topic. Two graphs, Bow Tie and Daisy, have stood out from previous research. In this work, we take a different approach, by viewing the Web as a hierarchy of three levels, namely page level, host level, and domain level. Such structures are analyzed and compared with a snapshot of Chinese Web in early 2006, involving 830 million pages, 17 million hosts, and 0.8 million domains. Some interesting results have emerged. For example, the Chinese Web appears more like a teapot (with a large size of SCC, a medium size of IN and a small size of OUT) at page level than the classic bow tie or daisy shape. Some challenging phenomena are also observed. For example, the INs become much smaller than OUTs at host and domain levels. Future work will tackle these puzzles.	A teapot graph and its hierarchical structure of the chinese web	NA:NA:NA:NA:NA	2018
Somnath Banerjee:Krishnan Ramanathan	Many real life datasets have skewed distributions of events when the probability of observing few events far exceeds the others. In this paper, we observed that in skewed datasets the state of the art collaborative filtering methods perform worse than a simple probabilistic model. Our test bench includes a real ad click stream dataset which is naturally skewed. The same conclusion is obtained even from the popular movie rating dataset when we pose a binary prediction problem of whether a user will give maximum rating to a movie or not.	Collaborative filtering on skewed datasets	NA:NA	2018
Jacek Jankowski:Sebastian Ryszard Kruk	The World Wide Web allows users to create and publish a variety of resources, including multimedia ones. Most of the contemporary best practices for designing web interfaces, however, do not take into account the 3D techniques. In this paper we present a novel approach for designing interactive web applications -- 2-Layer Interface Paradigm (2LIP). The background layer of the 2LIP-type user interface is a 3D scene, which a user cannot directly interact with. The foreground layer is HTML content. Only taking an action on this content (e.g. pressing a hyperlink, scrolling a page) can affect the 3D scene. We introduce a reference implementation of 2LIP: Copernicus - The Virtual 3D Encyclopedia, which shows one of the potential paths of the evolution of Wikipedia towards Web 3.0. Based on the evaluation of Copernicus we prove that designing web interfaces according to 2LIP provides users a better browsing experience, without harming the interaction.	2lip: the step towards the web3d	NA:NA	2018
Xinran Wang:Yoon-chan Jhi:Sencun Zhu:Peng Liu	We propose STILL, a signature-free remote exploit binary code injection attack blocker to protect web servers and web applications. STILL is robust to almost all anti-signature, anti-static-analysis and anti-emulation obfuscation.	Protecting web services from remote exploit code: a static analysis approach	NA:NA:NA:NA	2018
Mahmoud Barhamgi:Djamal Benslimane:Aris M. Ouksel	In this paper, we propose a new approach to automatically compose data providing Web services. Our approach exploits existing mature works done in data integration systems. Specifically, data providing services are modeled as RDF Parameterized views over mediated ontologies. Then, an RDF oriented algorithm to compose services based on query rewriting techniques was devised. We apply also an optimization algorithm on the generated composition to speed up its execution. The results of our experiments show that the algorithms scale up very well up to a large number of services, covering thus most realistic applications.	Composing and optimizing data providing web services	NA:NA:NA	2018
Samantha Kleinberg:Bud Mishra	Determining candidates' views on important issues is critical in deciding whom to support and vote for; but finding their statements and votes on an issue can be laborious. In this paper we present PSST, (Political Statement and Support Tracker), a search engine to facilitate analysis of political statements and votes over time. We show that prior tools for text analysis can be combined with minimal manual processing to provide a first step in the full automation of this process.	Psst: a web-based system for tracking political statements	NA:NA	2018
Wolfgang Holzinger:Bernhard Kruepl:Robert Baumgartner	Form mapping is the key problem that needs to be solved in order to get access to the hidden web. Currently available solutions for fully automatic mapping are not ready for commercial meta-search engines, which still have to rely on hand crafted code and are hard to maintain. We believe that a thorough formal description of the problem with semantic web technologies provides a promising perspective to develop a new class of vertical search engines that is more robust and easier to maintain than existing solutions. In this paper, instead of trying to tackle the mapping problem, we model the interaction necessary to fill out a web form. First, during a user-assisted phase, the connection from the visible elements on the form to the domain concepts is established. Then, with help from background knowledge about the possible interaction steps, a plan for filling out the form is derived.	Exploiting semantic web technologies to model web form interactions	NA:NA:NA	2018
Fabian Abel:Nicola Henze:Daniel Krause	This paper presents the GroupMe! system, a resource sharing system with advanced tagging functionality. GroupMe! provides a novel user interface, which enables users to organize and arrange arbitrary Web resources into groups. The content of such groups can be overlooked and inspected immediately as resources are visualized in a multimedia-based fashion. In this paper, we furthermore introduce new folksonomy-based ranking strategies that exploit the group structure shipped with GroupMe! folksonomies. Experiments show that those strategies significantly improve the performance of such ranking algorithms.	Groupme!	NA:NA:NA	2018
Carrie Grimes	We track a large set of "rapidly" changing web pages and examine the assumption that the arrival of content changes follows a Poisson process on a microscale. We demonstrate that there are significant differences in the behavior of pages that can be exploited to maintain freshness in a web corpus.	Microscale evolution of web pages	NA	2018
Rupesh R. Mehta:Amit Madaan	This work aims to provide a novel, site-specific web page segmentation and section importance detection algorithm, which leverages structural, content, and visual information. The structural and content information is leveraged via template, a generalized regular expression learnt over set of pages. The template along with visual information results into high sectioning accuracy. The experimental results demonstrate the effectiveness of the approach.	Web page sectioning using regex­-based template	NA:NA	2018
Arun Kumar:D. Janakiram	Services Computing is emerging as a new discipline. The acceptance of web services technology stems from the fact that services enable easy integration and interoperation of enterprise level distributed systems. However, currently software developers are forced to translate business level service requirements and encode them into programs using low level abstractions such as objects. We propose to introduce language constructs for Service Oriented Programming that would enable raising programming abstractions from objects to services.	Towards a programming language for services computing	NA:NA	2018
Lian'en Huang:Jonathan J. H. Zhu:Xiaoming Li	In this paper, we describe an experimental search engine on our Chinese web archive since 2001. The original data set contains nearly 3 billion Chinese web pages crawled from past 5 years. From the collection, 430 million "article-like" pages are selected and then partitioned into 68 million sets of similar pages. The titles and publication dates are determined for the pages. An index is built. When searching, the system returns related pages in a chronological order. This way, if a user is interested in news reports or commentaries for certain previously happened event, he/she will be able to find a quite rich set of highly related pages in a convenient way.	Histrace: building a search engine of historical events	NA:NA:NA	2018
Peter I. Hofgesang:Jan Peter Patist	Based on our field studies and consultations with field experts, we identified three main problems that are of key importance to online web personalization and customer relationship management: 1) detecting changes in individual behaviour, 2) reporting on user actions that may need special care, and 3) detecting changes in visitation frequency. We propose solutions to these problems and experiment on real-world data from an investment bank collected over 1.5 years of web traffic. These solutions can be applied on any domains where individuals tend to revisit the website and can be identified accurately.	Online change detection in individual web user behaviour	NA:NA	2018
Jeffrey P. Bigham:Craig M. Prince:Richard E. Ladner	People often use computers other than their own to access web content, but blind users are restricted to using computers equipped with expensive, special-purpose screen reading programs that they use to access the web. WebAnywhere is a web-based, self-voicing web application that enables blind web users to access the web from almost any computer that can produce sound without installing new software. WebAnywhere could serve as a convenient, low-cost solution for blind users on-the-go, for blind users unable to afford another screen reader and for web developers targeting accessible design. This paper describes the implementation of WebAnywhere, overviews an evaluation of it by blind web users, and summarizes a survey of public terminals that shows it can run on most public computers.	Webanywhere: enabling a screen reading interface for the web on any computer	NA:NA:NA	2018
Valerie King:Louis Lei Yu:Yan Zhuang	Guanxi is a type of dyadic social interaction based on feelings ("qing") and trust ("xin"). Long studied by scholars of Chinese origin, it has recently drawn the attention of researchers outside of China. We define the concept of guanxi as applied to the interaction between web sites. We explore methods to identify guanxi in the Chinese web, show the unique characteristics of the Chinese web which result from it, and introduce a mechanism for simulating guanxi in a web graph model.	Guanxi in the chinese web - a study of mutual linking	NA:NA:NA	2018
James Caverlee:Ling Liu:Steve Webb	We propose the SocialTrust framework for tamper-resilient trust establishment in online social networks. Two of the salient features of SocialTrust are its dynamic revision of trust by (i) distinguishing relationship quality from trust; and (ii) incorporating a personalized feedback mechanism for adapting as the social network evolves.	Towards robust trust establishment in web-based social networks with socialtrust	NA:NA:NA	2018
Robert Graham:Brian Eoff:James Caverlee	We present the design of Plurality, an interactive tagging system. Plurality's modular architecture allows users to automatically generate high-quality tags over Web content, as well as over archival and personal content typically beyond the reach of existing Web 2.0 social tagging systems. Three of the salient features of Plurality are: (i) its self-learning and feedback-sensitive capabilities based on a user's personalized tagging style; (ii) its leveraging of the collective intelligence of existing social tagging services; and (iii) its context-awareness for optimizing tag suggestions, e.g., based on spatial or temporal features.	Plurality: a context-aware personalized tagging system	NA:NA:NA	2018
Panagiotis Papadimitriou:Ali Dasdan:Hector Garcia-Molina	Web graphs are approximate snapshots of the web, created by search engines. Their creation is an error-prone procedure that relies on the availability of Internet nodes and the faultless operation of multiple software and hardware units. Checking the validity of a web graph requires a notion of graph similarity. Web graph similarity helps measure the amount and significance of changes in consecutive web graphs. These measurements validate how well search engines acquire content from the web. In this paper we study five similarity schemes: three of them adapted from existing graph similarity measures and two adapted from well-known document and vector similarity methods. We compare and evaluate all five schemes using a sequence of web graphs for Yahoo! and study if the schemes can identify anomalies that may occur due to hardware or other problems.	Web graph similarity for anomaly detection (poster)	NA:NA:NA	2018
Rifat Ozcan:Ismail Sengor Altingovde:Özgür Ulusoy	Query result caching is an important mechanism for search engine efficiency. In this study, we first review several query features that are used to determine the contents of a static result cache. Next, we introduce a new feature that more accurately represents the popularity of a query by measuring the stability of query frequency over a set of time intervals. Experimental results show that this new feature achieves hit ratios better than those of the previously proposed features.	Static query result caching revisited	NA:NA:NA	2018
Santanu Kolay:Paolo D'Alberto:Ali Dasdan:Arnab Bhattacharjee	A website can regulate search engine crawler access to its content using the robots exclusion protocol, specified in its robots.txt file. The rules in the protocol enable the site to allow or disallow part or all of its content to certain crawlers, resulting in a favorable or unfavorable bias towards some of them. A 2007 survey on the robots.txt usage of about 7,593 sites found some evidence of such biases, the news of which led to widespread discussions on the web. In this paper, we report on our survey of about 6 million sites. Our survey tries to correct the shortcomings of the previous survey and shows the lack of any significant preferences towards any particular search engine.	A larger scale study of robots.txt	NA:NA:NA:NA	2018
Allison P. Heath:Ryen W. White	Searchers have a choice about which Web search engine they use when looking for information online. If they are unsuccessful on one engine, users may switch to a different engine to continue their search. By predicting when switches are likely to occur, the search experience can be modified to retain searchers or ensure a quality experience for incoming searchers. In this poster, we present research on a technique for predicting search engine switches. Our findings show that prediction is possible at a reasonable level of accuracy, particularly when personalization or user grouping is employed. These findings have implications for the design of applications to support more effective online searching.	Defection detection: predicting search engine switching	NA:NA	2018
Yunhong Zhou:Victor Naroditskiy	We model budget-constrained keyword bidding in sponsored search auctions as a stochastic multiple-choice knapsack problem (S-MCKP) and design an algorithm to solve S-MCKP and the corresponding bidding optimization problem. Our algorithm selects items online based on a threshold function which can be built/updated using historical data. Our algorithm achieved about 99% performance compared to the offline optimum when applied to a real bidding dataset. With synthetic dataset and iid item-sets, its performance ratio against the offline optimum converges to one empirically with increasing number of periods.	Algorithm for stochastic multiple-choice knapsack problem and application to keywords bidding	NA:NA	2018
Ioannis Antonellis:Hector Garcia-Molina:Chi-Chao Chang	We focus on the problem of query rewriting for sponsored search. We base rewrites on a historical click graph that records the ads that have been clicked on in response to past user queries. Given a query q, we first consider Simrank [2] as a way to identify queries similar to q, i.e., queries whose ads a user may be interested in. We argue that Simrank fails to properly identify query similarities in our application, and we present two enhanced versions of Simrank: one that exploits weights on click graph edges and another that exploits evidence." We experimentally evaluate our new schemes against Simrank, using actual click graphs and queries form Yahoo!, and using a variety of metrics. Our results show that the enhanced methods can yield more and better query rewrites.	Simrank++: query rewriting through link analysis of the clickgraph (poster)	NA:NA:NA	2018
Li Ding:Jiao Tao:Deborah L. McGuinness	Many emerging semantic web applications include ontologies from one set of authors and instance data from another (often much larger) set of authors. Often ontologies are reused and instance data is integrated in manners unanticipated by their authors. Not surprisingly, many instance data rich applications encounter instance data that is not compatible with the expectations of the original ontology author(s). This line of work focuses on issues related to semantic expectation mismatches in instance data. Our initial results include a customizable and extensible service-oriented evaluation architecture, and a domain implementation called PmlValidator, which checks instance data using the corresponding ontologies and additional style requirements.	An initial investigation on evaluating semantic web instance data	NA:NA:NA	2018
Kavita A. Ganesan:Neelakantan Sundaresan:Harshal Deo	In this paper we describe our mining system which automatically mines tags from feedback text in an eCommerce scenario. It renders these tags in a visually appealing manner. Further, emoticons are attached to mined tags to add sentiment to the visual aspect.	Mining tag clouds and emoticons behind community feedback	NA:NA:NA	2018
Jing Bai:Yi Chang:Hang Cui:Zhaohui Zheng:Gordon Sun:Xin Li	Proximity of query terms in a document is an important criterion in IR. However, no investigation has been made to determine the most useful term sequences for which proximity should be considered. In this study, we test the effectiveness of using proximity of partial term sequences (n-grams) for Web search. We observe that the proximity of sequences of 3 to 5 terms is most effective for long queries, while shorter or longer sequences appear less useful. This suggests that combinations of 3 to 5 terms can best capture the intention in user queries. In addition, we also experiment with weighing the importance of query sub-sequences using query log frequencies. Our preliminary tests show promising empirical results.	Investigation of partial query proximity in web search	NA:NA:NA:NA:NA:NA	2018
Srinivas Vadrevu:Ya Zhang:Belle Tseng:Gordon Sun:Xin Li	In Web search ranking, the expected results for some queries could vary greatly depending upon location of the user. We name such queries regional sensitive queries. Identifying regional sensitivity of queries is important to meet users' needs. The objective of this work is to identify whether a user expects only regional results for a query. We present three novel features generated from search logs and build a meta query classifier to identify regional sensitive query. Experimental results show that the proposed method achieves high accuracy in identifying regional sensitive queries.	Identifying regional sensitive queries in web search	NA:NA:NA:NA:NA	2018
Zeinab Abbassi:Laks V. S. Lakshmanan	Motivated by several marketplace applications on rapidly growing online social networks, we study the problem of efficient offline matching algorithms for online exchange markets. We consider two main models of one-shot markets and exchange markets over time. For one-shot markets, we study three main variants of the problem: one-to-one exchange market problem, exchange market problem with short cycles, and probabilistic exchange market problem. We show that all the above problems are NP-hard, and propose heuristics and approximation algorithms for these problems. Experiments show that the number of items exchanged will increase when exchanges through cycles are allowed. Exploring algorithms for markets over time is an interesting direction for future work.	Offline matching approximation algorithms in exchange markets	NA:NA	2018
Shuiguang Deng:Zhaohui Wu:Jian Wu:Ying Li	We bring forward a two-phase semantic service discovery mechanism which supports both the operation matchmaking and operation-composition matchmaking. A serial of experiments on a service management framework show that the mechanism gains better performance on both discovery recall rate and precision than a traditional matchmaker.	An efficient two-phase service discovery mechanism	NA:NA:NA:NA	2018
Mingliang Zhu:Weiming Hu:Ou Wu:Xi Li:Xiaoqin Zhang	Currently most link-related applications treat all links in the same web page to be identical. One link-related application usually requires one certain property of hyperlinks but actually not all links have this property or they have this property on different levels. Based on a study of how human users judge the links, the idea of the link function classification (LFC) is introduced in this paper. The link functions reflect the purpose that links are created by web page designers and the way they are used by viewers. Links in a certain function class imply one certain relationship between the adjacent pages, and thus they can be assumed to have similar properties. An algorithm is proposed to analyze the link functions based on both vision and structure features which simulates the reaction on the links of human users. Current applications can be enhanced by LFC with a more accurate modeling of the web graph. New mining methods can be also developed by making more and stronger assumptions on links within each function class due to the purer property set they share.	User oriented link function classification	NA:NA:NA:NA:NA	2018
Jie Tang:Jing Zhang:Limin Yao:Juanzi Li	This paper addresses several key issues in extraction and mining of an academic social network: 1) extraction of a researcher social network from the existing Web; 2) integration of the publications from existing digital libraries; 3) expertise search on a given topic; and 4) association search between researchers. We developed a social network system, called ArnetMiner, based on proposed methods to the above problems. In total, 448,470 researcher profiles and 981,599 publications were extracted/integrated after the system having been in operation for two years. The paper describes the architecture and main features of the system. It also briefly presents the experimental results of the proposed methods.	Extraction and mining of an academic social network	NA:NA:NA:NA	2018
Xiaoyuan Wu:Alvaro Bolivar	As the largest online marketplace, eBay strives to promote its inventory throughout the Web via different types of online advertisement. Contextually relevant links to eBay assets on third party sites is one example of such advertisement avenues. Keyword extraction is the task at the core of any contextual advertisement system. In this paper, we explore a machine learning approach to this problem. The proposed solution uses linear and logistic regression models learnt from human labeled data, combined with document, text and eBay specific features. In addition, we propose a solution to identify the prevalent category of eBay items in order to solve the problem of keyword ambiguity.	Keyword extraction for contextual advertisement	NA:NA	2018
Mandar A. Rahurkar:Dan Roth:Thomas S. Huang	In a higher level task such as clustering of web results or word sense disambiguation, knowledge of all possible distinct concepts in which an ambiguous word can be expressed would be advantageous, for instance in determining the number of clusters in case of clustering web search results. We propose an algorithm to generate such a ranked list of distinct concepts associated with an ambiguous word. Concepts which are popular in terms of usage are ranked higher. We evaluate the coverage of the concepts inferred from our algorithm on the results retrieved by querying the ambiguous word using a major search engine and show a coverage of 85% for top 30 documents averaged over all keywords.	Which "Apple" are you talking about ?	NA:NA:NA	2018
Yunzhou Wu:Prashant Doshi	Although WS-BPEL is emerging as the prominent language for modeling executable business processes, its support for designing flexible processes is limited. An important need of many adaptive processes is for concurrent activities in the process to respect coordination constraints. These require that concurrent activities coordinate their behaviors in response to exogenous events. We show how coordination inducing constraints may be represented in WS-BPEL, and use generalized adaptation and constraint enforcement models to transform the traditional BPEL process to an adaptive one. The final outcome is an executable WS-BPEL process without extensions, able to adapt to events while respecting coordination constraints between activities.	Making BPEL flexible: adapting in the context of coordination constraints using WS-BPEL	NA:NA	2018
John F. Harney:Prashant Doshi	This paper introduces a novel method for composing Web services in the presence of external volatile information. Our approach, which we call the informed-presumptive, is compared to previous state-of-the-art approaches for Web service composition in volatile environments. We show empirically that the informed-presumptive strategy produces compositions in significantly less time than the other strategies with lesser backtracks.	Speeding up web service composition with volatile information	NA:NA	2018
Tao Zhou:Xiaolin Zheng:William Wei Song:Xiaofeng Du:Deren Chen	Generally, web services are provided with different QoS values, so they can be selected dynamically in service composition process. However, the conventional context free composition QoS model does not consider the changeability of QoS values and the context sensitive constraints during composition process. In this paper, we propose a rule based context sensitive QoS model to support the changeability of QoS values and the context sensitive constraints. By considering context in the QoS model, web service composition can be used widely and flexibly in the real world business.	Context-sensitive QoS model: a rule-based approach to web service composition	NA:NA:NA:NA:NA	2018
Jie Tang:Jing Zhang:Duo Zhang:Juanzi Li	Name ambiguity problem has been a challenging issue for a long history. In this paper, we intend to make a thorough investigation of the whole problem. Specifically, we formalize the name disambiguation problem in a unified framework. The framework can incorporate both attribute and relationship into a probabilistic model. We explore a dynamic approach for automatically estimating the person number K and employ an adaptive distance measure to estimate the distance between objects. Experimental results show that our proposed framework can significantly outperform the baseline method.	A unified framework for name disambiguation	NA:NA:NA:NA	2018
Katsuko T. Nakahira:Tetsuya Hoshino:Yoshiki Mikami	This paper proposes a method of crawling Web servers connected to the Internet without imposing a high processing load. We are using the crawler for a field survey of the digital divide, including the ability to connect to the network. Rather than employing normal Web 'page' crawling algorithm, which usually collect all pages found on the target server, we have developed 'server' crawling algorithm, which collect only minimum pages from the same server and achieved low-load and high-speed crawling of servers.	Low-load server crawler: design and evaluation	NA:NA:NA	2018
Michiaki Iwazume:Ken Kaneiwa:Koji Zettsu:Takafumi Nakanishi:Yutaka Kidawara:Yssushi Kiyoki	This paper proposes a general framework of for a system with a semantic browsing and visualization interface called Knowledge Communication, Collaboration and Creation Browser (KC3 Browser) integrates multimedia contests and web services on the grid networks, and makes a semantic mash-up called knowledge workspace (k-workspace) with various visual gadgets according to user's contexts (e.g. their interests, purpose and computational environments). KC3 Browser also achieves a link-free browsing for seamless knowledge access by generating semantic links based on an arbitrary knowledge models such as ontology and vector space models. It assists users to look down and to figure out various social and natural events from the web contents. We have implemented a prototype of KC3 Browser and tested it to an international project on risk intelligence against natural disaster.	KC3 browser: semantic mash-up and link-free browsing	NA:NA:NA:NA:NA:NA	2018
Wei Jin:Rohini Srihari:Abhishek Singh	Hypothesis generation is a crucial initial step for making scientific discoveries. This paper addresses the problem of automatically discovering interesting hypotheses from the web. Given a query containing one or two entities of interest, our algorithm automatically generates a semantic profile describing the specified entity or provides the potential connections between two entities of interest. We implemented a prototype on top of the Google search engine and the experimental results demonstrate the effectiveness of our algorithms.	Generating hypotheses from the web	NA:NA:NA	2018
Shuai Ding:Jinru He:Hao Yan:Torsten Suel	Web search engines are facing formidable performance challenges as they need to process thousands of queries per second over billions of documents. To deal with this heavy workload, current engines use massively parallel architectures of thousands of machines that require large hardware investments. We investigate new ways to build such high-performance IR systems based on Graphical Processing Units (GPUs). GPUs were originally designed to accelerate computer graphics applications through massive on-chip parallelism. Recently a number of researchers have studied how to use GPUs for other problem domains including databases and scientific computing [2,3,5], but we are not aware of previous attempts to use GPUs for large-scale web search. Our contribution here is to design a basic system architecture for GPU-based high-performance IR, and to describe how to perform highly efficient query processing within such an architecture. Preliminary experimental results based on a prototype implementation suggest that significant gains in query processing performance might be obtainable with such an approach.	Using graphics processors for high-performance IR query processing	NA:NA:NA:NA	2018
Yutaka I. Leon-Suematsu:Kikuo Yuta	Most of the faster community extraction algorithms are based on the Clauset, Newman and Moore (CNM), which is employed for networks with sizes up to 500,000 nodes. The modification proposed by Danon, Diaz and Arenas (DDA) obtains better modularity among CNM and its variations, but there is no improvement in speed as its authors expressed. In this paper, we identify some inefficiencies in the data structure employed by former algorithms. We propose a new framework for the algorithm and a modification of the DDA to make it applicable to large-scale networks. For instance, the community extraction of a network with 1 million nodes and 5 million edges was performed in about 14 minutes in contrast to former CNM that required 45 hours (192 times the former CNM, obtaining better modularity). The scalability of our improvements is shown by applying it to networks with sizes up to 10 million nodes, obtaining the best modularity and execution time compared to the former algorithms.	A framework for fast community extraction of large-scale networks	NA:NA	2018
Hongxia Jin:Vladimir Zbarsky	The fast development of the Web provides new ways for effective distribution of network-based digital goods. A digital marketplace provides a platform to enable Web users to effectively acquire, share, market and distribute digital content. However, the success of the digital marketplace business models hinges on securely managing the digital rights and usage of the digital content. For example, the digital content should be only consumable by paid users. This paper describes a Web-based system that enables the secure exchange of digital content between Web users and prevents users from illegally re-sell of the digital content. Part of our solution is based on broadcast encryption technology.	Enabling secure digital marketplace	NA:NA	2018
Masaya Eki:Tadachika Ozono:Toramatsu Shintani	We propose a method of classifying XML documents and extracting XML schema from XML by inductive inference based on constraint logic programming. The goal of this work is to type a large collection of XML approximately but efficiently. This can also process XML code written in a different schema or even code which is schema-less. Our approach is intended to achieve identification based on the syntax and semantics of the XML documents by information extraction using ontology, and to support retrieval and data management. Our approach has three steps. The first step is XML to predicates, the second step is to compare predicates and classifies structures which represent similar meanings in different structures, and the last step is predicates to rules by using ontology and to maintain XML Schema. We evaluate similarity of data type and data range by using an ontology dictionary, and XML Schema is made from results of second and last step.	Extracting XML schema from multiple implicit xml documents based on inductive reasoning	NA:NA:NA	2018
Adam Jatowt:Yukiko Kawai:Katsumi Tanaka	Recently, along with the rapid growth of the Web, the preservation efforts have also increased. As a consequence, large amounts of past Web data are stored in Web archives. This historical data can be used for better understanding of long-term page topics and characteristics. In this paper, we propose an interactive visualization system called Page History Explorer for exploring page histories. It allows for roughly portraying evolution of pages and summarizing their content over time. We use a temporal term cloud as a structure for visualizing prevailing and active terms appearing on pages in the past.	Visualizing historical content of web pages	NA:NA:NA	2018
Ming Mao:Yefei Peng:Michael Spring	Ontology mapping seeks to find semantic correspondences between similar elements of different ontologies. This paper proposes a neural network based approach to search for a global optimal solution that best satisfies ontology constraints. Experiments on OAEI benchmark tests show it dramatically improves the performance of preliminary mapping results.	Integrating the IAC neural network in ontology mapping	NA:NA:NA	2018
Manish Gupta:Amit Pathak:Soumen Chakrabarti	In entity-relation (ER) graphs (V,E), nodes V represent typed entities and edges E represent typed relations. For dynamic personalized PageRank queries, nodes are ranked by their steady-state probabilities obtained using the standard random surfer model. In this work, we propose a framework to answer top-k graph conductance queries. Our top-k ranking technique leads to a 4X speedup, and overall, our system executes queries 200-1600X faster than whole-graph PageRank. Some queries might contain hard predicates i.e. predicates that must be satisfied by the answer nodes. E.g. we may seek authoritative papers on public key cryptography, but only those written during 1997. We extend our system to handle hard predicates. Our system achieves these substantial query speedups while consuming only 10-20% of the space taken by a regular text index.	Fast algorithms for topk personalized pagerank queries	NA:NA:NA	2018
Xiaohui Yu:Yang Liu	In many text retrieval tasks, it is highly desirable to obtain a "similarity profile" of the document collection for a given query. We propose sampling-based techniques to address this need, using calibration techniques to improve the accuracy. Experimental results confirm the effectiveness of the proposed approaches.	Reasoning about similarity queries in text retrieval tasks	NA:NA	2018
Riddhiman Ghosh:Mohamed Dekhil	In this paper, we discuss challenges and provide solutions for capturing and maintaining accurate models of user profiles using semantic web technologies, by aggregating and sharing distributed fragments of user profile information spread over multiple services. Our framework for profile management allows for evolvable, extensible and expressive user profiles. We have implemented a prototype, targeting the retail domain, on the HP Labs Retail Store Assistant.	Mashups for semantic user profiles	NA:NA	2018
Yehia Taher:Marie-Christine Fauvet:Marlon Dumas:Djamal Benslimane	Web service may be unable to interact with each other because of incompatibilities between their interfaces. In this paper, we present an event driven approach which aims at adapting messages exchanged during service interactions. The proposed framework relies on the Complex Event Processing (CEP) technology, which provides an environment for the development of applications that need to continuously process, analyse and respond to event streams. Our main contribution is a system that enables developers to design and implement CEP-based adapters. These latter are deployed in a CEP engine which is responsible for continuously receiving messages and processing them according to rules implemented by the adapters. Resulting transformed messages are thus forwarded to their original service recipient.	Using CEP technology to adapt messages exchanged by web services	NA:NA:NA:NA	2018
Haiqiang Chen:Xueqi Cheng:Yue Liu	Finding the core members of a virtual community is an important problem in community analysis. Here we presented an simulated annealing algorithm to solve this problem by optimizing the user interests concentration ratio in user groups. As an example, we test this algorithm on a virtual community site and evaluate its results using human "gold standard" method.	Finding core members in virtual communities	NA:NA:NA	2018
Qiudan Li:Chunheng Wang:Guanggang Geng	With the rapid growth of wireless technologies and mobile devices, there is a great demand for personalized services in m-commerce. Collaborative filtering (CF) is one of successful techniques to produce personalized recommendations for users. This paper proposes a novel approach to improve CF algorithms, where the contextual information of a user and the multicriteria ratings of an item are considered besides the typical information on users and items. The multilinear singular value decomposition (MSVD) technique is utilized to explore both explicit relations and implicit relations among user, item and criterion. We implement the approach in an existing m-commerce platform, and encouraging experimental results demonstrate its effectiveness.	Improving personalized services in mobile commerce by a novel multicriteria rating approach	NA:NA:NA	2018
Keiji Yanai	We propose a new method to select relevant images to the given keywords from images gathered from theWeb based on the Probabilistic Latent Semantic Analysis (PLSA) model which is a probabilistic latent topic model originally proposed for text document analysis. The experimental results shows that the results by the proposed method is almost equivalent to or outperforms the results by existing methods. In addition, it is proved that our method can select more various images compared to the existing SVM-based methods.	Automatic web image selection with a probabilistic latent topic model	NA	2018
Nilanjan Banerjee:Dipanjan Chakraborty:Koustuv Dasgupta:Sumit Mittal:Seema Nagar	This paper presents R-U-In? - a social networking application that leverages Web 2.0 and IMS-based Converged Networks technologies to create a rich next-generation service. R-U-In? allows a user to search (in real-time) and solicit participation of like-minded partners for an activity of mutual interest (e.g. a rock concert, a soccer game, or a movie). It is an example of a situational mashup application that exploits content and capabilities of a Telecom operator, blended with Web 2.0 technologies, to provide an enhanced, value-added service experience.	R-U-in?: doing what you like, with people whom you like	NA:NA:NA:NA:NA	2018
Martin Szummer:Nick Craswell	A bipartite query-URL graph, where an edge indicates that a document was clicked for a query, is a useful construct for finding groups of related queries and URLs. Here we use this behavior graph for classification. We choose a click graph sampled from two weeks of image search activity, and the task of "adult" filtering: identifying content in the graph that is inappropriate for minors. We show how to perform classification using random walks on this graph, and two methods for estimating classifier parameters.	Behavioral classification on the click graph	NA:NA	2018
Yunhong Zhou:Deeparnab Chakrabarty:Rajan Lukose	We consider the budget-constrained bidding optimization problem for sponsored search auctions, and model it as an online (multiple-choice) knapsack problem. We design both deterministic and randomized algorithms for the online (multiple-choice) knapsack problems achieving a provably optimal competitive ratio. This translates back to fully automatic bidding strategies maximizing either profit or revenue for the budget-constrained advertiser. Our bidding strategy for revenue maximization is oblivious (i.e., without knowledge) of other bidders' prices and/or click-through-rates for those positions. We evaluate our bidding algorithms using both synthetic data and real bidding data gathered manually, and also discuss a sniping heuristic that strictly improves bidding performance. With sniping and parameter tuning enabled, our bidding algorithms can achieve a performance ratio above 90% against the optimum by the omniscient bidder.	Budget constrained bidding in keyword auctions and online knapsack problems	NA:NA:NA	2018
Zhi-li Wu:Chi-Wa Cheng:Chun-hung Li	Social media such as Web forum often have dense interactions between user and content where network models are often appropriate for analysis. Joint non-negative matrix factorization model of participation and content data can be viewed as a bipartite graph model between users and media and is proposed for analysis social media. The factorizations allow simultaneous automatic discovery of leaders and sub-communities in the Web forum as well as the core latent topics in the forum. Results on topic detection of Web forums and cluster analysis show that social features are highly effective for forum analysis.	Social and semantics analysis via non-negative matrix factorization	NA:NA:NA	2018
Yu Wang:Bingxing Fang:Xueqi Cheng:Li Guo:Hongbo Xu	Most template detection methods process web pages in batches that a newly crawled page can not be processed until enough pages have been collected. This results in large storage consumption and a huge delay of data refreshing. In this paper, we present an incremental framework to detect templates in which a page is processed as soon as it has been crawled. In this framework, we don't need to cache any web page. Experiments show that our framework consumes less than 7% storage than traditional methods. And also the speed of data refreshing is accelerated because of the incremental manner.	Incremental web page template detection	NA:NA:NA:NA:NA	2018
Gaogang XIE:Tingting He:Guangxing Zhang	Rogue Access Points (RAPs) pose serious security threats to local networks. An analytic model of prior probability distribution of Segmental TCP Jitter (STJ) is deduced from the mechanism of IEEE 802.11 MAC Distributed Coordinated Function (DCF) and used to differentiate the types of wire and WLAN connections which is the crucial step for RAPs detecting. STJ as the detecting metric can reflect more the characteristic of 802.11 MAC than ACK-Pair since it can eliminate the delay caused by packet transmission. The experiment on an operated network shows the average detection ratio of the algorithm with STJ is more than 92.8% and the average detection time is less than 1s with improvement of 20% and 60% over the detecting approach of ACK-Pair respectively. Farther more no WLAN training trace is needed in the detecting algorithm.	Rogue access point detection using segmental TCP jitter	NA:NA:NA	2018
Renato Iannella	The sharing and control of information on the Web needs to be balanced to provide the web community with the best experience and outcomes. The Policy-Aware Web is an emerging direction that may hold the key in getting this balance right by allowing future policy languages to maintain this harmonization. This panel will discuss, highlight, and debate the challenges in moving towards the Policy-Aware Web and the paths it could lead. The target audience includes all web stakeholders as most policies, such as privacy and rights, are intrinsic to all web users, information, services and content.	Towards the policy-aware web: the real web 3.0?	NA	2018
Greg Aaron:Katharine A. Bostik:Rod Rasmussen:Edmon Chung	Web site operators, Internet users, and online service providers are besieged by a growing array of abuses and threats. Spam leads users to online scams and phishing Web pages, which cyber-criminals implant on innocent sites to fool the unwary into revealing their financial data and passwords. Other criminals use Web sites to spread malware, which can steal personal data or take over users' computers into a botnet, which can be used to send spam or mount cyber-attacks against Web sites and other Internet services. Together, these abuses undermine user trust, hamper e-commerce, and cost the Internet community huge losses in money, service and support costs, and time. What should Web site operators and online service providers do to protect themselves and their users? What are Internet companies, organizations, and law enforcement doing (and not doing) to combat these problems? And how can the international Internet community work together on these problems? The panel brings together representatives from the chain of organizations that respond to Internet abuse problems, and promises a lively, compelling, and relevant discussion.	Protecting the web: phishing, malware, and other security threats	NA:NA:NA:NA	2018
Ajita John:Lada Adamic:Marc Davis:Frank Nack:David A. Shamma:Doree D. Seligmann	Blogs, wikis, tagging, podcasts, and social networking websites such as MySpace, Facebook, Flickr and YouTube have radically changed user interactions on the World Wide Web from a static, one-way, consumption model to a dynamic, multi-way, participation model. Broad user power and flexibility have changed how people engage in and experience their interconnections, interests, and collaborations. Online social interactions will evolve in the next decade to address the growing needs of its user community and make entries into many aspects of our lives. This evolution may very well be among the most exciting ones of our times where the individual and collective power of people to contribute and share content, experiences, ideas, expertise etc. may be even more enhanced than it is today. The enhancement may be shaped through a better understanding of user needs and behavior and it may be enabled through the seamless convergence of multi-modal technologies, new applications, new domains, data mining and better navigational and search capabilities. Some of these changes will also permeate into the workplace and change the way we work. This panel will discuss how online social interactions may evolve in the next decade and what impact it may have on diverse dimensions in our world.	The future of online social interactions: what to expect in 2020	NA:NA:NA:NA:NA:NA	2018
Sheetal K. Agarwal:Arun Kumar:Sougata Mukherjea:Amit A. Nanavati:Nitendra Rajput	Information Retrieval on the WWW is important because it is hard to find what one is looking for. There is a plethora of information available, and searching relevant information is a challenge. In the case of developing regions, we have the opposite problem: 1) Information availability of global markets is scarce. Most of the consumers and producers (of information as well as goods) are relegated to local markets in geographical vicinity. In order to reach wider markets, it is important for this local information to reach wider audiences. (Local information for global consumption LIG model). (2) At the same time, locally relevant information, such as delays in bus/train timings, mobile medical van schedule changes, electricity outage timings, is not easily available either. (Local information for local consumption LIL model). We introduce the term Information Uptrieval to address the reverse problem of acquiring, assimilating, aggregating and uploading global and local information that is relevant for developing regions to a platform that improves the reach of the information. While the WWW is an obvious example of one such platform, given the low internet penetration in such regions, we need to explore effective alternatives. Several innovative, but disconnected approaches have been attempted to address the information uptrieval problem, ranging from the use of DVDs (eSagu, http://www.esagu.in/esagu) through the use of wireless stations on motorcycles (First Mile Solutions, http://www.firstmilesolutions.com). Many of these have met with reasonable success in their pilot deployments.	Information "uptrieval": exploring models for content assimilation and aggregation for developing regions	NA:NA:NA:NA:NA	2018
Edward Chang:Ken Ong:Susanne Boll:Wei-Ying Ma	Rich media data, such as video, imagery, music, and gaming, do no longer play just a supporting role on the World Wide Web to text data. Thanks to Web 2.0, rich media is the primary content on sites such as Flickr, PicasaWeb, YouTube, and QQ. Because of massive user generated content, the volume of rich media being transmitted on the Internet has surpassed that of text. It is vital to properly manage these data to ensure efficient bandwidth utilization, to support effective indexing and search, and to safeguard copyrights (just to name a few). This panel invites both researchers and practitioners to discuss the challenges of Web-scale media-data management. In particular, the panelists will address issues such as leveraging Rich Media and Web 2.0, indexing, search, and scalability.	Rich media and web 2.0	NA:NA:NA:NA	2018
Susanne Boll:Christopher Jones:Eric Kansa:Puneet Kishor:Mor Naaman:Ross Purves:Arno Scharl:Erik Wilde	The World Wide Web has become the world's largest networked information resource, but references to geographical locations remain unstructured and typically implicit in nature. This lack of explicit spatial knowledge within the Web makes it difficult to service user needs for location-specific information. At present, spatial knowledge is hidden in many small information fragments such as addresses on Web pages, annotated photos with GPS co-ordinates, geographic mapping applications, and geotags in user-generated content. Several emerging formats that primarily or secondarily include location metadata, like GeoRSS, KML, and microformats, aim to improve this state of affairs. However, the question remains how to extract, index, mine, find, view, mashup, and exploit Web content using its location semantics. This work-shop brings together researchers from academia and industry labs to discuss and present the latest results and trends in all facets of the relationships between location concepts and Web information.	Location and the web (LocWeb 2008)	NA:NA:NA:NA:NA:NA:NA:NA	2018
Quan Z. Sheng:Ullas Nambiar:Amit P. Sheth:Biplav Srivastava:Zakaria Maamar:Said Elnaffar	This write-up provides a summary of the International Workshop on Context enabled Source and Service Selection, Integration and Adaptation (CSSSIA 2008), organized in conjunction with WWW 2008, at Beijing, China on April 22nd 2008. We outline the motivation for organizing the workshop, briefly describe the organizational details and program of the workshop, and summarize each of the papers accepted by the workshop. More information about the workshop can be found at http://www.cs.adelaide.edu.au/~csssia08/.	WS3: international workshop on context-enabled source and service selection, integration and adaptation (CSSSIA 2008)	NA:NA:NA:NA:NA:NA	2018
Christian Bizer:Tom Heath:Kingsley Idehen:Tim Berners-Lee	The Web is increasingly understood as a global information space consisting not just of linked documents, but also of Linked Data. More than just a vision, the resulting Web of Data has been brought into being by the maturing of the Semantic Web technology stack, and by the publication of an increasing number of datasets according to the principles of Linked Data. The Linked Data on the Web (LDOW2008) workshop brings together researchers and practitioners working on all aspects of Linked Data. The workshop provides a forum to present the state of the art in the field and to discuss ongoing and future research challenges. In this workshop summary we will outline the technical context in which Linked Data is situated, describe developments in the past year through initiatives such as the Linking Open Data community project, and look ahead to the workshop itself.	Linked data on the web (LDOW2008)	NA:NA:NA:NA	2018
Carlos Castillo:Kumar Chellapilla:Dennis Fetterly	Adversarial IR in general, and search engine spam, in particular, are engaging research topics with a real-world impact for Web users, advertisers and publishers. The AIRWeb workshop will bring researchers and practitioners in these areas together, to present and discuss state-of-the-art techniques as well as real-world experiences. Given the continued growth in search engine spam creation and detection efforts, we expect interest in this AIRWeb to surpass that of the previous three editions of the workshop (held jointly with WWW 2005, SIGIR 2006, and WWW 2007 respectively).	Fourth international workshop on adversarial information retrieval on the web (AIRWeb 2008)	NA:NA:NA	2018
Ewa Dominowska:Vanja Josifovski	Online advertising is a rapidly growing, multi-billion dollar industry. It has become a significant element of the Web browsing experience. Online advertising providers use sophisticated ad targeting and ranking algorithms with the dual aim of maximizing revenue while providing a superior user experience. As a result, advertising optimization is a very complex research problem, since it combines relevance with user interaction models, advertiser valuations, and commercial constraints. Online advertising integrates a number of core research areas: machine learning, data mining, search, auction theory, and user modeling. This workshop is intended to serve as an open forum for discussion of new ideas and current research in the field of online advertising. We expect that the workshop will promote a community of researchers interested in this area and yield future collaboration and exchanges. The research included should address problems faced by advertisers, end-users, advertising platforms, and the market.	First workshop on targeting and ranking for online advertising	NA:NA	2018
Rittwik Jana:Daniel Appelquist:Galit Zadok:Bin Wei	2008 will be the "Mobile Eureka! Moment". With the Olympics just round the corner and the mobile market experiencing a phenomenal growth, we believe that WWW 2008 will truly be the climax and focal point in the midst of this mobile revolution. Mobile Web Initiative spearheaded by W3C is making a strong stand on how to realize the vision of pervasive mobile computing. Three screen services (TV, PC, Phone) are demanding new architectures to be developed. Furthermore, the need to go beyond technology, demands an embrace of the human-centric aspects of mobile computing. The objective of this workshop is to provide a single forum for researchers, sociologists, and technologists to discuss the state-of-the-art, present their contributions, and set future directions in personalized applications for mobile users with a focus in rich social media. The transition from the wired Internet to the mobile web has generated a wide array of novel services bringing in billions of dollars in revenue in the recent years. So there are a few natural questions that one may ask. "What is "the" next big thing in the mobile web as far as applications and services are concerned?", "How will operators provide these services efficiently with this next wave of applications?" and "What are the emerging business models that drive the next generation of services?". In a nutshell, to answer some of the questions above, we believe that in general there is a convergence in the fixed and mobile worlds by means of standards like IP Multimedia Subsystems (IMS) [1]. This convergence allows enterprise and consumer applications to be blended via a unified infrastructure. This unification results in a number of advantages like a) application availability from any access methods, b) enhanced user experience for combined/blended services (presence, messaging, address book etc.), c) centralized user profiles shared between applications and common provisioning, d) flexible charging and billing of multimedia services in particular, e) scalable deployments with guaranteed Quality of Services and last but not least f) secure solutions through built in identity management, authentication and authorization procedures. There are a variety of novel applications that are starting to emerge. A general shift from static content to a more dynamic nature has been observed in recent years. In particular, multimedia and social networking are the foundations of emerging applications. One such example is the delivery of IPTV to the third screen (mobile, PDA). Social networking/messaging (applications like Twitter) and peer-to-peer applications are also on the verge of breakthroughs in the mobile landscape. Interestingly, a lot of the momentum has been reinvigorated with the introduction of smart phones like Apple's iPhone [2]. Devices are becoming more powerful and standards are being written to take advantage of these smart features. Finally, we believe that the mobile web is at the forefront of technology and this workshop will discuss many of these technical and business challenges that are on the wish list of everyday practitioners.	WS7 - MobEA VI: personal rich social media	NA:NA:NA:NA	2018
Huajun Chen:Kei Cheung:Michel Dumontier:Eric Prudhommeaux:Alan Ruttenberg:Susie Stephens:Yimin Wang	The Semantic Web for Health Care and Life Sciences Workshop will be held in Beijing, China, on April 22, 2008. The goal of the workshop is to foster the development and advancement in the use of Semantic Web technologies to facilitate collaboration, research and development, and innovation adoption in the domains of Health Care and Life Sciences, We also encourage the participation of all research communities in this event, with enhanced participation from Asia due to the location of the event. The workshop consists of two invited keynote talks, eight peer-reviewed presentations, and one panel discussion.	Report on semantic web for health care and life sciences workshop	NA:NA:NA:NA:NA:NA:NA	2018
Liu Wenyin:Qing Li:Xuedong Huang	A half-day single track workshop is designed to gather academic researchers and industrial practitioners at to share ideas and knowledge of know-how, and to discuss all relevant issues including the business models, enabling technologies, and killer applications, of Web-based question answering (QA), especially, the user-interactive QA services and applications. The workshop program consists of two sessions, one for academic papers, and the other for industrial practice papers. Each session consists of a leading talk, followed by three short presentations. Sufficient time is allocated for brainstorming discussions in each session.	International workshop on question answering on the web (QAWeb2008)	NA:NA:NA	2018
Hiroshi Nakagawa:Kentaro Torisawa:Marasu Kitsuregawa	The amount of information available on the Web has increased rapidly, reaching levels that few would ever have imagined possible. We live in what could be called the "information-explosion era," and this situation poses new problems for computer scientists. Users demand useful and reliable information from the Web in the shortest time possible, but the obstacles to fulfilling this demand are many including language barriers and the so-called "long tail." Even worse, users may provide only vague specifications of the information that they actually want, so that a more concrete specification must somehow be inferred by Web access tools. Natural language processing (NLP) is one of the key technologies for solving the above Web usability problems. Almost all the Web page provide with the essential information in the form of natural language texts, and the amount of these text information is huge. In order to offer solutions to these problems we must perform searching and extracting information from the Web texts using NLP technologies. The aim of this workshop: NLP Challenges in the Information Explosion Era (NLPIX 2008) is to bring researchers and practitioners together in order to discuss our most pressing needs with respect to accessing information on the Web, and to discuss new ideas in NLP technologies that might offer viable solutions for those issues.	WWW 2008 workshop: NLPIX2008 summary	NA:NA:NA	2018
Peter Dolog:Markus Kroetzsch:Sebastian Schaffert:Denny Vrandecic	This paper provides an overview on the synergies between social web and knowledge managemen, topics, program committee members as well as summary of accepted papers for the SWKM2008 workshop.	Workshop on social web and knowledge management (SWKM2008)	NA:NA:NA:NA	2018
