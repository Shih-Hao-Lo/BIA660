Bing-Yu Chen:Shih-Chiang Dai:Shuen-Huei Guan:Tomoyuki Nishita	In this extended abstract, we present a system that allows the user to animate character images in 3D space by applying an existed 3D character model with motion data. The character model with skeleton rigged is used as a template model to fit the silhouette of the character image. After assigning some corresponding points between the character image and template model, the system then fits the model to the image and transfer the colors and patterns of the image to the model as the textures. Finally, the user can apply any motion data to animate the fitted 3D character model in 3D space.	Animating character images in 3D space	NA:NA:NA:NA	2009
Fu-Chung Huang:Yu-Mei Chen:Tse-Hsien Wang:Bing-Yu Chen:Shuen-Huei Guan	Speech animation is traditionally considered as important but tedious work for most applications, because the muscles on the face are complex and dynamically interacting. In this paper, we introduce a framework for synthesizing a 3D lip-sync speech animation by a given speech sequence and its corresponding texts. We first identify the representative key-lip-shapes from a training video that are important for blend-shapes and guiding the artist to create corresponding 3D key-faces (lips). The training faces in the video are then cross-mapped to the crafted key-faces to construct the Dominated Animeme Models (DAM) for each kind of phoneme. Considering the coarticulation effects in animation control signals from the cross-mapped training faces, the DAM computes two functions: polynomial-fitted animeme shape functions and corresponding dominance weighting functions. Finally, given a novel speech sequence and its corresponding texts, a lip-sync speech animation can be synthesized in a short time with the DAM.	Animating lip-sync speech faces by dominated animeme models	NA:NA:NA:NA:NA	2009
Nozomi Kugimoto:Rui Miyazono:Kosuke Omori:Takeshi Fujimura:Shinichi Furuya:Haruhiro Katayose:Hiroyoshi Miwa:Noriko Nagata	Technologies recreating piano performance in the form of CG animation are eagerly anticipated by people working in various fields, such as content production, music education, etc. Nonetheless, much of the past research has dealt with the mechanical finger movements in piano practice support systems and performance support GUIs, etc. and there has been little research recreating the reality of finger movements. We are promoting research into the analysis and CG expression of realistic and natural piano fingering. This paper describes the following aspects of this research program: (i) measurement of piano fingering using motion capture technology, (ii) generation of a CG animation of fingering using offline/realtime rendering, and (iii) automatic generation of fingering using optimized algorithms. And finally we will introduce examples in which the fingering data created in (i) is used in TV animation.	CG animation for piano performance	NA:NA:NA:NA:NA:NA:NA:NA	2009
Shinsuke Nakamura:Masashi Shiraishi:Shigeo Morishima:Mayu Okumura:Yasushi Makihara:Yasushi Yagi	Characteristics of human motion, such as walking, running or jumping vary from person to person. Differences in human motion enable people to identify oneself or a friend. However, it is challenging to generate animation where individual characters exhibit characteristic motion using computer graphics. Our goal is to construct a system that synthesizes characteristic gait animation automatically. As a result, when crowd animation is generated for instance, the motion with the variation can be made using our system. In our system, we first acquire a silhouette image as input data using a video camera. Second, we extract gait feature from single view silhouette. Finally we automatically synthesize 3D gait animation using the method blending a small number of motion data [KOVAR, L et al 2003].This blending weight is estimated using the gait feature automatically.	Characteristic gait animation synthesis from single view silhouette	NA:NA:NA:NA:NA:NA	2009
Takeshi Miura:Kazutaka Mitobe:Takaaki Kaiga:Takashi Yukawa:Toshiyuki Taniguchi:Hideo Tamamoto	In the field of dance motion analysis, development of the technique for extraction of characteristic postures peculiar to each dance number is needed [Hachimura 2006]; extracted postures can be used as the indexes for the retrieval of motion data. In this study, the authors suggest a novel method for extraction of characteristic postures from the motion data of a dance number; the information of uniqueness of the dance number given by the statistical analysis of a database including motion data of plural dance numbers is used in the extraction process.	Extraction of characteristic postures in a dance by statistical analysis of a database of motion data	NA:NA:NA:NA:NA:NA	2009
Yohei Shimotori:Shiori Sugimoto:Shigeo Morishima	Shadows in 2D Anime play a significant role for expressing symbolic visual effects such as the character's position and shape. However, animators frequently can't draw detailed shadows according to their intentions because of time constraints and a lack of skilled animators. For solving this problem, we have developed a system that can generate shadows automatically. Our system provides simple shadows and shadows on the water by applying Simplification Filter and Water Mapping Filter. Also, our system only requires inputs of the 2D character animation layers generally composed in the Anime industry. Consequently, our system enables animators to intuitively produce Anime-like shadow animation in a short time.	Directable anime-like shadow based on water mapping filter	NA:NA:NA	2009
Ryo Takamizawa:Takanori Suzuki:Hiroyuki Kubo:Akinobu Maejima:Shigeo Morishima	MoCap-based facial expression synthesis techniques have been applied to provide CG character with expressive and accurate facial expressions [Deng et al. 2006: Lau et al. 2007]. The representative performance of these techniques depends on the variety of captured facial expressions. It is also difficult to guess what expressions are needed to synthesize expressive face before capture. Therefore, much MoCap data are required to construct a subspace employing dimensional compression techniques, and then the space enables us to synthesize expressions with linear-combination of basis vectors of the space. However, it is hard work to take much facial MoCap data to obtain expressive result.	Expressive facial subspace construction from key face selection	NA:NA:NA:NA:NA	2009
James D. Edge:Adrian Hilton:Philip Jackson	We describe a method for the synthesis of visual speech movements using a hybrid unit selection/model-based approach. Speech lip movements are captured using a 3D stereo face capture system and split up into phonetic units. A dynamic parameterisation of this data is constructed which maintains the relationship between lip shapes and velocities; within this parameterisation a model of how lips move is built and is used in the animation of visual speech movements from speech audio input. The mapping from audio parameters to lip movements is disambiguated by selecting only the most similar stored phonetic units to the target utterance during synthesis. By combining properties of model-based synthesis (e.g., HMMs, neural nets) with unit selection we improve the quality of our speech synthesis.	Model-based synthesis of visual speech movements from 3D video	NA:NA:NA	2009
Hiroto Yarimizu:Yasushi Ishibashi:Hiroyuki Kubo:Akinobu Maejima:Shigeo Morishima	Muscle-based facial animation [Lee et al. 1995] is one of the best approaches to realize facial expressions of characters. However, this approach does not consider the personal variation in facial tissue model such as skin thickness. So personal character in emotional expression can not be reflected in this model.	Muscle-based facial animation considering fat layer structure captured by MRI	NA:NA:NA:NA:NA	2009
Takashi Tokizaki:Yuuichi Tazaki:Hironori Mitake:Shoichi Hasegawa	Interactive applications such as Video Games require characters, which generate motions corresponding to user's interaction. Motion capture is an effective technique to reproduce realistic motion. However, to produce a motion which is appropriate to the operation of the user, a lot of motions must be prepared and one of the motions which is suitable for the user's operation must be selected and played. Because the user's operation changes the motion trajectory, unexpected contact to objects may happen. The amount of change on a trajectory depends on not only the trajectory of motion but also internal tensions of skeletal muscles - co-contraction level, when a person put one's hand down on a table or collides with an object. [Hogan 1984] proposed that reaching motion of human is supposed to be generated by spring damper characteristics of muscles dragging to the virtual trajectory. Human controls not only trajectories of motions but also spring-damper characteristics of muscles by changing co-contraction levels. Realistic character motions contacting to objects can be generated easily with virtual trajectory tracking control which is integrated to physics engines for character motions.	Pliant motion: integration of virtual trajectory control into LCP based physics engines	NA:NA:NA:NA	2009
D. Kravtsov:O. Fryazinov:V. Adzhiev:A. Pasko:P. Comninos	The modern world of computer graphics is mostly dominated by polygonal models. Due to their scalability and ease of rendering such models have various applications in a wide range of fields. Unfortunately some shape modelling and animation problems can hardly be overcome using polygonal models only. For example, dramatic changes of the shape (involving change of topology) or metamorphosis between different shapes can not be performed easily. The Function Representation (FRep) [Pasko et al. 1995] allows us to overcome some of the problems and simplify the process of the major model modification. Our system is based on a hybrid modelling concept, where polygonal and FRep models are combined together and can be evaluated in near-real or real time. It allows us to: • produce animations involving dramatic changes of the shape (e.g. metamorphosis, viscoelastic behaviour, character modifications etc) in short times (Fig. 1) • interactively create complex shapes with changing topology (Fig. 2) and specified level of detail (LOD) • integrate existing animated polygonal models and FRep models within a single model	Polygonal-functional hybrids for computer animation and games	NA:NA:NA:NA:NA	2009
Jianfeng Xu:Haruhisa Kato:Akio Yoneyama	This poster presents a motion retrieval algorithm, which searches the motions in the same category as a query's (known as logically similar motions) in a motion capture database. The challenge is that logically similar motions may not be numerically similar due to the motion variations [Müller et al. 2005]. In this poster, we propose a novel short-term feature that extracts both symbolized representation and continuous features from joint velocities in a motion clip, which is employed to effectively retrieve logically similar motions to the query. Although symbolized representation of human motion has been studied [Müller et al. 2005], our approach is different in that we consider temporal correlation instead of Müller's spatial relationship. Moreover, not only symbolized representation (dynamic pattern) but also continuous features (average speed) are extracted in our short-term feature. Furthermore, our method is more friendly to novices as it requires no prior knowledge to determine features. Our experiments demonstrate that our algorithm greatly improves the performance compared to two conventional methods.	Retrieval of motion capture data based on short-term feature extraction	NA:NA:NA	2009
Katsutoki Hamana:Hiroshi Mori:Atsushi Nakano:Junichi Hoshino	In recent years, many entertainment systems have relied on the progress of interaction technology to create characters that act autonomously. To show these lifelike characters, it is important for them to perform various actions, such as daily actions, reflex actions that require reacting to input from a user, perceiving actions where the character perceives an object and reacts to it, and actions based on personalities or feelings. This results in the problem of complex action planning. A character has to carry out the actions listed, keep schedules and maintain a personality, and react flexibly to user interaction, while still maintaining story flow. In this paper, we propose the Story Engine, which can execute various actions in multiple characters.	Story engine for interactive characters	NA:NA:NA:NA	2009
Evan Tice:Tim Tregubov:Kate Schnippering:Yoon-Ki Park:Ray diCiaccio:Max Friedman:Jennifer Huang:Justin Slick:Giulia Siccardo:Jessica Glago:Stephanie Trudeau:Daniel Gobaud:Daniel Garcia:Craig Slagel:Lorie Loeb	With glaciers melting, sea levels rising and natural disasters---such as hurricanes and cyclones---intensifying, climate change is a growing concern. While innovations in renewable energy are critical, research shows that changing energy use behavior has become increasingly important in the fight against global warming. GreenLite Dartmouth focuses on changing behavior by making energy conservation a priority for students by creating both an intellectual and emotional connection between daily actions and their adverse effects on the environment. We combine computer graphics, art, engineering, sociology, environmental science, systems-thinking and behavioral psychology to turn real-time energy use data into a meaningful interactive display. GreenLite employs innovative methods for displaying complex data using interactivity, storytelling, animation, competition and goal-setting. Appealing animated information-display and "mood" algorithms put data into context to make it meaningful. We incorporate a system of digital energy meters, a custom database, computational analysis, 2D and 3D animations, interactive design and a game-engine to spur behavior change and, hopefully, reverse the course of climate change.	GreenLite Dartmouth: unplug or the polar bear gets it	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2009
Saba Hashem Kawas	With today's increased interest in advanced digital networking and social online communities, many higher educational institutes have been exploring online three-dimensional virtual environments as a new medium for distance learning. These technologies provide multiple features for online interaction and collaboration, a visual cyberspace for text and voice communication, the ability to manipulate multimedia in real time with a group of individuals, and the opportunity to link users to Web sites.	H-link 3D: hyper-learning interface and navigational toolkit in 3D virtual worlds experimental interface design for cobalt, a Croquet metaverse	NA	2009
Akira Nakayasu:Kiyoshi Tomimatsu	Himawari is a sunflower robot composed of mechanical parts and electronic parts, such as servo motors, LEDs and shape-memory alloy actuators, that reacts, slowly and fluidly, facing and communicating to humans in front of it.	Himawari: a plant robot	NA:NA	2009
Benjamin Raynal:Xavier Gouchet:Venceslas Biri:Vincent Nozick	Designing a program that is not a tool of artistic creation, but a creator itself have been a real challenge for both digital artists and researchers. The most famous program of artistic creation is AARON [Cohen], which is in continual development since 1973 by its creator, Harold Cohen. Unfortunately, AARON cannot learn new styles or imagery by its own, each new capability must be hand-coded by Harold Cohen. Roxame [Berger], another artistic creation program created in 2001 by Pierre Berger, is based on artificial intelligence, and have its own style, emerging from both the artistic preferences of the user, and a stochastic process. This style can evolve and is refined at each work.	IArtist: a self learning computer artist	NA:NA:NA:NA	2009
Takuji Narumi:Tomohiro Akagawa:Young Ah Seong:Michitaka Hirose	To change the spatial structures of our living spaces, we usually take an architectural approach. However once such structures have been constructed, re-configuring them incurs substantial costs. Reserving the flexibility to change spatial design is essential for the effective use of spaces. Our research aims to make spatial design flexible. New characteristics are added to an existing space using information technology, so the relationship between the space and people within changes.	Thermotaxis	NA:NA:NA:NA	2009
Max Grosse:Gordon Wetzstein:Oliver Bimber:Anselm Grundhöfer	With adaptive coded aperture projection, we present solutions for taking projectors to the next level. By placing a programmable liquid crystal array at a projectors aperture plane we show how the depth of field (DOF) of a projection can be greatly enhanced. This allows focussed imagery to be shown on complex screens with varying distances to the projectors focal plane, such as projection domes as in planetariums or cylindrical canvases as in IMAX theaters. We demonstrate that adaptive apertures outperform previous methods of projector defocus compensation for objective lenses with static apertures. In addition, our adaptive apertures can perform the type of temporal contrast enhancement employed by common auto-iris projection lenses, and also produce high-quality depixelated images. The latter is beneficial for close-view displays with limited resolution, such as rear-projected TV sets.	Adaptive coded aperture projection	NA:NA:NA:NA	2009
Martha Carrer:Cruz Gabriel	The objective of this paper/presentation is to describe the potentialities of Mobile Tagging as a tool for increasing and spreading the effects of Mixed Realities. In this sense, we will start introducing the main concepts and some examples of Mixed Realities followed by the concepts and examples of Mobile Tagging, showing that they are connected and benefit each other.	Mobile tagging and mixed realities	NA:NA	2009
Norihiro Nakamura:Yoshiyuki Kokojima:Yasunobu Yamauchi	Resolution-independent rendering is important for many applications such as text rendering and rendering vector objects. This theme has attracted interest in recent years owing to the growing popularity of Flash and SVG-based applications [Loop and Blinn 2005]. We had previously presented a fast rendering method using a stencil buffer for deformable vector objects [Kokojima et al.]. One of the advantages of this method is that retriangulation is unnecessary when vector objects deform interactively. However, [Kokojima et al.] only deal with rendering vector objects on a flat surface.	Rendering of vector objects on curved surface using pivot triangle primitives	NA:NA:NA	2009
Takehito Teraguchi:Hiromasa Yamashita:Ken Masamune:Takeyoshi Dohi:Hongen Liao	Three-dimensional (3-D) displays have got a lot of attention because they have a much higher sense of realism and are more intuitive than 2-D displays. In particular, autostereoscopic displays are suitable for everyday use because they can be observed from an arbitrary viewpoint without supplementary glasses or tracking devices. Integral Videography (IV) is one of the methods for autostereoscopic animated images that extends Integral Photography (IP) to animation. IP/IV uses a combination of a lens array and a number of calculated elemental images with different perspectives. In particular, IV with a depth of several meters can be applied in many areas. However, most IV reports have an image depth of only several centimeters. Only a small deviation of a lens from its designed position would result in several degrees of deviation of the light ray from the back of the lens. We have developed the static autostereoscopic image by projecting the light sources from an object onto a photographic film through the lens-array [Liao et al. 2005]. In this study, we obtained animated IV of 1 m image depth with less distortion using our method to correct IV images. The method is technically unique.	Three-dimensional auto-stereoscopic animated image with a long viewing distance using high-precision image correction	NA:NA:NA:NA:NA	2009
Ilya Rosenberg:Ken Perlin:Charles Hendee:Alex Grau:Nadim Awad	Multi-touch input has been an active area of research for over two decades but has always suffered from the absence of an easily available high quality touch input device. For this reason, exciting user interfaces developed in the lab have appeared on CNN, but not on everyone's desk, computer screens, table-tops, walls and floors. What has been needed - and lacking - is a better mousetrap; an inexpensive, flexible and sensitive touch imaging technology.	The UnMousePad: the future of touch sensing	NA:NA:NA:NA:NA	2009
Wei-Chung Cheng:Jih-Fon Huang	This work demonstrates a novel channel for a smart display to interact with its user for enhancing the viewing experience. By using a wearable electro-oculography (EOG) circuit, the saccadic eye movements can be detected so that the user's viewing mode can be determined. Different gamut settings are used in the "fixation mode" versus "saccade mode," such that the fast eye movement induced artifacts can be suppressed. Furthermore, by analyzing the saccade patterns, we can determine the user is in the "image viewing" mode or "text reading" mode. Then different brightness, contrast, saturation settings can be assigned accordingly and automatically to improve the user's comfort level.	A saccade-contingent display for suppressing color breakup	NA:NA	2009
Jussi Huhtala:Ari-Heikki Sarjanoja:Jani Mäntyjärvi:Minna Isomursu:Jonna Häkkilä	Mobile devices have limitations compared to PCs due to their inferior computing power and small screens, but a successful design of animated transitions can hide processing delays and make the user experience smoother. In this paper, we describe the design of animated transitions and present a user study on how they are perceived.	Mobile screen transition animations	NA:NA:NA:NA:NA	2009
Danny Rado:Daniel F. Keefe	Despite the many challenges understanding how best to interact with large format displays, they are becoming increasingly popular for data analysis tasks in a variety of domains, including scientific, information, and geo-visualization. (Figure 1a shows a relatively small, 60" display; even larger, wall-size displays are also popular.) In order to make the most effective use of the full display, users typically stand and walk around in these environments. In fact, this physical navigation has been shown to be beneficial in data analysis tasks [1]. Since immobile input devices, such as mice, keyboards, or pen-tablets, do not naturally support interaction "on the move", new interactive techniques are needed to facilitate fluid interaction across a range of distances when working with large-format displays. We believe body-centric 3D, gestural input is particularly promising in this regard. Our work investigates techniques for reliable menu selection based upon these ideas, introducing new 3D input strategies for controlling menus. Our work builds upon previous techniques, such as rapMenu [3], which uses rotational hand movements and finger pinches to control menus from a distance.	rAir flow menus: toward reliable 3D gestural input for radial marking menus	NA:NA	2009
Kensuke Takada:Kyoko Higurashi:Tatsuhiko Suzuki:Misako Ota:Tetsuaki Baba:Kumiko Kushiyama	"Thermo-Pict" is a design apparatus produced by applying temperature visualization technology linked to an information display with the use of a thermograph sheet. Thermography is used to visualize the surface temperature of objects through their depiction as colors. This technology has been used primarily in the medical and research fields. Thermography display colors come in a wide range of hues and brightness that enables quick visualization of any object's surface temperature distribution. Use of this technology will be attempted as a tool in the production of design displays. [Fig. 1]	Temperature design display device to use peltier elements and liquid crystal thermograph sheet "Thermo-Pict"	NA:NA:NA:NA:NA:NA	2009
Celambarasan Ramasamy:Donald H. House:Andrew T. Duchowski:Brian Daugherty	This poster will analyze the feasibility of eye tracking as a tool for helping filmmakers to make decisions in a stereoscopic film production. In a conventional dialogue driven shot it is fairly easy to predict where the audiences would be looking. However, for visually complex shots it is not so obvious. In this case, eye tracking can be used as a tool to observe the gaze pattern of the audience to identify the regions of interest in the frame. This information could be used to budget the resources for the shot. It can also be used to identify elements that distract the audience from the flow of the movie. This technique could be used to help filmmakers to make more informed decisions during the film making process. We analyzed a student produced stereoscopic film using this technique. In our study, a number of subjects were asked to watch the film and their gaze data was recorded.	Using eye tracking to analyze stereoscopic filmmaking	NA:NA:NA:NA	2009
Yi-Ting Cheng:Virginia Tzeng:Yu Liang:Chuan-Chang Wang:Bing-Yu Chen:Yung-Yu Chuang:Ming Ouhyoung	The development in digital technologies and the widespread Web 2.0 concept have made many digital videos accessible. Editing and modifying digital videos have become an interesting and important topic. In this paper, we present a system for face replacement in video. Most digital processing software can perform face replacement only when the poses for the source and target faces are similar, and the manipulation process with those software is often time-consuming and labor-intensive. While previous work [Blanz et al. 2004] focuses on image face replacement, our system performs face replacement in video by constructing 3D models for both target and source faces and swapping them accordingly. 3D face models are created by fitting 3D morphable models [Blanz et al. 1999] and the input is reduced to two pictures for the face to be placed in.	3D-model-based face replacement in video	NA:NA:NA:NA:NA:NA:NA	2009
Che-Hua Yeh:Pei-Ruu Shih:Yin-Tzu Lin:Kuan-Ting Liu:Huang-Ming Chang:Ming Ouhyoung	This poster presents experimental results of three face recognition methods -- Support Vector Machine (SVM), Local Binary Pattern (LBP)-based, and Sparse Represented-based Classification (SRC). We will show the experimental results based on AR face database and on home photos. The experiments show that the three algorithms can achieve over 85% recognition rate in AR database. However, the recognition rate is extremely reduced in home photos. SVM and SRC-based method encounter challenges of selecting training model while LBP-based method encounters the challenge of merging over scattered clusters. Our goal is to improve the accuracy and efficiency especially in home photos based on the three methods.	A comparison of three methods of face recognition for home photos	NA:NA:NA:NA:NA:NA	2009
Jörn Loviscach	All of today's digital cameras record the date and time at which a photo has been taken; some cameras also record the geographical position. This work proposes yet another augmentation: to record temperatures at different spots picked by the user in the image. This has many applications for both family life and professional engineering: Was the water in the swimming pool heated? Was last Saturday night's party fever really a fever? On the serious side, an engineer may record the temperature of different chips on a printed circuit board or document heat loss due to bad building insulation.	Augmenting a camera with a thermometer	NA	2009
Yuji Morimoto:Yuichi Taguchi:Takeshi Naemura	Colorization is the process of adding color to monochrome images and video. It is used to increase the visual appeal of images such as old black and white photos, classic movies, and scientific visualizations. Since colorizing grayscale images involves assigning three-dimensional (RGB) pixel values to an image whose elements are characterized by one feature (luminance) only, the colorization problem does not have a unique solution. Hence, human interaction is typically required in the colorization process. Although existing colorization methods attempt to minimize the amount of user intervention, they require users to manually sellect a similar image to the target image or input a set of color seeds for different regions of the target image. In this paper, we present an entirely automatic colorization method using multiple images collected from the Web. The method generates various and natural colorized images from an input monochrome image by using the information of the scene structure.	Automatic colorization of grayscale images using multiple images on the web	NA:NA:NA	2009
Tongbo Chen:Abhijeet Ghosh:Paul Debevec	Separation of the diffuse and specular components of observed reflectance has been an active area of research in computer graphics and vision, with major applications in reflectance modeling and scene analysis. Traditionally, researchers have investigated diffusespecular separation under point or directional illumination conditions while employing polarization and, in the case of dielectric materials, color space analysis techniques. Recently, Ma et al. [2007] introduced a technique for estimating high quality diffuse and specular normals and albedo maps (see Fig. 1, (a) & (d)) of a specular object using polarized spherical gradient illumination. However, the employed polarization technique imposes view-point restriction, and results in insufficient light levels for performance capture with high speed acquisition. Hence, in this work, we look into an alternate diffuse-specular separation technique for spherical gradients based on a data-driven reflectance model. Traditional separation techniques based on color space analysis focus on removing specular reflections from the observation for scene analysis [Mallick et al. 2005]. In contrast, we focus on obtaining high quality estimates of both the diffuse and the specular reflectance components.	Data-driven diffuse-specular separation of spherical gradient illumination	NA:NA:NA	2009
Mary Hudachek-Buswell:Catherine Matos:Michael Stewart	In this presentation, the restoration of images blurred by atmospheric turbulence is examined. The proposal uses a new class of approximations to blurring operators representing Gaussian blur. The Toeplitz matrix representing the blur is transformed into a Cauchy-like (CL) matrix using the FFT. In addition to the CL structure, the transformed matrix has a rank structure. In particular, the off-diagonal blocks have low rank. This class of matrices can be approximated quickly, and the structure can be exploited for fast image restoration.	Deblurring with rank-structured inverse approximations	NA:NA:NA	2009
Nari Kim:Jong-Chul Yoon:In-Kwon Lee	In this work, we present an image based virtual dress up system according to user input model and garment image. At 'Registration' step, we asked the user manually setting the skeleton structure and matte out the alphamap from the image. Next step, our method automatically deforms the garment image corresponding to model's body. For the boundary fitting, our method uniformly sampled contour points and solves the optimization function. To enhance the more realistic scene, we reconstruct the 2D mesh to the 3D mesh according to a human's standard body shape. For the lighting effect we estimate the light position by using luminance value with the detected face region. Previous 3D scanner based virtual dress up system has expensive cost and under locational limitation issues, but our system integrates various image processing techniques and introduced an easy-to-use system for the general users. We present that our system produces a visually plausible and well-fitted virtual dress up results in a practical and usable way.	Image-based dress up system	NA:NA:NA	2009
Yingen Xiong:Xianglin Wang:Marius Tico:Chia-Kai Liang:Kari Pulli	We present the design and implementation of a mobile imaging system for high resolution panoramic image creation. The system comprises the following components: automatic camera motion tracking and high resolution image capturing, image registration on spherical manifold, image warping, image labeling, and image blending.	Panoramic imaging system for mobile devices	NA:NA:NA:NA:NA	2009
Kei Utsugi:Takuma Shibahara:Takafumi Koike:Takeshi Naemura	Seam carving is an image processing operator for content-aware image resizing [Avidan and Shamir 2007]. It generates an energy map from gradient intensity of pixels and searches for seams, which are vertical or horizontal continuous paths of pixels that run through local minimum energy areas. Removing or inserting pixels along a seam enables users to shrink or enlarge pictures by a wide range, while still retaining all details of the image.	Proportional constraint for seam carving	NA:NA:NA:NA	2009
Susan M. Munn:Jeff B. Pelz	Our portable video-based monocular eye tracker contains a headgear with two cameras that capture videos of the observer's right eye and the scene from the observer's perspective (Figure 1a). With this eye tracker, we typically obtain a position -- that represents the observer's point of regard (POR) -- in each frame of the scene video (Figure 1b without bottom left box). These POR positions are in the image coordinate system of the scene camera, which moves with the observer's head. Therefore, these POR positions do not tell us where the person is looking in an exocentric reference frame. Currently, the videos are analyzed manually by examining each frame. In short, we aim to automatically determine how long the observer spends fixating specific objects in the scene and in what order these objects are fixated.	Ray tracing to get 3D fixations on VOIs from portable eye tracker videos	NA:NA	2009
Naoki Kawai	Vector plot is a frequently used method for illustrating vector fields used in applications such as scientific visualization. Although the method is easy to implement and the resulting image captures the original vector field well, the streamlines are often positioned too closely or too sparsely to one another due to sources and sinks of the original vector field. This results in unevenness of visual density over the entire region, and some previous researches have treated the problem. Mebarki et al [1] proposed the improved strategy that the maximum vacant region should be given priority for a new streamline, but the results still lack uniformity. Other related works [2][3] suggested that both tapering streamlines and controlling intensity improve the visual uniformity of streamlines. We propose another approach for making streamlines look uniform with dotted and broken lines instead of tapering or intensity control. The results are binary images and consist of fixed width streamlines which preserve uniformity.	Uniform looking vector plot with streamline fragmentation	NA	2009
Chung-Lin Wen:Yu-Ting Wong:Bing-Yu Chen:Yoichi Sato	In this extended abstract, we propose a novel approach for video segmentation by utilizing motion information. Recently, graph-cutbased segmentation methods became popular in this domain but most of them dealt with color information only. Those methods possibly fail if there are regions similar in color between foreground and background. Unfortunately, it is usually hard to avoid, especially when objects are filmed under a natural environment. For instance, Figure 1(a) shows a result of graph cut with a small smoothness weighting, and hence some background regions are incorrectly labeled. On the contrary, if a larger smoothness weighting is used, some background regions near the foreground will be merged as shown in Figure 1(b). To improve those drawbacks, we propose a method based on both of color and motion information to conduct the segmentation. The method is useful because foreground and background usually have different motion patterns as shown in Figure 1(c).	Video segmentation with motion smoothness	NA:NA:NA:NA	2009
Alice Boit:Thomas Geimer:Jörn Loviscach	Graphical passwords [Suo et al. 2005] address vital problems of textual passwords: Users pick from a limited vocabulary; machine-generated passwords are hard to memorize. Graphical input, however, faces "shoulder surfing," as bystanders can watch the screen. Current solutions to this problem tend to impose high cognitive loads. We propose an easy-to-handle approach.	A random cursor matrix to hide graphical password input	NA:NA:NA	2009
Alexandros Zotos:Katerina Mania:Nick Mourkoussis	In order to economize on rendering computation, selective rendering guides high level of detail to specific regions of a synthetic scene and lower quality to the remaining scene, without compromising the level of information transmitted. Scene regions that have been rendered in low and high quality can be combined to form one complete scene. We propose a novel selective rendering approach which is task and gaze-independent, simulating cognitive creation of spatial hypotheses. Scene objects are rendered in varying quality (polygon count) according to how they are associated with the context (schema) of the scene.	A selective rendering algorithm based on memory schemas	NA:NA:NA	2009
Matthew Hirsch:Douglas Lanman:Ramesh Raskar:Henry Holtzman	We present a BiDirectional screen capable of both imaging and display, that uses an LCD as a spatial light modulator to support seamless transition from on-screen multi-touch interactions to off-screen hover-based gestures.	BiDi screen: depth and lighting aware interaction and display	NA:NA:NA:NA	2009
Jinha Lee:Yasuaki Kakehi:Takeshi Naemura	In this paper, we propose a novel block-shaped tangible interface named Bloxel (see Figure 1). A Bloxel is a translucent cubical block that glows in full color and communicates with the neighboring Bloxels through high-speed flickers.	Bloxels: glowing blocks as volumetric pixels	NA:NA:NA	2009
André Maximo:Maria Paula Saba:Luiz Velho	Introduction and Related Work Tabletop and tangible interfaces have become common in recent years. Technology trends in this area can be found in commercial products, such as Apple's iPhone™ and Microsoft Surface™, as well as in research ventures, such as Reactable and Perceptive Pixel initiatives. Nevertheless, natural human computer interfaces (HCI) to support this hardware technology are still non-intuitive.	collecTable: a natural interface for music collections	NA:NA:NA	2009
Andrew Bragdon:David H. Laidlaw	We explore the design of a multi-view interaction metaphor for 3D visualization in the CAVE. We then present the results of a formative evaluation of a "Wizard of Oz" [Kelley 1984] prototype. Although there has been significant prior work on 2D and 3D desktop applications utilizing multiple views, little prior work exists for multi-view systems in immersive virtual environments such as the CAVE, despite the clear advantages enjoyed by desktop analogues. Immersive 3D environments pose unique challenges for such a system. Since the contents of such views are themselves 3D, it is unclear whether users will be able to easily read views independently of one another, as in a naive implementation they might become intermingled; even in a system that is conscious of this problem, some vantage points may cause depth ambiguity problems which make it difficult to read each view. In addition, interaction techniques for controlling and managing such views must be explored. Thus, formative empirical testing is warranted to determine the viability of such a system.	The design and evaluation of a lightweight multi-view interaction metaphor for 3D visualization in the CAVE	NA:NA	2009
Mi Sun Lee:Mi-Gi Han:Joo-Youn Park:Su-e Park	Online spaces are being transformed into new social spaces with a variety of interpersonal relationships and social activities. Especially, cyber spaces based on three dimensions show various cross-cultural social relationships and activities compared with cyber spaces based on two dimensions. These phenomena have different characteristics, depending on users' cultural backgrounds. Relating to social issues in online spaces, many preliminary studies have been conducted. Especially, impressions have been considered important subjects related with social networks. In spite of that, sufficient cross-cultural research related with impressions in online spaces has not been conducted, especially based on 3-D cyber spaces. Therefore, the main goals of this study were to extract 3-D cyber factors formatting perceptional impressions and compare those factors based on cultural differences. In the preliminary research, we identified six impressions dimensions in 3-D cyber space: F1.Cheerful, F2.Logical, F3.Violent, F4.Selfish, F5.Warm, and F6.Seclusive (Lee, Kim & Park, 2009). In order to achieve our goal, first, we selected two countries considering Hofstede's culture dimensions (e.g. Power Distance, Individualism versus Collectivism, Masculinity versus Femininity, Uncertainty Avoidance) (Hofstede, 2005). Korea and America have very different cultural characteristics in terms of Hofstede's culture dimensions (Hofstede & Bond, 1984). Secondly, we conducted in-depth individual interviews. For these interviews, we recruited interviewees as actual users of 3-D cyber spaces (Second Life); depending on the frequency uses and interpersonal relations contained therein, we selected eight Korean participants and eight American participants. Before conducting interviews, we recorded normal lives of participants within a three-day span, for two hours of each day. Then, we conducted the survey to each participant seeing the video clips of others' virtual lives for the purpose of analyzing others' preserved impressions. In-depth interviews were conducted in 3-D cyber space using actual voices. The interview consisted of two parts of questions: 1) What are the factors relating with your perceived impressions?; and 2) If you help an avatar on the video clip before you saw to make clear his/her impression, how will you help? All interviews were recorded as video and audio clips. After collecting data, we analyzed data based on Grounded theory (Strauss, 1990) recognized qualitative research methods. First of all, we accurately transcribed all voice data to text data, and then separated data to minimal units of meaning considering interviewees' intentions. Finally, we extracted properties and grouped properties during axial coding. As a result, Factors formatting perceptional impression in 3-D cyber space was derived with distinction by Korean and American users. These derived factors were linguistic, visual, behavioral, relational, inner-environmental, and outer-environment. Of these, linguistic factors (106, 43%) and behavioral factors (57, 23%) were the most derived. Further, looking at the visual factors, the number of derived factors was similar among Korean and American users. Alternatively, we looked at the detailed factors derived with distinction by Korean and American users. The factors derived by Korean users included exposure degree of clothes, thickness of clothes, while the factors derived by American users included color of clothes and types of avatar. In conclusion, this study has theoretical and empirical significance. The theoretical significance, through the cultural differences research, is to understand how each intercultural impression provided role elements in Korea cultures and American cultures and to understand how the impression provided difference elements. Therefore, more extensive future research on the dimensions of the intercultural impression formation mechanism was proposed, based on this study. The empirical significance was to offer impression dimensions-related elements in 3D gaming to developers and designers in the development of related systems; furthermore, as the results provide data of how elements affect impression in intercultural perception and how in each dimension, the system will be able to provide a basis about impression formation elements in intercultural context.	Factors formatting perceptional impression in 3-D cyber spaces: a cross-cultural study of Korean and American users	NA:NA:NA:NA	2009
Piotr Dalka:Andrzej Czyzewski	The main goal of each HCI application is to make working with a computer as natural, intuitive and effective as possible. One of the main areas of applications of new human-computer interfaces is making possible to use computers for people with permanent or temporal motor disabilities in an efficient way. There are two main types of such solutions [Aggarwal and Cai 1999]. The first group utilizes devices mounted directly on the user's body. Applications in the second group are contactless and they use remote sensors only, therefore they are much more comfortable for a user. Amongst contactless solutions, vision-based human-computer interfaces are the most promising ones. They utilize cameras and image processing algorithms to detect signs and gestures made by a user and execute configured actions. The most common vision-based applications employ eye and hand tracking [Shin and Chun 2007].	LipMouse: novel multimodal human-computer interaction interface	NA:NA	2009
Fumitaka Ozaki:Takuo Imbe:Shin Kiyasu:Yuta Sugiura:Yusuke Mizukami:Shuichi Ishibashi:Maki Sugimoto:Masahiko Inami:Adrian D. Cheok:Naohito Okude:Masahiko Inakage	MYGLOBE is an interactive map media which allows us to share our cognitive maps. This map grows up with our own activities and shows our subjective view of the city by emphasizing roads or landmarks frequently used. Users can bring up their own city in the device by actually walking in the city, and also share their own maps with each other and discover unknown places. Present map services such as Google maps and Google Earth, provide mash-up tools which allow us to create our own favorite place on the map easily. We can use hand held GPS devices to make our own travel route and navigate to destination places. MYGLOBE allows us to not only tag their favorite places on the map but also change the shape of the map itself. Instead of an accurate geographic map, MYGLOBE provides maps reflecting the user's individual experiments and the view of the city. It can also be used as a communication tool to share the life history with your friends. MYGLOBE will enhance your city experience.	MYGLOBE: cognitive map as communication media	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2009
Duksu Kim:Jae-Pil Heo:Sung-eui Yoon	Collision detection between deformable models is one of fundamental tools of various applications including games. Collision detection can be classified into two categories: discrete and continuous collision detection methods. Discrete collision detection (DCD) has been demonstrated to show the interactive performance by using bounding volume hierarchies (BVHs). However, some colliding primitives may be missed since DCD methods find intersecting primitives only at discrete time steps. This issue can be a very serious problem in physical based simulation, CAD/CAM applications and etc. On the other hand, continuous collision detection (CCD) identifies the first time of contact of colliding primitives during a time interval between two discrete time steps.	PCCD: parallel continuous collision detection	NA:NA:NA	2009
Sho Kamuro:Kouta Minamizawa:Naoki Kawakami:Susumu Tachi	We propose a pen-shaped handheld haptic display that allows haptic interactions with virtual environments by generating kinesthetic sensations on the user's fingers; the user's movements are not restricted since the device does not have mechanical linkages. Unlike conventional haptic displays that provide vibrations, which are not representative of tactile sensation, our proposed device, named "Pen de Touch" (Figure 1), provides kinesthetic sensations to the muscles in the user's fingers.	Pen de Touch	NA:NA:NA:NA	2009
Toki Takeda:Kazuhiko Yamamoto:Reiji Tsuruno:Taketoshi Ushiama	The recent popularity of small mobile devices such as cellular phones, digital cameras, and game device has made it increasingly convenient to carry them with us as we go about our daily lives. However, the increased miniaturization and functionality range of these devices can make it hard to access and utilize their contents. For instance, when using a device with a small display screen, it is often impossible to display an entire area of interest in a single view and the existence of numerous buttons can make it difficult to manipulate the device. In this study, we propose a novel interface that allows intuitive browsing of the content of mobile devices using an actual map.	Actual map based interface for browsing content on mobile devices	NA:NA:NA:NA	2009
Christian Schulze:Laurens Nienhaus:Jörn Loviscach	Painting in a 3D geobrowser is interesting both for artistic uses such as virtual graffiti and for commercial applications such as architectural sketches. It seems straightforward to turn Google Earth into a 3D painting tool: Store the 3D mouse data that the software returns and construct polylines from them. However, this approach has two vexing drawbacks: First, when the user paints past an edge, the end of the stroke will be placed at an incorrect depth, see Figure 1; second, it is not possible to sketch in mid-air, for instance to indicate a planned height extension of a building.	Sketch-based annotations in Google Earth	NA:NA:NA	2009
Andrzej Czyzewski:Piotr Odya:Agnieszka Grabkowska:Michal Grabkowski:Bozena Kostek	Dyslexia (dysgraphia) therapy is often boring for children and, what even worse, its results can be unsatisfactory. Hence, many therapists insist on development new methods which would be more interesting for young patients. The Smart Pen is such a tool. It is designed for supporting the therapy of developmental dyslexia, with particular regard to dysgraphia.	Smart pen: new multimodal computer control tool for dyslexia therapy	NA:NA:NA:NA:NA	2009
Yasuko Hayashi:Kensei Jo:Yasuaki Kakehi:Takeshi Naemura	Generally, people use a keyboards as an interface for text input. However, unlike handwritten characters, typed characters are identical no matter who types them. To create variations in the appearance of typed characters, we usually decorate characters by changing fonts. For example, we change the font size, font color and boldness of the characters and the spacing between characters. However, due to these decorations, people must handle a few input processes such as checking the select menu. To improve current conditions, some research suggests methods of applying the user's unconscious actions while typing to decorate characters, using a laptop with a built-in acceleration sensor or body-worn electronic equipment [Iwasaki et al. 2009][Wang et al. 2004][TypeTrace 2006].	TypeTile: a keyboard system that decorates characters depending on the way of typing	NA:NA:NA:NA	2009
Kai-Yin Cheng:Ko-Yuan Chou:Sheng-Jie Luo:Bing-Yu Chen	Due to the development in digital technologies, people now can easily retain their valuable memory by taking pictures through digital cameras. The cheap digital storage also encourages people to take lots of photos as they want. However, due to the tremendous amount of digital photos, it is not easy for people to browse all of them. Therefore, some techniques are proposed to help people to enjoy the photos, although it may be difficult for some people to arrange a time slot to watch them intentionally. Hence, in this extended abstract, we propose a system, which can utilize the large number of photos as the program themes (background), so that people will not notice the synthesized background while they are working, but the program themes may still be able to remind their good memory when they taking a short rest.	Utilizing photos as program themes	NA:NA:NA:NA	2009
Tatsuya Ishikawa:Shoichi Hasegawa	Recently, hobby robots such as pet robots and humanoid robots for entertainment are spreading and becoming more and more familiar each day. Compared to robots such as HONDA's ASIMO, a hobby robot is much cheaper, less rigid and has far less precision in measuring and controlling the angle of its own joints. For this reason, although we can assign joint angle to a key frame, assigned posture cannot be taken like computer graphics. Therefore, to ensure the robot moves as desired, we need to actually look at the robot operating while adjusting key frames respectively. By this, the margin of error in the joint angle and distortion in mechanism can be avoided. For CG animation, the animator observes the animation in real-time and when a problem is encountered, the problem in the key frame is corrected by slowing the animation down or by examining each frame. However, with robots, sudden stopping of ambulatory action makes the robot fall down. Moreover, the error in joint angle and distortion in mechanism are different between operation and geostationary state because of dynamic influences.	Virtual stroboscope for robot motion design	NA:NA	2009
Stephan Wenger:Marcus Magnor:Christophe Morisset:Wolfgang Steffen	Distant astrophysical objects like planetary nebulae can normally only be observed from a single point of view, which makes deducing plausible 3D models a hard task that usually involves a lot of manual work [Nadeau et al. 2001]. However, additional physical assumptions can be used in order to estimate the missing depth information. In previous work [Wenger et al. 2009], a certain axial symmetry was assumed which is present in many planetary nebulae, so that tomographic methods could be used for the reconstruction. However, this assumption obviously fails for many of the most complex and interesting objects in question, and it only leads to unambiguous results as long as no absorption occurs within the nebula.	3D reconstruction of planetary nebulae using hybrid models	NA:NA:NA:NA	2009
Hiroki Fujishiro:Takanori Suzuki:Shinya Nakano:Akinobu Mejima:Shigeo Morishima	Everyone is interested in being more attractive. Leyvand et al have been proposed the method which can enhances of facial attractiveness into a photograph [Leyvand et al. 2008]. However, their method can't synthesize more attractive facial expression than that of an input face photograph. Meanwhile, amateur subjects' facial expressions often become unnatural when they act in front of the camera in experimental environments. At such a situation, a natural expression can be synthesized without performance skills.	A natural smile synthesis from an artificial smile	NA:NA:NA:NA:NA	2009
Kentaro Yamanaka:Shinsuke Nakamura:Shota Kobayashi:Akane Yano:Masashi Shiraishi:Shigeo Morishima	This paper presents a new methodology for constructing a skin deformation model using MRI and generating accurate skin deformations based on the model. Many methods to generate skin deformations have been proposed and they are classified into three main types. The first type is anatomically based modeling. Anatomically accurate deformations can be reconstructed but computation time is long and controlling generated motion is difficult. In addition, modeling whole body is very difficult. The second is skeleton-subspace deformation (SSD). SSD is easy to implement and fast to compute so it is the most common technique today. However, accurate skin deformations can't be easily realized with SSD. The last type consists of data-driven approaches including example-based methods. In order to construct our model from MRI images, we employ an example-based method. Using examples obtained from medical images, skin deformations can be modeled related to skeleton motions. Retargeting generated motions to other characters is generally difficult with this kind of methods. Kurihara and Miyata realize accurate skin deformations from CT images [Kurihara et al. 2004], but it doesn't mention the possibility of retargeting. With our model, however, generated deformations can be retargeted. Once the model is constructed, accurate skin deformations are easily generated applying our model to a skin mesh. In our experiment, we construct a skin deformation model which reconstructs pronosupination, rotational movement of forearm, and we use range scan data as a skin mesh to apply our model and generate accurate skin deformations.	Accurate skin deformation model of forearm using MRI	NA:NA:NA:NA:NA:NA	2009
Satoko Kasai:Shigeo Morishima	The presence of CG character is essential in anime and movie contents recently. Especially, personality and aging factor are also important in character modeling. However, the modeling CG character is based on hand-made process yet, so it costs huge amount of money and labor to give one character several variations. About a facial animation, muscle based process or blend shape based process is very popular in contents production, however, in case of considering aging mechanism on face skin and bone, the different model of each age has to be constructed for every character.	Aging model of human face by averaging geometry and filtering texture in database	NA:NA	2009
Ming Tang	This poster presents a range of urban simulation techniques and multidisciplinary research across Architecture design, urban design, interactive design and game design, as well as visual effects. Through a series of design experiments using City Generator, the 3D urban simulation tool, I explored an integrated set of design methods, such as genetic computation, and real time simulation with game engine. The complexity of 3D urban form is procedurally controlled by 2D GIS (Geography Information System) data input and digital elevation model (DEM), which allows designers to efficiently shape urban growth in a preferred direction.	City generator: GIS driven genetic evolution in urban simulation	NA	2009
Lu Liu:Tao Ju	Describing shapes is an important task in graphics and vision. A simple, concise descriptor that captures the essential shape properties of an object would greatly facilitate computer-based understanding of the object and applications such as matching and segmentation. For this reason, medial axes (MA) has become a popular shape descriptor since its introduction by Blum [Blum 1967]. The MA of an N-D object is an (N -- 1)-D geometry centered within the object. For example, the MA of a 2D object consists of medial curves at elongated parts, whereas the MA of a 3D object consists of medial surfaces describing the protrusions on the object.	Defining and computing multi-dimensional skeletons	NA:NA	2009
Sebastian Pena Serna:Andre Stork	Dynamic simplicial meshes will enable real time unconstrained deformation of triangular and tetrahedral meshes regardless of topological limitations, by means of combining a quality measure with complex topological operations and smoothing techniques, which automatically improve and maintain the quality of the mesh, aiming at mesh and geometry processing applications.	Dynamic simplicial meshes	NA:NA	2009
David Nilosek:Karl Walli	Automated synthetic terrain and architecture generation is now becoming feasible with calibrated camera remote sensing. This poster implements computer vision techniques that have recently become popular to extract "structure from motion" (SfM) of a calibrated camera with respect to a target. This process will build off of Microsoft's popular "PhotoSynth" technique and apply it to geographic scenes imaged from an airborne platform. Additionally, it will be augmented with new features to increase the fidelity of the 3D structure for realistic scene modeling. This includes the generation of both sparse and dense point clouds useful for synthetic macro/micro-scene reconstruction.	Aerial scene synthesis from images	NA:NA	2009
Jingyuan Huang:Stephen Mann:Bill Cowan	Müller et al. introduced CGA shape, a shape grammar for procedural modeling of architecture [Müller et al. 2006b], which they applied to Mayan archaeological site in Xkipché [Müller et al. 2006a]. Inspired by this application of shape grammars to archaeology, we built a simple reconstruction application that uses a shape grammar to build 3D Inca sites from 2D plans. The application can help users to create a quick 3D overview of an Inca site that they are studying.	Inca reconstruction using shape grammar	NA:NA:NA	2009
Yoshiki Mizushima:Shuhei Nomura:Genki Umeizumi:Noriko Nagata:Yoshiyuki Sakaguchi	The need for rendering woven fabrics arises frequently in computer graphics[N Adabala, N Magnenat-Thalmann, G Fei 2003]. Woven fabrics have a specific appearance, luster, and transparency. We have proposed a BRDF/BTDF model using the Henyey-Greenstein function and an algorithm for the real-time rendering of woven fabrics based on the texture look-up table[Uno et al. 2008]. However, in order to make the model more accurate, the microlevel BRDF/BTDF is necessary. The objective of this study is to express a more detailed texture of the cloth by creating a 3D model that especially takes into consideration the "twisted structure" of the yarn in the fine woven structure of the cloth and by making the rendering algorithm more precise.	Lace curtain: modeling and rendering of woven structures using BRDF/BTDF: production of a catalog of curtain animations	NA:NA:NA:NA:NA	2009
E. Riegel:T. Indinger:N. A. Adams	Within computational fluid dynamics (CFD) the Navier-Stokes (NS) equations are traditionally used to describe the physical properties of the fluid. An alternative approach to classical discretizations for the numerical solution of the Navier-Stokes equations, such as Finite-Difference and Finite-Volume schemes, is provided by the Lattice-Boltzmann equations [Benzi et al. 1992], [Chen and G. 1998]. The Lattice-Boltzmann method (LBM) uses a Cartesian grid for propagating and relaxing a discrete velocity distribution function on a lattice at discrete time steps. Usually a very large number of cells is necessary to obtain an accurate prediction of the macroscopic scales for pressure and velocity. However, due to the simple formulation of the underlying algorithm this method is well suited for parallelization and hardware acceleration using general purpose graphical processing units (GPGPU). LBM is used in engineering software for example to compute the aerodynamic drag of a car to improve its efficiency. Therefore LBM has a big practical importance. Improving the performance of a CFD simulation gives the engineers more time and better feedback during the engineering process leading to more efficient engineering processes and more efficient engineering products. Moreover a strong acceleration in simulation performance makes a new quality of physical simulation technology available for desktop computer software like entertainment and content creation software.	Numerical simulation of fluid flow on complex geometries using the Lattice-Boltzmann method and CUDA-enabled GPUs	NA:NA:NA	2009
Yu-Bin Yang:Jin-Jie Lin	In content-based 3D mesh retrieval, graph-based structure is one of the most important shape descriptors. The early work on this issue can be found in [Hilaga et al. 2001], in which a 3D mesh representation, Multiresolutional Reeb Graphs (MRGs), was proposed. Since then, many Reeb-Graph based descriptors have been designed to simplify and represent 3D meshes. However, most of those descriptors are only suitable for graph-based topology matching, which is time-consuming and unreliable. To address this issue, we propose a novel approach to representing 3D mesh by using a tree-like structure, Attributed Root Trees (ARTs). The advantages of our method are three-fold: (1) Tree matching is easier and efficient than graph matching; (2) The representation well reserves topological information of a 3D mesh, thus topology matching can be easily completed; (3) It is possible to perform multi-resolution matching on ARTs.	Representing 3D mesh with attributed root trees	NA:NA	2009
Ross Sowell:Lu Liu:Tao Ju:Cindy Grimm:Christopher Abraham:Garima Gokhroo:Daniel Low	MRI and CT scanners have long been used to produce three-dimensional samplings of anatomy elements for use in medical visualization and analysis. Physicians often need to construct surfaces representing the anatomical shape in order to conduct treatment, such as radiating a tumor. Traditionally, this is done by a time-consuming process in which an experienced physician marks a series of parallel contours that outline the object of interest.	User studies on the feasibility of oblique contouring	NA:NA:NA:NA:NA:NA:NA	2009
Trishul Mallikarjuna	This document introduces a conceptually novel, simple and ordinarily robust computer-vision-based method of extracting musical beats from regular physical gestures of a performer, implemented in VisiBeat: a grid-based percussion system on the Max/MSP/Jitter platform for collaborative interactive music.	A visual beat detection system for grid-based interactive percussion and synchronization	NA	2009
M. Cicconet:I. Paterman:P. Carvalho:L. Velho	Hardware based musical instruments are, in general, from the performer point of view, merely copies of real physical instruments. They do not provide facilities for being played, especially for musically untrained people.	The blues machine	NA:NA:NA:NA	2009
Sergio Krakowski:Luiz Velho:Francois Pachet	In this work, we address to the problem of making the machine listen and react to the musician in an improvisation situation with the purpose of generating high-quality music.	Pandeiro funk: experiments on rhythm-based interaction	NA:NA:NA	2009
Moohyun Cha:Jaikyung Lee:Byungil Choi:Hyokwang Lee:Soonhung Han	In order to simulate and visualize natural phenomena, especially fluid behavior such as smoke and fire, many novel studies have recently been conducted. Usually these methods use CFD (computational fluid dynamics), which calculate Navier-Stokes equations in real-time to generate realistic fluid motion and interactions, as well as high-performance GPU technologies. We proposed a new approach to the visual simulation of fluid flow by combining the use of pre-calculated CFD data with the real-time processing of such data. As the domain-specialized CFD solver predicts detailed fluid dynamics to an accuracy of a guaranteed error range, we could provide nearly actual behaviors of a fire-driven fluid flow. Moreover, this CFD data includes physical quantities such as temperature distribution, which can provide useful information to the training evaluation process. However, the data-driven method requires appropriate data processing techniques to create and manage large data sets. In this study, we developed a firefighter training simulator to demonstrate our proposed methods and explore related research issues.	A data-driven visual simulation of fire phenomena	NA:NA:NA:NA:NA	2009
Ippei Takauchi:Masatoshi Ochiai:Hiromu Saito:Ryo Asakura:Motofumi Hattori	In this paper, the authors discuss how to make 3DCG animations of flowers, wings, and cloths etc. which are modeled by surfaces. These 3DCG animations are obtained based on the numerical simulation for the Newtonian dynamics equations of surfaces. These equations are obtained from the potential functions (the energy functions) of the surfaces.	Desired deformation of continuum surfaces in 3DCG animation by time varying stable forms: application to make animations of flowers, wings, cloths etc.	NA:NA:NA:NA:NA	2009
Kazuhiko Yamamoto	There has been growing interest in the sound generating technique based on physics from the motions of 3d graphics objects. In recent work several methods have been proposed to physically simulate these audio events natably using modal synthesis [K. van den Doel et al. 2001] or finite element method [O'Brien et al. 2002]. However, in these mesh-based method, it needs complicated operation to preprocess, for example, to generate the computational mesh for 3d object, and to create the system's matrices, etc...	The framework of sound rendering for particle-based physics	NA	2009
Alejandro L. Garcia:Alice A. Carter:J. Courtney Granner:David Chai	"Physics for Animation Artists" is a joint project by the Department of Physics and the Animation/Illustration Program at San Jose State University to develop a physics curriculum specifically for art majors planning to enter the animation industry.	Physics for animation artists	NA:NA:NA:NA	2009
Biswarup Choudhury:Pisith Hao:Sharat Chandran	Amongst all atmospheric phenomena, rain is probably the most commonly used effect to create realistic immersive virtual environments, and to set the mood in movie storytelling. Although not immediately obvious, the beauty of rain emanates from the interplay of the involved light-matter interaction, generating effects of refraction and reflection, coupled with scattering effects. At the core, rain consists of water droplets under the influence of gravity. Current state of the art methods of generating rain are either computationally burdening, or not realistic enough. The key idea we introduce in this paper is to consider these droplets as transparent objects in the environment matting (EM) framework. This enables careful preprocessing to discover the light transport phenomena. We end up with a free-viewpoint real-time technique of simulating realistic droplets and rain in novel environments.	Real-time droplet modeling using color-space environment matting	NA:NA:NA	2009
Adrien Herubel:Venceslas Biri:Farchad Bidgolirad	In computer graphics, physically-based global illumination algorithms such as photon-mapping [2001] have a linear progression between complexity and quality. To a given quality, rendering time scales linearly with computer performances. With Moore's law call in question and increasing demand in quality, those algorithms need more and more optimisations.	Autonomous lighting agents in global illumination	NA:NA:NA	2009
Graham Fyffe	Image-based relighting is a powerful technique for synthesizing images of a scene under novel illumination conditions, based on a set of input photographs. While successful relighting methods exist, they either require many photographs [Debevec et al. 2000], or operate on a limited class of materials or illumination conditions [Ma et al. 2007][Ramamoorthi 2006].	Cosine lobe based relighting from gradient illumination photographs	NA	2009
Hiroyuki Kubo:Mai Hariu:Shuhei Wemler:Shigeo Morishima	Simulating sub-surface scattering is one of the most effective ways to realistically synthesize translucent materials such as marble, milk and human skin. In previous work, the method developed by Jensen et al. [2002] improved significantly on the speed of the simulation, yet still cannot produce real-time rendering. Thus, we have developed a simple local illumination model which mimics the presence of a subsurface scattering effect. Furthermore, this approach is easy implementable on the GPU and doesn't require any complicated pre-processing as is often the case in this area of research [Mertens et al. 2003].	Curvature-dependent local illumination approximation for translucent materials	NA:NA:NA:NA	2009
Greg Nichols:Chris Wyman	Area light sources are common in the real world, and thus important in realistic images. However, interactive rendering with area light sources is challenging, as each surface in a scene can receive light from every point in the area light. This problem is similar in nature to the rendering of single-bounce indirect illumination, and can be addressed with similar techniques.	Direct illumination from dynamic area lights	NA:NA	2009
Sajid Farooq:J. Paul Siebert	Gaussian Projection is an algorithm based on the Gaussian Pyramid, that can render point-sampled-geometry - obtained from stereo data in the form of range images - without polygonization, and at full native resolution. Gaussian Projection makes use of the GPU to perform efficient and fast multi-resolution rendering of point-based data, with automatic hole-filling (scattered point interpolation), and without any preprocessing other than Image Pyramid generation.	Gaussian projection: a novel PBR algorithm for real-time rendering	NA:NA	2009
Cyril Crassin:Fabrice Neyret:Sylvain Lefebvre:Miguel Sainz:Elmar Eisemann	Voxel representations are commonly used for scientific data visualization, but also for many special effects involving complex or fuzzy data (e.g., clouds, smoke, foam). Since voxel rendering permits better and easier filtering than triangle-based representations it is also an efficient high-quality choice for complex meshes (with several triangles per pixel) and detailed geometric data (e.g., boats in Pirates of the Caribbean).	Beyond triangles: gigavoxels effects in video games	NA:NA:NA:NA:NA	2009
Borom Tunwattanapong:Paul Debevec	We present a technique for relighting an image such that different areas of the image are illuminated with different combinations of lighting directions. The key idea is to capture illumination data using a lighting apparatus system such as Hawkins et al. [2004], calculate radial basis function interpolation of light constraints specified by users and render the calculated illumination result in realtime using GPU. The application can simulate the result of unnatural lighting conditions, for example, the image of a whole face lit from per pixel view dependence reflection angles or from gazing angles (see Fig. 1, a). The application can also render a high-resolution result at 1920 x 1080 in three to four minutes.	Interactive lighting manipulation application on GPU	NA:NA	2009
Kenshi Takayama:Takeo Igarashi	In our previous work of lapped solid textures [Takayama et al. 2008], layered (or 'type 1-b') texture exemplars were used to create solid textured models such as strata and cakes. However, no methods have been proposed so far to synthesize this kind of texture automatically. This poster proposes an extension of Kopf et al.'s method [2007] to synthesize such layered solid textures from single 2D exemplars.	Layered solid texture synthesis from a single 2D exemplar	NA:NA	2009
Seungju Bang:Kyoungju Park	In oriental paintings, artists have developed a unique style that exploits the effects of the dispersion of the ink, composed of soot and glue, onto absorbent paper. These artists produce these effects purposely by manipulating the ink concentration, stroke speed and brush angle. We describe these artistic styles of oriental painting and show how we render a single image in the oriental painting style. Our work is to take a single image as input and produce an oriental brushwork-like image automatically as a result. This nonphotorealistic rendering do not have richness of physical painting system such as 3-D brushes but reproduce artistic style of paintings to an image with a speed. In recent years, oriental brushwork rendering have been applied for 3-D objects e.g. Non-photorealistic rendering in Chinese painting of animals by Yeh and Ouhyoung 2002 and stereo images e.g. Humanistic oriental art created using automated computer pocessing and non-photorealistic rendering by Cheok et al. 2007 that require geometric details and depth information, but our method proposes a system that reproduce the styles and effects of oriental paintings from single 2-D image by using a set of image processing techniques. Figure 1 shows an input image and the reproduced oriental paintings. We focus on reproducing the stroke drawing and artistic shades that are essential in conventional oriental paintings. Some artistic styles of painting exploit the variety of lines and deep shades at once, giving a charming feel to the oriental brushwork. One general technique is "gu-ru law"Figure 2(a), in which artists draw tufted thin lines for boundaries and geometric details, and brush the interior with abstract but contrasting shades of ink. Another artistic style shades the whole objects instead of drawing its boundary with lines Figure 2(b), and paints near boundary regions with highly contrasting intensities. Our overall framework consists of two modules: stroke drawing and artistic shading Figure 3. These modules, essential to oriental painting, are generated using a set of techniques. Abstraction: Given an input image, we simplify it for geometric abstraction and color quantization using bilateral filtering. The abstracted image is then used as the input for our stroke drawing and artistic shading modules. Stroke drawing: We extract and draw representative lines for boundaries and geometric details. Line extraction includes selecting feature points, discarding small features, and clustering and linking similar points. Next, the spline curves are fitted to the linked points after the linked points are once again subdivided into several clusters according to their size. Line fitting produces smooth curves of appropriate length and curvature, similar to human-drawn strokes. Then, the curves' thickness is defined as proportional to the curvatures. Artistic shading: While oriental ink interacts with absorbent paper, ink disperses on the paper as water flows, the concentration of ink leaves bright-and-dark shades, and the residual ink disperses along the direction of the paper's fibers. We reproduce these effects by following steps. First, filtering over time is used to reproduce the ink dispersion effects when water spreads and stops, by applying blurring and sharpening filters consecutively. Next, we reproduce the bright and dark shades that arise from irregular ink concentration by stretching lightness contrast nonlinearly based on intensity values from blurred images. Finally, we reproduce the local patterns due to dispersion along the paper's fibers by using texture masks that are similar to textures of absorbent paper fibers. Composition: We compose the final results by combining stroke drawing and artistic shading, and by adding textures of absorbent papers for realistic effects. We produce the final oriental painting rendering as a composite of the results of stroke drawing and artistic shading from a single 2-D image. Figure 3 shows that strokes drawing; (a) is a map of edge by canny's edge filter, (b) is for clustering, linking sampled features and discarding some groups which have few features, (c) presents a thickness in each group, artistic shading; (d) Filtering overtime from input image, (e) contrast stretching, and (f) applying the mask of paper-fiber.	Oriental stylization with strokes and shades	NA:NA	2009
Yoon-Seok Choi:In-Kwon Lee:Bon-Ki Koo	This study describes a fully automated system for generating caricature images by using a painterly rendering method. This system transforms photos into caricature images automatically. A few similar approaches have been proposed by other researchers including [Gooch et al. 2004] and [Liang et al. 2002]. These methods, however, did not produce satisfying results, as the caricatures produced did not resemble handiwork. By simulating the brush strokes used by painters [Park et al. 2006], we reproduced the brush painting technique and incorporated it into our caricature system.	Painterly caricature maker	NA:NA:NA	2009
Tae-Joon Kim:Bochang Moon:Duksu Kim:Sung-Eui Yoon	Bounding volume hierarchies (BVHs) are widely used to accelerate the performance of various geometric and graphics applications. These applications include ray tracing, collision detection, visibility queries, dynamic simulation, and motion planning. These applications typically precompute BVHs of input models and traverse the BVHs at runtime in order to perform intersection or culling tests.	RACBVHs: random-accessible compressed bounding volume hierarchies	NA:NA:NA:NA	2009
Masashi Baba:Naoki Asada	Nowadays metallic paints are used in many situations. Although a lot of industrial products are painted because of the significant appearance, the reflection of the metallic paint is very complex and it is difficult to generate a photo-realistic image of a particular metallic paint. Recently, Rump et al. [Rump et al. 2008] proposed a method to acquire the reflectance and to generate photo-realistic images of metallic paints. They used BTF to capture and represent flakes in metallic paints, however, it is hard to capture and to store. In this paper, we propose a simple model to express the metallic paints including the sparkling effect of the flakes in metallic paints.	Reflection model of metallic paints for reflectance acquisition	NA:NA	2009
Jan-Phillip Tiesel:Christoph W. Borst	We describe an efficient single-pass rendering approach for composable 3D volumetric lenses. Composing rendering effects by intersecting multiple 3D lenses is a logical and intuitive extension of the Magic Lens metaphor and volumetric lenses. However, 3D lens composition was once considered intractable and recent multi-lens approaches require a number of passes that can grow exponentially with lens count. They generally involve substantial per-frame data structure generation or advanced techniques such as depth peeling. In contrast, we summarize a simple and effective technique that renders intersecting lenses of various shapes and effects in a single pass, does not require the maintenance of costly data structures, and can easily be incorporated into existing real-time rendering systems. It also supports more flexibility in the way complex lens effects are combined by using shade tree concepts to build composite shader programs.	Single-pass rendering of composable volumetric lens effects	NA:NA	2009
Kuntee Viriyothai:Paul Debevec	We present a technique for sampling the light probe image using variance minimization. The technique modifies median cut algorithm for light probe sampling [Debevec 2005] so that the variance of each region is minimized. The algorithm is fast, efficient, and easy to implement.	Variance minimization light probe sampling	NA:NA	2009
Christian Lipski:Christian Linz:Kai Berger:Marcus Magnor	We present an image-based rendering system to viewpoint-navigate through space and time of complex real-world, dynamic scenes. Our approach accepts unsynchronized, uncalibrated multi-video footage as input. Inexpensive, consumer-grade camcorders suffice to acquire arbitrary scenes, e.g., in the outdoors, without elaborate recording setup procedures. Instead of scene depth estimation, layer segmentation, or 3D reconstruction, our approach is based on dense image correspondences, treating view interpolation uniformly in space and time: spatial viewpoint navigation, slow motion, and freeze-and-rotate effects can all be created in the same fashion. Acquisition simplification, generalization to difficult scenes, and space-time symmetric interpolation amount to a widely applicable Virtual Video Camera system.	Virtual video camera: image-based viewpoint navigation through space and time	NA:NA:NA:NA	2009
Phoebe Coleman:Bill Elder	A 3D medical animation that visually describes the role of the Cystic Fibrosis Transmembrane conductance Regulator, which is directed by the code dictated by the Cystic Fibrosis gene in chromosome number seven.	The journey of the Cystic Fibrosis gene	NA:NA	2009
Takashi Nariya:Young Ah Seong:Tomoko Hashida:Takeshi Naemura	Global warming is one of many urgent problems we face and a positive individual attitude to the environment is necessary. The goal of our project is to raise awareness to the carbon dioxide (CO2) surrounding people which is a main factor causing global warming.	Spatio-temporal sensing and visualizing of CO2	NA:NA:NA:NA	2009
Alejandro Aguilar-Sierra	Modern technology allows to improve teaching methologies by allowing interactivity and to involve more senses in the learning process at an affordable price. In particular, visual learning has proved to be a very important way to understand otherwise elusive scientific principles. Our Visualization Laboratory for Earth Sciences is aiming to be a visual learning environment for Earth Science graduate students and at the same time to be a training platform for Computer Science undergraduates specializing in Graphics Application Programming. The importance of visual learning, specifically for Earth Sciences, has been examined previously [McGrath and Brown 2005], [Reynolds 2005].	Visualization laboratory for Earth Sciences: a multidisciplinary visual learning environment	NA	2009
Lisa Blum:Wolfgang Broll:Stefan Müller	Fascinated by a stunning variety of corals and fishes or mysterious wrecks more and more people are attracted by snorkeling and diving adventures. Virtual Reality scenarios like the virtual oceanarium [Froehlich 2000] try to satisfy this interest by allowing for discovery of underwater worlds in a riskfree and comfortable way, but a realistic feeling of diving is never achieved by virtual submarine worlds.	Augmented reality under water	NA:NA:NA	2009
Bruno Fernandes:Joaquin Fernández	Augmented Reality (AR) techniques have been applied to many application areas; however, there is still research that needs to be conducted on the best way to interact with AR content. Since hands are our main means of interaction with objects in real life, it would be natural for AR interfaces to allow free hand interaction with virtual objects. We present a system that tracks the 2D position of the user's hands on a tabletop surface, allowing the user to move, rotate and resize the virtual objects over this surface. Our implementation is based on a computer vision tracking system that processes the video stream of a single usb camera.	Bare hand interaction in tabletop augmented reality	NA:NA	2009
Hiroshi Mori:Kazuhito Shiratori:Tomoyuki Fujieda:Jun'ichi Hoshino	We propose the wellness entertainment system Versatile Training Field (VTF). In this system, we use the flexible foot interface as the input device. The system enables the user to move and jump freely in VR space by exaggerated movement corresponding to walking or jumping on the mini trampoline of the flexible foot interface. Improvements in exercise motivation and support for continuous exercise are achieved in our system, since it is possible to enjoy strolling through a virtual space, which is usually difficult to experience, by exercising on the mini trampoline without injury to the user's joints.	Flexible foot interface for versatile training field	NA:NA:NA:NA	2009
Takafumi Aoki:Hironori Mitake:Shoichi Hasegawa:Makoto Sato	We propose a new method for Symmetrical Haptic Interaction System with Virtual Creatures (VCs) in Mixed Reality. It's achieved by small and light haptic interface and Reactive VCs with touch sensations. People can touch VCs directly by fingers and watch their reaction. And VCs also can touch us directly.	Haptic ring: touching virtual creatures in mixed reality environments	NA:NA:NA:NA	2009
Daniel Makoto Tokunaga:Ricardo Nakamura:Romero Tori	The digital games industry is always looking for innovation in user experience. A new trend in this field is the use of Augmented Reality (AR) techniques for intuitive, novel game interfaces [Bernardes Jr et al. 2008]. Among the several technologies related to AR, video avatars are one of the most attractive for games, because they allow the insertion of the game player's image in the game. Furthermore, the availability of commodity stereo cameras makes it feasible to employ 3D video avatars in consumer games. However, when a non-photorealistic rendering style is required for design reasons (often the case for games), the use of a conventional video avatar, even with 3D information, results in visual inconsistencies. This work presents a new approach to enable nonphotorealistic rendering of a real-time 3D video avatar. The project builds upon a 3D video avatar system designed for teleconferencing over Internet 2 for educational purposes, extending it with this new rendering method. The expected result is a reduction in the visual and cognitive mismatch between player image and synthetic environment, allowing for a more immersive experience.	Non-photorealistic 3D video-avatar	NA:NA:NA	2009
Sheila Tejada	The mixed-reality game Robot RockStars combines a massively-interactive videogame controlled by multiple WiiGuitar players with a herd of Pleo and AIBO wireless robots as back-up singers, which can both react and affect gameplay. Attendees can affect gameplay by interacting with the robots or with mobile wireless devices. The main innovation lies in this project with the novelty of creating open-source, mixed-reality, plug-n-play tools that we build and the challenge for applications, such as the mixed-reality game Robot RockStars, to be constructed with them, allowing technology to go beyond what is currently possible for massively interactive collaboration between people and agents.	Robot rockstars: a mixed-reality game	NA	2009
Ryo Kishi:Yasuaki Kakehi:Takeshi Naemura	In this paper, we present a novel persistence of vision display named SteganoScan. SteganoScan is a stick shaped display device with a single line of LEDs that emits light in full color and shows 2D images without any screen.	SteganoScan: persistence of vision display with pixel-level visible light communication projector	NA:NA:NA	2009
Junfeng Yao:Xiaobiao Xie:Ming Zhang:Hui Zhang:Andy Ju An Wang	In recent years, virtual reality is experiencing a rapid development, which is also applied in plant morphology simulation. With a variety dynamic process of blooming, you are not only able to create a virtual landscape but to decorate a virtual space. In the internet, people can enjoy flowering anytime and anywhere to promote flower exhibitions and business. This paper focuses on the botanical characteristics of blooming process and creates a vivid effect based on Bezier curves and surfaces theory.	A 3-D flowering simulation based on botany characteristics and random generation algorithm	NA:NA:NA:NA:NA	2010
Shunsuke Matsuyama:Hironori Mitake:Shoichi Hasegawa	Recent progress of interactive techniques brought intuitive and physical interaction with characters in entertainment field such as console games. Conventional motion generation method requires preparing an enormous number of motion patterns in order to implement various reactions of characters. Therefore, for an easy way of implementation of various reactions, virtual creatures with sensorimotor models will become useful. Virtual creatures [Mitake et al. 2007] are characters with sensorimotor models generate motion in physics simulation environment.	A development environment for designing interactive characters with sensorimotor models	NA:NA:NA	2010
Aria Shahingohar:Roy Eagleson	The simulation of needle insertion is an important research area that has many applications in robotic and image guided brachytherapy cancer treatment, biopsies, and neurosurgery. Modeling of soft tissue plays an important role in the needle insertion simulation, but the use of Finite Element Method is complicated due to the need for remeshing in the neighbourhood of the needle tip. We are proposing to use a meshfree method for the tissue deformation modeling, in which new tissue nodes are added on the needle shaft as the needle is inserted into the tissue. In addition, we have utilized Nvidia's CUDA technology to accelerate the methods used in our framework.	A framework for GPU accelerated needle insertion simulation using meshfree methods	NA:NA	2010
Hirofumi Suda:Kentaro Yamanaka:Shigeo Morishima	We propose a skinning technique to improve expressive power of Skeleton Subspace Deformation (SSD) by adding the influence of the shape of skeletons to the deformation result by post-processing.	A skinning technique considering the shape of human skeletons	NA:NA:NA	2010
Michael Berger:Gregor Hofer:Hiroshi Shimodaira	Facial animation is difficult to do convincingly. The movements of the face are complex and subtle, and we are innately attuned to faces. It is particularly difficult and labor-intensive to accurately synchronize faces with speech. A technology-based solution to this problem is automated facial animation. There are various ways to automate facial animation, each of which drives a face from some input sequence. In performance-driven animation, the input sequence may be either facial motion capture or video of a face. In automatic lip-syncing, the input is audio (and possibly a text transcript), resulting in facial animation synchronized with that audio. In audio-visual text-to-speech synthesis (AVTTS), only text is input, and synchronous auditory and visual speech are synthesized.	Carnival: a modular framework for automated facial animation	NA:NA:NA	2010
D. Kravtsov:O. Fryazinov:V. Adzhiev:A. Pasko:P. Comninos	Polygonal models are widely used in computer animation. Static polygonal models are commonly animated using an underlying skeleton controlling the deformation of the mesh. This technique known as skeletal animation allows the artist to produce complex animation sequences in a relatively easy way. However, performing complex transitions between arbitrary animated meshes remains a challenging problem. There is a set of established techniques to perform metamorphosis (3D morphing) between static 3D meshes [Lazarus and Verroust 1998], but most of these can not be easily applied to animated meshes. The approach presented in this poster allows us to produce with great ease metamorphosing transitions between animated meshes of arbitrary topology using polygonal functional hybrids [Kravtsov et al. 2010].	Controlled metamorphosis of animated meshes using polygonal-functional hybrids	NA:NA:NA:NA:NA	2010
Hiroaki Gohara:Shiori Sugimoto:Shigeo Morishima	In anime production, some key-frames are drawn by artist precisely and then a great number of in-betweening frames are drawn by assistants' hands. However, it is seriously time-consuming and skilled work to draw many characters especially including face rotation. In this paper, we propose an automatic in-betweening technique for rotating face of hand drawn character only from a front image and a diagonal image (Fig.1). Baxter [2009] represented generating in-betweening using image morphing technique. However, their approach doesn't consider reflecting the artist's style and touch. Accordingly, we represent reflecting style and touch using morphing technique trained by his own database and introduced especially to generate a rotational in-betweening faces. This database contains center of gravity of each part (right eye, left eye, nose, mouth, eyebrow) and the contours on the facial image.	Data driven in-betweening for hand drawn rotating face	NA:NA:NA	2010
Michael J. Gourlay	Modeling continuous media such as fluids remains an elusive goal for interactive simulations. Fluids are particularly challenging because of the complexity imparted by the non-linear equations of motion, and the difficulty in creating stable simulations that retain spatial detail.	Fluid-body simulations using vortex particle operations	NA	2010
Dung A. Nguyen:Zhaoyang Wang	A 3D motion capturing and reconstructing system at high speed is presented. The system utilizes the fringe projection technique with one modified DLP projector, one camera and a computing unit to provide real-time reconstruction of forty-two 3D frames per second with the relative accuracy of 1/5000.	High speed 3D shape and motion capturing system	NA:NA	2010
Meredith McLendon:Ann McNamara:Tim McLaughlin:Ravindra Dwivedi	A digital creature's performance can be thought of as a combination of specifically defined motion and form; a combination that allows the viewer to comprehend the creature's action and intent. Computer graphics offers a variety of methods for defining motion including key-frame animation, data-driven action, rule-based and physically-based motion. However, all of these methods can be complex and time-consuming to implement. Essentially, most computer animation methods force the animator to think about motion at a low-level of abstraction. To create animation tools that simplify the process of creating expressive motion, we need to allow animators to work at a high-level of abstraction. We need determine the minimal elements of form and motion that visually communicate a maximal amount of information about an actor's identity or intentions. By attaching small reflective objects to joint pivot locations and recording at high contrast [Johansson 1973] developed a method for isolating motion from form as a collection of particles, now commonly known as a Point-Light Display (PLD). Manipulating this minimized visual information can even affect the perceived gender of PLD walkers. Cutting [1978] found that exaggerating the movement of points representing the hips and shoulders can bias gender recognition. The goal of our study was to investigate whether viewers use similar visual information to recognize expressive characteristics in animal motion PLDs as when viewing full representations and discover how it might be possible to use that visual information to influence the viewer's perception.	Lions and tigers and bears: investigating cues for expressive creature motion	NA:NA:NA:NA	2010
Gregor Hofer:Korin Richmond:Michael Berger	Talking computer animated characters are a common sight in video games and movies. Although doing the mouth animation by hand gives the best results it is not always feasible because of cost or time constraints. Therefore producing lip animation automatically is highly desirable. The problem can therefore be phrased as mapping from speech to lip animation or in other words as an acoustic inversion. In our work we propose a solution that takes a sequence of input frames of speech and maps it directly to an output sequence of animation frames. The key point is that there is no need for phonemes or visemes which cuts one step in the usual lip synchronization process.	Lip synchronization by acoustic inversion	NA:NA:NA	2010
Sriranjan Rasakatla:K. Madhava Krishna:Bipin Indurkhya	The Modular Legged robotic system [1] "Mod-Leg" presented here has been bio-inspired from a Snake's vertebrae and a caterpillar's legged structure. The system can be configured to a 4-legged robotic dog, a hexapod, a caterpillar and a Snake robot. This robot's novel design achieves compliance to the terrain using a combination of legs and electronically actuated universal spine. A unique simulator has been designed for this purpose. Some of the things we learned while developing this robotic system have been presented below.	"Mod-Leg" a modular legged robotic system	NA:NA:NA	2010
Adriana Schulz:Marcelo Cicconet:Luiz Velho	The one-way road leading music to motion has many manifestations. Choreographers build dance movements to match music features, subway passengers tap their feet following their iPod song hits, and the number one rule at any party is: if you wanna dance, dance to the music!	Motion scoring	NA:NA:NA	2010
Takeshi Miura:Kazutaka Mitobe:Takaaki Kaiga:Takashi Yukawa:Toshiyuki Taniguchi:Hideo Tamamoto:Noboru Yoshimura	It has been recognized that a technique to divide a raw motion-capture data stream of a dance into segments on the time axis is needed [Sonoda 2008]. In particular, the extraction of the higher-level information such as the hierarchical segmentation-structure is a subject of growing interest at the present time. In this study, the authors attempt to develop a method to segment dance motion in a multi-level style, namely in a hierarchical fashion.	Multi-level segmentation of dance motion by piecewise regression	NA:NA:NA:NA:NA:NA:NA	2010
Shoji Kunitomo:Shinsuke Nakamura:Shigeo Morishima	Realistic drape and motion of virtual clothing is now possible by using an up-to-date cloth simulator, but it is even difficult and time consuming to adjust and tune many parameters to achieve an authentic looking of a real particular fabric. Bhat et al. [2003] proposed a way to estimate the parameters from the video data of real fabrics. However, this projects structured light patterns on the fabrics, so it might not be possible to estimate the accurate value of the parameters if fabrics have colors and textures. In addition to the structured light patterns, they use a motion capture system to track how the fabrics move. In this paper, we will introduce a new method using only a motion capture system by attaching a few markers on fabric surface without any other devices. Moreover, animators can easily estimate the parameters of many kinds of fabrics with this method. Authentic looking and motion of simulated fabrics are realized by minimizing error function between captured motion data and synthetic motion considering both static and dynamic cloth features.	Optimization of cloth simulation parameters by considering static and dynamic features	NA:NA:NA	2010
Fiona Rivera:Phil Watten:Patrick Holroyd:Felix Beacher:Katerina Mania:Hugo Critchley	This research concentrates on providing high fidelity animation, only achievable with offline rendering solutions, for interactive fMRI-based experiments. Virtual characters are well established within the film, game and research worlds, yet much remains to be learned about which design, stylistic or behavioural factors combine to make a believable character. The definition of believability depends on context. When designing and implementing characters for entertainment, the concern is making believable characters that the audience will engage with. When using virtual characters in experiments, the aim is to create characters and synthetic spaces that people respond to in a similar manner to their real world counterparts. Research has shown that users show empathy for virtual characters. However, uncanny valley effects -- ie dips in user impressions -- can arise: behavioural fidelity expectations increase alongside increases in visual fidelity and vice versa. Often, characters used within virtual environments tend to be of fairly low fidelity due to technological constraints including rendering in real-time (Garau et al. 2003). This problem is addressed here by using non-linear playback and compositing of pre-rendered high fidelity sequences.	Real-time compositing framework for interactive stereo fMRI displays	NA:NA:NA:NA:NA:NA	2010
Cheng-Te Li:Hsun-Ping Hsieh:Tsung-Ting Kuo:Shou-De Lin	The goal of crowd simulation is to produce potential collective behaviors by simulating the movement process of a number of characters or agents. Some famous models are proposed to simulate crowd, including social force (e.g. [Helbing 2000]), cellular automata (e.g. [Chenny 2004]), and rule-based models (e.g. [Reynolds 1987]). Others use physiological (e.g. locomotion, energy level) and psychological (e.g. impatience, personality attributes) traits of agents to trigger heterogeneous behaviors [Pelechano 2007]. However, existing approaches do not consider the real-world social interactions among agents, and thus are unable to produce social-dependent scenarios. In this work, we propose to leverage the underlying social network, which captures social relationships among agents, for crowd simulation. A novel social-network-based framework, SocioCrowd, is developed (figure 1(a)) shows the virtual world). Based on SocioCrowd, we simulate three social-based scenarios, including community-guided flocking, following leading persons, and spatio-social information spreading. They display certain real-world social behaviors which are hardly modeled by existing methods. To lift the performance, our SocioCrowd is implemented by pure Java with GPU programming in ways of GSGL and JCUDA.	SocioCrowd: a social-network-based framework for crowd simulation	NA:NA:NA:NA	2010
Nobuhiko Mukai:Kentaro Ito:Masashi Nakagawa:Makoto Kosugi	One of the most challenging issues of computer graphics is to represent the behavior of fluid. Visualizing the fluid behavior requires to solve Navier-Stokes equations, which take huge amount of time so that some researches use many super computers for the simulation, and others utilize the GPU performance. The common fluid is Newtonian that can be described by a single constant value of viscosity, and there are many researches related to Newtonian. On the other hand, there is another type of fluid called non-Newtonian that cannot be described easily, and one of non-Newtonians is viscoelactic fluid. Viscoelastic fluid has the characteristics of both viscosity of fluid and elasticity of solid, and it is difficult to represent the behavior of viscoelastic fluid. [Goktekin et al. 2004] represented the behavior of viscoelastic fluid. His technique is based on Eulerian methods and added elastic terms to Navier-stokes equations, which govern fluid behavior. [Clavet et al. 2005] used particle method for representing fluid behavior. Particle method can represent fine behavior of the fluid such as rain drops, fountains, clay manipulation. Their researches could visualize many types of behavior of viscoelastic fluid, however, they cannot represent the spinnability, which has three characteristics: 1) it stretches very thin as if it is a string, 2) the radius is getting smaller gradually from the both ends and the center part has the least radius, and 3) it shrinks rapidly as if it is a rubber.	Spinnability simulation of viscoelastic fluid	NA:NA:NA:NA	2010
Daniele Federico:Damien Fagnou:Tom Reed	The creation of animation clips and the tweaking of existing character animation is often a tedious, time-consuming task, especially in large-scale CG productions. Traditionally these tasks were achieved by animators manually changing multiple keyframe values for all the relevant animation rig controls. Starting with the idea that a single animated object (e.g. a rig control) essentially defines a time-varying curve in 3D space - where the control points are defined by the keyframe values of the translation channels - we introduce a modeling approach for deforming these conceptual 3D curves. We will talk about our current implementation based on FFDs (Free Form Deformations) as described in [Sederberg and Parry 1986], but we strongly believe the same approach can have many other usages (e.g. modeling tools, collisions and obstacle avoidance).	Warping the space around an animated object	NA:NA:NA	2010
Paul D. Solt	Creating digital artwork requires a lot of time, talent, and effort from artists and programmers. It takes artists hours to design pleasing artwork and programmers even more time as they develop and debug complex graphics shaders. One way to aid in the creation of complex art is to use evolutionary computing called genetic programming. Genetic programming can be used to create mathematical expressions that can be rendered as an image. The image can be used as a texture in a 3D scene or as a starting point for additional artwork.	Artwork evolution	NA	2010
Young-Mi Kim:Jong-Soo Choi	This paper is about the study on an artwork, a black-and white drawing that has been expressed through a digital algorithm. Black-white drawings were popular during the Chosun era (1392--1910) reigned by kings and officials. The Oriental fine art, pursuing harmony with nature, is expressed in a moderate and restrained way, hence anyone would find it very soft and thus readily acceptable. Unlike the western paintings that fill the canvus to the very full, the oriental paintings treat even the blank space as a part making up a balanced painting. This artwork features Daegum, the decent traditional musical instrument which used to be played in loyal palaces or guest rooms of prestigious officials' residences, and a bamboo which was a frequent motive of gentlemen's paintings in the past. Daegum and the bamboo, expressed in a modern style in this work, make people appreciate the life that is full and rich. So, one can say they have been used here to make this "well-being art."	Breathe brush	NA:NA	2010
Daniel Tauber	I present a custom software for typing experiences that opposed to linear word processing renders visible individual writing styles on a personal computer using responsive typography in order to achieve a unique and personal representation of text analogous to handwriting.	Digital writing ductus: a visual representation of individual writing styles	NA	2010
Young-Mi Kim:Jong-Soo Choi	During the old days in the orient, people used to wipe cymbidium leaves or painted cymbidium for mental training by having a cymbidium always by their side. Through the act of wiping cymbidium leaves with utmost care, a cymbidium instilled with ancient philosophical ideas is visualized, and just as God breathed life into human nostrils and created a living life form, if a breath is breathed into a cymbidium flower, a cymbidium flower with an excellent fragrance is visualized. This work is an interactive visualization of an oriental cymbidium using modern technology which our oriental ancestors painted for mental training.	Ink-and-wash painting oriental cymbidium drawn with the tip of the fingers	NA:NA	2010
Özge Samanci:Anuj Tewari	In the digital era, the comics medium is transported from print to computer screen. Current digital comics (web comics or online comics) are confined to computer screen and use the affordances of digital medium in a limited way. GPS Comics: Seeing thru Walls is a GPS based comics story that expands the comic canvas and explores the idea of location-based comics. In Seeing thru Walls, in order to receive the meaning in a comic frame the player must experience a sensory detail (a smell, sound, breeze or an object) in her surroundings in the physical world. The concept of location-based comics is an unexplored idea and gives artists new meaning making strategies.	GPS comics: seeing thru walls	NA:NA	2010
Cem Sina Cetin	Graphic artists have a wide variety of applications to use for digital painting. Although each application has its own solution to enhance the user experience, most of them rely on the same standard feature; a single brush, which is completely dependent on user input for location. Although this is required for a fully controlled painting process, making small changes on this feature yields unpredictable results. My proposal for an alternate brush paradigm is using multiple brushes (as seen in the application "PD Particles"), which are not completely under control but rather moving within trajectories with random deviations, simultaneously. The trajectories are defined by controllable parameters and the user input. Since the rate of obedience to user input is dependant on the parameters, users can define the rate of deviation and thus switch between finger painting and generative painting, without changing the set of tools.	Musophobia	NA	2010
Tomoko Hashida:Yasuaki Kakehi:Takeshi Naemura	Drawing tools using digital technology can stimulate creativity in people. For example, the Wacky Brush tool in KidPix can produce effects (such as a line of dripping paint or a line of shapes) that cannot be obtained using ordinary paper and brushes [Hickman 1991]. This feature makes it easy for people to draw pictures having a combination of patterns. Such software, however, has so far been used only with electronic displays such as LCDs and PDPs. In this paper, we propose a mechanism that would allow the user to draw such pictures while using paper as a canvas instead of electronic displays. With this mechanism, a variety of patterns can be made to appear along lines traced out by the user by moving an electronic paint brush over paper. The advantages of using paper in this way include a high degree of freedom in shape and size as well as portability.	Photochromic canvas drawing with patterned light	NA:NA:NA	2010
Russell Deaton	Code (computer software and the technologies that it enables) is changing fundamentally how human beings interact with each other, and think about themselves and the world. It is a medium through which artists are increasingly expressing themselves. Code can serve as the tool which the artist uses to produce their work, or more interestingly, the artist takes the role of programmer and designs and implements an algorithm that generates the work of art. Thus, the artist's ideas are filtered and constrained through the filter of code, whose limitations and capabilities shape and inform the consequent artistic vision. For example, Casey Reas through his {Software} Structures takes verbal descriptions of processes to produce visual components and turns them into programs[Reas 2009]. In what follows, a Turing-universal model of some natural and manmade phenomena, Self-Assembly, is adapted to the automatic creation of visual art.	Self-assembled art	NA	2010
Shiho Hirayama:Yasuaki Kakehi	From childhood, we often play with bubbles. We find various aesthetic elements in a series of actions of soap bubbles: appearing, expanding, floating, bursting and disappearing. This time, we utilize the movements of soap bubbles as a pixel of an image and propose a novel interactive substantial display named "Shaboned Display." (see Figure 1)	Shaboned display: an interactive substantial display using soap bubbles	NA:NA	2010
Akira Nakayasu:Kiyoshi Tomimatsu	Recently there has been demand for display equipment capable of advanced expressions in spatial design. For example, there is the Adobe Interactive Wall at Union Square (New York City, 2007), and the Zero Energy Media Wall of greenPIX (Beijing, 2008) using LEDs placed on the whole facade. The simple display of information contents is becoming insufficient, and more appealing spatial designs combining information content with interactive art expression are becoming more important. In this paper, we propose a shape memory alloy motion display (SMD), a novel piece of display equipment taking advantage of the existence of an actual object. Then, we introduce an interactive art work plant based on SMD technology.	SMA motion display: plant	NA:NA	2010
Junfeng Yao:Xiaobiao Xie:Fengchun Lin:Xufa Ji:Xiaoyan Lin:Andy Ju An Wang	Recently, Xiamen University and Flying Information Technology Co., Ltd worked together and completed the development of The Online Custom-Built WEB3D Middleware System for Arts and Crafts, which will perform as a product 3D design and display center, its main features include the product demonstration background change, 3-Dimension design, 3-Dimension product display, product component reorganization and product material replacement.	The online customer-built WEB3D middleware system for arts and crafts	NA:NA:NA:NA:NA:NA	2010
Yuka Kubo:Hiroyuki Shindo:Koichi Hirota	For more than 1,300 years, beauty portraits have continued to be painted in Japan and virtually all have been stylized to a very unrealistic extent.	The Orikao Method: 3D scene reconstruction from Japanese beauty portraits	NA:NA:NA	2010
Robert B. Trempe, Jr	"24X7ATPHL: Codify" is an investigation into the novel usage of time-based animation software and procedural modeling as a method for visualizing time-based quantitative data via the construction of a qualitative, two-dimensional rendering. Treated as an experiment in the extrusion and aggregation of time-based qualitative instances, "24X7ATPHL: Codify" slows down and composites the accumulated information of seven days traffic (customer pickup and drop off) at an international airport; visualizing information in such a way as to not only notate the generations and changes in patterns, but also to show the beauty that can be found in data while unlocking the emergent potential for design. "Codify" makes use of the accumulation of NURBS geometries as a methodology for understanding the specific conditions of movement created by the interaction of existing architecture and user, the results of which are currently being used to develop everything from the design of several furniture pieces to that of a new cladding system for the Philadelphia International Airport.	[email protected]: Codify	NA	2010
Amy Martin:Wendy Ju	Bloom uses the metaphor of a desktop plant to remove task management from the already overloaded inbox and into a more human environment. When tasks in the inbox are starred, the email information is sent to an external touchscreen that then grows a flower for that specific task. The flower is activated on touch and the text of the email is displayed. Plucking the flower---touching, holding, then flicking the flower---removes that item from the task list. A large number of tools exist for managing tasks. Bloom is different in that it uses an organic, passive metaphor for visual display. Instead of having a series of piling text, whether in physical or digital form, Bloom does not visually overwhelm. A single task is as visually appealing as fifty. Additionally, although numerous email visualizations also exist much of this work has to do with overall inbox visualization and/or the display of relationships [1]. There is also precedence in using metaphor to visualize email as seen in Kjen Wilkens' Mail Garden. Bloom is distinct in both its focus on task management and our intent at full integration with existing email systems.	Bloom: an interactive, organic visualization of starred emails	NA:NA	2010
Yuki Igarashi	Line stone decoration is popular with young people. They enjoy applying line stone decorations to personal goods, such as notebook PCs, mobile phones and digital cameras. However, novices often find line stone decorations difficult to design, as the user must consider stroke length, stone width, and stone spacing. Hence, many people employ off-the-shelf design sheets or have their items decorated by an in-store professional. We have developed an interactive designing editor for line stone decoration. The user interactively draws freeform strokes on the canvas, as shown in Figure 1. The system then automatically generates a virtual line stone image (Fig. 1(b)), in which none of the stones overlap. Various off-line methods have been proposed for designing such decorations (e.g., tile mosaics [Hausner 2001]). Using our system, the user can create the designs interactively at the computer. Our system also creates a physical stencil pattern to help novice users to construct real line stone (Fig. 1 (c), (d)).	DECO: a designing editor for line stone decoration	NA	2010
Yuki Igarashi:Hiromasa Suzuki	Shape-matching toys are popular items for infants, and consist of boxes with many holes in different shapes along with corresponding blocks of the same shapes. To play with the toy, an infant finds and inserts a block matching the shape of a particular hole. It is difficult to design new shape-matching toys based on existing blocks. We assume that the user performs such design as shown in Fig. 1 (e) based on existing building blocks like those shown in Fig. 1 (a). The construction of the toy body can be roughly divided into three steps: gather the parts, lay them out on a wooden board and trace them using a pencil, and saw the wooden board. This manual method is straightforward, but errors cannot be rectified and it is also unsuitable for mass production. Accordingly, we propose the use of a laser cutter (e.g., Commax Laser System) or a cutting plotter (e.g., Craft ROBO). Today, services are available that allow the user to send a vector dataset to a company and have the corresponding wooden board returned to them.	Designing a new toy to fit other toy pieces: a shape-matching toy design based on existing building blocks	NA:NA	2010
Koh Sueda:Kazushi Kotani:Jun Rekimoto	Easy-Tagging Cam (or ETC) is a digital image recording system equipped with multiple shutter buttons. This system enables users to capture and tag photographs simultaneously. This function allows the user to be set free from tagging tasks. The users enable to develop re-useable photo storage continuously. This system also utilizes a life-log system thereby aiding the easy retrieval of information.	Easy-Tagging Cam: using social tagging to augment memory	NA:NA:NA	2010
Hiroki Yamada	Flower arrangement is one of famous traditional arts in Japan, and being enjoyed across the world now. People have been created the atmosphere in a room or represented one's mind by flower arrangement.	Floral melody: flower arrangement as music interface	NA	2010
Robert B. Trempe, Jr	"How Would You Like To Live" is a graphical articulation manifest from user sensory "wishes" supplied by an architectural client building a new home. It was crafted to help the designer in under-standing the needs of the client through emergent, patterned, non 1:1 results. Through the use of a parametrically-driven procedural network with parametric inputs supplied by the client, a graphical "depiction" of the user's hopes, dreams, and senses towards the occupation of domestic space was generated.	How would you like to live?	NA	2010
Ming Tabg:Jonathon Anderson	The name "Math-Morph" combines the notion of "mathematic" with the notion of "morphology. This project focuses on the study of "mathematic" as an embedded variability of spatial arrangement with procedural model. The influence of digital media and information technology on architectural education and practice is increasingly evident. Digital technology has reconditioned the design process that establishes new processes and techniques of fabrication. This reconditioning has influenced how we operate as architects. Today, architectural design and building construction are increasingly aided by and dependent on digital technology. These technologies allow architects to foresee the appearance and predict the performance of proposed buildings. Mathmorph proposes an interdisciplinary research in digital fabrication of unconventional 3D forms on a conceptual design level in order to explore their features in interacting with people and their potentials of being used as architectural forms. It describes an experimental approach which facilitates 3D form generation, visualization and fabrication.	Mathmorph	NA:NA	2010
Rebecca Findlay	Pixel Pusher represents a humorous symbol for all people who work with pixels on a daily basis such as teachers, students, or some other creative in the field relating them to the rigorous, time-consuming labor of a construction worker.	Rebecca Findlay's Pixel Pusher: poster abstract	NA	2010
Sophia Sobers	This architectural system explores the idea of using a parametric interface that reacts and changes based on user input while reproducing a series of affects (defined in psychology as the experience of emotion or feeling) on the user. The affects are predetermined, based on real world examples, and the system is designed in accordance. The overall premise for this project is to explore how tangible affects can be represented through parameters where the results are only visualized through the computer.	Reactive architecture	NA	2010
Kumiko Kushiyama:Tetsuaki Baba:Kouki Doi:Shinji Sasada	"Thermo-Pict neo" is a design apparatus produced by applying temperature visualization technology linked to an information display with the use of a thermograph sheet. Thermography is used to visualize the surface temperature of objects through their depiction as colors. This technology has been used primarily in the medical and research fields. Thermography display colors come in a wide range of hues and brightness that enables quick visualization of any object's surface temperature distribution. Use of this technology will be attempted as a tool in the production of design displays. [Fig.1]	Temperature design display device to use peltier elements and liquid crystal thermograph sheet: "Thermo-Pict neo"	NA:NA:NA:NA	2010
Hiroki Yamada	In this paper, the authors propose Tiny Dreamy Stories, which uses a traditional paper book as an interface to experience digital contents, so that it can keep the affordances of paper books while adding electronic augmentation. The aim of this study is to achieve both highly computer-supported contents and natural interface, e.g., highly efficient combination of physical and digital world. With Tiny Dreamy Stories, every person (especially who is not good at operating computers) can enjoy rich digital contents just by flipping pages.	Tiny Dreamy Stories: interactive paper book capable of changing the storylines	NA	2010
Jieun Kim:Carole Bouchard:Jean-Francois Omhover:Ameziane Aoussat	The TRENDS European project aimed at developing an image and text retrieval engine in order to support the activity of the designers in the early stages of their design process [TRENS 2007]. The study of the designers' activity has led us to the production of an image database in which designers will find inspirational material. A content-based image search engine has been elaborated, starting from recommendations taken from the methodology employed by the designers in their activity, to end with a complete system incorporating image retrieval technologies and various tools to extract relevant information from these images.	TRENDS: a content-based information retrieval system for designers	NA:NA:NA:NA	2010
Mary Huang	The design of typefaces is founded upon principles from the days of metal type, when creating individual fonts was a laborious process and constrained by physical requirements. Most digital type design follows those same conventions, even though fonts are now drawn with vectors and pixels. Fonts are still largely based on historical references and are created in the context of publishing.	TYPEFACE	NA	2010
Hwan-Soo Yoo:Seong-Whan Kim	We propose an Agritainment (agriculture with entertainment) framework, where users can learn how to cultivate plants and to breed livestock. To make an agricultural training joyful, we implement 3D collaborative space for training agricultural experience, which transforms monotonous training experience into realistic experience. Technical details include (1) multi-user networking, (2) realistic plant grow-up modeling, and (3) story-telling approach for immersive experience.	Agritainment: 3D collaborative space for training agricultural experience with entertainment elements	NA:NA	2010
Liliana Vega:Griselda Ledezma:Anayeli Hidalgo:Eduardo Ruiz:Omar Pinto:Ricardo Quintero:Leopoldo Zepeda	Recent results in educational research suggest the benefits of creating learning atmospheres in which students actively engage with the material as well as other classmates [1]. The idea of creating such an environment using a multiplayer mobile game represents a natural extension of the ubiquitous audio guides offered by most museums today.	Basic elements on game design for interactive museum exhibitions	NA:NA:NA:NA:NA:NA:NA	2010
David Bartle:Sam Rossoff:David Whittaker:Bruce Gooch:Kim Kerns:Jenny MacSween	Therapies that help restore abilities in individuals with brain damage are being investigated to help individuals with FAS. These methods focus on rehabilitation and exercises for the brain which improve specific cognitive capacities. We present Cognitive Carnival, a computer game therapy based on cognitive exercises, designed to improve the child's motivation and engagement of the tasks. Three minigames were developed, each based on improving one of three cognitive prinicples: executive function, continuous performance, and working memory. These minigames will be used in controlled therapy sessions with neuropsychologists for children with FAS to determine their effectiveness as a rehabilitative tool. Fetal Alcohol Syndrome (FAS) is a disorder that is caused by the ingestion of alcohol during pregnancy. Alcohol is a teratogen (substance that is toxic to the developing brain) and can result in abnormal brain development (brain damage). Children with FAS are faced with numerous obstacles, including significant problems with executive functions, attention, memory, and language. These conditions impede children with FAS from succeeding in school and living normal lives. There is estimated to be 0.5 to 2.0 children diagnosed with FAS per 1,000 births in the United States during the 1980's and 1990's [May and Gossage 2001]. It especially prevalent in remote communities. There is no cure. However, therapies that help restore abilities in individuals with brain damage are being explored to help individuals with FAS. These methods focus on rehabilitiation by means of an intervention by psychological professionals. The therapies are able to leverage the brain's plasticity to improve cognitive function [Neu 2002]. While adult brains show low levels of plasticity, children have more neurons and their brains continue to grow into their early 20's. Consequently, neurogenesis can be leveraged by supervised mental exercise. Classical therapy involves a trained therapist visiting local school, often times for a single child, to administer the therapy. The therapy itself consists of a set of exercises which the child preforms. This model of therapy, while effective, is inefficient and often times impractical for many areas. Additionally, the therapy has no builtin reward system and often times the therapist will offer the child candy or a small prize which provides little engagement with the tasks themselves. The minigames are intended for a controlled environment where a child with FAS is supervised by a neuroscientist. Over a course of weeks, the child will play each minigame, progressing through the difficulty levels as their abilities increase. A game-based therapy has multiple advantages over traditional exercises. Games tend to be more engaging than paper exercises. They also can accommodate built-in reward and motivation systems, instead of requiring the alternative of real-world incentives as the sole motivation for completing the tasks. Possibly the the most significant advantage is the ability to easily distribute the system using the Internet. This allows it to easily reach remote areas, where FAS is prevalent. Our therapy targeted three cognitive abilities: continuous performance, working memory, and executive function. Four minigames were created, each embodying at least one of the fundamental cognitive abilities affected by FAS. We decided to divide the therapy into minigames as each minigame could focus primarily on a single cognitive principle. This narrowed focus allowed neuropsychologists to measure progress on particular abilities. This also allowed for separate starting ability parameters for the children. The minigames focused on three cognitive principles typically impaired by FAS: continuous performance, working memory, and executive function. Continuous performance is the ability to sustain a consistent focus to an ongoing task continuing over an extended time period. Children without this ability may be at a major disadvantage in learning settings to their healthy counterparts. Working memory is the ability to temporarily store and manipulate information. It is an important basis for complex cognitive processes. Executive function is the ability to plan, problem solve, and make decisions. Each minigame has built-in difficulty levels, and the ability to manually adjust parameters for an individually-tailored therapy. Levels are then subdivided into repeated trials of the same parameters. The user progresses through each trial, receiving positive and negative feedback as appropriate, according to his or her performance. Upon the completion of a session, the user is presented with his or her results and a progress plot of their recent trials.	Cognitive games as therapy for children with FAS	NA:NA:NA:NA:NA:NA	2010
James R. Geraci:Erek R. Speed	Our work focuses on the area of using a high level language to improve program productivity, performance and portability. In general, this has been an area of intense research. There are a number of previous efforts including ZPL [Chamberlain and et al 2004], X10/Fortress/Chapel from IBM/SUN/Cray [Weiland 2007], Intel's CT/RapidMind [McCool 2006] and parallel VSIPL++ [Lebak and et al 2005] to name a few. However, while these languages do great things in simplifying parallel implementation of code, extensions beyond that are limited. The primary exception to this is VSIPL++ which implements several high level functions useful to the signal processing community. While most of these languages can be used to implement graphics or game related algorithms if necessary, none of them attempt to provide a platform that makes such development particularly easy. On the other hand, high level engines such as Renderman and Unreal provide the wanted abstractions but with little or no guarantees about extensibility, portability, or parallel performance. Our research focuses on adapting the parallel VSIPL++ API from the signal processing community to the graphics and game development environment.	Improving program productivity, performance and portability through a high level language for graphics and game development	NA:NA	2010
Madhuri Koushik:Eun Jung Lee:Laura Pieroni:Emily Sun:Chun-Wei Yeh	The goal of the project was to design an integrated system for the California Academy of Sciences that combined new technology (iPads in our case) with a social-networking based website to promote educational learning geared towards middle-school students. The experience begins when museum visitors create profiles on the California Academy of Sciences website. Initially they are able to personalize a limited number of characteristics of their avatars. Once they visit the museum, they play mini-games on iPad kiosks to accumulate points on their accounts. We developed five different educational mini-games, focusing on the areas of climate change, astronomy, evolution, and the food chain. The points gained on the iPad mini-games can then be redeemed at home by returning to the California Academy of Sciences website. Accessing the website from home allows the user to further personalize an avatar, learn more facts, and compare their scores on the mini-games and their avatar with those of their peers. Points can be redeemed to upgrade the avatar's available attributes and attires. By extending the museum experience to home and through increased level of social network interaction, learning is reinforced over a longer period of time. In a user testing session with 50 students of the target demographic age, 72% said they would be interested in redeeming their points online. They also had the opportunity to write a fact that they learned from the game. 67% of students that played the camouflage game (n=12) were able to state a fact that they learned from the game. Utilizing new technologies like the iPad is an opportunity to increase the number of initial users that create profiles on a new educational socialnetworking game website. Future research can focus on determining the extent of the educational effect of a system like this. For example, how might the experience of learning at the museum, enforcing that learning at home, and repeating through return visits to the website affect retention of facts?	iPad mini-games connected to an educational social networking website	NA:NA:NA:NA:NA	2010
Tetsuaki Baba:Kumiko Kushiyama:Kouki Doi	Today, many researchers reports studies about haptic, tactile or tangible art and entertainment. Particularly about temperature sensation, few interaction system has ever been presented because of it does not have good responsiveness. In this study, we shall design the video game interaction system that uses temperature sensation to users. First of all we investigate the relation of the rapidity of temperature change and user response time by using prototyped controller. Our game controller can offer temperature to users dynamically according to game situations. As a result, It was able to propose a basis of interaction system to take the temperate sensation to the game interaction.	ThermoGame: video game interaction system that offers dynamic temperature sensation to users	NA:NA:NA	2010
Kotaro Takahashi:Tomohito Yamamoto	For providing high presence, many kinds of display system have been developed [Hughes et al. 2005]. Typical examples are 3-dimensional display and multi-channel surround speaker system. Moreover, 3D movie such as "Avatar" or 3D TV have been brought to the market. However, these high realistic displays for visual, sound, or both, were usually composed of fixed and expensive equipment.	3D audio-visual display using mobile devices	NA:NA	2010
Mike Roberts:Mario Costa Sousa:Joseph Ross Mitchell	We present a novel GPU level set segmentation algorithm that is both work-efficient and step-efficient. Our algorithm has O(log n) step-complexity, in contrast to previous GPU algorithms [Lefohn et al. 2004; Jeong et al. 2009] which have O(n) step-complexity. Moreover our algorithm limits the active computational domain to the minimal set of changing elements by examining both the temporal and spatial derivatives of the level set field. We apply our algorithm to 3D medical images (Figure 1) and demonstrate that our algorithm reduces the total number of processed level set field elements by 16x and is 14x faster than previous GPU algorithms with no reduction in segmentation accuracy.	A work-efficient GPU algorithm for level set segmentation	NA:NA:NA	2010
Budirijanto Purnomo:Norman Rubin:Michael Houston	Modern GPUs have been shown to be highly efficient machines for data-parallel applications such as graphics, image, video processing, or physical simulation applications. For example, a single ATI Radeon™ HD 5870 GPU has a theoretical peak of 2.72 teraflops (1012 floating-point operations per second) with a video memory bandwidth of 153.6 GB/s. While it is not difficult to port CPU algorithms to run on GPUs, it is extremely challenging to optimize the algorithms to achieve teraflops performance on GPUs. Only a select few expert engineers with the application domain expertise, a deep understanding of the modern GPU architecture, and an intimate knowledge of shader compiler optimization can program GPUs close to their optimal capabilities. Many developers are content with several folds of improvements rather than one or several orders of magnitude acceleration compared to their optimized CPU implementations.	ATI Stream Profiler: a tool to optimize an OpenCL kernel on ATI Radeon GPUs	NA:NA:NA	2010
Yoshiharu Momonoi:Masahiro Sekine:Tatsuo Saishu:Yasunobu Yamauchi	We have proposed a flatbed autostereoscopic display using the one-dimensional (1-D) integral imaging (II) method [Hirayama 2006]. 1-D cylindrical lens array (lenticular sheet) is used in the 1D-II display, making it possible to observe a three-dimensional (3-D) image with the horizontal parallax ray. The flatbed autostereoscopic display system brought about a more effective stereoscopic experience than the conventional upright display. In the flatbed display configuration, observers perceive displayed objects as if they exist on a table, because it has real depth matching with a horizontal plane and uses bird's-eye view configuration.	Birds-eye view ray scan system for flatbed autostereoscopic displays	NA:NA:NA:NA	2010
William C. Thibault	Immersive multi-projector displays with dozens of projectors are becoming easier to build as projection technology proliferates. We envision scenarios such as a classroom of students with individual projectors, and informal groups of people with projector-equipped mobile devices, in which computer-generated imagery can be generated and displayed in real time. Ideally, the resolution of the multi-projector display should grow with the number of projectors, and support arbitrarily wide displays (to 4π steradians).	Camera-based calibration for scalable immersive rendering	NA	2010
Sriranjan Rasakatla:Kashyap Kompella:Krishna Koundinya	Here we present our idea of using a cell-phone (the Neo Freerunner) for tracking a Car's location using GPS and measuring the road's quality using the accelerometer in the cell-phone. Neo-Freerunner is an open source Linux phone by Open Moko Inc. The phone can run many flavors of linux like Android, Qt, SHR etc. Here the implementation was done in SHR.	Car tracking and vibration test rig using Neo-Freerunner	NA:NA:NA	2010
Kazuhisa Yanaka:Akifumi Momose:Masahiko Yoda	Chroma keying is a well-known technique for mixing two images in which a specific color of the foreground image is made transparent. When this technology is applied to integral photography (IP) images, each of which is a textured image in which images taken from hundreds of angles are integrated, it is very useful. IP is an ideal 3D display method because parallax in all directions can be obtained without the need for wearing special glasses. Moreover, it needs only simple hardware consisting of an LCD and a fly's eye lens. In particular, in the case of the extended fractional view (EFV) method [1][2], an inexpensive ready-made fly's eye lens can be used. Animation is also possible by displaying frames successively. However, creation of IP images is computationally intensive because multi-viewpoint rendering, in which a large number of images are observed from hundreds of viewpoints, is necessary. Therefore, we developed a chroma keying technology to reduce the processing time. By creating the foreground IP images and the background IP images separately and combining them later, the processing time could be reduced greatly, especially when the background was stationary.	Chroma keying between integral photography images	NA:NA:NA	2010
Douglas Lanman:Matthew Hirsch:Yunhee Kim:Ramesh Raskar	We optimize the performance of automultiscopic barrier-based displays, constructed by stacking a pair of LCD panels. To date, such displays have conventionally employed heuristically-determined parallax barriers, containing a fixed array of slits or pinholes, to provide view-dependent imagery. While recent methods adapt barriers to one or more viewers, we show that both layers can be adapted to the multi-view content as well. The resulting content-adaptive parallax barriers increase display brightness and frame rate. We prove that any 4D light field created by dual-stacked LCDs is the tensor product of two 2D mask functions. Thus, a pair of 1D masks only achieves a rank-1 approximation of a 2D light field. We demonstrate higher-rank approximations using temporal multiplexing.	Content-adaptive parallax barriers for automultiscopic 3D display	NA:NA:NA:NA	2010
Kip Haynes:Jacquelyn Morie:Eric Chance	Second Life (SL) is a popular 3D online virtual world designed for human interaction (also known as a MUVE, or multi-user virtual environment). It typically supports 60--70 thousand concurrent users. The assets and physical environments within SL are easy to create and use, and the environments themselves are very much part of the human interaction experience. However, the typical means of accessing SL is through a single computer screen, which lessens the immersion that is inherent in such a rich 3D world. Because of this, the SL virtual world is a good candidate for adaptation to large scale immersive displays such as a CAVE™ or other multi projector systems.	I want my virtual friends to be life size!: adapting Second Life to multi-screen projected environments	NA:NA:NA	2010
Shunsuke Yoshida:Sumio Yano:Hiroshi Ando	A tabletop is a useful shared space for diverse collaborative tasks. If the tabletop is considered to be interface, then expression through visual sensation, especially 3D images, is an important way to engage the principal human sense. Many 3D displays that can be observed from any direction have been proposed in recent years. However, some techniques force to wear special glasses and restrict the positions from which 3D images can be viewed [Kitamura et al. 2001]. Other glasses-free 3D displays employ obstructive apparatus on the table [Jones et al. 2007].	Implementation of a tabletop 3D display based on light field reproduction	NA:NA:NA	2010
Sriranjan Rasakatla	Many sensors like the laser range finder, stereo vision cameras which help in building a depth perception of the world around it in 3D are very costly. Here I present the designs and prototypes of few 3D perception sensors which have been built low cost using components off the shelf. These perception sensors use structured infrared light projection. The design is miniature compared to other 3D sensors like LIDAR, Laser scanner and Time of flight cameras.	Low cost 3D perception sensors	NA	2010
Hideaki Nii:James Teh Keng Soon:Adrian David Cheok	This paper describes the design of a parallax based Moving Slit Light Field Display (MSLFD). A MSLFD shows multi parallax images, by using vertical slits in an opaque cylinder surrounding multiple static flat panel displays. It allows viewers looking towards the cylinder to see an image from any position. Currently, various forms of 3D display have been developed and flat panel 3D display has been in practical use for some time. But commercial 3D displays only show a stereogram. On the other hand, Light Field Displays have been developed for displaying the dense ray information of the space, [Endo et al. 2005; Jones et al. 2007]. These displays use a "Parallax barrier" to control the ray direction to the observer. It shows parallax images without the use of an eye glass. We describe a system to reduce the size of the display by using two dimensional Organic Light- Emitting Diodes (OLED) and a rounding slit. OLEDs can act as the dense light source array and it can be controlled line by line. This system proposes a method to synchronize the movement of OLED's line and movement of slits. It can show many images in multi orientation (Figure 1(a)). In this paper we explain the principal method of design and how to expand the resolution and views of a MSLFD.	Moving Slit Light Field Display	NA:NA:NA	2010
Ishtiaq Rasool Khan	Two-layer encoding schemes for HDR images (and video) can not only reduce the storage requirements, but more importantly they can also ensure backward compatibility during transition from LDR to HDR age. The first layer is a tone-mapped LDR image, which can be shown on existing displays. The second layer is another LDR image and contains the residual information lost in tone-mapping, which can be used by HDR applications.	A backward compatible HDR encoding scheme	NA	2010
Chun-Te Wu:Wei-Hao Huang:Chih-Hao Liu:Wei-Jia Huang:Kai-Che Liu:Ludovic J. Angot	The new data structure, the bilateral grid, was presented by Jiawen et al. to make bilateral filter algorithm become simple implementation. Based on the data structure, the GPU CUDA-based optimization is proposed to have more efficiency in using GPU shared memory and massive multithreading. Meanwhile, a commercial application, the video 2d to 3d conversion which was presented by Ludovic et al. is also re-designed by applying the proposed CUDA-based bilateral grid three times to obtain better 3D quality in real-time. Depth map are created and modified by adjusting bilateral grid parameters.	A real-time video 2D-to-3D with the bilateral grid	NA:NA:NA:NA:NA:NA	2010
Masaru Tsuchida:Toru Takahashi:Koichi Ito:Takahito Kawanishi:Junji Yamato:Takafumi Aoki	In the digital archiving for cultural heritage preservation, in the medical field, and in some industrial fields, high-fidelity reproduction of color, gloss, texture, and shape are very important. Multiband or full-spectrum imaging technology is a solution for accurate color reproduction. Although several types of multi band camera systems have been developed [Yamaguchi 2000, Tominaga 2000, Helling 2004, Hashimoto 2008], all of them are multi-shot systems and they cannot take images of moving objects. Ohsawa et al. [2004] have developed a six-band HDTV camera system. However, the system requires very expensive customized equipment. In order to make multiband technology pervasive, equipment costs must be reduced and the systems have to be able to take images of moving objects. To meet these requirements, we developed a novel multiband image capturing system that combines multiband and stereo imaging techniques. This system can acquire both spectral color information and depth information at the same time. In this paper, we focus on the generation of six-band images from a pair of stereo image.	A stereo one-shot multi-band camera system for accurate color reproduction	NA:NA:NA:NA:NA:NA	2010
Christian Lipski:Christian Linz:Marcus Magnor	Over the last decade, considerable progress has been made on the so-called early vision problems. We present an optical flow algorithm for image morphing that incorporates recent advances in feature matching, energy minimization, stereo vision and image segmentation. At the core of our flow estimation we use Efficient Belief Propagation for energy minimization. While state-of-the-art algorithms only work on thumbnail-sized images, our novel feature downsampling scheme in combination with a simple, yet efficient data term compression can cope with high-resolution data. The incorporation of SIFT features into data term computation further resolves matching ambiguities, making long-range flows possible. We detect occluded areas by evaluating the symmetry of the flow fields, we further apply Geodesic matting to automatically inpaint these regions.	Belief propagation optical flow for high-resolution image morphing	NA:NA:NA	2010
Stavros Papastavrou:Demetris Hadjiachilleos:Georgios Stylianou	The identification of a bank note's value is a non-trivial task for the blind and the visually impaired. A popular approach adopted by many countries in order to facilitate the visually impaired, is the impression of a high-contrast, large-print region on their bank notes. Additionally, an approach used to facilitate the blind population is the impression of unique tactile marks on bank notes. However, even when tactile marks or different sizes (e.g. Euros) are used, blind and visually impaired people have practical difficulties in identifying them.	Blind-folded recognition of bank notes on the mobile phone	NA:NA:NA	2010
Xuan Dong:Yi (Amy) Pang:Jiangtao (Gene) Wen	We describe a novel and effective video enhancement algorithm for low lighting video. The algorithm works by first inverting the input low-lighting video and then applying an image de-haze algorithm on the inverted input. To facilitate faster computation and improve temporal consistency, correlations between temporally neighboring frames are utilized. Simulations using naive implementations of the algorithm show good enhancement results and 2x speed-up as compared with frame-wise enhancement algorithms, with further improvements in both quality and speed possible.	Fast efficient algorithm for enhancement of low lighting video	NA:NA:NA	2010
Lange Benoit:Rodriguez Nancy	Until now computer graphic researchers have tried to solve visualization problems introduced by the size of meshes. Modern tools produce large models and hardware is not able to render them in full resolution. For example, the digital Michelangelo project extracted a model with more than one billion polygons. One can notice hardware has become more and more powerful but meshes have also become more and more complex. To solve this issue, people have worked on many solutions. We can find solutions based on space subdivision, or based on visibility of objects like the use of a Z-buffer. But in 1976, Clark [Clark 1976] introduces the level of detail concept (LOD). The principle of LOD is the construction of several versions of the same 3D model at different resolutions. This is achieved by removing some object features. Luebke provides in [Luebke 1997] a very complete survey of LOD algorithms. The main issue with the simplification is that the mesh does not preserve appearance of the original mesh. Indeed, important features tend to disappear. For example, with the Quadric Error Metrics (QEM) algorithms and the cow mesh, the tail, horn and other characteristic points merge with the mesh at a low resolution. Our approach allows the simplified mesh to preserve important details.	LOD +: augmenting LOD with skeletons	NA:NA	2010
Shiro Ozawa:Takao Abe:Noriyuki Naruto:Toshihiro Nakae:Makoto Nakamura:Naoya Miyashita:Mitsunori Hirano:Kazuhiko Tanaka	In surface computing, one of the most important requirements is tracking an object placed on the surface and manipulating information related to that object. To recognize objects, the most popular technique is marker tracking using techniques such as RFID, tag-like TarckMate[Kumpf 2009] and so on. The issues with marker tracking are the effort required to paste the tag and the existence of objects that are difficult to mark with a tag. To recognize objects without tags, feature point tracking on the image plane is one of the most effective ways in the area of the computer vision[Lowe 2004]. Unfortunately it is difficult to extract features from images taken through the frosted glass that is often used in surface computing. In addition, one cannot extract the feature points from objects without strong texture. In this paper, we present a marker-less object recognition system using multi channel silhouettes and quantized polar coordinates.	Marker-less object recognition for surface computing	NA:NA:NA:NA:NA:NA:NA:NA	2010
Erich Marth:Guillermo Marcus	With the introduction of H.264, the complexity on video encoders has increased dramatically. As hardware based encoding solutions profit from the strict sequential design and already feature real time capabilities for high definition material, software solutions lack most of the encoding performance. More precisely, the performance of software encoders is limited due to the computation power of encoding system as well as the high level of codec-intern dependencies. As a consequence, software encoders supporting high definition needs are very rare.	Parallelization of the x264 encoder using OpenCL	NA:NA	2010
Eriko Kimura:Naoki Kawai:Kazunori Miyata	We have proposed a method called Bump Mapping onto Real Objects (BMRO)[1] for displaying the appearance of curved surface on flat media. The method converts normal vectors of modeled curved surface into directions of grooves by which anisotropic reflection occurs for displaying a curved surface. Although curved surfaces can appear on media by BMRO, it is still insufficient for practical use because the streamlines used for a pattern of grooves are often placed too closely or too sparsely to one another due to the vector plot employed for generating them. The simplest solution to avoid the non-uniformity is to divide the entire region into regular square or hexagonal cells and to fill each cell with parallel lines in a given direction instead of tracing the direction field strictly with streamlines, but this improvement causes aliasing to noticeably appear at the edges and ridges of the original model. In this article, we propose an improvement on generating cells that reduces aliasing for BMRO and makes it practical for industrial applications.	Practical 3D decoration on flat media with anisotropic reflection	NA:NA:NA	2010
Matthew Trentacoste:Rafal Mantiuk:Wolfgang Heidrich	The image quality of a digital viewfinder is considerably lower than that of a through-the-lens optical system. While the sensor may be capable of capturing 10 or 20 megapixels, the screen of the viewfinder is typically constrained to resolutions under 1 megapixel. The limited resolution makes it impossible to discern all the small details of the captured image. Small blurs and noise that are present in the full-size image can render the image unusable for certain tasks, yet these artifacts may be too small to be discernible in the downsampled version shown on the camera viewfinder.	Quality-preserving image downsizing	NA:NA:NA	2010
Zhengguo Li:Susanto Rahardja:Shiqian Wu:Zijian Zhu:Shoulie Xie	It is known that a high dynamic range (HDR) image can be produced by sequentially capturing a set of low dynamic range (LDR) images with different exposure times [Debevec and Malik 1997]. However, ghosting artifacts could be produced via this method when there are moving objects in a scene. In this poster, a similarity index is first introduced for such LDR images by using intensity mapping functions (IMFs) among them. The index is then applied to detect moving objects such that ghosting artifacts are removed from the eventual HDR image. The details are given as below.	Robust movement detection based on a new similarity index for HDR imaging	NA:NA:NA:NA:NA	2010
Thang M. Hoang	Fringe projection profilometry (FPP) is one of the most commonly used non-contact methods for retrieving the three-dimensional (3D) shape information of objects. In reality, the nonlinearity mostly caused by the gamma effect of digital otpic system, includes both projector and camera, gives inevitable intensity changes, which dramatically reduce the measurement accuracy. In this poster, a robust and simple scheme to eliminate the intensity nonlinearity induced by gamma effect. Firstly, by using phase shifting techniques, the gamma value involved in the measurement system can be detected accurately. Then, a gamma encoding process is applied to the system for future actual 3D shape measurements. With the proposed technique, high accuracy of measurement can be achieved with the traditional three-step phase-shifting algorithm.	Simple gamma correction for fringe projection profilometry system	NA	2010
Sun-Young Lee:Jong-Chul Yoon:In-Kwon Lee	Existing video matting approaches determine the alpha matte sequence frame-by-frame, which lead to flickering near the boundary of the foreground region. We reduce this effect by considering video data as a spatio-temporal cube, and extending a robust matting algorithm to a 3D solver. Our results demonstrate consistent and visually pleasing alpha mattes, and tend to preserve temporal coherence better than previous techniques.	Temporally coherent video matting	NA:NA:NA	2010
Jean-Charles Bazin:Soonkee Chung:Roger Blanco Ribera:Quang Pham:Inso Kweon	This paper introduces the new concept of virtual face sculpting. Given the images of a human face and a statue face (cf Fig 1-a and b), the goal of this application is to sculpt a virtual statue (cf Fig 1-c) as if the human face was sculpted on the statue. This problem is complicated and must face some important difficulties. For example, the virtual sculpture must verify the color and texture consistency of the original statue. Moreover, the structure of the human face must also not be modified, otherwise the person will not be recognizable.	Virtual face sculpting	NA:NA:NA:NA:NA	2010
Koki Nagano:Takeru Utsugi:Mika Hirano:Takeo Hamada:Akihiko Shirai:Masayuki Nakajima	We have enabled the superimposition of multiplexed images on the same screen at the same time with tangible and stable equipment. Our multiplex images can be seen by wearing special configured polarized glasses, and the image projection method is designed to be based on current 3D stereoscopic technology, which is now prevalent and making rapid progress, thus high compatibility with current contents industries is retained. Therefore our system enables the wide range of applications with new expressions and can easily be put into production.	A new "multiplex content" displaying system compatible with current 3D projection technology	NA:NA:NA:NA:NA:NA	2010
Seiya Matsuda:Tomohito Yamamoto	Recently, 3D movies such as "Avatar" have been popular because they can provide hyper reality. Most of these movies require special facility such as IMAX 3D. Therefore it has been difficult to introduce these movies to home environment. However, 3D TV for individual use has already been developed and is expected to become popular in a few years. In such situation, the demand of 3D contents for those systems will be higher and higher. However it is very difficult to create such contents because it requires exclusive tool, high technique and much cost.	A web system for creating and sharing 3D auditory contents	NA:NA	2010
Yuki Hirobe:Shinobu Kuroki:Katsunari Sato:Takumi Yoshida:Kouta Minamizawa:Susumu Tachi	Previously, pictures were painted using tools such as crayons or even by hand. Surfaces such as canvases or walls, provided the tactile sensations of the drawing surface while painting. However, this tactile experience has got lost because of advances in computer graphics software. Besides, a conventional multi-touch interface [1] can not provide tactile sensation. We propose a novel interactive painting interface called "Colorful Touch Palette" that may help us to rediscover our creativity. The user can touch the canvas having the electrode, select or blend tactile textures of their choice, draw a line, and experience the tactile sensations of painting as shown in Figure 1. Various tactile textures can be created by blending textures as paints. This interface can be used to design complex spatial tactile patterns for haptic-friendly products. Moreover, this system can be potentially used to create novel tactile paintings.	Colorful Touch Palette	NA:NA:NA:NA:NA:NA	2010
Yoshihiro Kuroda:Hirotoshi Ashida:Masataka Imura:Yoshiyuki Kagiyama:Osamu Oshiro	Liquid absorption affects the behavior of objects. Rain absorbed in the barrage can weaken its structure and cause the dam failure. A wet sponge ball bounces differently from a dry one. Porous media is a material that has internal pore space and is able to absorb liquid (e.g. a sponge or soil). Liquid absorption changes not only geometrical properties, e.g. volume, but also mechanical properties, e.g. elasticity. The aim of this study is to physically model the structural change of a porous media due to liquid absorption. Previous studies have focused on liquid flow inside the media[Lenaerts et al. 2008]. In contrast, this paper proposes a porous model that is able to simulate elastic change in a real sponge.	Force reflecting porous media with dynamic elasticity change	NA:NA:NA:NA:NA	2010
Michal Lech:Bozena Kostek	Nowadays, one of the main focuses of the Human-computer interaction area is controlling computers by gestures. Various gesture types provide means of controlling user interfaces and applications. However, most of them involve the front-facing camera and the user's gestures are recognized often from the static background. In addition, colorful gloves, gloves with motion sensors or infrared diodes are often used for this purpose.	Gesture controlled interactive whiteboard based on SVM and fuzzy logic	NA:NA	2010
Stephen David Beck:Shantenu Jha:Brygg Ullmer:Chris Branton:Sharath Maddineni	Laptop Orchestras (LOs) have recently become a very popular mode of musical expression. They engage groups of performers to use ordinary laptop computers as instruments and sound sources in the performance of specially created music software. By using an orchestral metaphor, LOs provide an engaging and challenging environment to experiment with human-computer interaction, network and machine latency, and sound/signal processing. While the LOs at Princeton and Stanford are perhaps the best known, LOs have now been established at many universities in the US and UK, and as private ensembles around the world. Perhaps the biggest challenge for LOs is the distribution, management and control of software across heterogeneous collections of networked computers. Software must be stored and distributed from a central repository, but launched on individual laptops immediately before performance. Each "composition" consists of unique combinations of software, user interfaces, and physical devices. Moreover, performers in a Laptop Orchestra can have a complex array of application layers to manage and launch before the start of a specific piece's performance. For example, one work written for our LO requires a bluetooth middleware application which reads gestures from a Wii-mote and converts them into OpenSoundControl [Wright and Freed 1997] messages to be forwarded to a custom Max application, all of which must be launched and configured before any performance. Combine this with the rapid turnaround from one composition to the next during a concert performance, and the problem of preparing members of the laptop orchestra with the appropriate tools for each piece becomes daunting. The GRENDL project leverages proven grid computing frameworks and approaches the Laptop Orchestra as a distributed computing platform for interactive computer music. This allows us to readily distribute software to each laptop in the orchestra depending on the laptop's internal configuration, its role in the composition, and the player assigned to that computer. Using the SAGA framework [Goodale et al.], GRENDL is able to run pre-distribution scripts on a master computer, distribute software to client computers, launch post-distribution scripts on the master computer and launch application scripts on client computers that in turn manage application environments for each composition. SAGA, the Simple API for Grid Applications, is a distributed computing middleware used to distribute, manage and process grid-based applications, typically for scientific research problems in such diverse fields as numerical relativity, computational fluid dynamics and materials science. Its functionality and stability are well regarded within the computational science community and SAGA has become a standard API for grid computing. Our initial experiments have demonstrated that SAGA can be used successfully in a concert environment. The Laptop Orchestra of Louisiana (LOLs) debut concert on April 14, 2010 used a prototype version of GRENDL to manage two of the seven works performed, and GRENDL worked flawlessly. GRENDL proposes to go further than just applying SAGA to the LO environment. We will use tangible and physical objects [Ullmer et al. 2008] to represent individuals, resources, roles and compositions such that GRENDL knows how to distribute software appropriate to the LO's environment, the individuals performing and their role in the composition. By using RFID-embeded objects [Ullmer et al. 2010], master and client computers determine who is at which computers, and what is being performed. Just as a music librarian knows where to place parts for each composition on which music stands, GRENDL will know where to send software and how to launch that software for each composition, laptop and performer. Extending SAGA to work with tangibles and in novel runtime environments, will require extensions to SAGA -- support for new interfaces and instruments -- as well as require some performance engineering in order for commands to be processed with lower latency than "traditional" distributed systems are designed to tolerate. The trans-disciplinary nature of GRENDL provides potential to shed new light on existing challenges in computational and computer science. The LO setting presents a unique perspective from which to investigate topics such as time-sensitive and dynamic job scheduling, latency-bound interaction, and effective user interfaces for grid computing environments. Some of the first iteration interaction technologies have been developed for distributed computational science applications, and some of what is learned through GRENDL will likely be applicable in that area.	GRENDL: grid enabled distribution and control for Laptop Orchestras	NA:NA:NA:NA:NA	2010
M. Cicconet:L. Velho:P. Carvalho:G. Cabral	Interfacing with the guitar using the audio signal is one of the oldest problems in Computer Music, and advances in the area were astonishing. In our days it is possible to simulate a huge range of amplifiers, apply many filter effects and evaluate the pitch of a plucked string robustly, to mention a few useful applications.	Guitar-leading band	NA:NA:NA:NA	2010
Ji-Hye An:Su-Jin Lee	Considerations of interface design have been limited to the senses of sight and hearing. However, as the sense of touch, such as haptics, began to be applied to equipment, new interaction has emerged. Due to the integrated nature of people (Goldstein, 2002), it is important for a new system that added tactile stimuli to correctly analyze and understand users' experiences. This study analyzes integrated cross modality user experiences from devices providing information on the senses of sight, hearing, and touch.	How people tend to organize sensory information into unified wholes in haptic phone?: focusing on cross modality interaction	NA:NA	2010
Kai Uwe Barthel:Sebastian Müller:David Backstein:Dirk Neumann:Klaus Jung	Internet image search systems mostly use words from the context of the web page containing the image as keywords. The performance of these search systems is rather poor, as the search systems neither know the intention of the searching user nor the semantic relationships of these images. Content-based image retrieval (CBIR) systems rely on the assumption that similar images share similar visual features. Despite intense research efforts, the results of CBIR systems have not reached the performance of text based search engines. The main problem of CBIR systems is the semantic gap between the content that can be described with low-level visual features and the description of image content that humans use with high-level semantic concepts. Some image retrieval systems have combined the keyword and the content-based visual search approach. However with this approach many images may be found that semantically do not match. In addition semantically similar images that visually look different cannot be found at all.	Image retrieval using collaborative filtering and visual navigation	NA:NA:NA:NA:NA	2010
Paulo F. U. Gotardo:Alan Price	Our research explores the use of real-time computer vision techniques and a pair of standard computer cameras to provide 3D human body awareness in an inexpensive, immersive environment system, Fig. 1. The goal is to enhance the user experience of immersion in a virtual scene that is displayed by a 3D screen. We combine stereo vision and stereo projection to allow for both the user and the virtual scene to become aware of each others 3D presence as part of a single, integrated 3D space.	Integrated space: authoring in an immersive environment with 3D body tracking	NA:NA	2010
Jae-Hee Park:Tackdon Han	Multi-touch sensing exists in a number of applications and is presently used in personal computing devices (i.e. laptops and desktop computers), mobile touch screens, kiosks, Interactive wall displays (i.e. subway station map), ATMS, and any display requiring an interactive platform. Current multi-touch sensing methods use capacitive and or resistive based touchscreens both which are expensive and difficult to make. Infrared based touchscreens is being studied as an alternative method that is effective and low-cost solution of producing equal results particularly with large interactive displays.	LLP+: multi-touch sensing using cross plane infrared laser light for interactive based displays	NA:NA	2010
Norbert Győrbíró:Henry Larkin:Michael Cohen	To remember important information, we often take pictures and arrange them into collections. Photos can also be gathered and organized via personal lifelogs and social media websites which may include contextual metadata such as location, participants, rating, and even emotional tags. However, memories and connections between places, events, and people can be difficult to recollect. Memory recall in our brain can depend on several factors: emotional level, context variability, loss of information during encoding, etc. As time passes, memories are gradually forgotten or become altered, e.g. due to collision with newly encoded information [Yi Chen 2010].	Long-term memory retention and recall of collected personal memories	NA:NA:NA	2010
Chun-Yu Tsai:Hung-Jung Lin:Tzu-Hao Kuo:Kai-Yin Cheng:I-Chao Shen:Bing-Yu Chen:Rung-Huei Liang	Hearing is one of human's five senses. In our daily life, we usually guess where we are and the surrounding conditions not only by the visual feedbacks of the surrounded scene, but also by environmental sounds. For example, subway stations usually hint people the door closing by an urgent sound. In Taiwan, the garbage trucks usually broadcast one special song, and people can judge whether the car is coming. Similarly, we usually can recognize our familiar people only by hearing the sounds they generated without actually seeing them. For example, John usually bats basketball while entering the room. Hence, before he enters the room, the familiar sounds is heard, and can be recognized. Moreover, through the sense of hearing, people can only use their peripheral attention to quickly know where they are and what happens.	MusicSpace: you "play" the music	NA:NA:NA:NA:NA:NA:NA	2010
Anusha Withana:Rika Matsui:Maki Sugimoto:Kentaro Harada:Masa Inakage	With recent advancements in digital photography, data storage and network technologies, publishing and sharing of digital images in Internet has been drastically increased. High popularity and growth of internet image libraries such as Flickr and Picasa are good examples for these trends. In order to enable easy browsing and searching, online storages store meta-data in the form of keywords to describe images. Meta-data could be a general description of the image, a specific tag or an annotation to describe spatial information within the image. In order to ease and improve the efficiency of tagging process, image processing and analysis algorithms has been combined to manual tagging systems [Yang et al. 2009]. Furthermore, meta-data can be used to create interesting presentations of images. Specially in exhibition displays, automated slide shows and digital photo frames, meta data are used to group related images and present them under different categories. However, presentations generated by most of the existing systems are limited to sequential displaying of individual images as correlated groups. In this paper we present a system that composite digital images in a narrative fashion utilizing objective and subjective tagging information. Proposed system can extract a region from one image and composite it into another image according to available meta-data. As a proof of concept we created a right to left continuous image panning application using Adobe Flash which combines different images according to their similarities and composite them together to create a narrative image presentation.	Narrative image composition using objective and subjective tagging	NA:NA:NA:NA:NA	2010
Toshihiro Nakae:Shiro Ozawa:Naoya Miyashita	We propose O-Link, a system that allows us to convey our experience by binding digital videos to a real object in order to facilitate intergenerational communications. We focus on two factors in designing our interface to build a closer relationship between grandparents and grandchildren.	O-Link: augmented object system for intergenerational communication	NA:NA:NA	2010
Frank Steinicke:Gerd Bruder:Scott Kuhl	In computer graphics one is often concerned with representing 3D objects on 2D displays, which provide often only a limited display field of view (DFOV) to the observer. Usually, planar geometric projections, in particular linear perspective projections, are applied, which make use of a straightforward mapping of graphical entities in a 3D view frustum to a 2D image plane. Corresponding to the DFOV introduced for computer screens, the aperture angle of the virtual camera is often denoted as geometric field of view (GFOV) [Kjelldahl and Prime 1995]. Projections of virtual objects on a computer screen are affected by the interplay between the GFOV that is used to render the scene, and the DFOV (see Figure 1). In this context, only little research has been conducted to identify perspective projections that appear realistic to users. Instead, graphics designers and developers often choose GFOVs that vary significantly from the DFOV [Steinicke et al. 2009].	Perception of perspective distortions of man-made virtual objects	NA:NA:NA	2010
Marcelo Cicconet:Paulo Cezar Carvalho	Computer hardware and music softwares have evolved to such a level that, in our days, it is possible to compose high quality music using only a simple laptop equipped with the proper applications.	Playing the QWERTY keyboard	NA:NA	2010
Dane M. Coffey:Daniel F. Keefe	Advances in high-performance (supercomputer) simulations are revolutionizing biomedical research. Figure 1 shows a visualiazation of data from a cutting-edge computational fluid dynamics (CFD) simulation of blood flow through a replacement heart valve. Our collaborators in medical device design hope to use these data as part of a new approach to redesigning the valve hinging mechanism, ultimately improving the longevity of these devices. Biomedical engineers face significant challenges in exploring and understanding these data.	Shadow WIM: a multi-touch, dynamic world-in-miniature interface for exploring biomedical data	NA:NA	2010
Kristian Gohlke:Michael Hlatky:Jörn Loviscach	During extended sessions with a graphical user interface (GUI), users often apply a small set of commands with high frequency. A majority of direct manipulation tasks on a GUI are carried out using the mouse, particularly when keyboard shortcuts are not provided or the user is not familiar with them. Thus, to invoke a certain command, the user is required to aim the mouse pointer at a given onscreen widget and click with the mouse. If the overall task requires a user to click on the same widget repeatedly as part of a sequence of different interleaved micro-tasks, the overall performance suffers, as each point-and-click action requires a considerable amount of time for correctly aiming at the respective control.	TapShot: screenshot snippets as GUI shortcuts	NA:NA:NA	2010
Yoichi Ochiai	Visible Breadboard is the Breadboard like interface which shows voltages of each and every hole by full color LED and enable us to make wiring by tracing with finger tips. Users could insert electrical material into the holes and make a circuit on this device. Users could understand what is happening in the circuit and correct the connections with finger tracing.	The visible electricity device: visible breadboard	NA	2010
Kumiko Kushiyama:Tetsuaki Baba:Kouki Doi:Shinji Sasada	"Thermo-Paradox" is a thermal design display device to use the thermal tactile Physiological illusions that can interactively present patterns of warm and cool temperatures. The technological success of a compact 80-pixel, 9-inch thermal display allows text information to be conveyed by temperature, which has never before been achieved, and the device compactness increases the degree of freedom in presentation methods. We propose this unprecedented tactile expression as a device that can display thermal images that interactively match a visual image, using the tactile Paradoxical sensation produced by the ability to control the temperature of each pixel. (Fig. 1)	Thermal design display device to use the thermal tactile illusions: "Thermo-Paradox"	NA:NA:NA:NA	2010
Pierre Rouanet:Pierre-Yves Oudeyer:David Filliat	Social robots are drawing an increasing interest both in scientific and economic communities and one of the main issues is the need to provide these robots with the ability to interact easily and naturally with humans. We believe that the interaction issues may have a very strong impact on the whole system and should be given more attention. Current research however focus mainly on the the visual perception and/or machine learning issues (see for example Steels and Kaplan [1]). We think that by focusing on the users and on the interface we can help them provide the learning system with very high quality learning examples.	Using mediator objects to easily and robustly teach visual objects to a robot	NA:NA:NA	2010
Patricia Codyre	Information and communication technologies (ITC) offer innovative ways to improve health services and systems. Integration of eHealth into the daily life of rural health-care workers is fast becoming a reality in developing countries. Computermediated communication systems can be used to bridge the gap between doctors in underserved regions with local shortages of medical expertise and medical specialists. eHealth for primary health-care includes applications that directly support disease prevention, patient diagnosis and patient management and care.	Using innovative ehealth interventions in a local health care context	NA	2010
Tanasai Sucontphunt:Zhigang Deng:Ulrich Neumann	Modeling a 3D face toward a specific person is a tedious and painstaking task even for skilled artists. Crafting a 3D cartoon-style or a 3D fiction-creature face to reflect a specific person likeness is even more challenging. For example, creating an ogre that keeps the actor/actress likeness or constructing a 3D avatar that reflects the person identity is an intensive process that involves high artistic skills to convey the human identity on the monster geometries. This work presents an automatic 3D face modeling system that transfers a target 3D human face identity (likeness) to any 3D character faces. This system can be used for a broad variety of a 3D face modeling such as an early stage 3D character face design or a individualized 3D avatars creation.	3D human face identity transfer using deformation gradient	NA:NA:NA	2010
Mohammed Yousef:Ahmed Hashem:Hassan Saad:Amr Gamal:Osama Galal:Khaled F. Hussain	Digital Content Creation (DCC) Applications (e.g. Blender, Autodesk 3ds Max) have long been used for the creation and editing of digital content. Due to current advancement in the field, the need for controlled automated work forced these applications to add support for small programming languages that gave power to artists without diving into many details. With time these languages developed into more mature languages and were used for more complex tasks (driving physics simulations, controlling particle systems, or even game engines). For long, these languages have been interpreted, embedded within the applications, lagging the UIs or incomparable with real programming languages (regarding Completeness, Expressiveness, Extensibility and Abstractions). Two approaches were used to implement those languages. Either build them from scratch (like MaxScript), or use an existing popular language and write a set of extensions to it and embed it (like Blender and Python). In practice, both those solutions suffer, the first method produces languages lacking being real, competitive languages and generally very inefficient, the second method has problems arising from not being dedicated in first place for that kind of applications so, they lack expressiveness facilities (like dedicated constructs) that support that particular domain, also it's very hard to optimize these languages for specific DCC situations.	A scripting language for Digital Content Creation applications	NA:NA:NA:NA:NA:NA	2010
Wael Abdelrahman:Sara Farag	Segmenting 3D meshes into distinct components is vital and necessary for more efficient processing and usability. The smaller segments are usually easier to process and can be associated with with semantics or geometric features. This can be used in 3D parametrization, 3D database creation, animation, deformation transfer and many other 3D graphics applications. However, automating such a process is challenging due to the variety and complexity of the input.	Automated 3D mesh segmentation using 2D footprints	NA:NA	2010
Olusola O. Aina:Jian Jun Zhang	Physically-based facial animation (FA) techniques are notoriously difficult to create, reuse, and art-direct. We address these shortcomings by proposing a rig-builder that automatically generates bony and soft-tissue substructures for any given head model. In an earlier work, [Aina 2009] presented a method for fitting a generic skull to any given head model as a first step toward automated rig-building. Here, we outline work done since, and give an overview of a method for creating muscles of facial expression (mimic muscles), and other soft-tissues in the gap between a given head model and a fitted generic skull.	Automatic muscle generation for physically-based facial animation	NA:NA	2010
Ergun Akleman:Jianer Chen:Yen-Lin Chen:Qing Xing	Any arbitrary twist of the edges of an extended graph rotation system induces a cyclic weaving on the corresponding surface [Akleman et al. 2009]. This recent theoretical result allows us to study generalized versions of textile weaving structures as cyclic weaving structures on arbitrary surfaces. In this work, we extend the study to twill weaving, which is used in fabrics such as denim or gabardine. Biaxial twill is a textile weave in which the weft (filling) threads pass over and under two consecutive warp threads and each row is obtained from the row above it by a shift of 1 unit to the right or to the left. The shift operation creates the characteristic diagonal pattern that makes the twill fabric visually appealing.	Cyclic twill-woven objects	NA:NA:NA:NA	2010
Kazuhiko Yamamoto:Toki Takeda:Ryoichi Ando:Syota Kawano	In this study, we propose a novel system to create vivid animated 3d creature model from user's freeform 2d stroke. The most famous technique that construct 3d model from user's 2d sketch is Teddy system[Igarashi et al. 1999]. Past teddy like system create 3D mesh from user sketched 2d-shape, only what is needed is the stroke. These approaches allow a lot of freedom of creativity to us. However, there are also the cases we need to build the identification or determine the character of contents such like most games. In these cases, although it proper to design or control the contents in advance by the developers, the past methods are too much freely to control their contents. To address this, our approach enables end-users to collaborate with product developer's design without decreasing the freedom of creativity. It creates 3d animated creature from the combination of user's freeform stroke and the primitive models that are created by contents' designer beforehand. The generated creature depend on the user's inspiration, so obtained variations are infinte, but don't lack of the identification of the contents which is designed by the creators. Furthermore, our approach makes us to animate the generated model as if it is alive much easier than past methods.	Darwin's Lake: sketch-based creature creation system enables users to collaborate with contents designers	NA:NA:NA:NA	2010
Jae-Pil Heo:Duksu Kim:Joon-Kyung Seong:Jeong-Mo Hong:Min Tang:Sung-Eui Yoon	Simulating complex phenomena such as fracture requires collision detection (CD) methods to avoid any inter-collisions among deforming models and self-collisions (i.e. intra-collisions) within each deforming model. CD is typically the main computational bottleneck of simulating such complex phenomena.	FASTCD: fracturing-aware stable collision detection	NA:NA:NA:NA:NA:NA	2010
Nuno Goncalves:Ana Catarina Nogueira	Reflectors attract the attention of people since they reflect discontinuous images of the world and often provide unexpected information of a non-direct field of view. This is why reflections still have a lot of research attention in rendering of images in computer graphics, computer vision and optics, amongst other fields.	Faster accurate reflections throught quadric mirrors	NA:NA	2010
Yasuyuki Tomita:Reiji Tsuruno	We present a new method for making wave animation from still water image. In our method, users can control the behavior of wave in the water surface intuitively and interactively. After we simulate the wave using a Spectral Method [Tessendorf 1999], we have the water surface corresponding to the projection system of static images. Previous works for animating water surface. Chuang et al [Chuang and Goldman 2005] proposed a method for generating an animating of picture using displacement mapping and warping, however, those methods are only effective for gentle and calm water surfaces. Contrarily, our method is adaptively used for large scale waves of water height field.	Motion texture animation of water surface	NA:NA	2010
Sara Farag:Wael Abdelrahman	Simulation of the interactions with deformable models is important in many applications such as medical training and tissue engineering. To physically model the 3D object, both the inner and outer segments need to be considered. This implies dealing with different materials and hence different deformation behavior. Thus, a physically-based simulation needs to augment the behavior of embedded materials when the materials are in direct physical contact, and produce a plausible net result in both visual and haptic cues.	Physical modeling of heterogeneous embedded deformable object deformation	NA:NA	2010
James Gregson:Zheng Wang	Reconstructing scanned geometry is an important operation in geometry processing. Volumetric algorithms reconstruct the object volume by transforming range images into global coordinates and using scanline algorithms to build a scalar field that can be isocontoured to obtain the surface [Curless and Levoy 1996].	Rapid surface and volume mesh generation from depth-augmented visual hulls	NA:NA	2010
Craig Reynolds	This poster describes an abstract computation model of the evolution of camouflage in nature. Evolution is represented by genetic programming. Camouflage patterns are represented by procedural texture synthesis. A 2D environment is represented by a supplied photo. A predator is represented by a human's visual perception, interacting through a graphical user interface.	Using interactive evolution to discover camouflage patterns	NA	2010
Ku-Jin Kim:Jung-Eun Lee:Nakhoon Baek	We present an interactive algorithm to compute Voronoi diagrams for protein molecules. In the research area of biochemistry, a molecule is generally represented as a set of 3D spheres with various radii. In this paper, we propose a method to compute Voronoi diagrams for a set of spheres in the 3D discrete domain. We achieved interactive construction of Voronoi diagrams through our adaptive subdivision scheme and massively parallel processing supported by current graphics hardware.	Voronoi diagram computation for protein molecules using graphics hardware	NA:NA:NA	2010
Pedro Santos:Thomas Gierlinger:Rafael Huff:Martin Ritz:André Stork	In the real world, the ratio between full brightness of the sun and complete darkness is in the range of 2.000.000.000:1. However today's projection display technology is limited to contrast ratios of approximately 10.000:1. This hinders a convincing simulation and presentation of lighting effects in professional markets such as car styling, architecture and industrial design. At the same time, High Dynamic Range Imaging (HDRI) has been developed as a new field of research resulting in breakthroughs in image based lighting. What is missing today are interactive visualisation systems that fully support HDR material and light information from the acquisition stage right through the processing stage to the display stage. Current software systems do exist to simulate the effect of light sources in virtual scenes. However, they require specialist training, they are complex to use, they cannot operate in real-time, often requiring modification and recalibration. Current systems also do not support HDRI. This means that not only do they lack the ability to simulate real lighting conditions, e.g. the position and intensity of the sun, cloudcover, but also the behaviour of materials in various light conditions.	A full HDR pipeline from acquisition to projection	NA:NA:NA:NA:NA	2010
Sudarshanram Shetty:Mike Bailey	This paper introduces a layered model for rendering human teeth, to be used in photorealistic rendering of humans for games and animations. While the lighting responses of teeth have been studied in the dental industry ([Joiner 2004], [Brodbelt et al 1981], [Zijp and ten Bosch 1993]) for the production of realistic-looking dentures, to our knowledge this is the first study of its type in computer graphics for the production of realistic renderings. Human teeth exhibit translucency and are characterized by complex light interaction. From a rendering perspective, we make use of a bank of sliders (Figure 2) to vary optical properties at runtime and achieve desired rendering effects, including aging effects as shown in Figure 1. We also make use of hand drawn distribution maps for different layers rather than texture maps to achieve diffuse coloring.	A physical rendering model for human teeth	NA:NA	2010
Shailen Agrawal:Subodh Kumar	Ambient occlusion has been tackled in many different ways to inculcate realism into renderings. Ambient occlusion is a crude approximation to global illumination. But performing a full global illumination in real-time has turned out to be computationally expensive. Combined with local rendering models, ambient occlusion can produce renderings which have increased realism.	Approximate ambient occlusion for dynamic scenes using the GPU	NA:NA	2010
Lesley Northam:Joe Istead:Craig Kaplan	Hand-drawn sketches often depict geometry, colour and texture using loose and roughly drawn lines. Many automated sketching algorithms focus on accurately depicting salient details using pen-and-ink drawings. The approach of Hertzmann et. al. [2000] sketches the contours and silhouettes of 3D meshes, while the interactive algorithm of Kalnins et. al. [2002] renders decorative lines with artistic brushes and suggestions. Other 2D algorithms render Sobel and Canny edges with artistic brushes [Orzan et al. 2007].	Artistic sketching with a painterly rendering algorithm	NA:NA:NA	2010
Borom Tunwattanapong:Abhijeet Ghosh:Paul Debevec	Traditional image-based relighting technique requires capturing a dense set of lighting directions surrounding the object and uses the linearity of light transport property together with the illumination data of the target environment to relight an object [Debevec et al. 2000]. However, this can be a very data intensive process because such datasets typically involve photographing hundreds of lighting directions. It is also difficult to modify or edit the data in post-production environments because the data is high dimensional. Adjustment has to be made in several dimensions in order to add artistic effects to the result. Difficulty in acquisition process is also one of the main problems. The capturing process typically lasts long enough to only be suitable for static objects. In this poster, we present a relighting technique which greatly reduces the number of images required for relighting, and still generate realistic results. We combine spherical harmonics with point lights to achieve efficient image based relighting. Spherical harmonics can efficiently capture smooth low frequency illumination [Ramamoorthi and Hanrahan 2001] while point lights capture high frequency directional illumination. Combining both techniques, we create relighting results which have both low and high frequency illumination data. This technique also benefits the acquisition process by reducing the number of required photographs which results in shorter capture time. In addition, fewer dimensions of the data can potentially simplify modification or editing of reflectance data.	Combining spherical harmonics and point-source illumination for efficient image-based relighting	NA:NA:NA	2010
Kyoji Matsusima:Masaki Nakamura:Sumio Nakahara:Ichiroh Kanaya	James Cameron's Avatar pioneered 3-D films in practical meaning. All audiences of the movie were happy even though their eye points were fixed when they were watching the movie. However, there are certain area that fixed eye points are not acceptable. This is why the authors focus on Computational Holography (see Fig. 1).	Computational holography: the real 3-D by fast wave-field rendering in ultra high resolution	NA:NA:NA:NA	2010
Ole Gulbrandsen	Sharply separating a diffuse surface into a light and dark side often results in unwanted details. Combining normals from the actual surface with the normals from a simplified surface we get better control of the dark side.	Controlling the dark side in toon shading	NA	2010
Tomohito Hattori:Hiroyuki Kubo:Shigeo Morishima	This paper discusses an approach for computing the ambient occlusion by curvature depended approximation of occlusion. Ambient occlusion is widely used to improve the realism of fast lighting simulation. The ambient occlusion is defined as follows.	Curvature depended local illumination approximation of ambient occlusion	NA:NA:NA	2010
Jeong-ho Ahn:Jong-Chul Yoon:In-Kwon Lee	When artists draw a picture of photorealistic scene in an image, they describe only specific parts that represent characteristic features carefully, but they express the parts about less important region roughly. In study about Non-photorealistic rendering, image abstraction research reflects such artist's character. Thus, methods about image abstraction commonly preserve image features and flatten non-feature area. Recently, Kyprianidis et al. [2009] introduced Anisotropic Kuwahara Filtering (AKF) which generates feature preserved image abstraction using the smoothed structure tensor. However since they used only color information to defining anisotropic ratio, different regions that have similar color are conquered by each other unintentionally. Hence, we propose the depth-based AKF method that considers not only color, but also depth to generate image abstraction where boundary feateures are effectively preserved.	Depth-based Anisotropic Kuwahara Filtering	NA:NA:NA	2010
Andrew Cox:Jan Kautz	Real-time applications require simple techniques that enable predictable high performance rendering, but illumination methods for GPUs tend either to be complex and fragile on the one hand or very limited and offering poor visual quality on the other. The addition of a plausible global look to an application's lighting impacts users' perceptions of its realism, and on the spectrum of existing approaches, screen space ambient occlusion (SSAO) lies at the simplest and most predictable end. We have extended screen space ambient occlusion by replacing its depth buffer comparisons with a sampling of a volumetric discretization of the scene, while still evaluating it in an image-space post-process and thus retaining its predictable performance. We show improvements in quality over depth buffer based alternatives at a reasonable additional cost.	Dynamic ambient occlusion from volumetric proxies	NA:NA	2010
Roger Hoang:Steve Koepnick:Joseph D. Mahsman:Matthew Sgambati:Cody J. White:Daniel S. Coming	Real time global illumination (GI) is a difficult and steadily researched area. Advances in the field could potentially benefit virtual reality applications by increasing users' sense of presence. In immersive virtual environments (IVE) like CAVEs, applications must support perspective-corrected stereoscopic rendering. Dmitriev et al. [2004] performed GI in a CAVE using Precomputed Radiance Transfer, which requires a static scene. Mortensen et al. [2007] also performed GI in a CAVE using Virtual Light Fields which did not allow moving lights or geometry. We present our attempt to find GI techniques that support dynamic lights and scene geometry in our 6-sided CAVE-like IVE (DRIVE6). We implemented two separate illumination techniques: GPU photon mapping (GPM) and multiresolution splatting for indirect illumination (MSII). Each technique makes trade-offs between image quality and speed, and appropriate use of each depends on the needs of the application. Anecdotal evidence suggests that these techniques increase the sense of presence, warranting formal study. A user study is planned.	Exploring global illumination for virtual reality	NA:NA:NA:NA:NA:NA	2010
Xuehui Liu:Xiaoguang Hao:Mengcheng Huang:Fang Liu:Mingquan Zhou:Hanqiu Sun:En-Hua Wu	Soft shadow generation is a challenging problem in realistic rendering. Previous methods using shadow map or shadow volume work well for point light sources but are difficult to be extended to area lights. This paper presents a new method for fast soft shadow generation under dynamic area light sources. Our algorithm encodes the depth distribution of the scene into a coarse depth grid in a preliminary pass from the light point of view. In the second pass, the scene is rendered from the camera viewpoint to capture the frontmost layer. During deferred shading, the area light is sampled and the irradiance of each shaded pixel is accumulated along the ray. Experimental results demonstrate high quality soft shadows with interactive performance for dynamic scenes and lighting.	Fast soft shadow by depth peeling	NA:NA:NA:NA:NA:NA:NA	2010
Mohammad Obaid:Ramakrishnan Mukundan:Mark Billinghurst	Facial caricature drawing exaggerates physical face features for a comical effect, and can create an entertaining, humorous, and cartoon-like description of a person's face. Recently, example-based approaches have been introduced to generate facial sketches. Most of these approaches exaggerate the caricature appearance by altering the overall facial shape based on capturing artists' exaggeration-prototypes. Rare attempts have been made to alter and control the facial expressions of the generated caricatures. Moreover, example-based approaches learn how to generate artistic sketch styles through a training phase with prototypes from artist sketches. One of the limitations to these systems is that they require a lot of manual work with a large number of training prototypes drawn by artists. In addition, the final appearance of the caricature can only be limited to the prototypes used in the training phase.	Generating and rendering expressive caricatures	NA:NA:NA	2010
Patrick Cozzi:Frank Stoner	Accurately rendering an ellipsoid is a fundamental problem for virtual globes in GIS and aerospace applications where the Earth's standard reference surface is non-spherical. The traditional approach of tessellating an ellipsoid into triangles and rendering via rasterization has several drawbacks [Miller and Gaskins 2009]. Geodetic grid tessellations oversample at the poles (2a), which leads to shading artifacts and ineffective culling. Tessellations based on subdividing an inscribed platonic solid lead to problematic triangles crossing the International Date Line and poles (2b).	GPU ray casting of virtual globes	NA:NA	2010
Christian Linz:Christian Lipski:Marcus A. Magnor	Multi-image interpolation in space and time has recently received considerable attention. Typically, the interpolated image is synthesized by adaptively blending several forward-warped images. Blending itself is a low-pass filtering operation: the interpolated images are prone to blurring and ghosting artifacts as soon as the underlying correspondence fields are imperfect. We address both issues and propose a multi-image interpolation algorithm that avoids blending. Instead, our algorithm decides for each pixel in the synthesized view from which input image to sample. Combined with a symmetrical long-range optical flow formulation for correspondence field estimation, our approach yields crisp interpolated images without ghosting artifacts.	Multi-image interpolation based on graph-cuts and symmetric optical flow	NA:NA:NA	2010
Lei Ma:Shuangjiu Xiao:Xubo Yang	We presented an multi-interfaces image based method to simulate the refraction and related light effects in real time on a normal graphic card. The multi-interfaces based representation of the refractor is obtained by hiring depth peeling ideas. This leads to significantly better results than two interfaces refraction where only the front and back face of the object was captured.	Multi-interfaces based refractive rendering	NA:NA:NA	2010
Daniel M. Tokunaga:Cléber G. Corrêa:Ricardo Nakamura:Fátima L. S. Nunes:Romero Tori	Spatial visualization of virtual contents appears to be, with the appearance of stereoscopic displays, the next step for increasing immersion in visual output. This kind of visualization can be used to facilitate the understanding of complex informations, like anatomic structures. One approach currently adopted for this purpose is the use of non-photorealisitic rendering (NPR), like proposed by Tietjen, Isenberg and Preim [2005]. However, this NPR style, which conventionally tries to simulate a 2D illustration, fused with the the stereoscopic 3D visualization can break the 3D perception of the virtual contents. This work aims to study the 3D perception of virtual contents represented using NPR techniques, in order to evaluate the influence of NPR in the 3D perception when used with stereoscopic information visualization. The stereoscopic NPR visualization was applied in VIDA, a system for the study of anatomic structures that enables the stereoscopic visualization and interaction with virtual objects [Tori et al. 2009], in order to proceed the user tests, the figure 1c show the system used in the test.	Non-photorealistic rendering in stereoscopic 3D visualization	NA:NA:NA:NA:NA	2010
Martin Eisemann:Elmar Eisemann:Hans-Peter Seidel:Marcus Magnor	We present a system to automatically construct high resolution images from an unordered set of low resolution photos. It consists of an automatic preprocessing step to establish correspondences between any given photos. The user may then choose one image and the algorithm automatically creates a higher resolution result, several octaves larger up to the desired resolution. Our recursive creation scheme allows to transfer specific details at subpixel positions of the original image. It adds plausible details to regions not covered by any of the input images and eases the acquisition for large scale panoramas spanning different resolution levels.	Photo zoom: high resolution from unordered image collections	NA:NA:NA:NA	2010
Mahdi MohammadBagher:Jan Kautz:Nicolas Holzschuch:Cyril Soler	We present an algorithm for computing Percentage-Closer Soft Shadows inside a screen-space rendering loop. Our algorithm is faster than traditional soft shadows based on percentage closer filtering, while providing soft shadows of similar visual quality. It combines naturally with a deferred shading pipeline, making it an ideal choice for video games. This algorithm is not only faster, but allows the use of larger shadow maps without dramatically affecting the rendering speed.	Screen-space Percentage-Closer Soft Shadows	NA:NA:NA:NA	2010
Tom Cuypers:Se Baek Oh:Tom Haber:Philippe Bekaert:Ramesh Raskar	Diffraction is a common phenomenon in nature when dealing with small scale occluders. It can be observed on biological surfaces, such as feathers and butterfly wings, and man--made objects like rainbow holograms. In acoustics, the effect of diffraction is even more significant due to the much longer wavelength of sound waves. In order to simulate effects such as interference and diffraction within a ray--based framework, the phase of light or sound waves needs to be integrated.	WBSDF for simulating wave effects of light and audio	NA:NA:NA:NA:NA	2010
Benjamin P. DeLillo	For more than a decade, rich 3D content on the web has only been available via external, often proprietary, browser plugins. However, a new standard has emerged to change this. WebGL, currently under development by the Khronos Group, is a standard specification for javascript bindings to OpenGL [Khronos 2009]. In September 2009 WebGL support made its way to development builds of Firefox 3.7. Since this time, WebGL has gained greater traction and visibility within developer communities. Although impressive demonstrations of WebGL are available [Vukicevic 2009], we believed that the creation of a development library would help kickstart interest in the creation of new applications.	WebGLU development library for WebGL	NA	2010
Stefan Elsen	WorldSeed introduces a fractal architecture that allows to generate and render full scale planets in real-time. Similar to existing concepts (e.g. by Szeliskit and Terzopoulos [1989] [1]) WorldSeed uses self-similar fractal subdivision to generate landscape detail. By expanding the concepts introduced by Bokeloh and Wand [2006] [2] to use triangular patches rather than rectangles, WorldSeed is capable of generating relatively distortion free spherical surfaces. 64bit integer seeds are used to generate consistent worlds, similar to a concept suggest by Teong Joo Ong et al. [2005] [3]. WorldSeed will be an integral part of a virtual reality application within the CodeVenture research project to teach basic modeling and programming skills to teenagers [4].	WorldSeed	NA	2010
Tai-Wei Kan:Chin-Hung Teng	The field of Augmented Reality (AR) has grown and progressed remarkably in recent years and many useful AR applications have been developed focusing on different areas such as game and education. However, most of these AR systems are designed for closed applications with limited number of users and restricted 3D contents. They are inappropriate for public environment with diverse 3D contents due to the following issues: (1) Limited number of markers. To ensure recognition accuracy, the number of markers is typically limited. (2) Marker registration. Pervasive AR systems often require a registration process each time a new marker is included in the systems. (3) Content management. Traditional AR systems are closed systems with all of their contents stored in a system server. This mechanism is inefficient for public systems with a huge volume of 3D contents. (4) Special Markers. The markers of traditional AR systems are often designed with particular patterns. They are not public or universal patterns.	A framework for multifunctional Augmented Reality based on 2D barcodes	NA:NA	2010
Hiroki Nishino	Markers are widely used for camera-based interaction. Yet, most of the marker tracking methods have considerable limitations in shapes and designs; they are not usually visually meaningful to the users. Such an issue on visually communicative designs can be very important to provide visual cues in a mobile/pervasive environment where a user must first notice a marker and understand its meaning before initiating interaction, unlike in an immersive environment with a head-mounted-display that keeps displaying information on the detected markers.	A shape-free, designable 6-DoF marker tracking method	NA	2010
Tej Tadi:Patrick Salamin:Frederic Vexo:Daniel Thalmann:Olaf Blanke	Over the years, different approaches have been explored to build effective learning methods in virtual reality but the design of effective 3D manipulation techniques still remains an important research problem. To this end, it is important to quantify behavioral and brain mechanisms underlying the geometrical mappings of the body with the environment and external objects, both within the virtual environments (VE), the real world and relative to each other. The successful mapping of such interactions entails the study of fundamental components of these interactions, such as the origin of the visuo-spatial perspective (1PP, 3PP) and how they contribute to the user's performance in the virtual environments. Here, we report data using a novel set-up exposing participants - during free navigation - with a scene view from either 3PP or the habitual first-person perspective (1PP).	Brain activity underlying third person and first person perspective training in virtual environments	NA:NA:NA:NA:NA	2010
Takashi Kajinami:Oribe Hayashi:Takuji Narumi:Tomohiro Tanikawa:Michitaka Hirose	In our research, we aim to construct an interactive exhibition system for museums to convey the background information about its exhibit, which today's museums need. Museums have to preserve their exhibits, and that was a limitation on the exhibition form. They cannot hold a quite new type of exhibition because it might jeopardize their exhibits. So they cannot do more than the exhibition with conventional display cases and panels, in other words a passive exhibition to convey the background information. Digital technologies untie them from this limitation. We can convey the background information effectively using CG in exhibitions, without jeopardizing real exhibits.	Digital display case: the exhibition sysytem for conveying the background information	NA:NA:NA:NA:NA	2010
Megha Davalath:Mat Sanford:Anton Agana:Ann McNamara:Frederic Parke	3D Immersive visualization systems provide a novel platform to present complex datasets and virtual environments (VEs). The objective of the research presented here is to compare user-interaction and performance between two immersive displays: a low-cost, tiled, multi-screen immersive visualization system and a more expensive, continuous, immersive visualization facility. The low cost system is designed using off-the-shelf components and constructed by arranging LCD displays in a tiled hemispherical layout. The expensive system is a Rockwell-Collins semi-rigid, rear projected, continuous curved screen. With the low cost paradigm, seams are introduced into the image where the displays are tiled. We hypothesize that the tiled system presents an equivalent visual experience, despite the seams introduced by connecting the screens. Both systems will be tested through psychophysical experimentation designed to measure aspects of human performance. Proving our hypothesis will impact lower budget organizations, currently unable to afford such displays, by providing an opportunity to work with lower cost immersive visualization systems at no sacrifice to user-experience.	Evaluating performance in immersive displays	NA:NA:NA:NA:NA	2010
Tae-Joon Kim:Yongyoung Byun:Yongjin Kim:Bochang Moon:Seungyong Lee:Sung-Eui Yoon	Ray tracing and collision detection are widely used for providing high-quality visualizations and user interactions. In these algorithms, we need to detect intersecting primitives between two input objects (e.g., a ray and a 3D object in ray tracing and two 3D objects in collision detection). In order to efficiently detect these intersecting primitives, hierarchical traversal and culling by using bounding volume hierarchies (BVHs) are commonly used.	HCCMeshes: hierarchical-culling oriented compact meshes	NA:NA:NA:NA:NA:NA	2010
Takuji Narumi:Takashi Kajinami:Tomohiro Tanikawa:Michitaka Hirose	So far, gustatory information has rarely been studied in relation to computers, even though there are lots of studies on visual, auditory, haptic and olfactory information. This scarcity of research on gustatory information has several reasons. One reason is that gustatory sensation is based on chemical signals, whose functions have not been fully understood yet. Another reason is that perception of gustatory sensation is affected by other factors, such as vision, olfaction, thermal sensation, and memories. Thus, complexity of cognition mechanism for gustatory sensation as described above makes it difficult to build up a gustatory display.	Meta cookie	NA:NA:NA:NA	2010
Tsouknidas Nikolaos:Tomimatsu Kiyoshi	Advancements in mobile technology have recently contributed to the surfacing of viable mobile augmented reality applications. Still, the main problem of mobile AR, as with all implementations of augmented reality, is the accurate and robust registration of the live camera feed and the digital contents (e.g. images, video, 3D models). So far, mobile AR applications make use of GPS and marker technology (fiducials) to solve this problem (e.g. Sekai Camera, Layar, AR-toolkit, Unifeye). The disadvantages are that, firstly, GPS can only guess the position of the device within a 5 to 10 meter radius, is subjected to weather changes, and does not work indoors. Secondly, although marker registration is very accurate, a marker has to be printed and visible by the camera in order to work.	QR-code calibration for mobile augmented reality applications: linking a unique physical location to the digital world	NA:NA	2010
Justin Ehrlich	The application of virtual reality is becoming ever more important as technology reaches new heights allowing virtual environments (VE) complete with global illumination. One successful application of virtual environments is educational interventions meant to treat individuals with autism spectrum disorder (ASD) since VEs induces pretense and presence, without the social fear of failing in the real world. Pretense and presence improves the user's ability to learn social skills and enhances perception-taking capabilities. Because of the lack of conclusive research of improving presence of individuals with ASD in a VE, this study evaluated ways of enhancing presence to improve a VE intervention for individuals with ASD. In the field of computer science visual realism, new research has surfaced linking illumination realism to presence. Since this research was limited to Neurologically Typical (NT) individuals (those without ASD), and because generalization is particularly important to individuals with ASD, this study targeted these individuals. Further, since head mounted displays (HMD) are impractical for widespread delivery of a VE intervention application and since the literature is inconclusive about the effect of VEs without HMDs on presence, this study used standard desktop displays. This work measured the extent to which visual realism induces the presence of a VE intervention, enumerated the specific characteristics of rendering that promote the sense of presence and the ability to generalize, and statistically verified the enhanced outcomes from using these techniques. After conducting a between-group study with 24 individuals with ASD, illumination realism was found to have a positive effect (effect size=0.6) on the presence felt by these individuals. This work contributes to the field of visualization and special education by providing empirical evidence supporting the claim that illumination realism increases the presence felt by users with ASD when interacting with a PVE.	The effect of desktop illumination realism on a user's sense of presence in a virtual learning environment	NA	2010
Woong Choi:Takahiro Fukumori:Kohei Furukawa:Kozaburo Hachimura:Takanobu Nishiura:Keiji Yano	Recently, extensive research has been undertaken on digital archiving of cultural properties in the field of cultural heritage. These investigations have examined the processes of recording and preserving both tangible and intangible materials through the use of digital technologies.	Virtual Yamahoko parade in virtual Kyoto	NA:NA:NA:NA:NA:NA	2010
Santi Fort	This research project is conducted by a consortium of European industrial and academic partners that include companies like: Technicolor, Digital Projection, DTS Europe, Doremi, Mediapro, Creative Wokers (CREW) and Datasat, and research centers: Barcelona Media, Joaneum research, University of Hasselt, University of Reading and Fraunhoffer. It is aimed to research, develop and demonstrate novel forms of compelling entertainment experiences based on new technologies for the capture, production, networked distribution and display of three-dimensional sound and images. 2020 3D Media research project is co-funded by the European Commission under the Seventh Framework Programme (FP7--ICT).	2020 3D media: new directions in immersive entertainment	NA	2010
Ippei Takauchi:Yuta Hara:Hiromu Saito:Ryo Asakura:Motofumi Hattori	It is becoming an important field in computer art to visualize High Dimensional Manifolds. [Banchoff 1990] [Kusabuka and AlgorithmicArt 2006] In this research, we will find the dynamics equation which express deforming motion of the α dimensional manifold r, and visualize many interesting motions of the deformed manifold r(t) by CG animation.	A mathematical model of deforming manifolds and their visualizations by CG animation	NA:NA:NA:NA:NA	2010
Dilip Banerjee:John Gross:Pradeep Reddy Gaddam:Marc Olano:William Hess:Judith Terrill:Terence Griffin:John Hagedorn:John Kelso:Steve Satterfield	In order to move away from the current prescriptive design methods towards performance based methods for the design of structures under fire, we need validated computer models. The next section describes our approach for modeling and analysis.	An integrated interactive visualization and analysis environment to study the impact of fire on building structures	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2010
Toshiki Takeuchi:Takuji Narumi:Kunihiro Nishimura:Tomohiro Tanikawa:Michitaka Hirose	Logging images, voices, etc. of one's daily life is called "lifelog". Recently, it is done research by many researchers because of rapid increasing information with a highly information-oriented society and increasing capacity and lowering price of logging device.	Forecast and visualization of future expenditure with logging and analyzing receipts	NA:NA:NA:NA:NA	2010
S. D. Laycock:M. B. Stocks:S. Hayward	A haptic feedback device enables a user to manipulate three dimensional structures and feel forces contained within complex data-sets such as those resulting from computational biology. However, as the data-set grows in size it becomes difficult to ensure that the user can easily interact with every part of it. One could scale the data-set down to fit into the haptic workspace, however, this could result in important features being missed. A secondary problem is enabling the user to select points efficiently within the three dimensional data set, where the perception of depth can be difficult. In this paper we present novel techniques to rapidly navigate large and complex data-sets with a haptic feedback device, whilst still permitting accurate and fast selection of points in three dimensional space. We have applied these techniques as part of software dedicated to studying the response of biomolecules to externally applied forces using elastic network models.	Navigation and exploration of large data-sets using a haptic feedback device	NA:NA:NA	2010
Shantanu H. Joshi:Ian Bowman:Robin Jennings:David Hasson:Zhizhong Liu:Arthur W. Toga:John D. Van Horn	Large scale neuroimaging data archival protocols are gradually becoming ubiquitous in both research as well as clinical settings. Current user-database interfaces are limited to textual searches and often require data-specific knowledge for performing queries. This is proving to be an obstacle for researchers who wish to obtain a holistic view of the data before designing pilot neuroscientific studies or even formulating statistical hypotheses. Instead of providing a restricted, unidimensional view of the data, we seek to place a multi-dimensional view of the entire neurodatabase at the user's disposal. With the aim of visual navigation of complete neuro-repositories, we introduce the concept of brain meta-spaces. The meta-space models the implicit nonlinear manifold where the neurological data resides, and encodes pair-wise dissimilarities between all individuals in a population. Additionally, the novelty in our approach lies in the user ability to simultaneously view and interact with many brains at once but doing so in a vast meta-space that encodes (dis)similarity in morphometry.	Visual mining of neuro-metaspaces	NA:NA:NA:NA:NA:NA:NA	2010
Kunihiro Nishimura:Jun'ichi Nakano:Tomohiro Tanikawa:Michitaka Hirose	The concept of this study is to collect "ant's eye view" to generate "bird's eye view". When we can collect large number of ant's eye views, we can integrate them and can generate bird's eye view. The idea of this study is an assumption that we can grasp both whole view and situations at multiple places when we can see real-time-report from various points. To achieve this idea, we focus on lifelog technology. Using a wearable computer or small devices and sensors, it is easy to get our daily-life data. We can record our photos, sounds, positions, and so on. It will be lifelog data. When we can collect multiple people's lifelog data, we can utilize them much more. In this study, we propose a visualization method for multiple people's life log data. The lifelog data is uploaded to the server, and the viewer visualizes the data that provides us to see the whole view of the data. We combined position information and sensor information of remote places, and visualized these data.	Visualization of multiple people's lifelog: collecting "Ant's-eye view" to generate "Bird's-eye view"	NA:NA:NA:NA	2010
Pedro Cruz:Penousal Machado	This is an information visualization project that narrates the decline of the British, French, Portuguese and Spanish empires during the 19th and 20th centuries. These empires were the main maritime empires in terms of land area during the referred centuries [Wikipedia]. The land area of the empires and its former colonies is continuously represented in the simulation. The size of the empires varies during the simulation as they gain, or lose, territories. The graphic representation forms were selected to attain a narrative that depicts the volatility, instability and dynamics of the expansion and decline of the empires. Furthermore, the graphic representation also aims at emphasizing the contrast between their maximum and current size, and portraying the contemporary heritage and legacy of the empires.	Visualizing empires decline	NA:NA	2010
Vipin Patel:GBCS Tejaswi Vinnakota:Soumyajit Deb:Manjunatha R. Rao	NA	A 3D animation and effects framework for mobile devices	NA:NA:NA:NA	2014
Tomokazu Ishikawa:Kento Okazaki:Masanori Kakimoto:Tomoyuki Nishita	NA	A video summarization technique of animation products according to film comic format	NA:NA:NA:NA	2014
Masaki Sato:Jun Kobayashi:Tomoaki Moriya:Yuki Morimoto:Tokiichiro Takahashi	NA	An icicle generation model based on the SPH method	NA:NA:NA:NA:NA	2014
Sophie Jörg:Alison E. Leonard:Sabarish Babu:Kara Gundersen:Dhaval Parmar:Kevin Boggs:Shaundra Bryant Daily	NA	Character animation and embodiment in teaching computational thinking	NA:NA:NA:NA:NA:NA:NA	2014
Rubaiat Habib Kazi:Fanny Chevalier:Tovi Grossman:Shengdong Zhao:George Fitzmaurice	NA	DRACO: sketching animated drawings with kinetic textures	NA:NA:NA:NA:NA	2014
Changgu Kang:Leonard Yoon:Min Seok Do:Sung-Hee Lee	NA	Environment-adaptive contact poses for virtual characters	NA:NA:NA:NA	2014
Takuya Kato:Shunsuke Saito:Masahide Kawai:Tomoyori Iwao:Akinobu Maejima:Shigeo Morishima	NA	Example-based blendshape sculpting with expression individuality	NA:NA:NA:NA:NA:NA	2014
Or Avrahamy:Mark Shovman	NA	From pain to happiness: interpolating meaningful gait patterns	NA:NA	2014
Syuhei Sato:Yoshinori Dobashi:Kei Iwasaki:Hiroyuki Ochiai:Tsuyoshi Yamamoto:Tomoyuki Nishita	NA	Generating various flow fields using principal component analysis	NA:NA:NA:NA:NA:NA	2014
Naoya Iwamoto:Shigeo Morishima	NA	Material parameter editing system for volumetric simulation models	NA:NA	2014
Todd Keeler:Robert Bridson	NA	Ocean waves animation using boundary integral equations and explicit mesh tracking	NA:NA	2014
Chie Furusawa:Tsukasa Fukusato:Narumi Okada:Tatsunori Hirai:Shigeo Morishima	NA	Quasi 3D rotation for hand-drawn characters	NA:NA:NA:NA:NA	2014
Felix Herbst:Alexander Schulze	NA	Real-time approximation of convincing spider behaviour	NA:NA	2014
Rina Tanaka:Hiroshi Mori:Fubito Toyama:Kenji Shoji	NA	Real-time avatar motion synthesis by replacing low confidence joint poses	NA:NA:NA:NA	2014
Kakuto Goto:Naoya Iwamoto:Shunsuke Saito:Shigeo Morishima	NA	The efficient and robust sticky viscoelastic material simulation	NA:NA:NA:NA	2014
Gerry Chan:Anthony Whitehead:Avi Parush	We examined the effects of personality pairings on enjoyment within two video game scenarios. It was hypothesized that one would enjoy playing with another person who possesses a personality type similar to their own and in a game scenario that matched their personality type. Results showed that cooperative pairings particularly enjoyed playing together. Implications for game design could include a personality survey to maximize enjoyment.	An evaluation of personality type pairings to improve video game enjoyment	NA:NA:NA	2014
Fuka Nojiri:Yasuaki Kakehi	NA	BelliesWave: color and shape changing pixels using bilayer rubber membranes	NA:NA	2014
Man Zhang:Jun Mitani:Yoshihiro Kanamori:Yukio Fukui	NA	Blocklizer: interactive design of stable mini block artwork	NA:NA:NA:NA	2014
Momoko Okazaki:Ken Nakagaki:Yasuaki Kakehi	NA	metamoCrochet: augmenting crocheting with bi-stable color changing inks	NA:NA:NA	2014
Takaki Kimura:Yasuaki Kakehi	NA	MOSS-xels: slow changing pixels using the shape of racomitrium canescens	NA:NA	2014
Stefan Petrovski:Panos Parthenios:Aineias Oikonomou:Katerina Mania	NA	Music as an interventional design tool for urban designers	NA:NA:NA:NA	2014
Laura K. Murphy:Philip Galanter	NA	Stylized trees and landscapes	NA:NA	2014
Michael Kuetemeyer:Anula Shetty	NA	Time Lens	NA:NA	2014
Akira Nakayasu	NA	Waving tentacles: a system and method for controlling a SMA actuator	NA	2014
Jinsil Hwaryoung Seo:James Storey:John Chavez:Diana Reyna:Jinkyo Suh:Michelle Pine	NA	ARnatomy: tangible AR app for learning gross anatomy	NA:NA:NA:NA:NA:NA	2014
Corrie Colombero:Andy Hunsucker:Pui Mo:Monét Rouse	NA	Augmented reality theater experience	NA:NA:NA:NA	2014
Hikari Tono:Saki Sakaguchi:Mitsunori Matsushita	This paper proposes a method for hiding information inside of an actual object and viewing the hidden information as shadows selectively by rotating the object. Our proposed system creates shadows by using the properties of the polarizing plates and the 1/2 wave-length boards. The goal of our research is to realize a novel method of information hiding.	Basic study on creation of invisible shadows by using infrared lights and polarizers	NA:NA:NA	2014
Tobias Alexander Franke	NA	Interactive relighting of arbitrary rough surfaces	NA	2014
Evangelia Mavromihelaki:Jessica Eccles:Neil Harrison:Hugo Critchley:Katerina Mania	NA	Cyberball3D+ for fMRI: implementing neuroscientific gaming	NA:NA:NA:NA:NA	2014
Yuta Ueda:Karin Iwazaki:Mina Shibasaki:Yusuke Mizushina:Masahiro Furukawa:Hideaki Nii:Kouta Minamizawa:Susumu Tachi	NA	HaptoMIRAGE: mid-air autostereoscopic display for seamless interaction with mixed reality environments	NA:NA:NA:NA:NA:NA:NA:NA	2014
Ann McNamara:Laura Murphy:Conrad Egan	This work in progress is investigating new ways to manage visual clutter in Augmented Reality (AR) applications through the use of eye tracking.	Investigating the use of eye-tracking for view management	NA:NA:NA	2014
Naoki Hashimoto:Akane Tashiro:Hisanori Saito:Satoshi Ogawa	NA	Multifocal projection for dynamic multiple objects	NA:NA:NA:NA	2014
Daisuke Kobayashi:Naoki Hashimoto	NA	Spatial augmented reality by using depth-based object tracking	NA:NA	2014
Chin-chia Tung:Tsung-Hua Li:Hong-Shiang Lin:Ming Ouhyoung	NA	Cage-based deformation transfer using mass spring system	NA:NA:NA:NA	2014
Masahiro Fujisaki:Daiki Kuwahara:Taro Nakamura:Akinobu Maejima:Takayoshi Yamashita:Shigeo Morishima	NA	Facial fattening and slimming simulation considering skull structure	NA:NA:NA:NA:NA:NA	2014
Chen Liu:Yong-Liang Yang:Ya-Hsuan Lee:Hung-Kuo Chu	NA	Image-based paper pop-up design	NA:NA:NA:NA	2014
Michal Smolik:Vaclav Skala	NA	In-core and out-core memory fast parallel triangulation algorithm for large data sets in E2 and E3	NA:NA	2014
Syed Altaf Ganihar:Shreyas Joshi:Shankar Shetty:Uma Mudenagudi	In this paper we propose to address the problem of 3D object categorization. We model the 3D object as a 2D Riemannian manifold and propose metric tensor and Christoffel symbols as a novel set of features. The proposed set of features capture the local and global geometry of 3D objects by exploiting the positional dependence of the features. The categorization of 3D objects is carried out using polynomial kernel SVM classifier. The effectiveness of the proposed framework is demonstrated on 3D objects obtained from different datasets and achieve comparable results.	Metric tensor and Christoffel symbols based 3D object categorization	NA:NA:NA:NA	2014
Ai Mizokawa:Taro Nakamura:Akinobu Maejima:Shigeo Morishima	NA	Photorealistic facial image from monochrome pencil sketch	NA:NA:NA:NA	2014
C. Antonio Sánchez:Sidney Fels	NA	PolyMerge: a fast approach for hex-dominant mesh generation	NA:NA	2014
Xuaner Zhang:Lam Yuk Wong	Without physically trying on a garment, online clothes shoppers are unable to decide the best size or color, and therefore are more likely to purchase clothes that do not fit. A solution to this online fit problem is virtual fitting. Using 3D modeling to help customers visualize how garments look on them requires garment simulation in real time. This paper proposes an innovative approach that utilizes machine learning to meet this real-time requirement and can be applied to virtual fitting systems.	Virtual fitting: real-time garment simulation for online shopping	NA:NA	2014
Koharu Horishita:Syuhei Tsutsumi:Saki Sakaguchi:Mitsunori Matsushita	This paper proposes a novel display that is in harmony with its surroundings. Our proposed display presents information using the different shades of color that can be generated in fur.	A nonluminous display using fur to represent different shades of color	NA:NA:NA:NA	2014
Yuto Uehara:Shinji Mizuno	NA	A virtual 3D photocopy system	NA:NA	2014
Wataru Wakita:Hiromi T. Tanaka	NA	An unconstrained tactile rendering with tablet device based on time-series haptic sensing with bilateral control	NA:NA	2014
Tomohiro Amemiya:Hiroaki Gomi	NA	Buru-Navi3: movement instruction using illusory pulled sensation created by thumb-sized vibrator	NA:NA	2014
Chi-Chiang Huang:Rong-Hao Liang:Liwei Chan:Bing-Yu Chen	NA	Dart-It: interacting with a remote display by throwing your finger touch	NA:NA:NA:NA	2014
Edgar Flores:Sidney Fels	NA	Design of a robotic face for studies on facial perception	NA:NA	2014
Matt Adcock:Bruce Thomas:Chris Gunn:Ross Smith	NA	Enabling physical telework with spatial augmented reality	NA:NA:NA:NA	2014
MHD Yamen Saraiji:Yusuke Mizushina:Charith Lasantha Fernando:Masahiro Furukawa:Youichi Kamiyama:Kouta Minamizawa:Susumu Tachi	NA	Enforced telexistence	NA:NA:NA:NA:NA:NA:NA	2014
Mon-Chu Chen:Yi-Ching Huang:Kuan-Ying Wu	NA	Gaze-based drawing assistant	NA:NA:NA	2014
Sota Suzuki:Haruto Suzuki:Mie Sato	NA	Grasping a virtual object with a bare hand	NA:NA:NA	2014
Naoya Maeda:Maki Sugimoto	NA	Pathfinder vision: tele-operation robot interface for supporting future prediction using stored past images	NA:NA	2014
Michelle Holloway:Cindy Grimm:Ruth West:Ross Sowell	NA	A guided approach to segmentation of volumetric data	NA:NA:NA:NA	2014
Janelle Arita:Jinsil Hwaryoung Seo:Stephen Aldriedge	NA	Soft tangible interaction design with tablets for young children	NA:NA:NA	2014
Kevin Fan:Yuta Sugiura:Kouta Minamizawa:Sohei Wakisaka:Masahiko Inami:Naotaka Fujii	NA	Ubiquitous substitutional reality: re-experiencing the past in immersion	NA:NA:NA:NA:NA:NA	2014
Leonardo Meli:Stefano Scheggi:Claudio Pacchierotti:Domenico Prattichizzo	NA	Wearable haptics and hand tracking via an RGB-D camera for immersive tactile experiences	NA:NA:NA:NA	2014
Ishtiaq Rasool Khan	NA	A new quantization scheme for HDR two-layer encoding schemes	NA	2014
Masahide Kawai:Tomoyori Iwao:Akinobu Maejima:Shigeo Morishima	NA	Automatic deblurring for facial image based on patch synthesis	NA:NA:NA:NA	2014
Yusuke Sekikawa:Sang-won Leigh:Koichiro Suzuki	We propose Coded Lens, a novel system for lensless photography. The system does not require highly calibrated optics, but instead, utilizes a coded aperture for guiding lights. Compressed sensing (CS) is used to reconstruct scene from the raw image obtained through the coded aperture. Experimenting with synthetic and real scenes, we show the applicability of the technique and also demonstrate additional functionality such as changing focus programmatically. We believe this will lead to a more compact, cheaper and even versatile imaging systems.	Coded Lens: using coded aperture for low-cost and versatile imaging	NA:NA:NA	2014
Yong-Ho Lee:In-Kwon Lee	NA	Color correction algorithm based on local similarity of stereo images	NA:NA	2014
Yasin Nazzar:Jonathan Bouchard:James J. Clark	NA	Detection of stereo window violation in 3D movies	NA:NA:NA	2014
Shunya Kawamura:Tsukasa Fukusato:Tatsunori Hirai:Shigeo Morishima	NA	Efficient video viewing system for racquet sports with automatic summarization focusing on rally scenes	NA:NA:NA:NA	2014
Hisataka Suzuki:Rex Hsieh:Akihiko Shirai	NA	ExPixel: PixelShader for multiplex-image hiding in consumer 3D flat panels	NA:NA:NA	2014
Morgane Rivière:Makoto Okabe	The vectorization process transforms an image in the algebraic representation of if its contours. In the case of hand-drawn cartoons, the pen stroke made by the artist defines such a contour. That's why drawing can be interpreted as a series of junctions between nodes, or, in other words, as a topological graph. Many softwares tackle this subject (Adobe Live Trace, Win-Topo...). We will focus here on a program developed in 2013 by a team of researchers from the ETH Zurich and Disney Studios [Noris et al. 2013]. Their method was very efficient for solving ambiguous cases in the drawing, however their implementation was very slow: the vectorization of a 2048x2048 cartoon could need more than 3 minutes of computation. We have improved their method achieving interactive speeds.	Extraction of a cartoon's topology	NA:NA	2014
Takahiro Fuji:Tsukasa Fukusato:Shoto Sasaki:Taro Masuda:Tatsunori Hirai:Shigeo Morishima	NA	Face retrieval system by similarity of impression based on hair attribute	NA:NA:NA:NA:NA:NA	2014
Yuji Aramaki:Yusuke Matsui:Toshihiko Yamasaki:Kiyoharu Aizawa	NA	Interactive segmentation for manga	NA:NA:NA:NA	2014
Judith E. Fan:Daniel Yamins:James DiCarlo:Nicholas B. Turk-Browne	NA	Mapping core similarity among visual objects across image modalities	NA:NA:NA:NA	2014
Jérémy Riviere:Pieter Peers:Abhijeet Ghosh	We present two approaches for acquiring spatially varying reflectance of planar samples using a mobile device. For samples with rough specular BRDF, we propose to employ the back camera and flash pair on any typical mobile device for freeform handheld reflectance acquisition using dense backscattering measurements under flash illumination. For samples with highly specular BRDF, we instead employ a 10" tablet for illuminating the sample with extended illumination while employing the front camera for reflectance acquisition. With this setup, we also exploit the tablet's LCD screen polarization for diffuse-specular separation.	Mobile surface reflectometry	NA:NA:NA	2014
Ding Chen:Ryuuki Sakamoto	NA	Optimizing infinite homography for bullet-time effect	NA:NA	2014
Shunsuke Saito:Ryuuki Sakamoto:Shigeo Morishima	NA	Patch-based fast image interpolation in spatial and temporal direction	NA:NA:NA	2014
Paul Olczak:Jack Tumblin	NA	Photometric camera calibration: precise, labless, and automated with AutoLum	NA:NA	2014
Kouta Takeuchi:Shinya Shimizu:Kensaku Fujii:Akira Kojima:Keita Takahashi:Toshiaki Fujii	NA	Scene-independent super-resolution for plenoptic cameras	NA:NA:NA:NA:NA:NA	2014
Hsin-Wei Wang:Ming-Wei Chang:Hong-Shiang Lin:Ming Ouhyoung	NA	Segmentation based stereo matching using color grouping	NA:NA:NA:NA	2014
Pin-Hua Lu:Chien-Wen Chu:I-Chen Lin	NA	Stereoscopic architectural image inpainting	NA:NA:NA	2014
Joan Sol Roo:Christian Richardt	NA	Temporally coherent video de-anaglyph	NA:NA	2014
Gregor Miller:Sidney Fels	NA	VisionGL: towards an API for integrating vision and graphics	NA:NA	2014
Krzysztof Zieliński:Yi-Ting Tsai:Ming Ouhyoung	NA	Yet another vector representation for images using eikonal surfaces	NA:NA:NA	2014
Liana Manukyan:Antonio Martins:Sophie A. Montandon:Michel Bessant:Michel C. Milinkovitch	NA	A versatile high-resolution scanning system and its application to statistical analysis of lizards' skin colour time-evolution	NA:NA:NA:NA:NA	2014
Kim Jaeyoung:Kang Byongsue:Rhee Shinyoung:Kim Byengwol:Yun Hyeonjin:Sung Junghwan	NA	Bitcube: the new kind of physical programming interface with embodied programming	NA:NA:NA:NA:NA:NA	2014
Shaohui Jiao:Haitao Wang:Mingcai Zhou:Xun Sun:Tao Hong	NA	Efficient sub-pixel based light field reconstruction on integral imaging display	NA:NA:NA:NA:NA	2014
Shunsuke Yoshida	NA	Implementations toward interactive glasses-free tabletop 3D display	NA	2014
Reza Qarehbaghi:Hao Jiang:Bozena Kaminska	NA	Nano-Media: multi-channel full color image with embedded covert information display	NA:NA:NA	2014
Yoichi Ochiai:Takayuki Hoshi:Jun Rekimoto	NA	Pixie dust: graphics generated by levitated and animated objects in computational acoustic-potential field	NA:NA:NA	2014
Hisham Bedri:Micha Feigin:Michael Everett:Ivan Filho:Gregory L. Charvat:Ramesh Raskar	Seeing around corners, in the dark, and through smoke is difficult without specialized sensors[Velten et al. 2012], and so far impossible with a mobile phone. We use an active audio system to sense objects around occluders. Current techniques perform passive localization of sound sources with a microphone array, however, we demonstrate that with one microphone and one speaker pair, such as the ones found in mobile phones, it is possible to sense the specular reflection of silent objects such as mannequins around occluding objects. We demonstrate this technique by sensing a mannequin occluded by a wall.	Seeing around corners with a mobile phone?: synthetic aperture audio imaging	NA:NA:NA:NA:NA:NA	2014
Kazuhisa Yanaka	NA	Simple projection-type integral photography system using single projector and fly's eye lens	NA	2014
F. Ferreira:M. Cabral:O. Belloc:G. Miller:C. Kurashima:R. de Deus Lopes:I. Stavness:J. Anacleto:M. Zuffo:S. Fels	NA	Spheree: a 3D perspective-corrected interactive spherical scalable display	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2014
Jefferson Amstutz:Scott Shaw:Lee Butler	NA	Visually programming GPUs in VSL	NA:NA:NA	2014
Ungyeon Yang:Ki-Hong Kim	NA	Wearable display for visualization of 3D objects at your fingertips	NA:NA	2014
Benjamin Bruneau:Matthias Segui Serera	NA	2D additive and dynamic shadows	NA:NA	2014
Yasunari Ikeda:Issei Fujishiro:Toru Matsuoka	NA	An object space approach to shadowing for hair-shaped objects	NA:NA:NA	2014
Jin-Woo Kim:Jung-Min Kim:MinWoo Lee:Tack-Don Han	NA	Asynchronous BVH reconstruction on CPU-GPU hybrid architecture	NA:NA:NA:NA	2014
George Alex Koulieris:George Drettakis:Douglas Cunningham:Nikolaos Sidorakis:Katerina Mania	NA	Context-aware material selective rendering for mobile graphics	NA:NA:NA:NA:NA	2014
Christoph Müller:Fabian Gärtner	NA	Cross-compiled 3D web applications: problems and solutions	NA:NA	2014
Yusuke Tokuyoshi:Tiago da Silva:Takashi Kanai	NA	Directionality-aware rectilinear texture warped shadow maps	NA:NA:NA	2014
Youyou Wang:Ozgur Gonen:Ergun Akleman	NA	Global illumination for 2D artworks with vector field rendering	NA:NA:NA	2014
Midori Okamoto:Shohei Adachi:Hiroaki Ukaji:Kazuki Okami:Shigeo Morishima	NA	Measured curvature-dependent reflectance function for synthesizing translucent materials in real-time	NA:NA:NA:NA:NA	2014
Ryohei Tanaka:Yuki Morimoto:Hideki Todo:Tokiichiro Takahashi	NA	Parametric stylized highlight for character animation based on 3D scene data	NA:NA:NA:NA	2014
Xi M. Chen:Timothy Lambert:Eric Penner	NA	Pre-integrated deferred subsurface scattering	NA:NA:NA	2014
Pu Wang:Diana Bicazan:Abhijeet Ghosh	We present an approach for realistic rerendering of landscape photographs. We first extract a view dependent depth map from single input landscape images by examining global and local pixel color distributions and demonstrate application in novel viewpoint renderings. For relighting, we assume diffuse reflectance and relight landscapes by estimating the irradiance due the sky in the input photograph. Finally, we also take into account specular reflections on water surfaces which are common in landscape photography and demonstrate relighting of scenes with still water.	Rerendering landscape photographs	NA:NA:NA	2014
Takashi Ejiri:Yuki Morimoto:Tokiichiro Takahashi	NA	Shading approach for artistic stroke thickness using 2D light position	NA:NA:NA	2014
Adrian Jarabo:Julio Marco:Adolfo Munoz:Raul Buisan:Wojciech Jarosz:Diego Gutierrez	NA	Theory and analysis of transient rendering	NA:NA:NA:NA:NA:NA	2014
Lukas Hermanns:Tobias Alexander Franke	NA	Screen space cone tracing for glossy reflections	NA:NA	2014
Andrew K. Ho:Mark A. Nicosia:Angela Dietsch:William Pearson:Jana Rieger:Nancy Solomon:Maureen Stone:Yoko Inamoto:Eiichi Saitoh:Sheldon Green:Sidney Fels	NA	3D dynamic visualization of swallowing from multi-slice computed tomography	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2014
Takefumi Hayashi:Narihito Naoe:Naho Komatsubara:Kenji Sumiya:Kay Yonezawa	NA	Development of cultural capital content using ultra-high resolution images	NA:NA:NA:NA:NA	2014
Abir Al-Hajri:Matthew Fong:Gregor Miller:Sidney Fels	NA	How personal video navigation history can be visualized	NA:NA:NA:NA	2014
Daiki Matsumoto:Yusuke Matsui:Toshihiko Yamasaki:Kiyoharu Aizawa:Takanori Katagiri	NA	IllustStyleMap: visualization of illustrations based on similarity of drawing style of authors	NA:NA:NA:NA:NA	2014
Chuong Nguyen:David Lovell:Rolf Oberprieler:Debbie Jennings:Matt Adcock:Eleanor Gates-Stuart:John La Salle	NA	Natural-color 3D insect models for education, entertainment, biosecurity and science	NA:NA:NA:NA:NA:NA:NA	2014
Donald Madden:Andrew Scanlon:Yunxian Zhou:Tae Eun Choe:Martin Smith	We introduce a distributed augmented reality framework for aerial video which uses CPU/GPU acceleration to correct sensor metadata errors, create a geo-referenced scene model registered to the video, overlay important data, and stream to multiple web clients in order to improve situational awareness during real-time missions.	Real time video overlays	NA:NA:NA:NA:NA	2014
Amol Mahurkar:Ameya Joshi:Naren Nallapareddy:Pradyumna Reddy:Micha Feigin:Achuta Kadambi:Ramesh Raskar	NA	Selective visualization of anomalies in fundus images via sparse and low rank decomposition	NA:NA:NA:NA:NA:NA:NA	2014
Chun-Chia Chiu:Yi-Hsiang Lo:Wei-Ting Ruan:Cheng-Han Yang:Ruen-Rone Lee:Hung-Kuo Chu	Scribble art is a kind of illustrative drawing. Artists use continuous lines to convey the impression of an image or concept of a design. Unlike conventional line drawings such as sketching and hatching that commonly comprise of short and straight line segments, scribble artists aim at depicting the image with long and continuous curves. In this work, we study a typical curve pattern, circular scribble that appears most frequently in the artworks. Circular lines are drawn in either clockwise or counter-clockwise direction with varying radius in the circular scribble arts. The artists delicately trace along a seemingly random path and control the size and orientation of circular line pattern to depict a subject of their artwork. The main challenges lie in producing smooth transition between grayscale levels and preserving dominant image features using continuous loops and intersections of a circular scribble. Thus, the creation of circular scribble art is skill-demanding and time-consuming. In order to facilitate such process, we introduce a systematic approach to automatically synthesize circular scribble arts from images by tracing along a virtual path using solely a single continuous circular scribble with varying radius and orientation. We have tested our approach using a wide range of images and generate visually pleasing circular scribble arts (see Figure 1).	Continuous circular scribble arts	NA:NA:NA:NA:NA:NA	2015
Yuki Koyama:Daisuke Sakamoto:Takeo Igarashi	Exploring various visual designs by tweaking parameters is a common practice when designing digital content. For example, if we want to clean up a photo for use at the top of a web page, we adjust the design parameters---brightness, contrast, saturation, etc.---to explore which combination of parameters provides the best result. Similar situations can be found anywhere in computer graphics applications, such as tweaking shader parameters for game development.	Crowd-powered parameter analysis for computational design exploration	NA:NA:NA	2015
Xiang 'Anthony' Chen:Stelian Coros:Jennifer Mankoff:Scott E. Hudson	One powerful aspect of 3D printing is its ability to extend, repair, or more generally modify everyday objects. However, nearly all existing work implicitly assumes that whole objects are to be printed from scratch. Designing objects as extensions or enhancements of existing ones is a laborious process in most of today's 3D authoring tools. This paper presents a framework for 3D printing to augment existing objects that covers a wide range of attachment options. We illustrate the framework through three exemplar attachment techniques - print-over, print-to-affix, and print-through. We implemented these techniques in Encore, a design tool that supports a range of analysis with visualization for users to explore design options and tradeoffs among these metrics. Encore also generates 3D models for production, addressing issues such as support jigs and contact geometry between the attached part and the original object.	Encore: 3D printed augmentation of everyday objects with printed-over, affixed and interlocked attachments	NA:NA:NA:NA	2015
Kazutaka Nakashima:Takeo Igarashi	Construction of a free-form 3D surface model is still difficult. However, in our point of view, construction of a simple voxel model is relatively easy because it can be built with blocks. Even small children can build a voxel model. We present a method to convert a voxel model into a free-form surface model in order to facilitate construction of surface models.	Extraction of a smooth surface from voxels preserving sharp creases	NA:NA	2015
Chengcheng Tang:Xiang Sun:Alexandra Gomes:Johannes Wallner:Helmut Pottmann	We solve the form-finding problem for polyhedral meshes in a way which combines form, function and fabrication; taking care of user-specified constraints like boundary interpolation, planarity of faces, statics, panel size and shape, enclosed volume, and cost. Our main application is the interactive modeling of meshes for architectural and industrial design. Our approach can be described as guided exploration of the constraint space whose algebraic structure is simplified by introducing auxiliary variables and ensuring that constraints are at most quadratic.	Form-finding with polyhedral meshes made simple	NA:NA:NA:NA:NA	2015
Rukmini Goswami:Tim Tregubov:Lorie Loeb	Attention is a limited resource that intrinsically dictates our perceptions, memories, and behaviors. Further, visuospatial attention correlates highly with user engagement, heart rate, and arousal [El-Nasr et al. 2010]. Artists and interactive game designers strive to capture and direct attention, yet even in the most carefully crafted graphic narratives viewer eye paths -- a proxy for attention -- vary up to 20 percent [McCloud 1994; Jain et al. 2012]. Our aim is to use attentional measures to enrich graphic novel narratives. FrameShift uses eye tracking to measure reader attention and changes text and visual elements later on in the story accordingly. We have built an extensible framework for using attention to introduce perceptual changes in narratives. We use attention as an indirect method for interactions and introduce shiftable frame nodes that change readers' belief states over time.	FrameShift: shift your attention, shift the story	NA:NA:NA	2015
EunJin Kim:Hyeon-Jeong Suk	In the process of editorial design, a harmonious match between a picture and a solid color is often essential to achieve a high quality of a graphical art work. Color as such is a compelling cue to elicit emotional responses and thus can enhance the emotional quality of an image. Tools and methods have been developed to automatize the color selection process, and a noticeable progress has been achieved to extract perceptually dominant colors of an image. However, little attention has been paid to the emotional characteristics of selected colors, and it has been highly relying on the color designers' manual judgments. In this study, we propose a computational method that creates a color that enhances both aesthetic and affective quality of an image, and call it a theme color.	Hue extraction and tone match: generating a theme color to enhance the emotional quality of an image	NA:NA	2015
Azusa Mama:Yuki Morimoto:Katsuto Nakajima	Modeling 3D trees is a major theme in the field of computer graphics [Steven et al. 2012]. However, there has been little research on generating illustrations of trees [Yu-Sheng et al. 2012]. One of the ways to generate them is to render their 3D models. However, it is difficult to obtain the characteristic flat representation of illustrations because of the concentration of foliage in the central part of the tree. We present a system to generate a wide variety of tree illustrations by controlling the density of branches, the shape of canopy, and the overlap of flowers and leaves (Fig. 1).	Interactive tree illustration generation system	NA:NA:NA	2015
Raf Ramakers:Kashyap Todi:Kris Luyten	We present PaperPulse, a design and fabrication approach that enables designers without a technical background to produce standalone interactive paper artifacts by augmenting them with electronics. With PaperPulse designers overlay pre-designed visual elements with interactive widgets and specify functional relations between them using a logic demonstration and recording approach, called Pulsation. When the design is finished, PaperPulse generates layered electronic circuit designs, code that can be deployed on a microcontroller, and instructions for assembly.	PaperPulse: an integrated approach for embedding electronics in paper designs	NA:NA:NA	2015
Yuki Igarashi:Jun Mitani	Patchwork is a well-known type of needlework that involves sewing pieces of fabric into a larger design. It is commonly used to form quilts, but can also be used to make bags, wall hangings, cushion covers, and other items. Larger designs are usually based on repeating patterns, which are built up using different shapes. Professional patchwork designers design original patterns; however, novice users usually use geometric patterns or off-the-shelf patterns for each piece; this is because it is difficult for novices to design patterns while visualizing the resulting larger fabric.	Patchy: an interactive patchwork design system	NA:NA	2015
Shinji Mizuno:Marino Isoda:Rei Ito:Mei Okamoto:Momoko Kondo:Saya Sugiura:Yuki Nakatani:Motomi Hirose	Drawing on a sketchbook is one of the most familiar arts and people of all ages can enjoy it. Thus a lot of CG applications on which a user can create 2D and 3DCG images with drawing operations have been developed [Kondo et al. 2013]. On the other hand, dancing is also familiar to many people. Thus a digital content that is a mixture of drawing and dancing could be very attractive.	Sketch dance stage	NA:NA:NA:NA:NA:NA:NA:NA	2015
Daria Tsoupikova:Scott Rettberg:Roderick Coover:Arthur Nishimoto	We describe the first virtual reality art performance Hearts and Minds: The Interrogations Project developed using a novel method for direct output of the Unity-based virtual reality projects into CAVE2™ [Febretti et al. 2013] environment. This work incorporates original research, technological innovation and creative arts in an adaptation of veterans' testimonies detailing US military interrogations and abuses of prisoners in Iraq during the American counter-insurgency campaign in the early 2000s. It presents a debate focusing on interrogation methods, torture and its consequences, and Post Traumatic Stress Disorder experienced by solders who have participated in such acts. This work was developed at the Electronic Visualization Lab in Chicago through a unique international collaboration between artists, scientists, and researchers from five different Universities. The methods developed for this project allow hands-on education of virtual reality by letting students create their own virtual environments and exhibit them in the CAVE2 quickly.	The battle for hearts and minds: interrogation and torture in the age of war	NA:NA:NA:NA	2015
Rebecca Kleinberger	Our voice is an important part of our individuality but the relationship we have with our own voice is not obvious. We don't hear it the same way others do, and our brain treats it differently from any other sound we hear [Houde et al. 2002]. Yet its sonority is highly linked to our body and mind, and deeply connected with how we are perceived by society and how we see ourselves. The V3 system (Vocal Vibrations Visualization) offers a interactive visualization of vocal vibration patterns. We developed the hexauscultation mask, a head set sensor that measures bioacoustic signals from the voice at 6 points of the face and throat. Those signals are sent and processed to offer a real-time visualization of the relative vibration intensities at the 6 measured points. This system can be used in various situations such as vocal training, tool design for the deaf community, design of HCI for speech disorder treatment and prosody acquisition but also simply for personal vocal exploration.	V3: an interactive real-time visualization of vocal vibrations	NA	2015
Shogo Fukushima:Takeshi Naemura	When we snap strings playing with a CMOS camera, the strings seems to vibrate in a wobbly slow motion pattern. Because a CMOS sensor scans one line of video in sequence, fast moving objects are distorted during the scanning sequence. The morphing and distorting are called a rolling shutter effect, which is considered to be an artistic photographic techniques like strip photography and slit-scan photography. However, the effect can only be seen on a camera finder or a PC screen; the guitar player and audience are quite unlikely to notice it by the naked eye.	Wobble strings: spatially divided stroboscopic effect for augmenting wobbly motion of stringed instruments	NA:NA	2015
Sang-won Leigh:Harshit Agrawal:Pattie Maes	We present a drone-based drawing system where a user's sketch on a desk is transformed across scale and time, and transferred onto a larger canvas at a distance in real-time. Various spatio-temporal transformations like scaling, mirroring, time stretching, recording and playing back over time, and simultaneously drawing at multiple locations allow for creating various artistic effects. The unrestricted motion of the drone promises scalability and a huge potential as an artistic medium.	Z-drawing: a flying agent system for computer-assisted drawing	NA:NA:NA	2015
Katsutoshi Masai:Yuta Sugiura:Masa Ogata:Katsuhiro Suzuki:Fumihiko Nakamura:Sho Shimamura:Kai Kunze:Masahiko Inami:Maki Sugimoto	Facial expression is a powerful way for us to exchange information nonverbally. They can give us insights into how people feel and think. There are a number of works related to facial expression detection in computer vision. However, most works focus on camera-based systems installed in the environment. With this method, it is difficult to track user's face if user moves constantly. Moreover, user's facial expression can be recognized at only a limited place.	AffectiveWear: toward recognizing facial expression	NA:NA:NA:NA:NA:NA:NA:NA:NA	2015
Hugo Talbot:Frederick Roy:Stéphane Cotin	Cryotherapy is a rapidly growing minimally invasive technique for the treatment of different kinds of tumors, such as breast cancer, renal and prostate cancer. Several hollow needles are percutaneously inserted in the target area under image guidance and a gas (usually argon) is then decompressed inside the needles. Based on the Thompson-Joule principle, the temperature drops drown and a ball of ice crystals forms around the tip of each needle. Radiologists rely on the geometry of this iceball (273 K), visible on computer tomographic (CT) or magnetic resonance (MR) images, to assess the status of the ablation. However, cellular death only occurs when the temperature falls below 233 K. The complexity of the procedure therefore resides in planning the optimal number, position and orientation of the needles required to treat the tumor, while avoiding any damage to the surrounding healthy tissues.	Augmented reality for cryoablation procedures	NA:NA:NA	2015
Jun Nishida:Hikaru Takatori:Kosuke Sato:Kenji Suzuki	Understanding and perceiving the world from a child's perspective is a very important key not only to design products and architecture, but also to remind staff who work closely with children, such as hospitals and kindergartens. Ida et al. investigated the universality of devices and architecture in public spaces by recording videos through a hand-held camera positioned at a child's eye level [Ida et al. 2010]. In this study, we propose a novel wearable suit called CHILDHOOD that virtually realizes a child's eye and hand movements by attaching a viewpoint translator and hand exoskeletons (Figure 1a). We hypothesized that virtualizing a child's body size by transforming our own body while preserving embodied interactions with actual surroundings would provide an augmented experience of a child's perspective. This could assist designers in evaluating product accessibility through their own body interactions in real time. In addition, augmented child experience can help staff and parents remember how children feel and touch the world.	CHILDHOOD: wearable suit for augmented child experience	NA:NA:NA:NA	2015
Mark Bolas:Ashok Kuruvilla:Shravani Chintalapudi:Fernando Rabelo:Vangelis Lympouridis:Christine Barron:Evan Suma:Catalina Matamoros:Cristina Brous:Alicja Jasina:Yawen Zheng:Andrew Jones:Paul Debevec:David Krum	There is rapidly growing interest in the creation of rendered environments and content for tracked head-mounted stereoscopic displays for virtual reality. Currently, the most popular approaches include polygonal environments created with game engines, as well as 360 degree spherical cameras used to capture live action video. These tools were not originally designed to leverage the more complex visual cues available in VR when users laterally shift viewpoints, manually interact with models, and employ stereoscopic vision. There is a need for a fresh look at graphics techniques that can capitalize upon the unique affordances that make VR so compelling.	Creating near-field VR using stop motion characters and a touch of light-field rendering	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2015
Liang-Chen Wu:Jia-Ye Li:Yu-Hsuan Huang:Ming Ouhyoung	In making 3D animation with traditional method, we usually edit 3D objects in 3-dimension space on the screen; therefore, we have to use input devices to edit and to observe 3D models. However, those processes can be improved. With the improvement in gesture recognition nowadays, virtual information operations are no longer confined to the mouse and keyboard. We can use the recognized gestures to apply to difficult operations in editing model motion. And for observing 3D model, we would use head tracking from external devices to improve it. It would be easy to observe the interactive results without complicated operation because the system will accurately map the real world head movements.	First-person view animation editing utilizing video see-through augmented reality	NA:NA:NA:NA	2015
Michael Saenz:Joshua Strunk:Kelly Maset:Jinsil Harwaryoung Seo:Erica Malone	We present FlexAR, a kinetic tangible augmented reality [Billinghurst,2008] application for anatomy education. Anatomy has been taught traditionally in two dimensions, particularly for those in non-medical fields such as artists. Medical students gain hands-on experience through cadaver dissection [[Winkelmann, 2007]. However, with dissection becoming less practical, researchers have begun evaluating techniques for teaching anatomy through technology.	FlexAR: anatomy education through kinetic tangible augmented reality	NA:NA:NA:NA:NA	2015
Nazim Haouchine:Alexandre Bilger:Jeremie Dequidt:Stephane Cotin	The considerable advances in Computer Vision for hand and finger tracking made it possible to have several sorts of interactions in Augmented Reality systems (AR), such as object grasping, object translation or surface deformation [Chun and Höllerer 2013]. However, no method has yet considered interaction than involves topological changes of the augmented model (like mesh cutting).	Fracture in augmented reality	NA:NA:NA:NA	2015
Toshiaki Nakasu:Tsukasa Ike:Kazunori Imoto:Yasunobu Yamauchi	In maintenance of electric power control panels, a worker has to do a lot of manual work such as pushing buttons and turning on/off selector switches. Therefore, a hands-free gesture operating system is needed. Tsukada [Tsukada et al. 2002] proposed a gesture operating system using an acceleration sensor and switches. Although it is a simple task to control a home appliance by gesture, users have to use both gesture and switch on/off to perform more complicated tasks such as controlling and recording documents in maintenance work. Therefore, the system becomes complicated. We propose a novel switch-less assist system for maintenance work with a simple structure that recognizes gesture using only an acceleration sensor. Ike [Ike et al. 2014] proposed a hand gesture operating system that enables users to control a TV remotely by adopting "Tapping" as a click signal. The system recognizes tapping by detecting a pulse-like acceleration pattern corresponding to a micro collision generated by tapping. However, it is difficult to recognize tapping because maintenance work includes many micro collisions generated by touching things. We adopt "Tapping & Finger up", i.e., tapping fingers and turning up a finger, gestures that rarely occur in maintenance work, and design a gesture system enabling users to perform maintenance tasks and gesture operation seamlessly. Our system helps users do maintenance work easily and intuitively without interrupting work.	Hands-free gesture operation for maintenance work using finger-mounted acceleration sensor	NA:NA:NA:NA	2015
Bruno Marques:Nazim Haouchine:Rosalie Plantefeve:Stephane Cotin	Minimally invasive surgery (MIS) is a recent surgical technique where the surgeon does not interact directly with the patient's organs. In contrast to open surgery, the surgeon manipulates the organs through instruments inserted in the patient's abdominal cavity while observing the organ from a display showing the video stream captured by an endoscopic camera. While the benefits of MIS for patients are clearly claimed, performing these operations remains very challenging for the surgeons, due to the loss of depth perception caused by this indirect manipulation. To tackle this limitation, the research community suggests to use augmented reality (AR) during the procedure [Haouchine et al. 2013]. The objective towards the use of AR during surgery is to be able to overlay the 3D model of the organ (that can be obtained from a pre-operative scan of the patient) onto the video stream. Surgical AR made considerable advances and reached a certain maturity in the estimation of tumors and vessels localisation. Howerver, very few studies have investigated depth perception and visualization of internal structures [Lerotic et al. 2007], which is considered by surgeons as a very sensitive issue. This study suggests a method to compensate the loss of depth perception while enhancing organ vessels and tumors to surgeons. This method relies on a combination of contour rendering technique and adaptive alpha blending to effectively perceive the vessels and tumors depth. In addition, this technique is designed to achieve real-time to satisfy the requirements of clinical routines, and has been tested on real human surgery.	Improving depth perception during surgical augmented reality	NA:NA:NA:NA	2015
Prashanth Bollam:Eesha Gothwal:GBCS Tejaswi, V:Shailesh Kumar:Soumyajit Deb	The recent boom in computing capabilities of mobile devices has led to the introduction of Virtual Reality into the mobile ecosystem. We demonstrate a framework for the Samsung Gear VR headset that allows developers to create a totally immersive AR & VR experience with no need for interfacing with external devices or cables thereby making it a truly autonomous mobile VR experience. The significant benefits of this system over existing ones are - a fully hands free experience where hands could be used for gesture based input, the ability to use the Head Mounted Display (HMD) sensor for improved head and positional tracking and automatic peer to peer network creation for communication between phones. The most important factor in our system is to provide an intuitive way to interact with virtual objects in AR and VR. And users should be able to switch from AR to VR world and vice versa seamlessly.	Mobile collaborative augmented reality with real-time AR/VR switching	NA:NA:NA:NA:NA	2015
Xing Zhang:Umur A Ciftci:Lijun Yin	In recent years, Virtual Reality (VR) has become a new media to provide users an immersive experience. Events happening in the VR connect closer to our emotions as compared to other interfaces. The emotion variations are reflected as our facial expressions. However, the current VR systems concentrate on "giving" information to the user, yet ignore "receiving" emotional status from the user, while this information definitely contributes to the media content rating and the user experience. On the other hand, traditional controllers become difficult to use due to the obscured view point. Hand and head gesture based control is an option [Cruz-Neira et al. 1993]. However, certain sensor devices need to be worn to assure control accuracy and users are easy to feel tired. Although face tracking achieves accurate result in both 2D and 3D scenarios, the current state-of-the-art systems cannot work when half of the face is occluded by the VR headset because the shape model is trained by data from the whole face.	Mouth gesture based emotion awareness and interaction in virtual reality	NA:NA:NA	2015
Toshikazu Ohshima:Shun Kawaguchi:Yuma Tanaka	The MR Coral Sea is a mixed reality tiny aquarium. The intent of the system is to play with little virtual fish. A player can interact with the virtual fish via Coral Display, an interactive device with multisensory physical feedback. When a player presents his or her hand above the device, fish bait appears on the palm, and the virtual fish come to eat it. The device provides the user with a feeling of spatial existence through illumination and vibration.	MR coral sea evolved: mixed reality aquarium with physical MR displays	NA:NA:NA	2015
Yong Yi Lee:Junho Choi:Yong Hwi Kim:Jong Hun Lee:Moon Gu Son:Bilal Ahmed:Kwan H. Lee	Traditional museums have shown interest in exhibiting a meaningful representation of cultural heritage. However, existing stereotypical exhibition fails to attract the visitors' interest continuously as it provides only static and non-interactive contents and transmits information unilaterally. Recently, high performance measurement techniques have rapidly developed to a degree that allows for the realistic digitization of cultural heritage. Based on this digitized cultural heritage, dynamic and interactive content, such as 3D video and augmented reality, have been made to improve the immersion of visitors. In spite of these attempts, the sense of artificiality is still a challenge because most existing methods demonstrate their content via screen displays.	RiSE: reflectance transformation imaging in spatial augmented reality for exhibition of cultural heritage	NA:NA:NA:NA:NA:NA:NA	2015
Masasuke Yasumoto:Takehiro Teraoka	"Shadow Shooter" is a VR shooter game that uses the "e-Yumi 3D" bow interface and real physical interactive content that changes a 360-degree all-around view in a room into virtual game space (Figure 1). This system was constructed by developing our previous interactive "Light Shooter" content based on "The Electric Bow Interface" [Yasumoto and Ohta 2013]. Shadow Shooter expands the virtual game space to all the walls in a room just as in Jones' "Room Alive" [Jones et al. 2014]; however, it does not require large-scale equipment such as multiple projectors. It only requires the e-Yumi 3D device that consists of a real bow's components added to Willis's interface with a mobile projector [Willis et al. 2013]. Thus, we constructed a unique device for Shadow Shooter that easily changes the 360-degree all-around view into a virtual game space.	Shadow shooter: 360-degree all-around virtual 3d interactive content	NA:NA	2015
Paul Debevec:Greg Downing:Mark Bolas:Hsuen-Yueh Peng:Jules Urbach	Todays most compelling virtual reality experiences shift the users viewpoint within the virtual environment based on input from a head-tracking system, giving a compelling sense of motion parallax. While this is straightforward for computer generated scenes, photographic VR content generally does not provide motion parallax in response to head motion. Even 360° stereo panoramas, which offer separated left and right views, fail to allow the vantage point to change in response to head motion.	Spherical light field environment capture for virtual reality using a motorized pan/tilt head and offset camera	NA:NA:NA:NA:NA	2015
Stefano Scheggi:Leonardo Meli:Claudio Pacchierotti:Domenico Prattichizzo	The complexity of the world around us is creating a demand for novel interfaces that will simplify and enhance the way we interact with the environment. The recently unveiled Android Wear operating system addresses this demand by providing a modern system for all those companies that are now developing wearable devices, also known as "wearables". Wearability of robotic devices will enable novel forms of human intention recognition through haptic signals and novel forms of communication between humans and robots. Specifically, wearable haptics will enable devices to communicate with humans during their interaction with the environment they share. Wearable haptic technology have been introduced in our everyday life by Sony. In 1997 its DualShock controller for PlayStation revolutionized the gaming industry by introducing a simple but effective vibrotactile feedback. More recently, Apple unveiled the Apple Watch, which embeds a linear actuator that can make the watch vibrate. It is used whenever the wearer receives an alert or notification, or to communicate with other Apple Watch owners.	Touch the virtual reality: using the leap motion controller for hand tracking and wearable tactile devices for immersive haptic rendering	NA:NA:NA:NA	2015
Seunghyun Woo:Daeyun An:Jongmin Oh:Gibeom Hong	Due to various features being available in the vehicle such as multimedia, the dashboard has become rather complicated. Therefore, an increased need for HMI(Human Machine Interface) research has arisen in the design creation process. However, there are issues such as design changes occurring even after the design is selected due to the initial evaluation being too simple to cover all of the requirements. Designers do not consider carefully HMI the during sketching phase and issues with designs are discovered too far along in the process. This study suggests an HMI simulation tool system based on projection to pre-evaluate an HMI prior to selecting specifications through virtual function implementation. This system evaluates each function of centerfacia through quantitative criteria such as performance time and distraction time. As a result, the objective of the system is to quickly analyze and validate designs through virtual means and find interface issues with a quantitative method.	WAOH: virtual automotive HMI evaluation tool	NA:NA:NA:NA	2015
Nobuki Yoda:Takeo Igarashi	In 2D game graphics, textures are packed into a single texture called a sprite sheet in order to achieve efficient rendering. The sprite sheet can be compressed to save memory by using various compression methods such as block-based compressions and 16 bpp (bits per pixel) tone reduction. These methods are not without some problems, though. Block-based compressions are GPU-dependent, and high-quality compressions such as ASTC [Nystad et al. 2012] are often unavailable on mobile devices. 16 bpp tone reduction--often used with dithering--can create undesirable noise when it is scaled up (Figure 1c).	Decomposition of 32 bpp into 16 bpp textures with alpha	NA:NA	2015
Shaohui Jiao:Xiaofeng Tong:Eric Li:Wenlong Li	Fur simulation is crucial in many graphic applications since it can greatly enhance the realistic visual effect of virtual objects, e.g. animal avatars. However, due to its high computational cost of massive fur strands processing and motion complexity, dynamic fur is regarded as a challenging task, especially on the mobile platforms with low computing power. In order to support real-time fur rendering in mobile applications, we propose a novel method called textured offset surfaces (TOS). In particular, the furry surface is represented by a set of offset surfaces, as shown in Figure 1(a). The offset surfaces are shifted outwards from the original mesh. Each offset surface is textured with scattering density (red rectangles in Figure 1(a)) to implicitly represent the fur geometry, whose value can be changed by texture warping to simulate the fur animation. In order to achieve high quality anisotropic illumination result, as shown in Figure 1(b), Kajiya/Banks lighting model is employed in the rendering phase.	Dynamic fur on mobile using textured offset surfaces	NA:NA:NA:NA	2015
Kai-Wen Liu:I-Peng Lin:Shih-Wei Sun:Wen-Huang Cheng:Xiaoniu Su-Chu Hsu	Interaction with virtual objects among different devices attracts lots of attention recently. LuminAR [Linder and Maes] was developed for a portable and compact projector-camera system for interactive displaying. THAW [Leigh et al.] was proposed to use a back-facing camera of a smartphone to assist the interactive displaying. RealSense [Lin et al.] was adopted the built-in compass sensor on a mobile device to calibrate the relative position among different mobile devices. However, the complex calibration process of LuminAR [Linder and Maes] and THAW [Leigh et al.] limited the applications. On the other hand, as addressed by the authors, once the users with mobile devices using RealSense [Lin et al.] move larger than 15°, the positioning relationship cannot be kept stable. Therefore, in this paper, a 3D positioning scheme is proposed based on the built-in gyro sensor on a mobile device for effective and intuitive calibration and allow users to freely move the mobile devices with a natural user experience.	G-spacing: a gyro sensor based relative 3D space positioning scheme	NA:NA:NA:NA:NA	2015
Jinhong Park:Minkyu Kim:Sunho Ki:Youngduke Seo:Chulho Shin	Although the mobile industry has recently begun trending towards high quality graphics content, it is still difficult to satisfy this trend due to performance, power and thermal issue of GPU/CPU in mobile application processor.	Half frame forwarding: frame-rate up conversion for tiled rendering GPU	NA:NA:NA:NA:NA	2015
Antoinette Leanna Bumatay:Jinsil Hwaryoung Seo	Stress is physical response that affects everyone in varying degrees. Throughout history, people have developed various practices to help cope with stress. Many of these practices focus on bringing awareness to the body and breath. Studies have shown that mindfulness meditation and paced breathing are effective tools for stress management [Brown, 2005].	Mobile haptic system design to evoke relaxation through paced breathing	NA:NA	2015
Ravi Krishnaswamy	Engineering documents e.g. 'blueprints' are one of the traditional forms of paper based information moving more to the digital realm. With mobile and the evolution of GPUs on mobile, there are tremendous opportunities for applications that view and interact with engineering documents.	Performance and precision: mobile solutions for high quality engineering drawings	NA	2015
Kristian Sons:Felix Klein:Jan Sutter:Philipp Slusallek	Graphics hardware has become ubiquitous: Integrated into CPUs and into mobile devices and recently even embedded into cars. With the advent of WebGL, accelerated graphics is finally accessible from within the web browser. However, still the capabilities of GPUs are almost exclusively exploited by the video game industry, where experts produce specialized content for game engines.	The XML3D architecture	NA:NA:NA:NA	2015
Nobuhisa Hanamitsu:Kanata Nakamura:Mhd Yamen Saraiji:Kouta Minamizawa:Susumu Tachi	Twech is a mobile platform that enables users to share visuo-tactile experience and search other experiences for tactile data. User can record and share visuo-tactile experiences by using a visuo-tactile recording and displaying attachment for smartphone, allows the user to instantly such as tweet, and re-experience shared data such as visuo-motor coupling. Further, Twech's search engine finds similar other experiences, which were scratched material surfaces, communicated with animals or other experiences, for uploaded tactile data by using search engine is based on deep learning that ware expanded for recognizing tactile materials. Twech provides a sharing and finding haptic experiences and users re-experience uploaded visual-tactile data from cloud server.	Twech: a mobile platform to search and share visuo-tactile experiences	NA:NA:NA:NA:NA	2015
Masasuke Yasumoto:Takehiro Teraoka	Various studies have been done on the combined use of mobile devices. Ohta's Pinch [Ohta and Tanaka 2012] and Leigh's THAW [Leigh et al. 2014] are representative studies. However, they have certain limitations; Pinch cannot dynamically correspond to the positional relations of the devices, and THAW cannot recognize the devices' spatial positional relations. We constructed VISTouch so that it does not require a particular kind of external sensor, and it enables multiple mobile devices to dynamically obtain other devices' relative positions in real time. We summarize VISTouch in this paper.	VISTouch	NA:NA	2015
Haruki Sato:Tatsunori Hirai:Tomoyasu Nakano:Masataka Goto:Shigeo Morishima	This paper presents a system that can automatically add a soundtrack to a video clip by replacing and concatenating an existing song's musical bars considering a user's preference. Since a soundtrack makes a video clip attractive, adding a soundtrack to a clip is one of the most important processes in video editing. To make a video clip more attractive, an editor of the clip tends to add a soundtrack considering its timing and climax. For example, editors often add chorus sections to the climax of the clip by replacing and concatenating musical bars in an existing song. However, in the process, editors should take naturalness of rearranged soundtrack into account. Therefore, editors have to decide how to replace musical bars in a song considering its timing, climax, and naturalness of rearranged soundtrack simultaneously. In this case, editors are required to optimize the soundtrack by listening to the rearranged result as well as checking the naturalness and synchronization between the result and the video clip. However, this repetitious work is time-consuming. [Feng et al. 2010] proposed an automatic soundtrack addition method. However, since this method automatically adds soundtrack with data-driven approach, this method cannot consider timing and climax which a user prefers.	A music video authoring system synchronizing climax of video clips and music via rearrangement of musical bars	NA:NA:NA:NA:NA	2015
Ergun Akleman:Siran Liu:Donald House	In this work, we present a simple mathematical approach to art directed shader development. We have tested this approach over two semesters in an introductory level graduate rendering & shading class at Texas A&M University. The students in the class each chose an artist's style to mimic, and then easily created rendered images strongly resembling that style (see Figures 1). The method provides shader developers an intuitive process, giving them a high level of visual control in the creation of stylized depictions.	Art directed rendering & shading using control images	NA:NA:NA	2015
Hiroki Kagiyama:Masahide Kawai:Daiki Kuwahara:Takuya Kato:Shigeo Morishima	In movie and video game productions, synthesizing subtle eye and corresponding head movements of CG character is essential to make a content dramatic and impressive. However, to complete them costs a lot of time and labors because they often have to be made by manual operations of skilled artists.	Automatic synthesis of eye and head animation according to duration and point of gaze	NA:NA:NA:NA:NA	2015
Shugo Yamaguchi:Chie Furusawa:Takuya Kato:Tsukasa Fukusato:Shigeo Morishima	Anime designers often paint actual sceneries to serve as background images based on photographs to complement characters. As painting background scenery is time consuming and cost ineffective, there is a high demand for techniques that can convert photographs into anime styled graphics. Previous approaches for this purpose, such as Image Quilting [Efros and Freeman 2001] transferred a source texture onto a target photograph. These methods synthesized corresponding source patches with the target elements in a photograph, and correspondence was achieved through nearest-neighbor search such as PatchMatch [Barnes et al. 2009]. However, the nearest-neighbor patch is not always the most suitable patch for anime transfer because photographs and anime background images differ in color and texture. For example, real-world color need to be converted into specific colors for anime; further, the type of brushwork required to realize an anime effect, is different for different photograph elements (e.g. sky, mountain, grass). Thus, to get the most suitable patch, we propose a method, wherein we establish global region correspondence before local patch match. In our proposed method, BGMaker, (1) we divide the real and anime images into regions; (2) then, we automatically acquire correspondence between each region on the basis of color and texture features, and (3) search and synthesize the most suitable patch within the corresponding region. Our primary contribution in this paper is a method for automatically acquiring correspondence between target regions and source regions of different color and texture, which allows us to generate an anime background image while preserving the details of the source image.	BGMaker: example-based anime background image creation from a photograph	NA:NA:NA:NA:NA	2015
Siran Liu:Ergun Akleman	In this work, we have developed an approach to include global illumination effects into Chinese Paintings (see Figure 1). Our method provides a robust approach to represent tone and value in a way similar to how Chinese Ink-and-Brush is painted. The method, especially, supports reflection, shadow, atmospheric, depth and weathering effects. Using the method, we can recapture the aesthetic of irregularity in shapes and forms commonly seen in Chinese Painting. We also arrange composition in 3D to obtain multi-camera imagee that matches the compositions in Chinese painting. We also included cinematic lighting aesthetic in 3D Chinese painting to enhance mood and storytelling.	Chinese ink and brush painting with reflections	NA:NA	2015
Jonah Friedman:Andrew C. Jones	In 3D production for commercials, television, and film, ID mattes are commonly used to modify rendered images without re-rendering. ID mattes are bitmap images used to isolate specific objects, or multiple objects, such as all of the buttons on a shirt. Many 3D pipelines are built to provide compositors with ID mattes in addition to beauty renders to allow flexibility.	Fully automatic ID mattes with support for motion blur and transparency	NA:NA	2015
Benjamin Knowles:Oleg Fryazinov	With the increasing quality of real-time graphics it is vital to make sure assets move in a convincing manner otherwise the players immersion can be broken. Grass is an important area as it can move substantially and often takes up a large portion of screen space in games. Animation of grass is a subject to academic research [Fernando 2004; Perbet and Cani 2001] as well as a technology which is implemented in a number of video games. The list includes, but is not limited to, games such as Far Cry 4, Battlefield 4, Dear Esther and Unigine Valley. Comparing video games assets with reality, it can be seen that the current methods have a number of problems which decrease the realism of the resulting grass animation. These problems include: 1) the visible planar nature of grass geometry and 2) problems with the grass movement which include over-connectivity of grass blades in respect to their neighbours, no obvious wind direction and exaggerated swaying motions. In this paper we propose to increase realism of the grass by focusing on its movement. The main contributions of this work are: 1) Distinguishing ambient and directional components of the wind and 2) The method for calculating directional wind by using a grayscale map and wind vector. The grass was implemented with vertex shaders in line with the majority of methods described in academic literature (e.g. [Fernando 2004]) and implemented in modern games.	Increasing realism of animated grass in real-time game environments	NA:NA	2015
Seungbae Bang:Byungkuk Choi:Roger Blanco I Ribera:Meekyoung Kim:Sung-Hee Lee:Junyong Noh	Skeleton-driven animation is a widespread technique, which is frequently used in film and video game productions to animate 3D characters. The process of preparing characters for skeletal animation is referred to as character rigging. Commercial applications such as Maya or 3DS Max provide many tool that support this process, including the 'joint tool' and the 'paint skin weights tool'. Most of these tools are difficult to use for novice users. Even for professional artists, it requires many hours of intensive effort.	Interactive rigging	NA:NA:NA:NA:NA:NA	2015
Simon Pabst:Hansung Kim:Lukáš Polok:Viorela Ila:Ted Waine:Adrian Hilton:Jeff Clifford	Modern digital film production uses large quantities of data captured on-set, such as videos, digital photographs, LIDAR scans, spherical photography and many other sources to create the final film frames. The processing and management of this massive amount of heterogeneous data consumes enormous resources. We propose an integrated pipeline for 2D/3D data registration aimed at film production, based around the prototype application Jigsaw. It allows users to efficiently manage and process various data types from digital photographs to 3D point clouds. A key step in the use of multi-modal 2D/3D data for content production is the registration into a common coordinate frame (match moving). 3D geometric information is reconstructed from 2D data and registered to the reference 3D models using 3D feature matching [Kim and Hilton 2014]. We present several highly efficient and robust approaches to this problem. Additionally, we have developed and integrated a fast algorithm for incremental marginal covariance calculation [Ila et al. 2015]. This allows us to estimate and visualize the 3D reconstruction error directly on-set, where insufficient coverage or other problems can be addressed right away. We describe the fast hybrid multi-core and GPU accelerated techniques that let us run these algorithms on a laptop. Jigsaw has been used and evaluated in several major digital film productions and significantly reduced the time and work required to manage and process on-set data.	Jigsaw: multi-modal big data management in digital film production	NA:NA:NA:NA:NA:NA:NA	2015
Daniel Camozzato:Leandro Dihl:Ivan Silveira:Fernando Marson:Soraia R. Musse	Computer graphics applications require models which are crafted by hand, requiring skill and time. In architectural applications an automated system that converts 2D floor plan images into 3D building models can be used to lower the modeling cost, and in video games procedural algorithms can be used to generate content, including cities, buildings and floor plans. One motivation to generate floor plans for predetermined building exteriors is that building generators often create only a façade without the interior. Another motivation is in planning real-world layouts, often tackled as an optimization problem (e.g. [Merrell et al. 2010] and [Peng et al. 2014]). The state of the art in [Merrell et al. 2010] uses machine learning and stochastic optimization to generate realistic layouts. However, it is unsuitable in our case because the exterior appears as a result of the layout. Our approach generates floor plans using both the building exterior and user requisites as constraints. The proposed method [Camozzato et al. 2015] handles a variety of image styles and building shapes, and the run time remains low (around 1 ms versus a 30 s optimization reported by [Merrell et al. 2010]).	Procedural floor plan generation from building sketches	NA:NA:NA:NA:NA	2015
Chun-Kai Huang:Yi-Ling Chen:I-Chao Shen:Bing-Yu Chen	We introduce an interactive method suitable for retargeting both 3D objects and scenes under a general framework. Initially, an input object or scene is decomposed into a collection of constituent components embraced by corresponding control bounding volumes which capture the intra-structures of the object or the semantic groupings of the objects in the scene. The overall retargeting is accomplished through a constrained optimization by manipulating the control bounding volumes. Without inferring the intricate dependencies between the components, we define a minimal set of constraints that maintain the spatial arrangement and connectivity between the components to regularize valid retargeting results. The default retargeting behavior can then be easily altered by additional semantic constraints imposed by users.	Retargeting 3D objects and scenes	NA:NA:NA:NA	2015
Yu Wang:Marc Olano	We present a framework for modeling solid-fluid phase change. Our framework is physically-motivated, with geometric constraints applied to define rigid dynamics using shape matching. In each simulation step, particle positions are updated using an extended SPH solver where they are treated as fluid. Then a geometric constraint is computed based on current particle configuration, which consists of an optimal translation and an optimal rotation. Our approach differs from methods such as [Carlson et al. 2004] in that we solve rigid dynamics by using a stable geometric constraint [Müller et al. 2005] embedded in a fluid simulator.	Rigid fluid	NA:NA	2015
Katsuhisa Kanazawa:Ryoma Tanabe:Tomoaki Moriya:Tokiichiro Takahashi	Realistic representation of nature scenes is one of the most challenging areas in computer graphics community. There are important factors to synthesize realistic scenes in 3D CG which are decayed materials such as dead trees, weathered statues, rusty metals and so on. We are interested in the methodology for simulating its decaying processes. In this paper, we propose a simple method for rust aging simulation based on a probabilistic cellular automaton model taking into account object's geometries.	Rust aging simulation considering object's geometries	NA:NA:NA:NA	2015
I Chiang:Po-Han Lin:Yuan-Hung Chang:Ming Ouhyoung	Synthesizing competitive interactions between two avatars in a physics-based simulation remains challenging. Most previous works rely on reusing motion capture data. They also need an offline preprocessing step to either build motion graphs or perform motion analysis. On the other hand, an online motion synthesis algorithm [Hämäläinen et al. 2014] can produce physically plausible motions including balance recovery and dodge projectiles without prior data. They use a kd-tree sequential Monte Carlo sampler to optimize the joint angle trajectories. We extend their approach and propose a new objective function to create two-character animations in a close-range combat. The principles of attack and defense are designed according to fundamental theory of Chinese martial arts. Instead of following a series of fixed Kung Fu forms, our method gives 3D avatars the freedom to explore diverse movements and through pruning can finally evolve an optimal way for fighting.	Synthesizing close combat using sequential Monte Carlo	NA:NA:NA:NA	2015
Jaehwan Kim:JongYoul Park:Kyoung Park	Unsupervised matting, whose goal is to extract interesting foreground components from arbitrary and natural background regions without any additional information of the contents of the corresponding scenes, plays an important role in many computer vision and graphics applications. Especially, the precisely extracted object images from the matting process can be useful for automatic generation of large-scale annotated training sets with more accuracy, as well as for improving the performance of a variety of applications including content-based image retrieval. However, unsupervised matting problem is intrinsically ill-posed so that it is hard to generate a perfect segmented object matte from a given image without any prior knowledge. This additional information is usually fed by means of a trimap which is a rough pre-segmented image consisting of three subregions of foreground, background and unknown. When such matting process is applied to object collections in a large-scale image set, the requirement for manually specifying every trimap for each of independent input images can be a serious drawback definitely. Recently, automatic detection of salient object regions in images has been widely researched in computer vision tasks including image segmentation, object recognition and so on. Although there are many different types of proposal measures in methodology under the common perceptual assumption of a salient region standing out its surrounding neighbors and capturing the attention of a human observer, most final saliency maps having lots of noises are not sufficient to take advantage of the consequent computational processes of highly accurate low-level representation of images.	UnAMT: unsupervised adaptive matting tool for large-scale object collections	NA:NA:NA	2015
Naoki Nozawa:Daiki Kuwahara:Shigeo Morishima	A reconstruction of a human face shape from a single image is an important theme for criminal investigation such as recognition of suspected people from surveillance cameras with only a few frames. It is, however, still difficult to recover a face shape from a non-frontal face image. Method using shading cues on a face depends on the lighting circumstance and cannot be adapted to images in which shadows occurs, for example [Kemelmacher et al. 2011]. On the other hand, [Blanz et al. 2004] reconstructed a shape by 3D Morphable Model (3DMM) only with facial feature points. This method, however, requires the pose-wise correspondences of vertices in the model to feature points of input image because a face contour cannot be seen when the facial direction is not the front. In this paper, we propose a method which can reconstruct a facial shape from a non-frontal face image only with a single general correspondence table. Our method searches for the correspondences of points on a facial contour in the iterative reconstruction process, and makes the reconstruction simple and stable.	3D face reconstruction from a single non-frontal face image	NA:NA:NA	2015
Toshihiko Yamasaki:Yusuke Nakano:Kiyoharu Aizawa	3D printing is becoming a more common technology and has a growing number of applications. Although 3D compression algorithms have been studied in the computer graphics (CG) community for decades, the quality of the compressed 3D models are discussed only in the CG space. In this paper, we discuss the relationship between the PSNR of the compressed 3D models and the human perception to the printed objects. We conducted subjective evaluation by inviting 13 people and found that there is a clear linear relationship between them. Such a quality perception model is useful for estimating the printing quality of the compressed 3D models and deciding reasonable compression parameters.	A prediction model on 3D model compression and its printed quality based on subjective study	NA:NA:NA	2015
Toru Kawanabe:Tomoko Hashida	In recent years, there has been rapid development of techniques for superimposing virtual information on real-world scenes and changing the appearance of actual scenes in arbitrary ways. We are particularly interested in means of arbitrarily changing the appearance of real-world scenes without the use of physical interfaces such as glasses or other devices worn by the user. In this paper, we refer to such means as spatial displays. Typical examples of spatial displays include a system that can change the transparency or physical properties of buildings [Rekimoto, 2012] and a system that projects video images [Raskar, 2001]. However, those systems have restrictions such as requiring some kind of physical interface between the user and the scene or not being usable in a well-lit environment. Taking a different approach, we turned our attention to a natural phenomenon referred to as heat haze, in which the appearance of objects is altered by changes in the refractive index of air caused by differences in temperature distribution. We propose the atmoRefractor, a system that can generate and control heat haze on a small scale without an additional physical interface such as lenses. That locally controllable heat haze effect can be used to direct attention by changing the appearance of certain parts of scenes.	atmoRefractor: spatial display by controlling heat haze	NA:NA	2015
Tony Tung	Consumer RGBD sensors are becoming ubiquitous and can be found in many devices such as laptops (e.g., Intel's RealSense) or tablets (e.g., Google Tango, Structure, etc.). They have become popular in graphics, vision, and HCI communities as they enable numerous applications such as 3D capture, gesture recognition, virtual fitting, etc. Nowadays, common sensors can deliver a stream of color images and depth maps in VGA resolution at 30 fps. While the color image is usually of sufficient quality for visualization, depth information (represented as a point cloud) is usually too sparse and noisy for readable rendering.	Augmented dynamic shape for live high quality rendering	NA	2015
Nobuhiko Mukai:Naoki Mita:Youngha Chang	[Hong et al. 2008] proposed a hybrid method of Eulerian grids and Lagrangian particles to represent small-scale bubbles in large-scale water. In the simulation, bubbles rise freely in the water; however, bubble seeds are set at random at the bottom and they disappear as soon as they arrive at the water surface. [Patkar et al. 2013] also proposed a hybrid Lagrangian-Eulerian framework to visualize both small and large scale bubbles. A bubble that exits the spout of a water dispenser flows up in the water with changing its shape; however, they did not simulate the rupture process of bubble. Then, [Mukai et al. 2012] tried a rupture simulation by using MPS method; however, it did not consider the high density ratio of the water to the air so that the bubble ruptured gradually. Therefore, this paper proposes a method of bubble rupture simulation, where a wave is generated after a bubble has ruptured rapidly, by considering the high density ratio of the water to the air.	Bubble rupture simulation by considering high density ratio	NA:NA:NA	2015
Yajie Yan:Tao Ju:David Letscher:Erin Chambers	Medial axis is a classical shape descriptor that is widely used in computer graphics, computer vision, and pattern recognition. Defined elegantly as the locus of points with multiple nearest neighbors on the object boundary, the medial axis preserves both the structure and topology of the object in a compact form - a geometry that has one lower dimension than the object itself.	Burning the medial axis	NA:NA:NA:NA	2015
Hisashi Watanabe:Toshiya Fujii:Tatsuya Nakamura:Tsuguhiro Korenaga	It is a common philosophical question as to whether your blue is the same as my blue. The two-tone striped dress shown in Figure 1, which attracted a lot of attention on the Internet, gave us a clear answer: "No." Some people see the dress as blue and black, whereas others insist it's white and gold. So your blue can be my white. Why is it that people looking at the same picture perceive totally different color combinations?	Color perception difference: white and gold, or black and blue?	NA:NA:NA:NA	2015
Yang Kang:Chi Xu:Shujin Lin:Songhua Xu:Xiaonan Luo:Qiang Chen	Sketching is a natural human practice. With the popularity of multi-touch tablets and styluses, sketching has become a more popular means of human-computer interaction. However, accurately recognizing sketches is rather challenging, especially when they are drawn by non-professionals. Therefore, automatic sketch understanding has attracted much research attention. To tackle the problem, we propose to segment sketch drawings before analyzing the semantic meanings of sketches for the purpose of developing a sketch-based 3D model retrieval system.	Component segmentation of sketches used in 3D model retrieval	NA:NA:NA:NA:NA:NA	2015
Afsaneh Rafighi:Sahand Seifi:Oscar Meruvia-Pastor	This paper presents a novel method for automatic registration of video streams originated from two depth-sensing cameras. The system consists of a sender and receiver, in which the sender obtains the streams from two RGBD sensors placed arbitrarily around a room and produces a unified scene as a registered point cloud. A conventional method to support a multi-depth sensor system is through calibration. However, calibration methods are time consuming and require the use of external markers prior to streaming. If the cameras are moved, calibration has to be repeated. The motivation of this work is to facilitate the use of RGBD sensors for non-expert users, so that cameras need not to be calibrated, and if cameras are moved, the system will automatically recover the alignment of the video streams. DeReEs [Seifi et al. 2014], a new registration algorithm, is used, since it is fast and successful in registering scenes with small overlapping sections.	Continuous and automatic registration of live RGBD video streams with partial overlapping views	NA:NA:NA	2015
Michelle Holloway:Tao Ju:Cindy Grimm	In clinical practice, when a subject is imaged (i.e. CT scan or MRI) the result is a 3D image of volumetric data. In order to study the organ, bone, or other object of interest, this data needs to be segmented to obtain a 3D model that can be used in any number of down stream applications. When used for treatment planning these segmentations need to not only be accurate but also produced quickly to avoid health risks. Automatic segmentation methods are becoming more reliable but many experts in the scientific community still rely on time consuming manual segmentation.	Contour guided surface deformation for volumetric segmentation	NA:NA:NA	2015
Peihong Guo:Ergun Akleman:He Ying:Xiaoning Wang:Wei Liu	In this work, we present some of the unexpected observations resulted from our recent research. We, recently, needed to identify a small number of important critical points, i.e. minimum, maximum and saddle points, on a given manifold mesh surface. All critical points on a manifold triangular mesh can be identified using discrete Gaussian curvature, which is given as ki = 2π − Σj θi,j where ki is vertex defect (the discrete Gaussian curvature) of the vertex i and θi,j is the corner of the vertex in the triangle j. A very useful property coming with vertex defect is the discrete version of Gauss-Bonnet theorem: the sum of all vertex defects is always constant as Σi ki = 2π(2−2g) where g is the genus of the mesh. Any vertex with a non-zero vertex defect is really an critical point of the surface. However, identification of interesting critical points is hard with vertex defect alone. As it can be seen in Figure 1(a), even we ignore vertex defects that are small, too many vertices are still chosen and this information is not really useful to make any conclusion of the shape of the surface.	Critical points with discrete Morse theory	NA:NA:NA:NA:NA	2015
Nahomi Maki:Kazuhisa Yanaka	Various colors, such as in a prism, are observed in properly cut diamond even under white light because of dispersion. Properly-cut diamond brings about scintillation when viewing angle is changed, because total reflection inside a diamond tends to occur frequently due to the large refractive index. Moreover, strong rainbow colors are seen because of high dispersion ratio.	Display of diamond dispersion using wavelength-division rendering and integral photography	NA:NA	2015
Slim Ouni:Guillaume Gris	One main concern of audiovisual speech research is the intelligibility of audiovisual speech (i.e., talking head). In fact, lip reading is crucial for challenged population as hard of hearing people. For audiovisual synthesis and animation, this suggests that one should pay careful attention to modeling the region of the face that participates actively during speech. Above all, a facial animation system needs extremely good representations of lip motion and deformation in order to achieve realism and effective communication.	Dynamic realistic lip animation using a limited number of control points	NA:NA	2015
Byeongjun Choi:Woong Seo:Insung Ihm	In the ray-tracing community, the surface-area heuristic (SAH) has been employed as a de facto standard strategy for building a high-quality kd-tree. Aiming to improve both time and space efficiency of the conventional SAH-based kd-tree in ray tracing, we propose to use an extended kd-tree representation for which an effective tree-construction algorithm is provided. Our experiments with several test scenes revealed that the presented kd-tree scheme significantly reduced the memory requirement for representing the tree structure, while also increasing the overall frame rate for rendering.	Enhancing time and space efficiency of kd-tree for ray-tracing static scenes	NA:NA:NA	2015
Hisataka Suzuki:Rex Hsieh:Ryotaro Tsuda:Akihiko Shirai	In recent years, 3D technology has become so widespread that the technology alone no longer fascinates the viewers. To achieve further technical innovations on display experience, we should explore the limitations of 3D devices.	ExPixel FPGA: multiplex hidden imagery for HDMI video sources	NA:NA:NA:NA	2015
Yoichi Ochiai:Kota Kumagai:Takayuki Hoshi:Jun Rekimoto:Satoshi Hasegawa:Yoshio Hayasaki	We envision a laser-induced plasma technology in general applications for public use. If laser-induced plasma aerial images were made available, many useful applications such as spatial aerial AR, aerial user interfaces, volumetric images could be produced. This would be a highly effective display for the expression of three-dimensional information. Volumetric expression has considerable merit because the content scale corresponds to the human body; therefore, this technology could be usefully applied to wearable materials and spatial user interactions. Further, laser focusing technology can add an additional dimension to conventional projection technology, which is designed for surface mapping, while laser focusing technology is capable of volumetric mapping. This technology can be effectively used in real-world-oriented user interfaces.	Fairy lights in femtoseconds: aerial and volumetric graphics rendered by focused femtosecond laser combined with computational holographic fields	NA:NA:NA:NA:NA:NA	2015
Jérémy Levallois:David Coeurjolly:Jacques-Olivier Lachaud	During a snowfall, the snow crystals accumulate on the ground and gradually form a complex porous medium constituted of air, water vapour, ice and sometimes liquid water. This ground-lying snow transforms with time, depending on the physical parameters of the environment. The main purpose of the digitalSnow project is to provide efficient computational tools to study the metamorphism of real snow microstructures from 3D images acquired using X tomography techniques. We design 3D image-based numerical models than can simulate the shape evolution of the snow microstructure during its metamorphism. As a key measurement, (mean) curvature of snow microstructure boundary plays a crucial role in metamorphosis equations (mostly driven by mean curvature flow). In our previous work, we have proposed robust 2D curvature and 3D mean and principal curvatures estimators using integral invariants. In short, curvature quantities are estimated using a spherical convolution kernel with given radius R applied on point surfaces [Coeurjolly et al. 2014]. The specific aspect of these estimators is that they are defined on (isothetic) digital surfaces (boundary of shape in Z3). Tailored for this digital model, these estimators allow us to mathematically prove their multigrid convergence, i.e. for a class of mathematical shapes (e.g. C3-boundary and bounded positive curvature), the estimated quantity converges to the underlying Euclidean one when shapes are digitized on grids with gridstep tending to zero. In this work, we propose to use the radius R of our curvature estimators as a scale-space parameter to extract features on digital shapes. Many feature estimators exist in the literature, either on point clouds or meshes ("ridge-valley", threshold on principal curvatures, spectral analysis from Laplacian matrix eigenvalues, . . . ). In the context of objects in Z3 and using our robust curvature estimator, we define a new feature extraction approach on which theoretical results can be proven in the multigrid framework.	Feature extraction on digital snow microstructures	NA:NA:NA	2015
A. Andreadis:R. Gregor:I. Sipiran:P. Mavridis:G. Papaioannou:T. Schreck	The problem of object restoration from eroded fragments where large parts could be missing is of high relevance in archaeology. Manual restoration is possible and common in practice but it is a tedious and error-prone process, which does not scale well. Solutions for specific parts of the problem have been proposed but a complete reassembly and repair pipeline is absent from the bibliography. We propose a shape restoration pipeline consisting of appropriate methods for automatic fragment reassembly and shape completion. We demonstrate the effectiveness of our approach using real-world fractured objects.	Fractured 3D object restoration and completion	NA:NA:NA:NA:NA:NA	2015
Caigui Jiang:Chengcheng Tang:Jun Wang:Johannes Wallner:Helmut Pottmann	In freeform architecture and fabrication aware design, repetitive geometry is a very important contribution to the reduction of production costs. This poster addresses two closely related geometric rationalizations of freeform surfaces with repetitive elements: freeform honeycomb structures defined as torsion-free structures where the walls of cells meet at 120 degrees, and Lobel frames formed by equilateral triangles. There turns out to be an interesting duality between these two structures, and this poster discusses the geometric relation, computation, modeling as well as applications of them.	Freeform honeycomb structures and lobel frames	NA:NA:NA:NA:NA	2015
Antoine Toisoul:Abhijeet Ghosh	We present a novel approach for image based relighting using the lighting controls available in a regular room. We employ individual light sources available in the room such as windows and house lights as basis lighting conditions. We further optimize the projection of a desired lighting environment into the sparse room lighting basis in order to closely approximate the target lighting environment with the given lighting basis. We achieve plausible relit results that compare favourably with ground truth relighting with dense sampling of the reflectance field.	Image based relighting using room lighting basis	NA:NA	2015
Daniel Rakita:Tomislav Pejsa:Bilge Mutlu:Michael Gleicher	Motion-captured performances seldom include eye gaze, because capturing this motion requires eye tracking technology that is not typically part of a motion capture setup. Yet having eye gaze information is important, as it tells us what the actor was attending to during capture and it adds to the expressivity of their performance.	Inferring gaze shifts from captured body motion	NA:NA:NA:NA	2015
Hiroki Yamamoto:Hajime Kajita:Hanyuool Kim:Naoya Koizumi:Takeshi Naemura	In design process and medical visualization, e.g. CT/MRI cross-sectional images, exterior and interior images can help users to understand the overall shape of volumetric objects. For this purpose, displays need to provide both vertical and horizontal images at the same time. To display cross-sectional images, an LCD display [Cassinelli et al. 2009] and image projection [Nagakura et al. 2006] have been proposed. Although these displays could show internal images of volumetric objects, seamless crossing of internal and external images cannot be realized since the images are limited to physical displays.	Mid-air plus: a 2.5 D cross-sectional mid-air display with transparency control	NA:NA:NA:NA:NA	2015
Takuya Kato:Akira Kato:Naomi Okamura:Taro Kanai:Ryo Suzuki:Yuko Shirai	Trees have been a pillar of our lives not just for human but for all the species living in the earth. Despite of its blessings for our lives, the heaps of problems around forestry have not been solved. One of the major problems in this field is that most of the forest are not been sorted into an organized database. Detailed natural data have never been provided even in famous map applications, Google earth for instance, induced from its difficulty. The forest database has been demanded in many regions as it provides beneficial information for both industrial and environmental aspects. It even helps many divisions such as CG animations to simulate not only a tree itself but also the mountain or the forest as a whole depending on given natural conditions.	Musasabi: 2D/3D intuitive and detailed visualization system for the forest	NA:NA:NA:NA:NA:NA	2015
Beibei Wang:Xiangxu Meng:Tamy Boubekeur	Point-Based Global Illumination (PBGI) [2008] is a popular rendering method in special effects and motion picture productions. This algorithm provides a diffuse global illumination solution by caching radiance in a mesh-less hierarchical data structure during a pre-process, while solving for visibility over this cache, at rendering time and for each receiver, using microbuffers, which are localized depth and color buffer inspired from real time rendering environments. As a result, noise free ambient occlusion, indirect soft shadows and color bleeding effects are computed efficiently for high resolution image output and in a temporally coherent fashion. We propose an evolution of this method to address the case of non-diffuse inter-reflections and refractions using wavelets instead of spherical harmonics (see Fig. 1). We also propose a new importance-driven adaptive microbuffer model to capture accurately incoming radiance at a point. Furthermore, we evaluate outgoing radiance using a fast wavelet radiance product, containing the memory footprint by encoding hierarchically the wavelets tree.	Non-diffuse effects for point-based global illumination	NA:NA:NA	2015
Hajime Kajita:Naoya Koizumi:Takeshi Naemura	Mid-air imaging has the advantage of expression along the depth direction. For example, MARIO [1], a mid-air display, can form an image in the depth range of 30 cm by physically moving the light source display. Multi-layered mid-air images can be displayed at various depths, but such multi-layered images are transparent and experience color mixture due to the addition of light from the light source displays. It is difficult to see the front of transparent images because they have no occlusion expression.	OpaqueLusion: opaque mid-air images using dynamic mask for occlusion expression	NA:NA:NA	2015
Christian Hafner:Przemyslaw Musialski:Thomas Auzinger:Michael Wimmer:Leif Kobbelt	Keyboard percussion instruments such as xylophones and glockenspiels are composed of an arrangement of bars. These are varied in some of their geometrical properties---typically the length---in order to influence their acoustic behavior. Most instruments in this family do not deviate from simple geometrical shapes, since designing the natural frequency spectrum of complex shapes usually involves a pain-staking trial-and-error process and has been reserved to gifted artisans or professional manufacturers.	Optimization of natural frequencies for fabrication-aware shape modeling	NA:NA:NA:NA:NA	2015
Junichi Sugita:Tokiichiro Takahashi	Many people have been familiar with subtractive color model based on pigment color compositing since their early childhood. However, the RGB color space is not comprehensible for children due to additive color compositing. In the RGB color space, the resulting mixture color is often different from colors viewer expected. CMYK is a well-known subtractive color space, but its three primal colors are not familiar. Kubelka-Munk model (KM model in short) simulates pigment compositing as well as paint-like appearance by physically-based simulation. However, it is difficult to use KM model because of many simulation parameters.	Paint-like compositing based on RYB color model	NA:NA	2015
Naoki Hashimoto:Koki Kosaka	We propose a photometric compensation for projecting arbitrary images on practical surfaces of our everyday life. Although many previous proposals have achieved fine compensation at their experimental environments [Nayar et al. 2003], they cannot support practical targets including high-contrast texture. In order to adapt to such situation, we need a time-consuming iterative processing with camera feedback. Even though the iterative processing is applied, we cannot obtain fine compensation because no camera pixels of a projector-camera system (procam) correspond perfectly to the pixels of the projector [Mihara et al. 2014].	Photometric compensation for practical and complex textures	NA:NA	2015
Takefumi Hiraki:Issei Takahashi:Shotaro Goto:Shogo Fukushima:Takeshi Naemura	Forming images by using a swarm of mobile robots has emerged as a new platform for computer entertainment. Each robot has colored lighting, and the swarm represents various abstract patterns by using the lighting and the locomotion.	Phygital field: integrated field with visible images and robot swarm controlled by invisible images	NA:NA:NA:NA:NA	2015
Ari Rapkin Blenkhorn	The glory is a colorful atmospheric phenomenon which resembles a small circular rainbow on the front surface of a cloudbank. It is most frequently seen from aircraft when the observer is directly between the sun and the clouds. Glories are also sometimes seen by skydivers looking down through thin cloud layers. They are always centered around the shadow of the observer's head (or camera).	Real-time rendering of atmospheric glories	NA	2015
Hiroyuki Kubo:Kohe Tokoi:Yasuhiro Mukaigawa	To synthesize realistic translucent materials in computer graphics, it is necessary to simulate the effect of subsurface scattering. In previous works, several methods are proposed for rendering such materials in real-time. The screen space subsurface scattering (SSSS) is developed by Jimenez et al. [2009], yet the speed of rendering is not very practical for low-end computational environment, because screen space techniques require huge number of texture samplings. We previously propose a curvature-based shading method [Kubo et al. 2010] which approximates the effect of subsurface scattering according to the curvature. Since the curvature is determined by the surface shape of neighbors, it is not able to compute the effect of scattering light from the behind of the object. In this paper, we propose a novel shading method depending on the translucency magnitude which represents the significance of the subsurface scattering effect. According to the translucency magnitude, we modulate the reflectance to imitate the effect of subsurface scattering. Since this modulation is very simple to compute, we are able to render translucent materials in real-time not only in high-end workstations but also low-end mobile devices.	Real-time rendering of subsurface scattering according to translucency magnitude	NA:NA:NA	2015
Kang Zhang:Wuyi Yu:Mary Manhein:Warren Waggenspack:Xin Li	Geometric restoration that composes 3D fragmented pieces into the original complete object is an important computer graphics and geometric processing problem. Automatic and effective restoration has applications in many fields such as archeological reconstruction, digital heritage archiving, forensic evidence processing, to name a few. For example, archaeologists reconstruct ceramic fragments (sherds) into complete pots in order to analyze the information of the ancient society. Forensic scientists reassemble skull fragments into complete skull for face reconstruction and body identification. In both of these problems we need to solve a composition of digitized thin-shell fragments with different shapes, sizes, and resolutions. This problem remains very challenging.	Reassembling 3D thin shells using integrated template guidance and fracture region matching	NA:NA:NA:NA:NA	2015
Keita Sekijima:Hiroya Tanaka	Digital materials are discrete elements such as LEGO Blocks that it can be a kind of reconfigurable 3D matters. There are two advantages of using digital material rather than a continuous material. Firstly, it is easy to change the form after shaping by assembling and disassembling the elements. Secondly, There is never that the error of the part impacts the whole form in the shaping because the elements can be connected exactly by the joint system. There are many researches of digital material focus on the modular connection by press fitting or bonding. Such a digital material can't be assembled and disassembled smoothly after shaped. In our research, we designed the digital material "Kelvin Block" (figure 1a) that specialized in smoothly reconfiguring, and we developed the machine "3D Assembler" (figure 1b) to arrange Kelvin Blocks automatically. The size of Kelvin Block is 40mmx40mmx40mm that is optimized to the volume of the joint system.	Reconfigurable three-dimensional prototype system using digital materials	NA:NA	2015
Francisco Inácio:Jan P. Springer	Maintaining a high steady frame rate is an important aspect in interactive real-time graphics. It is mainly influenced by the number of objects and the number of lights to be processed for a 3d scene. The upper-bound effort for rendering a scene is then defined by the number of objects times the number of lights, i. e. O(NO · NL). Deferred shading reduces this upper bound to the number of objects plus the number of lights, i. e. O(NO + NL), by separating the rendering process into two phases: geometry processing and lighting evaluation. The geometry processing rasterizes all objects but only retains visible fragments in a G-Buffer for the current viewpoint. The lighting evaluation then only needs to process those surviving fragments to compute the final image (for the current viewpoint). Unfortunately, this approach not only trades computational effort for memory but also requires the re-creation of the G-Buffer every time the viewpoint changes. Additionally, transparent objects cannot be encoded into a G-Buffer and must be separately processed. Post-rendering 3d warping [Mark et al. 1997] is one particular technique that allows to create images from G-Buffer information for new viewpoints. However, this only works with sufficient fragment information. Objects not encoded in the G-Buffer, because they were not visible from the original viewpoint, will create visual artifacts at discontinuities between objects. We propose fragment-history volumes (FHV) to create novel viewpoints from a discrete representation of the entire scene using current graphics hardware and present an initial performance comparison.	Reducing geometry-processing overhead for novel viewpoint creation	NA:NA	2015
Fumiya Narita:Shunsuke Saito:Takuya Kato:Tsukasa Fukusato:Shigeo Morishima	Dressing virtual characters is necessary for many applications, while modeling clothing is a significant bottleneck. Therefore, it has been proposed that the idea of Garment Transfer for transfer-ring clothing model from one character to another character [Brouet et al. 2012]. In recent years, this idea has been extended to be applicable between characters in various poses and shapes [Narita et al. 2014]. However, texture design of clothing is not preserved in their method since they deform the source clothing model to fit the target body (see Figure 1(a)(c)).	Texture preserving garment transfer	NA:NA:NA:NA:NA	2015
Paul Kilgo:Jerry Tessendorf	A Monte Carlo multiple scattering technique for participating media is extended. Validation against an experimentally well-studied optics problem is discussed. Designing initial paths for a numerical integration of Feynman path integrals is posed. A plot of the resulting integration is discussed.	Toward validation of a Monte Carlo rendering technique	NA:NA	2015
Caleb Brose:Martin Thuo:Jeremy W. Sheaffer	We present a system for tracking the movement and deformation of drops of water in free fall and collision. Our data comes from a high-speed camera which records 60,000 frames per second. The data is noisy, and is compromised by an unfortunate camera angle and poor lighting which contribute to caustics, reflections, and shadows in the image. Given an input video, we apply techniques from image processing, computer vision and computational geometry to track the the droplet's position and shape. While our tool could monitor the movement of transparent fluids in a more general environment, our data specifically depicts water colliding with hydrophobic materials. The output of our processing is used by materials scientists to better our understanding of the interactions between water and hydrophobic surfaces. These interactions have direct application in the materials engineering of next generation printing technologies.	Tracking water droplets under descent and deformation	NA:NA:NA	2015
Xueming Yu:Shanhe Wang:Jay Busch:Thai Phan:Tracy McSheery:Mark Bolas:Paul Debevec	High-end facial performance capture solutions typically use head-mounted camera systems which provide one or more close-up video streams of each actor's performance. These provide clear views of each actor's performance, but can be bulky, uncomfortable, get in the way of sight lines, and prevent actors from getting close to each other. To address this, we propose a virtual head-mounted camera system: an array of cameras placed around around the performance capture volume which automatically track zoomed-in, sharply focussed, high-resolution views of the each actor's face from a multitude of directions. The resulting imagery can be used in conjunction with body motion capture data to derive nuanced facial performances without head-mounted cameras.	Virtual headcam: pan/tilt mirror-based facial performance tracking	NA:NA:NA:NA:NA:NA:NA	2015
Adam Finkelstein	NA	Session details: Drawing, painting & stylization	NA	2018
Yong Jae Lee:C. Lawrence Zitnick:Michael F. Cohen	We present ShadowDraw, a system for guiding the freeform drawing of objects. As the user draws, ShadowDraw dynamically updates a shadow image underlying the user's strokes. The shadows are suggestive of object contours that guide the user as they continue drawing. This paradigm is similar to tracing, with two major differences. First, we do not provide a single image from which the user can trace; rather ShadowDraw automatically blends relevant images from a large database to construct the shadows. Second, the system dynamically adapts to the user's drawings in real-time and produces suggestions accordingly. ShadowDraw works by efficiently matching local edge patches between the query, constructed from the current drawing, and a database of images. A hashing technique enforces both local and global similarity and provides sufficient speed for interactive feedback. Shadows are created by aggregating the edge maps from the best database matches, spatially weighted by their match scores. We test our approach with human subjects and show comparisons between the drawings that were produced with and without the system. The results show that our system produces more realistically proportioned line drawings.	ShadowDraw: real-time user guidance for freehand drawing	NA:NA:NA	2018
Johannes Schmid:Martin Sebastian Senn:Markus Gross:Robert W. Sumner	We present a technique to generalize the 2D painting metaphor to 3D that allows the artist to treat the full 3D space as a canvas. Strokes painted in the 2D viewport window must be embedded in 3D space in a way that gives creative freedom to the artist while maintaining an acceptable level of controllability. We address this challenge by proposing a canvas concept defined implicitly by a 3D scalar field. The artist shapes the implicit canvas by creating approximate 3D proxy geometry. An optimization procedure is then used to embed painted strokes in space by satisfying different objective criteria defined on the scalar field. This functionality allows us to implement tools for painting along level set surfaces or across different level sets. Our method gives the power of fine-tuning the implicit canvas to the artist using a unified painting/sculpting metaphor. A sculpting tool can be used to paint into the implicit canvas. Rather than adding color, this tool creates a local change in the scalar field that results in outward or inward protrusions along the field's gradient direction. We address a visibility ambiguity inherent in 3D stroke rendering with a depth offsetting method that is well suited for hardware acceleration. We demonstrate results with a number of 3D paintings that exhibit effects difficult to realize with existing systems.	OverCoat: an implicit canvas for 3D painting	NA:NA:NA:NA	2018
Derek Nowrouzezahrai:Jared Johnson:Andrew Selle:Dylan Lacewell:Michael Kaschalk:Wojciech Jarosz	We present a method for generating art-directable volumetric effects, ranging from physically-accurate to non-physical results. Our system mimics the way experienced artists think about volumetric effects by using an intuitive lighting primitive, and decoupling the modeling and shading of this primitive. To accomplish this, we generalize the physically-based photon beams method to allow arbitrarily programmable simulation and shading phases. This provides an intuitive design space for artists to rapidly explore a wide range of physically-based as well as plausible, but exaggerated, volumetric effects. We integrate our approach into a real-world production pipeline and couple our volumetric effects to surface shading.	A programmable system for artistic volumetric lighting	NA:NA:NA:NA:NA:NA	2018
Michael Kass:Davide Pesare	A wide variety of non-photorealistic rendering techniques make use of random variation in the placement or appearance of primitives. In order to avoid the "shower-door" effect, this random variation should move with the objects in the scene. Here we present coherent noise tailored to this purpose. We compute the coherent noise with a specialized filter that uses the depth and velocity fields of a source sequence. The computation is fast and suitable for interactive applications like games.	Coherent noise for non-photorealistic rendering	NA:NA	2018
Karen Liu	NA	Session details: Capturing & modeling humans	NA	2018
Takaaki Shiratori:Hyun Soo Park:Leonid Sigal:Yaser Sheikh:Jessica K. Hodgins	Motion capture technology generally requires that recordings be performed in a laboratory or closed stage setting with controlled lighting. This restriction precludes the capture of motions that require an outdoor setting or the traversal of large areas. In this paper, we present the theory and practice of using body-mounted cameras to reconstruct the motion of a subject. Outward-looking cameras are attached to the limbs of the subject, and the joint angles and root pose are estimated through non-linear optimization. The optimization objective function incorporates terms for image matching error and temporal continuity of motion. Structure-from-motion is used to estimate the skeleton structure and to provide initialization for the non-linear optimization procedure. Global motion is estimated and drift is controlled by matching the captured set of videos to reference imagery. We show results in settings where capture would be difficult or impossible with traditional motion capture systems, including walking outside and swinging on monkey bars. The quality of the motion reconstruction is evaluated by comparing our results against motion capture data produced by a commercially available optical system.	Motion capture from body-mounted cameras	NA:NA:NA:NA:NA	2018
Feng Xu:Yebin Liu:Carsten Stoll:James Tompkin:Gaurav Bharaj:Qionghai Dai:Hans-Peter Seidel:Jan Kautz:Christian Theobalt	We present a method to synthesize plausible video sequences of humans according to user-defined body motions and viewpoints. We first capture a small database of multi-view video sequences of an actor performing various basic motions. This database needs to be captured only once and serves as the input to our synthesis algorithm. We then apply a marker-less model-based performance capture approach to the entire database to obtain pose and geometry of the actor in each database frame. To create novel video sequences of the actor from the database, a user animates a 3D human skeleton with novel motion and viewpoints. Our technique then synthesizes a realistic video sequence of the actor performing the specified motion based only on the initial database. The first key component of our approach is a new efficient retrieval strategy to find appropriate spatio-temporally coherent database frames from which to synthesize target video frames. The second key component is a warping-based texture synthesis approach that uses the retrieved most-similar database frames to synthesize spatio-temporally coherent target video frames. For instance, this enables us to easily create video sequences of actors performing dangerous stunts without them being placed in harm's way. We show through a variety of result videos and a user study that we can synthesize realistic videos of people, even if the target motions and camera views are different from the database content.	Video-based characters: creating new human performances from a multi-view video database	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Tom Funkhouser	NA	Session details: Understanding shapes	NA	2018
Maks Ovsjanikov:Wilmot Li:Leonidas Guibas:Niloy J. Mitra	As large public repositories of 3D shapes continue to grow, the amount of shape variability in such collections also increases, both in terms of the number of different classes of shapes, as well as the geometric variability of shapes within each class. While this gives users more choice for shape selection, it can be difficult to explore large collections and understand the range of variations amongst the shapes. Exploration is particularly challenging for public shape repositories, which are often only loosely tagged and contain neither point-based nor part-based correspondences. In this paper, we present a method for discovering and exploring continuous variability in a collection of 3D shapes without correspondences. Our method is based on a novel navigation interface that allows users to explore a collection of related shapes by deforming a base template shape through a set of intuitive deformation controls. We also help the user to select the most meaningful deformations using a novel technique for learning shape variability in terms of deformations of the template. Our technique assumes that the set of shapes lies near a low-dimensional manifold in a certain descriptor space, which allows us to avoid establishing correspondences between shapes, while being rotation and scaling invariant. We present results on several shape collections taken directly from public repositories.	Exploration of continuous variability in collections of 3D shapes	NA:NA:NA:NA	2018
Matthew Fisher:Manolis Savva:Pat Hanrahan	Modeling virtual environments is a time consuming and expensive task that is becoming increasingly popular for both professional and casual artists. The model density and complexity of the scenes representing these virtual environments is rising rapidly. This trend suggests that data-mining a 3D scene corpus could be a very powerful tool enabling more efficient scene design. In this paper, we show how to represent scenes as graphs that encode models and their semantic relationships. We then define a kernel between these relationship graphs that compares common virtual substructures in two graphs and captures the similarity between their corresponding scenes. We apply this framework to several scene modeling problems, such as finding similar scenes, relevance feedback, and context-based model search. We show that incorporating structural relationships allows our method to provide a more relevant set of results when compared against previous approaches to model context search.	Characterizing structural relationships in scenes using graph kernels	NA:NA:NA	2018
Siddhartha Chaudhuri:Evangelos Kalogerakis:Leonidas Guibas:Vladlen Koltun	Assembly-based modeling is a promising approach to broadening the accessibility of 3D modeling. In assembly-based modeling, new models are assembled from shape components extracted from a database. A key challenge in assembly-based modeling is the identification of relevant components to be presented to the user. In this paper, we introduce a probabilistic reasoning approach to this problem. Given a repository of shapes, our approach learns a probabilistic graphical model that encodes semantic and geometric relationships among shape components. The probabilistic model is used to present components that are semantically and stylistically compatible with the 3D model that is being assembled. Our experiments indicate that the probabilistic model increases the relevance of presented components.	Probabilistic reasoning for assembly-based 3D modeling	NA:NA:NA:NA	2018
Adam Bargteil	NA	Session details: Contact & constraints	NA	2018
David I. W. Levin:Joshua Litven:Garrett L. Jones:Shinjiro Sueda:Dinesh K. Pai	Simulating viscoelastic solids undergoing large, nonlinear deformations in close contact is challenging. In addition to inter-object contact, methods relying on Lagrangian discretizations must handle degenerate cases by explicitly remeshing or resampling the object. Eulerian methods, which discretize space itself, provide an interesting alternative due to the fixed nature of the discretization. In this paper we present a new Eulerian method for viscoelastic materials that features a collision detection and resolution scheme which does not require explicit surface tracking to achieve accurate collision response. Time-stepping with contact is performed by the efficient solution of large sparse quadratic programs; this avoids constraint sticking and other difficulties. Simulation and collision processing can share the same uniform grid, making the algorithm easy to parallelize. We demonstrate an implementation of all the steps of the algorithm on the GPU. The method is effective for simulation of complicated contact scenarios involving multiple highly deformable objects, and can directly simulate volumetric models obtained from medical imaging techniques such as CT and MRI.	Eulerian solid simulation with contact	NA:NA:NA:NA:NA	2018
Aleka McAdams:Yongning Zhu:Andrew Selle:Mark Empey:Rasmus Tamstorf:Joseph Teran:Eftychios Sifakis	We present a new algorithm for near-interactive simulation of skeleton driven, high resolution elasticity models. Our methodology is used for soft tissue deformation in character animation. The algorithm is based on a novel discretization of corotational elasticity over a hexahedral lattice. Within this framework we enforce positive definiteness of the stiffness matrix to allow efficient quasistatics and dynamics. In addition, we present a multigrid method that converges with very high efficiency. Our design targets performance through parallelism using a fully vectorized and branch-free SVD algorithm as well as a stable one-point quadrature scheme. Since body collisions, self collisions and soft-constraints are necessary for real-world examples, we present a simple framework for enforcing them. The whole approach is demonstrated in an end-to-end production-level character skinning system.	Efficient elasticity for character skinning with contact and collisions	NA:NA:NA:NA:NA:NA:NA	2018
Changxi Zheng:Doug L. James	Contact sound models based on linear modal analysis are commonly used with rigid body dynamics. Unfortunately, treating vibrating objects as "rigid" during collision and contact processing fundamentally limits the range of sounds that can be computed, and contact solvers for rigid body animation can be ill-suited for modal contact sound synthesis, producing various sound artifacts. In this paper, we resolve modal vibrations in both collision and frictional contact processing stages, thereby enabling non-rigid sound phenomena such as micro-collisions, vibrational energy exchange, and chattering. We propose a frictional multibody contact formulation and modified Staggered Projections solver which is well-suited to sound rendering and avoids noise artifacts associated with spatial and temporal contact-force fluctuations which plague prior methods. To enable practical animation and sound synthesis of numerous bodies with many coupled modes, we propose a novel asynchronous integrator with model-level adaptivity built into the frictional contact solver. Vibrational contact damping is modeled to approximate contact-dependent sound dissipation. Results are provided that demonstrate high-quality contact resolution with sound.	Toward high-quality modal contact sound	NA:NA	2018
Shinjiro Sueda:Garrett L. Jones:David I. W. Levin:Dinesh K. Pai	A significant challenge in applications of computer animation is the simulation of ropes, cables, and other highly constrained strandlike physical curves. Such scenarios occur frequently, for instance, when a strand wraps around rigid bodies or passes through narrow sheaths. Purely Lagrangian methods designed for less constrained applications such as hair simulation suffer from difficulties in these important cases. To overcome this, we introduce a new framework that combines Lagrangian and Eulerian approaches. The two key contributions are the reduced node, whose degrees of freedom precisely match the constraint, and the Eulerian node, which allows constraint handling that is independent of the initial discretization of the strand. The resulting system generates robust, efficient, and accurate simulations of massively constrained systems of rigid bodies and strands.	Large-scale dynamic simulation of highly constrained strands	NA:NA:NA:NA	2018
Karol Myszkowski	NA	Session details: Tone editing	NA	2018
Rafat Mantiuk:Kil Joong Kim:Allan G. Rempel:Wolfgang Heidrich	Visual metrics can play an important role in the evaluation of novel lighting, rendering, and imaging algorithms. Unfortunately, current metrics only work well for narrow intensity ranges, and do not correlate well with experimental data outside these ranges. To address these issues, we propose a visual metric for predicting visibility (discrimination) and quality (mean-opinion-score). The metric is based on a new visual model for all luminance conditions, which has been derived from new contrast sensitivity measurements. The model is calibrated and validated against several contrast discrimination data sets, and image quality databases (LIVE and TID2008). The visibility metric is shown to provide much improved predictions as compared to the original HDR-VDP and VDP metrics, especially for low luminance conditions. The image quality predictions are comparable to or better than for the MS-SSIM, which is considered one of the most successful quality metrics. The code of the proposed metric is available on-line.	HDR-VDP-2: a calibrated visual metric for visibility and quality predictions in all luminance conditions	NA:NA:NA:NA	2018
Michael D. Tocci:Chris Kiser:Nora Tocci:Pradeep Sen	Although High Dynamic Range (HDR) imaging has been the subject of significant research over the past fifteen years, the goal of acquiring cinema-quality HDR images of fast-moving scenes using available components has not yet been achieved. In this work, we present an optical architecture for HDR imaging that allows simultaneous capture of high, medium, and low-exposure images on three sensors at high fidelity with efficient use of the available light. We also present an HDR merging algorithm to complement this architecture, which avoids undesired artifacts when there is a large exposure difference between the images. We implemented a prototype high-definition HDR-video system and we present still frames from the acquired HDR video, tonemapped with various techniques.	A versatile HDR video production system	NA:NA:NA:NA	2018
Adam G. Kirk:James F. O'Brien	In this paper we present a perceptually based algorithm for modeling the color shift that occurs for human viewers in low-light scenes. Known as the Purkinje effect, this color shift occurs as the eye transitions from photopic, cone-mediated vision in well-lit scenes to scotopic, rod-mediated vision in dark scenes. At intermediate light levels vision is mesopic with both the rods and cones active. Although the rods have a spectral response distinct from the cones, they still share the same neural pathways. As light levels decrease and the rods become increasingly active they cause a perceived shift in color. We model this process so that we can compute perceived colors for mesopic and scotopic scenes from spectral image data. We also describe how the effect can be approximated from standard high dynamic range RGB images. Once we have determined rod and cone responses, we map them to RGB values that can be displayed on a standard monitor to elicit the intended color perception when viewed photopically. Our method focuses on computing the color shift associated with low-light conditions and leverages current HDR techniques to control the image's dynamic range. We include results generated from both spectral and RGB input images.	Perceptually based tone mapping for low-light conditions	NA:NA	2018
Robert Carroll:Ravi Ramamoorthi:Maneesh Agrawala	Changing the color of an object is a basic image editing operation, but a high quality result must also preserve natural shading. A common approach is to first compute reflectance and illumination intrinsic images. Reflectances can then be edited independently, and recomposed with the illumination. However, manipulating only the reflectance color does not account for diffuse interreflections, and can result in inconsistent shading in the edited image. We propose an approach for further decomposing illumination into direct lighting, and indirect diffuse illumination from each material. This decomposition allows us to change indirect illumination from an individual material independently, so it matches the modified reflectance color. To address the underconstrained problem of decomposing illumination into multiple components, we take advantage of its smooth nature, as well as user-provided constraints. We demonstrate our approach on a number of examples, where we consistently edit material colors and the associated interreflections.	Illumination decomposition for material recoloring with consistent interreflections	NA:NA:NA	2018
Mark Levoy	NA	Session details: Capturing geometry & appearance	NA	2018
Shuang Zhao:Wenzel Jakob:Steve Marschner:Kavita Bala	The appearance of complex, thick materials like textiles is determined by their 3D structure, and they are incompletely described by surface reflection models alone. While volume scattering can produce highly realistic images of such materials, creating the required volume density models is difficult. Procedural approaches require significant programmer effort and intuition to design specialpurpose algorithms for each material. Further, the resulting models lack the visual complexity of real materials with their naturally-arising irregularities. This paper proposes a new approach to acquiring volume models, based on density data from X-ray computed tomography (CT) scans and appearance data from photographs under uncontrolled illumination. To model a material, a CT scan is made, resulting in a scalar density volume. This 3D data is processed to extract orientation information and remove noise. The resulting density and orientation fields are used in an appearance matching procedure to define scattering properties in the volume that, when rendered, produce images with texture statistics that match the photographs. As our results show, this approach can easily produce volume appearance models with extreme detail, and at larger scales the distinctive textures and highlights of a range of very different fabrics like satin and velvet emerge automatically---all based simply on having accurate mesoscale geometry.	Building volumetric appearance models of fabric using micro CT imaging	NA:NA:NA:NA	2018
Peiran Ren:Jiaping Wang:John Snyder:Xin Tong:Baining Guo	We present a simple, fast solution for reflectance acquisition using tools that fit into a pocket. Our method captures video of a flat target surface from a fixed video camera lit by a hand-held, moving, linear light source. After processing, we obtain an SVBRDF. We introduce a BRDF chart, analogous to a color "checker" chart, which arranges a set of known-BRDF reference tiles over a small card. A sequence of light responses from the chart tiles as well as from points on the target is captured and matched to reconstruct the target's appearance. We develop a new algorithm for BRDF reconstruction which works directly on these LDR responses, without knowing the light or camera position, or acquiring HDR lighting. It compensates for spatial variation caused by the local (finite distance) camera and light position by warping responses over time to align them to a specular reference. After alignment, we find an optimal linear combination of the Lambertian and purely specular reference responses to match each target point's response. The same weights are then applied to the corresponding (known) reference BRDFs to reconstruct the target point's BRDF. We extend the basic algorithm to also recover varying surface normals by adding two spherical caps for diffuse and specular references to the BRDF chart. We demonstrate convincing results obtained after less than 30 seconds of data capture, using commercial mobile phone cameras in a casual environment.	Pocket reflectometry	NA:NA:NA:NA:NA	2018
Micah K. Johnson:Forrester Cole:Alvin Raj:Edward H. Adelson	We describe a system for capturing microscopic surface geometry. The system extends the retrographic sensor [Johnson and Adelson 2009] to the microscopic domain, demonstrating spatial resolution as small as 2 microns. In contrast to existing microgeometry capture techniques, the system is not affected by the optical characteristics of the surface being measured---it captures the same geometry whether the object is matte, glossy, or transparent. In addition, the hardware design allows for a variety of form factors, including a hand-held device that can be used to capture high-resolution surface geometry in the field. We achieve these results with a combination of improved sensor materials, illumination design, and reconstruction algorithm, as compared to the original sensor of Johnson and Adelson [2009].	Microgeometry capture using an elastomeric sensor	NA:NA:NA:NA	2018
Vitor F. Pamplona:Erick B. Passos:Jan Zizka:Manuel M. Oliveira:Everett Lawson:Esteban Clua:Ramesh Raskar	We introduce an interactive method to assess cataracts in the human eye by crafting an optical solution that measures the perceptual impact of forward scattering on the foveal region. Current solutions rely on highly-trained clinicians to check the back scattering in the crystallin lens and test their predictions on visual acuity tests. Close-range parallax barriers create collimated beams of light to scan through sub-apertures, scattering light as it strikes a cataract. User feedback generates maps for opacity, attenuation, contrast and sub-aperture point-spread functions. The goal is to allow a general audience to operate a portable high-contrast light-field display to gain a meaningful understanding of their own visual conditions. User evaluations and validation with modified camera optics are performed. Compiled data is used to reconstruct the individual's cataract-affected view, offering a novel approach for capturing information for screening, diagnostic, and clinical analysis.	CATRA: interactive measuring and modeling of cataracts	NA:NA:NA:NA:NA:NA:NA	2018
Sylvain Lefebvre	NA	Session details: Sampling & noise	NA	2018
Raanan Fattal	Stochastic point distributions with blue-noise spectrum are used extensively in computer graphics for various applications such as avoiding aliasing artifacts in ray tracing, halftoning, stippling, etc. In this paper we present a new approach for generating point sets with high-quality blue noise properties that formulates the problem using a statistical mechanics interacting particle model. Points distributions are generated by sampling this model. This new formulation of the problem unifies randomness with the requirement for equidistant point spacing, responsible for the enhanced blue noise spectral properties. We derive a highly efficient multi-scale sampling scheme for drawing random point distributions from this model. The new scheme avoids the critical slowing down phenomena that plagues this type of models. This derivation is accompanied by a model-specific analysis. Altogether, our approach generates high-quality point distributions, supports spatially-varying spatial point density, and runs in time that is linear in the number of points generated.	Blue-noise point sampling using kernel density model	NA	2018
Mohamed S. Ebeida:Andrew A. Davidson:Anjul Patney:Patrick M. Knupp:Scott A. Mitchell:John D. Owens	We solve the problem of generating a uniform Poisson-disk sampling that is both maximal and unbiased over bounded non-convex domains. To our knowledge this is the first provably correct algorithm with time and space dependent only on the number of points produced. Our method has two phases, both based on classical dart-throwing. The first phase uses a background grid of square cells to rapidly create an unbiased, near-maximal covering of the domain. The second phase completes the maximal covering by calculating the connected components of the remaining uncovered voids, and by using their geometry to efficiently place unbiased samples that cover them. The second phase converges quickly, overcoming a common difficulty in dart-throwing methods. The deterministic memory is O(n) and the expected running time is O(n log n), where n is the output size, the number of points in the final sample. Our serial implementation verifies that the log n dependence is minor, and nearly O(n) performance for both time and memory is achieved in practice. We also present a parallel implementation on GPUs to demonstrate the parallel-friendly nature of our method, which achieves 2.4x the performance of our serial version.	Efficient maximal poisson-disk sampling	NA:NA:NA:NA:NA:NA	2018
Li-Yi Wei:Rui Wang	Sampling is a core component for many graphics applications including rendering, imaging, animation, and geometry processing. The efficacy of these applications often crucially depends upon the distribution quality of the underlying samples. While uniform sampling can be analyzed by using existing spatial and spectral methods, these cannot be easily extended to general non-uniform settings, such as adaptive, anisotropic, or non-Euclidean domains. We present new methods for analyzing non-uniform sample distributions. Our key insight is that standard Fourier analysis, which depends on samples' spatial locations, can be reformulated into an equivalent form that depends only on the distribution of their location differentials. We call this differential domain analysis. The main benefit of this reformulation is that it bridges the fundamental connection between the samples' spatial statistics and their spectral properties. In addition, it allows us to generalize our method with different computation kernels and differential measurements. Using this analysis, we can quantitatively measure the spatial and spectral properties of various non-uniform sample distributions, including adaptive, anisotropic, and non-Euclidean domains.	Differential domain analysis for non-uniform sampling	NA:NA	2018
Ares Lagae:George Drettakis	Solid noise is a fundamental tool in computer graphics. Surprisingly, no existing noise function supports both high-quality antialiasing and continuity across sharp edges. In this paper we show that a slicing approach is required to preserve continuity across sharp edges, and we present a new noise function that supports anisotropic filtering of sliced solid noise. This is made possible by individually filtering the slices of Gabor kernels, which requires the proper treatment of phase. This in turn leads to the introduction of the phase-augmented Gabor kernel and random-phase Gabor noise, our new noise function. We demonstrate that our new noise function supports both high-quality anti-aliasing and continuity across sharp edges, as well as anisotropy.	Filtering solid Gabor noise	NA:NA	2018
Paolo Cignoni	NA	Session details: Geometry Acquisition	NA	2018
Yangyan Li:Xiaokun Wu:Yiorgos Chrysathou:Andrei Sharf:Daniel Cohen-Or:Niloy J. Mitra	Given a noisy and incomplete point set, we introduce a method that simultaneously recovers a set of locally fitted primitives along with their global mutual relations. We operate under the assumption that the data corresponds to a man-made engineering object consisting of basic primitives, possibly repeated and globally aligned under common relations. We introduce an algorithm to directly couple the local and global aspects of the problem. The local fit of the model is determined by how well the inferred model agrees to the observed data, while the global relations are iteratively learned and enforced through a constrained optimization. Starting with a set of initial RANSAC based locally fitted primitives, relations across the primitives such as orientation, placement, and equality are progressively learned and conformed to. In each stage, a set of feasible relations are extracted among the candidate relations, and then aligned to, while best fitting to the input data. The global coupling corrects the primitives obtained in the local RANSAC stage, and brings them to precise global alignment. We test the robustness of our algorithm on a range of synthesized and scanned data, with varying amounts of noise, outliers, and non-uniform sampling, and validate the results against ground truth, where available.	GlobFit: consistently fitting primitives by discovering global relations	NA:NA:NA:NA:NA:NA	2018
Yotam Livny:Soeren Pirk:Zhanglin Cheng:Feilong Yan:Oliver Deussen:Daniel Cohen-Or:Baoquan Chen	We present a lobe-based tree representation for modeling trees. The new representation is based on the observation that the tree's foliage details can be abstracted into canonical geometry structures, termed lobe-textures. We introduce techniques to (i) approximate the geometry of given tree data and encode it into a lobe-based representation, (ii) decode the representation and synthesize a fully detailed tree model that visually resembles the input. The encoded tree serves as a light intermediate representation, which facilitates efficient storage and transmission of massive amounts of trees, e.g., from a server to clients for interactive applications in urban environments. The method is evaluated by both reconstructing laser scanned trees (given as point sets) as well as re-representing existing tree models (given as polygons).	Texture-lobes for tree modelling	NA:NA:NA:NA:NA:NA:NA	2018
Elmar Eisemann	NA	Session details: Stochastic rendering & visibility	NA	2018
Carl Johan Gribel:Rasmus Barringer:Tomas Akenine-Möller	We present a novel visibility algorithm for rendering motion blur with per-pixel anti-aliasing. Our algorithm uses a number of line samples over a rectangular group of pixels, and together with the time dimension, a two-dimensional spatio-temporal visibility problem needs to be solved per line sample. In a coarse culling step, our algorithm first uses a bounding volume hierarchy to rapidly remove geometry that does not overlap with the current line sample. For the remaining triangles, we approximate each triangle's depth function, along the line and along the time dimension, with a number of patch triangles. We resolve for the final color using an analytical visibility algorithm with depth sorting, simple occlusion culling, and clipping. Shading is decoupled from visibility, and we use a shading cache for efficient reuse of shaded values. In our results, we show practically noise-free renderings of motion blur with high-quality spatial anti-aliasing and with competitive rendering times. We also demonstrate that our algorithm, with some adjustments, can be used to accurately compute motion blurred ambient occlusion.	High-quality spatio-temporal rendering using semi-analytical visibility	NA:NA:NA	2018
Jaakko Lehtinen:Timo Aila:Jiawen Chen:Samuli Laine:Frédo Durand	Traditionally, effects that require evaluating multidimensional integrals for each pixel, such as motion blur, depth of field, and soft shadows, suffer from noise due to the variance of the high-dimensional integrand. In this paper, we describe a general reconstruction technique that exploits the anisotropy in the temporal light field and permits efficient reuse of samples between pixels, multiplying the effective sampling rate by a large factor. We show that our technique can be applied in situations that are challenging or impossible for previous anisotropic reconstruction methods, and that it can yield good results with very sparse inputs. We demonstrate our method for simultaneous motion blur, depth of field, and soft shadows.	Temporal light field reconstruction for rendering distribution effects	NA:NA:NA:NA:NA	2018
Jaakko Lehtinen	NA	Session details: Volumes & photons	NA	2018
Eugene D'Eon:Geoffrey Irving	We present a new BSSRDF for rendering images of translucent materials. Previous diffusion BSSRDFs are limited by the accuracy of classical diffusion theory. We introduce a modified diffusion theory that is more accurate for highly absorbing materials and near the point of illumination. The new diffusion solution accurately decouples single and multiple scattering. We then derive a novel, analytic, extended-source solution to the multilayer search-light problem by quantizing the diffusion Green's function. This allows the application of the diffusion multipole model to material layers several orders of magnitude thinner than previously possible and creates accurate results under high-frequency illumination. Quantized diffusion provides both a new physical foundation and a variable-accuracy construction method for sum-of-Gaussians BSSRDFs, which have many useful properties for efficient rendering and appearance capture. Our BSSRDF maps directly to previous real-time rendering algorithms. For film production rendering, we propose several improvements to previous hierarchical point cloud algorithms by introducing a new radial-binning data structure and a doubly-adaptive traversal strategy.	A quantized-diffusion model for rendering translucent materials	NA:NA	2018
Scott Schaefer	NA	Session details: Geometry processing	NA	2018
Ming Chuang:Michael Kazhdan	We present a general framework for performing geometry filtering through the solution of a screened Poisson equation. We show that this framework can be efficiently adapted to a changing Riemannian metric to support curvature-aware filtering and describe a parallel and streaming multigrid implementation for solving the system. We demonstrate the practicality of our approach by developing an interactive system for mesh editing that allows for exploration of a large family of curvature-guided, anisotropic filters.	Interactive and anisotropic geometry processing using the screened Poisson equation	NA:NA	2018
KangKang Yin	NA	Session details: Call animal control!	NA	2018
Jie Tan:Yuting Gu:Greg Turk:C. Karen Liu	We present a general approach to creating realistic swimming behavior for a given articulated creature body. The two main components of our method are creature/fluid simulation and the optimization of the creature motion parameters. We simulate two-way coupling between the fluid and the articulated body by solving a linear system that matches acceleration at fluid/solid boundaries and that also enforces fluid incompressibility. The swimming motion of a given creature is described as a set of periodic functions, one for each joint degree of freedom. We optimize over the space of these functions in order to find a motion that causes the creature to swim straight and stay within a given energy budget. Our creatures can perform path following by first training appropriate turning maneuvers through offline optimization and then selecting between these motions to track the given path. We present results for a clownfish, an eel, a sea turtle, a manta ray and a frog, and in each case the resulting motion is a good match to the real-world animals. We also demonstrate a plausible swimming gait for a fictional creature that has no real-world counterpart.	Articulated swimming creatures	NA:NA:NA:NA	2018
Stelian Coros:Andrej Karpathy:Ben Jones:Lionel Reveret:Michiel van de Panne	We develop an integrated set of gaits and skills for a physics-based simulation of a quadruped. The motion repertoire for our simulated dog includes walk, trot, pace, canter, transverse gallop, rotary gallop, leaps capable of jumping on-and-off platforms and over obstacles, sitting, lying down, standing up, and getting up from a fall. The controllers use a representation based on gait graphs, a dual leg frame model, a flexible spine model, and the extensive use of internal virtual forces applied via the Jacobian transpose. Optimizations are applied to these control abstractions in order to achieve robust gaits and leaps with desired motion styles. The resulting gaits are evaluated for robustness with respect to push disturbances and the traversal of variable terrain. The simulated motions are also compared to motion data captured from a filmed dog.	Locomotion skills for simulated quadrupeds	NA:NA:NA:NA:NA	2018
Dan B. Goldman	NA	Session details: By-example image synthesis	NA	2018
Fei Yang:Jue Wang:Eli Shechtman:Lubomir Bourdev:Dimitri Metaxas	We address the problem of correcting an undesirable expression on a face photo by transferring local facial components, such as a smiling mouth, from another face photo of the same person which has the desired expression. Direct copying and blending using existing compositing tools results in semantically unnatural composites, since expression is a global effect and the local component in one expression is often incompatible with the shape and other components of the face in another expression. To solve this problem we present Expression Flow, a 2D flow field which can warp the target face globally in a natural way, so that the warped face is compatible with the new facial component to be copied over. To do this, starting with the two input face photos, we jointly construct a pair of 3D face shapes with the same identity but different expressions. The expression flow is computed by projecting the difference between the two 3D shapes back to 2D. It describes how to warp the target face photo to match the expression of the reference photo. User studies suggest that our system is able to generate face composites with much higher fidelity than existing methods.	Expression flow for 3D-aware face component transfer	NA:NA:NA:NA:NA	2018
Ira Kemelmacher-Shlizerman:Eli Shechtman:Rahul Garg:Steven M. Seitz	We present an approach for generating face animations from large image collections of the same person. Such collections, which we call photobios, sample the appearance of a person over changes in pose, facial expression, hairstyle, age, and other variations. By optimizing the order in which images are displayed and cross-dissolving between them, we control the motion through face space and create compelling animations (e.g., render a smooth transition from frowning to smiling). Used in this context, the cross dissolve produces a very strong motion effect; a key contribution of the paper is to explain this effect and analyze its operating range. The approach operates by creating a graph with faces as nodes, and similarities as edges, and solving for walks and shortest paths on this graph. The processing pipeline involves face detection, locating fiducials (eyes/nose/mouth), solving for pose, warping to frontal views, and image comparison based on Local Binary Patterns. We demonstrate results on a variety of datasets including time-lapse photography, personal photo collections, and images of celebrities downloaded from the Internet. Our approach is the basis for the Face Movies feature in Google's Picasa.	Exploring photobios	NA:NA:NA:NA	2018
Chongyang Ma:Li-Yi Wei:Xin Tong	A variety of phenomena can be characterized by repetitive small scale elements within a large scale domain. Examples include a stack of fresh produce, a plate of spaghetti, or a mosaic pattern. Although certain results can be produced via manual placement or procedural/physical simulation, these methods can be labor intensive, difficult to control, or limited to specific phenomena. We present discrete element textures, a data-driven method for synthesizing repetitive elements according to a small input exemplar and a large output domain. Our method preserves both individual element properties and their aggregate distributions. It is also general and applicable to a variety of phenomena, including different dimensionalities, different element properties and distributions, and different effects including both artistic and physically realistic ones. We represent each element by one or multiple samples whose positions encode relevant element attributes including position, size, shape, and orientation. We propose a sample-based neighborhood similarity metric and an energy optimization solver to synthesize desired outputs that observe not only input exemplars and output domains but also optional constraints such as physics, orientation fields, and boundary conditions. As a further benefit, our method can also be applied for editing existing element distributions.	Discrete element textures	NA:NA:NA	2018
Olga Sorkine	NA	Session details: Colorful	NA	2018
Peter O'Donovan:Aseem Agarwala:Aaron Hertzmann	This paper studies color compatibility theories using large datasets, and develops new tools for choosing colors. There are three parts to this work. First, using on-line datasets, we test new and existing theories of human color preferences. For example, we test whether certain hues or hue templates may be preferred by viewers. Second, we learn quantitative models that score the quality of a five-color set of colors, called a color theme. Such models can be used to rate the quality of a new color theme. Third, we demonstrate simple proto-types that apply a learned model to tasks in color design, including improving existing themes and extracting themes from images.	Color compatibility from large datasets	NA:NA:NA	2018
Baoyuan Wang:Yizhou Yu:Ying-Qing Xu	Color and tone adjustments are among the most frequent image enhancement operations. We define a color and tone style as a set of explicit or implicit rules governing color and tone adjustments. Our goal in this paper is to learn implicit color and tone adjustment rules from examples. That is, given a set of examples, each of which is a pair of corresponding images before and after adjustments, we would like to discover the underlying mathematical relationships optimally connecting the color and tone of corresponding pixels in all image pairs. We formally define tone and color adjustment rules as mappings, and propose to approximate complicated spatially varying nonlinear mappings in a piecewise manner. The reason behind this is that a very complicated mapping can still be locally approximated with a low-order polynomial model. Parameters within such low-order models are trained using data extracted from example image pairs. We successfully apply our framework in two scenarios, low-quality photo enhancement by transferring the style of a high-end camera, and photo enhancement using styles learned from photographers and designers.	Example-based image color and tone style enhancement	NA:NA:NA	2018
Behzad Sajadi:Aditi Majumder:Kazuhiro Hiwada:Atsuto Maki:Ramesh Raskar	We present a camera with switchable primaries using shiftable layers of color filter arrays (CFAs). By layering a pair of CMY CFAs in this novel manner we can switch between multiple sets of color primaries (namely RGB, CMY and RGBCY) in the same camera. In contrast to fixed color primaries (e.g. RGB or CMY), which cannot provide optimal image quality for all scene conditions, our camera with switchable primaries provides optimal color fidelity and signal to noise ratio for multiple scene conditions. Next, we show that the same concept can be used to layer two RGB CFAs to design a camera with switchable low dynamic range (LDR) and high dynamic range (HDR) modes. Further, we show that such layering can be generalized as a constrained satisfaction problem (CSP) allowing to constrain a large number of parameters (e.g. different operational modes, amount and direction of the shifts, placement of the primaries in the CFA) to provide an optimal solution. We investigate practical design options for shiftable layering of the CFAs. We demonstrate these by building prototype cameras for both switchable primaries and switchable LDR/HDR modes. To the best of our knowledge, we present, for the first time, the concept of shiftable layers of CFAs that provides a new degree of freedom in photography where multiple operational modes are available to the user in a single camera for optimizing the picture quality based on the nature of the scene geometry, color and illumination.	Switchable primaries using shiftable layers of color filter arrays	NA:NA:NA:NA:NA	2018
Eugene Zhang	NA	Session details: Surfaces	NA	2018
Jonathan D. Denning:William B. Kerr:Fabio Pellacini	The construction of polygonal meshes remains a complex task in Computer Graphics, taking tens of thousands of individual operations over several hours of modeling time. The complexity of modeling in terms of number of operations and time makes it difficult for artists to understand all details of how meshes are constructed. We present MeshFlow, an interactive system for visualizing mesh construction sequences. MeshFlow hierarchically clusters mesh editing operations to provide viewers with an overview of the model construction while still allowing them to view more details on demand. We base our clustering on an analysis of the frequency of repeated operations and implement it using substituting regular expressions. By filtering operations based on either their type or which vertices they affect, MeshFlow also ensures that viewers can interactively focus on the relevant parts of the modeling process. Automatically generated graphical annotations visualize the clustered operations. We have tested MeshFlow by visualizing five mesh sequences each taking a few hours to model, and we found it to work well for all. We have also evaluated MeshFlow with a case study using modeling students. We conclude that our system provides useful visualizations that are found to be more helpful than video or document-form instructions in understanding mesh construction.	MeshFlow: interactive visualization of mesh construction sequences	NA:NA:NA	2018
Topraj Gurung:Mark Luffel:Peter Lindstrom:Jarek Rossignac	We propose LR (Laced Ring)---a simple data structure for representing the connectivity of manifold triangle meshes. LR provides the option to store on average either 1.08 references per triangle or 26.2 bits per triangle. Its construction, from an input mesh that supports constant-time adjacency queries, has linear space and time complexity, and involves ordering most vertices along a nearly-Hamiltonian cycle. LR is best suited for applications that process meshes with fixed connectivity, as any changes to the connectivity require the data structure to be rebuilt. We provide an implementation of the set of standard random-access, constant-time operators for traversing a mesh, and show that LR often saves both space and traversal time over competing representations.	LR: compact connectivity representation for triangle meshes	NA:NA:NA:NA	2018
Aaron Hertzmann	NA	Session details: Image processing	NA	2018
Sylvain Paris:Samuel W. Hasinoff:Jan Kautz	The Laplacian pyramid is ubiquitous for decomposing images into multiple scales and is widely used for image analysis. However, because it is constructed with spatially invariant Gaussian kernels, the Laplacian pyramid is widely believed as being unable to represent edges well and as being ill-suited for edge-aware operations such as edge-preserving smoothing and tone mapping. To tackle these tasks, a wealth of alternative techniques and representations have been proposed, e.g., anisotropic diffusion, neighborhood filtering, and specialized wavelet bases. While these methods have demonstrated successful results, they come at the price of additional complexity, often accompanied by higher computational cost or the need to post-process the generated results. In this paper, we show state-of-the-art edge-aware processing using standard Laplacian pyramids. We characterize edges with a simple threshold on pixel values that allows us to differentiate large-scale edges from small-scale details. Building upon this result, we propose a set of image filters to achieve edge-preserving smoothing, detail enhancement, tone mapping, and inverse tone mapping. The advantage of our approach is its simplicity and flexibility, relying only on simple point-wise nonlinearities and small Gaussian convolutions; no optimization or post-processing is required. As we demonstrate, our method produces consistently high-quality results, without degrading edges or introducing halos.	Local Laplacian filters: edge-aware image processing with a Laplacian pyramid	NA:NA:NA	2018
Eduardo S. L. Gastal:Manuel M. Oliveira	We present a new approach for performing high-quality edge-preserving filtering of images and videos in real time. Our solution is based on a transform that defines an isometry between curves on the 2D image manifold in 5D and the real line. This transform preserves the geodesic distance between points on these curves, adaptively warping the input signal so that 1D edge-preserving filtering can be efficiently performed in linear time. We demonstrate three realizations of 1D edge-preserving filters, show how to produce high-quality 2D edge-preserving filters by iterating 1D-filtering operations, and empirically analyze the convergence of this process. Our approach has several desirable features: the use of 1D operations leads to considerable speedups over existing techniques and potential memory savings; its computational cost is not affected by the choice of the filter parameters; and it is the first edge-preserving filter to work on color images at arbitrary scales in real time, without resorting to subsampling or quantization. We demonstrate the versatility of our domain transform and edge-preserving filters on several real-time image and video processing tasks including edge-preserving filtering, depth-of-field effects, stylization, recoloring, colorization, detail enhancement, and tone mapping.	Domain transform for edge-aware image and video processing	NA:NA	2018
Yoav HaCohen:Eli Shechtman:Dan B. Goldman:Dani Lischinski	This paper presents a new efficient method for recovering reliable local sets of dense correspondences between two images with some shared content. Our method is designed for pairs of images depicting similar regions acquired by different cameras and lenses, under non-rigid transformations, under different lighting, and over different backgrounds. We utilize a new coarse-to-fine scheme in which nearest-neighbor field computations using Generalized PatchMatch [Barnes et al. 2010] are interleaved with fitting a global non-linear parametric color model and aggregating consistent matching regions using locally adaptive constraints. Compared to previous correspondence approaches, our method combines the best of two worlds: It is dense, like optical flow and stereo reconstruction methods, and it is also robust to geometric and photometric variations, like sparse feature matching. We demonstrate the usefulness of our method using three applications for automatic example-based photograph enhancement: adjusting the tonal characteristics of a source image to match a reference, transferring a known mask to a new image, and kernel estimation for image deblurring.	Non-rigid dense correspondence with applications for image enhancement	NA:NA:NA:NA	2018
Jernej Barbic	NA	Session details: Example-based simulation	NA	2018
Huamin Wang:James F. O'Brien:Ravi Ramamoorthi	Cloth often has complicated nonlinear, anisotropic elastic behavior due to its woven pattern and fiber properties. However, most current cloth simulation techniques simply use linear and isotropic elastic models with manually selected stiffness parameters. Such simple simulations do not allow differentiating the behavior of distinct cloth materials such as silk or denim, and they cannot model most materials with fidelity to their real-world counterparts. In this paper, we present a data-driven technique to more realistically animate cloth. We propose a piecewise linear elastic model that is a good approximation to nonlinear, anisotropic stretching and bending behaviors of various materials. We develop new measurement techniques for studying the elastic deformations for both stretching and bending in real cloth samples. Our setup is easy and inexpensive to construct, and the parameters of our model can be fit to observed data with a well-posed optimization procedure. We have measured a database of ten different cloth materials, each of which exhibits distinctive elastic behaviors. These measurements can be used in most cloth simulation systems to create natural and realistic clothing wrinkles and shapes, for a range of different materials.	Data-driven elastic models for cloth: modeling and measurement	NA:NA:NA	2018
Sebastian Martin:Bernhard Thomaszewski:Eitan Grinspun:Markus Gross	We propose an example-based approach for simulating complex elastic material behavior. Supplied with a few poses that characterize a given object, our system starts by constructing a space of prefered deformations by means of interpolation. During simulation, this example manifold then acts as an additional elastic attractor that guides the object towards its space of prefered shapes. Added on top of existing solid simulation codes, this example potential effectively allows us to implement inhomogeneous and anisotropic materials in a direct and intuitive way. Due to its example-based interface, our method promotes an art-directed approach to solid simulation, which we exemplify on a set of practical examples.	Example-based elastic materials	NA:NA:NA:NA	2018
François Faure:Benjamin Gilles:Guillaume Bousquet:Dinesh K. Pai	A new method to simulate deformable objects with heterogeneous material properties and complex geometries is presented. Given a volumetric map of the material properties and an arbitrary number of control nodes, a distribution of the nodes is computed automatically, as well as the associated shape functions. Reference frames attached to the nodes are used to apply skeleton subspace deformation across the volume of the objects. A continuum mechanics formulation is derived from the displacements and the material properties. We introduce novel material-aware shape functions in place of the traditional radial basis functions used in meshless frameworks. In contrast with previous approaches, these allow coarse deformation functions to efficiently resolve non-uniform stiffnesses. Complex models can thus be simulated at high frame rates using a small number of control nodes.	Sparse meshless models of complex deformable solids	NA:NA:NA:NA	2018
Okan Arikan	NA	Session details: Facial animation	NA	2018
Haoda Huang:Jinxiang Chai:Xin Tong:Hsiang-Tao Wu	This paper introduces a new approach for acquiring high-fidelity 3D facial performances with realistic dynamic wrinkles and fine-scale facial details. Our approach leverages state-of-the-art motion capture technology and advanced 3D scanning technology for facial performance acquisition. We start the process by recording 3D facial performances of an actor using a marker-based motion capture system and perform facial analysis on the captured data, thereby determining a minimal set of face scans required for accurate facial reconstruction. We introduce a two-step registration process to efficiently build dense consistent surface correspondences across all the face scans. We reconstruct high-fidelity 3D facial performances by combining motion capture data with the minimal set of face scans in the blendshape interpolation framework. We have evaluated the performance of our system on both real and synthetic data. Our results show that the system can capture facial performances that match both the spatial resolution of static face scans and the acquisition speed of motion capture systems.	Leveraging motion capture and 3D scanning for high-fidelity facial performance acquisition	NA:NA:NA:NA	2018
Thabo Beeler:Fabian Hahn:Derek Bradley:Bernd Bickel:Paul Beardsley:Craig Gotsman:Robert W. Sumner:Markus Gross	We present a new technique for passive and markerless facial performance capture based on anchor frames. Our method starts with high resolution per-frame geometry acquisition using state-of-the-art stereo reconstruction, and proceeds to establish a single triangle mesh that is propagated through the entire performance. Leveraging the fact that facial performances often contain repetitive subsequences, we identify anchor frames as those which contain similar facial expressions to a manually chosen reference expression. Anchor frames are automatically computed over one or even multiple performances. We introduce a robust image-space tracking method that computes pixel matches directly from the reference frame to all anchor frames, and thereby to the remaining frames in the sequence via sequential matching. This allows us to propagate one reconstructed frame to an entire sequence in parallel, in contrast to previous sequential methods. Our anchored reconstruction approach also limits tracker drift and robustly handles occlusions and motion blur. The parallel tracking and mesh propagation offer low computation times. Our technique will even automatically match anchor frames across different sequences captured on different occasions, propagating a single mesh to all performances.	High-quality passive facial performance capture using anchor frames	NA:NA:NA:NA:NA:NA:NA:NA	2018
J. Rafael Tena:Fernando De la Torre:Iain Matthews	Linear models, particularly those based on principal component analysis (PCA), have been used successfully on a broad range of human face-related applications. Although PCA models achieve high compression, they have not been widely used for animation in a production environment because their bases lack a semantic interpretation. Their parameters are not an intuitive set for animators to work with. In this paper we present a linear face modelling approach that generalises to unseen data better than the traditional holistic approach while also allowing click-and-drag interaction for animation. Our model is composed of a collection of PCA sub-models that are independently trained but share boundaries. Boundary consistency and user-given constraints are enforced in a soft least mean squares sense to give flexibility to the model while maintaining coherence. Our results show that the region-based model generalises better than its holistic counterpart when describing previously unseen motion capture data from multiple subjects. The decomposition of the face into several regions, which we determine automatically from training data, gives the user localised manipulation control. This feature allows to use the model for face posing and animation in an intuitive style.	Interactive region-based linear 3D face models	NA:NA:NA	2018
Thibaut Weise:Sofien Bouaziz:Hao Li:Mark Pauly	This paper presents a system for performance-based character animation that enables any user to control the facial expressions of a digital avatar in realtime. The user is recorded in a natural environment using a non-intrusive, commercially available 3D sensor. The simplicity of this acquisition device comes at the cost of high noise levels in the acquired data. To effectively map low-quality 2D images and 3D depth maps to realistic facial expressions, we introduce a novel face tracking algorithm that combines geometry and texture registration with pre-recorded animation priors in a single optimization. Formulated as a maximum a posteriori estimation in a reduced parameter space, our method implicitly exploits temporal coherence to stabilize the tracking. We demonstrate that compelling 3D facial dynamics can be reconstructed in realtime without the use of face markers, intrusive lighting, or complex scanning hardware. This makes our system easy to deploy and facilitates a range of new applications, e.g. in digital gameplay or social interactions.	Realtime performance-based facial animation	NA:NA:NA:NA	2018
Mathieu Desbrun	NA	Session details: Mapping & warping shapes	NA	2018
Alec Jacobson:Ilya Baran:Jovan Popović:Olga Sorkine	Object deformation with linear blending dominates practical use as the fastest approach for transforming raster images, vector graphics, geometric models and animated characters. Unfortunately, linear blending schemes for skeletons or cages are not always easy to use because they may require manual weight painting or modeling closed polyhedral envelopes around objects. Our goal is to make the design and control of deformations simpler by allowing the user to work freely with the most convenient combination of handle types. We develop linear blending weights that produce smooth and intuitive deformations for points, bones and cages of arbitrary topology. Our weights, called bounded biharmonic weights, minimize the Laplacian energy subject to bound constraints. Doing so spreads the influences of the controls in a shape-aware and localized manner, even for objects with complex and concave boundaries. The variational weight optimization also makes it possible to customize the weights so that they preserve the shape of specified essential object features. We demonstrate successful use of our blending weights for real-time deformation of 2D and 3D shapes.	Bounded biharmonic weights for real-time deformation	NA:NA:NA:NA	2018
Vladimir G. Kim:Yaron Lipman:Thomas Funkhouser	This paper describes a fully automatic pipeline for finding an intrinsic map between two non-isometric, genus zero surfaces. Our approach is based on the observation that efficient methods exist to search for nearly isometric maps (e.g., Möbius Voting or Heat Kernel Maps), but no single solution found with these methods provides low-distortion everywhere for pairs of surfaces differing by large deformations. To address this problem, we suggest using a weighted combination of these maps to produce a "blended map." This approach enables algorithms that leverage efficient search procedures, yet can provide the flexibility to handle large deformations. The main challenges of this approach lie in finding a set of candidate maps {mi} and their associated blending weights {bi(p)} for every point p on the surface. We address these challenges specifically for conformal maps by making the following contributions. First, we provide a way to blend maps, defining the image of p as the weighted geodesic centroid of mi(p). Second, we provide a definition for smooth blending weights at every point p that are proportional to the area preservation of mi at p. Third, we solve a global optimization problem that selects candidate maps based both on their area preservation and consistency with other selected maps. During experiments with these methods, we find that our algorithm produces blended maps that align semantic features better than alternative approaches over a variety of data sets.	Blended intrinsic maps	NA:NA:NA	2018
Kai Xu:Hanlin Zheng:Hao Zhang:Daniel Cohen-Or:Ligang Liu:Yueshan Xiong	We introduce an algorithm for 3D object modeling where the user draws creative inspiration from an object captured in a single photograph. Our method leverages the rich source of photographs for creative 3D modeling. However, with only a photo as a guide, creating a 3D model from scratch is a daunting task. We support the modeling process by utilizing an available set of 3D candidate models. Specifically, the user creates a digital 3D model as a geometric variation from a 3D candidate. Our modeling technique consists of two major steps. The first step is a user-guided image-space object segmentation to reveal the structure of the photographed object. The core step is the second one, in which a 3D candidate is automatically deformed to fit the photographed target under the guidance of silhouette correspondence. The set of candidate models have been pre-analyzed to possess useful high-level structural information, which is heavily utilized in both steps to compensate for the ill-posedness of the analysis and modeling problems based only on content in a single image. Equally important, the structural information is preserved by the geometric variation so that the final product is coherent with its inherited structural information readily usable for subsequent model refinement or processing.	Photo-inspired model-driven 3D object modeling	NA:NA:NA:NA:NA:NA	2018
Nils Thuerey	NA	Session details: Fluid simulation	NA	2018
Barbara Solenthaler:Markus Gross	We propose a two-scale method for particle-based fluids that allocates computing resources to regions of the fluid where complex flow behavior emerges. Our method uses a low- and a high-resolution simulation that run at the same time. While in the coarse simulation the whole fluid is represented by large particles, the fine level simulates only a subset of the fluid with small particles. The subset can be arbitrarily defined and also dynamically change over time to capture complex flows and small-scale surface details. The low- and high-resolution simulations are coupled by including feedback forces and defining appropriate boundary conditions. Our method offers the benefit that particles are of the same size within each simulation level. This avoids particle splitting and merging processes, and allows the simulation of very large resolution differences without any stability problems. The model is easy to implement, and we show how it can be integrated into a standard SPH simulation as well as into the incompressible PCISPH solver. Compared to the single-resolution simulation, our method produces similar surface details while improving the efficiency linearly to the achieved reduction rate of the particle number.	Two-scale particle simulation	NA:NA	2018
Nuttapong Chentanez:Matthias Müller	We present a new Eulerian fluid simulation method, which allows real-time simulations of large scale three dimensional liquids. Such scenarios have hitherto been restricted to the domain of off-line computation. To reduce computation time we use a hybrid grid representation composed of regular cubic cells on top of a layer of tall cells. With this layout water above an arbitrary terrain can be represented without consuming an excessive amount of memory and compute power, while focusing effort on the area near the surface where it most matters. Additionally, we optimized the grid representation for a GPU implementation of the fluid solver. To further accelerate the simulation, we introduce a specialized multi-grid algorithm for solving the Poisson equation and propose solver modifications to keep the simulation stable for large time steps. We demonstrate the efficiency of our approach in several real-world scenarios, all running above 30 frames per second on a modern GPU. Some scenes include additional features such as two-way rigid body coupling as well as particle representations of sub-grid detail.	Real-time Eulerian water simulation using a restricted tall cell grid	NA:NA	2018
Michael B. Nielsen:Robert Bridson	Art direction of high resolution naturalistic liquid simulations is notoriously hard, due to both the chaotic nature of the physics and the computational resources required. Resimulating a scene at higher resolution often produces very different results, and is too expensive to allow many design cycles. We present a method of constraining or guiding a high resolution liquid simulation to stay close to a finalized low resolution version (either simulated or directly animated), restricting the solve to a thin outer shell of liquid around a guide shape. Our method is generally faster than an unconstrained simulation and can be integrated with a standard fluid simulator. We demonstrate several applications, with both simulated and hand-animated inputs.	Guide shapes for high resolution naturalistic liquid simulation	NA:NA	2018
Jeffrey N. Chadwick:Doug L. James	We propose a practical method for synthesizing plausible fire sounds that are synchronized with physically based fire animations. To enable synthesis of combustion sounds without incurring the cost of time-stepping fluid simulations at audio rates, we decompose our synthesis procedure into two components. First, a low-frequency flame sound is synthesized using a physically based combustion sound model driven with data from a visual flame simulation run at a relatively low temporal sampling rate. Second, we propose two bandwidth extension methods for synthesizing additional high-frequency flame sound content: (1) spectral bandwidth extension which synthesizes higher-frequency noise matching combustion sound spectra from theory and experiment; and (2) data-driven texture synthesis to synthesize high-frequency content based on input flame sound recordings. Various examples and comparisons are presented demonstrating plausible flame sounds, from small candle flames to large flame jets.	Animating fire with sound	NA:NA	2018
Niloy Mitra	NA	Session details: Procedural & interactive modeling	NA	2018
Manfred Lau:Akira Ohgawara:Jun Mitani:Takeo Igarashi	Although there is an abundance of 3D models available, most of them exist only in virtual simulation and are not immediately usable as physical objects in the real world. We solve the problem of taking as input a 3D model of a man-made object, and automatically generating the parts and connectors needed to build the corresponding physical object. We focus on furniture models, and we define formal grammars for IKEA cabinets and tables. We perform lexical analysis to identify the primitive parts of the 3D model. Structural analysis then gives structural information to these parts, and generates the connectors (i.e. nails, screws) needed to attach the parts together. We demonstrate our approach with arbitrary 3D models of cabinets and tables available online.	Converting 3D furniture models to fabricatable parts and connectors	NA:NA:NA:NA	2018
Lap-Fai Yu:Sai-Kit Yeung:Chi-Keung Tang:Demetri Terzopoulos:Tony F. Chan:Stanley J. Osher	We present a system that automatically synthesizes indoor scenes realistically populated by a variety of furniture objects. Given examples of sensibly furnished indoor scenes, our system extracts, in advance, hierarchical and spatial relationships for various furniture objects, encoding them into priors associated with ergonomic factors, such as visibility and accessibility, which are assembled into a cost function whose optimization yields realistic furniture arrangements. To deal with the prohibitively large search space, the cost function is optimized by simulated annealing using a Metropolis-Hastings state search step. We demonstrate that our system can synthesize multiple realistic furniture arrangements and, through a perceptual study, investigate whether there is a significant difference in the perceived functionality of the automatically synthesized results relative to furniture arrangements produced by human designers.	Make it home: automatic optimization of furniture arrangement	NA:NA:NA:NA:NA:NA	2018
Paul Merrell:Eric Schkufza:Zeyang Li:Maneesh Agrawala:Vladlen Koltun	We present an interactive furniture layout system that assists users by suggesting furniture arrangements that are based on interior design guidelines. Our system incorporates the layout guidelines as terms in a density function and generates layout suggestions by rapidly sampling the density function using a hardware-accelerated Monte Carlo sampler. Our results demonstrate that the suggestion generation functionality measurably increases the quality of furniture arrangements produced by participants with no prior training in interior design.	Interactive furniture layout using interior design guidelines	NA:NA:NA:NA:NA	2018
Wolfgang Heidrich	NA	Session details: Video resizing & stabilization	NA	2018
Yu-Shuen Wang:Jen-Hung Hsiao:Olga Sorkine:Tong-Yee Lee	The key to high-quality video resizing is preserving the shape and motion of visually salient objects while remaining temporally-coherent. These spatial and temporal requirements are difficult to reconcile, typically leading existing video retargeting methods to sacrifice one of them and causing distortion or waving artifacts. Recent work enforces temporal coherence of content-aware video warping by solving a global optimization problem over the entire video cube. This significantly improves the results but does not scale well with the resolution and length of the input video and quickly becomes intractable. We propose a new method that solves the scalability problem without compromising the resizing quality. Our method factors the problem into spatial and time/motion components: we first resize each frame independently to preserve the shape of salient regions, and then we optimize their motion using a reduced model for each pathline of the optical flow. This factorization decomposes the optimization of the video cube into sets of sub-problems whose size is proportional to a single frame's resolution and which can be solved in parallel. We also show how to incorporate cropping into our optimization, which is useful for scenes with numerous salient objects where warping alone would degenerate to linear scaling. Our results match the quality of state-of-the-art retargeting methods while dramatically reducing the computation time and memory consumption, making content-aware video resizing scalable and practical.	Scalable and coherent video resizing with per-frame optimization	NA:NA:NA:NA	2018
Zeev Farbman:Dani Lischinski	This paper presents a method for reducing undesirable tonal fluctuations in video: minute changes in tonal characteristics, such as exposure, color temperature, brightness and contrast in a sequence of frames, which are easily noticeable when the sequence is viewed. These fluctuations are typically caused by the camera's automatic adjustment of its tonal settings while shooting. Our approach operates on a continuous video shot by first designating one or more frames as anchors. We then tonally align a sequence of frames with each anchor: for each frame, we compute an adjustment map that indicates how each of its pixels should be modified in order to appear as if it was captured with the tonal settings of the anchor. The adjustment map is efficiently updated between successive frames by taking advantage of temporal video coherence and the global nature of the tonal fluctuations. Once a sequence has been aligned, it is possible to generate smooth tonal transitions between anchors, and also further control its tonal characteristics in a consistent and principled manner, which is difficult to do without incurring strong artifacts when operating on unstable sequences. We demonstrate the utility of our method using a number of clips captured with a variety of video cameras, and believe that it is well-suited for integration into today's non-linear video editing tools.	Tonal stabilization of video	NA:NA	2018
Mark Meyer	NA	Session details: Fast simulation	NA	2018
Nobuyuki Umetani:Danny M. Kaufman:Takeo Igarashi:Eitan Grinspun	We present a novel interactive tool for garment design that enables, for the first time, interactive bidirectional editing between 2D patterns and 3D high-fidelity simulated draped forms. This provides a continuous, interactive, and natural design modality in which 2D and 3D representations are simultaneously visible and seamlessly maintain correspondence. Artists can now interactively edit 2D pattern designs and immediately obtain stable accurate feedback online, thus enabling rapid prototyping and an intuitive understanding of complex drape form.	Sensitive couture for interactive garment modeling and editing	NA:NA:NA:NA	2018
Jernej Barbič:Yili Zhao	This paper shows a method to extend 3D nonlinear elasticity model reduction to open-loop multi-level reduced deformable structures. Given a volumetric mesh, we decompose the mesh into several subdomains, build a reduced deformable model for each domain, and connect the domains using inertia coupling. This makes model reduction deformable simulations much more versatile: localized deformations can be supported without prohibitive computational costs, parts can be re-used and precomputation times shortened. Our method does not use constraints, and can handle large domain rigid body motion in addition to large deformations, due to our derivation of the gradient and Hessian of the rotation matrix in polar decomposition. We show real-time examples with multi-level domain hierarchies and hundreds of reduced degrees of freedom.	Real-time large-deformation substructuring	NA:NA	2018
Matthias Müller:Nuttapong Chentanez	We propose a new fast and robust method to simulate various types of solid including rigid, plastic and soft bodies as well as one, two and three dimensional structures such as ropes, cloth and volumetric objects. The underlying idea is to use oriented particles that store rotation and spin, along with the usual linear attributes, i.e. position and velocity. This additional information adds substantially to traditional particle methods. First, particles can be represented by anisotropic shapes such as ellipsoids, which approximate surfaces more accurately than spheres. Second, shape matching becomes robust for sparse structures such as chains of particles or even single particles because the undefined degrees of freedom are captured in the rotational states of the particles. Third, the full transformation stored in the particles, including translation and rotation, can be used for robust skinning of graphical meshes and for transforming plastic deformations back into the rest state.	Solid simulation with oriented particles	NA:NA	2018
Ladislav Kavan:Dan Gerszewski:Adam W. Bargteil:Peter-Pike Sloan	We propose a method for learning linear upsampling operators for physically-based cloth simulation, allowing us to enrich coarse meshes with mid-scale details in minimal time and memory budgets, as required in computer games. In contrast to classical subdivision schemes, our operators adapt to a specific context (e.g. a flag flapping in the wind or a skirt worn by a character), which allows them to achieve higher detail. Our method starts by pre-computing a pair of coarse and fine training simulations aligned with tracking constraints using harmonic test functions. Next, we train the upsampling operators with a new regularization method that enables us to learn mid-scale details without overfitting. We demonstrate generalizability to unseen conditions such as different wind velocities or novel character motions. Finally, we discuss how to re-introduce high frequency details not explainable by the coarse mesh alone using oscillatory modes.	Physics-inspired upsampling for cloth simulation in games	NA:NA:NA:NA	2018
Kari Pulli	NA	Session details: Stereo & disparity	NA	2018
Simon Heinzle:Pierre Greisen:David Gallup:Christine Chen:Daniel Saner:Aljoscha Smolic:Andreas Burg:Wojciech Matusik:Markus Gross	Stereoscopic 3D has gained significant importance in the entertainment industry. However, production of high quality stereoscopic content is still a challenging art that requires mastering the complex interplay of human perception, 3D display properties, and artistic intent. In this paper, we present a computational stereo camera system that closes the control loop from capture and analysis to automatic adjustment of physical parameters. Intuitive interaction metaphors are developed that replace cumbersome handling of rig parameters using a touch screen interface with 3D visualization. Our system is designed to make stereoscopic 3D production as easy, intuitive, flexible, and reliable as possible. Captured signals are processed and analyzed in real-time on a stream processor. Stereoscopy and user settings define programmable control functionalities, which are executed in real-time on a control processor. Computational power and flexibility is enabled by a dedicated software and hardware architecture. We show that even traditionally difficult shots can be easily captured using our system.	Computational stereo camera system with programmable control loop	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Gordon Wetzstein:Douglas Lanman:Wolfgang Heidrich:Ramesh Raskar	We develop tomographic techniques for image synthesis on displays composed of compact volumes of light-attenuating material. Such volumetric attenuators recreate a 4D light field or high-contrast 2D image when illuminated by a uniform backlight. Since arbitrary oblique views may be inconsistent with any single attenuator, iterative tomographic reconstruction minimizes the difference between the emitted and target light fields, subject to physical constraints on attenuation. As multi-layer generalizations of conventional parallax barriers, such displays are shown, both by theory and experiment, to exceed the performance of existing dual-layer architectures. For 3D display, spatial resolution, depth of field, and brightness are increased, compared to parallax barriers. For a plane at a fixed depth, our optimization also allows optimal construction of high dynamic range displays, confirming existing heuristics and providing the first extension to multiple, disjoint layers. We conclude by demonstrating the benefits and limitations of attenuation-based light field displays using an inexpensive fabrication method: separating multiple printed transparencies with acrylic sheets.	Layered 3D: tomographic image synthesis for attenuation-based light field and high dynamic range displays	NA:NA:NA:NA	2018
Piotr Didyk:Tobias Ritschel:Elmar Eisemann:Karol Myszkowski:Hans-Peter Seidel	Binocular disparity is an important cue for the human visual system to recognize spatial layout, both in reality and simulated virtual worlds. This paper introduces a perceptual model of disparity for computer graphics that is used to define a metric to compare a stereo image to an alternative stereo image and to estimate the magnitude of the perceived disparity change. Our model can be used to assess the effect of disparity to control the level of undesirable distortions or enhancements (introduced on purpose). A number of psycho-visual experiments are conducted to quantify the mutual effect of disparity magnitude and frequency to derive the model. Besides difference prediction, other applications include compression, and re-targeting. We also present novel applications in form of hybrid stereo images and backward-compatible stereo. The latter minimizes disparity in order to convey a stereo impression if special equipment is used but produces images that appear almost ordinary to the naked eye. The validity of our model and difference metric is again confirmed in a study.	A perceptual model for disparity	NA:NA:NA:NA:NA	2018
Eitan Grinspun	NA	Session details: Fun with shapes	NA	2018
Shiqing Xin:Chi-Fu Lai:Chi-Wing Fu:Tien-Tsin Wong:Ying He:Daniel Cohen-Or	A 3D burr puzzle is a 3D model that consists of interlocking pieces with a single-key property. That is, when the puzzle is assembled, all the pieces are notched except one single key component which remains mobile. The intriguing property of the assembled burr puzzle is that it is stable, perfectly interlocked, without glue or screws, etc. Moreover, a burr puzzle consisting of a small number of pieces is still rather difficult to solve since the assembly must follow certain orders while the combinatorial complexity of the puzzle's piece arrangements is extremely high. In this paper, we generalize the 6-piece orthogonal burr puzzle (a knot) to design and model burr puzzles from 3D models. Given a 3D input model, we first interactively embed a network of knots into the 3D shape. Our method automatically optimizes and arranges the orientation of each knot, and modifies pieces of adjacent knots with an appropriate connection type. Then, following the geometry of the embedded pieces, the entire 3D model is partitioned by splitting the solid while respecting the assembly motion of embedded pieces. The main technical challenge is to enforce the single-key property and ensure the assembly/disassembly remains feasible, as the puzzle pieces in a network of knots are highly interlocked. Lastly, we also present an automated approach to generate the visualizations of the puzzle assembly process.	Making burr puzzles from 3D models	NA:NA:NA:NA:NA:NA	2018
Xian-Ying Li:Tao Ju:Yan Gu:Shi-Min Hu	Pop-up books are a fascinating form of paper art with intriguing geometric properties. In this paper, we present a systematic study of a simple but common class of pop-ups consisting of patches falling into four parallel groups, which we call v-style pop-ups. We give sufficient conditions for a v-style paper structure to be pop-uppable. That is, it can be closed flat while maintaining the rigidity of the patches, the closing and opening do not need extra force besides holding two patches and are free of intersections, and the closed paper is contained within the page border. These conditions allow us to identify novel mechanisms for making pop-ups. Based on the theory and mechanisms, we developed an interactive tool for designing v-style pop-ups and an automated construction algorithm from a given geometry, both of which guaranteeing the pop-uppability of the results.	A geometric study of v-style pop-ups: theories and algorithms	NA:NA:NA:NA	2018
Johannes Kopf:Dani Lischinski	We describe a novel algorithm for extracting a resolution-independent vector representation from pixel art images, which enables magnifying the results by an arbitrary amount without image degradation. Our algorithm resolves pixel-scale features in the input and converts them into regions with smoothly varying shading that are crisply separated by piecewise-smooth contour curves. In the original image, pixels are represented on a square pixel lattice, where diagonal neighbors are only connected through a single point. This causes thin features to become visually disconnected under magnification by conventional means, and creates ambiguities in the connectedness and separation of diagonal neighbors. The key to our algorithm is in resolving these ambiguities. This enables us to reshape the pixel cells so that neighboring pixels belonging to the same feature are connected through edges, thereby preserving the feature connectivity under magnification. We reduce pixel aliasing artifacts and improve smoothness by fitting spline curves to contours in the image and optimizing their control points.	Depixelizing pixel art	NA:NA	2018
Ron Maharik:Mikhail Bessmeltsev:Alla Sheffer:Ariel Shamir:Nathan Carr	We present an algorithm for creating digital micrography images, or micrograms, a special type of calligrams created from minuscule text. These attractive text-art works successfully combine beautiful images with readable meaningful text. Traditional micrograms are created by highly skilled artists and involve a huge amount of tedious manual work. We aim to simplify this process by providing a computerized digital micrography design tool. The main challenge in creating digital micrograms is designing textual layouts that simultaneously convey the input image, are readable and appealing. To generate such layout we use the streamlines of singularity free, low curvature, smooth vector fields, especially designed for our needs. The vector fields are computed using a new approach which controls field properties via a priori boundary condition design that balances the different requirements we aim to satisfy. The optimal boundary conditions are computed using a graph-cut approach balancing local and global design considerations. The generated layouts are further processed to obtain the final micrograms. Our method automatically generates engaging, readable micrograms starting from a vector image and an input text while providing a variety of optional high-level controls to the user.	Digital micrography	NA:NA:NA:NA:NA	2018
Yaron Lipman	NA	Session details: Discrete differential geometry	NA	2018
Pengbo Bo:Helmut Pottmann:Martin Kilian:Wenping Wang:Johannes Wallner	The most important guiding principle in computational methods for freeform architecture is the balance between cost efficiency on the one hand, and adherence to the design intent on the other. Key issues are the simplicity of supporting and connecting elements as well as repetition of costly parts. This paper proposes so-called circular arc structures as a means to faithfully realize freeform designs without giving up smooth appearance. In contrast to non-smooth meshes with straight edges where geometric complexity is concentrated in the nodes, we stay with smooth surfaces and rather distribute complexity in a uniform way by allowing edges in the shape of circular arcs. We are able to achieve the simplest possible shape of nodes without interfering with known panel optimization algorithms. We study remarkable special cases of circular arc structures which possess simple supporting elements or repetitive edges, we present the first global approximation method for principal patches, and we show an extension to volumetric structures for truly three-dimensional designs.	Circular arc structures	NA:NA:NA:NA:NA	2018
Marc Alexa:Max Wardetzky	While the theory and applications of discrete Laplacians on triangulated surfaces are well developed, far less is known about the general polygonal case. We present here a principled approach for constructing geometric discrete Laplacians on surfaces with arbitrary polygonal faces, encompassing non-planar and non-convex polygons. Our construction is guided by closely mimicking structural properties of the smooth Laplace--Beltrami operator. Among other features, our construction leads to an extension of the widely employed cotan formula from triangles to polygons. Besides carefully laying out theoretical aspects, we demonstrate the versatility of our approach for a variety of geometry processing applications, embarking on situations that would have been more difficult to achieve based on geometric Laplacians for simplicial meshes or purely combinatorial Laplacians for general meshes.	Discrete Laplacians on general polygonal meshes	NA:NA	2018
Patrick Mullen:Pooran Memari:Fernando de Goes:Mathieu Desbrun	We introduce Hodge-optimized triangulations (HOT), a family of well-shaped primal-dual pairs of complexes designed for fast and accurate computations in computer graphics. Previous work most commonly employs barycentric or circumcentric duals; while barycentric duals guarantee that the dual of each simplex lies within the simplex, circumcentric duals are often preferred due to the induced orthogonality between primal and dual complexes. We instead promote the use of weighted duals ("power diagrams"). They allow greater flexibility in the location of dual vertices while keeping primal-dual orthogonality, thus providing a valuable extension to the usual choices of dual by only adding one additional scalar per primal vertex. Furthermore, we introduce a family of functionals on pairs of complexes that we derive from bounds on the errors induced by diagonal Hodge stars, commonly used in discrete computations. The minimizers of these functionals, called HOT meshes, are shown to be generalizations of Centroidal Voronoi Tesselations and Optimal Delaunay Triangulations, and to provide increased accuracy and flexibility for a variety of computational purposes.	HOT: Hodge-optimized triangulations	NA:NA:NA:NA	2018
Keenan Crane:Ulrich Pinkall:Peter Schröder	We introduce a new method for computing conformal transformations of triangle meshes in R3. Conformal maps are desirable in digital geometry processing because they do not exhibit shear, and therefore preserve texture fidelity as well as the quality of the mesh itself. Traditional discretizations consider maps into the complex plane, which are useful only for problems such as surface parameterization and planar shape deformation where the target surface is flat. We instead consider maps into the quaternions H, which allows us to work directly with surfaces sitting in R3. In particular, we introduce a quaternionic Dirac operator and use it to develop a novel integrability condition on conformal deformations. Our discretization of this condition results in a sparse linear system that is simple to build and can be used to efficiently edit surfaces by manipulating curvature and boundary data, as demonstrated via several mesh processing applications.	Spin transformations of discrete surfaces	NA:NA:NA	2018
Ariel Shamir	NA	Session details: Interactive image editing	NA	2018
Hsiang-Ting Chen:Li-Yi Wei:Chun-Fa Chang	Revision control is a vital component of digital project management and has been widely deployed for text files. Binary files, on the other hand, have received relatively less attention. This can be inconvenient for graphics applications that use a significant amount of binary data, such as images, videos, meshes, and animations. Existing strategies such as storing whole files for individual revisions or simple binary deltas could consume significant storage and obscure vital semantic information. We present a nonlinear revision control system for images, designed with the common digital editing and sketching workflows in mind. We use DAG (directed acyclic graph) as the core structure, with DAG nodes representing editing operations and DAG edges the corresponding spatial, temporal and semantic relationships. We visualize our DAG in RevG (revision graph), which provides not only as a meaningful display of the revision history but also an intuitive interface for common revision control operations such as review, replay, diff, addition, branching, merging, and conflict resolving. Beyond revision control, our system also facilitates artistic creation processes in common image editing and digital painting workflows. We have built a prototype system upon GIMP, an open source image editor, and demonstrate its effectiveness through formative user study and comparisons with alternative revision control systems.	Nonlinear revision control for images	NA:NA:NA	2018
Bill Mark	NA	Session details: Real-time rendering hardware	NA	2018
Samuli Laine:Timo Aila:Tero Karras:Jaakko Lehtinen	We present a novel method for increasing the efficiency of stochastic rasterization of motion and defocus blur. Contrary to earlier approaches, our method is efficient even with the low sampling densities commonly encountered in realtime rendering, while allowing the use of arbitrary sampling patterns for maximal image quality. Our clipless dual-space formulation avoids problems with triangles that cross the camera plane during the shutter interval. The method is also simple to plug into existing rendering systems.	Clipless dual-space bounds for faster stochastic rasterization	NA:NA:NA:NA	2018
Tim Foley:Pat Hanrahan	In creating complex real-time shaders, programmers should be able to decompose code into independent, localized modules of their choosing. Current real-time shading languages, however, enforce a fixed decomposition into per-pipeline-stage procedures. Program concerns at other scales -- including those that cross-cut multiple pipeline stages -- cannot be expressed as reusable modules. We present a shading language, Spark, and its implementation for modern graphics hardware that improves support for separation of concerns into modules. A Spark shader class can encapsulate code that maps to more than one pipeline stage, and can be extended and composed using object-oriented inheritance. In our tests, shaders written in Spark achieve performance within 2% of HLSL.	Spark: modular, composable shaders for graphics hardware	NA:NA	2018
Jane McGonigal	NA	Keynote: Jane McGonigal	NA	2018
Jack M. Wang:Samuel R. Hamner:Scott L. Delp:Vladlen Koltun	We present a technique for automatically synthesizing walking and running controllers for physically-simulated 3D humanoid characters. The sagittal hip, knee, and ankle degrees-of-freedom are actuated using a set of eight Hill-type musculotendon models in each leg, with biologically-motivated control laws. The parameters of these control laws are set by an optimization procedure that satisfies a number of locomotion task terms while minimizing a biological model of metabolic energy expenditure. We show that the use of biologically-based actuators and objectives measurably increases the realism of gaits generated by locomotion controllers that operate without the use of motion capture data, and that metabolic energy expenditure provides a simple and unifying measurement of effort that can be used for both walking and running control optimization.	Optimizing locomotion controllers using biologically-based actuators and objectives	NA:NA:NA:NA	2018
Jie Tan:Greg Turk:C. Karen Liu	We present a physically-based system to simulate and control the locomotion of soft body characters without skeletons. We use the finite element method to simulate the deformation of the soft body, and we instrument a character with muscle fibers to allow it to actively control its shape. To perform locomotion, we use a variety of intuitive controls such as moving a point on the character, specifying the center of mass or the angular momentum, and maintaining balance. These controllers yield an objective function that is passed to our optimization solver, which handles convex quadratic program with linear complementarity constraints. This solver determines the new muscle fiber lengths, and moreover it determines whether each point of contact should remain static, slide, or lift away from the floor. Our system can automatically find an appropriate combination of muscle contractions that enables a soft character to fulfill various locomotion tasks, including walking, jumping, crawling, rolling and balancing.	Soft body locomotion	NA:NA:NA	2018
Marek Vondrak:Leonid Sigal:Jessica Hodgins:Odest Jenkins	Marker-less motion capture is a challenging problem, particularly when only monocular video is available. We estimate human motion from monocular video by recovering three-dimensional controllers capable of implicitly simulating the observed human behavior and replaying this behavior in other environments and under physical perturbations. Our approach employs a state-space biped controller with a balance feedback mechanism that encodes control as a sequence of simple control tasks. Transitions among these tasks are triggered on time and on proprioceptive events (e.g., contact). Inference takes the form of optimal control where we optimize a high-dimensional vector of control parameters and the structure of the controller based on an objective function that compares the resulting simulated motion with input observations. We illustrate our approach by automatically estimating controllers for a variety of motions directly from monocular video. We show that the estimation of controller structure through incremental optimization and refinement leads to controllers that are more stable and that better approximate the reference motion. We demonstrate our approach by capturing sequences of walking, jumping, and gymnastics.	Video-based 3D motion capture through biped control	NA:NA:NA:NA	2018
Sergey Levine:Jack M. Wang:Alexis Haraux:Zoran Popović:Vladlen Koltun	Interactive, task-guided character controllers must be agile and responsive to user input, while retaining the flexibility to be readily authored and modified by the designer. Central to a method's ease of use is its capacity to synthesize character motion for novel situations without requiring excessive data or programming effort. In this work, we present a technique that animates characters performing user-specified tasks by using a probabilistic motion model, which is trained on a small number of artist-provided animation clips. The method uses a low-dimensional space learned from the example motions to continuously control the character's pose to accomplish the desired task. By controlling the character through a reduced space, our method can discover new transitions, tractably precompute a control policy, and avoid low quality poses.	Continuous character control with low-dimensional embeddings	NA:NA:NA:NA:NA	2018
Xiaobai Chen:Abulhair Saparov:Bill Pang:Thomas Funkhouser	This paper investigates "Schelling points" on 3D meshes, feature points selected by people in a pure coordination game due to their salience. To collect data for this investigation, we designed an online experiment that asked people to select points on 3D surfaces that they expect will be selected by other people. We then analyzed properties of the selected points, finding that: 1) Schelling point sets are usually highly symmetric, and 2) local curvature properties (e.g., Gauss curvature) are most helpful for identifying obvious Schelling points (tips of protrusions), but 3) global properties (e.g., segment centeredness, proximity to a symmetry axis, etc.) are required to explain more subtle features. Based on these observations, we use regression analysis to combine multiple properties into an analytical model that predicts where Schelling points are likely to be on new meshes. We find that this model benefits from a variety of surface properties, particularly when training data comes from examples in the same object class.	Schelling points on 3D surface meshes	NA:NA:NA:NA	2018
Maks Ovsjanikov:Mirela Ben-Chen:Justin Solomon:Adrian Butscher:Leonidas Guibas	We present a novel representation of maps between pairs of shapes that allows for efficient inference and manipulation. Key to our approach is a generalization of the notion of map that puts in correspondence real-valued functions rather than points on the shapes. By choosing a multi-scale basis for the function space on each shape, such as the eigenfunctions of its Laplace-Beltrami operator, we obtain a representation of a map that is very compact, yet fully suitable for global inference. Perhaps more remarkably, most natural constraints on a map, such as descriptor preservation, landmark correspondences, part preservation and operator commutativity become linear in this formulation. Moreover, the representation naturally supports certain algebraic operations such as map sum, difference and composition, and enables a number of applications, such as function or annotation transfer without establishing point-to-point correspondences. We exploit these properties to devise an efficient shape matching method, at the core of which is a single linear solve. The new method achieves state-of-the-art results on an isometric shape matching benchmark. We also show how this representation can be used to improve the quality of maps produced by existing shape matching methods, and illustrate its usefulness in segmentation transfer and joint analysis of shape collections.	Functional maps: a flexible representation of maps between shapes	NA:NA:NA:NA:NA	2018
Mathias Eitz:Ronald Richter:Tamy Boubekeur:Kristian Hildebrand:Marc Alexa	We develop a system for 3D object retrieval based on sketched feature lines as input. For objective evaluation, we collect a large number of query sketches from human users that are related to an existing data base of objects. The sketches turn out to be generally quite abstract with large local and global deviations from the original shape. Based on this observation, we decide to use a bag-of-features approach over computer generated line drawings of the objects. We develop a targeted feature transform based on Gabor filters for this system. We can show objectively that this transform is better suited than other approaches from the literature developed for similar tasks. Moreover, we demonstrate how to optimize the parameters of our, as well as other approaches, based on the gathered sketches. In the resulting comparison, our approach is significantly better than any other system described so far.	Sketch-based shape retrieval	NA:NA:NA:NA:NA	2018
Jonathan Ragan-Kelley:Andrew Adams:Sylvain Paris:Marc Levoy:Saman Amarasinghe:Frédo Durand	Using existing programming tools, writing high-performance image processing code requires sacrificing readability, portability, and modularity. We argue that this is a consequence of conflating what computations define the algorithm, with decisions about storage and the order of computation. We refer to these latter two concerns as the schedule, including choices of tiling, fusion, recomputation vs. storage, vectorization, and parallelism. We propose a representation for feed-forward imaging pipelines that separates the algorithm from its schedule, enabling high-performance without sacrificing code clarity. This decoupling simplifies the algorithm specification: images and intermediate buffers become functions over an infinite integer domain, with no explicit storage or boundary conditions. Imaging pipelines are compositions of functions. Programmers separately specify scheduling strategies for the various functions composing the algorithm, which allows them to efficiently explore different optimizations without changing the algorithmic code. We demonstrate the power of this representation by expressing a range of recent image processing applications in an embedded domain specific language called Halide, and compiling them for ARM, x86, and GPUs. Our compiler targets SIMD units, multiple cores, and complex memory hierarchies. We demonstrate that it can handle algorithms such as a camera raw pipeline, the bilateral grid, fast local Laplacian filtering, and image segmentation. The algorithms expressed in our language are both shorter and faster than state-of-the-art implementations.	Decoupling algorithms from schedules for easy optimization of image processing pipelines	NA:NA:NA:NA:NA:NA	2018
Eduardo S. L. Gastal:Manuel M. Oliveira	We present a technique for performing high-dimensional filtering of images and videos in real time. Our approach produces high-quality results and accelerates filtering by computing the filter's response at a reduced set of sampling points, and using these for interpolation at all N input pixels. We show that for a proper choice of these sampling points, the total cost of the filtering operation is linear both in N and in the dimension d of the space in which the filter operates. As such, ours is the first high-dimensional filter with such a complexity. We present formal derivations for the equations that define our filter, as well as for an algorithm to compute the sampling points. This provides a sound theoretical justification for our method and for its properties. The resulting filter is quite flexible, being capable of producing responses that approximate either standard Gaussian, bilateral, or non-local-means filters. Such flexibility also allows us to demonstrate the first hybrid Euclidean-geodesic filter that runs in a single pass. Our filter is faster and requires less memory than previous approaches, being able to process a 10-Megapixel full-color image at 50 fps on modern GPUs. We illustrate the effectiveness of our approach by performing a variety of tasks ranging from edge-aware color filtering in 5-D, noise reduction (using up to 147 dimensions), single-pass hybrid Euclidean-geodesic filtering, and detail enhancement, among others.	Adaptive manifolds for real-time high-dimensional filtering	NA:NA	2018
Manuel Lang:Oliver Wang:Tunc Aydin:Aljoscha Smolic:Markus Gross	We present an efficient and simple method for introducing temporal consistency to a large class of optimization driven image-based computer graphics problems. Our method extends recent work in edge-aware filtering, approximating costly global regularization with a fast iterative joint filtering operation. Using this representation, we can achieve tremendous efficiency gains both in terms of memory requirements and running time. This enables us to process entire shots at once, taking advantage of supporting information that exists across far away frames, something that is difficult with existing approaches due to the computational burden of video data. Our method is able to filter along motion paths using an iterative approach that simultaneously uses and estimates per-pixel optical flow vectors. We demonstrate its utility by creating temporally consistent results for a number of applications including optical flow, disparity estimation, colorization, scribble propagation, sparse data up-sampling, and visual saliency computation.	Practical temporal consistency for image-based graphics applications	NA:NA:NA:NA:NA	2018
Peng Guan:Loretta Reiss:David A. Hirshberg:Alexander Weiss:Michael J. Black	We describe a complete system for animating realistic clothing on synthetic bodies of any shape and pose without manual intervention. The key component of the method is a model of clothing called DRAPE (DRessing Any PErson) that is learned from a physics-based simulation of clothing on bodies of different shapes and poses. The DRAPE model has the desirable property of "factoring" clothing deformations due to body shape from those due to pose variation. This factorization provides an approximation to the physical clothing deformation and greatly simplifies clothing synthesis. Given a parameterized model of the human body with known shape and pose parameters, we describe an algorithm that dresses the body with a garment that is customized to fit and possesses realistic wrinkles. DRAPE can be used to dress static bodies or animated sequences with a learned model of the cloth dynamics. Since the method is fully automated, it is appropriate for dressing large numbers of virtual characters of varying shape. The method is significantly more efficient than physical simulation.	DRAPE: DRessing Any PErson	NA:NA:NA:NA:NA	2018
Remi Brouet:Alla Sheffer:Laurence Boissieux:Marie-Paule Cani	We present a fully automatic method for design-preserving transfer of garments between characters with different body shapes. For real-life garments, such transfer is performed through a knowledge intensive and time consuming process, known as pattern grading. Our first contribution is to reformulate the criteria used in professional pattern-grading as a set of geometric requirements, respectively expressing shape or design preservation, proportionality, and fit. We then propose a fully automatic garment transfer algorithm which satisfies all of these criteria while ensuring the physical plausibility of the result. Specifically, we formulate garment transfer as a constrained optimization problem and solve it efficiently through iterative quadratic minimization. As demonstrated by our results, our method is able to automatically generate design-preserving versions of existing garments for target characters whose proportions and body shape significantly differ from those of the source. The method correctly handles the transfer of multiple layers of garment. Lastly, when source 2D patterns are available, we output graded patterns suitable for manufacturing the transferred garments. Our fully automatic design-preserving transfer method leads to significant time savings for both computer artists and fashion designers.	Design preserving garment transfer	NA:NA:NA:NA	2018
Cem Yuksel:Jonathan M. Kaldor:Doug L. James:Steve Marschner	Recent yarn-based simulation techniques permit realistic and efficient dynamic simulation of knitted clothing, but producing the required yarn-level models remains a challenge. The lack of practical modeling techniques significantly limits the diversity and complexity of knitted garments that can be simulated. We propose a new modeling technique that builds yarn-level models of complex knitted garments for virtual characters. We start with a polygonal model that represents the large-scale surface of the knitted cloth. Using this mesh as an input, our interactive modeling tool produces a finer mesh representing the layout of stitches in the garment, which we call the stitch mesh. By manipulating this mesh and assigning stitch types to its faces, the user can replicate a variety of complicated knitting patterns. The curve model representing the yarn is generated from the stitch mesh, then the final shape is computed by a yarn-level physical simulation that locally relaxes the yarn into realistic shape while preserving global shape of the garment and avoiding "yarn pull-through," thereby producing valid yarn geometry suitable for dynamic simulation. Using our system, we can efficiently create yarn-level models of knitted clothing with a rich variety of patterns that would be completely impractical to model using traditional techniques. We show a variety of example knitting patterns and full-scale garments produced using our system.	Stitch meshes for modeling knitted clothing with yarn-level detail	NA:NA:NA:NA	2018
Min H. Kim:Todd Alan Harvey:David S. Kittle:Holly Rushmeier:Julie Dorsey:Richard O. Prum:David J. Brady	Sophisticated methods for true spectral rendering have been developed in computer graphics to produce highly accurate images. In addition to traditional applications in visualizing appearance, such methods have potential applications in many areas of scientific study. In particular, we are motivated by the application of studying avian vision and appearance. An obstacle to using graphics in this application is the lack of reliable input data. We introduce an end-to-end measurement system for capturing spectral data on 3D objects. We present the modification of a recently developed hyperspectral imager to make it suitable for acquiring such data in a wide spectral range at high spectral and spatial resolution. We capture four megapixel images, with data at each pixel from the near-ultraviolet (359 nm) to near-infrared (1,003 nm) at 12 nm spectral resolution. We fully characterize the imaging system, and document its accuracy. This imager is integrated into a 3D scanning system to enable the measurement of the diffuse spectral reflectance and fluorescence of specimens. We demonstrate the use of this measurement system in the study of the interplay between the visual capabilities and appearance of birds. We show further the use of the system in gaining insight into artifacts from geology and cultural heritage.	3D imaging spectroscopy for measuring hyperspectral patterns on solid objects	NA:NA:NA:NA:NA:NA:NA	2018
Matthew O'Toole:Ramesh Raskar:Kiriakos N. Kutulakos	We present primal-dual coding, a photography technique that enables direct fine-grain control over which light paths contribute to a photo. We achieve this by projecting a sequence of patterns onto the scene while the sensor is exposed to light. At the same time, a second sequence of patterns, derived from the first and applied in lockstep, modulates the light received at individual sensor pixels. We show that photography in this regime is equivalent to a matrix probing operation in which the elements of the scene's transport matrix are individually re-scaled and then mapped to the photo. This makes it possible to directly acquire photos in which specific light transport paths have been blocked, attenuated or enhanced. We show captured photos for several scenes with challenging light transport effects, including specular inter-reflections, caustics, diffuse inter-reflections and volumetric scattering. A key feature of primal-dual coding is that it operates almost exclusively in the optical domain: our results consist of directly-acquired, unprocessed RAW photos or differences between them.	Primal-dual coding to probe light transport	NA:NA:NA	2018
Yue Dong:Xin Tong:Fabio Pellacini:Baining Guo	We present a solution for viewing high dynamic range (HDR) images with spatially-varying distributions of glossy materials printed on reflective media. Our method exploits appearance variations of the glossy materials in the angular domain to display the input HDR image at different exposures. As viewers change the print orientation or lighting directions, the print gradually varies its appearance to display the image content from the darkest to the brightest levels. Our solution is based on a commercially available printing system and is fully automatic. Given the input HDR image and the BRDFs of a set of available inks, our method computes the optimal exposures of the HDR image for all viewing conditions and the optimal ink combinations for all pixels by minimizing the difference of their appearances under all viewing conditions. We demonstrate the effectiveness of our method with print samples generated from different inputs and visualized under different viewing and lighting conditions.	Printing spatially-varying reflectance for reproducing HDR images	NA:NA:NA:NA	2018
Yuting Ye:C. Karen Liu	Capturing human activities that involve both gross full-body motion and detailed hand manipulation of objects is challenging for standard motion capture systems. We introduce a new method for creating natural scenes with such human activities. The input to our method includes motions of the full-body and the objects acquired simultaneously by a standard motion capture system. Our method then automatically synthesizes detailed and physically plausible hand manipulation that can seamlessly integrate with the input motions. Instead of producing one "optimal" solution, our method presents a set of motions that exploit a wide variety of manipulation strategies. We propose a randomized sampling algorithm to search for as many as possible visually diverse solutions within the computational time budget. Our results highlight complex strategies human hands employ effortlessly and unconsciously, such as static, sliding, rolling, as well as finger gaits with discrete relocation of contact points.	Synthesis of detailed hand manipulations using contact sampling	NA:NA	2018
Sang Hoon Yeo:Martin Lesmana:Debanga R. Neog:Dinesh K. Pai	We present a novel framework for animating human characters performing fast visually guided tasks, such as catching a ball. The main idea is to consider the coordinated dynamics of sensing and movement. Based on experimental evidence about such behaviors, we propose a generative model that constructs interception behavior online, using discrete submovements directed by uncertain visual estimates of target movement. An important aspect of this framework is that eye movements are included as well, and play a central role in coordinating movements of the head, hand, and body. We show that this framework efficiently generates plausible movements and generalizes well to novel scenarios.	Eyecatch: simulating visuomotor coordination for object interception	NA:NA:NA:NA	2018
Igor Mordatch:Emanuel Todorov:Zoran Popović	We present a motion synthesis framework capable of producing a wide variety of important human behaviors that have rarely been studied, including getting up from the ground, crawling, climbing, moving heavy objects, acrobatics (hand-stands in particular), and various cooperative actions involving two characters and their manipulation of the environment. Our framework is not specific to humans, but applies to characters of arbitrary morphology and limb configuration. The approach is fully automatic and does not require domain knowledge specific to each behavior. It also does not require pre-existing examples or motion capture data. At the core of our framework is the contact-invariant optimization (CIO) method we introduce here. It enables simultaneous optimization of contact and behavior. This is done by augmenting the search space with scalar variables that indicate whether a potential contact should be active in a given phase of the movement. These auxiliary variables affect not only the cost function but also the dynamics (by enabling and disabling contact forces), and are optimized together with the movement trajectory. Additional innovations include a continuation scheme allowing helper forces at the potential contacts rather than the torso, as well as a feature-based model of physics which is particularly well-suited to the CIO framework. We expect that CIO can also be used with a full physics model, but leave that extension for future work.	Discovery of complex behaviors through contact-invariant optimization	NA:NA:NA	2018
Mathias Eitz:James Hays:Marc Alexa	Humans have used sketching to depict our visual world since prehistoric times. Even today, sketching is possibly the only rendering technique readily available to all humans. This paper is the first large scale exploration of human sketches. We analyze the distribution of non-expert sketches of everyday objects such as 'teapot' or 'car'. We ask humans to sketch objects of a given category and gather 20,000 unique sketches evenly distributed over 250 object categories. With this dataset we perform a perceptual study and find that humans can correctly identify the object category of a sketch 73% of the time. We compare human performance against computational recognition methods. We develop a bag-of-features sketch representation and use multi-class support vector machines, trained on our sketch dataset, to classify sketches. The resulting recognition method is able to identify unknown sketches with 56% accuracy (chance is 0.4%). Based on the computational model, we demonstrate an interactive sketch recognition system. We release the complete crowd-sourced dataset of sketches to the community.	How do humans sketch objects?	NA:NA:NA	2018
Cloud Shao:Adrien Bousseau:Alla Sheffer:Karan Singh	We facilitate the creation of 3D-looking shaded production drawings from concept sketches. The key to our approach is a class of commonly used construction curves known as cross-sections, that function as an aid to both sketch creation and viewer understanding of the depicted 3D shape. In particular, intersections of these curves, or cross-hairs, convey valuable 3D information, that viewers compose into a mental model of the overall sketch. We use the artist-drawn cross-sections to automatically infer the 3D normals across the sketch, enabling 3D-like rendering. The technical contribution of our work is twofold. First, we distill artistic guidelines for drawing cross-sections and insights from perception literature to introduce an explicit mathematical formulation of the relationships between cross-section curves and the geometry they aim to convey. We then use these relationships to develop an algorithm for estimating a normal field from cross-section curve networks and other curves present in concept sketches. We validate our formulation and algorithm through a user study and a ground truth normal comparison. As demonstrated by the examples throughout the paper, these contributions enable us to shade a wide range of concept sketches with a variety of rendering styles.	CrossShade: shading concept sketches using cross-section curves	NA:NA:NA:NA	2018
Jingwan Lu:Fisher Yu:Adam Finkelstein:Stephen DiVerdi	Digital painters commonly use a tablet and stylus to drive software like Adobe Photoshop. A high quality stylus with 6 degrees of freedom (DOFs: 2D position, pressure, 2D tilt, and 1D rotation) coupled to a virtual brush simulation engine allows skilled users to produce expressive strokes in their own style. However, such devices are difficult for novices to control, and many people draw with less expensive (lower DOF) input devices. This paper presents a data-driven approach for synthesizing the 6D hand gesture data for users of low-quality input devices. Offline, we collect a library of strokes with 6D data created by trained artists. Online, given a query stroke as a series of 2D positions, we synthesize the 4D hand pose data at each sample based on samples from the library that locally match the query. This framework optionally can also modify the stroke trajectory to match characteristic shapes in the style of the library. Our algorithm outputs a 6D trajectory that can be fed into any virtual brush stroke engine to make expressive strokes for novices or users of limited hardware.	HelpingHand: example-based stroke stylization	NA:NA:NA:NA	2018
Moritz Bächer:Bernd Bickel:Doug L. James:Hanspeter Pfister	Articulated deformable characters are widespread in computer animation. Unfortunately, we lack methods for their automatic fabrication using modern additive manufacturing (AM) technologies. We propose a method that takes a skinned mesh as input, then estimates a fabricatable single-material model that approximates the 3D kinematics of the corresponding virtual articulated character in a piecewise linear manner. We first extract a set of potential joint locations. From this set, together with optional, user-specified range constraints, we then estimate mechanical friction joints that satisfy inter-joint non-penetration and other fabrication constraints. To avoid brittle joint designs, we place joint centers on an approximate medial axis representation of the input geometry, and maximize each joint's minimal cross-sectional area. We provide several demonstrations, manufactured as single, assembled pieces using 3D printers.	Fabricating articulated characters from skinned meshes	NA:NA:NA:NA	2018
Ondrej Stava:Juraj Vanek:Bedrich Benes:Nathan Carr:Radomír Měch	The use of 3D printing has rapidly expanded in the past couple of years. It is now possible to produce 3D-printed objects with exceptionally high fidelity and precision. However, although the quality of 3D printing has improved, both the time to print and the material costs have remained high. Moreover, there is no guarantee that a printed model is structurally sound. The printed product often does not survive cleaning, transportation, or handling, or it may even collapse under its own weight. We present a system that addresses this issue by providing automatic detection and correction of the problematic cases. The structural problems are detected by combining a lightweight structural analysis solver with 3D medial axis approximations. After areas with high structural stress are found, the model is corrected by combining three approaches: hollowing, thickening, and strut insertion. Both detection and correction steps are repeated until the problems have been eliminated. Our process is designed to create a model that is visually similar to the original model but possessing greater structural integrity.	Stress relief: improving structural strength of 3D printable objects	NA:NA:NA:NA:NA	2018
Yuki Igarashi:Takeo Igarashi:Jun Mitani	We introduce the interactive system "Beady" to assist the design and construction of customized 3D beadwork. The user first creates a polygonal mesh model called the design model that represents the overall structure of the beadwork. Each edge of the mesh model corresponds to a bead in the beadwork. We provide two methods to create the design model. One is interactive modeling from scratch. The user defines the mesh topology with gestural interaction and the system continuously adjusts edge lengths by considering the physical constraints among neighboring beads. The other is automatic conversion that takes an existing polygonal model as input and generates a near-hexagonal mesh model with a near-uniform edge length as output. The system then converts the design model into a beadwork model with the appropriate wiring. Computation of an appropriate wiring path requires careful consideration, and we present an algorithm based on face stripification of the mesh. The system also provides a visual step-by-step guide to assist the manual beadwork construction process. We show several beadwork designs constructed by the authors and by test users using the system.	Beady: interactive beadwork design and construction	NA:NA:NA	2018
Sören Pirk:Ondrej Stava:Julian Kratt:Michel Abdul Massih Said:Boris Neubert:Radomír Měch:Bedrich Benes:Oliver Deussen	We present a dynamic tree modeling and representation technique that allows complex tree models to interact with their environment. Our method uses changes in the light distribution and proximity to solid obstacles and other trees as approximations of biologically motivated transformations on a skeletal representation of the tree's main branches and its procedurally generated foliage. Parts of the tree are transformed only when required, thus our approach is much faster than common algorithms such as Open L-Systems or space colonization methods. Input is a skeleton-based tree geometry that can be computed from common tree production systems or from reconstructed laser scanning models. Our approach enables content creators to directly interact with trees and to create visually convincing ecosystems interactively. We present different interaction types and evaluate our method by comparing our transformations to biologically based growth simulation techniques.	Plastic trees: interactive self-adapting botanical tree models	NA:NA:NA:NA:NA:NA:NA:NA	2018
Jaakko Lehtinen:Timo Aila:Samuli Laine:Frédo Durand	Stochastic techniques for rendering indirect illumination suffer from noise due to the variance in the integrand. In this paper, we describe a general reconstruction technique that exploits anisotropy in the light field and permits efficient reuse of input samples between pixels or world-space locations, multiplying the effective sampling rate by a large factor. Our technique introduces visibility-aware anisotropic reconstruction to indirect illumination, ambient occlusion and glossy reflections. It operates on point samples without knowledge of the scene, and can thus be seen as an advanced image filter. Our results show dramatic improvement in image quality while using very sparse input samplings.	Reconstructing the indirect light field for global illumination	NA:NA:NA:NA	2018
James Gregson:Michael Krimerman:Matthias B. Hullin:Wolfgang Heidrich	We present a novel approach for highly detailed 3D imaging of turbulent fluid mixing behaviors. The method is based on visible light computed tomography, and is made possible by a new stochastic tomographic reconstruction algorithm based on random walks. We show that this new stochastic algorithm is competitive with specialized tomography solvers such as SART, but can also easily include arbitrary convex regularizers that make it possible to obtain high-quality reconstructions with a very small number of views. Finally, we demonstrate that the same stochastic tomography approach can also be used to directly re-render arbitrary 2D projections without the need to ever store a 3D volume grid.	Stochastic tomography and its applications in 3D imaging of mixing fluids	NA:NA:NA:NA	2018
Morten Bojsen-Hansen:Hao Li:Chris Wojtan	We present a method for recovering a temporally coherent, deforming triangle mesh with arbitrarily changing topology from an incoherent sequence of static closed surfaces. We solve this problem using the surface geometry alone, without any prior information like surface templates or velocity fields. Our system combines a proven strategy for triangle mesh improvement, a robust multi-resolution non-rigid registration routine, and a reliable technique for changing surface mesh topology. We also introduce a novel topological constraint enforcement algorithm to ensure that the output and input always have similar topology. We apply our technique to a series of diverse input data from video reconstructions, physics simulations, and artistic morphs. The structured output of our algorithm allows us to efficiently track information like colors and displacement maps, recover velocity information, and solve PDEs on the mesh as a post process.	Tracking surfaces with evolving topology	NA:NA:NA	2018
Vladimir G. Kim:Wilmot Li:Niloy J. Mitra:Stephen DiVerdi:Thomas Funkhouser	Large collections of 3D models from the same object class (e.g., chairs, cars, animals) are now commonly available via many public repositories, but exploring the range of shape variations across such collections remains a challenging task. In this work, we present a new exploration interface that allows users to browse collections based on similarities and differences between shapes in user-specified regions of interest (ROIs). To support this interactive system, we introduce a novel analysis method for computing similarity relationships between points on 3D shapes across a collection. We encode the inherent ambiguity in these relationships using fuzzy point correspondences and propose a robust and efficient computational framework that estimates fuzzy correspondences using only a sparse set of pairwise model alignments. We evaluate our analysis method on a range of correspondence benchmarks and report substantial improvements in both speed and accuracy over existing alternatives. In addition, we demonstrate how fuzzy correspondences enable key features in our exploration tool, such as automated view alignment, ROI-based similarity search, and faceted browsing.	Exploring collections of 3D models using fuzzy correspondences	NA:NA:NA:NA:NA	2018
Evangelos Kalogerakis:Siddhartha Chaudhuri:Daphne Koller:Vladlen Koltun	We present an approach to synthesizing shapes from complex domains, by identifying new plausible combinations of components from existing shapes. Our primary contribution is a new generative model of component-based shape structure. The model represents probabilistic relationships between properties of shape components, and relates them to learned underlying causes of structural variability within the domain. These causes are treated as latent variables, leading to a compact representation that can be effectively learned without supervision from a set of compatibly segmented shapes. We evaluate the model on a number of shape datasets with complex structural variability and demonstrate its application to amplification of shape databases and to interactive shape synthesis.	A probabilistic model for component-based shape synthesis	NA:NA:NA:NA	2018
Yi-Ting Yeh:Lingfeng Yang:Matthew Watson:Noah D. Goodman:Pat Hanrahan	We present a novel Markov chain Monte Carlo (MCMC) algorithm that generates samples from transdimensional distributions encoding complex constraints. We use factor graphs, a type of graphical model, to encode constraints as factors. Our proposed MCMC method, called locally annealed reversible jump MCMC, exploits knowledge of how dimension changes affect the structure of the factor graph. We employ a sequence of annealed distributions during the sampling process, allowing us to explore the state space across different dimensionalities more freely. This approach is motivated by the application of layout synthesis where relationships between objects are characterized as constraints. In particular, our method addresses the challenge of synthesizing open world layouts where the number of objects are not fixed and optimal configurations for different numbers of objects may be drastically different. We demonstrate the applicability of our approach on two open world layout synthesis problems: coffee shops and golf courses.	Synthesizing open worlds with constraints using locally annealed reversible jump MCMC	NA:NA:NA:NA:NA	2018
Kai Xu:Hao Zhang:Daniel Cohen-Or:Baoquan Chen	We introduce set evolution as a means for creative 3D shape modeling, where an initial population of 3D models is evolved to produce generations of novel shapes. Part of the evolving set is presented to a user as a shape gallery to offer modeling suggestions. User preferences define the fitness for the evolution so that over time, the shape population will mainly consist of individuals with good fitness. However, to inspire the user's creativity, we must also keep the evolving set diverse. Hence the evolution is "fit and diverse", drawing motivation from evolution theory. We introduce a novel part crossover operator which works at the finer-level part structures of the shapes, leading to significant variations and thus increased diversity in the evolved shape structures. Diversity is also achieved by explicitly compromising the fitness scores on a portion of the evolving population. We demonstrate the effectiveness of set evolution on man-made shapes. We show that selecting only models with high fitness leads to an elite population with low diversity. By keeping the population fit and diverse, the evolution can generate inspiring, and sometimes unexpected, shapes.	Fit and diverse: set evolution for inspiring 3D shape galleries	NA:NA:NA:NA	2018
Wenzel Jakob:Steve Marschner	It is a long-standing problem in unbiased Monte Carlo methods for rendering that certain difficult types of light transport paths, particularly those involving viewing and illumination along paths containing specular or glossy surfaces, cause unusably slow convergence. In this paper we introduce Manifold Exploration, a new way of handling specular paths in rendering. It is based on the idea that sets of paths contributing to the image naturally form manifolds in path space, which can be explored locally by a simple equation-solving iteration. This paper shows how to formulate and solve the required equations using only geometric information that is already generally available in ray tracing systems, and how to use this method in in two different Markov Chain Monte Carlo frameworks to accurately compute illumination from general families of paths. The resulting rendering algorithms handle specular, near-specular, glossy, and diffuse surface interactions as well as isotropic or highly anisotropic volume scattering interactions, all using the same fundamental algorithm. An implementation is demonstrated on a range of challenging scenes and evaluated against previous methods.	Manifold exploration: a Markov Chain Monte Carlo technique for rendering scenes with difficult specular transport	NA:NA	2018
Bruce Walter:Pramook Khungurn:Kavita Bala	Scenes modeling the real-world combine a wide variety of phenomena including glossy materials, detailed heterogeneous anisotropic media, subsurface scattering, and complex illumination. Predictive rendering of such scenes is difficult; unbiased algorithms are typically too slow or too noisy. Virtual point light (VPL) based algorithms produce low noise results across a wide range of performance/accuracy tradeoffs, from interactive rendering to high quality offline rendering, but their bias means that locally important illumination features may be missing. We introduce a bidirectional formulation and a set of weighting strategies to significantly reduce the bias in VPL-based rendering algorithms. Our approach, bidirectional lightcuts, maintains the scalability and low noise global illumination advantages of prior VPL-based work, while significantly extending their generality to support a wider range of important materials and visual cues. We demonstrate scalable, efficient, and low noise rendering of scenes with highly complex materials including gloss, BSSRDFs, and anisotropic volumetric models.	Bidirectional lightcuts	NA:NA:NA	2018
Jan Novák:Derek Nowrouzezahrai:Carsten Dachsbacher:Wojciech Jarosz	We present an efficient many-light algorithm for simulating indirect illumination in, and from, participating media. Instead of creating discrete virtual point lights (VPLs) at vertices of random-walk paths, we present a continuous generalization that places virtual ray lights (VRLs) along each path segment in the medium. Furthermore, instead of evaluating the lighting independently at discrete points in the medium, we calculate the contribution of each VRL to entire camera rays through the medium using an efficient Monte Carlo product sampling technique. We prove that by spreading the energy of virtual lights along both light and camera rays, the singularities that typically plague VPL methods are significantly diminished. This greatly reduces the need to clamp energy contributions in the medium, leading to robust and unbiased volumetric lighting not possible with current many-light techniques. Furthermore, by acting as a form of final gather, we obtain higher-quality multiple-scattering than existing density estimation techniques like progressive photon beams.	Virtual ray lights for rendering scenes with participating media	NA:NA:NA:NA	2018
Hagit Schechter:Robert Bridson	We propose a new ghost fluid approach for free surface and solid boundary conditions in Smoothed Particle Hydrodynamics (SPH) liquid simulations. Prior methods either suffer from a spurious numerical surface tension artifact or drift away from the mass conservation constraint, and do not capture realistic cohesion of liquid to solids. Our Ghost SPH scheme resolves this with a new particle sampling algorithm to create a narrow layer of ghost particles in the surrounding air and solid, with careful extrapolation and treatment of fluid variables to reflect the boundary conditions. We also provide a new, simpler form of artificial viscosity based on XSPH. Examples demonstrate how the new approach captures real liquid behaviour previously unattainable by SPH with very little extra cost.	Ghost SPH for animating water	NA:NA	2018
Nadir Akinci:Markus Ihmsen:Gizem Akinci:Barbara Solenthaler:Matthias Teschner	We propose a momentum-conserving two-way coupling method of SPH fluids and arbitrary rigid objects based on hydrodynamic forces. Our approach samples the surface of rigid bodies with boundary particles that interact with the fluid, preventing deficiency issues and both spatial and temporal discontinuities. The problem of inhomogeneous boundary sampling is addressed by considering the relative contribution of a boundary particle to a physical quantity. This facilitates not only the initialization process but also allows the simulation of multiple dynamic objects. Thin structures consisting of only one layer or one line of boundary particles, and also non-manifold geometries can be handled without any additional treatment. We have integrated our approach into WCSPH and PCISPH, and demonstrate its stability and flexibility with several scenarios including multiphase flow.	Versatile rigid-fluid coupling for incompressible SPH	NA:NA:NA:NA:NA	2018
Oleksiy Busaryev:Tamal K. Dey:Huamin Wang:Zhong Ren	Bubbles and foams are important features of liquid surface phenomena, but they are difficult to animate due to their thin films and complex interactions in the real world. In particular, small bubbles (having diameter <1cm) in a dense foam are highly affected by surface tension, so their shapes are much less deformable compared with larger bubbles. Under this small bubble assumption, we propose a more accurate and efficient particle-based algorithm to simulate bubble dynamics and interactions. The key component of this algorithm is an approximation of foam geometry, by treating bubble particles as the sites of a weighted Voronoi diagram. The connectivity information provided by the Voronoi diagram allows us to accurately model various interaction effects among bubbles. Using Voronoi cells and weights, we can also explicitly address the volume loss issue in foam simulation, which is a common problem in previous approaches. Under this framework, we present a set of bubble interaction forces to handle miscellaneous foam behaviors, including foam structure under Plateau's laws, clusters formed by liquid surface bubbles, bubble-liquid and bubble-solid coupling, bursting and coalescing. Our experiment shows that this method can be straightforwardly incorporated into existing liquid simulators, and it can efficiently generate realistic foam animations, some of which have never been produced in graphics before.	Animating bubble interactions in a liquid foam	NA:NA:NA:NA	2018
Sunghyun Cho:Jue Wang:Seungyong Lee	Videos captured by hand-held Cameras often contain significant camera shake, causing many frames to be blurry. Restoring shaky videos not only requires smoothing the camera motion and stabilizing the content, but also demands removing blur from video frames. However, video blur is hard to remove using existing single or multiple image deblurring techniques, as the blur kernel is both spatially and temporally varying. This paper presents a video deblurring method that can effectively restore sharp frames from blurry ones caused by camera shake. Our method is built upon the observation that due to the nature of camera shake, not all video frames are equally blurry. The same object may appear sharp on some frames while blurry on others. Our method detects sharp regions in the video, and uses them to restore blurry regions of the same content in nearby frames. Our method also ensures that the deblurred frames are both spatially and temporally coherent using patch-based synthesis. Experimental results show that our method can effectively remove complex video blur under the presence of moving objects and other outliers, which cannot be achieved using previous deconvolution-based approaches.	Video deblurring for hand-held cameras using patch-based synthesis	NA:NA:NA	2018
Hao-Yu Wu:Michael Rubinstein:Eugene Shih:John Guttag:Frédo Durand:William Freeman	Our goal is to reveal temporal variations in videos that are difficult or impossible to see with the naked eye and display them in an indicative manner. Our method, which we call Eulerian Video Magnification, takes a standard video sequence as input, and applies spatial decomposition, followed by temporal filtering to the frames. The resulting signal is then amplified to reveal hidden information. Using our method, we are able to visualize the flow of blood as it fills the face and also to amplify and reveal small motions. Our technique can run in real time to show phenomena occurring at the temporal frequencies selected by the user.	Eulerian video magnification for revealing subtle changes in the world	NA:NA:NA:NA:NA:NA	2018
Jiamin Bai:Aseem Agarwala:Maneesh Agrawala:Ravi Ramamoorthi	We present a semi-automated technique for selectively deanimating video to remove the large-scale motions of one or more objects so that other motions are easier to see. The user draws strokes to indicate the regions of the video that should be immobilized, and our algorithm warps the video to remove the large-scale motion of these regions while leaving finer-scale, relative motions intact. However, such warps may introduce unnatural motions in previously motionless areas, such as background regions. We therefore use a graph-cut-based optimization to composite the warped video regions with still frames from the input video; we also optionally loop the output in a seamless manner. Our technique enables a number of applications such as clearer motion visualization, simpler creation of artistic cinemagraphs (photos that include looping motions in some regions), and new ways to edit appearance and complicated motion paths in video by manipulating a de-animated representation. We demonstrate the success of our technique with a number of motion visualizations, cinemagraphs and video editing examples created from a variety of short input videos, as well as visual and numerical comparison to previous techniques.	Selectively de-animating video	NA:NA:NA:NA	2018
Floraine Berthouzoz:Wilmot Li:Maneesh Agrawala	We present a set of tools designed to help editors place cuts and create transitions in interview video. To help place cuts, our interface links a text transcript of the video to the corresponding locations in the raw footage. It also visualizes the suitability of cut locations by analyzing the audio/visual features of the raw footage to find frames where the speaker is relatively quiet and still. With these tools editors can directly highlight segments of text, check if the endpoints are suitable cut locations and if so, simply delete the text to make the edit. For each cut our system generates visible (e.g. jump-cut, fade, etc.) and seamless, hidden transitions. We present a hierarchical, graph-based algorithm for efficiently generating hidden transitions that considers visual features specific to interview footage. We also describe a new data-driven technique for setting the timing of the hidden transition. Finally, our tools offer a one click method for seamlessly removing 'ums' and repeated words as well as inserting natural-looking pauses to emphasize semantic content. We apply our tools to edit a variety of interviews and also show how they can be used to quickly compose multiple takes of an actor narrating a story.	Tools for placing cuts and transitions in interview video	NA:NA:NA	2018
James Tompkin:Kwang In Kim:Jan Kautz:Christian Theobalt	The abundance of mobile devices and digital cameras with video capture makes it easy to obtain large collections of video clips that contain the same location, environment, or event. However, such an unstructured collection is difficult to comprehend and explore. We propose a system that analyzes collections of unstructured but related video data to create a Videoscape: a data structure that enables interactive exploration of video collections by visually navigating -- spatially and/or temporally -- between different clips. We automatically identify transition opportunities, or portals. From these portals, we construct the Videoscape, a graph whose edges are video clips and whose nodes are portals between clips. Now structured, the videos can be interactively explored by walking the graph or by geographic map. Given this system, we gauge preference for different video transition styles in a user study, and generate heuristics that automatically choose an appropriate transition style. We evaluate our system using three further user studies, which allows us to conclude that Videoscapes provides significant benefits over related methods. Our system leads to previously unseen ways of interactive spatio-temporal exploration of casually captured videos, and we demonstrate this on several video collections.	Videoscapes: exploring sparse, unstructured video collections	NA:NA:NA:NA	2018
Stelian Coros:Sebastian Martin:Bernhard Thomaszewski:Christian Schumacher:Robert Sumner:Markus Gross	We present a method for controlling the motions of active deformable characters. As an underlying principle, we require that all motions be driven by internal deformations. We achieve this by dynamically adapting rest shapes in order to induce deformations that, together with environment interactions, result in purposeful and physically-plausible motions. Rest shape adaptation is a powerful concept and we show that by restricting shapes to suitable subspaces, it is possible to explicitly control the motion styles of deformable characters. Our formulation is general and can be combined with arbitrary elastic models and locomotion controllers. We demonstrate the efficiency of our method by animating curve, shell, and solid-based characters whose motion repertoires range from simple hopping to complex walking behaviors.	Deformable objects alive!	NA:NA:NA:NA:NA:NA	2018
Jernej Barbič:Funshing Sin:Eitan Grinspun	We present an interactive animation editor for complex deformable object animations. Given an existing animation, the artist directly manipulates the deformable body at any time frame, and the surrounding animation immediately adjusts in response. The automatic adjustments are designed to respect physics, preserve detail in both the input motion and geometry, respect prescribed bilateral contact constraints, and controllably and smoothly decay in space-time. While the utility of interactive editing for rigid body and articulated figure animations is widely recognized, a corresponding approach to deformable bodies has not been technically feasible before. We achieve interactive rates by combining spacetime model reduction, rotation-strain coordinate warping, linearized elasticity, and direct manipulation. This direct editing tool can serve the final stages of animation production, which often call for detailed, direct adjustments that are otherwise tedious to realize by re-simulation or frame-by-frame editing.	Interactive editing of deformable simulations	NA:NA:NA	2018
Klaus Hildebrandt:Christian Schulz:Christoph von Tycowicz:Konrad Polthier	Creating motions of objects or characters that are physically plausible and follow an animator's intent is a key task in computer animation. The spacetime constraints paradigm is a valuable approach to this problem, but it suffers from high computational costs. Based on spacetime constraints, we propose a framework for controlling the motion of deformable objects that offers interactive response times. This is achieved by a model reduction of the underlying variational problem, which combines dimension reduction, multipoint linearization, and decoupling of ODEs. After a preprocess, the cost for creating or editing a motion is reduced to solving a number of one-dimensional spacetime problems, whose solutions are the wiggly splines introduced by Kass and Anderson [2008]. We achieve interactive response times through a new fast and robust numerical scheme for solving the one-dimensional problems that is based on a closed-form representation of the wiggly splines.	Interactive spacetime control of deformable objects	NA:NA:NA:NA	2018
Fabian Hahn:Sebastian Martin:Bernhard Thomaszewski:Robert Sumner:Stelian Coros:Markus Gross	We present a method that brings the benefits of physics-based simulations to traditional animation pipelines. We formulate the equations of motions in the subspace of deformations defined by an animator's rig. Our framework fits seamlessly into the workflow typically employed by artists, as our output consists of animation curves that are identical in nature to the result of manual keyframing. Artists can therefore explore the full spectrum between handcrafted animation and unrestricted physical simulation. To enhance the artist's control, we provide a method that transforms stiffness values defined on rig parameters to a non-homogeneous distribution of material parameters for the underlying FEM model. In addition, we use automatically extracted high-level rig parameters to intuitively edit the results of our simulations, and also to speed up computation. To demonstrate the effectiveness of our method, we create compelling results by adding rich physical motions to coarse input animations. In the absence of artist input, we create realistic passive motion directly in rig space.	Rig-space physics	NA:NA:NA:NA:NA:NA	2018
Bruno Galerne:Ares Lagae:Sylvain Lefebvre:George Drettakis	Procedural noise is a fundamental tool in Computer Graphics. However, designing noise patterns is hard. In this paper, we present Gabor noise by example, a method to estimate the parameters of bandwidth-quantized Gabor noise, a procedural noise function that can generate noise with an arbitrary power spectrum, from exemplar Gaussian textures, a class of textures that is completely characterized by their power spectrum. More specifically, we introduce (i) bandwidth-quantized Gabor noise, a generalization of Gabor noise to arbitrary power spectra that enables robust parameter estimation and efficient procedural evaluation; (ii) a robust parameter estimation technique for quantized-bandwidth Gabor noise, that automatically decomposes the noisy power spectrum estimate of an exemplar into a sparse sum of Gaussians using non-negative basis pursuit denoising; and (iii) an efficient procedural evaluation scheme for bandwidth-quantized Gabor noise, that uses multi-grid evaluation and importance sampling of the kernel parameters. Gabor noise by example preserves the traditional advantages of procedural noise, including a compact representation and a fast on-the-fly evaluation, and is mathematically well-founded.	Gabor noise by example	NA:NA:NA:NA	2018
Xin Sun:Guofu Xie:Yue Dong:Stephen Lin:Weiwei Xu:Wencheng Wang:Xin Tong:Baining Guo	We introduce a vector representation called diffusion curve textures for mapping diffusion curve images (DCI) onto arbitrary surfaces. In contrast to the original implicit representation of DCIs [Orzan et al. 2008], where determining a single texture value requires iterative computation of the entire DCI via the Poisson equation, diffusion curve textures provide an explicit representation from which the texture value at any point can be solved directly, while preserving the compactness and resolution independence of diffusion curves. This is achieved through a formulation of the DCI diffusion process in terms of Green's functions. This formulation furthermore allows the texture value of any rectangular region (e.g. pixel area) to be solved in closed form, which facilitates anti-aliasing. We develop a GPU algorithm that renders anti-aliased diffusion curve textures in real time, and demonstrate the effectiveness of this method through high quality renderings with detailed control curves and color variations.	Diffusion curve textures for resolution independent texture mapping	NA:NA:NA:NA:NA:NA:NA:NA	2018
Shuang Zhao:Wenzel Jakob:Steve Marschner:Kavita Bala	Woven fabrics have a wide range of appearance determined by their small-scale 3D structure. Accurately modeling this structural detail can produce highly realistic renderings of fabrics and is critical for predictive rendering of fabric appearance. But building these yarn-level volumetric models is challenging. Procedural techniques are manually intensive, and fail to capture the naturally arising irregularities which contribute significantly to the overall appearance of cloth. Techniques that acquire the detailed 3D structure of real fabric samples are constrained only to model the scanned samples and cannot represent different fabric designs. This paper presents a new approach to creating volumetric models of woven cloth, which starts with user-specified fabric designs and produces models that correctly capture the yarn-level structural details of cloth. We create a small database of volumetric exemplars by scanning fabric samples with simple weave structures. To build an output model, our method synthesizes a new volume by copying data from the exemplars at each yarn crossing to match a weave pattern that specifies the desired output structure. Our results demonstrate that our approach generalizes well to complex designs and can produce highly realistic results at both large and small scales.	Structure-aware synthesis for predictive woven fabric appearance	NA:NA:NA:NA	2018
Yahan Zhou:Haibin Huang:Li-Yi Wei:Rui Wang	Point samples with different spectral noise properties (often defined using color names such as white, blue, green, and red) are important for many science and engineering disciplines including computer graphics. While existing techniques can easily produce white and blue noise samples, relatively little is known for generating other noise patterns. In particular, no single algorithm is available to generate different noise patterns according to user-defined spectra. In this paper, we describe an algorithm for generating point samples that match a user-defined Fourier spectrum function. Such a spectrum function can be either obtained from a known sampling method, or completely constructed by the user. Our key idea is to convert the Fourier spectrum function into a differential distribution function that describes the samples' local spatial statistics; we then use a gradient descent solver to iteratively compute a sample set that matches the target differential distribution function. Our algorithm can be easily modified to achieve adaptive sampling, and we provide a GPU-based implementation. Finally, we present a variety of different sample patterns obtained using our algorithm, and demonstrate suitable applications.	Point sampling with general noise spectrum	NA:NA:NA:NA	2018
Alec Jacobson:Ilya Baran:Ladislav Kavan:Jovan Popović:Olga Sorkine	Skinning transformations are a popular way to articulate shapes and characters. However, traditional animation interfaces require all of the skinning transformations to be specified explicitly, typically using a control structure (a rig). We propose a system where the user specifies only a subset of the degrees of freedom and the rest are automatically inferred using nonlinear, rigidity energies. By utilizing a low-order model and reformulating our energy functions accordingly, our algorithm runs orders of magnitude faster than previous methods without compromising quality. In addition to the immediate boosts in performance for existing modeling and real time animation tools, our approach also opens the door to new modes of control: disconnected skeletons combined with shape-aware inverse kinematics. With automatically generated skinning weights, our method can also be used for fast variational shape modeling.	Fast automatic skinning transformations	NA:NA:NA:NA:NA	2018
Martin Bokeloh:Michael Wand:Hans-Peter Seidel:Vladlen Koltun	We present an approach to high-level shape editing that adapts the structure of the shape while maintaining its global characteristics. Our main contribution is a new algebraic model of shape structure that characterizes shapes in terms of linked translational patterns. The space of shapes that conform to this characterization is parameterized by a small set of numerical parameters bounded by a set of linear constraints. This convex space permits a direct exploration of variations of the input shape. We use this representation to develop a robust interactive system that allows shapes to be intuitively manipulated through sparse constraints.	An algebraic model for parameterized shape editing	NA:NA:NA:NA	2018
Behzad Sajadi:M. Gopi:Aditi Majumder	Digital projection technology has improved significantly in recent years. But, the relationship of cost with respect to available resolution in projectors is still super-linear. In this paper, we present a method that uses projector light modulator panels (e.g. LCD or DMD panels) of resolution n X n to create a perceptually close match to a target higher resolution cn X cn image, where c is a small integer greater than 1. This is achieved by enhancing the resolution using smaller pixels at specific regions of interest like edges. A target high resolution image (cn X cn) is first decomposed into (a) a high resolution (cn X cn) but sparse edge image, and (b) a complementary lower resolution (n X n) non-edge image. These images are then projected in a time sequential manner at a high frame rate to create an edge-enhanced image -- an image where the pixel density is not uniform but changes spatially. In 3D ready projectors with readily available refresh rate of 120Hz, such a temporal multiplexing is imperceptible to the user and the edge-enhanced image is perceptually almost identical to the target high resolution image. To create the higher resolution edge image, we introduce the concept of optical pixel sharing. This reduces the projected pixel size by a factor of 1/c2 while increasing the pixel density by c2 at the edges enabling true higher resolution edges. Due to the sparsity of the edge pixels in an image we are able to choose a sufficiently large subset of these to be displayed at the higher resolution using perceptual parameters. We present a statistical analysis quantifying the expected number of pixels that will be reproduced at the higher resolution and verify it for different types of images.	Edge-guided resolution enhancement in projectors via optical pixel sharing	NA:NA:NA	2018
Gordon Wetzstein:Douglas Lanman:Matthew Hirsch:Ramesh Raskar	We introduce tensor displays: a family of compressive light field displays comprising all architectures employing a stack of time-multiplexed, light-attenuating layers illuminated by uniform or directional backlighting (i.e., any low-resolution light field emitter). We show that the light field emitted by an N-layer, M-frame tensor display can be represented by an Nth-order, rank-M tensor. Using this representation we introduce a unified optimization framework, based on nonnegative tensor factorization (NTF), encompassing all tensor display architectures. This framework is the first to allow joint multilayer, multiframe light field decompositions, significantly reducing artifacts observed with prior multilayer-only and multiframe-only decompositions; it is also the first optimization method for designs combining multiple layers with directional backlighting. We verify the benefits and limitations of tensor displays by constructing a prototype using modified LCD panels and a custom integral imaging backlight. Our efficient, GPU-based NTF implementation enables interactive applications. Through simulations and experiments we show that tensor displays reveal practical architectures with greater depths of field, wider fields of view, and thinner form factors, compared to prior automultiscopic displays.	Tensor displays: compressive light field synthesis using multilayer displays with directional backlighting	NA:NA:NA:NA	2018
Vitor F. Pamplona:Manuel M. Oliveira:Daniel G. Aliaga:Ramesh Raskar	We introduce tailored displays that enhance visual acuity by decomposing virtual objects and placing the resulting anisotropic pieces into the subject's focal range. The goal is to free the viewer from needing wearable optical corrections when looking at displays. Our tailoring process uses aberration and scattering maps to account for refractive errors and cataracts. It splits an object's light field into multiple instances that are each in-focus for a given eye sub-aperture. Their integration onto the retina leads to a quality improvement of perceived images when observing the display with naked eyes. The use of multiple depths to render each point of focus on the retina creates multi-focus, multi-depth displays. User evaluations and validation with modified camera optics are performed. We propose tailored displays for daily tasks where using eyeglasses are unfeasible or inconvenient (e.g., on head-mounted displays, e-readers, as well as for games); when a multi-focus function is required but undoable (e.g., driving for farsighted individuals, checking a portable device while doing physical activities); or for correcting the visual distortions produced by high-order aberrations that eyeglasses are not able to.	Tailored displays to compensate for visual aberrations	NA:NA:NA:NA	2018
Soheil Darabi:Eli Shechtman:Connelly Barnes:Dan B. Goldman:Pradeep Sen	Current methods for combining two different images produce visible artifacts when the sources have very different textures and structures. We present a new method for synthesizing a transition region between two source images, such that inconsistent color, texture, and structural properties all change gradually from one source to the other. We call this process image melding. Our method builds upon a patch-based optimization foundation with three key generalizations: First, we enrich the patch search space with additional geometric and photometric transformations. Second, we integrate image gradients into the patch representation and replace the usual color averaging with a screened Poisson equation solver. And third, we propose a new energy based on mixed L2/L0 norms for colors and gradients that produces a gradual transition between sources without sacrificing texture sharpness. Together, all three generalizations enable patch-based solutions to a broad class of image melding problems involving inconsistent sources: object cloning, stitching challenging panoramas, hole filling from multiple photos, and image harmonization. In several cases, our unified method outperforms previous state-of-the-art methods specifically designed for those applications.	Image melding: combining inconsistent images using patch-based synthesis	NA:NA:NA:NA:NA	2018
Brian Summa:Julien Tierny:Valerio Pascucci	A fundamental step in stitching several pictures to form a larger mosaic is the computation of boundary seams that minimize the visual artifacts in the transition between images. Current seam computation algorithms use optimization methods that may be slow, sequential, memory intensive, and prone to finding suboptimal solutions related to local minima of the chosen energy function. Moreover, even when these techniques perform well, their solution may not be perceptually ideal (or even good). Such an inflexible approach does not allow the possibility of user-based improvement. This paper introduces the Panorama Weaving technique for seam creation and editing in an image mosaic. First, Panorama Weaving provides a procedure to create boundaries for panoramas that is fast, has low memory requirements and is easy to parallelize. This technique often produces seams with lower energy than the competing global technique. Second, it provides the first interactive technique for the exploration of the seam solution space. This powerful editing capability allows the user to automatically extract energy minimizing seams given a sparse set of constraints. With a variety of empirical results, we show how Panorama Weaving allows the computation and editing of a wide range of digital panoramas including unstructured configurations.	Panorama weaving: fast and flexible seam processing	NA:NA:NA	2018
Su Xue:Aseem Agarwala:Julie Dorsey:Holly Rushmeier	Compositing is one of the most commonly performed operations in computer graphics. A realistic composite requires adjusting the appearance of the foreground and background so that they appear compatible; unfortunately, this task is challenging and poorly understood. We use statistical and visual perception experiments to study the realism of image composites. First, we evaluate a number of standard 2D image statistical measures, and identify those that are most significant in determining the realism of a composite. Then, we perform a human subjects experiment to determine how the changes in these key statistics influence human judgements of composite realism. Finally, we describe a data-driven algorithm that automatically adjusts these statistical measures in a foreground to make it more compatible with its background in a composite. We show a number of compositing results, and evaluate the performance of both our algorithm and previous work with a human subjects study.	Understanding and improving the realism of image composites	NA:NA:NA:NA	2018
Hao Pan:Yi-King Choi:Yang Liu:Wenchao Hu:Qiang Du:Konrad Polthier:Caiming Zhang:Wenping Wang	We present a new method for modeling discrete constant mean curvature (CMC) surfaces, which arise frequently in nature and are highly demanded in architecture and other engineering applications. Our method is based on a novel use of the CVT (centroidal Voronoi tessellation) optimization framework. We devise a CVT-CMC energy function defined as a combination of an extended CVT energy and a volume functional. We show that minimizing the CVT-CMC energy is asymptotically equivalent to minimizing mesh surface area with a fixed volume, thus defining a discrete CMC surface. The CVT term in the energy function ensures high mesh quality throughout the evolution of a CMC surface in an interactive design process for form finding. Our method is capable of modeling CMC surfaces with fixed or free boundaries and is robust with respect to input mesh quality and topology changes. Experiments show that the new method generates discrete CMC surfaces of improved mesh quality over existing methods.	Robust modeling of constant mean curvature surfaces	NA:NA:NA:NA:NA:NA:NA:NA	2018
Nobuyuki Umetani:Takeo Igarashi:Niloy J. Mitra	Geometric modeling and the physical validity of shapes are traditionally considered independently. This makes creating aesthetically pleasing yet physically valid models challenging. We propose an interactive design framework for efficient and intuitive exploration of geometrically and physically valid shapes. During any geometric editing operation, the proposed system continuously visualizes the valid range of the parameter being edited. When one or more constraints are violated after an operation, the system generates multiple suggestions involving both discrete and continuous changes to restore validity. Each suggestion also comes with an editing mode that simultaneously adjusts multiple parameters in a coordinated way to maintain validity. Thus, while the user focuses on the aesthetic aspects of the design, our computational design framework helps to achieve physical realizability by providing active guidance to the user. We demonstrate our framework on plank-based furniture design with nail-joint and frictional constraints. We use our system to design a range of examples, conduct a user study, and also fabricate a physical prototype to test the validity and usefulness of the system.	Guided exploration of physically valid shapes for furniture design	NA:NA:NA	2018
Etienne Vouga:Mathias Höbinger:Johannes Wallner:Helmut Pottmann	Self-supporting masonry is one of the most ancient and elegant techniques for building curved shapes. Because of the very geometric nature of their failure, analyzing and modeling such strutures is more a geometry processing problem than one of classical continuum mechanics. This paper uses the thrust network method of analysis and presents an iterative nonlinear optimization algorithm for efficiently approximating freeform shapes by self-supporting ones. The rich geometry of thrust networks leads us to close connections between diverse topics in discrete differential geometry, such as a finite-element discretization of the Airy stress potential, perfect graph Laplacians, and computing admissible loads via curvatures of polyhedral surfaces. This geometric viewpoint allows us, in particular, to remesh self-supporting shapes by self-supporting quad meshes with planar faces, and leads to another application of the theory: steel/glass constructions with low moments in nodes.	Design of self-supporting surfaces	NA:NA:NA:NA	2018
Alec Rivers:Ilan E. Moyer:Frédo Durand	Many kinds of digital fabrication are accomplished by precisely moving a tool along a digitally-specified path. This precise motion is typically accomplished fully automatically using a computer-controlled multi-axis stage. With that approach, one can only create objects smaller than the positioning stage, and large stages can be quite expensive. We propose a new approach to precise positioning of a tool that combines manual and automatic positioning: in our approach, the user coarsely positions a frame containing the tool in an approximation of the desired path, while the device tracks the frame's location and adjusts the position of the tool within the frame to correct the user's positioning error in real time. Because the automatic positioning need only cover the range of the human's positioning error, this frame can be small and inexpensive, and because the human has unlimited range, such a frame can be used to precisely position tools over an unlimited range.	Position-correcting tools for 2D digital fabrication	NA:NA:NA	2018
Olivier Bau:Ivan Poupyrev	REVEL is an augmented reality (AR) tactile technology that allows for change to the tactile feeling of real objects by augmenting them with virtual tactile textures using a device worn by the user. Unlike previous attempts to enhance AR environments with haptics, we neither physically actuate objects or use any force- or tactile-feedback devices, nor require users to wear tactile gloves or other apparatus on their hands. Instead, we employ the principle of reverse electrovibration where we inject a weak electrical signal anywhere on the user body creating an oscillating electrical field around the user's fingers. When sliding his or her fingers on a surface of the object, the user perceives highly distinctive tactile textures augmenting the physical object. By tracking the objects and location of the touch, we associate dynamic tactile sensations to the interaction context. REVEL is built upon our previous work on designing electrovibration-based tactile feedback for touch surfaces [Bau, et al. 2010]. In this paper we expand tactile interfaces based on electrovibration beyond touch surfaces and bring them into the real world. We demonstrate a broad range of application scenarios where our technology can be used to enhance AR interaction with dynamic and unobtrusive tactile feedback.	REVEL: tactile feedback technology for augmented reality	NA:NA	2018
Ludovic Hoyet:Rachel McDonnell:Carol O'Sullivan	With recent advances in real-time graphics technology, more realistic, believable and appealing virtual characters are needed than ever before. Both player-controlled avatars and non-player characters are now starting to interact with the environment, other virtual humans and crowds. However, simulating physical contacts between characters and matching appropriate reactions to specific actions is a highly complex problem, and timing errors, force mismatches and angular distortions are common. To investigate the effect of such anomalies on the perceived realism of two-character interactions, we captured a motion corpus of pushing animations and corresponding reactions and then conducted a series of perceptual experiments. We found that participants could easily distinguish between five different interaction forces, even when only one of the characters was visible. Furthermore, they were sensitive to all three types of anomalous interactions: timing errors of over 150ms were acceptable less than 50% of the time, with early or late reactions being equally perceptible; participants could perceive force mismatches, though over-reactions were more acceptable than under-reactions; finally, angular distortions when a character reacts to a pushing force reduce the acceptability of the interactions, but there is some evidence for a preference of expansion away from the pushing character's body. Our results provide insights to aid in designing motion capture sessions, motion editing strategies and balancing animation budgets.	Push it real: perceiving causality in virtual interactions	NA:NA:NA	2018
Rachel McDonnell:Martin Breidt:Heinrich H. Bülthoff	The realistic depiction of lifelike virtual humans has been the goal of many movie makers in the last decade. Recently, films such as Tron: Legacy and The Curious Case of Benjamin Button have produced highly realistic characters. In the real-time domain, there is also a need to deliver realistic virtual characters, with the increase in popularity of interactive drama video games (such as L.A. Noire™ or Heavy Rain™). There have been mixed reactions from audiences to lifelike characters used in movies and games, with some saying that the increased realism highlights subtle imperfections, which can be disturbing. Some developers opt for a stylized rendering (such as cartoon-shading) to avoid a negative reaction [Thompson 2004]. In this paper, we investigate some of the consequences of choosing realistic or stylized rendering in order to provide guidelines for developers for creating appealing virtual characters. We conducted a series of psychophysical experiments to determine whether render style affects how virtual humans are perceived. Motion capture with synchronized eye-tracked data was used throughout to animate custom-made virtual model replicas of the captured actors.	Render me real?: investigating the effect of render style on the perception of animated virtual humans	NA:NA:NA	2018
Krzysztof Templin:Piotr Didyk:Tobias Ritschel:Karol Myszkowski:Hans-Peter Seidel	Human stereo perception of glossy materials is substantially different from the perception of diffuse surfaces: A single point on a diffuse object appears the same for both eyes, whereas it appears different to both eyes on a specular object. As highlights are blurry reflections of light sources they have depth themselves, which is different from the depth of the reflecting surface. We call this difference in depth impression the "highlight disparity". Due to artistic motivation, for technical reasons, or because of incomplete data, highlights often have to be depicted on-surface, without any disparity. However, it has been shown that a lack of disparity decreases the perceived glossiness and authenticity of a material. To remedy this contradiction, our work introduces a technique for depiction of glossy materials, which improves over simple on-surface highlights, and avoids the problems of physical highlights. Our technique is computationally simple, can be easily integrated in an existing (GPU) shading system, and allows for local and interactive artistic control.	Highlight microdisparity for improved gloss depiction	NA:NA:NA:NA:NA	2018
Xuan Yang:Linling Zhang:Tien-Tsin Wong:Pheng-Ann Heng	By extending from monocular displays to binocular displays, one additional image domain is introduced. Existing binocular display systems only utilize this additional image domain for stereopsis. Our human vision is not only able to fuse two displaced images, but also two images with difference in detail, contrast and luminance, up to a certain limit. This phenomenon is known as binocular single vision. Humans can perceive more visual content via binocular fusion than just a linear blending of two views. In this paper, we make a first attempt in computer graphics to utilize this human vision phenomenon, and propose a binocular tone mapping framework. The proposed framework generates a binocular low-dynamic range (LDR) image pair that preserves more human-perceivable visual content than a single LDR image using the additional image domain. Given a tone-mapped LDR image (left, without loss of generality), our framework optimally synthesizes its counterpart (right) in the image pair from the same source HDR image. The two LDR images are different, so that they can aggregately present more human-perceivable visual richness than a single arbitrary LDR image, without triggering visual discomfort. To achieve this goal, a novel binocular viewing comfort predictor (BVCP) is also proposed to prevent such visual discomfort. The design of BVCP is based on the findings in vision science. Through our user studies, we demonstrate the increase of human-perceivable visual richness and the effectiveness of the proposed BVCP in conservatively predicting the visual discomfort threshold of human observers.	Binocular tone mapping	NA:NA:NA:NA	2018
Romain Vergne:Pascal Barla:Roland W. Fleming:Xavier Granier	We present a novel method for producing convincing pictures of shaded objects based entirely on 2D image operations. This approach, which we call image-based shading design, offers direct artistic control in the picture plane by deforming image primitives so that they appear to conform to specific 3D shapes. Using a differential analysis of reflected radiance, we identify the two types of surface flows involved in the depiction of shaded objects, which are consistent with recent perceptual studies. We then introduce two novel deformation operators that closely mimic surface flows while providing direct artistic controls in real-time.	Surface flows for image-based shading design	NA:NA:NA:NA	2018
Lukas Hosek:Alexander Wilkie	We present a physically-based analytical model of the daytime sky. Based on the results of a first-principles brute force simulation of radiative transfer in the atmosphere, we use the same general approach of fitting basis function coefficients to radiance data as the Perez and Preetham models do. However, we make several modifications to this process, which together significantly improve the rendition of sunsets and high atmospheric turbidity setups -- known weak points of the Preetham model. Additionally, our model accounts for ground albedo, and handles each spectral component independently. The latter property makes it easily extensible to the near ultraviolet range of the spectrum, so that the daylight appearance of surfaces that include optical brighteners can be properly predicted. Due to its similar mathematical properties, the new model can be used as a drop-in replacement of the Preetham model.	An analytic model for full spectral sky-dome radiance	NA:NA	2018
Tyson Brochu:Essex Edwards:Robert Bridson	Continuous collision detection (CCD) between deforming triangle mesh elements in 3D is a critical tool for many applications. The standard method involving a cubic polynomial solver is vulnerable to rounding error, requiring the use of ad hoc tolerances, and nevertheless is particularly fragile in (near-)planar cases. Even with per-simulation tuning, it may still cause problems by missing collisions or erroneously flagging non-collisions. We present a geometrically exact alternative guaranteed to produce the correct Boolean result (significant collision or not) as if calculated with exact arithmetic, even in degenerate scenarios. Our critical insight is that only the parity of the number of collisions is needed for robust simulation, and this parity can be calculated with simpler non-constructive predicates. In essence we analyze the roots of the nonlinear system of equations defining CCD through careful consideration of the boundary of the parameter domain. The use of new conservative culling and interval filters allows typical simulations to run as fast as with the non-robust version, but without need for tuning or worries about failure cases even in geometrically degenerate scenarios. We demonstrate the effectiveness of geometrically exact detection with a novel adaptive cloth simulation, the first to guarantee to remain intersection-free despite frequent curvature-driven remeshing.	Efficient geometrically exact continuous collision detection	NA:NA:NA	2018
Bin Wang:François Faure:Dinesh K. Pai	A method for image-based contact detection and modeling, with guaranteed precision on the intersection volume, is presented. Unlike previous image-based methods, our method optimizes a nonuniform ray sampling resolution and allows precise control of the volume error. By cumulatively projecting all mesh edges into a generalized 2D texture, we construct a novel data structure, the Error Bound Polynomial Image (EBPI), which allows efficient computation of the maximum volume error as a function of ray density. Based on a precision criterion, EBPI pixels are subdivided or clustered. The rays are then cast in the projection direction according to the non-uniform resolution. The EBPI data, combined with ray-surface intersection points and normals, is also used to detect transient edges at surface intersections. This allows us to model intersection volumes at arbitrary resolution, while avoiding the geometric computation of mesh intersections. Moreover, the ray casting acceleration data structures can be reused for the generation of high quality images.	Adaptive image-based intersection volume	NA:NA:NA	2018
Changxi Zheng:Doug L. James	In this paper, we accelerate self-collision detection (SCD) for a deforming triangle mesh by exploiting the idea that a mesh cannot self collide unless it deforms enough. Unlike prior work on subspace self-collision culling which is restricted to low-rank deformation subspaces, our energy-based approach supports arbitrary mesh deformations while still being fast. Given a bounding volume hierarchy (BVH) for a triangle mesh, we precompute Energy-based Self-Collision Culling (ESCC) certificates on bounding-volume-related sub-meshes which indicate the amount of deformation energy required for it to self collide. After updating energy values at runtime, many bounding-volume self-collision queries can be culled using the ESCC certificates. We propose an affine-frame Laplacian-based energy definition which sports a highly optimized certificate pre-process, and fast runtime energy evaluation. The latter is performed hierarchically to amortize Laplacian energy and affine-frame estimation computations. ESCC supports both discrete and continuous SCD with detailed and nonsmooth geometry. We observe significant culling on many examples, with SCD speed-ups up to 26X.	Energy-based self-collision culling for arbitrary mesh deformations	NA:NA	2018
Youyi Zheng:Xiang Chen:Ming-Ming Cheng:Kun Zhou:Shi-Min Hu:Niloy J. Mitra	Images are static and lack important depth information about the underlying 3D scenes. We introduce interactive images in the context of man-made environments wherein objects are simple and regular, share various non-local relations (e.g., coplanarity, parallelism, etc.), and are often repeated. Our interactive framework creates partial scene reconstructions based on cuboid-proxies with minimal user interaction. It subsequently allows a range of intuitive image edits mimicking real-world behavior, which are otherwise difficult to achieve. Effectively, the user simply provides high-level semantic hints, while our system ensures plausible operations by conforming to the extracted non-local relations. We demonstrate our system on a range of real-world images and validate the plausibility of the results using a user study.	Interactive images: cuboid proxies for smart image manipulation	NA:NA:NA:NA:NA:NA	2018
Sudipta N. Sinha:Johannes Kopf:Michael Goesele:Daniel Scharstein:Richard Szeliski	We present a system for image-based modeling and rendering of real-world scenes containing reflective and glossy surfaces. Previous approaches to image-based rendering assume that the scene can be approximated by 3D proxies that enable view interpolation using traditional back-to-front or z-buffer compositing. In this work, we show how these can be generalized to multiple layers that are combined in an additive fashion to model the reflection and transmission of light that occurs at specular surfaces such as glass and glossy materials. To simplify the analysis and rendering stages, we model the world using piecewise-planar layers combined using both additive and opaque mixing of light. We also introduce novel techniques for estimating multiple depths in the scene and separating the reflection and transmission components into different layers. We then use our system to model and render a variety of real-world scenes with reflections.	Image-based rendering for scenes with reflections	NA:NA:NA:NA:NA	2018
Carl Doersch:Saurabh Singh:Abhinav Gupta:Josef Sivic:Alexei A. Efros	Given a large repository of geotagged imagery, we seek to automatically find visual elements, e. g. windows, balconies, and street signs, that are most distinctive for a certain geo-spatial area, for example the city of Paris. This is a tremendously difficult task as the visual features distinguishing architectural elements of different places can be very subtle. In addition, we face a hard search problem: given all possible patches in all images, which of them are both frequently occurring and geographically informative? To address these issues, we propose to use a discriminative clustering approach able to take into account the weak geographic supervision. We show that geographically representative image elements can be discovered automatically from Google Street View imagery in a discriminative manner. We demonstrate that these elements are visually interpretable and perceptually geo-informative. The discovered visual elements can also support a variety of computational geography tasks, such as mapping architectural correspondences and influences within and across cities, finding representative elements at different geo-spatial scales, and geographically-informed image retrieval.	What makes Paris look like Paris?	NA:NA:NA:NA:NA	2018
Steven S. An:Doug L. James:Steve Marschner	We present a practical data-driven method for automatically synthesizing plausible soundtracks for physics-based cloth animations running at graphics rates. Given a cloth animation, we analyze the deformations and use motion events to drive crumpling and friction sound models estimated from cloth measurements. We synthesize a low-quality sound signal, which is then used as a target signal for a concatenative sound synthesis (CSS) process. CSS selects a sequence of microsound units, very short segments, from a database of recorded cloth sounds, which best match the synthesized target sound in a low-dimensional feature-space after applying a hand-tuned warping function. The selected microsound units are concatenated together to produce the final cloth sound with minimal filtering. Our approach avoids expensive physics-based synthesis of cloth sound, instead relying on cloth recordings and our motion-driven CSS approach for realism. We demonstrate its effectiveness on a variety of cloth animations involving various materials and character motions, including first-person virtual clothing with binaural sound.	Motion-driven concatenative synthesis of cloth sounds	NA:NA:NA	2018
Jeffrey N. Chadwick:Changxi Zheng:Doug L. James	We introduce an efficient method for synthesizing acceleration noise -- sound produced when an object experiences abrupt rigid-body acceleration due to collisions or other contact events. We approach this in two main steps. First, we estimate continuous contact force profiles from rigid-body impulses using a simple model based on Hertz contact theory. Next, we compute solutions to the acoustic wave equation due to short acceleration pulses in each rigid-body degree of freedom. We introduce an efficient representation for these solutions -- Precomputed Acceleration Noise -- which allows us to accurately estimate sound due to arbitrary rigid-body accelerations. We find that the addition of acceleration noise significantly complements the standard modal sound algorithm, especially for small objects.	Precomputed acceleration noise for improved rigid-body sound	NA:NA:NA	2018
Steffen Weißmann:Ulrich Pinkall	We show that the motion of rigid bodies under water can be realistically simulated by replacing the usual inertia tensor and scalar mass by the so-called Kirchhoff tensor. This allows us to model fluid-body interaction without simulating the surrounding fluid at all. We explain some of the phenomena that arise and compare our results against real experiments. It turns out that many real scenarios (sinking bodies, balloons) can be matched using a single, hand-tuned scaling parameter. We describe how to integrate our method into an existing physics engine, which makes underwater rigid body dynamics run in real time.	Underwater rigid body dynamics	NA:NA	2018
Richard Tonge:Feodor Benevolenski:Andrey Voroshilov	We present a parallel iterative rigid body solver that avoids common artifacts at low iteration counts. In large or real-time simulations, iteration is often terminated before convergence to maximize scene size. If the distribution of the resulting residual energy varies too much from frame to frame, then bodies close to rest can visibly jitter. Projected Gauss-Seidel (PGS) distributes the residual according to the order in which contacts are processed, and preserving the order in parallel implementations is very challenging. In contrast, Jacobi-based methods provide order independence, but have slower convergence. We accelerate projected Jacobi by dividing each body mass term in the effective mass by the number of contacts acting on the body, but use the full mass to apply impulses. We further accelerate the method by solving contacts in blocks, providing wallclock performance competitive with PGS while avoiding visible artifacts. We prove convergence to the solution of the underlying linear complementarity problem and present results for our GPU implementation, which can simulate a pile of 5000 objects with no visible jittering at over 60 FPS.	Mass splitting for jitter-free parallel rigid body simulation	NA:NA:NA	2018
Breannan Smith:Danny M. Kaufman:Etienne Vouga:Rasmus Tamstorf:Eitan Grinspun	Resolving simultaneous impacts is an open and significant problem in collision response modeling. Existing algorithms in this domain fail to fulfill at least one of five physical desiderata. To address this we present a simple generalized impact model motivated by both the successes and pitfalls of two popular approaches: pair-wise propagation and linear complementarity models. Our algorithm is the first to satisfy all identified desiderata, including simultaneously guaranteeing symmetry preservation, kinetic energy conservation, and allowing break-away. Furthermore, we address the associated problem of inelastic collapse, proposing a complementary generalized restitution model that eliminates this source of nontermination. We then consider the application of our models to the synchronous time-integration of large-scale assemblies of impacting rigid bodies. To enable such simulations we formulate a consistent frictional impact model that continues to satisfy the desiderata. Finally, we validate our proposed algorithm by correctly capturing the observed characteristics of physical experiments including the phenomenon of extended patterns in vertically oscillated granular materials.	Reflections on simultaneous impact	NA:NA:NA:NA:NA	2018
Min Tang:Dinesh Manocha:Miguel A. Otaduy:Ruofeng Tong	We present a simple algorithm to compute continuous penalty forces to determine collision response between rigid and deformable models bounded by triangle meshes. Our algorithm computes a well-behaved solution in contrast to the traditional stability and robustness problems of penalty methods, induced by force discontinuities. We trace contact features along their deforming trajectories and accumulate penalty forces along the penetration time intervals between the overlapping feature pairs. Moreover, we present a closed-form expression to compute the continuous and smooth collision response. Our method has very small additional overhead compared to previous penalty methods, and can significantly improve the stability and robustness. We highlight its benefits on several benchmarks.	Continuous penalty forces	NA:NA:NA:NA	2018
Yaron Lipman	The problem of mapping triangular meshes into the plane is fundamental in geometric modeling, where planar deformations and surface parameterizations are two prominent examples. Current methods for triangular mesh mappings cannot, in general, control the worst case distortion of all triangles nor guarantee injectivity. This paper introduces a constructive definition of generic convex spaces of piecewise linear mappings with guarantees on the maximal conformal distortion, as-well as local and global injectivity of their maps. It is shown how common geometric processing objective functionals can be restricted to these new spaces, rather than to the entire space of piecewise linear mappings, to provide a bounded distortion version of popular algorithms.	Bounded distortion mapping spaces for triangular meshes	NA	2018
Ashish Myles:Denis Zorin	Global parametrization of surfaces requires singularities (cones) to keep distortion minimal. We describe a method for finding cone locations and angles and an algorithm for global parametrization which aim to produce seamless parametrizations with low metric distortion. The idea of the method is to evolve the metric of the surface, starting with the original metric so that a growing fraction of the area of the surface is constrained to have zero Gaussian curvature; the curvature becomes gradually concentrated at a small set of vertices which become cones. We demonstrate that the resulting parametrizations have significantly lower metric distortion compared to previously proposed methods.	Global parametrization by incremental flattening	NA:NA	2018
Marcel Campen:David Bommes:Leif Kobbelt	We present a theoretical framework and practical method for the automatic construction of simple, all-quadrilateral patch layouts on manifold surfaces. The resulting layouts are coarse, surface-embedded cell complexes well adapted to the geometric structure, hence they are ideally suited as domains and base complexes for surface parameterization, spline fitting, or subdivision surfaces and can be used to generate quad meshes with a high-level patch structure that are advantageous in many application scenarios. Our approach is based on the careful construction of the layout graph's combinatorial dual. In contrast to the primal this dual perspective provides direct control over the globally interdependent structural constraints inherent to quad layouts. The dual layout is built from curvature-guided, crossing loops on the surface. A novel method to construct these efficiently in a geometry- and structure-aware manner constitutes the core of our approach.	Dual loops meshing: quality quad layouts on manifolds	NA:NA:NA	2018
Daniele Panozzo:Yaron Lipman:Enrico Puppo:Denis Zorin	Direction fields, line fields and cross fields are used in a variety of computer graphics applications ranging from non-photorealistic rendering to remeshing. In many cases, it is desirable that fields adhere to symmetry, which is predominant in natural as well as man-made shapes. We present an algorithm for designing smooth N-symmetry fields on surfaces respecting generalized symmetries of the shape, while maintaining alignment with local features. Our formulation for constructing symmetry fields is based on global symmetries, which are given as input to the algorithm, with no isometry assumptions. We explore in detail the properties of generalized symmetries (reflections in particular), and we also develop an algorithm for the robust computation of such symmetry maps, based on a small number of correspondences, for surfaces of genus zero.	Fields on symmetric surfaces	NA:NA:NA:NA	2018
Tobias Pfaff:Nils Thuerey:Markus Gross	Buoyant turbulent smoke plumes with a sharp smoke-air interface, such as volcanic plumes, are notoriously hard to simulate. The surface clearly shows small-scale turbulent structures which are costly to resolve. In addition, the turbulence onset is directly visible at the interface, and is not captured by commonly used turbulence models. We present a novel approach that employs a triangle mesh as a high-resolution surface representation combined with a coarse Eulerian solver. On the mesh, we solve the interfacial vortex sheet equations, which allows us to accurately simulate buoyancy induced turbulence. For complex boundary conditions we propose an orthogonal turbulence model that handles vortices caused by obstacle interaction. In addition, we demonstrate a re-sampling scheme to remove surfaces that are hidden inside the bulk volume. In this way we are able to achieve highly detailed simulations of turbulent plumes efficiently.	Lagrangian vortex sheets for animating fluids	NA:NA:NA	2018
Christopher Batty:Andres Uribe:Basile Audoly:Eitan Grinspun	We present the first reduced-dimensional technique to simulate the dynamics of thin sheets of viscous incompressible liquid in three dimensions. Beginning from a discrete Lagrangian model for elastic thin shells, we apply the Stokes-Rayleigh analogy to derive a simple yet consistent model for viscous forces. We incorporate nonlinear surface tension forces with a formulation based on minimizing discrete surface area, and preserve the quality of triangular mesh elements through local remeshing operations. Simultaneously, we track and evolve the thickness of each triangle to exactly conserve liquid volume. This approach enables the simulation of extremely thin sheets of viscous liquids, which are difficult to animate with existing volumetric approaches. We demonstrate our method with examples of several characteristic viscous sheet behaviors, including stretching, buckling, sagging, and wrinkling.	Discrete viscous sheets	NA:NA:NA:NA	2018
Zhan Yuan:Yizhou Yu:Wenping Wang	Implicit functions have a wide range of applications in entertainment, engineering and medical imaging. A standard two-phase implicit function only represents the interior and exterior of a single object. To facilitate solid modeling of heterogeneous objects with multiple internal regions, object-space multiphase implicit functions are much desired. Multiphase implicit functions have much potential in modeling natural organisms, heterogeneous mechanical parts and anatomical atlases. In this paper, we introduce a novel class of object-space multiphase implicit functions that are capable of accurately and compactly representing objects with multiple internal regions. Our proposed multiphase implicit functions facilitate true object-space geometric modeling of heterogeneous objects with non-manifold features. We present multiple methods to create object-space multiphase implicit functions from existing data, including meshes and segmented medical images. Our algorithms are inspired by machine learning algorithms for training multicategory max-margin classifiers. Comparisons demonstrate that our method achieves an error rate one order of magnitude smaller than alternative techniques.	Object-space multiphase implicit functions	NA:NA:NA	2018
Powei Feng:Joe Warren	Divided differences play a fundamental role in the construction of univariate B-splines over irregular knot sequences. Unfortunately, generalizations of divided differences to irregular knot geometries on two-dimensional domains are quite limited. As a result, most spline constructions for such domains typically focus on regular (or semi-regular) knot geometries. In the planar harmonic case, we show that the discrete Laplacian plays a role similar to that of the divided differences and can be used to define well-behaved harmonic B-splines. In our main contribution, we then construct an analogous discrete bi-Laplacian for both planar and curved domains and show that its corresponding biharmonic B-splines are also well-behaved. Finally, we derive a fully irregular, discrete refinement scheme for these splines that generalizes knot insertion for univariate B-splines.	Discrete bi-Laplacians and biharmonic b-splines	NA:NA	2018
Menglei Chai:Lvdi Wang:Yanlin Weng:Yizhou Yu:Baining Guo:Kun Zhou	Human hair is known to be very difficult to model or reconstruct. In this paper, we focus on applications related to portrait manipulation and take an application-driven approach to hair modeling. To enable an average user to achieve interesting portrait manipulation results, we develop a single-view hair modeling technique with modest user interaction to meet the unique requirements set by portrait manipulation. Our method relies on heuristics to generate a plausible high-resolution strand-based 3D hair model. This is made possible by an effective high-precision 2D strand tracing algorithm, which explicitly models uncertainty and local layering during tracing. The depth of the traced strands is solved through an optimization, which simultaneously considers depth constraints, layering constraints as well as regularization terms. Our single-view hair modeling enables a number of interesting applications that were previously challenging, including transferring the hairstyle of one subject to another in a potentially different pose, rendering the original portrait in a novel view and image-space hair editing.	Single-view hair modeling for portrait manipulation	NA:NA:NA:NA:NA:NA	2018
Thabo Beeler:Bernd Bickel:Gioacchino Noris:Paul Beardsley:Steve Marschner:Robert W. Sumner:Markus Gross	Although facial hair plays an important role in individual expression, facial-hair reconstruction is not addressed by current face-capture systems. Our research addresses this limitation with an algorithm that treats hair and skin surface capture together in a coupled fashion so that a high-quality representation of hair fibers as well as the underlying skin surface can be reconstructed. We propose a passive, camera-based system that is robust against arbitrary motion since all data is acquired within the time period of a single exposure. Our reconstruction algorithm detects and traces hairs in the captured images and reconstructs them in 3D using a multiview stereo approach. Our coupled skin-reconstruction algorithm uses information about the detected hairs to deliver a skin surface that lies underneath all hairs irrespective of occlusions. In dense regions like eyebrows, we employ a hair-synthesis method to create hair fibers that plausibly match the image data. We demonstrate our scanning system on a number of individuals and show that it can successfully reconstruct a variety of facial-hair styles together with the underlying skin surface.	Coupled 3D reconstruction of sparse facial hair and skin	NA:NA:NA:NA:NA:NA:NA	2018
Alla Sheffer	NA	Session details: Geometry & topology	NA	2018
Tamal K. Dey:Fengtao Fan:Yusu Wang	A special family of non-trivial loops on a surface called handle and tunnel loops associates closely to geometric features of "handles" and "tunnels" respectively in a 3D model. The identification of these handle and tunnel loops can benefit a broad range of applications from topology simplification/repair, and surface parameterization, to feature and shape recognition. Many of the existing efficient algorithms for computing non-trivial loops cannot be used to compute these special type of loops. The two algorithms known for computing handle and tunnel loops provably have a serious drawback that they both require a tessellation of the interior and exterior spaces bounded by the surface. Computing such a tessellation of three dimensional space around the surface is a non-trivial task and can be quite expensive. Furthermore, such a tessellation may need to refine the surface mesh, thus causing the undesirable side-effect of outputting the loops on an altered surface mesh. In this paper, we present an efficient algorithm to compute a basis for handle and tunnel loops without requiring any 3D tessellation. This saves time considerably for large meshes making the algorithm scalable while computing the loops on the original input mesh and not on some refined version of it. We use the concept of the Reeb graph which together with several key theoretical insights on linking number provide an initial set of loops that provably constitute a handle and a tunnel basis. We further develop a novel strategy to tighten these handle and tunnel basis loops to make them geometrically relevant. We demonstrate the efficiency and effectiveness of our algorithm as well as show its robustness against noise, and other anomalies in the input.	An efficient computation of handle and tunnel loops via Reeb graphs	NA:NA:NA	2018
Alec Jacobson:Ladislav Kavan:Olga Sorkine-Hornung	Solid shapes in computer graphics are often represented with boundary descriptions, e.g. triangle meshes, but animation, physically-based simulation, and geometry processing are more realistic and accurate when explicit volume representations are available. Tetrahedral meshes which exactly contain (interpolate) the input boundary description are desirable but difficult to construct for a large class of input meshes. Character meshes and CAD models are often composed of many connected components with numerous self-intersections, non-manifold pieces, and open boundaries, precluding existing meshing algorithms. We propose an automatic algorithm handling all of these issues, resulting in a compact discretization of the input's inner volume. We only require reasonably consistent orientation of the input triangle mesh. By generalizing the winding number for arbitrary triangle meshes, we define a function that is a perfect segmentation for watertight input and is well-behaved otherwise. This function guides a graphcut segmentation of a constrained Delaunay tessellation (CDT), providing a minimal description that meets the boundary exactly and may be fed as input to existing tools to achieve element quality. We highlight our robustness on a number of examples and show applications of solving PDEs, volumetric texturing and elastic simulation.	Robust inside-outside segmentation using generalized winding numbers	NA:NA:NA	2018
Gilbert Louis Bernstein:Chris Wojtan	This paper presents a method for computing topology changes for triangle meshes in an interactive geometric modeling environment. Most triangle meshes in practice do not exhibit desirable geometric properties, so we develop a solution that is independent of standard assumptions and robust to geometric errors. Specifically, we provide the first method for topology change applicable to arbitrary non-solid, non-manifold, non-closed, self-intersecting surfaces. We prove that this new method for topology change produces the expected conventional results when applied to solid (closed, manifold, non-self-intersecting) surfaces---that is, we prove a backwards-compatibility property relative to prior work. Beyond solid surfaces, we present empirical evidence that our method remains tolerant to a variety of surface aberrations through the incorporation of a novel error correction scheme. Finally, we demonstrate how topology change applied to non-solid objects enables wholly new and useful behaviors.	Putting holes in holey geometry: topology change for arbitrary surfaces	NA:NA	2018
Jonathan D. Denning:Fabio Pellacini	This paper presents MeshGit, a practical algorithm for diffing and merging polygonal meshes typically used in subdivision modeling workflows. Inspired by version control for text editing, we introduce the mesh edit distance as a measure of the dissimilarity between meshes. This distance is defined as the minimum cost of matching the vertices and faces of one mesh to those of another. We propose an iterative greedy algorithm to approximate the mesh edit distance, which scales well with model complexity, providing a practical solution to our problem. We translate the mesh correspondence into a set of mesh editing operations that transforms the first mesh into the second. The editing operations can be displayed directly to provide a meaningful visual difference between meshes. For merging, we compute the difference between two versions and their common ancestor, as sets of editing operations. We robustly detect conflicting operations, automatically apply non-conflicting edits, and allow the user to choose how to merge the conflicting edits. We evaluate MeshGit by diffing and merging a variety of meshes and find it to work well for all.	MeshGit: diffing and merging meshes for polygonal modeling	NA:NA	2018
Alexander Hornung	NA	Session details: Color & compositing	NA	2018
Ivaylo Boyadzhiev:Sylvain Paris:Kavita Bala	Good lighting is crucial in photography and can make the difference between a great picture and a discarded image. Traditionally, professional photographers work in a studio with many light sources carefully set up, with the goal of getting a near-final image at exposure time, with post-processing mostly focusing on aspects orthogonal to lighting. Recently, a new workflow has emerged for architectural and commercial photography, where photographers capture several photos from a fixed viewpoint with a moving light source. The objective is not to produce the final result immediately, but rather to capture useful data that are later processed, often significantly, in photo editing software to create the final well-lit image. This new workflow is flexible, requires less manual setup, and works well for time-constrained shots. But dealing with several tens of unorganized layers is painstaking, requiring hours to days of manual effort, as well as advanced photo editing skills. Our objective in this paper is to make the compositing step easier. We describe a set of optimizations to assemble the input images to create a few basis lights that correspond to common goals pursued by photographers, e.g., accentuating edges and curved regions. We also introduce modifiers that capture standard photographic tasks, e.g., to alter the lights to soften highlights and shadows, akin to umbrellas and soft boxes. Our experiments with novice and professional users show that our approach allows them to quickly create satisfying results, whereas working with unorganized images requires considerably more time. Casual users particularly benefit from our approach since coping with a large number of layers is daunting for them and requires significant experience.	User-assisted image compositing for photographic lighting	NA:NA:NA	2018
Sharon Lin:Daniel Ritchie:Matthew Fisher:Pat Hanrahan	We present a probabilistic factor graph model for automatically coloring 2D patterns. The model is trained on example patterns to statistically capture their stylistic properties. It incorporates terms for enforcing both color compatibility and spatial arrangements of colors that are consistent with the training examples. Using Markov Chain Monte Carlo, the model can be sampled to generate a diverse set of new colorings for a target pattern. This general probabilistic framework allows users to guide the generated suggestions via conditional inference or additional soft constraints. We demonstrate results on a variety of coloring tasks, and we evaluate the model through a perceptual study in which participants judged sampled colorings to be significantly preferable to other automatic baselines.	Probabilistic color-by-numbers: suggesting pattern colorizations using factor graphs	NA:NA:NA:NA	2018
Yoav HaCohen:Eli Shechtman:Dan B. Goldman:Dani Lischinski	With dozens or even hundreds of photos in today's digital photo albums, editing an entire album can be a daunting task. Existing automatic tools operate on individual photos without ensuring consistency of appearance between photographs that share content. In this paper, we present a new method for consistent editing of photo collections. Our method automatically enforces consistent appearance of images that share content without any user input. When the user does make changes to selected images, these changes automatically propagate to other images in the collection, while still maintaining as much consistency as possible. This makes it possible to interactively adjust an entire photo album in a consistent manner by manipulating only a few images. Our method operates by efficiently constructing a graph with edges linking photo pairs that share content. Consistent appearance of connected photos is achieved by globally optimizing a quadratic cost function over the entire graph, treating user-specified edits as constraints in the optimization. The optimization is fast enough to provide interactive visual feedback to the user. We demonstrate the usefulness of our approach using a number of personal and professional photo collections, as well as internet collections.	Optimizing color consistency in photo collections	NA:NA:NA:NA	2018
Nicolas Bonneel:Kalyan Sunkavalli:Sylvain Paris:Hanspeter Pfister	In most professional cinema productions, the color palette of the movie is painstakingly adjusted by a team of skilled colorists -- through a process referred to as color grading -- to achieve a certain visual look. The time and expertise required to grade a video makes it difficult for amateurs to manipulate the colors of their own video clips. In this work, we present a method that allows a user to transfer the color palette of a model video clip to their own video sequence. We estimate a per-frame color transform that maps the color distributions in the input video sequence to that of the model video clip. Applying this transformation naively leads to artifacts such as bleeding and flickering. Instead, we propose a novel differential-geometry-based scheme that interpolates these transformations in a manner that minimizes their curvature, similarly to curvature flows. In addition, we automatically determine a set of keyframes that best represent this interpolated transformation curve, and can be used subsequently, to manually refine the color grade. We show how our method can successfully transfer color palettes between videos for a range of visual styles and a number of input video clips.	Example-based video color grading	NA:NA:NA:NA	2018
Yaser Sheikh	NA	Session details: Faces & hands	NA	2018
Sofien Bouaziz:Yangang Wang:Mark Pauly	We present a new algorithm for realtime face tracking on commodity RGB-D sensing devices. Our method requires no user-specific training or calibration, or any other form of manual assistance, thus enabling a range of new applications in performance-based facial animation and virtual interaction at the consumer level. The key novelty of our approach is an optimization algorithm that jointly solves for a detailed 3D expression model of the user and the corresponding dynamic tracking parameters. Realtime performance and robust computations are facilitated by a novel subspace parameterization of the dynamic facial expression space. We provide a detailed evaluation that shows that our approach significantly simplifies the performance capture workflow, while achieving accurate facial tracking for realtime applications.	Online modeling for realtime facial animation	NA:NA:NA	2018
Chen Cao:Yanlin Weng:Stephen Lin:Kun Zhou	We present a real-time performance-driven facial animation system based on 3D shape regression. In this system, the 3D positions of facial landmark points are inferred by a regressor from 2D video frames of an ordinary web camera. From these 3D points, the pose and expressions of the face are recovered by fitting a user-specific blendshape model to them. The main technical contribution of this work is the 3D regression algorithm that learns an accurate, user-specific face alignment model from an easily acquired set of training data, generated from images of the user performing a sequence of predefined facial poses and expressions. Experiments show that our system can accurately recover 3D face shapes even for fast motions, non-frontal faces, and exaggerated expressions. In addition, some capacity to handle partial occlusions and changing lighting conditions is demonstrated.	3D shape regression for real-time facial animation	NA:NA:NA:NA	2018
Hao Li:Jihun Yu:Yuting Ye:Chris Bregler	We introduce a real-time and calibration-free facial performance capture framework based on a sensor with video and depth input. In this framework, we develop an adaptive PCA model using shape correctives that adjust on-the-fly to the actor's expressions through incremental PCA-based learning. Since the fitting of the adaptive model progressively improves during the performance, we do not require an extra capture or training session to build this model. As a result, the system is highly deployable and easy to use: it can faithfully track any individual, starting from just a single face scan of the subject in a neutral pose. Like many real-time methods, we use a linear subspace to cope with incomplete input data and fast motion. To boost the training of our tracking model with reliable samples, we use a well-trained 2D facial feature tracker on the input video and an efficient mesh deformation algorithm to snap the result of the previous step to high frequency details in visible depth map regions. We show that the combination of dense depth maps and texture features around eyes and lips is essential in capturing natural dialogues and nuanced actor-specific emotions. We demonstrate that using an adaptive PCA model not only improves the fitting accuracy for tracking but also increases the expressiveness of the retargeted character.	Realtime facial animation with on-the-fly correctives	NA:NA:NA:NA	2018
Yangang Wang:Jianyuan Min:Jianjie Zhang:Yebin Liu:Feng Xu:Qionghai Dai:Jinxiang Chai	This paper describes a new method for acquiring physically realistic hand manipulation data from multiple video streams. The key idea of our approach is to introduce a composite motion control to simultaneously model hand articulation, object movement, and subtle interaction between the hand and object. We formulate video-based hand manipulation capture in an optimization framework by maximizing the consistency between the simulated motion and the observed image data. We search an optimal motion control that drives the simulation to best match the observed image data. We demonstrate the effectiveness of our approach by capturing a wide range of high-fidelity dexterous manipulation data. We show the power of our recovered motion controllers by adapting the captured motion data to new objects with different properties. The system achieves superior performance against alternative methods such as marker-based motion capture and kinematic hand motion tracking.	Video-based hand manipulation capture through composite motion control	NA:NA:NA:NA:NA:NA:NA	2018
Kari Pulli	NA	Session details: Computational light capture	NA	2018
Andreas Velten:Di Wu:Adrian Jarabo:Belen Masia:Christopher Barsi:Chinmaya Joshi:Everett Lawson:Moungi Bawendi:Diego Gutierrez:Ramesh Raskar	We present femto-photography, a novel imaging technique to capture and visualize the propagation of light. With an effective exposure time of 1.85 picoseconds (ps) per frame, we reconstruct movies of ultrafast events at an equivalent resolution of about one half trillion frames per second. Because cameras with this shutter speed do not exist, we re-purpose modern imaging hardware to record an ensemble average of repeatable events that are synchronized to a streak sensor, in which the time of arrival of light from the scene is coded in one of the sensor's spatial dimensions. We introduce reconstruction methods that allow us to visualize the propagation of femtosecond light pulses through macroscopic scenes; at such fast resolution, we must consider the notion of time-unwarping between the camera's and the world's space-time coordinate systems to take into account effects associated with the finite speed of light. We apply our femto-photography technique to visualizations of very different scenes, which allow us to observe the rich dynamics of time-resolved light transport effects, including scattering, specular reflections, diffuse interreflections, diffraction, caustics, and subsurface scattering. Our work has potential applications in artistic, educational, and scientific visualizations; industrial imaging to analyze material properties; and medical imaging to reconstruct subsurface elements. In addition, our time-resolved technique may motivate new forms of computational photography.	Femto-photography: capturing and visualizing the propagation of light	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Felix Heide:Matthias B. Hullin:James Gregson:Wolfgang Heidrich	Transient imaging is an exciting a new imaging modality that can be used to understand light propagation in complex environments, and to capture and analyze scene properties such as the shape of hidden objects or the reflectance properties of surfaces. Unfortunately, research in transient imaging has so far been hindered by the high cost of the required instrumentation, as well as the fragility and difficulty to operate and calibrate devices such as femtosecond lasers and streak cameras. In this paper, we explore the use of photonic mixer devices (PMD), commonly used in inexpensive time-of-flight cameras, as alternative instrumentation for transient imaging. We obtain a sequence of differently modulated images with a PMD sensor, impose a model for local light/object interaction, and use an optimization procedure to infer transient images given the measurements and model. The resulting method produces transient images at a cost several orders of magnitude below existing methods, while simultaneously simplifying and speeding up the capture process.	Low-budget transient imaging using photonic mixer devices	NA:NA:NA:NA	2018
Kshitij Marwah:Gordon Wetzstein:Yosuke Bando:Ramesh Raskar	Light field photography has gained a significant research interest in the last two decades; today, commercial light field cameras are widely available. Nevertheless, most existing acquisition approaches either multiplex a low-resolution light field into a single 2D sensor image or require multiple photographs to be taken for acquiring a high-resolution light field. We propose a compressive light field camera architecture that allows for higher-resolution light fields to be recovered than previously possible from a single image. The proposed architecture comprises three key components: light field atoms as a sparse representation of natural light fields, an optical design that allows for capturing optimized 2D light field projections, and robust sparse reconstruction methods to recover a 4D light field from a single coded 2D projection. In addition, we demonstrate a variety of other applications for light field atoms and sparse coding, including 4D light field compression and denoising.	Compressive light field photography using overcomplete dictionaries and optimized projections	NA:NA:NA:NA	2018
Alkhazur Manakov:John F. Restrepo:Oliver Klehm:Ramon Hegedüs:Elmar Eisemann:Hans-Peter Seidel:Ivo Ihrke	We propose a non-permanent add-on that enables plenoptic imaging with standard cameras. Our design is based on a physical copying mechanism that multiplies a sensor image into a number of identical copies that still carry the plenoptic information of interest. Via different optical filters, we can then recover the desired information. A minor modification of the design also allows for aperture sub-sampling and, hence, light-field imaging. As the filters in our design are exchangeable, a reconfiguration for different imaging purposes is possible. We show in a prototype setup that high dynamic range, multispectral, polarization, and light-field imaging can be achieved with our design.	A reconfigurable camera add-on for high dynamic range, multispectral, polarization, and light-field imaging	NA:NA:NA:NA:NA:NA:NA	2018
Doug James	NA	Session details: Rods & shells	NA	2018
Romain Casati:Florence Bertails-Descoubes	Thin elastic filaments in real world such as vine tendrils, hair ringlets or curled ribbons often depict a very smooth, curved shape that low-order rod models --- e.g., segment-based rods --- fail to reproduce accurately and compactly. In this paper, we push forward the investigation of high-order models for thin, inextensible elastic rods by building the dynamics of a G2-continuous piecewise 3D clothoid: a smooth space curve with piecewise affine curvature. With the aim of precisely integrating the rod kinematic problem, for which no closed-form solution exists, we introduce a dedicated integration scheme based on power series expansions. It turns out that our algorithm reaches machine precision orders of magnitude faster compared to classical numerical integrators. This property, nicely preserved under simple algebraic and differential operations, allows us to compute all spatial terms of the rod kinematics and dynamics in both an efficient and accurate way. Combined with a semi-implicit time-stepping scheme, our method leads to the efficient and robust simulation of arbitrary curly filaments that exhibit rich, visually pleasing configurations and motion. Our approach was successfully applied to generate various scenarios such as the unwinding of a curled ribbon as well as the aesthetic animation of spiral-like hair or the fascinating growth of twining plants.	Super space clothoids	NA:NA	2018
Duo Li:Shinjiro Sueda:Debanga R. Neog:Dinesh K. Pai	We present a novel approach for simulating thin hyperelastic skin. Real human skin is only a few millimeters thick. It can stretch and slide over underlying body structures such as muscles, bones, and tendons, revealing rich details of a moving character. Simulating such skin is challenging because it is in close contact with the body and shares its geometry. Despite major advances in simulating elastodynamics of cloth and soft bodies for computer graphics, such methods are difficult to use for simulating thin skin due to the need to deal with non-conforming meshes, collision detection, and contact response. We propose a novel Eulerian representation of skin that avoids all the difficulties of constraining the skin to lie on the body surface by working directly on the surface itself. Skin is modeled as a 2D hyperelastic membrane with arbitrary topology, which makes it easy to cover an entire character or object. Unlike most Eulerian simulations, we do not require a regular grid and can use triangular meshes to model body and skin geometry. The method is easy to implement, and can use low resolution meshes to animate high-resolution details stored in texture-like maps. Skin movement is driven by the animation of body shape prescribed by an artist or by another simulation, and so it can be easily added as a post-processing stage to an existing animation pipeline. We provide several examples simulating human and animal skin, and skin-tight clothes.	Thin skin elastodynamics	NA:NA:NA:NA	2018
Olivier Rémillard:Paul G. Kry	We present a new technique for simulating high resolution surface wrinkling deformations of composite objects consisting of a soft interior and a harder skin. We combine high resolution thin shells with coarse finite element lattices and define frequency based constraints that allow the formation of wrinkles with properties matching those predicted by the physical parameters of the composite object. Our two-way coupled model produces the expected wrinkling behavior without the computational expense of a large number of volumetric elements to model deformations under the surface. We use C1 quadratic shape functions for the interior deformations, allowing very coarse resolutions to model the overall global deformation efficiently, while avoiding visual artifacts of wrinkling at discretization boundaries. We demonstrate that our model produces wrinkle wavelengths that match both theoretical predictions and high resolution volumetric simulations. We also show example applications in simulating wrinkles on passive objects, such as furniture, and for wrinkles on faces in character animation.	Embedded thin shells for wrinkle simulation	NA:NA	2018
Rahul Narain:Tobias Pfaff:James F. O'Brien	We present a technique for simulating plastic deformation in sheets of thin materials, such as crumpled paper, dented metal, and wrinkled cloth. Our simulation uses a framework of adaptive mesh refinement to dynamically align mesh edges with folds and creases. This framework allows efficient modeling of sharp features and avoids bend locking that would be otherwise caused by stiff in-plane behavior. By using an explicit plastic embedding space we prevent remeshing from causing shape diffusion. We include several examples demonstrating that the resulting method realistically simulates the behavior of thin sheets as they fold and crumple.	Folding and crumpling adaptive sheets	NA:NA:NA	2018
Oleksiy Busaryev:Tamal K. Dey:Huamin Wang	The fractures of thin plates often exhibit complex physical behaviors in the real world. In particular, fractures caused by tearing are different from fractures caused by in-plane motions. In this paper, we study how to make thin-plate fracture animations more realistic from three perspectives. We propose a stress relaxation method, which is applied to avoid shattering artifacts after generating each fracture cut. We formulate a fracture-aware remeshing scheme based on constrained Delaunay triangulation, to adaptively provide more fracture details. Finally, we use our multi-layered model to simulate complex fracture behaviors across thin layers. Our experiment shows that the system can efficiently and realistically simulate the fractures of multi-layered thin plates.	Adaptive fracture simulation of multi-layered thin plates	NA:NA:NA	2018
Adam Finkelstein	NA	Session details: Line drawing	NA	2018
C. Lawrence Zitnick	In this paper, we propose a general purpose approach to handwriting beautification using online input from a stylus. Given a sample of writings, drawings, or sketches from the same user, our method improves a user's strokes in real-time as they are drawn. Our approach relies on one main insight. The appearance of the average of multiple instances of the same written word or shape is better than most of the individual instances. We utilize this observation using a two-stage approach. First, we propose an efficient real-time method for finding matching sets of stroke samples called tokens in a potentially large database of writings from a user. Second, we refine the user's most recently written strokes by averaging them with the matching tokens. Our approach works without handwriting recognition, and does not require a database of predefined letters, words, or shapes. Our results show improved results for a wide range of writing styles and drawings.	Handwriting beautification using token means	NA	2018
Alex Limpaecher:Nicolas Feltman:Adrien Treuille:Michael Cohen	We propose a new method for the large-scale collection and analysis of drawings by using a mobile game specifically designed to collect such data. Analyzing this crowdsourced drawing database, we build a spatially varying model of artistic consensus at the stroke level. We then present a surprisingly simple stroke-correction method which uses our artistic consensus model to improve strokes in real-time. Importantly, our auto-corrections run interactively and appear nearly invisible to the user while seamlessly preserving artistic intent. Closing the loop, the game itself serves as a platform for large-scale evaluation of the effectiveness of our stroke correction algorithm.	Real-time drawing assistance through crowdsourcing	NA:NA:NA:NA	2018
Itamar Berger:Ariel Shamir:Moshe Mahler:Elizabeth Carter:Jessica Hodgins	We use a data-driven approach to study both style and abstraction in sketching of a human face. We gather and analyze data from a number of artists as they sketch a human face from a reference photograph. To achieve different levels of abstraction in the sketches, decreasing time limits were imposed -- from four and a half minutes to fifteen seconds. We analyzed the data at two levels: strokes and geometric shape. In each, we create a model that captures both the style of the different artists and the process of abstraction. These models are then used for a portrait sketch synthesis application. Starting from a novel face photograph, we can synthesize a sketch in the various artistic styles and in different levels of abstraction.	Style and abstraction in portrait sketching	NA:NA:NA:NA:NA	2018
Tianjia Shao:Wilmot Li:Kun Zhou:Weiwei Xu:Baining Guo:Niloy J. Mitra	Concept sketches are popularly used by designers to convey pose and function of products. Understanding such sketches, however, requires special skills to form a mental 3D representation of the product geometry by linking parts across the different sketches and imagining the intermediate object configurations. Hence, the sketches can remain inaccessible to many, especially non-designers. We present a system to facilitate easy interpretation and exploration of concept sketches. Starting from crudely specified incomplete geometry, often inconsistent across the different views, we propose a globally-coupled analysis to extract part correspondence and inter-part junction information that best explain the different sketch views. The user can then interactively explore the abstracted object to gain better understanding of the product functions. Our key technical contribution is performing shape analysis without access to any coherent 3D geometric model by reasoning in the space of inter-part relations. We evaluate our system on various concept sketches obtained from popular product design books and websites.	Interpreting concept sketches	NA:NA:NA:NA:NA:NA	2018
Yongjin Kim:Yunjin Lee:Henry Kang:Seungyong Lee	This paper discusses stereoscopic 3D imaging based on line drawing of 3D shapes. We describe the major issues and challenges in generating stereoscopic 3D effects using lines only, with a couple of relatively simple approaches called each-eye-based and center-eye-based. Each of these methods has its shortcomings, such as binocular rivalry and inaccurate lines. We explain why and how these problems occur, then describe the concept of stereo-coherent lines and an algorithm to extract them from 3D shapes. We also propose a simple method to stylize stereo lines that ensures the stereo coherence of stroke textures across binocular views. The proposed method provides viewers with unique visual experience of watching 2D drawings popping out of the screen like 3D.	Stereoscopic 3D line drawing	NA:NA:NA:NA	2018
Diego Gutierrez	NA	Session details: Perception	NA	2018
Peter Vangorp:Christian Richardt:Emily A. Cooper:Gaurav Chaurasia:Martin S. Banks:George Drettakis	Image-based rendering (IBR) creates realistic images by enriching simple geometries with photographs, e.g., mapping the photograph of a building façade onto a plane. However, as soon as the viewer moves away from the correct viewpoint, the image in the retina becomes distorted, sometimes leading to gross misperceptions of the original geometry. Two hypotheses from vision science state how viewers perceive such image distortions, one claiming that they can compensate for them (and therefore perceive scene geometry reasonably correctly), and one claiming that they cannot compensate (and therefore can perceive rather significant distortions). We modified the latter hypothesis so that it extends to street-level IBR. We then conducted a rigorous experiment that measured the magnitude of perceptual distortions that occur with IBR for façade viewing. We also conducted a rating experiment that assessed the acceptability of the distortions. The results of the two experiments were consistent with one another. They showed that viewers' percepts are indeed distorted, but not as severely as predicted by the modified vision science hypothesis. From our experimental results, we develop a predictive model of distortion for street-level IBR, which we use to provide guidelines for acceptability of virtual views and for capture camera density. We perform a confirmatory study to validate our predictions, and illustrate their use with an application that guides users in IBR navigation to stay in regions where virtual views yield acceptable perceptual distortions.	Perception of perspective distortions in image-based rendering	NA:NA:NA:NA:NA:NA	2018
Yaron Lipman	NA	Session details: Surfaces & differential geometry	NA	2018
Felix Knöppel:Keenan Crane:Ulrich Pinkall:Peter Schröder	We present a method for constructing smooth n-direction fields (line fields, cross fields, etc.) on surfaces that is an order of magnitude faster than state-of-the-art methods, while still producing fields of equal or better quality. Fields produced by the method are globally optimal in the sense that they minimize a simple, well-defined quadratic smoothness energy over all possible configurations of singularities (number, location, and index). The method is fully automatic and can optionally produce fields aligned with a given guidance field such as principal curvature directions. Computationally the smoothest field is found via a sparse eigenvalue problem involving a matrix similar to the cotan-Laplacian. When a guidance field is present, finding the optimal field amounts to solving a single linear system.	Globally optimal direction fields	NA:NA:NA:NA	2018
Daniele Panozzo:Ilya Baran:Olga Diamanti:Olga Sorkine-Hornung	We consider the problem of generalizing affine combinations in Euclidean spaces to triangle meshes: computing weighted averages of points on surfaces. We address both the forward problem, namely computing an average of given anchor points on the mesh with given weights, and the inverse problem, which is computing the weights given anchor points and a target point. Solving the forward problem on a mesh enables applications such as splines on surfaces, Laplacian smoothing and remeshing. Combining the forward and inverse problems allows us to define a correspondence mapping between two different meshes based on provided corresponding point pairs, enabling texture transfer, compatible remeshing, morphing and more. Our algorithm solves a single instance of a forward or an inverse problem in a few microseconds. We demonstrate that anchor points in the above applications can be added/removed and moved around on the meshes at interactive framerates, giving the user an immediate result as feedback.	Weighted averages on surfaces	NA:NA:NA:NA	2018
Keenan Crane:Ulrich Pinkall:Peter Schröder	We present a formulation of Willmore flow for triangulated surfaces that permits extraordinarily large time steps and naturally preserves the quality of the input mesh. The main insight is that Willmore flow becomes remarkably stable when expressed in curvature space -- we develop the precise conditions under which curvature is allowed to evolve. The practical outcome is a highly efficient algorithm that naturally preserves texture and does not require remeshing during the flow. We apply this algorithm to surface fairing, geometric modeling, and construction of constant mean curvature (CMC) surfaces. We also present a new algorithm for length-preserving flow on planar curves, which provides a valuable analogy for the surface case.	Robust fairing via conformal curvature flow	NA:NA:NA	2018
Chris Wojtan	NA	Session details: Fluid grids & meshes	NA	2018
Theodore Kim:John Delaney	We present a new subspace integration method that is capable of efficiently adding and subtracting dynamics from an existing high-resolution fluid simulation. We show how to analyze the results of an existing high-resolution simulation, discover an efficient reduced approximation, and use it to quickly "re-simulate" novel variations of the original dynamics. Prior subspace methods have had difficulty re-simulating the original input dynamics because they lack efficient means of handling semi-Lagrangian advection methods. We show that multi-dimensional cubature schemes can be applied to this and other advection methods, such as MacCormack advection. The remaining pressure and diffusion stages can be written as a single matrix-vector multiply, so as with previous subspace methods, no matrix inversion is needed at runtime. We additionally propose a novel importance sampling-based fitting algorithm that asymptotically accelerates the precomputation stage, and show that the Iterated Orthogonal Projection method can be used to elegantly incorporate moving internal boundaries into a subspace simulation. In addition to efficiently producing variations of the original input, our method can produce novel, abstract fluid motions that we have not seen from any other solver.	Subspace fluid re-simulation	NA:NA	2018
Bo Zhu:Wenlong Lu:Matthew Cong:Byungmoon Kim:Ronald Fedkiw	We present an efficient grid structure that extends a uniform grid to create a significantly larger far-field grid by dynamically extending the cells surrounding a fine uniform grid while still maintaining fine resolution about the regions of interest. The far-field grid preserves almost every computational advantage of uniform grids including cache coherency, regular subdivisions for parallelization, simple data layout, the existence of efficient numerical discretizations and algorithms for solving partial differential equations, etc. This allows fluid simulations to cover large domains that are often infeasible to enclose with sufficient resolution using a uniform grid, while still effectively capturing fine scale details in regions of interest using dynamic adaptivity.	A new grid structure for domain extension	NA:NA:NA:NA:NA	2018
Tamy Boubekeur	NA	Session details: Points	NA	2018
Lei He:Scott Schaefer	We present an algorithm for denoising triangulated models based on L0 minimization. Our method maximizes the flat regions of the model and gradually removes noise while preserving sharp features. As part of this process, we build a discrete differential operator for arbitrary triangle meshes that is robust with respect to degenerate triangulations. We compare our method versus other anisotropic denoising algorithms and demonstrate that our method is more robust and produces good results even in the presence of high noise.	Mesh denoising via L0 minimization	NA:NA	2018
Hui Huang:Shihao Wu:Daniel Cohen-Or:Minglun Gong:Hao Zhang:Guiqing Li:Baoquan Chen	We introduce L1-medial skeleton as a curve skeleton representation for 3D point cloud data. The L1-median is well-known as a robust global center of an arbitrary set of points. We make the key observation that adapting L1-medians locally to a point set representing a 3D shape gives rise to a one-dimensional structure, which can be seen as a localized center of the shape. The primary advantage of our approach is that it does not place strong requirements on the quality of the input point cloud nor on the geometry or topology of the captured shape. We develop a L1-medial skeleton construction algorithm, which can be directly applied to an unoriented raw point scan with significant noise, outliers, and large areas of missing data. We demonstrate L1-medial skeletons extracted from raw scans of a variety of shapes, including those modeling high-genus 3D objects, plant-like structures, and curve networks.	L1-medial skeleton of point cloud	NA:NA:NA:NA:NA:NA:NA	2018
Hui Lin:Jizhou Gao:Yu Zhou:Guiliang Lu:Mao Ye:Chenxi Zhang:Ligang Liu:Ruigang Yang	We present a complete system to semantically decompose and reconstruct 3D models from point clouds. Different than previous urban modeling approaches, our system is designed for residential scenes, which consist of mainly low-rise buildings that do not exhibit the regularity and repetitiveness as high-rise buildings in downtown areas. Our system first automatically labels the input into distinctive categories using supervised learning techniques. Based on the semantic labels, objects in different categories are reconstructed with domain-specific knowledge. In particular, we present a novel building modeling scheme that aims to decompose and fit the building point cloud into basic blocks that are block-wise symmetric and convex. This building representation and its reconstruction algorithm are flexible, efficient, and robust to missing data. We demonstrate the effectiveness of our system on various datasets and compare our building modeling scheme with other state-of-the-art reconstruction algorithms to show its advantage in terms of both quality and speed.	Semantic decomposition and reconstruction of residential scenes from LiDAR data	NA:NA:NA:NA:NA:NA:NA:NA	2018
Andrew Selle	NA	Session details: Voxels & liquids	NA	2018
Michael B. Nielsen:Ole Østerby	Physics based simulation of the dynamics of water spray - water droplets dispersed in air - is a means to increase the visual plausibility of computer graphics modeled phenomena such as waterfalls, water jets and stormy seas. Spray phenomena are frequently encountered by the visual effects industry and often challenge state of the art methods. Current spray simulation pipelines typically employ a combination of Lagrangian (particle) and Eulerian (volumetric) methods - the Eulerian methods being used for parts of the spray where individual droplets are not apparent. However, existing Eulerian methods in computer graphics are based on gas solvers that will for example exhibit hydrostatic equilibrium in certain scenarios where the air is expected to rise and the water droplets fall. To overcome this problem, we propose to simulate spray in the Eulerian domain as a two-way coupled two-continua of air and water phases co-existing at each point in space. The fundamental equations originate in applied physics and we present a number of contributions that make Eulerian two-continua spray simulation feasible for computer graphics applications. The contributions include a Poisson equation that fits into the operator splitting methodology as well as (semi-)implicit discretizations of droplet diffusion and the drag force with improved stability properties. As shown by several examples, our approach allows us to more faithfully capture the dynamics of spray than previous Eulerian methods.	A two-continua approach to Eulerian simulation of water spray	NA:NA	2018
Morten Bojsen-Hansen:Chris Wojtan	Our work concerns the combination of an Eulerian liquid simulation with a high-resolution surface tracker (e.g. the level set method or a Lagrangian triangle mesh). The naive application of a high-resolution surface tracker to a low-resolution velocity field can produce many visually disturbing physical and topological artifacts that limit their use in practice. We address these problems by defining an error function which compares the current state of the surface tracker to the set of physically valid surface states. By reducing this error with a gradient descent technique, we introduce a novel physics-based surface fairing method. Similarly, by treating this error function as a potential energy, we derive a new surface correction force that mimics the vortex sheet equations. We demonstrate our results with both level set and mesh-based surface trackers.	Liquid surface tracking with error compensation	NA:NA	2018
Misha Kazhdan	NA	Session details: Shape analysis	NA	2018
Oliver van Kaick:Kai Xu:Hao Zhang:Yanzhen Wang:Shuyang Sun:Ariel Shamir:Daniel Cohen-Or	We introduce an unsupervised co-hierarchical analysis of a set of shapes, aimed at discovering their hierarchical part structures and revealing relations between geometrically dissimilar yet functionally equivalent shape parts across the set. The core problem is that of representative co-selection. For each shape in the set, one representative hierarchy (tree) is selected from among many possible interpretations of the hierarchical structure of the shape. Collectively, the selected tree representatives maximize the within-cluster structural similarity among them. We develop an iterative algorithm for representative co-selection. At each step, a novel cluster-and-select scheme is applied to a set of candidate trees for all the shapes. The tree-to-tree distance for clustering caters to structural shape analysis by focusing on spatial arrangement of shape parts, rather than their geometric details. The final set of representative trees are unified to form a structural co-hierarchy. We demonstrate co-hierarchical analysis on families of man-made shapes exhibiting high degrees of geometric and finer-scale structural variabilities.	Co-hierarchical analysis of shape structures	NA:NA:NA:NA:NA:NA:NA	2018
Vladimir G. Kim:Wilmot Li:Niloy J. Mitra:Siddhartha Chaudhuri:Stephen DiVerdi:Thomas Funkhouser	As large repositories of 3D shape collections continue to grow, understanding the data, especially encoding the inter-model similarity and their variations, is of central importance. For example, many data-driven approaches now rely on access to semantic segmentation information, accurate inter-model point-to-point correspondence, and deformation models that characterize the model collections. Existing approaches, however, are either supervised requiring manual labeling; or employ super-linear matching algorithms and thus are unsuited for analyzing large collections spanning many thousands of models. We propose an automatic algorithm that starts with an initial template model and then jointly optimizes for part segmentation, point-to-point surface correspondence, and a compact deformation model to best explain the input model collection. As output, the algorithm produces a set of probabilistic part-based templates that groups the original models into clusters of models capturing their styles and variations. We evaluate our algorithm on several standard datasets and demonstrate its scalability by analyzing much larger collections of up to thousands of shapes.	Learning part-based templates from large collections of 3D shapes	NA:NA:NA:NA:NA:NA	2018
Shi-Sheng Huang:Ariel Shamir:Chao-Hui Shen:Hao Zhang:Alla Sheffer:Shi-Min Hu:Daniel Cohen-Or	We present a method for organizing a heterogeneous collection of 3D shapes for overview and exploration. Instead of relying on quantitative distances, which may become unreliable between dissimilar shapes, we introduce a qualitative analysis which utilizes multiple distance measures but only in cases where the measures can be reliably compared. Our analysis is based on the notion of quartets, each defined by two pairs of shapes, where the shapes in each pair are close to each other, but far apart from the shapes in the other pair. Combining the information from many quartets computed across a shape collection using several distance measures, we create a hierarchical structure we call categorization tree of the shape collection. This tree satisfies the topological (qualitative) constraints imposed by the quartets creating an effective organization of the shapes. We present categorization trees computed on various collections of shapes and compare them to ground truth data from human categorization. We further introduce the concept of degree of separation chart for every shape in the collection and show the effectiveness of using it for interactive shapes exploration.	Qualitative organization of collections of shapes via quartet analysis	NA:NA:NA:NA:NA:NA:NA	2018
Raif M. Rustamov:Maks Ovsjanikov:Omri Azencot:Mirela Ben-Chen:Frédéric Chazal:Leonidas Guibas	We develop a novel formulation for the notion of shape differences, aimed at providing detailed information about the location and nature of the differences or distortions between the two shapes being compared. Our difference operator, derived from a shape map, is much more informative than just a scalar global shape similarity score, rendering it useful in a variety of applications where more refined shape comparisons are necessary. The approach is intrinsic and is based on a linear algebraic framework, allowing the use of many common linear algebra tools (e.g, SVD, PCA) for studying a matrix representation of the operator. Remarkably, the formulation allows us not only to localize shape differences on the shapes involved, but also to compare shape differences across pairs of shapes, and to analyze the variability in entire shape collections based on the differences between the shapes. Moreover, while we use a map or correspondence to define each shape difference, consistent correspondences between the shapes are not necessary for comparing shape differences, although they can be exploited if available. We give a number of applications of shape differences, including parameterizing the intrinsic variability in a shape collection, exploring shape collections using local variability at different scales, performing shape analogies, and aligning shape collections.	Map-based exploration of intrinsic shape differences and variability	NA:NA:NA:NA:NA:NA	2018
Wojciech Matusik	NA	Session details: Image-based reconstruction	NA	2018
Changil Kim:Henning Zimmer:Yael Pritch:Alexander Sorkine-Hornung:Markus Gross	This paper describes a method for scene reconstruction of complex, detailed environments from 3D light fields. Densely sampled light fields in the order of 109 light rays allow us to capture the real world in unparalleled detail, but efficiently processing this amount of data to generate an equally detailed reconstruction represents a significant challenge to existing algorithms. We propose an algorithm that leverages coherence in massive light fields by breaking with a number of established practices in image-based reconstruction. Our algorithm first computes reliable depth estimates specifically around object boundaries instead of interior regions, by operating on individual light rays instead of image patches. More homogeneous interior regions are then processed in a fine-to-coarse procedure rather than the standard coarse-to-fine approaches. At no point in our method is any form of global optimization performed. This allows our algorithm to retain precise object contours while still ensuring smooth reconstructions in less detailed areas. While the core reconstruction method handles general unstructured input, we also introduce a sparse representation and a propagation scheme for reliable depth estimates which make our algorithm particularly effective for 3D input, enabling fast and memory efficient processing of "Gigaray light fields" on a standard GPU. We show dense 3D reconstructions of highly detailed scenes, enabling applications such as automatic segmentation and image-based rendering, and provide an extensive evaluation and comparison to existing image-based reconstruction techniques.	Scene reconstruction from high spatio-angular resolution light fields	NA:NA:NA:NA:NA	2018
Derek Bradley:Derek Nowrouzezahrai:Paul Beardsley	Flora is an element in many computer-generated scenes. But trees, bushes and plants have complex geometry and appearance, and are difficult to model manually. One way to address this is to capture models directly from the real world. Existing techniques have focused on extracting macro structure such as the branching structure of trees, or the structure of broad-leaved plants with a relatively small number of surfaces. This paper presents a finer scale technique to demonstrate for the first time the processing of densely leaved foliage - computation of 3D structure, plus extraction of statistics for leaf shape and the configuration of neighboring leaves. Our method starts with a mesh of a single exemplar leaf of the target foliage. Using a small number of images, point cloud data is obtained from multi-view stereo, and the exemplar leaf mesh is fitted non-rigidly to the point cloud over several iterations. In addition, our method learns a statistical model of leaf shape and appearance during the reconstruction phase, and a model of the transformations between neighboring leaves. This information is useful in two ways - to augment and increase leaf density in reconstructions of captured foliage, and to synthesize new foliage that conforms to a user-specified layout and density. The result of our technique is a dense set of captured leaves with realistic appearance, and a method for leaf synthesis. Our approach excels at reconstructing plants and bushes that are primarily defined by dense leaves and is demonstrated with multiple examples.	Image-based reconstruction and synthesis of dense foliage	NA:NA:NA	2018
Menglei Chai:Lvdi Wang:Yanlin Weng:Xiaogang Jin:Kun Zhou	This paper presents a single-view hair modeling technique for generating visually and physically plausible 3D hair models with modest user interaction. By solving an unambiguous 3D vector field explicitly from the image and adopting an iterative hair generation algorithm, we can create hair models that not only visually match the original input very well but also possess physical plausibility (e.g., having strand roots fixed on the scalp and preserving the length and continuity of real strands in the image as much as possible). The latter property enables us to manipulate hair in many new ways that were previously very difficult with a single image, such as dynamic simulation or interactive hair shape editing. We further extend the modeling approach to handle simple video input, and generate dynamic 3D hair models. This allows users to manipulate hair in a video or transfer styles from images to videos.	Dynamic hair manipulation in images and videos	NA:NA:NA:NA:NA	2018
Linjie Luo:Hao Li:Szymon Rusinkiewicz	Existing hair capture systems fail to produce strands that reflect the structures of real-world hairstyles. We introduce a system that reconstructs coherent and plausible wisps aware of the underlying hair structures from a set of still images without any special lighting. Our system first discovers locally coherent wisp structures in the reconstructed point cloud and the 3D orientation field, and then uses a novel graph data structure to reason about both the connectivity and directions of the local wisp structures in a global optimization. The wisps are then completed and used to synthesize hair strands which are robust against occlusion and missing data and plausible for animation and simulation. We show reconstruction results for a variety of complex hairstyles including curly, wispy, and messy hair.	Structure-aware hair capture	NA:NA:NA	2018
Eli Shechtman	NA	Session details: Video & warping	NA	2018
Zicheng Liao:Neel Joshi:Hugues Hoppe	Given a short video we create a representation that captures a spectrum of looping videos with varying levels of dynamism, ranging from a static image to a highly animated loop. In such a progressively dynamic video, scene liveliness can be adjusted interactively using a slider control. Applications include background images and slideshows, where the desired level of activity may depend on personal taste or mood. The representation also provides a segmentation of the scene into independently looping regions, enabling interactive local adjustment over dynamism. For a landscape scene, this control might correspond to selective animation and deanimation of grass motion, water ripples, and swaying trees. Converting arbitrary video to looping content is a challenging research problem. Unlike prior work, we explore an optimization in which each pixel automatically determines its own looping period. The resulting nested segmentation of static and dynamic scene regions forms an extremely compact representation.	Automated video looping with progressive dynamism	NA:NA:NA	2018
Shuaicheng Liu:Lu Yuan:Ping Tan:Jian Sun	We present a novel video stabilization method which models camera motion with a bundle of (multiple) camera paths. The proposed model is based on a mesh-based, spatially-variant motion representation and an adaptive, space-time path optimization. Our motion representation allows us to fundamentally handle parallax and rolling shutter effects while it does not require long feature trajectories or sparse 3D reconstruction. We introduce the 'as-similar-as-possible' idea to make motion estimation more robust. Our space-time path smoothing adaptively adjusts smoothness strength by considering discontinuities, cropping size and geometrical distortion in a unified optimization framework. The evaluation on a large variety of consumer videos demonstrates the merits of our method.	Bundled camera paths for video stabilization	NA:NA:NA:NA	2018
Kaiming He:Huiwen Chang:Jian Sun	Stitched panoramic images mostly have irregular boundaries. Artists and common users generally prefer rectangular boundaries, which can be obtained through cropping or image completion techniques. In this paper, we present a content-aware warping algorithm that generates rectangular images from stitched panoramic images. Our algorithm consists of two steps. The first local step is mesh-free and preliminarily warps the image into a rectangle. With a grid mesh placed on this rectangle, the second global step optimizes the mesh to preserve shapes and straight lines. In various experiments we demonstrate that the results of our approach are often visually plausible, and the introduced distortion is often unnoticeable.	Rectangling panoramic images via warping	NA:NA:NA	2018
Neal Wadhwa:Michael Rubinstein:Frédo Durand:William T. Freeman	We introduce a technique to manipulate small movements in videos based on an analysis of motion in complex-valued image pyramids. Phase variations of the coefficients of a complex-valued steerable pyramid over time correspond to motion, and can be temporally processed and amplified to reveal imperceptible motions, or attenuated to remove distracting changes. This processing does not involve the computation of optical flow, and in comparison to the previous Eulerian Video Magnification method it supports larger amplification factors and is significantly less sensitive to noise. These improved capabilities broaden the set of applications for motion processing in videos. We demonstrate the advantages of this approach on synthetic and natural video sequences, and explore applications in scientific analysis, visualization and video enhancement.	Phase-based video motion processing	NA:NA:NA:NA	2018
Jehee Lee	NA	Session details: Design & authoring	NA	2018
Romain Prévost:Emily Whiting:Sylvain Lefebvre:Olga Sorkine-Hornung	Imbalance suggests a feeling of dynamism and movement in static objects. It is therefore not surprising that many 3D models stand in impossibly balanced configurations. As long as the models remain in a computer this is of no consequence: the laws of physics do not apply. However, fabrication through 3D printing breaks the illusion: printed models topple instead of standing as initially intended. We propose to assist users in producing novel, properly balanced designs by interactively deforming an existing model. We formulate balance optimization as an energy minimization, improving stability by modifying the volume of the object, while preserving its surface details. This takes place during interactive editing: the user cooperates with our optimizer towards the end result. We demonstrate our method on a variety of models. With our technique, users can produce fabricated objects that stand in one or more surprising poses without requiring glue or heavy pedestals.	Make it stand: balancing shapes for 3D fabrication	NA:NA:NA:NA	2018
Mélina Skouras:Bernhard Thomaszewski:Stelian Coros:Bernd Bickel:Markus Gross	We present a method for fabrication-oriented design of actuated deformable characters that allows a user to automatically create physical replicas of digitally designed characters using rapid manufacturing technologies. Given a deformable character and a set of target poses as input, our method computes a small set of actuators along with their locations on the surface and optimizes the internal material distribution such that the resulting character exhibits the desired deformation behavior. We approach this problem with a dedicated algorithm that combines finite-element analysis, sparse regularization, and constrained optimization. We validate our pipeline on a set of two- and three-dimensional example characters and present results in simulation and physically-fabricated prototypes.	Computational design of actuated deformable characters	NA:NA:NA:NA:NA	2018
Stelian Coros:Bernhard Thomaszewski:Gioacchino Noris:Shinjiro Sueda:Moira Forberg:Robert W. Sumner:Wojciech Matusik:Bernd Bickel	We present an interactive design system that allows non-expert users to create animated mechanical characters. Given an articulated character as input, the user iteratively creates an animation by sketching motion curves indicating how different parts of the character should move. For each motion curve, our framework creates an optimized mechanism that reproduces it as closely as possible. The resulting mechanisms are attached to the character and then connected to each other using gear trains, which are created in a semi-automated fashion. The mechanical assemblies generated with our system can be driven with a single input driver, such as a hand-operated crank or an electric motor, and they can be fabricated using rapid prototyping devices. We demonstrate the versatility of our approach by designing a wide range of mechanical characters, several of which we manufactured using 3D printing. While our pipeline is designed for characters driven by planar mechanisms, significant parts of it extend directly to non-planar mechanisms, allowing us to create characters with compelling 3D motions.	Computational design of mechanical characters	NA:NA:NA:NA:NA:NA:NA:NA	2018
Yili Zhao:Jernej Barbič	Physically based simulation can produce quality motion of plants, but requires an authoring stage to convert plant "polygon soup" triangle meshes to a format suitable for physically based simulation. We give a system that can author complex simulation-ready plants in a manner of minutes. Our system decomposes the plant geometry, establishes a hierarchy, builds and connects simulation meshes, and detects instances. It scales to anatomically realistic geometry of adult plants, is robust to non-manifold input geometry, gaps between branches or leaves, free-flying leaves not connected to any branch, spurious geometry, and plant self-collisions in the input configuration. We demonstrate the results using a FEM model reduction simulator that can compute large-deformation dynamics of complex plants at interactive rates, subject to user forces, gravity or randomized wind. We also provide plant fracture (with pre-specified patterns), inverse kinematics to easily pose plants, as well as interactive design of plant material properties. We authored and simulated over 100 plants from diverse climates and geographic regions, including broadleaf (deciduous) trees and conifers, bushes and flowers. Our largest simulations involve anatomically realistic adult trees with hundreds of branches and over 100,000 leaves.	Interactive authoring of simulation-ready plants	NA:NA	2018
Floraine Berthouzoz:Akash Garg:Danny M. Kaufman:Eitan Grinspun:Maneesh Agrawala	We present techniques for automatically parsing existing sewing patterns and converting them into 3D garment models. Our parser takes a sewing pattern in PDF format as input and starts by extracting the set of panels and styling elements (e.g. darts, pleats and hemlines) contained in the pattern. It then applies a combination of machine learning and integer programming to infer how the panels must be stitched together to form the garment. Our system includes an interactive garment simulator that takes the parsed result and generates the corresponding 3D model. Our fully automatic approach correctly parses 68% of the sewing patterns in our collection. Most of the remaining patterns contain only a few errors that can be quickly corrected within the garment simulator. Finally we present two applications that take advantage of our collection of parsed sewing patterns. Our garment hybrids application lets users smoothly interpolate multiple garments in the 2D space of patterns. Our sketch-based search application allows users to navigate the pattern collection by drawing the shape of panels.	Parsing sewing patterns into 3D garments	NA:NA:NA:NA:NA	2018
Jinxiang Chai	NA	Session details: Data-driven animation	NA	2018
Matt Stanton:Yu Sheng:Martin Wicke:Federico Perazzi:Amos Yuen:Srinivasa Narasimhan:Adrien Treuille	This paper extends Galerkin projection to a large class of non-polynomial functions typically encountered in graphics. We demonstrate the broad applicability of our approach by applying it to two strikingly different problems: fluid simulation and radiosity rendering, both using deforming meshes. Standard Galerkin projection cannot efficiently approximate these phenomena. Our approach, by contrast, enables the compact representation and approximation of these complex non-polynomial systems, including quotients and roots of polynomials. We rely on representing each function to be model-reduced as a composition of tensor products, matrix inversions, and matrix roots. Once a function has been represented in this form, it can be easily model-reduced, and its reduced form can be evaluated with time and memory costs dependent only on the dimension of the reduced space.	Non-polynomial Galerkin projection on deforming meshes	NA:NA:NA:NA:NA:NA:NA	2018
Doyub Kim:Woojong Koh:Rahul Narain:Kayvon Fatahalian:Adrien Treuille:James F. O'Brien	The central argument against data-driven methods in computer graphics rests on the curse of dimensionality: it is intractable to precompute "everything" about a complex space. In this paper, we challenge that assumption by using several thousand CPU-hours to perform a massive exploration of the space of secondary clothing effects on a character animated through a large motion graph. Our system continually explores the phase space of cloth dynamics, incrementally constructing a secondary cloth motion graph that captures the dynamics of the system. We find that it is possible to sample the dynamical space to a low visual error tolerance and that secondary motion graphs containing tens of gigabytes of raw mesh data can be compressed down to only tens of megabytes. These results allow us to capture the effect of high-resolution, off-line cloth simulation for a rich space of character motion and deliver it efficiently as part of an interactive application.	Near-exhaustive precomputation of secondary cloth effects	NA:NA:NA:NA:NA:NA	2018
Zhili Chen:Renguo Feng:Huamin Wang	Real-world cloth exhibits complex behaviors when it contacts deformable bodies. In this paper, we study how to improve the simulation of cloth-body interactions from three perspectives: collision, friction, and air pressure. We propose an efficient and robust algorithm to detect the collisions between cloth and deformable bodies, using the surface traversal technique. We develop a friction measurement device and we use it to capture frictional data from real-world experiments. The derived friction model can realistically handle complex friction properties of cloth, including anisotropy and nonlinearity. To produce pressure effects caused by the air between cloth and deformable bodies, we define an air mass field on the cloth layer and we use real-world air permeability data to animate it over time. Our results demonstrate the efficiency and accuracy of our system in simulating objects with a three-layer structure (i.e., a cloth layer, an air layer, and an inner body layer), such as pillows, comforters, down jackets, and stuffed toys.	Modeling friction and air effects between cloth and deformable bodies	NA:NA:NA	2018
David Wilkie:Jason Sewall:Ming Lin	'Virtualized traffic' reconstructs and displays continuous traffic flows from discrete spatio-temporal traffic sensor data or procedurally generated control input to enhance a sense of immersion in a dynamic virtual environment. In this paper, we introduce a fast technique to reconstruct traffic flows from in-road sensor measurements or procedurally generated data for interactive 3D visual applications. Our algorithm estimates the full state of the traffic flow from sparse sensor measurements (or procedural input) using a statistical inference method and a continuum traffic model. This estimated state then drives an agent-based traffic simulator to produce a 3D animation of vehicle traffic that statistically matches the original traffic conditions. Unlike existing traffic simulation and animation techniques, our method produces a full 3D rendering of individual vehicles as part of continuous traffic flows given discrete spatio-temporal sensor measurements. Instead of using a color map to indicate traffic conditions, users could visualize and fly over the reconstructed traffic in real time over a large digital cityscape.	Flow reconstruction for data-driven traffic animation	NA:NA:NA	2018
Chongyang Ma:Li-Yi Wei:Sylvain Lefebvre:Xin Tong	Many natural phenomena consist of geometric elements with dynamic motions characterized by small scale repetitions over large scale structures, such as particles, herds, threads, and sheets. Due to their ubiquity, controlling the appearance and behavior of such phenomena is important for a variety of graphics applications. However, such control is often challenging; the repetitive elements are often too numerous for manual edit, while their overall structures are often too versatile for fully automatic computation. We propose a method that facilitates easy and intuitive controls at both scales: high-level structures through spatial-temporal output constraints (e.g. overall shape and motion of the output domain), and low-level details through small input exemplars (e.g. element arrangements and movements). These controls are suitable for manual specification, while the corresponding geometric and dynamic repetitions are suitable for automatic computation. Our system takes such user controls as inputs, and generates as outputs the corresponding repetitions satisfying the controls. Our method, which we call dynamic element textures, aims to produce such controllable repetitions through a combination of constrained optimization (satisfying controls) and data driven computation (synthesizing details). We use spatial-temporal samples as the core representation for dynamic geometric elements. We propose analysis algorithms for decomposing small scale repetitions from large scale themes, as well as synthesis algorithms for generating outputs satisfying user controls. Our method is general, producing a range of artistic effects that previously required disparate and specialized techniques.	Dynamic element textures	NA:NA:NA:NA	2018
Bedřich Beneš	NA	Session details: Building structures & layouts	NA	2018
Daniele Panozzo:Philippe Block:Olga Sorkine-Hornung	We present a complete design pipeline that allows non-expert users to design and analyze masonry structures without any structural knowledge. We optimize the force layouts both geometrically and topologically, finding a self-supported structure that is as close as possible to a given target surface. The generated structures are tessellated into hexagonal blocks with a pattern that prevents sliding failure. The models can be used in physically plausible virtual environments or 3D printed and assembled without reinforcements.	Designing unreinforced masonry models	NA:NA:NA	2018
Yang Liu:Hao Pan:John Snyder:Wenping Wang:Baining Guo	Masonry structures must be compressively self-supporting; designing such surfaces forms an important topic in architecture as well as a challenging problem in geometric modeling. Under certain conditions, a surjective mapping exists between a power diagram, defined by a set of 2D vertices and associated weights, and the reciprocal diagram that characterizes the force diagram of a discrete self-supporting network. This observation lets us define a new and convenient parameterization for the space of self-supporting networks. Based on it and the discrete geometry of this design space, we present novel geometry processing methods including surface smoothing and remeshing which significantly reduce the magnitude of force densities and homogenize their distribution.	Computing self-supporting surfaces by regular triangulation	NA:NA:NA:NA:NA	2018
Fernando de Goes:Pierre Alliez:Houman Owhadi:Mathieu Desbrun	We present a novel approach for the analysis and design of self-supporting simplicial masonry structures. A finite-dimensional formulation of their compressive stress field is derived, offering a new interpretation of thrust networks through numerical homogenization theory. We further leverage geometric properties of the resulting force diagram to identify a set of reduced coordinates characterizing the equilibrium of simplicial masonry. We finally derive computational form-finding tools that improve over previous work in efficiency, accuracy, and scalability.	On the equilibrium of simplicial masonry structures	NA:NA:NA:NA	2018
Peng Song:Chi-Wing Fu:Prashant Goswami:Jianmin Zheng:Niloy J. Mitra:Daniel Cohen-Or	A reciprocal frame (RF) is a self-supported three-dimensional structure made up of three or more sloping rods, which form a closed circuit, namely an RF-unit. Large RF-structures built as complex grillages of one or a few similar RF-units have an intrinsic beauty derived from their inherent self-similar and highly symmetric patterns. Designing RF-structures that span over large domains is an intricate and complex task. In this paper, we present an interactive computational tool for designing RF-structures over a 3D guiding surface, focusing on the aesthetic aspect of the design. There are three key contributions in this work. First, we draw an analogy between RF-structures and plane tiling with regular polygons, and develop a computational scheme to generate coherent RF-tessellations from simple grammar rules. Second, we employ a conformal mapping to lift the 2D tessellation over a 3D guiding surface, allowing a real-time preview and efficient exploration of wide ranges of RF design parameters. Third, we devise an optimization method to guarantee the collinearity of contact joints along each rod, while preserving the geometric properties of the RF-structure. Our tool not only supports the design of wide variety of RF pattern classes and their variations, but also allows preview and refinement through interactive controls.	Reciprocal frame structures made easy	NA:NA:NA:NA:NA:NA	2018
Steve Marschner	NA	Session details: Global illumination	NA	2018
Jaakko Lehtinen:Tero Karras:Samuli Laine:Miika Aittala:Frédo Durand:Timo Aila	We introduce a novel Metropolis rendering algorithm that directly computes image gradients, and reconstructs the final image from the gradients by solving a Poisson equation. The reconstruction is aided by a low-fidelity approximation of the image computed during gradient sampling. As an extension of path-space Metropolis light transport, our algorithm is well suited for difficult transport scenarios. We demonstrate that our method outperforms the state-of-the-art in several well-known test scenes. Additionally, we analyze the spectral properties of gradient-domain sampling, and compare it to the traditional image-domain sampling.	Gradient-domain metropolis light transport	NA:NA:NA:NA:NA:NA	2018
Soham Uday Mehta:Brandon Wang:Ravi Ramamoorthi:Fredo Durand	We introduce an algorithm for interactive rendering of physically-based global illumination, based on a novel frequency analysis of indirect lighting. Our method combines adaptive sampling by Monte Carlo ray or path tracing, using a standard GPU-accelerated raytracer, with real-time reconstruction of the resulting noisy images. Our theoretical analysis assumes diffuse indirect lighting, with general Lambertian and specular receivers. In practice, we demonstrate accurate interactive global illumination with diffuse and moderately glossy objects, at 1-3 fps. We show mathematically that indirect illumination is a structured signal in the Fourier domain, with inherent band-limiting due to the BRDF and geometry terms. We extend previous work on sheared and axis-aligned filtering for motion blur and shadows, to develop an image-space filtering method for interreflections. Our method enables 5--8X reduced sampling rates and wall clock times, and converges to ground truth as more samples are added. To develop our theory, we overcome important technical challenges---unlike previous work, there is no light source to serve as a band-limit in indirect lighting, and we also consider non-parallel geometry of receiver and reflecting surfaces, without first-order approximations.	Axis-aligned filtering for interactive physically-based diffuse indirect lighting	NA:NA:NA:NA	2018
Denis Zorin	NA	Session details: Quads & meshing	NA	2018
Kenshi Takayama:Daniele Panozzo:Alexander Sorkine-Hornung:Olga Sorkine-Hornung	Coarse quad meshes are the preferred representation for animating characters in movies and video games. In these scenarios, artists want explicit control over the edge flows and the singularities of the quad mesh. Despite the significant advances in recent years, existing automatic quad remeshing algorithms are not yet able to achieve the quality of manually created remeshings. We present an interactive system for manual quad remeshing that provides the user with a high degree of control while avoiding the tediousness involved in existing manual tools. With our sketch-based interface the user constructs a quad mesh by defining patches consisting of individual quads. The desired edge flow is intuitively specified by the sketched patch boundaries, and the mesh topology can be adjusted by varying the number of edge subdivisions at patch boundaries. Our system automatically inserts singularities inside patches if necessary, while providing the user with direct control of their topological and geometrical locations. We developed a set of novel user interfaces that assist the user in constructing a curve network representing such patch boundaries. The effectiveness of our system is demonstrated through a user evaluation with professional artists. Our system is also useful for editing automatically generated quad meshes.	Sketch-based generation and editing of quad meshes	NA:NA:NA:NA	2018
David Bommes:Marcel Campen:Hans-Christian Ebke:Pierre Alliez:Leif Kobbelt	Quadrilateral remeshing approaches based on global parametrization enable many desirable mesh properties. Two of the most important ones are (1) high regularity due to explicit control over irregular vertices and (2) smooth distribution of distortion achieved by convex variational formulations. Apart from these strengths, state-of-the-art techniques suffer from limited reliability on real-world input data, i.e. the determined map might have degeneracies like (local) non-injectivities and consequently often cannot be used directly to generate a quadrilateral mesh. In this paper we propose a novel convex Mixed-Integer Quadratic Programming (MIQP) formulation which ensures by construction that the resulting map is within the class of so called Integer-Grid Maps that are guaranteed to imply a quad mesh. In order to overcome the NP-hardness of MIQP and to be able to remesh typical input geometries in acceptable time we propose two additional problem specific optimizations: a complexity reduction algorithm and singularity separating conditions. While the former decouples the dimension of the MIQP search space from the input complexity of the triangle mesh and thus is able to dramatically speed up the computation without inducing inaccuracies, the latter improves the continuous relaxation, which is crucial for the success of modern MIQP optimizers. Our experiments show that the reliability of the resulting algorithm does not only annihilate the main drawback of parametrization based quad-remeshing but moreover enables the global search for high-quality coarse quad layouts - a difficult task solely tackled by greedy methodologies before.	Integer-grid maps for reliable quad meshing	NA:NA:NA:NA:NA	2018
Zichun Zhong:Xiaohu Guo:Wenping Wang:Bruno Lévy:Feng Sun:Yang Liu:Weihua Mao	This paper introduces a particle-based approach for anisotropic surface meshing. Given an input polygonal mesh endowed with a Riemannian metric and a specified number of vertices, the method generates a metric-adapted mesh. The main idea consists of mapping the anisotropic space into a higher dimensional isotropic one, called "embedding space". The vertices of the mesh are generated by uniformly sampling the surface in this higher dimensional embedding space, and the sampling is further regularized by optimizing an energy function with a quasi-Newton algorithm. All the computations can be re-expressed in terms of the dot product in the embedding space, and the Jacobian matrices of the mappings that connect different spaces. This transform makes it unnecessary to explicitly represent the coordinates in the embedding space, and also provides all necessary expressions of energy and forces for efficient computations. Through energy optimization, it naturally leads to the desired anisotropic particle distributions in the original space. The triangles are then generated by computing the Restricted Anisotropic Voronoi Diagram and its dual Delaunay triangulation. We compare our results qualitatively and quantitatively with the state-of-the-art in anisotropic surface meshing on several examples, using the standard measurement criteria.	Particle-based anisotropic surface meshing	NA:NA:NA:NA:NA:NA:NA	2018
Holly Rushmeier	NA	Session details: Advanced rendering	NA	2018
Rasmus Barringer:Tomas Akenine-Möller	Edge aliasing continues to be one of the most prominent problems in real-time graphics, e.g., in games. We present a novel algorithm that uses shared memory between the GPU and the CPU so that these two units can work in concert to solve the edge aliasing problem rapidly. Our system renders the scene as usual on the GPU with one sample per pixel. At the same time, our novel edge aliasing algorithm is executed asynchronously on the CPU. First, a sparse set of important pixels is created. This set may include pixels with geometric silhouette edges, discontinuities in the frame buffer, and pixels/polygons under user-guided artistic control. After that, the CPU runs our sparse rasterizer and fragment shader, which is parallel and SIMD:ified, and directly accesses shared resources (e.g., render targets created by the GPU). Our system can render a scene with shadow mapping with adaptive anti-aliasing with 16 samples per important pixel faster than the GPU with 8 samples per pixel using multi-sampling anti-aliasing. Since our system consists of an extensive code base, it will be released to the public for exploration and usage.	A4: asynchronous adaptive anti-aliasing using shared memory	NA:NA	2018
Viktor Kämpe:Erik Sintorn:Ulf Assarsson	We show that a binary voxel grid can be represented orders of magnitude more efficiently than using a sparse voxel octree (SVO) by generalising the tree to a directed acyclic graph (DAG). While the SVO allows for efficient encoding of empty regions of space, the DAG additionally allows for efficient encoding of identical regions of space, as nodes are allowed to share pointers to identical subtrees. We present an efficient bottom-up algorithm that reduces an SVO to a minimal DAG, which can be applied even in cases where the complete SVO would not fit in memory. In all tested scenes, even the highly irregular ones, the number of nodes is reduced by one to three orders of magnitude. While the DAG requires more pointers per node, the memory cost for these is quickly amortized and the memory consumption of the DAG is considerably smaller, even when compared to an ideal SVO without pointers. Meanwhile, our sparse voxel DAG requires no decompression and can be traversed very efficiently. We demonstrate this by ray tracing hard and soft shadows, ambient occlusion, and primary rays in extremely high resolution DAGs at speeds that are on par with, or even faster than, state-of-the-art voxel and triangle GPU ray tracing.	High resolution sparse voxel DAGs	NA:NA:NA	2018
Robert Bridson	NA	Session details: Water & snow with particles	NA	2018
Alexey Stomakhin:Craig Schroeder:Lawrence Chai:Joseph Teran:Andrew Selle	Snow is a challenging natural phenomenon to visually simulate. While the graphics community has previously considered accumulation and rendering of snow, animation of snow dynamics has not been fully addressed. Additionally, existing techniques for solids and fluids have difficulty producing convincing snow results. Specifically, wet or dense snow that has both solid- and fluid-like properties is difficult to handle. Consequently, this paper presents a novel snow simulation method utilizing a user-controllable elasto-plastic constitutive model integrated with a hybrid Eulerian/Lagrangian Material Point Method. The method is continuum based and its hybrid nature allows us to use a regular Cartesian grid to automate treatment of self-collision and fracture. It also naturally allows us to derive a grid-based semi-implicit integration scheme that has conditioning independent of the number of Lagrangian particles. We demonstrate the power of our method with a variety of snow phenomena including complex character interactions.	A material point method for snow simulation	NA:NA:NA:NA:NA	2018
Ryoichi Ando:Nils Thürey:Chris Wojtan	We introduce a new method for efficiently simulating liquid with extreme amounts of spatial adaptivity. Our method combines several key components to drastically speed up the simulation of large-scale fluid phenomena: We leverage an alternative Eulerian tetrahedral mesh discretization to significantly reduce the complexity of the pressure solve while increasing the robustness with respect to element quality and removing the possibility of locking. Next, we enable subtle free-surface phenomena by deriving novel second-order boundary conditions consistent with our discretization. We couple this discretization with a spatially adaptive Fluid-Implicit Particle (FLIP) method, enabling efficient, robust, minimally-dissipative simulations that can undergo sharp changes in spatial resolution while minimizing artifacts. Along the way, we provide a new method for generating a smooth and detailed surface from a set of particles with variable sizes. Finally, we explore several new sizing functions for determining spatially adaptive simulation resolutions, and we show how to couple them to our simulator. We combine each of these elements to produce a simulation algorithm that is capable of creating animations at high maximum resolutions while avoiding common pitfalls like inaccurate boundary conditions and inefficient computation.	Highly adaptive liquid simulations on tetrahedral meshes	NA:NA:NA	2018
Miles Macklin:Matthias Müller	In fluid simulation, enforcing incompressibility is crucial for realism; it is also computationally expensive. Recent work has improved efficiency, but still requires time-steps that are impractical for real-time applications. In this work we present an iterative density solver integrated into the Position Based Dynamics framework (PBD). By formulating and solving a set of positional constraints that enforce constant density, our method allows similar incompressibility and convergence to modern smoothed particle hydro-dynamic (SPH) solvers, but inherits the stability of the geometric, position based dynamics method, allowing large time steps suitable for real-time applications. We incorporate an artificial pressure term that improves particle distribution, creates surface tension, and lowers the neighborhood requirements of traditional SPH. Finally, we address the issue of energy loss by applying vorticity confinement as a velocity post process.	Position based fluids	NA:NA	2018
Ilya Baran	NA	Session details: Deformation & distortion	NA	2018
Ashish Myles:Denis Zorin	The quality of a global parametrization is determined by a number of factors, including amount of distortion, number of singularities (cones), and alignment with features and boundaries. Placement of cones plays a decisive role in determining the overall distortion of the parametrization; at the same time, feature and boundary alignment also affect the cone placement. A number of methods were proposed for automatic choice of cone positions, either based on singularities of cross-fields and emphasizing alignment, or based on distortion optimization. In this paper we describe a method for placing cones for seamless global parametrizations with alignment constraints. We use a close relation between variation-minimizing cross-fields and related 1-forms and conformal maps, and demonstrate how it leads to a constrained optimization problem formulation. We show for boundary-aligned parametrizations metric distortion may be reduced by cone chains, sometimes to an arbitrarily small value, and the trade-off between the distortion and the number of cones can be controlled by a regularization term. Constrained parametrizations computed using our method have significantly lower distortion compared to the state-of-the art field-based method, yet maintain feature and boundary alignment. In the most extreme cases, parametrization collapse due to alignment constraints is eliminated.	Controlled-distortion constrained global parametrization	NA:NA	2018
Noam Aigerman:Yaron Lipman	We introduce an efficient algorithm for producing provably injective mappings of tetrahedral meshes with strict bounds on their tetrahedra aspect-ratio distortion. The algorithm takes as input a simplicial map (e.g., produced by some common deformation or volumetric parameterization technique) and projects it on the space of injective and bounded-distortion simplicial maps. Namely, finds a similar map that is both bijective and bounded-distortion. As far as we are aware, this is the first algorithm to produce injective or bounded-distortion simplicial maps of tetrahedral meshes. The construction of the algorithm was made possible due to a novel closed-form solution to the problem of finding the closest orientation-preserving bounded-distortion matrix to an arbitrary matrix in three (and higher) dimensions. The algorithm is shown to have quadratic convergence, usually not requiring more than a handful of iterations to converge. Furthermore, it is readily generalized to simplicial maps of any dimension, including mixed dimensions. Finally, it can deal with different distortion spaces, such as bounded isometric distortion. During experiments we found the algorithm useful for producing bijective and bounded-distortion volume parameterizations and deformations of tetrahedral meshes, and improving tetrahedral meshes, increasing the tetrahedra quality produced by state-of-the-art techniques.	Injective and bounded distortion mappings in 3D	NA:NA	2018
David Harmon:Denis Zorin	Subspace techniques greatly reduce the cost of nonlinear simulation by approximating deformations with a small custom basis. In order to represent the deformations well (in terms of a global metric), the basis functions usually have global support, and cannot capture localized deformations. While reduced-space basis functions can be localized to some extent, capturing truly local deformations would still require a very large number of precomputed basis functions, significantly degrading both precomputation and online performance. We present an efficient approach to handling local deformations that cannot be predicted, most commonly arising from contact and collisions, by augmenting the subspace basis with custom functions derived from analytic solutions to static loading problems. We also present a new cubature scheme designed to facilitate fast computation of the necessary runtime quantities while undergoing a changing basis. Our examples yield a two order of magnitude speedup over full-coordinate simulations, striking a desirable balance between runtime speeds and expressive ability.	Subspace integration with local deformations	NA:NA	2018
Renjie Chen:Ofir Weber:Daniel Keren:Mirela Ben-Chen	Planar shape interpolation is widely used in computer graphics applications. Despite a wealth of interpolation methods, there is currently no approach that produces shapes with a bounded amount of distortion with respect to the input. As a result, existing interpolation methods may produce shapes that are significantly different than the input and can suffer from fold-overs and other visual artifacts, making them less useful in many practical scenarios. We introduce a novel shape interpolation scheme designed specifically to produce results with a bounded amount of conformal (angular) distortion. Our method is based on an elegant continuous mathematical formulation and provides several appealing properties such as existence and uniqueness of the solution as well as smoothness in space and time domains. We further present a discretization and an efficient practical algorithm to compute the interpolant and demonstrate its usability and good convergence behavior on a wide variety of input shapes. The method is simple to implement and understand. We compare our method to state-of-the-art interpolation methods and demonstrate its superiority in various cases.	Planar shape interpolation with bounded distortion	NA:NA:NA:NA	2018
Szymon Rusinkiewicz	NA	Session details: Materials	NA	2018
Borom Tunwattanapong:Graham Fyffe:Paul Graham:Jay Busch:Xueming Yu:Abhijeet Ghosh:Paul Debevec	We present a novel technique for acquiring the geometry and spatially-varying reflectance properties of 3D objects by observing them under continuous spherical harmonic illumination conditions. The technique is general enough to characterize either entirely specular or entirely diffuse materials, or any varying combination across the surface of the object. We employ a novel computational illumination setup consisting of a rotating arc of controllable LEDs which sweep out programmable spheres of incident illumination during 1-second exposures. We illuminate the object with a succession of spherical harmonic illumination conditions, as well as photographed environmental lighting for validation. From the response of the object to the harmonics, we can separate diffuse and specular reflections, estimate world-space diffuse and specular normals, and compute anisotropic roughness parameters for each view of the object. We then use the maps of both diffuse and specular reflectance to form correspondences in a multiview stereo algorithm, which allows even highly specular surfaces to be corresponded across views. The algorithm yields a complete 3D model and a set of merged reflectance maps. We use this technique to digitize the shape and reflectance of a variety of objects difficult to acquire with other techniques and present validation renderings which match well to photographs in similar lighting.	Acquiring reflectance and shape from continuous spherical harmonic illumination	NA:NA:NA:NA:NA:NA:NA	2018
Miika Aittala:Tim Weyrich:Jaakko Lehtinen	Spatially-varying reflectance and small geometric variations play a vital role in the appearance of real-world surfaces. Consequently, robust, automatic capture of such models is highly desirable; however, current systems require either specialized hardware, long capture times, user intervention, or rely heavily on heuristics. We describe an acquisition setup that utilizes only portable commodity hardware (an LCD display, an SLR camera) and contains no moving parts. In particular, a laptop screen can be used for illumination. Our setup, aided by a carefully constructed image formation model, automatically produces realistic spatially-varying reflectance parameters over a wide range of materials from diffuse to almost mirror-like specular surfaces, while requiring relatively few photographs. We believe our system is the first to offer such generality, while requiring only standard office equipment and no user intervention or parameter tuning. Our results exhibit a good qualitative match to photographs taken under novel viewing and lighting conditions for a range of materials.	Practical SVBRDF capture in the frequency domain	NA:NA:NA	2018
Sean Bell:Paul Upchurch:Noah Snavely:Kavita Bala	The appearance of surfaces in real-world scenes is determined by the materials, textures, and context in which the surfaces appear. However, the datasets we have for visualizing and modeling rich surface appearance in context, in applications such as home remodeling, are quite limited. To help address this need, we present OpenSurfaces, a rich, labeled database consisting of thousands of examples of surfaces segmented from consumer photographs of interiors, and annotated with material parameters (reflectance, material names), texture information (surface normals, rectified textures), and contextual information (scene category, and object names). Retrieving usable surface information from uncalibrated Internet photo collections is challenging. We use human annotations and present a new methodology for segmenting and annotating materials in Internet photo collections suitable for crowdsourcing (e.g., through Amazon's Mechanical Turk). Because of the noise and variability inherent in Internet photos and novice annotators, designing this annotation engine was a key challenge; we present a multi-stage set of annotation tasks with quality checks and validation. We demonstrate the use of this database in proof-of-concept applications including surface retexturing and material and image browsing, and discuss future uses. OpenSurfaces is a public resource available at http://opensurfaces.cs.cornell.edu/.	OpenSurfaces: a richly annotated catalog of surface appearance	NA:NA:NA:NA	2018
Richard Zhang	NA	Session details: Surface reconstruction	NA	2018
Qian-Yi Zhou:Vladlen Koltun	We present an approach to detailed reconstruction of complex real-world scenes with a handheld commodity range sensor. The user moves the sensor freely through the environment and images the scene. An offline registration and integration pipeline produces a detailed scene model. To deal with the complex sensor trajectories required to produce detailed reconstructions with a consumer-grade sensor, our pipeline detects points of interest in the scene and preserves detailed geometry around them while a global optimization distributes residual registration errors through the environment. Our results demonstrate that detailed reconstructions of complex scenes can be obtained with a consumer-grade camera.	Dense scene reconstruction with points of interest	NA:NA	2018
Jiawen Chen:Dennis Bautembach:Shahram Izadi	We address the fundamental challenge of scalability for real-time volumetric surface reconstruction methods. We design a memory efficient, hierarchical data structure for commodity graphics hardware, which supports live reconstruction of large-scale scenes with fine geometric details. Our sparse data structure fuses overlapping depth maps from a moving depth camera into a single volumetric representation, from which detailed surface models are extracted. Our hierarchy losslessly streams data bidirectionally between GPU and host, allowing for unbounded reconstructions. Our pipeline, comprised of depth map post-processing, camera pose estimation, volumetric fusion, surface extraction, and streaming, runs entirely in real-time. We experimentally demonstrate that a shallow hierarchy with relatively large branching factors yields the best memory/speed tradeoff, consuming an order of magnitude less memory than a regular grid. We compare an implementation of our data structure to existing methods and demonstrate higher-quality reconstructions on a variety of large-scale scenes, all captured in real-time.	Scalable real-time volumetric surface reconstruction	NA:NA:NA	2018
Paul Kry	NA	Session details: Sounds & solids	NA	2018
Sai-Keung Wong:Wen-Chieh Lin:Chun-Hung Hung:Yi-Jheng Huang:Shing-Yeu Lii	We present a novel radial-view-based culling method for continuous self-collision detection (CSCD) of skeletal models. Our method targets closed triangular meshes used to represent the surface of a model. It can be easily integrated with bounding volume hierarchies (BVHs) and used as the first stage for culling non-colliding triangle pairs. A mesh is decomposed into clusters with respect to a set of observer primitives (i.e., observer points and line segments) on the skeleton of the mesh so that each cluster is associated with an observer primitive. One BVH is then built for each cluster. At the runtime stage, a radial view test is performed from the observer primitive of each cluster to check its collision state. Every pair of clusters is also checked for collisions. We evaluated our method on various models and compared its performance with prior methods. Experimental results show that our method reduces the number of the bounding volume overlapping tests and the number of potentially colliding triangle pairs, thereby improving the overall process of CSCD.	Radial view based culling for continuous self-collision detection of skeletal models	NA:NA:NA:NA:NA	2018
Matthias Müller:Nuttapong Chentanez:Tae-Yong Kim	We propose a new fast, robust and controllable method to simulate the dynamic destruction of large and complex objects in real time. The common method for fracture simulation in computer games is to pre-fracture models and replace objects by their pre-computed parts at run-time. This popular method is computationally cheap but has the disadvantages that the fracture pattern does not align with the impact location and that the number of hierarchical fracture levels is fixed. Our method allows dynamic fracturing of large objects into an unlimited number of pieces fast enough to be used in computer games. We represent visual meshes by volumetric approximate convex decompositions (VACD) and apply user-defined fracture patterns dependent on the impact location. The method supports partial fracturing meaning that fracture patterns can be applied locally at multiple locations of an object. We propose new methods for computing a VACD, for approximate convex hull construction and for detecting islands in the convex decomposition after partial destruction in order to determine support structures.	Real time dynamic fracture with volumetric approximate convex decompositions	NA:NA:NA	2018
Wilmot Li	NA	Session details: Artistic rendering & stylization	NA	2018
Michal Lukáč:Jakub Fišer:Jean-Charles Bazin:Ondřej Jamriška:Alexander Sorkine-Hornung:Daniel Sýkora	In this paper we propose a reinterpretation of the brush and the fill tools for digital image painting. The core idea is to provide an intuitive approach that allows users to paint in the visual style of arbitrary example images. Rather than a static library of colors, brushes, or fill patterns, we offer users entire images as their palette, from which they can select arbitrary contours or textures as their brush or fill tool in their own creations. Compared to previous example-based techniques related to the painting-by-numbers paradigm we propose a new strategy where users can generate salient texture boundaries by our randomized graph-traversal algorithm and apply a content-aware fill to transfer textures into the delimited regions. This workflow allows users of our system to intuitively create visually appealing images that better preserve the visual richness and fluidity of arbitrary example images. We demonstrate the potential of our approach in various applications including interactive image creation, editing and vector image stylization.	Painting by feature: texture boundaries for example-based image creation	NA:NA:NA:NA:NA:NA	2018
Jingwan Lu:Connelly Barnes:Stephen DiVerdi:Adam Finkelstein	Conventional digital painting systems rely on procedural rules and physical simulation to render paint strokes. We present an interactive, data-driven painting system that uses scanned images of real natural media to synthesize both new strokes and complex stroke interactions, obviating the need for physical simulation. First, users capture images of real media, including examples of isolated strokes, pairs of overlapping strokes, and smudged strokes. Online, the user inputs a new stroke path, and our system synthesizes its 2D texture appearance with optional smearing or smudging when strokes overlap. We demonstrate high-fidelity paintings that closely resemble the captured media style, and also quantitatively evaluate our synthesis quality via user studies.	RealBrush: painting with examples of physical media	NA:NA:NA:NA	2018
Jorge Lopez-Moreno:Stefan Popov:Adrien Bousseau:Maneesh Agrawala:George Drettakis	Vector graphics represent images with compact, editable and scalable primitives. Skillful vector artists employ these primitives to produce vivid depictions of material appearance and lighting. However, such stylized imagery often requires building complex multi-layered combinations of colored fills and gradient meshes. We facilitate this task by introducing vector shade trees that bring to vector graphics the flexibility of modular shading representations as known in the 3D rendering community. In contrast to traditional shade trees that combine pixel and vertex shaders, our shade nodes encapsulate the creation and blending of vector primitives that vector artists routinely use. We propose a set of basic shade nodes that we design to respect the traditional guidelines on material depiction described in drawing books and tutorials. We integrate our representation as an Adobe Illustrator plug-in that allows even inexperienced users to take a line drawing, apply a few clicks and obtain a fully colored illustration. More experienced artists can easily refine the illustration, adding more details and visual features, while using all the vector drawing tools they are already familiar with. We demonstrate the power of our representation by quickly generating illustrations of complex objects and materials.	Depicting stylized materials with vector shade trees	NA:NA:NA:NA:NA	2018
Pierre Bénard:Forrester Cole:Michael Kass:Igor Mordatch:James Hegarty:Martin Sebastian Senn:Kurt Fleischer:Davide Pesare:Katherine Breeden	Skilled artists, using traditional media or modern computer painting tools, can create a variety of expressive styles that are very appealing in still images, but have been unsuitable for animation. The key difficulty is that existing techniques lack adequate temporal coherence to animate these styles effectively. Here we augment the range of practical animation styles by extending the guided texture synthesis method of Image Analogies [Hertzmann et al. 2001] to create temporally coherent animation sequences. To make the method art directable, we allow artists to paint portions of keyframes that are used as constraints. The in-betweens calculated by our method maintain stylistic continuity and yet change no more than necessary over time.	Stylizing animation by example	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Tobias Günther:Christian Rössl:Holger Theisel	For the visualization of dense line fields, the careful selection of lines to be rendered is a vital aspect. In this paper, we present a global line selection approach that is based on an optimization process. Starting with an initial set of lines that covers the domain, all lines are rendered with a varying opacity, which is subject to the minimization of a bounded-variable least-squares problem. The optimization strives to keep a balance between information presentation and occlusion avoidance. This way, we obtain view-dependent opacities of the line segments, allowing a real-time free navigation while minimizing the danger of missing important structures in the visualization. We compare our technique with existing local and greedy approaches and apply it to data sets in flow visualization, medical imaging, physics, and computer graphics.	Opacity optimization for 3D line fields	NA:NA:NA	2018
Li-Yi Wei	NA	Session details: Structures, faces & building	NA	2018
Hao Zhang:Kai Xu:Wei Jiang:Jinjie Lin:Daniel Cohen-Or:Baoquan Chen	We present an algorithm for hierarchical and layered analysis of irregular facades, seeking a high-level understanding of facade structures. By introducing layering into the analysis, we no longer view a facade as a flat structure, but allow it to be structurally separated into depth layers, enabling more compact and natural interpretations of building facades. Computationally, we perform a symmetry-driven search for an optimal hierarchical decomposition defined by split and layering operations applied to an input facade. The objective is symmetry maximization, i.e., to maximize the sum of symmetry of the substructures resulting from recursive decomposition. To this end, we propose a novel integral symmetry measure, which behaves well at both ends of the symmetry spectrum by accounting for all partial symmetries in a discrete structure. Our analysis results in a structural representation, which can be utilized for structural editing and exploration of building facades.	Layered analysis of irregular facades via symmetry maximization	NA:NA:NA:NA:NA:NA	2018
Fan Bao:Dong-Ming Yan:Niloy J. Mitra:Peter Wonka	Good building layouts are required to conform to regulatory guidelines, while meeting certain quality measures. While different methods can sample the space of such good layouts, there exists little support for a user to understand and systematically explore the samples. Starting from a discrete set of good layouts, we analytically characterize the local shape space of good layouts around each initial layout, compactly encode these spaces, and link them to support transitions across the different local spaces. We represent such transitions in the form of a portal graph. The user can then use the portal graph, along with the family of local shape spaces, to globally and locally explore the space of good building layouts. We use our framework on a variety of different test scenarios to showcase an intuitive design, navigation, and exploration interface.	Generating and exploring good building layouts	NA:NA:NA:NA	2018
Kun Xu:Kang Chen:Hongbo Fu:Wei-Lun Sun:Shi-Min Hu	This work presents Sketch2Scene, a framework that automatically turns a freehand sketch drawing inferring multiple scene objects to semantically valid, well arranged scenes of 3D models. Unlike the existing works on sketch-based search and composition of 3D models, which typically process individual sketched objects one by one, our technique performs co-retrieval and co-placement of 3D relevant models by jointly processing the sketched objects. This is enabled by summarizing functional and spatial relationships among models in a large collection of 3D scenes as structural groups. Our technique greatly reduces the amount of user intervention needed for sketch-based modeling of 3D scenes and fits well into the traditional production pipeline involving concept design followed by 3D modeling. A pilot study indicates that it is promising to use our technique as an alternative but more efficient tool of standard 3D modeling for 3D scene construction.	Sketch2Scene: sketch-based co-retrieval and co-placement of 3D models	NA:NA:NA:NA:NA	2018
Joseph Teran	NA	Session details: Skinning & deformation	NA	2018
Binh Huy Le:Zhigang Deng	Weighted linear interpolation has been widely used in many skinning techniques including linear blend skinning, dual quaternion blend skinning, and cage based deformation. To speed up performance, these skinning models typically employ a sparseness constraint, in which each 3D model vertex has a small fixed number of non-zero weights. However, the sparseness constraint also imposes certain limitations to skinning models and their various applications. This paper introduces an efficient two-layer sparse compression technique to substantially reduce the computational cost of a dense-weight skinning model, with insignificant loss of its visual quality. It can directly work on dense skinning weights or use example-based skinning decomposition to further improve its accuracy. Experiments and comparisons demonstrate that the introduced sparse compression model can significantly outperform state of the art weight reduction algorithms, as well as skinning decomposition algorithms with a sparseness constraint.	Two-layer sparse compression of dense-weight blend skinning	NA:NA	2018
Rodolphe Vaillant:Loïc Barthe:Gaël Guennebaud:Marie-Paule Cani:Damien Rohmer:Brian Wyvill:Olivier Gourmel:Mathias Paulin	Geometric skinning techniques, such as smooth blending or dual-quaternions, are very popular in the industry for their high performances, but fail to mimic realistic deformations. Other methods make use of physical simulation or control volume to better capture the skin behavior, yet they cannot deliver real-time feedback. In this paper, we present the first purely geometric method handling skin contact effects and muscular bulges in real-time. The insight is to exploit the advanced composition mechanism of volumetric, implicit representations for correcting the results of geometric skinning techniques. The mesh is first approximated by a set of implicit surfaces. At each animation step, these surfaces are combined in real-time and used to adjust the position of mesh vertices, starting from their smooth skinning position. This deformation step is done without any loss of detail and seamlessly handles contacts between skin parts. As it acts as a post-process, our method fits well into the standard animation pipeline. Moreover, it requires no intensive computation step such as collision detection, and therefore provides real-time performances.	Implicit skinning: real-time skin deformation with contact modeling	NA:NA:NA:NA:NA:NA:NA:NA	2018
Xian-Ying Li:Tao Ju:Shi-Min Hu	We present a new method for interpolating both boundary values and gradients over a 2D polygonal domain. Despite various previous efforts, it remains challenging to define a closed-form interpolant that produces natural-looking functions while allowing flexible control of boundary constraints. Our method builds on an existing transfinite interpolant over a continuous domain, which in turn extends the classical mean value interpolant. We re-derive the interpolant from the mean value property of biharmonic functions, and prove that the interpolant indeed matches the gradient constraints when the boundary is piece-wise linear. We then give closed-form formula (as generalized barycentric coordinates) for boundary constraints represented as polynomials up to degree 3 (for values) and 1 (for normal derivatives) over each polygon edge. We demonstrate the flexibility and efficiency of our coordinates in two novel applications, smooth image deformation using curved cage networks and adaptive simplification of gradient meshes.	Cubic mean value coordinates	NA:NA:NA	2018
Philip Dutré	NA	Session details: Sampling	NA	2018
Xin Sun:Kun Zhou:Jie Guo:Guofu Xie:Jingui Pan:Wencheng Wang:Baining Guo	Line segment sampling has recently been adopted in many rendering algorithms for better handling of a wide range of effects such as motion blur, defocus blur and scattering media. A question naturally raised is how to generate line segment samples with good properties that can effectively reduce variance and aliasing artifacts observed in the rendering results. This paper studies this problem and presents a frequency analysis of line segment sampling. The analysis shows that the frequency content of a line segment sample is equivalent to the weighted frequency content of a point sample. The weight introduces anisotropy that smoothly changes among point samples, line segment samples and line samples according to the lengths of the samples. Line segment sampling thus makes it possible to achieve a balance between noise (point sampling) and aliasing (line sampling) under the same sampling rate. Based on the analysis, we propose a line segment sampling scheme to preserve blue-noise properties of samples which can significantly reduce noise and aliasing artifacts in reconstruction results. We demonstrate that our sampling scheme improves the quality of depth-of-field rendering, motion blur rendering, and temporal light field reconstruction.	Line segment sampling with blue-noise properties	NA:NA:NA:NA:NA:NA:NA	2018
Kartic Subr:Jan Kautz	Each pixel in a photorealistic, computer generated picture is calculated by approximately integrating all the light arriving at the pixel, from the virtual scene. A common strategy to calculate these high-dimensional integrals is to average the estimates at stochastically sampled locations. The strategy with which the sampled locations are chosen is of utmost importance in deciding the quality of the approximation, and hence rendered image. We derive connections between the spectral properties of stochastic sampling patterns and the first and second order statistics of estimates of integration using the samples. Our equations provide insight into the assessment of stochastic sampling strategies for integration. We show that the amplitude of the expected Fourier spectrum of sampling patterns is a useful indicator of the bias when used in numerical integration. We deduce that estimator variance is directly dependent on the variance of the sampling spectrum over multiple realizations of the sampling pattern. We then analyse Gaussian jittered sampling, a simple variant of jittered sampling, that allows a smooth trade-off of bias for variance in uniform (regular grid) sampling. We verify our predictions using spectral measurement, quantitative integration experiments and qualitative comparisons of rendered images.	Fourier analysis of stochastic sampling strategies for assessing bias and variance in integration	NA:NA	2018
Wojciech Jarosz	NA	Session details: Precomputed rendering	NA	2018
Thorsten-Walther Schmidt:Jan Novák:Johannes Meng:Anton S. Kaplanyan:Tim Reiner:Derek Nowrouzezahrai:Carsten Dachsbacher	Industry-quality content creation relies on tools for lighting artists to quickly prototype, iterate, and refine final renders. As industry-leading studios quickly adopt physically-based rendering (PBR) across their art generation pipelines, many existing tools have become unsuitable as they address only simple effects without considering underlying PBR concepts and constraints. We present a novel light transport manipulation technique that operates directly on path-space solutions of the rendering equation. We expose intuitive direct and indirect manipulation approaches to edit complex effects such as (multi-refracted) caustics, diffuse and glossy indirect bounces, and direct/indirect shadows. With our sketch- and object-space selection, all built atop a parameterized regular expression engine, artists can search and isolate shading effects to inspect and edit. We classify and filter paths on the fly and visualize the selected transport phenomena. We survey artists who used our tool to manipulate complex phenomena on both static and animated scenes.	Path-space manipulation of physically-based light transport	NA:NA:NA:NA:NA:NA:NA	2018
Peiran Ren:Jiaping Wang:Minmin Gong:Stephen Lin:Xin Tong:Baining Guo	We present radiance regression functions for fast rendering of global illumination in scenes with dynamic local light sources. A radiance regression function (RRF) represents a non-linear mapping from local and contextual attributes of surface points, such as position, viewing direction, and lighting condition, to their indirect illumination values. The RRF is obtained from precomputed shading samples through regression analysis, which determines a function that best fits the shading data. For a given scene, the shading samples are precomputed by an offline renderer. The key idea behind our approach is to exploit the nonlinear coherence of the indirect illumination data to make the RRF both compact and fast to evaluate. We model the RRF as a multilayer acyclic feed-forward neural network, which provides a close functional approximation of the indirect illumination and can be efficiently evaluated at run time. To effectively model scenes with spatially variant material properties, we utilize an augmented set of attributes as input to the neural network RRF to reduce the amount of inference that the network needs to perform. To handle scenes with greater geometric complexity, we partition the input space of the RRF model and represent the subspaces with separate, smaller RRFs that can be evaluated more rapidly. As a result, the RRF model scales well to increasingly complex scene geometry and material variation. Because of its compactness and ease of evaluation, the RRF model enables real-time rendering with full global illumination effects, including changing caustics and multiple-bounce high-frequency glossy interreflections.	Global illumination with radiance regression functions	NA:NA:NA:NA:NA:NA	2018
Shuang Zhao:Miloš Hašan:Ravi Ramamoorthi:Kavita Bala	The highest fidelity images to date of complex materials like cloth use extremely high-resolution volumetric models. However, rendering such complex volumetric media is expensive, with brute-force path tracing often the only viable solution. Fortunately, common volumetric materials (fabrics, finished wood, synthesized solid textures) are structured, with repeated patterns approximated by tiling a small number of exemplar blocks. In this paper, we introduce a precomputation-based rendering approach for such volumetric media with repeated structures based on a modular transfer formulation. We model each exemplar block as a voxel grid and precompute voxel-to-voxel, patch-to-patch, and patch-to-voxel flux transfer matrices. At render time, when blocks are tiled to produce a high-resolution volume, we accurately compute low-order scattering, with modular flux transfer used to approximate higher-order scattering. We achieve speedups of up to 12× over path tracing on extremely complex volumes, with minimal loss of quality. In addition, we demonstrate that our approach outperforms photon mapping on these materials.	Modular flux transfer: efficient rendering of high-resolution volumes with repeated structures	NA:NA:NA:NA	2018
Frédo Durand	NA	Session details: Display hardware	NA	2018
Felix Heide:Gordon Wetzstein:Ramesh Raskar:Wolfgang Heidrich	Recent years have seen proposals for exciting new computational display technologies that are compressive in the sense that they generate high resolution images or light fields with relatively few display parameters. Image synthesis for these types of displays involves two major tasks: sampling and rendering high-dimensional target imagery, such as light fields or time-varying light fields, as well as optimizing the display parameters to provide a good approximation of the target content. In this paper, we introduce an adaptive optimization framework for compressive displays that generates high quality images and light fields using only a fraction of the total plenoptic samples. We demonstrate the framework for a large set of display technologies, including several types of auto-stereoscopic displays, high dynamic range displays, and high-resolution displays. We achieve significant performance gains, and in some cases are able to process data that would be infeasible with existing methods.	Adaptive image synthesis for compressive displays	NA:NA:NA:NA	2018
James Tompkin:Simon Heinzle:Jan Kautz:Wojciech Matusik	Lenticular prints are a popular medium for producing automultiscopic glasses-free 3D images. The light field emitted by such prints has a fixed spatial and angular resolution. We increase both perceived angular and spatial resolution by modifying the lenslet array to better match the content of a given light field. Our optimization algorithm analyzes the input light field and computes an optimal lenslet size, shape, and arrangement that best matches the input light field given a set of output parameters. The resulting emitted light field shows higher detail and smoother motion parallax compared to fixed-size lens arrays. We demonstrate our technique using rendered simulations and by 3D printing lens arrays, and we validate our approach in simulation with a user study.	Content-adaptive lenticular prints	NA:NA:NA:NA	2018
Rajinder Sodhi:Ivan Poupyrev:Matthew Glisson:Ali Israr	AIREAL is a novel haptic technology that delivers effective and expressive tactile sensations in free air, without requiring the user to wear a physical device. Combined with interactive computers graphics, AIREAL enables users to feel virtual 3D objects, experience free air textures and receive haptic feedback on gestures performed in free space. AIREAL relies on air vortex generation directed by an actuated flexible nozzle to provide effective tactile feedback with a 75 degrees field of view, and within an 8.5cm resolution at 1 meter. AIREAL is a scalable, inexpensive and practical free air haptic technology that can be used in a broad range of applications, including gaming, mobile applications, and gesture interaction among many others. This paper reports the details of the AIREAL design and control, experimental evaluations of the device's performance, as well as an exploration of the application space of free air haptic displays. Although we used vortices, we believe that the results reported are generalizable and will inform the design of haptic displays based on alternative principles of free air tactile actuation.	AIREAL: interactive tactile experiences in free air	NA:NA:NA:NA	2018
Bernd Bickel	NA	Session details: 3D printing	NA	2018
Desai Chen:David I. W. Levin:Piotr Didyk:Pitchaya Sitthi-Amorn:Wojciech Matusik	Multi-material 3D printing allows objects to be composed of complex, heterogenous arrangements of materials. It is often more natural to define a functional goal than to define the material composition of an object. Translating these functional requirements to fabri-cable 3D prints is still an open research problem. Recently, several specific instances of this problem have been explored (e.g., appearance or elastic deformation), but they exist as isolated, monolithic algorithms. In this paper, we propose an abstraction mechanism that simplifies the design, development, implementation, and reuse of these algorithms. Our solution relies on two new data structures: a reducer tree that efficiently parameterizes the space of material assignments and a tuner network that describes the optimization process used to compute material arrangement. We provide an application programming interface for specifying the desired object and for defining parameters for the reducer tree and tuner network. We illustrate the utility of our framework by implementing several fabrication algorithms as well as demonstrating the manufactured results.	Spec2Fab: a reducer-tuner model for translating specifications to 3D prints	NA:NA:NA:NA:NA	2018
Kiril Vidimče:Szu-Po Wang:Jonathan Ragan-Kelley:Wojciech Matusik	3D printing hardware is rapidly scaling up to output continuous mixtures of multiple materials at increasing resolution over ever larger print volumes. This poses an enormous computational challenge: large high-resolution prints comprise trillions of voxels and petabytes of data and simply modeling and describing the input with spatially varying material mixtures at this scale is challenging. Existing 3D printing software is insufficient; in particular, most software is designed to support only a few million primitives, with discrete material choices per object. We present OpenFab, a programmable pipeline for synthesis of multi-material 3D printed objects that is inspired by RenderMan and modern GPU pipelines. The pipeline supports procedural evaluation of geometric detail and material composition, using shader-like fablets, allowing models to be specified easily and efficiently. We describe a streaming architecture for OpenFab; only a small fraction of the final volume is stored in memory and output is fed to the printer with little startup delay. We demonstrate it on a variety of multi-material objects.	OpenFab: a programmable pipeline for multi-material fabrication	NA:NA:NA:NA	2018
Qingnan Zhou:Julian Panetta:Denis Zorin	Direct digital manufacturing is a set of rapidly evolving technologies that provide easy ways to manufacture highly customized and unique products. The development pipeline for such products is radically different from the conventional manufacturing pipeline: 3D geometric models are designed by users often with little or no manufacturing experience, and sent directly to the printer. Structural analysis on the user side with conventional tools is often unfeasible as it requires specialized training and software. Trial-and-error, the most common approach, is time-consuming and expensive. We present a method that would identify structural problems in objects designed for 3D printing based on geometry and material properties only, without specific assumptions on loads and manual load setup. We solve a constrained optimization problem to determine the "worst" load distribution for a shape that will cause high local stress or large deformations. While in its general form this optimization has a prohibitively high computational cost, we demonstrate that an approximate method makes it possible to solve the problem rapidly for a broad range of printed models. We validate our method both computationally and experimentally and demonstrate that it has good predictive power for a number of diverse 3D printed shapes.	Worst-case structural analysis	NA:NA:NA	2018
Karl D. D. Willis:Andrew D. Wilson	We introduce InfraStructs, material-based tags that embed information inside digitally fabricated objects for imaging in the Terahertz region. Terahertz imaging can safely penetrate many common materials, opening up new possibilities for encoding hidden information as part of the fabrication process. We outline the design, fabrication, imaging, and data processing steps to fabricate information inside physical objects. Prototype tag designs are presented for location encoding, pose estimation, object identification, data storage, and authentication. We provide detailed analysis of the constraints and performance considerations for designing InfraStruct tags. Future application scenarios range from production line inventory, to customized game accessories, to mobile robotics.	InfraStructs: fabricating information inside physical objects for imaging in the terahertz region	NA:NA	2018
Diego Nehab	NA	Session details: Hardware rendering	NA	2018
Michael J. Doyle:Colin Fowler:Michael Manzke	Ray-tracing algorithms are known for producing highly realistic images, but at a significant computational cost. For this reason, a large body of research exists on various techniques for accelerating these costly algorithms. One approach to achieving superior performance which has received comparatively little attention is the design of specialised ray-tracing hardware. The research that does exist on this topic has consistently demonstrated that significant performance and efficiency gains can be achieved with dedicated microarchitectures. However, previous work on hardware ray-tracing has focused almost entirely on the traversal and intersection aspects of the pipeline. As a result, the critical aspect of the management and construction of acceleration data-structures remains largely absent from the hardware literature. We propose that a specialised microarchitecture for this purpose could achieve considerable performance and efficiency improvements over programmable platforms. To this end, we have developed the first dedicated microarchitecture for the construction of binned SAH BVHs. Cycle-accurate simulations show that our design achieves significant improvements in raw performance and in the bandwidth required for construction, as well as large efficiency gains in terms of performance per clock and die area compared to manycore implementations. We conclude that such a design would be useful in the context of a heterogeneous graphics processor, and may help future graphics processor designs to reduce predicted technology-imposed utilisation limits.	A hardware unit for fast SAH-optimised BVH construction	NA:NA:NA	2018
Josiah Manson:Scott Schaefer	We present a method to create high-quality sampling filters by combining a prescribed number of texels from several resolutions in a mipmap. Our technique provides fine control over the number of texels we read per texture sample so that we can scale quality to match a memory bandwidth budget. Our method also has a fixed cost regardless of the filter we approximate, which makes it feasible to approximate higher-quality filters such as a Lánczos 2 filter in real-time rendering. To find the best set of texels to represent a given sampling filter and what weights to assign those texels, we perform a cardinality-constrained least-squares optimization of the most likely candidate solutions and encode the results of the optimization in a small table that is easily stored on the GPU. We present results that show we accurately reproduce filters using few texel reads and that both quality and speed scale smoothly with available bandwidth. When using four or more texels per sample, our image quality exceeds that of trilinear interpolation.	Cardinality-constrained texture filtering	NA:NA	2018
Petrik Clarberg:Robert Toth:Jacob Munkberg	Stochastic sampling in time and over the lens is essential to produce photo-realistic images, and it has the potential to revolutionize real-time graphics. In this paper, we take an architectural view of the problem and propose a novel hardware architecture for efficient shading in the context of stochastic rendering. We replace previous caching mechanisms by a sorting step to extract coherence, thereby ensuring that only non-occluded samples are shaded. The memory bandwidth is kept at a minimum by operating on tiles and using new buffer compression methods. Our architecture has several unique benefits not traditionally associated with deferred shading. First, shading is performed in primitive order, which enables late shading of vertex attributes and avoids the need to generate a G-buffer of pre-interpolated vertex attributes. Second, we support state changes, e.g., change of shaders and resources in the deferred shading pass, avoiding the need for a single über-shader. We perform an extensive architectural simulation to quantify the benefits of our algorithm on real workloads.	A sort-based deferred shading architecture for decoupled sampling	NA:NA:NA	2018
Andrew Nealen	NA	Session details: Laplacians, light field & layouts	NA	2018
Dilip Krishnan:Raanan Fattal:Richard Szeliski	We present a new multi-level preconditioning scheme for discrete Poisson equations that arise in various computer graphics applications such as colorization, edge-preserving decomposition for two-dimensional images, and geodesic distances and diffusion on three-dimensional meshes. Our approach interleaves the selection of fine-and coarse-level variables with the removal of weak connections between potential fine-level variables (sparsification) and the compensation for these changes by strengthening nearby connections. By applying these operations before each elimination step and repeating the procedure recursively on the resulting smaller systems, we obtain a highly efficient multi-level preconditioning scheme with linear time and memory requirements. Our experiments demonstrate that our new scheme outperforms or is comparable with other state-of-the-art methods, both in terms of operation count and wall-clock time. This speedup is achieved by the new method's ability to reduce the condition number of irregular Laplacian matrices as well as homogeneous systems. It can therefore be used for a wide variety of computational photography problems, as well as several 3D mesh processing tasks, without the need to carefully match the algorithm to the problem characteristics.	Efficient preconditioning of laplacian matrices for computer graphics	NA:NA:NA	2018
Jean-David Génevaux:Éric Galin:Eric Guérin:Adrien Peytavie:Bedrich Benes	We present a framework that allows quick and intuitive modeling of terrains using concepts inspired by hydrology. The terrain is generated from a simple initial sketch, and its generation is controlled by a few parameters. Our terrain representation is both analytic and continuous and can be rendered by using varying levels of detail. The terrain data are stored in a novel data structure: a construction tree whose internal nodes define a combination of operations, and whose leaves represent terrain features. The framework uses rivers as modeling elements, and it first creates a hierarchical drainage network that is represented as a geometric graph over a given input domain. The network is then analyzed to construct watersheds and to characterize the different types and trajectories of rivers. The terrain is finally generated by combining procedural terrain and river patches with blending and carving operators.	Terrain generation using procedural models based on hydrology	NA:NA:NA:NA:NA	2018
Jan Kautz	NA	Session details: Appearance fabrication	NA	2018
Anat Levin:Daniel Glasner:Ying Xiong:Frédo Durand:William Freeman:Wojciech Matusik:Todd Zickler	Recent attempts to fabricate surfaces with custom reflectance functions boast impressive angular resolution, yet their spatial resolution is limited. In this paper we present a method to construct spatially varying reflectance at a high resolution of up to 220dpi, orders of magnitude greater than previous attempts, albeit with a lower angular resolution. The resolution of previous approaches is limited by the machining, but more fundamentally, by the geometric optics model on which they are built. Beyond a certain scale geometric optics models break down and wave effects must be taken into account. We present an analysis of incoherent reflectance based on wave optics and gain important insights into reflectance design. We further suggest and demonstrate a practical method, which takes into account the limitations of existing micro-fabrication techniques such as photolithography to design and fabricate a range of reflection effects, based on wave interference.	Fabricating BRDFs at high spatial resolution using wave optics	NA:NA:NA:NA:NA:NA:NA	2018
Yanxiang Lan:Yue Dong:Fabio Pellacini:Xin Tong	Surfaces in the real world exhibit complex appearance due to spatial variations in both their reflectance and local shading frames (i.e. the local coordinate system defined by the normal and tangent direction). For opaque surfaces, existing fabrication solutions can reproduce well only the spatial variations of isotropic reflectance. In this paper, we present a system for fabricating surfaces with desired spatially-varying reflectance, including anisotropic ones, and local shading frames. We approximate each input reflectance, rotated by its local frame, as a small patch of oriented facets coated with isotropic glossy inks. By assigning different ink combinations to facets with different orientations, this bi-scale material can reproduce a wider variety of reflectance than the printer gamut, including anisotropic materials. By orienting the facets appropriately, we control the local shading frame. We propose an algorithm to automatically determine the optimal facets orientations and ink combinations that best approximate a given input appearance, while obeying manufacturing constraints on both geometry and ink gamut. We fabricate the resulting surface with commercially available hardware, a 3D printer to fabricate the facets and a flatbed UV printer to coat them with inks. We validate our method by fabricating a variety of isotropic and anisotropic materials with rich variations in normals and tangents.	Bi-scale appearance fabrication	NA:NA:NA:NA	2018
James L. Mohler	NA	Session details: Educators program	NA	2018
Gary R. Bertoline:Cary Laxer	Computer graphics is a powerful medium used to communicate information and knowledge. It is a discipline whose time has come. Until recently it was a mysterious specialty involving expensive display hardware, considerable computing resources, and specialized software. In the last few years, computer graphics has found its way into the mainstream of society, from entertainment, to engineering design, to the web, and virtually every industry. Much of this has been the result of spectacular improvements in the price and performance of computer graphics hardware and software. Interactive computer graphics is finding its way into nearly every discipline, industry, home, hospital, theatre, football stadium, automobile, appliance, and engineering office. Computer graphics is or will have an impact on nearly everything we do. The discipline of computer graphics is like a wide-open frontier where no matter which direction you move you will find more opportunities and undiscovered applications. Computer graphics is the pictorial synthesis of real or imagined objects from their computer-based models. A related field is image processing which is the reconstruction of models of 2D and 3D objects from their pictures (Foley, et al, 1996). Traditionally, computer graphics is contained within the well-recognized disciplines of computer science, electrical and computer engineering, and art and design. The discipline of computer graphics has traditionally focused on developing new software algorithms (computer science) and hardware innovations (electrical and computer engineering) with an attempt to force art and design principles into the mix. Those types of developments will continue but the discipline is beginning to mature to a point where it is more than just about hardware and software developments and innovations. More attention must be given to the effective and novel use of the hardware and software developments in the context of graphics communication. This is a much more holistic approach to computer graphics as a discipline. This holistic approach is more focused on the end user and how these tools can be used in much more profound and revolutionary ways. Obviously there have been many major and profound applications of computer graphics in its short history. However, the recent past of computer graphics is but a prelude of what is to come. Computer graphics can and will touch every human in a number of ways from entertainment to assisting in finding the cure of the most dreaded diseases. Computer graphics can and will have a profound effect on every type of business, industry, government, education, and the home. But it will take a very special type of education to prepare this next generation of computer graphics specialists. Computer graphics is a powerful medium but only if combined with the principles of information design. The principles of information design are universal, like mathematics, and are not bound to unique features of a particular language or culture. The universal nature of the graphic language makes it a powerful tool in today's society where collecting, analyzing, and communicating all the available information is so important. Information becomes knowledge and knowledge can be communicated more efficiently through computer graphics. Knowledge can become power and is the catalyst for stunning new developments in virtually every field. The pure computer graphics discipline of the future will not be in computer science, art and design, technology, or electrical and computer engineering. The computer graphics discipline of the future will have its legacy in all these disciplines but will look to merge the software and hardware technology with the human communication process, which will result in novel ways of solving problems and disseminating information. As shown in Figure 1, computer graphics is the overlap between art, science and technology, and psychology. The ideal student of this emerging discipline is bright, articulate, visual, analytic, and motivated by a passion for computer graphics. This student uses both sides of their brain but is keenly focused on the visual mode to solve problems. They are the modern-day Da Vincis, capable of visualizing what is nonexistent and finding solutions to complex problems. New opportunities are unfolding that require special talents and abilities for people with high visualization abilities who can use computer graphics tools to visualize scientific concepts and for the analysis and manipulation of complex three-dimensional information. As these new opportunities continue to unfold, the skill in manipulating and creating imagery may become more important than skill with words and numbers. Different kinds of tools may require different talents and favor a different type of discipline.	A knowledge base for the computer graphics discipline	NA:NA	2018
Jason Della Rocca:Robin Hunike:Warren Spector:Eric Zimmerman	Electronic gaming has changed. What was a curiosity twenty years ago is now one of the most popular forms of entertainment for all age groups. Games are now an accepted, pervasive part of US and world culture. The new ubiquity of games demands that we understand them as commercial products, aesthetic objects, learning contexts and cultural phenomena. This, in turn, creates a demand for people to fill a variety of roles associated with the creation, consumption and analysis of games. For gaming to have a healthy future, industry and academia must find or create new contributors with new goals, ideas and experiences. We believe that educators have the best chance of creating and fostering these new goals and ideas while industry brings to the table a vital experiential understanding of the realities of game development. Academia and industry must work together, then, if progress is to be made. Schools have begun to recognize the cultural and academic relevance of games. In addition to their increasing presence in the fabric of student life, games are rapidly evolving -- diversifying and broadening their role as a communication medium. As universities design academic programs for the critical analysis of games and begin researching related interactive entertainment technologies, they see the value of including industry veterans in their plans. The International Game Developers Association's Education Committee, comprised of both developers and academics, has been working on a curriculum framework and set of guidelines to aid in the development of game-oriented curriculum. During this session, the Education Committee's Curriculum Subcommittee will present its work to date, and solicit input and criticism on the framework. The goal is to generate significant feedback from the academic community to further refine the framework. Here are some of the guiding principles used to develop the curriculum framework: • Gaming is an interdisciplinary field. This curriculum framework presents a wide range of topics encompassed by the acts of game creation, analysis and criticism. We strongly advocate cross-disciplinary study in any game-related course or curriculum. • Analysis, practice and context are equally important. Game development students must be exposed to analytical, practical and contextual materials. • The framework was designed to be versatile. Acknowledging the realities of educational institutions, especially when it comes to the introduction of new disciplines to the curriculum, we felt we had to create a game studies outline that was as versatile as possible. • The framework is not vocational but is career-oriented. The curriculum framework is focused not on churning out developers, per se, but on turning out well-rounded students of gaming, some of whom will no doubt choose to become developers. The IGDA's curriculum framework and related endeavors can be found online at the IGDA web site: http://www.igda.org/Committees/education.htm	Game development, design and analysis curriculum	NA:NA:NA:NA	2018
Donna Cox	Originally, "university" was given its name because it was believed that a student would go to the university to find his or her place within the universe. Today's liberal arts education has limited this universal process and forces an early decision: the "major." Students follow a well-defined curriculum that often precludes interdisciplinary study and cross-campus interaction. While most universities provide structure for in-depth, discipline-specific education; few offer curricula that take the student over many academic routes before making a decision on the major. Technical requirements can be the most constraining due to the vast foundational knowledge that must be mastered in the sciences and engineering. Educational reward and economic systems force segregation of the disciplines. Faculty are often rewarded to be discipline specific, in both research and teaching. Students are not able to take classes across campus because the "major's" students fill the courses leaving no room for non-majors. Universities are struggling due to low budgets and cutbacks. And it is very difficult to manage students who want to create new degrees that span campus boundaries. In contrast to this segregation of disciplines, the computer graphics industry often demands generalist knowledge. Often a programmer is required to design or a designer is required to have advanced technical skills. It is apparent in the area of computer graphics, visual literacy is one of the most critical skills that span both art and science [West 1991]. A National Research Council committee studying these issues found a strain on the talent pool in 3-D computer graphics and that universities are failing to provide adequate cross-disciplinary training: "The development of workers with a mixture of technical and artistic capabilities represents a particular challenge because of its interdisciplinary nature. Whereas computer science and electrical engineering departments will train technical workers to address questions about networking and distributed simulation, the creation of visually literate workers demands cooperation between engineering and art departments, which are separated by large cultural and institutional gaps" [Zyda et al. 1997]. These cultural divisions have been talked about since C. P. Snow's essay on "Two Cultures." However there are many cultures and cultures within cultures. Within the educational system, there are many illogical divisions within and among departments. Certainly the divide between science and art is the greatest in education. How does this type of education prepare the student to find himself or herself in the universe? Given the constraints of contemporary educational systems, how can we educators inspire the Renaissance person, one who has truly interdisciplinary knowledge and skills? Issues to consider: • How do we focus students' attention without constraining the breadth of knowledge? • As educators, we are asked to prepare students for the "market"...do we really know the market? • Should artists be taught to develop software as part of their curriculum? • What are examples of organizing projects that encourage cross-disciplinary thinking? • How does one find willing collaborative partners on large campuses? • How can technology help in this quest to match collaborators and facilitate interdisciplinary activity? • How can we change or by-pass organizational structures that inhibit the Renaissance person? • Do collaborative teams in the classroom facilitate or hinder this kind of thinking? • Given the vast knowledge that an interdisciplinary study must cover, should college terms be extended?	Inspiring the renaissance person	NA	2018
Dena Elisabeth Eber:Bonnie Mitchell:Heather Elliott	Despite the artistic maturity computer graphics has gained throughout the 1990s and into the new millennium, what we call the art student "wow" factor in computer graphics---the phenomenon of student obsession over new technology instead of artistic substance---is still a point of contention. It remains a challenge to teach computer graphics to art students while maintaining a balance between thoughtful art and sophisticated technology. In this session we will, with the help of the participants, reveal pedagogical solutions and uncover various approaches to teaching art students to use digital media in rich and expressive ways. When the curve of new computer graphics technology was steeper and interfaces were not as intuitive as they now are, much of student learning was centered on the hardware and software and students struggled to comprehend digital media's place in the art world. In essence, we were in what Marshall McLuhan referred to as the first phase of a new technology---a stage in which, in the context of teaching digital media in the arts, the students were trying to understand computer graphics technology in terms of what they already knew. A digital image became a painting or a photograph, and 3D modeling and animation were understood in terms of film studies. Further, many students did not understand the scope of digital art and how it encompassed sculptural digital installations, how the art did not have to result in physical objects, or how the pieces could exist as interactive programs or websites. Students also worried more about the technology---what it could do and how to use it---rather than understanding the unique ways in which they could create aesthetically pleasing and profound works of art. Today, computer graphics-based art is now closer to being established in the art world and embraces a more intuitive and sensible mode in which to create. However, the "wow" factor is still a problem and the changing pace of technology still keeps students and teachers alike tuned into the tool. Students are still trying to understand what the technology can do and often forget why or what they are making. Art instructors have a genuine struggle with balancing the time to teach students how to do something digitally with teaching them how to express themselves visually. Further, some software programs are truly complex and the concepts behind them require the student to understand, at least intuitively, the laws of physics, lighting, and how things change over time. Perhaps in some ways we are still in the first phase of digital technology in the arts. From uncomplicated to elaborate software, how can instructors approach lessons without being bogged down with too much technology? Should instructors teach software or should they teach concepts, thus emphasizing the importance of technology as a means to an end rather than the end? What are some examples of lessons, assignments, or overall approaches that give proper weight to art and technology? How can teachers facilitate unique and expressive works of art? Even if instructors are successful teaching individual expression with computer graphics technology, what domain within this vast field is most important? What are some areas on which teachers could focus their energies? Are some computer graphics disciplines harder to teach than others? Is it more difficult to express ideas and compose formally beautiful works in some forms of digital graphics? Why or why not? The best lessons and the best pedagogical approaches still will not hinder the dedicated special effects gurus from obsessing about what the software can do instead of the resulting work. How can instructors help these students care more about the art works? Finally, and maybe most importantly, how can teachers keep up with the technology? Perhaps it is the teacher's concern with keeping current that influences the way they teach and what the students perceive as important. What are some solutions for instructors to keep up with changing computer graphics technology without compromising the core of the art they want to teach? We ask the participants to come prepared to talk about some of these ideas and more. Once we target issues to resolve, we will ask the forum participants to put forward their gems: lessons, approaches, philosophical bends, and ideas that have worked or they believe will work. Attendees will leave the session with a notebook full of ideas and solutions to teaching students how to be expressive and create aesthetically pleasing works of computer graphics based art.	Teaching gems for art and design	NA:NA:NA	2018
Bruce Wands	This forum will give attendees a chance to present their own views on creativity and curriculum, as well as hear those of educators from a diverse group of colleges. Computer graphics education has grown tremendously in the last five years, particularly on the department level. Many issues have arisen related to the place of computer graphics education within a specific department's curriculum. They include the type of courses offered, challenges arising from the impact of this added educational component, and the desire to maintain traditional art education elements, such as theory and critique. Creativity has historically been addressed in theory and critique classes. It is also now being taught along with software in computer graphics classes. Outcomes from this forum will allow educators to gain insight into their approach to nurturing creativity as it relates to computer graphics education. Other topics to be discussed include interdisciplinary approaches to curriculum, creating content for courses, and the relationship of computer graphics to traditional art education. The role of creativity in computer science classes, particularly programming, will also be discussed.	The role of creativity in computer graphics education	NA	2018
Mike Bailey:Steve Cunningham:Lew Hitchner	Moderators and attendees of this forum will present, discuss, and assess examples of "best practices" for teaching computer graphics to computer science and engineering students. Types of "teaching gems" to be presented include: classroom lectures and demonstrations, lab exercises, homework projects, self-instruction techniques such as tutorials, demo programs, and Web applets, use of analog models, examples from industry and the rest of the "real world", and field trips (real and virtual). The forum moderators will contribute presentations by electronic submission from virtual attendees prior to the conference and by attendees on site during the forum session. Three outcomes are anticipated from this forum: 1. presentation of several teaching gems, 2. group analysis and discussion of how best to apply each gem in a course and it's learning effectiveness, and 3. posting of the results for public access on the SIGGRAPH Education Committee web site, http://www.siggraph.org/education.	Teaching gems for computer science and engineering	NA:NA:NA	2018
Darlene Wolfe:Jeff Scheetz:Roger Cotton:Timothy Comolli:Chris Stapleton	Professional animators and digital effects artists and companies often work with high school teachers to explore relationships that can be developed and maintained between high schools and industry. The goals of this panel are to make more educators aware of the on-line and in person mentoring, sharing, and peer training that can occur in graphics, animation and related fields for 1) improving curricula and integrated technologies, 2) creating or furthering personal student interest or as a career vehicle and foundation, and 3) encouraging teachers to become involved in, and comfortable with, the new media. When Josh Spector wrote "Studios drawn to CGI features" for the Hollywood Reporter [January 2, 2002], he noted the studios™ big interest in animation and computer graphics based on Shrek, Monsters, Inc. and the realization that there were audiences for the non-Disney animated feature. Youth of all ages, as well as adults, not only enjoy CGI, but can also create it themselves with the current technology on home computers. Today's high school students, with the current technology available in home machines, are able to create products comparable to that of professionals, using computer technology that was very expensive just a few years ago. Although many students do not have access to the Internet or recent computer technology, those that do are producing programs for the Internet, their schools, and even, for sales. To gain and maintain the skill, one must practice. To give every child an even chance, the technology must be made available through education. Ideally, students would be given or lent computers with appropriate software for their classes, skills and academic or hobby interests. In order to do that, administrators and teachers today must first recognize the convergence of electronic media, information technology, communication, and entertainment. Then plans must be made or programs adapted to further the educational needs and foundations of the student (at any age). Therefore, it is imperative that teachers and administrators integrate 21st century technology into the curricula. However, we are sometimes afraid or hesitant to make changes we do not thoroughly understand, much less that are new to us. Many of today's educators are nearing retirement and there is a growing shortage of teachers. There is usually a shortage of funds in public education, too. Schools, therefore, often fall behind in the technology learning curve. New and exciting curriculum may be welcomed with open arms and with great desire but a lack of resources, combined with fear and trepidation by the very people who must learn and teach it, may negate its implementation.	K-12 and industry partnering	NA:NA:NA:NA:NA	2018
Roberta Tarbell:LiQin Tan	"Animating Art History for Teaching" is a creative learning methodology that incorporates new perspectives for introductory art history courses through digital technology created by computer animation and art history professors and their students.	Animating art history for teaching	NA:NA	2018
Anne Morgan Spalter	Reuse is vital in the education world because the time and money necessary to create high quality educational software is prohibitive. Estimates for the cost of creating a single well designed, highly graphical and interactive online course in the commercial domain range from several hundred thousand dollars to a million or more. Thus the idea of reusable software components that can be easily shared is tremendously appealing. In fact, "component" has become a buzzword in the educational software community, with millions of dollars from the National Science Foundation and other sponsors funding a wide variety of "component-based" projects. But few, if any, of these projects, have approached the grand vision of creating repositories of easy to reuse components for developers and educators. This paper investigates some of the factors that stand in the way of achieving this goal. We begin by defining the word component and looking at several projects using components, with a focus on our Exploratories project at Brown University. We then discuss challenges in: Searching and Metadata, Quality Assurance, Programming in the University Environment, Platform and System Specificity, Social Issues, Intellectual Property Issues, and Critical Mass. We look at relevant software engineering issues and describe why we believe educational applications have unique factors that should be considered when using components.	Problems with using components in educational software	NA	2018
Tiffany Holmes	This paper presents new curriculum for an introductory course in art and technology in which students compare the software industry with fast food to investigate patterns of consumption in our culture.	What do computers eat?: teaching beginners to think critically about technology and art	NA	2018
Kevin L. Novins	This paper describes the design and implementation of a student-centered course in advanced undergraduate computer graphics. Instead of providing students with all necessary background before exposing them to recent research, students start with the latest SIGGRAPH proceedings and discover what topics they need to learn to understand them. The power of the group of students is exploited by having each student write a tutorial on one of the topics. Students then trade these written tutorials before being tested on the SIGGRAPH papers themselves. The course aims to nurture lifelong learning skills as well as an understanding of state of the art methods in computer graphics.	SIGGRAPH as textbook: learning skills for undergraduates	NA	2018
Hannes Kaufmann:Dieter Schmalstieg	Construct3D is a three-dimensional geometric construction tool specifically designed for mathematics and geometry education. It is based on the mobile collaborative augmented reality system "Studierstube." We describe our efforts in developing a system for the improvement of spatial abilities and maximization of transfer of learning. In order to support various teacher-student interaction scenarios we implemented flexible methods for context and user dependent rendering of parts of the construction. Together with hybrid hardware setups they allow the use of Construct3D in today's classrooms and provide a test bed for future evaluations. Means of application and integration in mathematics and geometry education at the high school, as well as the university, level are being discussed. Anecdotal evidence supports our claim that Construct3D is easy to learn, encourages experimentation with geometric constructions, and improves spatial skills.	Mathematics and geometry education with collaborative augmented reality	NA:NA	2018
Dennis J. Bouvier	A number of published papers recommend teaching scene graphs in the introductory computer graphics course [Bouvier 2002; Cunningham 1999; Hitchner and Sowizral 1999; Wolfe 1999]. However, little has been published concerning how to effectively use scene graphs in the introductory computer graphics course. This paper summarizes possible scene graphs exercises and teaching experience of the author.	Assignment: scene graphs in computer graphics courses	NA	2018
Sampson D. Asare:Petros M. Mashwama:Steve Cunningham	Computer graphics is becoming a tool for communication, change, and development throughout the world, and in order for this tool to be effective, there must be an educational base for its use. In southern Africa the development of computer graphics is underway, but is has not made the impact it could have. We believe that one of the main reasons for this is that there has not been as much educational development as is needed in the region. This note will describe some of the challenges that are faced by educational institutions in southern Africa as they try to develop the computer graphics education in the region, and will discuss some of the ways these institutions are working to meet these challenges.	Building computer graphics education in developing African countries	NA:NA:NA	2018
Julie Callahan:Charles C. H. Jui	Flash provides the ASPIRE team a means to create interactive virtual learning environments, where teachers and students can explore places and things previously limited by the classroom or simply by time constraints. Flash can become an immersive world for online science education. Producing curriculum support material in Flash provides many benefits for teachers and students. Teachers are provided with an affordable tool, which aligns to local and national curriculum standards. This material provides a rich classroom experience at little cost to the educator, while also satisfying the curriculum goals. Students can participate in an engaging learning experience, where science becomes an experience beyond a textbook lesson or a classroom lab. Situations are presented that would be either difficult to reproduce due to cost or scale. For example, students can investigate the causes of tides in the ASPIRE Flash activity "Gravity and Tides." Flash, the software available for authoring these activities, can produce a mathematically correct, physically based model that students can observe and investigate. These lessons provide a high-quality experience; meanwhile, the cost and speed of production of these activities continue to decrease.	Macromedia flash in physics education aspire's interactive online labs and lessons	NA:NA	2018
Fiona Bailey:Magnus Moar	Children invent imaginary worlds and enact scenarios within them on a daily basis as part of their imaginative play. Given the opportunity and the tools, what kind of worlds would children create for themselves within a virtual space, and what kind of learning can emerge within these playful, child-centered spaces? In VERTEX, young children inhabit an imaginary virtual world that they have designed and created using 3D modeling tools and net-based virtual worlds software. Crossing traditional subject disciplines and involving local and remote collaboration, the project demonstrates children's design and communication abilities above and beyond the expectations of the curriculum.	The Vertex project: exploring the creative use of shared 3D virtual worlds in the primary (K-12) classroom	NA:NA	2018
Dave Pape:Josephine Anstey	Immersive, interactive virtual reality is a tool with hypothetically limitless uses. However, so far it has been put to serious use primarily in technical application areas such as computational science, automotive engineering, and chemical exploration. Groups working in these fields often have large budgets and can afford expensive, advanced displays. VR should also be of value to schools and museums, but most of them have much smaller budgets than major research labs, or are not able to support high-end graphics workstations. A simple, affordable, projection based display system can make VR far more accessible. In schools, displays could be put into individual classrooms and not just a central computer lab. In the museum world, small institutions would be capable of showing cutting edge digital work that previously has been restricted to a few large museums. This workshop describes the construction of a single screen, passive stereo, VR display based on commodity, or otherwise low-cost, components. There are many options available for the major elements of such a system and the basic system can be modified or adapted to many different styles of use. Figure 1 shows a photo of such a system in use at the University at Buffalo.	Building an affordable projective, immersive display	NA:NA	2018
Gregory P. Garvey	This workshop, first given at SIGGRAPH 2001 [Garvey 2002] is now offered in two-parts. Part I introduces the process of transferring life drawings into 3D models using MAYA followed by a life drawing session. In Part II, workshop attendees working in the CAL import digitized life drawings into MAYA for setup as image planes to guide the modeling process. The goal of the workshop is to explore and develop skills of observational figure drawing and integrate them with the process of 3D modeling.	Life drawing and 3D figure modeling with MAYA	NA	2018
Mitch Williams	3D curriculum, once focused on modeling, rendering and animation, now includes web 3D and interactivity. But what options exist for new courses in web 3D? How in-depth are some web 3D tools and their learning curves? Should the curriculum expand to make 3D artists into programmers? What considerations should we give to the end-users' experience in viewing an online web 3D portfolio? This session examines how these issues are being addressed in teaching "Interactive Web 3D Media" at UC Berkeley, UC Irvine and UCLA Digital Arts and Entertainment Studies Extension. The session also provides hands-on experience with various tools and technologies.	Integrating web 3D into 3D animation curricula	NA	2018
Susana Maria Halpine	Integrating multimedia applications in the classroom can be overwhelming. Grants may address the cost of computer hardware, but where can instructors find the time to explore available software? Many visualization programs are free or low cost, but students will not grasp the importance of what they are viewing without proper conceptual introduction. Furthermore, many K-12 instructors are now expected to teach topics, including basic chemistry concepts, in which they may lack proper training. The STArt! teaching Science Through Art program was developed to help teachers prepare for these educational challenges. Using an "Artist in Residence" format, workshops are developed in collaboration with participating teachers. Specifically, STArt! focuses on basic concepts addressed in the new California K-12 Science Content Standards. The program introduces molecular visualization software using narrative discussions, educational animation, and hands-on workshops using art materials and everyday objects. By exploring different learning modes, it makes basic science concepts more understandable to a broader audience. Furthermore, by collaborating with instructors within their classrooms, the program provides a creative resource for teachers in meeting the academic standards.	Hi tech - lo tech: K-12 science visualization	NA	2018
Adam Watkins	As 3D student skills progress, sooner or later they begin work on accurately proportioned human models. While caricature design is often very forgiving in its realization, human designs need to be very close to "right on." This presents a tremendous challenge for 3D animation students, many of whom have had limited anatomy training. The traditional solution to source material has been to provide photographic references. The standard front and side shots provide a good starting point for students to work from but provide no information on appropriate polygon topology or details such as the curvature of the head between the eye and the temple. Yet, it is impractical to get a live model to provide 3D reference by sitting next to the student as he or she models. The difficulty of finding a good 3D reference for students to work from becomes the challenge. At the University of the Incarnate Word, we have begun using a traditional method of face casting. Traditionally, this technique is used to create plaster masks or molds upon which prosthetics can be constructed. The mold is reusable and can be constructed from plaster or even lighter cements.	Teaching human facial modeling through plaster face casting	NA	2018
Scott Senften	NA	Session details: Emerging technologies	NA	2018
John M. Fujii	In computer graphics, history frames the possible, imagination paints the impossible, and passion fills in the rest.	Tomorrow's yesterday: mapping the E-Tech continuum	NA	2018
Laroussi Bouguila:Makoto Sato:Shoichi Hasegawa:Hashimoto Naoki:Naoki Matsumoto:Atsushi Toyama:Jelel Ezzine:Dalel Maghrebi	The project presents a new locomotion interface for virtual environment with large display system. Users will be able to direct and control the traveling in the VE by in-place stepping and turning actions. Using a turntable technology, Visual feedback is continuously provided though the use of screen of limited size.	A new step-in-place locomotion interface for virtual environment with large display system	NA:NA:NA:NA:NA:NA:NA:NA	2018
Horst Hörtner:Christopher Lindinger:Robert Praxmarer:Andreas Riedler	The ARS BOX is a projection-based (cave-like), PC-based VR system. It significantly reduces the time and money needed to develop and present Immersive Virtual Environment (IVE) applications while simultaneously expanding the options available compared to similar systems. A handheld PC serves as its interaction interface, making possible numerous innovative applications.	ARS BOX with palmist: advanced VR-system based on commodity hardware	NA:NA:NA:NA	2018
Michael Haller:Daniel Dobler:Philipp Stampfl	Augmented reality (AR) is not only a new type of computer entertainment, but it can also be used for serious applications. Because the user is directly involved in the virtual world, enhanced reality can be more engaging than traditional computer work. Especially for a better sound impression, AR could be a solution for many problems. The sound component is still missing in current AR applications, which combine live video and computer graphics to produce real-time visual effects.	Augmenting the reality with 3D sound sources	NA:NA:NA	2018
Hiroo Iwata:Hiroaki Yano:Hiromi Igawa	Audio Haptics is a rendering technique for auditory and haptic sensations. Sound and force are generated by using a physical model of virtual objects. We developed a software for real-time calculation of a physical model of virtual objects. A speaker is set at the grip of the haptic interface for spatial localization of the sound.	Audio haptics	NA:NA:NA	2018
Henry Newton-Dunn:Hiroaki Nakano:James Gibson	Block Jam is a musical interface controlled by the arrangement of 24 tangible blocks. By positioning the blocks (Figure 1), musical phrases and sequences are created, allowing multiple users to play and collaborate.	Block jam	NA:NA:NA	2018
Dave Warner:Matt Carbone	Cyberarium knowledge fountain is a opportunity: • To demonstrate socially responsible applications of communication technology • To provide a social exchange at which multiple communities can explore unconventional applications of advanced technological concepts • To identify key areas where information technologies can be effectively applied to improve quality of life • To encourage young minds to explore science and think about how to make the world a better place	Cyberarium knowledge fountain: center for really neat research	NA:NA	2018
Zack Butler:Robert Fitch:Keith Kotay:Daniela Rus	A robot designed for a single purpose can perform a specific task very well, but it may perform poorly on a different task, or in a different environment. This is acceptable if the environment is structured; however, if the task is in an unknown environment, then a robot with the ability to change shape to suit the environment and the required functionality will be more likely to succeed. We wish to create more versatile robots by using self-reconfiguration: hundreds of small modules will autonomously organize and reorganize as geometric structures to best fit the terrain on which the robot has to move, the shape of the object the robot has to manipulate, or the sensing needs for the given task. For example, a robot could synthesize a snake shape to travel through a narrow tunnel, and then morph into a six-legged insect to navigate on rough terrain upon exit.	Distributed systems of self-reconfiguring robots	NA:NA:NA:NA	2018
Patrick Baudisch:Nathan Good	Focus plus context screens are wall-size, low-resolution displays with an embedded, high-resolution display region. Focus plus context screens allow users to view details of a document up close, while simultaneously seeing peripheral parts of the document in lower resolution. Application areas range from geographic information systems to interactive simulations and games.	Focus plus context screens: visual context and immersion on the desktop	NA:NA	2018
Andrew Joel:Michael Brown:Almos Elekes	The Immersive and Interactive Rear-Projected Stereo DLP Reality Center aims to improve the quality of immersive visualization and increase the efficacy of seamless real-time interaction with complex stereoscopic data.	Immersive and interactive rear-projected stereo DLP reality center	NA:NA:NA	2018
Cindy M. Grimm:William D. Smart	Lewis is a (short) human-sized mobile robot that wanders through crowded rooms, taking pictures of people, much like a photographer at a wedding reception does. The goal is take high-quality, well-composed photographs of people non-intrusively, and to offer these pictures as keep-sakes of the conference.	Lewis the robotic photographer	NA:NA	2018
Hiroo Iwata:Hiroaki Yano:Motohiro Tsuzuki:Fumitaka Nakaizumi:Takayuki Yoshioka:Yutaka Miyakita	The NONA-Vision is a high-resolution, wide-angle video capture and projection system. The display is composed of nine rear-projection screens. Images for the nine screens are captured by a specialized camera-head, in which the optical centers of nine video cameras are located at the identical position.	NONA-vision	NA:NA:NA:NA:NA:NA	2018
Kiyoshi Kiyokawa:Hiroyuki Ohno:Yoshinori Kurata	There are several approaches to realize a 3D display that can be viewed by multiple co-located users, and each has its own pros and cons. A volumetric display utilizes afterimage effects, and it usually can only show transparent images within its volume. Some projection-based systems support independent viewpoints for more than two users [1], however, virtual objects can be shown only within a viewing frustum in front of the screen. On the other hand, head mount display (HMD) based Augmented Reality (AR) can show virtual objects at arbitrary locations [2]. Besides, an HMD can potentially show correct occlusion phenomena between virtual and real scenes. While a video see-through HMD severely degrades the quality of the real scene and adds a certain system delay, an optical see-through HMD keeps the intrinsic quality of the real scene. However, a virtual scene had to be a semi-transparent ghost due to a half-silvered optical combiner so far.	Occlusive optical see-through displays in a collaborative setup	NA:NA:NA	2018
Michael Meehan:Mary Whitton:Sharif Razzaque:Paul Zimmons:Brent Insko:Greg Combe:Ben Lok:Thorsten Scheuermann:Samir Naik:Jason Jerald:Mark Harris:Angus Antley:Frederick P. Brooks	A common metric of VE quality is presence --- the degree to which the user feels like they are in the virtual scene as opposed to the real world. Presence is important for many VE applications [Hodges et al. 1994]. Since presence is a subjective condition, it is most commonly measured by self-reporting, either during the VE experience or immediately afterwards by questionnaires. There is vigorous debate in the literature as to how to best measure presence [Meehan 2001].	Physiological reaction and presence in stressful virtual environments	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Cynthia Breazeal:Andrew Brooks:Matt Hancher:Josh Strickon:Cory Kidd:John McBean:Dan Stiehl	We have created an articulated robotic creature and situated it within an interactive terrarium to explore the aesthetic, expressive, and interactive qualities that give robots an organic and engaging presence to people.	Public anemone: an organic robot creature	NA:NA:NA:NA:NA:NA:NA	2018
Hiroto Matsuoka:Akira Onozawa:Hidenori Sato:Hisao Nojima	The vision of this work is to regenerate real objects that had existed at some moment in the past and/or at some remote location as if they have been transferred to the present across space and time. The objects could be museum pieces or items in stores, for example. For this work, we have developed a quick and fully automated system that can capture a three-dimensional image of real objects. This success brought us close to our goal of "regeneration of real objects in the real world."	Regeneration of real objects in the real world	NA:NA:NA:NA	2018
Hideyuki Ando:Takeshi Miki:Masahiko Inami:Taro Maeda	"Smart Finger" is a novel type of tactile display for Augmented Reality (AR) that is wearable, like a press-on fingernail. This device allows the user to feel various textures while tracing his or her fingers along smooth objects. This wearable AR interface can supplement bump mapping information to real objects.	SmartFinger: nail-mounted tactile display	NA:NA:NA:NA	2018
Joseph A. Paradiso:Che King Leo:Nicholas Yu:Marc Downie	We have developed a very simple retrofit to a large display surface that enables knocks or taps to be located and characterized (e.g., determining type of hit --- metallic tap, knuckle tap, or bash --- and intensity) in real time. We do this by analyzing the waveforms captured by 4 piezoelectric transducers (one mounted in each corner of the surface) and a dynamic microphone (mounted anywhere on the glass) in a digital signal processor. Differential timing yields the position, frequency content infers the kind of hit, and peak amplitude reflects the intensity. This technique was first explored in collaboration between Paradiso and Ishii [Ishii et. al. 1999] to make an interactive ping-pong table. Moving to glass display surfaces introduced significant problems, however --- knuckle taps are low-frequency impulses that vary considerably hit-to-hit, and the bending waves propagating through the glass are highly dispersive. A heuristically-guided cross-correlation algorithm [Paradiso et al. 2002] was developed to counteract these effects and provide spatial measurements that can resolve knuckle impacts to within σ = 2-4 cm (depending on the material thickness) across a 2-meter sheet of glass. As the requisite hardware is minimal, and everything is mounted on the inside sheet of glass, this is a very simple retrofit to, for example, store window displays, ushering in an entirely new concept of interactive window browsing, where passers-by can interact with information on the store's products by simply knocking. We have explored this concept in retail, where one of our trackers was installed on the main display window of an American Greetings store near Rockefeller Center in Manhattan for this year's Christmas-Valentine's Day season (right figure), and in museums (e.g., left figure, which shows the system running at the Ars Electronica Center in Linz, Austria).	The interactive window	NA:NA:NA:NA	2018
Oliver Bimber:Bernd Fröhlich:Dieter Schmalstieg:L. Miguel Encarnação	The Virtual Showcase is a new projection-based and application-specific Augmented Reality display that offers an innovative way of accessing, presenting, and interacting with scientific and cultural content. Conceptually, the Virtual Showcase is compatible with the conventional showcases used, for instance, by museums. However, it allows the display of computer generated 3D graphics and animations together with real artifacts within the same space. From the technological point of view, the Virtual Showcase provides perspective correct stereoscopic viewing for multiple users, high resolution, low parallax (reflected projection plane inside the showcase), and support for mutual occlusion between real and virtual objects.	The virtual showcase: a projection-based multi-user augmented reality display	NA:NA:NA:NA	2018
Kenji Tanaka:Junya Hayashi:Yutaka Kunita:Masahiko Inami:Taro Maeda:Susumu Tachi	TWISTER, (Telexistence Wide-angle Immersive STERe-oscope) is an immersive full-color autostereoscopic display, designed for a face-to-face telecommunication system called "mutual telexistence", where people in distant locations can communicate as if they were in the same virtual three dimensional space.	TWISTER: a media booth	NA:NA:NA:NA:NA:NA	2018
Damion Shelton:George Stetten:Wilson Chang	From the discovery of X-rays over a century ago, clinicians have been presented with a wide assortment of imaging modalities yielding maps of localized structure and function within the patient. Some imaging modalities are tomographic, meaning that the data are localized into voxels, rather than projected along lines of sight as with conventional X-ray images. Tomographic modalities include magnetic resonance (MR), computerized tomography (CT), ultrasound, and others. Tomographic images, with their spatially distinct voxels, are essential to our present work.	Ultrasound visualization with the sonic flashlight	NA:NA:NA	2018
Daijiro Koga:Takahiro Itagaki	Chanbara is the name of our game, which reproduces the original ancient art of "samurai sword fighting". For this work, we created a new force feedback device called "GEK12". The player holds the sword and wears an HMD. In Chanbara, the player walks around a virtual arena to battle against CG characters.	Virtual Chanbara	NA:NA	2018
Christopher Shaw	NA	Session details: Panels	NA	2018
Kurt Akeley:David Kirk:Larry Seiler:Philipp Slusallek:Brad Grantham	Ray-tracing produces images of stunning quality but is difficult to make interactive. Rasterization is fast but making realistic images with it requires splicing many different algorithms together. Both GPU and CPU hardware grow faster each year. Increased GPU performance facilitates new techniques for interactive realism, including high polygon counts, multipass rendering, and texture-intensive techniques such as bumpmapping and shadows. On the other hand, increased CPU performance and dedicated ray-tracing hardware push the potential framerate of ray-tracing ever higher.	When will ray-tracing replace rasterization?	NA:NA:NA:NA:NA	2018
Norman I. Badler:Nadia Magenat-Thalmann:Laurie McCulloch:Evan Marc Hirsch:Phil LoPiccolo	Computer graphics technology has progressed to the point where it is possible to create digital humans that are virtually indistinguishable from the real items. The potential benefits are immense, but there are implications to consider as well, in a range of applications that includes film, video, the Web, and gaming. This panel of experts from diverse disciplines of computer graphics will discuss how far we have come in the use of digital humans, where they are heading, and what they will mean to us.	Digital humans: what roles will they play?	NA:NA:NA:NA:NA	2018
Natalie Jeremijenko:Thecla Schiphorst:Michael Mateas:Wolfgang Strauss:Will Wright:Andruid Kerne	Interface ecology is an emerging metadisciplinary approach, in which the creation of rich interactive experiences spans n disciplines --- such as computer graphics, mathematics, gaming, visual art, performance, and cultural theory. Interfaces extend beyond interactive artifacts, activities, and social spaces, forming intricate ecosystems. Interfaces are the catalytic border zones where systems of representation meet, mix, and recombine. Through this recombination, interface ecosystems generate fundamental innovations of form, experience, knowledge, and technology. This panel brings together a diverse range of practitioners who work from concept to experience not in terms of a particular discipline, métier, or medium but with a practice that interconnects multiple systems, forming a whole.	Extending interface practice: an ecosystem approach	NA:NA:NA:NA:NA:NA	2018
David S. Ebert:Bill Buxton:Patricia Davies:Elliot K. Fishman:Andrew Glassner	Computer graphics research and hardware has matured as a field to the point that high-quality computer graphics is becoming ubiquitous. Computer graphics shortly will be where word processing is today: everyone uses it, but there are very few people doing basic research in word processing. All of the challenges lie in the applications and use of this technology to enable advances in many fields. This panel will combine experts in computer graphics and associated technology with experts from a few applications areas to discuss the possibilities and future ways that computer graphics can advance discovery in many fields.	The future of computer graphics: an enabling technology?	NA:NA:NA:NA:NA	2018
Michael Gleicher:Michael Cohen:Nancy Pollard:Jessica Hodgins:Michiel van de Panne	"Three-minute madness." Papers from the new ACM SIGGRAPH Symposium on Computer Animation are summarized in three minutes or less, followed by a discussion of new directions in computer animation research.	Symposium on computer animation in fast forward	NA:NA:NA:NA:NA	2018
Vincent Scheib:Theo Engell-Nielsen:Saku Lehtinen:Eric Haines:Phil Taylor	For 20 years, an underground movement has produced short real-time animations running on home computers. This group, the "demoscene," primarily consists of students who pursue their technical and artistic interests beyond the classroom, to create inspiring works of real-time art. These productions encompass a broad range of computer graphics techniques such as procedural geometry, real-time ray-tracing, and real-time shading. Game developers have been utilizing this talent pool yet it has little visibility in the SIGGRAPH community. This panel explores the demoscene, technical tricks used in demos, and how scene educational and creative aspects can contribute to the SIGGRAPH community.	The demo scene	NA:NA:NA:NA:NA	2018
Scott Elson:Eamonn Butler:Scott Clark:Carlos Saldanha	This panel seeks to demystify, debunk and goad the dialogue of 3D character animation. This panel is motivated by the sea change that is currently effecting our industry. In the last year and a half what was once a battle to get 3D into Hollywood has completely crossed over --- Hollywood can't get enough. 11 years ago the quote from top Disney management was "there will never be a digital character in a Disney feature film." Now traditional animators and artists are giving up their fear for pragmatism and embracing 3D in droves. What does that mean for those already working in 3D? What does it portend? What can we learn from the traditional animator?	Animation's turning tide	NA:NA:NA:NA	2018
Mark Ollila:Staffan Björk:Kevin Bradshaw:Steven Feiner:Kari Pulli	With the number of mobile devices exceeding PCs, research is required in many areas with respect to graphics and interaction. There are problems with interaction, streaming, graphics algorithms, bandwidth with current and future devices. This panel examines the state of the art from both an industrial and research point of view, and provides directions for future work in this area.	Unsolved problems in mobile computer graphics and interaction	NA:NA:NA:NA:NA	2018
Loren Carpenter:Brian Fisher:Richard A. May:Norbert Streitz:David J. Kasik	The world of display devices is expanding rapidly, both literally and figuratively. New commercial and research devices come in larger sizes (measured in meters, not inches) and different physical forms (e.g. rectangular surfaces, cylindrical segments, truncated spheres). Such expansion means that graphics and interactive techniques are becoming far more amenable to group activities and can display more and more data at once.	Graphics in the large: is bigger better?	NA:NA:NA:NA:NA	2018
Margaret S. Geroch:Evan Hirsch:Joan Staveley:Tom Tolles:Barb Helfer:Suba Varadarajan	The application of motion capture in the movie industry has continued to increase in the last couple of years, ranging from background action to major characters. Using motion capture in a production pipeline requires both motion capture and animation experience. An animator needs an understanding of biomechanics and how the body moves both for planning marker placement and to be able to apply motion capture to a character in such a way that it fits the character and the story development. For motion capture to be usable, it requires planning well in advance, knowing what needs to be captured and how it's going to be applied.	How does motion capture affect animation?	NA:NA:NA:NA:NA:NA	2018
Noah Wardrip-Fruin:Andrew Stern:Peter Molyneux:Michael Mateas:Bernard Yee	While pure theory can be interesting, this panel starts where the rubber meets the road for interactive stories --- real systems, and the practice-oriented insights of creators. Expect to hear concrete examples and solutions to hard problems, ranging from the technical to the artistic. Of course, there's also a lot of controversy about what interactive stories are, and how to best make them, which this panel doesn't plan to ignore. It includes people working on major systems that represent three different approaches, and puts them together with a moderator that knows the hot buttons (as well as the points of agreement). The three approaches can be called: narrative game, interactive drama, and massively multiplayer storytelling.	Interactive stories: real systems, three solutions	NA:NA:NA:NA:NA	2018
Bob Nicoll:Glenn Entis:Patrick Gilmore:J. C. Herz:Alex Pham:Will Wright:Ken Perlin	Driven by trends in silicon and software, computer gaming is the medium that will define the recreational and cultural experience of the twenty-first century in the way that motion pictures and their offspring television, defined the recreation of the twentieth. Or so we assert! This panel will debate the truth of the statement that gaming is the dominant medium of the future with believers, skeptics, and outside referees.	Games: the dominant medium of the future	NA:NA:NA:NA:NA:NA:NA	2018
Marc Barr	NA	Session details: sigKIDS	NA	2018
Daria Tsoupikova	Being interested in Interactive Animation, Illustration, Multimedia, and the creative use of programming languages, I explored these areas to produce an educational game for preschool children. The following four chapters present the concept and development of an experimental interactive game titled "Winter Dreams".	Interactive animation as an educational tool in "Winter Dreams"	NA	2018
Karen Monahan	Can games be educational as well as fun? Educators and children's media developers have long been trying to answer this question. Much of the software produced for kids and marketed as educational is comprised of the type of games that Seymour Papert has so adeptly defined as "edutainment".1 Many of these games are drill-type memorization exercises where the game activity has little or no relation to the "educational" component. Mainstream educational software might be fun, even engaging, but very little of it challenges the user intellectually or encourages open-ended discovery.	Gaming as an educational tool: internet scavenger hunt	NA	2018
Hilary J. Wright	FORM is an educational software prototype designed to encourage children's creative play with fundamental geometric shapes. The open-ended program serves as a creative tool to help form a deep and lasting understanding of the beauty, elegance and underlying unity of math, science, design and nature.	FORM	NA	2018
Satoko Moroi:Shinji Sasada:Ryoji Shibata	"The Floating Words" is a new device for playing with words. This installation provides the viewer with a new feeling: If you speak to the microphone, your voice drips into a water pool as water drops, and will begin to float. You can stir or ladle the "letters" with a stick or ladle. The installation of version_1 was exhibited at SIGGRAPH2001 Art Gallery, and it was succeeded very much. This time, we add new expression to new version: "The Floating Words for KIDS", and the expression is device to learn alphabet playing with the "The Floating Words". The expression is: If you ladle some letters with a ladle, they will turn to the characters showing word that has the alphabet in an initial. This project is a proposal of using this installation for learning alphabet for kids. These feeling: it seem like a magic are results from tricks of combination of voice recognition system, 3D magnetic sensor, simple computer graphics and real water and so on.	The floating words for kids: interactive installation for learning alphabet	NA:NA:NA	2018
James Dai:Michael Wu:Jonathan Cohen:Troy Wu:Maria Klawe	Whereas traditional research on collaborative educational systems has primarily focused on how to define better modes of digital interaction, this approach is found lacking when applied to developing collaborative systems for elementary school aged children. It is creatively and collaboratively restrictive to filter the enthusiastic interactions of these excited 12-year-old children through progressively more complicated GUIs. Current E-GEMS research examines design factors of educational systems that recognize and facilitate the social context of the classroom and hopes to encourage, rather than restrict, peer-to-peer social discussion and interaction. By correlating observed interactions in the digital domain with those in the social domain, we hope to shed light on design factors of collaborative systems that can be an integral and exciting part of a child's mathematical education. The vessel of our current research is the two-player collaborative mathematical exercise PrimeClimb, developed at E-GEMS. This paper describes the study conducted with PrimeClimb and documents the methodology of data capture and analysis used in the study -- methods borrowed from ethnography, education research and sociology.	Toys to teach: mathematics as a collaborative climbing exercise	NA:NA:NA:NA:NA	2018
Yuichiro Kamata	JOLLEE-MAIL PLAYGROUND is a simulated environment where children can experience the Internet communication in a more involving fashion.	Jollee-Mail Playground	NA	2018
Robert Dunn	The Virtual Dig is an interactive adventure for young people and provides an educational experience in archaeology and historical architecture. The web site was produced for the Israel Museum, Jerusalem, in collaboration with museum archaeologists and can be seen online at the addresses: "http://www.imj.org.il/arc-tel" and "http://www.arcv.org".	The virtual dig	NA	2018
DaShawn L. Hall:Kenneth Sakatani	Virtual Reality or VR is a computer-designed environment that allows the user to interact within the digital environment as he or she would in a real world situation. VR has been successfully used in game development, medical technology and flight simulation. Virtual Studio is a partial immersive, interactive virtual learning environment that is based on the metaphor of the artist studio. Its primarily use will be to teach elementary students about the techniques, styles and history of artist and art. Each part of Virtual Studio is named after major artists, who used or pioneered a particular style or technique. There are three core sections of Virtual Studio: 1) Architectural Design, 2) Sculptural Design, 3) Digital Painting and Color Theory.	Virtual studio: virtual reality in art	NA:NA	2018
Lori Scarlatos	Over the years, educators and government officials have searched for ways to improve learning in our schools, particularly in the areas of math and science. Many have come to recognize that collaborative activities, learning through play, and teacher guidance can help children to get over their initial fears and even begin to enjoy these subjects. Yet, at the same time, shrinking school budgets are making it harder to support these approaches to learning. Tangible Interfaces for Collaborative Learning Environments (TICLE) was conceived in response to this need [Scarlatos 2002].	An application of tangible interfaces in collaborative learning environments	NA	2018
Jonathan T. Blocksom	GollyGee Blocks™ is a 3D modeling program for children. Children use it to stack, transform, color and texture 3D objects in a 3D scene which can be viewed from any angle. GollyGee Blocks™ was designed as an educational, open-ended creativity tool for 3D graphics.	GollyGee Blocks™: a 3D modeler for children	NA	2018
Jacqueline Nuwame	Can oral storytelling live in a digital medium? Anansi's World of Folklore is a celebration of the art of storytelling, created as a broadband site for showcasing and collecting folktales. It was created to discover a way to give traditional oral storytelling a meaningful place on the Internet.	Anansi's world of folklore	NA	2018
Christopher Stapleton	The Immersive Jukebox offers users a choice of musical experiences exploring various influences of African American Blues music: traditional African music, spirituals, work songs, and others. Inspired by the curriculum of the International House of Blues Foundation (IHOBF)* Blues SchoolHouse program, it introduces students and teachers to the music, art and history of the blues and its cultural origins.	The ToyScouts' immersive jukebox: University of Central Florida	NA	2018
Douglas R. Roble	NA	Session details: Sketches and applications	NA	2018
Doug Cooper	DreamWorks Feature animated film "Spirit" takes new steps in the hybridization of 2D and 3D production techniques for Character Animation, even to the extent of using both approaches within the same shot.	2D/3D hybrid character animation on "Spirit"	NA	2018
Kotro Petri:Mannerkoski Olli:Lesonen Hannu:Lustila Risto	Digital media convergence is creating a vast new parallel universe, the digital dimension, where the range of available services is growing exponentially. Different forms of media and communication are integrating into consumer devices. This means new challenges for information user interfaces, especially from the point of view of user experience. The sketch introduced here is a functional prototype, a proof-of-concept, of a new kind of graphical user interface for interactive television. It integrates different services for television, Internet, radio, personal communication and games under one user interface.	3D browser for interactive television	NA:NA:NA:NA	2018
Huirong Han:Juli Yamashita:Issei Fujishiro	Is a 2-dimensional (2D) force feedback device capable of presenting 3-dimensional (3D) shapes? The answer is a qualified "yes." "Force shading", a haptic counterpart of bump mapping in computer graphics, presents a non-flat shape on a nominally flat surface by varying the force vector direction in haptic rendering [Morgenbesser and Srinivasan 1996][Robles-De-La-Torre and Hayward 2001]. To our knowledge, such phenomena have been qualitatively measured only by 3D devices, and a quantitative comparison to 2D devices has not been made. We compare thresholds of human shape perception of the plane experimentally, using 2D and 3D force feedback devices.	3D haptic shape perception using a 2D device	NA:NA:NA	2018
Kevin Thomason:Robert V. Cavaleri	This sketch presents how the 3D Layout Department at Blue Sky Studios dealt with the production pipeline issues of transforming their first feature film Ice Age from sequences of hand-drawn storyboards into blocked out, ready-to-animate scenes in 3D for the Animation Department. To achieve this, custom tools were developed to automate sequence set-up and distribution of shots, in order to minimize the artists' concern with data management and maximize the creative time available to accommodate revisions in the story.	3D layout and propagation of environmental phenomena for Ice Age	NA:NA	2018
Arunachalam Somasundaram:Rick Parent	A consumer-grade camera is used to view the motion of individuals moving in an area, such as that observed in the lobby of a building. By assuming individuals are engaged in fairly normal walking behavior, we can extract sufficient information to synthetically reconstruct the scene using simple image processing techniques. The reconstructed scene can then be viewed from any angle and used to track interesting individuals. Features of the person such as shirt and pants color can be extracted from the video and applied to the synthetic model thus allowing the real individual to be recognized from viewing the reconstruction. The technique proposed has the advantage of using a single camera and would find applications in gait analysis and security. This is a work in progress and we present some initial results of a single moving figure.	3D reconstruction of walking behaviour using a single camera	NA:NA	2018
Patrick Min:Joyce Chen:Thomas Funkhouser	This sketch describes our experiences with creating query interfaces for an online search engine for 3D models. With the advent of affordable powerful 3D graphics hardware, and the improvement of model acquisition methods, an increasing number of 3D models is available on the web, creating a need for a 3D model search engine [Paquet and Rioux 2000; Suzuki 2001]. An important problem that arises for such an application is how to create an effective query interface. To investigate this issue, we created several different query interfaces, and tested them both in controlled experiments as well as in a publicly available 3D model search engine.	A 2D sketch interface for a 3D model search engine	NA:NA:NA	2018
Masaki Oshita:Akifumi Makinouchi	In this sketch, we present a middleware for computer games that has the ability to realize dynamic motion control of characters. The specific novelty of the middleware is to produce dynamically changing motions in response to physical interaction between the character and environments such as collision impulses and external forces as shown in Figure 1.	A dynamic motion control middleware for computer games	NA:NA	2018
Alexandre François:Elaine Kang:Umberto Malesci	This sketch presents the design and construction of a handheld virtual mirror device. The perception of the world reflected through a mirror depends on the viewer's position with respect to the mirror and the 3-D geometry of the world. In order to simulate a real mirror on a computer screen, images of the observed world, consistent with the viewer's position, must be synthesized and displayed in real-time. Our system is build around a flat LCD screen manipulated by the user, a single camera fixed on the screen, and a tracking device. The continuous input video stream and tracker data is used to synthesize, in real-time, a continuous video stream displayed on the LCD screen. The synthesized video stream is a close approximation of what the user would see on the screen surface if it were a real mirror.	A handheld virtual mirror	NA:NA:NA	2018
Barrera Salvador:Hiroki Takahashi:Masayuki Nakajima	Presently, most virtual reality systems use upper body parts to interact with objects in the virtual environment. This situation is caused by technological limitations of current interface devices. Starting from this viewpoint we developed a new interface for detecting ankle motions relative to the knee. We believe that hands-free navigation, unlike the majority of navigation techniques based on hand motions, has the greatest potential for maximizing the interactivity of virtual environments since navigation modes are more direct motion of the feet.	A new interface for the virtual world foot motion sensing input device	NA:NA:NA	2018
Juan Buhler:Dan Wexler	"Bokeh" is a Japanese word used to describe the quality of the out-of-focus areas as rendered on film by a physical lens. In areas that are out of focus, the circle of confusion (i.e., the distribution of light absorbed onto film from a single point of light in the scene) is bigger than in areas that are focused. Different lenses distribute light within the circle of confusion differently, depending on a number of factors, like the shape and number of diaphragm blades and the optical design of the lens itself. For example, lenses that are designed to correct for optical aberrations tend to render a circle of confusion that is more intense on the edges and slightly less so in the center. Other lenses, like the so called "mirror" lenses, which include reflective elements besides refractive ones, typically produce donut-shaped circles of confusion since some of the light paths are cut from the center by the reflective element. Moreover, in a real lens the circle of confusion sometimes becomes elliptical on the areas outside of the center of the image, with the shorter axis of the ellipsis oriented radially from it. Standard rendering techniques model depth of field using point sampling techniques which converge towards a bokeh model with uniform density across the circle of confusion. This represents an idealized lens that does not exist in reality. Real lenses have non-uniform bokeh distributions. For example, a smooth, or "creamy" lens will have a Gaussian distribution across the circle of confusion. Real lenses may have different distributions at different points on the film plane. Our technique allows the animator to specify an arbitrary probability density function to represent the distribution of intensity within the circle of confusion. The probability curve is used by the renderer to jitter the location of the sample point on the lens. For example, if the density function is a Gaussian, more samples will be taken towards the center of the lens than at the edges. Conversely, for a density function that simulates a mirror lens, fewer samples are taken near the center of the lens because the mirror blocks light as it moves through the lens. By generating enough sample points, the model converges on the true bokeh density function. We will also show the effect of specifying two shapes for the light distribution, one for close and the other for far focused areas, and the deformation of the circle of confusion so it becomes elliptical on the edges of the image. Future work includes the specification of the diaphragm shape, and the possible implementation of this technique as a particle renderer, which would provide efficient sampling at a much lower cost. Additional work to add support for stratified sampling is also possible.	A phenomenological model for bokeh rendering	NA:NA	2018
Maria A. Alberti:Dario Maggiorini:Paola Trapani	In the design of multimedia communication artifacts few, if any, tools support the early stage of a creative process: the heuristic project. In this work we give a proof of concept of an application addressed to a specific kind of heuristic project: given the logical sequence of episodes of a narrative, the fabula, the goal is to obtain different plots expressed in multi-modal language. The case study is provided by the task of transposing a written synopsis to the multi-modal language of a movie. We adopted the semiotic theory of Greimas to analyze the narrative and reveal its deep structure. The application enables users to interact with this structure in order to simulate and anticipate the effects of meaning resulting from their manipulation.	A semiotic approach to narrative manipulation	NA:NA:NA	2018
Alan Price:Dan Bailey	This sketch presents how the real-time 3D interactive simulation, "The Virtual Tour of the Cone Sisters' Apartments", from the SIGGRAPH 02 Art Gallery exhibition was designed, authored and produced by the Imaging Research Center (IRC) at UMBC for two installations at the Baltimore Museum of Art (BMA).	A virtual reconstruction of the Cone sisters' apartments	NA:NA	2018
Pieter Peers:Philip Dutré	Image-based relighting represents a class of techniques that apply new lighting conditions to a scene, given a set of basis images. In this sketch we present a relighting technique that, for a single viewpoint, accurately captures the reflectance field of objects, without restrictions on their geometrical complexity or material properties. Once the reflectance field is captured, the objects can be relit under arbitrary lighting conditions. To achieve such accurate results, our method combines the strengths of both the Light Stage [Debevec et al. 2000] and environment matting [Zongker et al. 1999; Chuang et al. 2000] into a single framework.	Accurate image based re-lighting through optimization	NA:NA	2018
Askold Strat:Manuel M. Oliveira	We describe the implementation of a portable 3D-camera prototype based on a consumer grade digital camera and an inexpensive laser raster generator. Such hand-held device can be used to capture smooth shapes by acquiring one or more images. Its output can be either a 3D wireframe or a textured model.	An inexpensive 3D camera	NA:NA	2018
Taly Sharon	We describe here two installations of direct manipulation systems in the art and education domain. In these installations we use a pressure sensitive computer-projected canvas for user manipulation. As the user presses the canvas, an art piece is created, or image layers are revealed.	Art and education using direct manipulation of a sensor array	NA	2018
Daniel Dobler:Michael Haller:Philipp Stampfl	This sketch describes the Mixed Reality application ASR (Augmented Sound Reality) which uses the overlay of virtual images on the real world to support the placement of three dimensional sound sources. Our system allows to place sound sources in a virtual or real room with the advantage of feeling, seeing and hearing them. This implies a more intuitive and better feeling of space and 3D sound.	ASR: augmented sound reality	NA:NA:NA	2018
Xiaoyang Mao:Yoshinori Nagasaka:Atsumi Imamiya	Line Integral Convolution (LIC)[Cabral and Leedom 1993]] is a texture based vector field visualization technique. Why using LIC for pencil drawing generation? Let us look at the two images shown in Figure 1. Figure 1(a) is a digitized sample of a real pencil drawing. Look over it, we can perceive the traces of parallel pencil strokes and a gray scale tone built with the strokes. If we look at any local area of the image, however, we can find that the direction of strokes and the intensity of pixels vary randomly. The variance of intensity results from the interaction of lead material and drawing paper. The LIC image shown in Figure 1(b), however, presents the very similar features. Since an LIC image is obtained by low-pass filtering a white noise along the streamlines of a vector field, we can see traces along streamlines. On the other hand, the intensities of pixels within any local area vary randomly as the input image is a white noise. Such similarity suggests us that we can imitate the tone of pencil drawings with an LIC image.	Automatic generation of pencil drawing using LIC	NA:NA:NA	2018
Liselott Brunnberg:Mark Ollila	This sketch presents a prototype developed as part of the Backseat gaming project. The aim of the project is to explore how to make use of mobile properties for developing compelling and fun game experiences. The prototype is developed for use in a highly mobile situation, that of a car passenger and is realized by the use of mobile devices and the users physical location during speed to merge the virtual content and surrounding road context into an augmented reality game. In this research, in addition to location, we also introduce variables such as speed, direction, timing, changing surrounding, fast movement of manipulative objects and multiple entry and exits.	Backseat gaming: augmented reality with speed	NA:NA	2018
Diane Gromala	BioMorphic Typography is a new conception of writing and a morphing typeface driven by biofeedback. It enables users to become aware of their autonomic physiological functions while they type, in real-time. In doing so, BioMorphic Typography seeks to challenge longstanding Western notions about the relationship among the senses, representation, and technology.	BioMorphic typography	NA	2018
Xiaohuan Corina Wang:Cary Phillips	This sketch describes a new method for deforming the skin of a digital character around its skeleton by computing statistical fit to an input training exercise. In this input, the skeleton and the skin move together, by arbitrary external means, through a range of motion representative of what the character is expected to achieve in practice. Using least-squares fitting techniques, we compute the coefficients, or "weights," of our new deformation equation. The result is that the equation, which is compact and efficient to evaluate, generalizes the motion represented in the input. Once the training process is complete, even characters with high levels of geometric detail can move at interactive frames rates.	Body building through weight training: using fitting techniques for skin animation	NA:NA	2018
Tobias Skog:Sara Ljungblad:Lars Erik Holmquist	The field of display technology is rapidly developing, and LCD-and plasma-displays are already invading our surroundings. Alternative technologies such as "electronic ink", electro-luminescent materials, and even color-changing textiles [Holmquist and Melin 2001] will further increase the number of possibilities to integrate computer graphics in our everyday lives. We believe that computer graphics for everyday life will have requirements that are very different from those of a Web page or a movie special effect. To explore this, we have developed a type of applications that anticipates a future use of computer graphics, so-called Informative Art.	Bringing computer graphics to everyday environments with informative art	NA:NA:NA	2018
Michael J. Lyons:Gert J. Van Tonder:Ian Shortreed:Nobuji Tetsutani	Zen gardens exhibit sophisticated visual designs achieved with minimal compositions and engender a calm, contemplative atmosphere. Here we use perceptual models to study Zen garden design with the aim of discovering guidelines for the creation of calm visual spaces.	Calming visual spaces: learning from Kyoto Zen gardens	NA:NA:NA:NA	2018
Jay Riddle	This technical sketch surveys the wide variety of projection techniques that are employed in today's video and computer games. We will discuss the evolution of the "game view" and the reasons behind the choice of perspective, some of which are practical, while others are legacies. We will also explore the limits inherent in choosing a particular technique.	Cameras and point-of-view in the gamespace	NA	2018
Doug Cooper	The opening shot in "Spirit" runs a full 3 minutes without scene cuts, and introduces the audience to the breathtaking scenery of the mythic Old West. This elaborate scene was accomplished with every possible technique by mixing traditional drawing and painting art forms with 3D environments, effects, and digital characters. This Sketch will attempt to summarize some of the key challenges in producing this scene, which took over two years to complete.	Challenges of the homeland pan in "Spirit"	NA	2018
Youngha Chang:Suguru Saito:Masayuki Nakajima	Each painter renders a painting in her own style. This style can be distinguished by looking at elements such as motif, color, shape deformation and texture. Previously, [Hertzmann et al. 2001] suggested a method for applying the texture of an image to a photograph. In this paper, we will focus on the element of "color".	Color transformation based on the basic color categories of a painting	NA:NA:NA	2018
Daisuke Goto:Junichi Hoshino	In this sketch, we propose a new technique for computer generated clay animation. Unlike the traditional approaches based on physical simulations, we focus on generating various animation effects produced by the clay animator.	Computer generated clay animation	NA:NA	2018
Koh Kakusho:Yutaka Minekura:Michihiko Minoh:Shinobu Mizuta:Tomoko Nakatsu:Kohei Shiota	This sketch describes three dimensional (3D) computer graphics (CG) produced to illustrate the development of a human embryo for education in embryology, which is one of the basic subjects in professional medical education (Fig. 1). Although similar CG have already been produced for TV programs, they are insufficient in precision for professional medical education.	Computer graphics to illustrate the development of a human embryo for professional medical education	NA:NA:NA:NA:NA:NA	2018
Kyle Odermatt:Chris Springfield	Since the first color short, Walt Disney animated films have been known for their beautiful and breathtaking environments that seamlessly integrate with the hand drawn characters that populate them. Over the years, technological innovations have streamlined the process of creating these environments while still maintaining the extremely high artistic standards that have made our films famous. Other advances, like the multi-plane camera, have helped to revolutionize how the camera moves through these environments. In general, such technological innovations are inspired by an artistic need which must be achieved with out being prohibitively expensive. On Walt Disney's "Treasure Planet", new innovations to the DeepCanvas renderer were inspired by such artistic needs and desires.	Creating 3D painterly environments for Disney's "Treasure Planet"	NA:NA	2018
Brian Goldberg	Our team was tasked with the project of creating a full-screen, completely believable, CG stunt double for a well-known motion picture star. Successful project completion required adapting and applying many years worth of computer graphics research in a new and novel way. Most importantly, development had to be done within usual production constraints---ultimately guaranteed results, known pipeline, quick turnaround on tests, and a final product that would rival any work in this genre ever attempted.	Creation of a photo-real CG human	NA	2018
Fumio Matsumoto:Akira Wakita	CT is a project to reconstruct an existing urban space as a 3D information city on the web. A visitor can browse the city with "building wall browsers" and communicate with other visitors.	CT (city tomography)	NA:NA	2018
Johannes Hirche:Alexander Ehlert	Displacement Mapping [Cook 1984] is commonly used in commercial rendering software for adding surface detail. In contrast to Bump Mapping not only the normal is perturbed, the surface is modified as well. A given point P on a surface is displaced to a new point P' in the direction of the surface normal of the point P, N scaled with the value stored in the Displacement Map:	Curvature driven sampling of displacement maps	NA:NA	2018
Bonny Lhotka	In working with custom designs for digital imaging on textiles, source images and garment patterns are scanned and the images mapped to fit the pattern design in Adobe Photoshop. For these pieces the images were scanned on the Eclipse 48" x 96" flatbed scanner.	Custom designs for digital imaging on textiles	NA	2018
Gerhard Kurka	Densely occluded regions containing many stacked objects along the line of view generally show a high local depth-complexity (DC). With respect to occlusion culling such regions require dense occluder-sets, in order to provide gap free occlusion. Using pure size or distance based selection heuristics fail to consider such regions. This sketch presents a novel dynamic occluder selection approach, which selects occluders that cover regions of high DC. Depth-Complexity Based Occluder Selection [Kurka 2001] is based on the evaluation of a low resolution (128x128 pixel) raw scene depiction (RSD), which is rendered once per frame by a simple but fast image-based rendering (IBR) technique.	Depth-complexity based occluder selection	NA	2018
Guangzheng Fei:Nadia Magnenat-Thalmann:Kangying Cai:Enhua Wu	Instead of generating an initial in-core model in first pass and performing a second pass adaptive processing over the original model according to the detail analysis of the in-core model, in this paper we propose a single-pass algorithm to realize detail calibration by introducing a scheme of interlaced sampling in order to obtain higher efficiency.	Detail calibration for out-of-core model simplification through interlaced sampling	NA:NA:NA:NA	2018
Patrick Dalton:Rob Rosenblum:Shyh-Chyuan Huang:Lawrence Lee:Hank Driskill	Our goal was to create realistic digital pyrotechnic effects for the film "Reign of Fire". The specific effects we needed were fire, smoke, fog, flying embers and dust kicked up from dragons flying overhead. We also needed the effects to interact with the environment as well as the dragons. To make the challenge more difficult, we realized that it wasn't good enough to create real world physical interactions; the film needed imagery that was larger than life and exagerated. We needed to be able to art direct the simulations.	Digital pyro for Reign of Fire	NA:NA:NA:NA:NA	2018
Masayuki Takemura:Yuichi Ohta	Eye-contact plays a very important role in the face-to-face human communication. In the shared augmented reality space, however, the eye-contact between two people is blocked by the head-mounted displays. An HMD is a necessary device for merging the virtual and real worlds to realize the augmented/mixed reality. [Ohta and Tamura 1999] As a side effect, it covers the user eyes. In a shared AR/MR space, multiple users want to share the real world as well as the virtual objects. However, the eye-contact information in the real world is dropped because of the HMDs.	Diminishing head-mounted display for shared augmented reality	NA:NA	2018
Koji Mikami:Toru Tokuhara:Mitsuru Kaneko	In this sketch, we will demonstrate a video storyboard tool targeted for 3D computer animation. Our tool provides limited but specialized functions of standard 3D computer graphics software, focusing on ease of scene construction, camera control and the ability to preview in realtime. This allows for quicker and easier creation of video storyboards over existing approaches.	Diorama engine: a 3D video storyboard editor for 3D computer animation	NA:NA:NA	2018
Mashhuda Glencross:James Marsh:Jon Cook:Sylvain Daubrenet:Steve Pettifer:Roger Hubbold	DIVIPRO is a prototype tool for simulating the assembly and disassembly of mechanical engineering components. It is aimed particularly at situations where the responsibility for decision making is shared between geographically dispersed design teams, and provides a collaborative environment in which different team members concurrently visualize and manipulate models. It is currently being evaluated using models from the aerospace and medical industries.	DIVIPRO: Distributed Interactive VIrtual PROtotoyping	NA:NA:NA:NA:NA:NA	2018
Leonie Schäfer:Elaine M. Raybourn:Amanda Oldroyd	This sketch describes DocuDrama, a tool that offers a generation of interactive narratives that are based on activities in a collaborative virtual environment. DocuDrama [Schäfer et al. 2001] is built as part of TOWER [2002], a Theatre of Work Enabling Relationships, which allows project members to be aware of project relevant activities as well as to establish and maintain the social relationships that intensify team coherence.	DocuDrama conversations	NA:NA:NA	2018
Ernest J Petti:Thomas V Thompson, II:Adolph Lusinsky:Hank Driskill	In recent years there have been several movies starring creatures with scaled surfaces. Among these are Jurassic Park, Dragonheart, and Lake Placid. The surfaces of these creatures have generally been constructed by layering painted textures atop displacement maps. This gives the model texture, but the scales stretch and shrink under the movement of the creature, giving a rubbery look that is not realistic.	Dragon scales: the evolution of scale tool for Reign of Fire	NA:NA:NA:NA	2018
Ivan Poupyrev:Shigeaki Maruyama	There is a sense of satisfaction using a pen or pencil to write or draw. Feeling the imperfections on the paper as the pen tip moves over it and observing marks emerge create the inherently physical and intimate feeling of drawing. Our extensive discussions with artists and designers suggest that this physicality and intimacy with drawing not only brings enjoyment, but also perhaps assist in the artist's creative process.	Drawing with feeling: designing tactile display for pen	NA:NA	2018
Carlos Gonzalez-Ochoa:David Eberle:Rob Dressel	The challenge of animating believable dragons was presented to Disney Feature Animation and The Secret Lab (TSL) for the film "Reign of Fire". The film called for 100ft creatures with wing spans of 300ft that could undergo enormous speeds and accelerations. The artistic direction required each dragon to have wings that transition between a variety of physical behaviors and interact with the environment. To solve this challenge the Muscle and Skin system used in Disney's Dinosaur [Eskuri 2000] was extended with a variety of controls to do physical simulation. In this sketch we discuss some of the issues encompassing the creation of this simulator and give an overview of the successful and unsuccessful paths taken during its development.	Dynamic simulation of wing motion on "Reign of Fire"	NA:NA:NA	2018
Jimmy Chim:Hyunsuk Kim	This sketch presents a solution for creating expressive computer facial animation using off-the-shelf software. We will describe how to apply dynamic skin deformation on a CG character using Maya Cloth and how to create artist-friendly animation controls. Historically, facial animation with sophisticated skin deformation based on physical characteristics of skin could only be done using proprietary software in big studios, which are not accessible to small studios, independent animators and students. The solution that we have developed is easy to understand and does not involve any programming knowledge.	Dynamic skin deformation and animation controls using maya cloth for facial animation	NA:NA	2018
Adam Burr:Ross Scroble	This sketch explains how the Animation Team at Blue Sky Studios executed a challenging sequence for "Ice Age" and describes in detail a particularly useful animation software tool called followThrough.	Dynamics and dodos: rigging and animation methods for Ice Age	NA:NA	2018
Gianfranco Doretto:Stefano Soatto	This technical sketch presents a simple and efficient algorithm for editing realistic sequences of images of dynamic scenes that exhibit some form of temporal regularity. Such scenes include flowing water, steam, smoke, flames, foliage of trees in wind, crowds, dense traffic flow etc. We call this kind of scenes dynamic textures.	Editable dynamic textures	NA:NA	2018
Bill Hill	Embodied Interaction, explores the components of interactive media, with a primary focus on the role that interaction design has on virtual experiences. My electronic artwork is concerned with the transformation of the human species, specifically its biological components and its behavioral characteristics. This transformation or evolution is an environmental reaction to the manifestations of science and technology. This presentation examines the need to address the physical body and how the action of users needs to be interconnected with the interface and content of a interactive piece. From the development of opaque sculptural input devices to the use of transparent technologies my interactive installations seeks to examine the process of conditioning users; their predetermined interaction; and the physicality of computing.	Embodied interaction	NA	2018
Alan Kapler	High-resolution voxel rendering is no longer the pipe dream it once was. In the age of multi-gigabyte RAM, voxels are finding an ever-increasing role in creating cutting-edge visual effects. Digital Domain has developed a unique, animator-friendly way of working with voxels that is revolutionizing the way volumetric effects are done.	Evolution of a VFX voxel tool	NA	2018
Hiroshi Mori:Jun'ichi Hoshino	Generating realistic human motions from sparse pose descriptions are important for computer animation and virtual human applications. In this sketch, we propose an example-based motion interpolation technique using independent component analysis (ICA). The proposed method is also useful for human motion conversion, motion blending, and converting symbolic descriptions to continuous motion.	Example-based interpolation of human motion	NA:NA	2018
Ian Mackinnon	This sketch concerns the development of a renderer which takes advantage of the two dimensional nature of the frames it produces. The project is chiefly inspired by the way in which artists have used 2D abstraction in the past, from scribbled cartoons to cubist paintings. It would seem that exploiting the plane of the canvas can help us to make more interesting compositions.	Exploiting screen space	NA	2018
James W. Davis:Vignesh S. Kannappan	Given a single motion-capture sequence of a person performing a dynamic activity at a particular intensity (or effort), our goal is to automatically warp that movement into a natural-looking exaggerated version of that action. Consider warping a movement of a person lifting a lightweight box to make the movement appear as if the box were actually very heavy. We describe an efficient data-driven approach applicable to animation re-use that learns the underlying regularity in an action to select the most "expressive" features to exaggerate. Other "style-based" approaches are presented in [Gleicher 1998; Brand and Hertzmann 2000; Vasilescu 2001].	Expressive features for movement exaggeration	NA:NA	2018
Tazama U. St. Julien:Chris D. Shaw	The Firefighter Training Simulation is a virtual environment being developed at Georgia Tech in collaboration with the Atlanta Fire Department. The VE user is a commanding officer trainee who instructs teams of virtual firefighters to perform different actions to help put out virtual fires. This simulation was developed using the Simple Virtual Environment (SVE) Library, an extensible framework for building VE applications [Kessler 2000].	Firefighter training virtual environment	NA:NA	2018
Samantha Krukowski	A painting is a landscape of possibilities (fig.1) - a form field, a material experiment, a background or foreground, a place of play and imagination. Photographs of people looking at paintings reveal them looking into them, finding things in them that were never consciously put down, never put on the canvas. The way people look at paintings requires a painter to remember the world outside of the work that it, itself, points to. Not only is the materiality of the painted field interesting--its image, vibrant hues and liquid surfaces. What is increasingly interesting, given the possibilities of expanded visualization systems, is how a painting might move into and out of itself, towards and away from its material existence, based on what may or may not be found in its home medium.	Folded: negotiating the space between real and virtual worlds	NA	2018
Timothy Chen:Sidney Fels:Thecla Schiphorst	We have created a new interactive experience piece called Flow-Field. Participants touch and caress a multi-point touchpad, the MTC Express, in a CAVE (CAVE Automatic Virtual Environment), directly controlling a flowing particle field. Collisions in the particle field emit musical sounds providing a new type of musical interface that uses a dynamic flow process for its underlying musical structure. The particle flow field circles around the participant in a cylindrical path. Obstructions formed by whole hand input disturb the flow field like a hand in water. The interaction has very low latency and a fast frame rate, providing a visceral, dynamic experience. In FlowField, participants explore interaction through caress, suggesting reconnection with a sense of play, and experiencing a world through touch.	FlowField: investigating the semantics of caress	NA:NA:NA	2018
Markus Kurtz:Greg Duda	This sketch explains the technical and artistic techniques used by Digital Domain to produce the Ford of Bruinen sequence in New Line Cinema's Lord of the Rings.	Foamy creatures: Digital Domain wrangles whitewater for "Lord of the Rings"	NA:NA	2018
Saty Raghavachary	This sketch describes a way of generating realistic cracks and fragments to visually simulate brittle fracture on polygonal surfaces.	Fracture generation on polygonal meshes using Voronoi polygons	NA	2018
L. Streit:W. Heidrich	Feathers, like hair and fur, dramatically alter the appearance of a surface. However, unlike hair and fur, feathers have a wide range of colours and patterns and a well-defined branching structure. The variety of individual feather structures and patterns contribute to the surface appearance. Thus, it is important to model the complete range of feather types found in a feather coat. Since a feather coat can consist of thousands of structurally unique feathers, it is desirable to automatically generate most of the feathers, while maintaining intuitive control over the coat design and the creation of a wide variety of feather types. Previous work on feathers [Dai et al. 1995] does not address the intuitive generation of such a coat.	Generating feather coats using Bezier curves	NA:NA	2018
Tamotsu Machida	Display technology has been advancing every year. However, the topologically speaking, almost all of these use the same "Planar" surface for their displays. The topology of our display, to be introduced here, is completely different, in that we use "Spherical" surface. GEO-COSMOS, the world's first full color spherical display, is truly remarkable. It is also the main exhibit of the National Museum of Emerging Science and Innovation, opened in Tokyo on July 10, 2001. As shown in Figure 1 and 2, the images/movies on this display are intended for the viewers from all directions.	GEO-COSMOS: world's first spherical display	NA	2018
Koen Meinds:Bart Barenbrug:Frans Peters	Texture mapping is a standard feature of 3D graphics systems. To avoid aliasing artifacts, proper filtering is mandatory. We have developed a novel algorithm for texture mapping and filtering that is suited for hardware implementation. To eliminate texture aliasing artifacts, our algorithm uses higher order FIR filters that are know from digital signal processing. Compared to current state-of-the-art "anisotropic level 8 filtering", that can be found in commercially available advanced graphics accelerators, our method produces higher image quality at equal costs.	Hardware-accelerated texture and edge antialiasing using FIR filters	NA:NA:NA	2018
Michael Kazhdan:Thomas Funkhouser	With the advent of the world wide web, the number of available 3D models has increased substantially and the challenge has changed from "How do we generate 3D models?" to "How do we find them?" In this sketch we describe a new 3D model matching and indexing algorithm that uses spherical harmonics to compute discriminating similarity measures without requiring repair of model degeneracies or alignment of orientations. It provides 46-245% better performance than related shape matching methods during precision-recall experiments, and it is fast enough to return query results from a repository of 20,000 models in under half a second.	Harmonic 3D shape matching	NA:NA	2018
Cynthia Beth Rubin:Daniel F. Keefe	Hiding Spaces is an immersive VR Cave artwork which pushes past the limitations of physical media by exploring the new ambiguities that can delight the viewer in the virtual world. By using innovative tools developed especially for creative work within the Cave environment, in combination with more established digital methods and artistic practice, the authors collaborated to produce a work which transgresses the usual borders of 2D and 3D, including those that are common even in VR environments.	Hiding spaces: a CAVE of elusive immateriality	NA:NA	2018
Bill Spitzak	Most rendering software today destroys their accurate lighting and shading calculations by doing an inaccurate linear conversion to a screen image. This sketch presents a technique that quickly converts floating point data to a screen image while preserving the correct brightness levels and original detail.	High-speed conversion of floating point images to 8-bit	NA	2018
Dan Maynes-Aminzade:Beng-Kiang Tan:Ken Goulding:Catherine Vaucelle	This sketch presents Hover, a device that enhances remote telecommunication by providing a sense of the activity and presence of remote users. The motion of a remote persona is manifested as the playful movements of a ball floating in midair. Hover is both a communication medium and an aesthetic object.	Hover: conveying remote presence	NA:NA:NA:NA	2018
David Esneault:Mitch Kopelman:Jodi Whitsel	At Blue Sky Studios, two overriding principles have guided the development of the renderer and production lighting tools: 1) the lighting model should be as physically accurate as possible, and 2) be straightforward and easy to use so that the computers take care of the technical work leaving the artists free to concentrate on the creative aspects of lighting a scene. Blue Sky's proprietary renderer, CGIStudio™, has one of the most robust lighting models in the industry. The renderer's realistic approach to how light actually behaves in the real world lets the artist add details like soft shadows, reflections, and radiosity at the flip of a switch. Artists are able to achieve complex and subtle lighting with relatively simple lighting rigs, allowing them to get the image as 'right' as possible in the original render.	How a CSG-based raytracer saves time: lighting and scripting for Ice Age	NA:NA:NA	2018
Ari Rapkin	This sketch presents how cloth simulation is used at Industrial Light + Magic to create clothing for digital characters in Star Wars: Episode 2 and other films. Many costumes have to match complex physical costumes worn by live actors in other shots. Digital costumes have not only to look the same as their physical counterparts, but also move identically in response to their wearers' movements and environmental influences..	How to dress like a Jedi: techniques for digital clothing	NA	2018
Simon Gibson:Jon Cook:Toby Howard:Roger Hubbold	The ICARUS system is a suite of software packages, developed at the University of Manchester, that allows geometric models to be quickly and easily reconstructed from image and video sequences captured with uncalibrated digital cameras. The system combines automatic and semi-automatic camera calibration algorithms with an easy-to-use interactive model-building phase (Figure 1). Surface textures are automatically extracted from images and mapped onto the reconstructed models.	ICARUS: interactive reconstruction from uncalibrated image sequences	NA:NA:NA:NA	2018
Yonatan Wexler:Andrew W. Fitzgibbon:Andrew Zisserman	Environment matting is a powerful technique for modelling the complex light-transport properties of real-world optically active elements: transparent, refractive and reflective objects. Zongker et al [1999] and Chuang et al [2000] show how environment mattes can be computed for real objects under carefully controlled laboratory conditions. However, for many objects of interest, such calibration is difficult to arrange. For example, we might wish to determine the distortion caused by filming through an ancient window where the glass has flowed; we may have access only to archive footage; or we might simply want a more convenient means of acquiring the matte.	Image-based environment matting	NA:NA:NA	2018
Da Young Ju:Jin-Ho Yoo:Kyoung Chin Seo:Gregory Sharp:Sang Wook Lee	Visual impressions from two-dimensional artistic paintings greatly vary under different illumination conditions, but this effect has been largely overlooked in most poster productions and electronic display. The light-dependent impressions are more pronounced in oil paintings and they arise mainly from the non-diffuse specular reflectances. We present an efficient method of representing the variability of lighting conditions on artistic paintings utilizing both simple empirical reflectance models and an image-based lighting method. The Lambertian and Phong models account for a significant portion of image variations depending on illumination directions, and residual intensity and color variations that cannot be explained by the reflection models are processed in a manner that is similar to the image-based lighting methods. Our technique allows brush strokes and paint materials to be clearly visible with relatively low data dimensionality.	Image-based illumination for electronic display of artistic paintings	NA:NA:NA:NA:NA	2018
Mark W. Scott	The inspiration for the project described in this sketch was certainly the M. C. Escher lithograph <u>ASCENDING AND DESCENDING</u>. An image of this drawing can be seen at <u>http://www.worldofescher.com/gallery</u>. It is based on the continuous staircase illusion of L. S. and Roger Penrose and depicts a three-dimensional scene that appears to have properties that contradict what is possible in an actual Cartesian representation of spatial objects. The goal of this project was to add animation and a navigable viewpoint to this illusion by using the three-dimensional environment of OpenGL.	Implementing the continuous staircase illusion in OpenGL	NA	2018
Abhinav Dayal:Benjamin Watson:David Luebke	Realtime rendering requires accurate display of a dynamic scene with minimal delay. Frameless rendering [Bishop et al. 1994] offers unique flexibility in this regard: because it samples time per pixel, it can respond to change with very little delay, and at any location in the image. However, sampling is random, resulting in blurring in changing image regions. We present an approach for improving frameless rendering by making sampling sensitive to change in the image, as suggested in [Bishop et al. 1994]. By measuring this change in visual terms, we are able to direct sampling to those regions of change. The resulting algorithm produces sharper imagery, while introducing minimal overhead into the standard frameless algorithm.	Improving frameless rendering by focusing on change	NA:NA:NA	2018
Dorothy Simpson Krause	On September 11 I was in the Middle East on my continuing quest to understand why, in the name of religion, one would kill another who did not share their beliefs. The evening before I had crossed the border to Israel from Jordan, where I had photographed the crumbling sandstone ruins of Petra.	Integrating lenticular into digital printmaking	NA	2018
Thomas Howard:Bryan Morse	This sketch presents a suite of interactive image-editing tools based on properties of and manipulation of image level sets. This suite includes level-set smoothing, level-set constrained sharpening, and level-set "nudging" (image distortion).	Interactive level-set tools for photo editing	NA:NA	2018
Patric Ljung:Mark Dieckmann:Anders Ynnerman	Visual analysis of time varying scientific data can be divided into four different categories with an increasing degree of user interaction. 1) Production of static images representing scientific data at selected times. 2) Production of video sequences in which graphical representation, time line and viewpoints are predefined. 3) Interactive streaming of logged data sets, allowing the user to alter graphical representation, filtering, time lines and viewpoints. 4) Real time interaction with the simulation or experiment that produces the data, allowing the user to alter parameters, graphical representation, filtering, time lines and viewpoints.	Interactive visualization of large scale time varying data sets	NA:NA:NA	2018
J. Cliff Woolley:David Luebke:Ben Watson	Interruptible rendering is a novel approach to the fidelity-versus-performance tradeoff ubiquitous in real-time rendering. Interruptible rendering unifies spatial error, caused by rendering coarse approximations for speed, and temporal error, caused by the delay imposed by rendering, into a single image-space error metric. The heart of this approach is a progressive rendering framework that renders a coarse image into the back buffer and continuously refines it, while tracking the temporal error. When the temporal error exceeds the spatial error caused by coarse rendering, further refinement is pointless and the image is displayed. We discuss the requirements for a rendering algorithm to be suitable for interruptible use, and describe one such algorithm based on hierarchical splatting. Interruptible rendering provides a low-latency, self-tuning approach to interactive rendering. Interestingly, it also leads to a "one-and-a-half buffered" approach that renders sometimes to the back buffer and sometimes to the front buffer.	Interruptible rendering	NA:NA:NA	2018
Robert Falco:Hank Driskill	Standard texturing of a geometric surface works by mapping a 2D image into the parametric space of the surface. This technique works great whenever the texture image is created (painted, etc) for a specific surface with predetermined contours. Serious problems arise when the same texture is applied to the same surface but after some deformation has been applied. The result is the all too familiar rubbery texture look. This stretchy appearance can be particularly noticeable on surfaces that have identifiable traits such as scars, holes, text, repeating patterns, or in our case reptilian scales.	Inverse texture warping	NA:NA	2018
Steve DiPaola	Imagine an n-dimensional space describing every conceivable humanoid face, where each dimension represents a different facial characteristic. Within this continuous space, it would be possible to traverse a path from any face to any other face, morphing through locally similar faces along that path.	Investigating face space	NA	2018
Cindy M. Grimm:William D. Smart	Lewis is a (short) human-sized mobile robot that wanders through crowded rooms taking pictures of people, much like a photographer at a wedding reception does. The goal is to take high-quality, well-composed photographs of people non-intrusively, and to offer these pictures as keep-sakes of the conference.	Lewis the robotic photographer	NA:NA	2018
Marc Downie	This sketch outlines some of the background to a collaborative artwork created in September 2001 by Marc Downie, Shelley Eshkar and Paul Kaiser. This digital portrait of dance legend Merce Cunningham uses as a point of departure a motion-captured recording of 'Loops', his solo dance for hands and fingers; it uses new real-time non-photorealistic rendering techniques; and it exploits an advanced behavior architecture to structure the performance of the piece. In the resulting animation motion-captured joints become nodes in a network that sets them into fluctuating relationships with one another, at times suggesting the hands underlying them, but more often depicting complex cat's-cradle variations.	Loops: a digital portrait	NA	2018
Piotr Karwas	The need to recreate reality is a driving force behind art. Mimesis-imitation of the real world as a main function of art was observed and analyzed as early as the fourth century B. C. by Socrates, Plato, and Aristotle. Although artists searched for inner inspiration, the psychological need to record reality was always present, appearing during the renaissance, in baroque painting, the art of film, and probably reaching it's most sophisticated form in computer graphics and animation. Seventy years ago, Polish critic Karol Irzykowski, in his book "The 10th Muse", predicted that one day animated film would evolve into the most important film genre of the future. He also contemplated the possibility of animation reaching a state of realism in which it could show "ordinary things and people". He would be surprised to learn how true his words were to become. Digital media has given us an opportunity to go even further towards that goal of "registering reality". In this sketch, I present these concepts in relation to Digital Domain's work on New Line Cinema's release of J. R. R. Tolkien's classic tale "Lord of the Rings: Fellowship of the Ring".	Lord of the Rings: animation that was not there	NA	2018
Damian Isla:Bruce Blumberg	It has been suggested that investing animated characters with low-level cognitive models can allow a rich set of low-level behavior to be produced automatically. For example, a character that has the cognitive capability of Object Persistence can intelligently direct its gaze over a scene and even respond emotionally to certain world events. This level of cognitive modeling allows for complete behavioral control by a human controller or a script, if that degree of control is necessary for the application.	"Low level" intelligence for "low level" character animation	NA:NA	2018
Tatsuo Yotsukura:Mitsunori Takahashi:Shigeo Morishima:Kazunori Nakamura:Hirokazu Kudoh	In recent years, tremendous advances have been achieved in the 3D computer graphics used in the entertainment industry, and in the semiconductor technologies used to fabricate graphics chips and CPUs. However, although good reproduction of facial expressions is possible through 3D CG, the creation of realistic expressions and mouth motion is not a simple task.	Magical face: integrated tool for muscle based facial animation	NA:NA:NA:NA:NA	2018
Raphaël Grasset:Jean-Dominique Gascuel	Using augmented reality (AR) in collaborative situations is appealing: it combines the use of natural metaphors of communication (gesture, voice, expression), with the power of virtual ones (simulation, animation, persistent data). But few 3D AR collaborative systems are devoted to keep human's ability (like grasping, writing). The motivation of this research is to mix together virtual reality techniques ([Schmalstieg et al. 2000]) and computer human interaction techniques ([Fjeld et al. 2002]), so to have the best of both worlds.	MARE: multiuser augmented reality environment on table setup	NA:NA	2018
Philo Tan Chua:Rebecca Crivella:Bo Daly:Ning Hu:Russ Schaaf:David Ventura:Todd Camill:Jessica Hodgins:Randy Pausch	We present a wireless virtual reality system and MasterMotion, the full-body training application built with it. We combine realtime full-body optical motion capture with wireless audio/video broadcast, belt-worn electronics, and a lightweight head-mounted display (HMD), to provide a wide-area, untethered virtual environment system that allows exploration of new application areas.	MasterMotion: full body wireless virtual reality for Tai Chi	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
S. Gumhold	Several methods have been proposed to help the user to position lighting sources for a given view of a 3d-scene. In the first kind of methods the user defines a desired illumination through highlight and shadow locations for which the lighting system optimizes the light positions. The more automatic approaches are based on perceptual image metrics. [Marks et al. 1997] define an image metric that measures how different two images are perceived. A collection of maximally different images is presented to the user for selection. [Shacked and Lischinski 2001] define a perceptual based image quality metric composed of six contributing terms, for which the user has to specify weights before the system searches for a locally optimal light source placement.	Maximum entropy light source placement	NA	2018
Charlotte Belland	This animation sketch presents a study of human-driven character animation with motion capture as presented as part of the 2002 SIGGRAPH Course Motion Capture: Pipeline, Applications, and Use.	MOCAP game reserve: a study of puppetry and motion capture	NA	2018
Chen Shen:Kris K. Hauser:Christine M. Gatchalian:James F. O'Brien	This technical sketch describes how a standard analysis technique known as modal decomposition can be used for real-time modeling of viscoelastic deformation. While most prior work on interactive deformation has relied on geometrically simple models and advantageously selected material parameters to achieve interactive speeds, the approach described here has two qualities that we belive should be required of a real-time deformation method: the simulation cost is decoupled from both the model's geometric complexity and from stiffness of the material's parameters. Additionally, the simulation may be advanced at arbitrarily large time-steps without introducing objectionable errors such as artificial damping.	Modal analysis for real-time viscoelastic deformation	NA:NA:NA:NA	2018
Bryan E. Feldman:James F. O'Brien	This technical sketch presents a method for modeling the appearance of snow drifts formed by the accumulation of wind-blown snow near buildings and other obstacles. Our method combines previous work on snow accumulation [Fearing] with techniques for incompressible fluid flows [Fedkiw et al.]. By computing the three-dimensional flow of air in the volume around the obstacles our method is able to model how the snow is convected, deposited, and lifted by the wind. The results demonstrate realistic snow accumulation patterns with deep windward and leeward drifts, furrows, and low accumulation in wind shadowed areas. (See figure.)	Modeling the accumulation of wind-driven snow	NA:NA	2018
Hamish Carr:Thomas Theußl:Torsten Möller	Theußl et al. [Theußl et al. 2001] showed that volumetric data sampled on a body-centred cubic (BCC) lattice is nearly 30% more efficient than data sampled on a cubic lattice, and produced volume renderings using splatting. We extend this work to generate isosurfaces based on the BCC lattice, and also on the hexagonal-close packed (HCP) grid. This sketch presents a modified version of marching octahedra that simplifies the BCC mesh to an octahedral mesh to reduce the number of triangles generated for the isosurface.	Modified marching octahedra for optimal regular meshes	NA:NA:NA	2018
Stefan Gustavson	Modern real time motion capture systems are complex, large scale installations. They focus on accuracy, reliability, ease of use for technical staff and convenience for performers, but unfortunately they are also prohibitively expensive for many applications where motion capture might otherwise prove useful. Low budget work and experiments, VR applications and university research could all benefit from a low cost alternative for motion capture. Such applications could also tolerate a somewhat lower quality, and possibly even some slight inconvenience for users and performers.	Motion capture done dirt cheap	NA	2018
Eric B. Lum:Aleksander Stompel:Kwan-Liu Ma	Motion provides strong visual cues for the perception of shape and depth, as demonstrated by cognitive scientists and visual artists. We present a novel visualization technique that uses particle systems to add supplemental motion cues. Based on a set of rules following perceptual and physical principles, particles flowing over the surface of an object not only bring out, but also attract attention to essential shape information of the object that might not be readily visible with conventional rendering. Replacing still images with animations in this fashion, we show with both surface and volumetric models that the resulting visualizations effectively enhance the perception of three-dimensional shape and structure.	Motion-based shape illustration	NA:NA:NA	2018
Marc Cardle:Stephen Brooks:Loic Barthe:Mo Hassan:Peter Robinson	Sounds are generally associated with motion events in the real world, and as a result there is an intimate connection between the two. Hence, producing effective animations requires the correct association of sound and motion, which remains an essential, yet difficult, task. But unlike prior, application-specific systems such as [Lytle90; Singer97], we address this problem with a general framework for synchronizing motion curves to perceptual cues extracted from the music. The user is able to modify existing motions rather than needing to incorporate unadapted musical motions into animations. An additional fundamental feature of our system is the use of music analysis techniques on complementary MIDI and analog audio representations of the same soundtrack.	Music-driven motion editing	NA:NA:NA:NA:NA	2018
Konstantin Economou:Mark Ollila:Martin Etherton Friberg:Anders Ynnerman	As cultural heritages go, or rather stand firm, the Norrköping "industrial landscape" (industrilandskapet in Swedish) has been the object of cultural regeneration over the last two decades. Once, one of Sweden's largest city center industrial sites, major supplier of textile and paper it has now become the nexus of cultural activity. Museums, a concert hall and, the university, have now made the scenery different. In turning the landscape over we might think that old industrial life has succumbed to the changes in industrialisation making Norrköping move from manufacture to knowledge industry. In museums, this era of "old" is depicted in nostalgic ways - as "lost" cultural heritage rather than as living imagery. And, where did manufacturing go? Is it the case that industry has changed or has it just moved.	Now and then, here and there: industrilandskapet	NA:NA:NA:NA	2018
Timothy Nohe	This sketch presents an artwork entitled Occidio, which has been produced to interpret the alarming phenomena of global warming in the form of a computer-controlled sound and DVD installation. Occidio interprets NASA scientific visualizations through the interplay of video projections, event-triggered synthetic sound generated by optical theremins, and metaphoric sculptural forms. Visitors to Occidio shall "see," and, "hear" interpretations of global warming.	Occidio	NA	2018
Saty Raghavachary:Fernando Benitez	This sketch describes how to construct a painterly 'wall' of fire, one which exhibits realistic motion while managing to maintain an artistic look.	Painterly fire	NA:NA	2018
Kevin G Suffern	This sketch presents fractal art work created by ray tracing the specular highlights of point lights on the inside surface of a hollow sphere. The sphere has a mirror surface on the inside that contributes no colour to the images, but there is spread in the local specular reflection. The resulting images consist entirely of specular highlights and their reflections. I call this painting with light because, although recursive ray tracing is used, no objects are visible.	Painting with light	NA	2018
Kyoko Murakami:Reiji Tsuruno	There has been any works specifically on computer generated 3D pastel rendering. Some rendering or imaging softwares can generate pastel drawing-like images recently, but most of them cannot modeling and rendering objects as 3D, and do not have enough power of expression in comparison with real-pastel drawings.	Pastel-like rendering considering the properties of pigments and support medium	NA:NA	2018
Hunter Murphy:Andrew T. Duchowski	A common assumption exploited in perceptual Virtual Reality studies is that eye movements made while immersed in VR generally do not deviate more than 30° (visual angle) from the head-centric view direction (e.g., see Barnes [1979]). In this sketch we report eye tracking evidence which generally supports this observation in the context of peripheral Level Of Detail management during a visual search task in VR. We present results from experiments based on the work of Watson et al. [1997] and discuss an extension to the peripheral degradation paradigm to include a dynamic eye-slaved high-resolution inset.	Perceptual gaze extent & level of detail in VR: looking outside the box	NA:NA	2018
Patrick Ledda:Alan Chalmers:Greg Ward	A major goal of realistic image synthesis is to generate images that are both physically and perceptually indistinguishable from reality. One of the practical obstacles in reaching this goal is that the natural world exhibits a wide range of colors and intensities. The range of the luminances in the real world can vary from 10-4cd/m2 (for starlight) to 105cd/m2 (for a daylight scene). Reproducing these luminances on a cathode-ray tube (CRT) display is currently not possible as the achievable intensities are about 100 cd/m2 and the practical ratio between maximum and minimum pixel intensity is approximately 100:1. At the University of Bristol, we have constructed a High Dynamic Range (HDR) viewer that is capable of achieving a 10,000:1 contrast ratio. This sketch investigates, by means of psychophysical experiments, the benefits such a HDR device has to offer realistic computer graphics.	Perceptual tone mapping operators for high dynamic range scenes	NA:NA:NA	2018
Markus Manninen	This animation sketch presents a production case study of how computer graphics work was designed to support and allow for director Jonathan Glazer's live action shooting style on "Odyssey", his latest internationally acclaimed commercial for Levi's. The film features a man and a woman in an ecstatic state of freedom to move. They achieve this by first escaping their drab interior surroundings, relentlessly running through a succession of walls. Once outside, they run vertically up two enormous trees and upon reaching the very tip take a huge leap and launch themselves up towards the stars.	Performance driven computer graphics making Odyssey	NA	2018
Isao Mihara:Takahiro Harashima:Shunichi Numazaki:Miwako Doi	A new video camera system, Pop.eye, is able to capture a realtime 3D video based on color image and reflection image. The current prototype system (Figure 1) can capture 160x120 sized images at a rate of 25 frames per second. The captured image of a stuffed toy is viewed as the "Pop-out" image frame shown in Figure 2-left. The "Pop-out" shape is constructed from the captured reflection image. Furthermore, this system is able to output an enhanced "Pop-out" movie that extracts only the target object from background by using depth-key (Figure 2-right).	Pop.eye: a pop-out video camera system for personal use	NA:NA:NA:NA	2018
Adrian Secord:Wolfgang Heidrich	Non-photorealistic rendering often requires placing drawing primitives onto a 2D canvas in such a way that the resulting tone approximates that of a greyscale reference image. Several iterative methods have been used where each stroke is tentatively placed on the canvas and the resulting tone is evaluated with respect to the reference image. [Salisbury et al. 1994; Salisbury et al. 1997; Praun et al. 2001] If the stroke over-darkens the output image it is rejected, otherwise it is accepted. While this back-and-forth iteration between the output and the reference image is capable of producing high-quality results, it is extremely costly in terms of computation and memory references.	Probabilistically placing primitives	NA:NA	2018
Kathleen Gretchen Greene	A plague of frogs, a river of stones, a dragon's back of scales... When a large group should exhibit variation in some characteristic, how can we describe and experiment with that variation and the look and feel of the group as a whole?	Probability paint: controlling group characteristics with PDFs	NA	2018
Jonah Hall	Dreamworks Pictures film "The Time Machine" presented a number of interesting problems related to creating time-lapse photography using computer animation. I was assigned with the task of making realistic vines creep and grow along the surface of the green house as the main character began to move forward in time. In order to keep such a complicated task manageable, I needed to organize the order in which things were processed to keep things workable as changes were made. The pipeline needed to be designed in such a way so that the maximum amount of processing is taken care of at the earliest stage.	Pushing the limits of L-systems for time-lapse vine growth in "The Time Machine"	NA	2018
Jerome Grosjean:Sabine Coquillart	Virtual environments (VEs) like the projection-based Responsive Workbench have greatly enhanced the interactive visualization and manipulation of 3D objects. In these configurations, the classical desktop interaction techniques have to be reconsidered. In particular, a simple operation like text typing, although needed for basic operations like saving one's work under a specific file name or entering a precise numerical value inside an application, is still problematic.	Quikwriting on the responsive workbench	NA:NA	2018
Oliver Bimber:L. Miguel Encarnação	Paleontology is filled with mysteries about organisms such as plants and animals that lived thousands, millions, and billions of years before the first modern humans walked the earth. To solve these mysteries, paleontologists rely on the excavation, analysis, and interpretation of fossils. Fossils are the remains or traces of ancient life forms that are usually preserved in stones and rocks. Examples include bones, teeth, shells, leaf imprints, nests, and footprints. Such fossil discoveries reveal what life on our planet was like long ago. Fossils also disclose the evolution of organisms over time and how they are related to one another. While fossils reveal what ancient living things looked like, they keep us guessing about their color, sounds, and most of all their behavior. Each year, paleontologists continue to piece together the stories of the past.	RAPTOR: towards augmented paleontology	NA:NA	2018
Sarah Witt	The Sony PlayStation2, with its powerful rendering and vector processing capabilities, built-in MPEG decoder, and large capacity hard disc drive possesses fundamental assets that make it an ideal platform for processing video. Research done at Sony BPRL, where we have extensive experience of video effects processing, has shown that this inexpensive games console can, in fact, create in real time the type of video effects which normally require investment in dedicated hardware (or endless patience as a PC renders them slowly).	Real time video effects on a PlayStation2	NA	2018
Simon Prince:Adrian David Check:Farzam Farbiz:Todd Williamson:Nik Johnson:Mark Billinghurst:Hirokazu Kato	We demonstrate a real-time 3-D augmented reality video-conferencing system. The observer sees the real world from his viewpoint, but modified so that the image of a remote collaborator is rendered into the scene. For each frame, we estimate the transformation between the camera and a fiducial marker using techniques developed in Kato and Billinghurst [1999]. We use a shape-from-silhouette algorithm to generate the appropriate view of the collaborator in real time. This is based on simultaneous measurements from fifteen calibrated cameras that surround the collaborator. The novel view is then superimposed upon the real world image and appropriate directional audio is added. The result gives the strong impression that the virtual collaborator is a real part of the scene.	Real-time 3D interaction for augmented and virtual reality	NA:NA:NA:NA:NA:NA:NA	2018
Jason L. Mitchell:Chris Brennan:Drew Card	In Non-Photorealistic Rendering (NPR), outlines at object silhouettes, shadow edges and texture boundaries are important visual cues which have previously been difficult to generate in real-time. We present an image-space technique which uses pixel shading hardware to generate these three classes of outlines in real time. In all three cases, we render alternate representations of the desired scene into texture maps which are subsequently processed by pixel shaders to find discontinuities corresponding to outlines in the scene. The outlines are then composited with the shaded scene.	Real-time image-space outlining for non-photorealistic rendering	NA:NA:NA	2018
Ruigang Yang:Greg Welch:Gary Bishop:Herman Towles	We present a novel use of commodity graphics hardware that effectively combines a plane-sweeping algorithm [Collins 1996] and view synthesis in a single step for real-time, on-line 3D view synthesis. Unlike typical stereo algorithms that use image-based metrics to estimate depths, we focus on using image-based metrics to directly estimate images. Using real-time imagery from a few calibrated cameras, our method can generate new images from nearby viewpoints, without any prior geometric information or requiring any user interaction, in real time and on line.	Real-time view synthesis using commodity graphics hardware	NA:NA:NA:NA	2018
Tomáš Staudek:Petr Machala	Exact aesthetics is a challenging field of the computer-aided visual creativity, reconstructing the methods of design and criticism on an algorithmic basis and integrating a computer into processes of an artistic creation and aesthetic evaluation. The discipline involves principles of mathematics, geometry, theory of communication, perceptual psychology, computer graphics, or generative arts into classifying and assessing the aesthetic phenomena. The sketch introduces recent applications in this domain.	Recent exact aesthetics applications	NA:NA	2018
Andrzej Zarzycki	There is a certain mystery surrounding the unbuilt projects or unrealized ideas of famous architects. Often there is an expectation of deeper meaning and hidden genius present in unfulfilled buildings. Some critics go as far as to claim that the best and most interesting projects remain unrealized because of the progressiveness of the ideas associated with those buildings.	Reconstructing or inventing the past: a computer simulation of the unbuilt church by Alvar Aalto	NA	2018
Hiroto Matsuoka:Akira Onozawa:Hidenori Sato:Hisao Nojima	The vision of this work is make it possible to regenerate real objects that had existed at some moment in the past and/or at some remote location as if they have been transported to the present across space and time. The objects could be museum pieces or items in stores, for example. In this work, we have developed a quick and fully automated system that can capture a three-dimensional image of real objects. This success has brought us close to realizing our vision.	Regeneration of real objects in the real world	NA:NA:NA:NA	2018
Suguru Saito:Akane Kani:Youngha Chang:Masayuki Nakajima	In simple style drawings, like Comics and traditional cel animation, curved strokes are relatively important. Of course, the shape of the curve is the most important. However, subtle changes of curve width cannot be ignored. We propose a powerful method allowing subtle width changes to be applied to general 2D curve data. The algorithm is based on curvature information of the input curve, and keeps carefully the impression of the original curvature. The resulting image expresses a pen-and-ink drawing style.	Rich curve drawing	NA:NA:NA:NA	2018
Jörg Peters:Xiaobin Wu	Given a composite spline surface, we show how to efficiently construct two matching triangulations that sandwich the surface. Such a two-sided enclosure, (s-, s+), supports collision detection, re-approximation for format conversion, meshing with tolerance, one-sided smoothing and silhouette detection as illustrated in Figure 2.	Sandwiching surfaces	NA:NA	2018
Johnny Gibson	For The Time Machine the CG effects team faced the daunting task of producing imagery that represented long exposure and time-lapse photography of various types of terrain. The scope of the effort and the shot design largely preempted the use of simulated or exclusively explicit techniques for terrain construction. The challenge we faced was to procedurally model surface detail features into terrain that would visually represent the erosion of volumes of earth and rock over time. These features had to be consistent over time and through the entire volume of earth and rock through which the terrain would erode. These restrictions gave rise to two shader techniques used both in the procedural construction of geometry and in the displacement phase of terrain shading: bouldering and gulleying.	Shader analytical approximations for terrain animation in "The Time Machine"	NA	2018
Aaron Hertzmann:Nuria Oliver:Brian Curless:Steven M. Seitz	This sketch presents "Shape Analogies," a method for learning line styles from examples. With this approach, an artist or end-user simply draws in the desired style; the system analyzes the drawings and generates new imagery in the same style. For example, to design an outline style for a nervous character, one may draw a jittery stroke; to design an outline style for a robot, one may draw a very rigid style with many sharp angles.	Shape analogies	NA:NA:NA:NA	2018
Marco Tarini:Hendrik Lensch:Michael Goesele:Hans-Peter Seidel	Objects with mirroring surfaces are left out of the scope of most recent 3D scanning methods. We developed a new acquisition approach, shape-from-distortion, that focuses on that category of objects, requires only a still camera and a monitor, and generates high quality range scans (plus a normal field). Our contributions are a novel acquisition technique based on environment matting [Chuang et al. 2000] and corresponding geometry reconstruction method that recovers a very precise geometry model for mirroring objects.	Shape from distortion: 3D range scanning of mirroring objects	NA:NA:NA:NA	2018
Eric Guaglione:Doug Sweetland	In character animation, it is often said that a strong pose is supported by its' silhouette. Since the shape of a 3D character represented in a graphical, 2D screen-space is ultimately our goal, shouldn't our tools better support posing the silhouette? Yet commercial software packages have focused on implementations to pose the character from the inside out, rather than looking at the shape inward.	Shape-based character animation	NA:NA	2018
John Haddon	The animated short "a flatpack project" was intended to emulate the aesthetics of traditional art techniques in a digital medium. This necessitated the creation of custom rendering and image processing code to reproduce the appearance of line drawings in both pencil and ink, along with other effects such as the bleeding of ink in water and the application of pastel to paper. This sketch describes some of the techniques used in achieving these ends.	Sketchy rendering	NA	2018
Valerio Pascucci	In recent years subdivision methods have been successfully applied to the multi-resolution representation and compression of surface meshes. Unfortunately their use in the volumetric case has remained impractical because of the use of tensor-product generalizations that induce an excessive growth of the mesh size before sufficient number is preformed. This technical sketch presents a new subdivision technique that refines volumetric (and higher-dimensional) meshes at the same rate of surface meshes. The scheme builds adaptive refinements of a mesh without using special decompositions of the cells connecting different levels of resolution. Lower dimensional "sharp" features are also handled directly in a natural way. The averaging rules allow to reproduce the same smoothness of the two best known previous tensor-product refinement methods [Bajaj et al. 2001;MacCracken and Joy 1996].	Slow growing volumetric subdivision	NA	2018
Maria Giannakouros	Night scenes filmed on indoor stage sets, and in day or evening light, all have one thing in common-the absence of a starry sky. Even shots filmed at night, under perfect conditions, fail to capture stars. Since the focus is on local subjects which need adequate lighting to be captured, the sky is too under exposed to capture stars.	Star fields in 2D	NA	2018
Umesh Shukla	Painterly animation has till now been a part of research groups and enthusiast. This film attempts to bring it closer to the masses using commercially available software.	"Still I Rise" painterly animation off the shelf	NA	2018
Ramon Montoya-Vozmediano:Mark Hammel	A project currently in production at Walt Disney Feature Animation requires realistic hair with a high degree of artistic control. In this sketch we present a set of techniques that achieve these goals along the production pipeline.	Stylized flowing hair controlled with NURBS surfaces	NA:NA	2018
Sandra Villarreal	Synchronous Pronouncement is a generative interactive installation that will explore the aesthetics of an interactive medium as an extension of our body. The irregularity and unpredictability of our world is a result of unique patterns left behind by movements. The installation will be a representation of interacting processes that operate in nature. Autonomous behaviors generated by code will be triggered by the presence of users for the creation of a unique real-time audiovisual experience.	Synchronous pronouncement	NA	2018
Ali Mazalek:Glorianna Davenport	Over the centuries, stories have moved from the physical environment (around campfires and on the stage), to the printed page, then to movie, television, and computer screens. Today, using wireless and tag sensing technologies, story creators are able to bring digital stories back into our physical environment. The Tangible Viewpoints project explores how physical objects and augmented surfaces can be used as tangible embodiments of different character perspectives in a multiple point-of-view interactive narrative. These graspable surrogates provide a more direct mode of navigation to the story world, bringing us closer to bridging the gap between the separate realms of bits and atoms within the field of digital storytelling.	Tangible viewpoints: a physical interface for exploring character-driven narratives	NA:NA	2018
Dan Maynes-Aminzade:Randy Pausch:Steve Seitz	At SIGGRAPH in 1991, Loren and Rachel Carpenter unveiled an interactive entertainment system that allowed members of a large audience to control an onscreen game using red and green reflective paddles. In the spirit of this approach, we present a new set of techniques that enable members of an audience to participate, either cooperatively or competitively, in shared entertainment experiences. Our techniques allow audiences with hundreds of people to control onscreen activity by (1) leaning left and right in their seats, (2) batting a beach ball while its shadow is used as a pointing device, and (3) pointing laser pointers at the screen. All of these techniques can be implemented with inexpensive, off the shelf hardware.	Techniques for interactive audience participation	NA:NA:NA	2018
David Mould:Eugene Fiume	A great deal of attention has been paid to the problem of texture synthesis. Procedural techniques have become commonplace. Yet while a proliferation of models, and the success of nonparametric synthesis-from-example methods, has made an extraordinary variety of textures realizable, the creation of novel textures remains a challenge.	Textures from nonlinear dynamical cascades	NA:NA	2018
Jose L. Hernandez-Rebollar:Nicholas Kyriakopoulos:Robert W. Lindeman	We present The AcceleGlove, a novel whole-hand input device to manipulate three different virtual objects: a virtual hand, icons on a virtual desktop and a virtual keyboard using the 26 postures of the American Sign Language (ASL) alphabet.	The AcceleGlove: a whole-hand input device for virtual reality	NA:NA:NA	2018
Volker Paelke:Joerg Stoecklein:Lennart Groetzbach:Christian Geiger:Christian Reimann:Waldemar Rosenbach	The AR-ENIGMA combines a PDA (personal digital assistant) with a camera, a high-speed wireless network connection and AR (augmented reality) technology to enable museum visitors to interact with an Enigma encryption machine.	The AR-ENIGMA: a PDA based interactive illustration	NA:NA:NA:NA:NA:NA	2018
John Jay Miller:Weidong Wang:Gavin Jenkins	Inadequately or poorly designed environments and tools of daily living impose barriers to people with a disability. This issue needs to be addressed in order for people with disabilities to lead full and purposeful lives. To accomplish this goal it is imperative that designers of environments and artifacts have an in-depth knowledge of human functioning in the performance of tasks and problem-solving strategies to develop environments and products that best accommodate performance of these tasks. They require an understanding and useful characterizations of the abilities of people with disabilities and relevant mechanisms to incorporate this into a modern design process.	The development of a functional visualization system for the creation of digital human models	NA:NA:NA	2018
Vincent Masselus:Philip Dutré:Frederik Anrys	The Free-form Light Stage captures the reflectance field of an object using a freely movable, hand-held light source. By photographing the object under different illumination conditions, we are able to render the object under any lighting condition, using a linear combination of basis images. Our technique builds on recent techniques, such as the Light Stage [Debevec et al. 2000], where light sources are placed at fixed and known positions (e.g. using a gantry). We remove this limitation, and put no restrictions on light source placement.	The free-form light stage	NA:NA:NA	2018
A. Fleming Seay:Diane Gromala:Larry Hodges:Chris Shaw	During the Emerging Technologies exhibition at Siggraph 2001, over 400 attendees experienced The Meditation Chamber. This immersive, bio-interactive environment was designed to use visual, audio, and tactile cues to create, guide, and maintain a user's guided relaxation and meditation experience. During this sketch, the project's producers will discuss the design and implementation of this unique installation. We will also show footage from the experience and discuss the subjective relaxation measures and the GSR, heart rate and respiration data generated by 411 Siggraph attendees.	The meditation chamber: a debriefing	NA:NA:NA:NA	2018
Hideyuki Ando:Takeshi Miki:Masahiko Inami:Taro Maeda	The "SmartFinger" is a new type of tactile display, which is worn on the nail side of the finger. It does not inhibit our tactile sensation, since the ball of the finger is naked and we can feel the environment directly. It is important to insert nothing between a finger and an object.	The nail-mounted tactile display for the behavior modeling	NA:NA:NA:NA	2018
David K. McAllister:Anselmo A. Lastra:Benjamin P. Cloward:Wolfgang Heidrich	Combining texture mapping with bi-directional reflectance distribution functions (BRDFs) yields a representation of surface appearance with both spatial and angular detail. We call a texture map with a unique BRDF at each pixel a spatial bi-directional reflectance distribution function, or SBRDF. The SBRDF is a six-dimensional function representing the reflectance from each incident direction to each exitant direction at each surface point. Because of the high dimensionality of the SBRDF, previous appearance capture and representation work has focused on either spatial or angular detail, has relied on a small set of basis BRDFs, or has only treated spatial detail statistically [Dana 1999; Lensch 2001].	The spatial bi-directional reflectance distribution function	NA:NA:NA:NA	2018
Tsunemi Takahashi:Heihachi Ueki:Atsushi Kunimatsu:Hiroko Fujii	This sketch describes the modeling of the interaction between fluid and rigid bodies, how to simulate scenes in which fluid pressure acts on a rigid body, and conversely, in which rigid body motion drives a force back to fluid. We construct the interface between fluid simulation using the Cubic Interpolated Propagation (CIP) method and rigid body simulation using the impulse-based method. For fast simulation, we apply the CIP method to uniform structured meshes. For treating the interaction between rigid bodies and fluid efficiently, we use Volume of Solid (VOS) for rigid bodies, and for the collision among rigid bodies, we use Polygon-Polygon collision detection. For fast response to rigid body's collision, we use smaller time step for rigid body than for fluid.	The simulation of fluid-rigid body interaction	NA:NA:NA:NA	2018
Kajsa Ellegård:Johan Torne:Henric Joanson:Anders Ynnerman:Matthew Cooper:Mark Ollila	In natural sciences the practice of using computers to visualize and analyze data was adopted early. In social and cultural studies, however, computer technology has not been as commonly used for visualization purposes. This project to visualize Time Geographical datasets, the Time Geography Project, is therefore somewhat groundbreaking.	The time geography project: using computer graphics to visualize problems in social science	NA:NA:NA:NA:NA:NA	2018
Blair MacIntyre:Jay David Bolter:Jeannie Vaughan:Brendan Hannigan:Emmanuel Moreno:Markus Haas:Maribeth Gandy	"Three Angry Men" is a novel augmented reality experience that explores the use of Augmented Reality (AR) as a dramatic medium. The user participates in an AR version of the famous twentieth-century play, "Twelve Angry Men," [Rose 1983] which for practical reasons we have abbreviated into a scene involving 3 characters (thus, "Three Angry Men"). The participant finds herself immersed in a physical jury-room, where virtual characters (jurors in the drama, rendered as video-based characters overlaid at appropriate 3D locations around the physical table using a see-through head-worn display) debate the guilt of a young man on trial for murder (see Figure 1).	Three angry men: dramatizing point-of-view using augmented reality	NA:NA:NA:NA:NA:NA:NA	2018
Christopher Jaynes:Joan Mazur:Cindy Lio	The Metaverse is a synthesized world that combines computer-generated elements with the real-world to either augment of supplant a users view reality. To fully participate in this meta-world, users must access it through a unique interface that is visually immersive and non-restrictive, interactive and collaborative. Our ongoing research program in immersive environments emphasizes techniques that make widespread deployment and use of the Metaverse feasible. To that end, we are addressing technical challenges related to self-configuring, self-monitoring displays. Other researchers have made significant progress in overcoming the obstacles related to building tilted displays. Although these technical advances have made low-cost immersive displays feasible for a larger community of users, there is little understanding of the remaining HCI obstacles that prevent widespread deployment of such systems. Our early work, as part of a five-year evaluation project, focuses on defining the methods that will be required to understand usability and utility of the technology. Additionally, we are developing of tools that will assist researchers in characterizing the behavior of users within nonrestrictive immersive environments.	Towards visualizing HCI for immersive environments: the meta-situational tracker	NA:NA:NA	2018
Lori L. Scarlatos:Saira Qureshi:Shalva S. Landy	Children naturally learn about their world by manipulating objects within it. Playing with blocks and puzzles helps to develop their understanding of spatial relationships and other mathematical concepts. Using physical objects also allows them to work and learn in groups. Yet sometimes they need outside intervention from an adult or knowledgeable guide to help them learn more and stay engaged longer. Unfortunately, instructors often have too many students to give each one adequate attention. Our work focuses on developing computer-based "guides on the side" that can "watch" as children play with physical puzzles, and offer help or suggestions as needed. Our approach is to use the physical puzzle pieces as parts of a tangible interface. With our system, children are free to explore and collaborate without a computer, yet they can benefit from the computer's instruction as they need it. We have successfully implemented and tested a 2D Tangram puzzle using this approach [Scarlatos 2002].	Tracking 3D puzzle pieces for collaborative learning environments	NA:NA:NA	2018
Kenji Tanaka:Junya Hayashi:Yutaka Kunita:Masahiko Inami:Taro Maeda:Susumu Tachi	TWISTER (Telexistence Wide-angle Immersive STERe-oscope) is an immersive full-color autostereoscopic display, designed for a face-to-face telecommunication system called "mutual telexistence", where people in distant locations can communicate as if they were in the same virtual three dimensional space.	TWISTER: a media booth	NA:NA:NA:NA:NA:NA	2018
Brad Parker	Digital Domain's character animation team was asked to create a complex destruction sequence for Time Machine's main villain... the Uber Morlock. This was to be no ordinary demise. Our villain had to age rapidly. His skin was to wrinkle, discolor, and disintegrate. His hair was to recede, his clothing to oxidize and fall apart. His muscles were to atrophy, eventually releasing his bones one by one. Finally his bones were to break apart and turn to dust.	Uber destruction in "The Time Machine"	NA	2018
John Isidoro:Jason L. Mitchell	Recent advances in real-time fur rendering have enabled the development of more realistic furry characters. In this sketch, we outline a number of advances to the shell and fin based fur rendering technique by Lengyel et al [2001] using the pixel and vertex shader capabilities of modern 3D hardware.	User customizable real-time fur	NA:NA	2018
Bent Dalgaard Larsen:Jakob Andreas Bærentzen:Niels Jørgen Christensen	Distributed VR promises to change the way we work and collaborate [Singhal and Zyda 1999]. In this sketch we will extend the accessibility of the virtual world originally developed in [Larsen and Eriksen 1998] by introducing the use of the modern cellular phone as a platform for primitive interfaces to VR applications. We believe that our use of a cellular phone has led to the first completely pocketable platform for VR user interfaces.	Using cellular phones to interact with virtual environments	NA:NA:NA	2018
Michael Flaxman	Traditional architectural software tools are well suited to individual building design, but don't scale up well to large landscapes and cities. In city and regional planning applications, it is important to visualize city blocks and neighborhoods, a task which requires integration of data from geographic information systems and 3D surface models from CAD. However, the development of semantically and visually rich real-time representations of cities and large landscapes has proven difficult. Two major problems are the acquisition of appropriate data, and the volume of data once acquired. A typical city or suburban landscape may have thousands of streets, tens of thousands of buildings, and hundreds of thousands of trees and bushes. Three dimensional data specifically describing all of these objects is not usually available, and visual simulations must be based on abstract classifications originally developed for entirely different purposes, such as tax assessor's databases or generalized vegetation maps.	Using the virtual terrain project to plan real cities: alternative futures for Hangzhou, China	NA	2018
Neill Campbell:Colin Dalton:David Gibson:Barry Thomas	Recently, there have been attempts at creating 'video textures', that is, synthesising new video clips based on existing ones. Schodl et al. showed new video clips by carefully choosing sub-loops of an original video sequence that could be replayed.	Video textures using the auto-regressive process	NA:NA:NA:NA	2018
Bernadette Kiss:Gábor Szijártó:Barnabás Takács	We describe our ongoing research on creating a Virtual Human Interface that employs photo-realistic virtual people and animated characters to provide digital media users with information, learning services and entertainment in a highly personalized manner. Our system was designed to be able to create emotional engagement between the virtual character and the user, thus increasing the efficiency of learning and/or absorbing any information broadcasted through this device. We developed innovative technologies for (i) photo-real facial modeling & animation, (ii) context dependent motion libraries with on-line retargeting, (iii) artificial emotions to modulate the characters' behavior and (iv) artificial vision to make the virtual human "aware" of its surroundings. The second key aspect of our solution is a simple to use high level content authoring process, comprising of video-based MPEG4 facial tracking and an innovative interface called the "Disc Controller", which allows users to create new actors, make them move and even direct them to achieve a final rendered output within minutes.	Virtual human interface: towards building an intelligent animated agent	NA:NA:NA	2018
Patrick Hartling:Allen Bierbaum:Carolina Cruz-Neira	Developers of virtual environments often face a difficult problem: users must have some way to interact with the virtual world. The application developers must determine how to map available inputs (buttons, gestures, etc.) to actions within the virtual environment (VE). As a result, user interfaces may be limited by the input hardware available with a given virtual reality (VR) system.	Virtual reality interfaces using Tweek	NA:NA:NA	2018
Akira Kubota:Kiyoharu Aizawa	In this sketch, we present a novel approach to image-based rendering (IBR) techniques for generating a virtual view image from different positions with arbitrary focus using two differently focused images captured from a fixed position. In the conventional approach [Potmesil and Chakravarty 1981], focus blur have been produced by applying a realistic camera model to given 3D objects. We propose here a much simpler and more effective method only using space-invariant filters to render both parallax and focusing effects on objects in a scene, according to the virtual camera's position and focus depth respectively. The proposed method does not need any segmentation and 3D modeling.	Virtual view generation by linear processing of two differently focused images	NA:NA	2018
Dietmar Offenhuber	This web3d project explores how the concept of non-linear space -- that is space structured by relative units -- can be used in VR and architecture. It offers a dynamic view on Los Angeles' structure, radically different from usual architectural representations.	Wegzeit: the geometry of relative distance	NA	2018
Jackie White	NA	Session details: Special session - industrial light & magic	NA	2018
Dawn Yamada:Geoff Campbell:Sebastian Marino:Zoran Kacic-Alesic:James Tooley	Computer graphics play a starring role in the production of Star Wars Episode II: Attack of the Clones. This session focuses on the creation of the digital cast of the latest prequel to the Star Wars saga. Industrial Light & Magic developed a variety of systems to make the computer-generated characters in this film stand up to the actors with whom they share the screen, both in visual quality and physical realism. These systems also made it possible for digital doubles to stand in for actors in scenes either too difficult or too dangerous to shoot practically. In an effort to match the fidelity of motion of the computer graphics characters to that of their live-action counterparts, physically based simulation was used extensively throughout the production of this film. Multi-layered clothing, skin with underlying musculoskeletal structures, and the motion of rigid bodies each played a key role in imparting a new level of physical realism into the performance of computer graphics elements. The challenge of employing this level of proceduralism is also providing methods for directing the resulting performances. In this session, we will present an overview of the pipeline and systems used to produce Episode II, with the focus of the discussion being on the specialization required to evolve technologies, deeply rooted in academic research, into effective filmmaking tools. The panel will include individuals who played key roles in the development of key digital characters for the latest prequel to Star Wars.	Yoda and beyond: creating the digital cast of Star Wars episode II	NA:NA:NA:NA:NA	2018
Jason Della Rocca:Raph Koster:Lorne Lanning:Scott Miller:Warren Spector:Will Wright	Prominent members of the International Game Developers Association will investigate and discuss the direction of the game industry and the impact interactive entertainment will have on our future. This panel of game industry revolutionaries will explore how game design, character development, online connectivity, business models, and social and cultural implications all weave together with advances in technology to drive the industry forward.	The fate of play: game industry revolutionaries speak out	NA:NA:NA:NA:NA:NA	2018
Mary Reardon	Sony Pictures Imageworks takes you for a spin through the virtual world of Spider-Man. Scott Stokdyk and his team reveal how the effects, buildings, and characters were created in the computer and integrated with the live action. CG Supervisor Ken Hahn discusses the complexities involved in creating the buildings of New York City. CG Supervisor Peter Nofz explains the challenges of setting up characters for animation, and Character look Lead Greg Anderson shows us the process of look development for character lighting.	Spider-Man: behind the mask	NA	2018
Jill Smolin:Scott Clark:Jennifer Emberly:Stephen A. Fossati:Doug Sweetland:Barry Weiss	Cartoons turned a corner when Chuck Jones came to town. He transformed Walt Disney's vision to one of wit, humor and mischief. We watched as Wile E. Coyote repeatedly attempted to trap the Roadrunner, only to fall victim to his own falling anvils; Pepe Le Pew's aromatic expressions taught us everything we need to know about unsuccessful romance; Marvin Martian's Gladiator skirt, tennis shoes and romanesque helmet gave us an alternate view of aliens out to destroy the earth. And of course, Bugs Bunny's gregarious self-confidence enabled him to outwit, outsmart, and outsing any adversary. For most of the 20th Century, Chuck Jones has shaped the way we see a particular side of the world, and our art and our souls are all the better for it.	What's up doc?: a fond remembrance of Chuck Jones	NA:NA:NA:NA:NA:NA	2018
Dan Collins	NA	Session details: Studio	NA	2018
Bill Brody:Glenn G. Chappell:Chris Hartman	BLUIsculpt™ is an interactive virtual reality application that permits a user to freely sketch voxels inside a ten foot cube for output as physical objects. It connects the imagination to physical form. Our interface presents the user with real-time feedback in the form of surfaces. We save a file representing the surface in rapid protyping format. Generating a solid-by-rapid prototyping completes the cycle of perception and imagination that starts in the physical world, proceeds through vision, thought, imagination that starts in the physical world, proceeds through vision, thought, imagination and the dance of drawing to finally arrive at tangible sculpture. Our enterprise is based on the premise that symbolic representation, of which drawing is an exemplar, derives from perception, imagination and thought. We hold that these mental activities recruit and employ the same brain machinery that is fundamental to voluntary physical movement. Such is the common wisdom of generations of artists, and is well-supported by a mass of more recent studies in cognitive and evolutionary neuroscience.	BLUIsculpt™	NA:NA:NA	2018
DaShawn L. Hall	From the Oscar winning film, Gladiator, to studios, such as Pacific Data Images (PDI), Cinema 4D XL has been used for background design, modeling, and pre-visualization artwork. With its user-friendly interface, powerful animation, advanced modeling tools, and high-speed rendering technology, major film and animation studios, as well as educators have utilized it. Cinema 4D XL's advanced tools, such as a VRML plug-in and various modeling systems, can be used to integrate art with other subjects in the classroom. Because of its user-friendly architecture, Cinema 4D XL is well suited for introductory courses in 3D animation and modeling. With one of the fastest radiosity rendering engines, advanced students and studio professionals will also value the application. Students at all levels will have the capability of building professional digital portfolios.	Cinema 4D XL: advanced 3D software for educators and studio professionals	NA	2018
O. Makai Smith:James Stewart	Drawing Circle borrows the traditional structure and activities of a life drawing class to explore that structure's potential for digital media. It proposes using the familiar structure of a well-lit model or still life surrounded by easels, within the Studio at SIGGRAPH. We intend to explore a merger of this time-tested convention with 3D modeling in an attempt to provide a setting for the further development of participant's skills, and to promote the Studio as a place for active learning and group investigation.	Drawing circle	NA:NA	2018
Lily Díaz-Kommonen	This presentation describes a concept proposal for a system that will create an archive of the activities of the Studio at SIGGRAPH 2002. The initial parameters that constrain the design process are that the materials of such repository must be displayed and archived during the conference itself, and they must be formatted in such a manner that facilitates their retrieval and replay at a later date. Additionally, in order to reflect the diversity of content present, the archive must be created in a collaborative manner, and with the help of other Studio participants. The amplitude of the desired coverage is yet another factor to be considered. All of these factors must be considered in the light of the quantity of data that can be potentially generated by such endeavor.	Collaborative frameworks: a proposal for an archive in the studio	NA	2018
Simon Allardice	NA	Session details: Web graphics	NA	2018
Michela Ledwidge	This is a presentation of the web3d pipeline and production process used to deliver the multi-lingual interactive animated short film Horses for Courses --- winner of the Web3d Roundup art prize at SIGGRAPH 2001. The film features English, French and Spanish soundtracks, different endings, mouse triggered hotspots, and manual camera controls. Photography, low-polygon modelling, and optimised texture maps, were key to publishing the film as low-bandwidth streaming media in July 2001. The pipeline developed for this project has since been used to achieve higher resolution output for offline applications.	A case study in web3d film-making: Horses for Courses	NA	2018
Márcio O. Costa:Jônatas Manzolli:Dan Sharoni	As an interactive, computerized, network oriented musical composition tool, Rabisco allows users to create stream of MIDI data in real-time by drawing simple sketches using a simple 2-D graphical tablet (pad). Utilizing a client-server architecture, Rabisco allows virtual joint compositions where several musicians interact over the network or the internet, each using a Rabisco client. Current applications includes Distance Learning, Interactive Music Composition, and Interaction with real world devices such as robots.	A distributed interactive composition tool	NA:NA:NA	2018
Bing-Yu Chen:Tomoyuki Nishita	Solid texturing [Peachey 1985; Perlin 1985] has become a well-known computer graphics technology since it was first presented more than fifteen years ago. However, solid texturing still remains problems today, because it consumes too much time and has a very high memory requirement. Although some methods have recently been proposed to solve these problems, almost all of them need the support of hardware accelerators. Hence, these methods could not be applied to all kinds of machines, especially the low-cost ones available over the Internet. Therefore, we present a new method for procedural solid texturing in this paper. Our approach could almost render an object with procedural solid texturing in real-time using only a software solution. The basic idea of this approach is similar to the cache mechanism used for main memory control. Furthermore, to demonstrate that our approach is widely applicable we choose pure Java for it's implementation, since this could not receive any benefit from the hardware and could be executed on the Internet directly.	Adaptive solid texturing for web graphics	NA:NA	2018
Alain Chesnais:Tim Beck:Rudy Ziegler	One of the most labor-intensive tasks in the development of web sites is the creation of derivative images for display of a visual element at multiple sizes or in multiple styles. For instance, a typical online catalog will have at least three different visual representations for each product sold: one thumbnail sized image for visual browsing, one mid sized image for viewing a product description and one large sized image for viewing product detail. The approach outlined in this presentation is to enable the web server to actively generate any derivative image from a high-resolution base image. Derivative images are then cached on the server to speed delivery of subsequent requests for the same derived content.	Architecting a distributed dynamic image server for the web	NA:NA:NA	2018
Alfredo Andia	The "Internet Studio Network" is an initiative designed to create academic relationships among architectural schools to work on semester-studio projects collaborating via the Internet. Past participants included a maximum of 300 architectural students from Miami, Argentina, Chile, Ecuador and Venezuela, who collaborated in semester-long design studios via the Internet and videoconference technology during the Fall, 2001.	Architectural studios online: the "internet studio network"	NA	2018
Guillaume Clary	Banja is the first communal adventure game in 3D flash on the web.	Banja: flash programming system & game design	NA	2018
Lynda Weinman	Color has long played an essential role in successful visual design, but the web has some distinct constraints and considerations that warrant further study by the web graphics and SIGGRAPH community. The objective of this lecture is to establish what special constraints related to color exist, share design principles and techniques to address those constraints, and critique existing examples of web design on the success or lack thereof of color communication.	Color aesthetics for web graphics creation	NA	2018
Anders Henrysson:Mark Ollila	Hybrid representations - that of using image and procedural methods to synthesize images. Procedural methods allow us to describe media (2D images, 3D objects etc.) with very little information and render it photorealistically. Since the procedure is run on the client (for instance a PC or a mobile phone with limited network), it makes sense to adapt the procedure to the properties of the client.	Combining procedural, polygonal, and bitmap representations using XML	NA:NA	2018
Eric Morin	While setting out to create a multimedia authoring tool, the Anark development team set out to use both common design paradigms and evolving technologies. Considering current Web graphics and general multimedia trends, there is a thrust for incorporating multiple types of media into a single visual presentation. Technologies currently exist to bring the concept of pure 3D to Web graphics, but much of the difficulty in creating 3D content comes in the form of multiple and confusing workflows. What started from this thought is an investigation into the technology, use and exposure of 3D technology not only as a core component, but also as a technique to achieve new design metaphors and practices.	Content creation with Anark studio	NA	2018
J. Paul Nykamp	Over the past few years, 3D on the web has been emerging as a viable format. The growth in both computing power and bandwidth has made this technically possible. 3D has many advantages over more traditional frame animations, such as QTVR object movies. These include a greater ability to display objects from any angle, and a greater ability to animate objects and have them react to user interaction. One of the challenges of creating compelling 3D, has been to make it "photorealistic". Recently a new type of 3D has emerged, which is "photographic" 3D. Photography is now being used to create both textures and geometry for 3D objects, and not just in the lab. Diginiche is doing this efficiently and commercially now, using Viewpoint technology.	Creating and implementing photographic 3D on the web	NA	2018
Teresa Lang	AXEL is a complete authoring software to create 3d interactive animation for the Internet. Developed by MindAvenue, AXEL makes it easy for designers to create interactive 3D content without scripting.	Creating interactive 3D web content using AXEL	NA	2018
Akira Wakita:Fumio Matsumoto	CT is a project to reconstruct an existing urban space as a 3D information city on the web. A visitor can browse the city with "building wall browsers" and communicate with other visitors.	CT (city tomography)	NA:NA	2018
Peter Coppin:Karl Fischer:Natalie Koch:Dana Martinelli:W. Ronald McCloskey:Michael Wagner	Telepresence is experiencing a place without physically being there. Telepresence interfaces receive information from robots or sensors in distant, hard to reach places. Scientists use telepresence to explore places that are inaccessible to human beings, such as Mars. However, the technology used on such missions is so complex that the missions themselves are as inaccessible to the public as the extreme environments being studied. Subsequently, design and engineering barriers have kept this vast resource off-limits to America's classrooms despite the Internet's widespread proliferation. Existing public telepresence interfaces either do not scale well to worldwide dissemination or do not fully engage school students.	EventScope: discovering Mars with internet-based virtual environments	NA:NA:NA:NA:NA:NA	2018
Robert Reinhardt	To date, the most common forms of real-time messaging between individuals on the Internet have been text-based, including e-mail and instant messaging. Studies have suggested that Internet users overall spend more hours online with e-mail and instant messaging (IM) than with all other web browsing.1 Macromedia's new technology opens up exciting possibilities for rich media applications that allow Internet users to hear and see each other instantly, using standard microphones and video devices. Although the reach of broadband connectivity is expanding into more businesses and homes, most Internet users have yet to utilize the full potential of two-way, high-speed communication available with broadband connections.	Flash MX live: real-time video and audio delivery in multi-user environments	NA	2018
Fusako Nishikubo:Manabu Tanaka:Shinta Ookino:Hiroshi Ogihara:Kazuhito Ezawa	This web content adopts the web3D technology designed for children, especially for younger children.	Gearation: the web3D content for children	NA:NA:NA:NA:NA	2018
Martin Isenburg	The most popular way of distributing 3D content on the Web is in form of a textual representation of the scene such as VRML and its variants. The advantage of such a description is that it is very author friendly in the sense of being meaningful to the human reader. A scene represented in a textual format can be viewed, understood, and modified with any text editor (see Figure 1). Most importantly, anyone can do this, even without knowledge about the specific software package that generated the 3D content.	Geometry compression for ASCII scenes	NA	2018
Annette Weintraub	This presentation describes an approach to interface and experience design in which fact and fiction are mixed in multiple narrative modes. The Mirror That Changes is a Flash-based Web project that integrates fact and fiction, introspection and information in several simultaneous narratives of animated text, voiceover, ambient sound and moving image. Designed to explore issues of water and sustainability from the multiple perspectives of personal use and global resource, The Mirror merges extensive research on water use with a sensual evocation of water so that information and symbolism mesh within a minimalist envelope.	Integrating multiple narratives: the mirror that changes	NA	2018
Ed Sims:Dan Silverglate	Interactive, life-like characters can enhance motivation, communication, and knowledge retention in computer-based learning [Lester et al. 2000]. However, until now, the development cost and computational requirements for highquality animation have limited its widespread use.	Interactive 3D characters for web-based learning and accessibility	NA:NA	2018
Christian Babski:Stéphane Carion:Patrick Keller:Christophe Guignard	_knowscape builds new networked communities of knowledge. It explores original forms of online [user's] representation and builds new kinds of virtual world that carry on information.	_knowscape, a 3D multi-user experimental web browser	NA:NA:NA:NA	2018
Viswanath Parameswaran	Flash has simplified deployment of content on multiple platforms and browsers. Now with the ability to handle Unicode, it takes web applications into the next level. We will be looking at ways for creating multilingual content and a sample multilingual application using FlashMX.	Multilingual flash applications	NA	2018
Hiroya Tanaka:Masatoshi Arikawa:Ryosuke Shibasaki	Pseudo-3D photo collage is a new technique for creating extensive pseudo-3D scenes on the Web. This technique enables users to create, publish, navigate and share pseudo-3D scenes by easy operations. Our basic idea comes from an artistic representation "photo collage" on 2D canvases, that is, a general method of scanning and arranging original photos. Photo collage is originally 2D and static graphic representation, while our proposed representation is pseudo-3D and interactive one. Our developed system for pseudo-3D photo collage is called STAMP (Spatio-Temporal Associations with Multiple Photographs). STAMP includes basic two components as tools: STAMP-Maker and STAMP-Navigator, for creating and navigating pseudo-3D scenes respectively.	Pseudo-3D photo collage	NA:NA:NA	2018
Hidenori Watanave	"Rhythm Engine" (REg) is a spatial communication tool with "Music" and "visual effects" beyond the space time. "REg" proposes an ideal way of new un-simultaneous communication to the current web-world where mainly "exchanging words" on "real time" is getting more focused.	Rhythm engine	NA	2018
Dean Jackson	Scalable Vector Graphics (SVG) is a language for representing two-dimensional graphics. It was developed by the World Wide Web Consortium (W3C) to be the open standard format for both static and animated vector graphics on Web appliances, from desktop machines to mobile devices. The SVG 1.0 specification, whose authors Include representatives from Adobe, Microsoft, Sun, Kodak, Corel, Macromedia, IBM and Apple, became a W3C Recommendation in September 2001. SVG Is rapidly becoming the open standard of choice for graphics on the Web, and the many SVG implementations already in existence ensure the SVG documents can be viewed on a wide range of platforms.	Scalable vector graphics (SVG): the world wide web consortium's recommendation for high quality web graphics	NA	2018
A. Basu:L Cheng:A. Mistri:D. Wolford	In this report we describe and demonstrate our technology for creating and browsing super high resolution (SHR) & 3D digital content for a variety of applications including museum artifacts and galleries, archeology, anthropology, art design and heritage conservation. SHR 3D images and associated wireframes require large bandwidth to be transmitted in full detail. To address limitations resulting from limited bandwidth, two operations are performed: (i) bandwidth is optimally monitored using a statistical model and (ii) the quality of the 3D objects transmitted are adjusted to best fit the measured bandwidth. Regions of interest (ROIs) specified by users are stored in multiple levels of detail hierarchy. Our approach extends past systems [Martinez et al. 2000] for 2D image browsing, and supports quality of service (QoS) [Vogel et al. 1995] based retrieval. Experimental results demonstrate the feasibility of the proposed approach.	Scalable visualization of super high resolution 3D images for museum archiving	NA:NA:NA:NA	2018
Philipp Hoschka	The Synchronized Multimedia Integration Language (SMIL; pronounced "smile") enables authors to bring interactive audiovisual content to the Web. With SMIL, producing audio-visual content is easy; it does not require learning a programming language and can be done using a simple text editor.	SMIL: an introduction	NA	2018
Kazuyuki Okada:Masa Inakage	"Spoiral" is a multi-user online mystery game, which is implemented with web3D technology. (Fig1) Five players experience an ad-lib drama together, as if they are playing a role in a detective novel, in which the story proceeds in real time with real people.	Spoiral: an online ad-lib mystery	NA:NA	2018
Yasuhiro Santo:Catherine Hu:Mamata Rao	The idea behind the Bridge as a collaborative groupware arose initially with the need to find appropriate tools and environments to facilitate international design collaborations. In particular, this tool needs to support on-line collaborative activities in design learning where language is a barrier and conventional groupware fails to facilitate effective communication. Moreover, designers are inherently dependent on visual elements in all levels and phases of design development, something which current text-based groupware again fails to support.	The bridge: an environment for collaborative design learning	NA:NA:NA	2018
Sandro M. Corsaro	Flash is the future of television animation. It cuts costs, saves time, and empowers the artist. There is a gap in the education and understanding of how to effectively utilize this program for broadcast animation purposes. This lecture will serve to educate this gap from both an artistic and production point of view. Creating broadcast animation has long been limited to having lots of money and time. The average half hour cartoon runs between 300,000 to 1.5 million dollars to produce. The traditional animation process takes over twelve weeks to produce one episode with most of the grunt work being done overseas. Flash is an artistically empowering program that will change the face of animation within the next year. Flash is slowly creeping into broadcast. Some traditionalists have hesitated to accept Flash animation as viable method of production is because the taint from the dot com era. The reason Flash has not been accepted as a viable mainstream production method is because for the most part, the animation that has been produced on the web lacked the quality of television and film. Currently, there are very few animators who have walked on both sides of the fence. But any of them will tell you Flash Animation will become a necessity in the television industry.	The evolution of animation: bedrock revisited	NA	2018
Lily Díaz-Kommonen	Sometimes web sites get built to promote a company or a product. It is also possible that a website is constructed as part of the documentation strategy in a project. In my presentation I want to talk about how a website can be used to foster collaboration among the partners in a research project.	The Raisio archaeology archive: using design to build collaborations	NA	2018
Branden Hall:Samuel Wan	The behavioral and cognitive principles of collaboration are well understood, i.e. how people negotiate common meaning in order to work together. During the implementation of collaborative systems, however, the significance of these principles in humancomputer interaction are often shadowed by the low-level challenges of building networked applications. The REALITY CLUSTER project explores the question "What if building networked applications was easy?" by utilizing new technologies recently introduced by Macromedia and the Flash 6 plugin. The REALITY CLUSTER allows multiple users to manipulate a graphical representation of both real-time and stored information in a common repository. The user interface for REALITY CLUSTER borrows principles found in information visualization literature to show relationships between multiple nodes of information while providing users with both focus and context in navigating the nodes. Each node may consist of either recordings or real-time channels for video, audio, text, and static graphics. Hopefully, the REALITY CLUSTER prototype will open web developers to new perspectives in designing web applications. We believe that these technologies from Macromedia, combined with strong grounding in HCI principles and software engineering, will fulfill the promise of a truly disintermediated network communication.	The reality cluster: realtime multimedia communication with persistence	NA:NA	2018
Alan D. Hudson:Justin Couch:Stephen N. Matsuba	This presentation outlines the development process of the Xj3D browser. Xj3D is an open source API for developing X3D and VRML 97 applications. It is also the sample implementation and test bed for the next generation VRML specification known as Extensible 3D (X3D). Indeed, Xj3D was initiated by the Web3D Consortium to provide input to the X3D authors and the 3D graphics community with input concerning problems and ambiguities with the specification.	The Xj3D browser: community-based 3D software development	NA:NA:NA	2018
Yuichiro Haraguchi:Sakura Toyabe:Masaru Murata	TTT is a Web community tool that promotes new encounters mediated by friends.	TTT: a web community tool mediated by friends	NA:NA:NA	2018
T. J. Jankun-Kelly:Kwan-Liu Ma	The exploration of complex data sets requires interfaces to present and navigate through the visualization of the data. In recent work [Jankun-Kelly and Ma 2001], we produced a visualization exploration spreadsheet to address this issue. The developed application, however, was implemented for off-line use only. For data sets on remote sites, this approach is not appropriate. Thus, a web-based version of the visualization exploration spreadsheet is needed. This abstract discusses the process of transforming the interface from an off-line to an on-line design.	VisSheet redux: redesigning a visualization exploration spreadsheet for the web	NA:NA	2018
Aaron Hertzmann	NA	Session details: Texture synthesis by example	NA	2018
Vivek Kwatra:Arno Schödl:Irfan Essa:Greg Turk:Aaron Bobick	In this paper we introduce a new algorithm for image and video texture synthesis. In our approach, patch regions from a sample image or video are transformed and copied to the output and then stitched together along optimal seams to generate a new (and typically larger) output. In contrast to other techniques, the size of the patch is not chosen a-priori, but instead a graph cut technique is used to determine the optimal patch region for any given offset between the input and output texture. Unlike dynamic programming, our graph cut technique for seam optimization is applicable in any dimension. We specifically explore it in 2D and 3D to perform video texture synthesis in addition to regular image synthesis. We present approximative offset search techniques that work well in conjunction with the presented patch size optimization. We show results for synthesizing regular, random, and natural images and videos. We also demonstrate how this method can be used to interactively merge different images to generate new scenes.	Graphcut textures: image and video synthesis using graph cuts	NA:NA:NA:NA:NA	2018
Michael F. Cohen:Jonathan Shade:Stefan Hiller:Oliver Deussen	We present a simple stochastic system for non-periodically tiling the plane with a small set of Wang Tiles. The tiles may be filled with texture, patterns, or geometry that when assembled create a continuous representation. The primary advantage of using Wang Tiles is that once the tiles are filled, large expanses of non-periodic texture (or patterns or geometry) can be created as needed very efficiently at runtime.Wang Tiles are squares in which each edge is assigned a color. A valid tiling requires all shared edges between tiles to have matching colors. We present a new stochastic algorithm to non-periodically tile the plane with a small set of Wang Tiles at runtime.Furthermore, we present new methods to fill the tiles with 2D texture, 2D Poisson distributions, or 3D geometry to efficiently create at runtime as much non-periodic texture (or distributions, or geometry) as needed. We leverage previous texture synthesis work and adapt it to fill Wang Tiles. We demonstrate how to fill individual tiles with Poisson distributions that maintain their statistical properties when combined. These are used to generate a large arrangement of plants or other objects on a terrain. We show how such environments can be rendered efficiently by pre-lighting the individual Wang Tiles containing the geometry.We also extend the definition of Wang Tiles to include a coding of the tile corners to allow discrete objects to overlap more than one edge. The larger set of tiles provides increased degrees of freedom.	Wang Tiles for image and texture generation	NA:NA:NA:NA	2018
Jingdan Zhang:Kun Zhou:Luiz Velho:Baining Guo:Heung-Yeung Shum	We present an approach for decorating surfaces with progressively-variant textures. Unlike a homogeneous texture, a progressively-variant texture can model local texture variations, including the scale, orientation, color, and shape variations of texture elements. We describe techniques for modeling progressively-variant textures in 2D as well as for synthesizing them over surfaces. For 2D texture modeling, our feature-based warping technique allows the user to control the shape variations of texture elements, making it possible to capture complex texture variations such as those seen in animal coat patterns. In addition, our feature-based blending technique can create a smooth transition between two given homogeneous textures, with progressive changes of both shapes and colors of texture elements. For synthesizing textures over surfaces, the biggest challenge is that the synthesized texture elements tend to break apart as they progressively vary. To address this issue, we propose an algorithm based on texton masks, which mark most prominent texture elements in the 2D texture sample. By leveraging the power of texton masks, our algorithm can maintain the integrity of the synthesized texture elements on the target surface.	Synthesis of progressively-variant textures on arbitrary surfaces	NA:NA:NA:NA:NA	2018
Iddo Drori:Daniel Cohen-Or:Hezy Yeshurun	We present a new method for completing missing parts caused by the removal of foreground or background elements from an image. Our goal is to synthesize a complete, visually plausible and coherent image. The visible parts of the image serve as a training set to infer the unknown parts. Our method iteratively approximates the unknown regions and composites adaptive image fragments into the image. Values of an inverse matte are used to compute a confidence map and a level set that direct an incremental traversal within the unknown area from high to low confidence. In each step, guided by a fast smooth approximation, an image fragment is selected from the most similar and frequent examples. As the selected fragments are composited, their likelihood increases along with the mean confidence of the image, until reaching a complete image. We demonstrate our method by completion of photographs and paintings.	Fragment-based image completion	NA:NA:NA	2018
Stephen Marschner	NA	Session details: Images, video, and texture	NA	2018
Patrick Pérez:Michel Gangnet:Andrew Blake	Using generic interpolation machinery based on solving Poisson equations, a variety of novel tools are introduced for seamless editing of image regions. The first set of tools permits the seamless importation of both opaque and transparent source image regions into a destination region. The second set is based on similar mathematical ideas and allows the user to modify the appearance of the image seamlessly, within a selected region. These changes can be arranged to affect the texture, the illumination, and the color of objects lying in the region, or to make tileable a rectangular selection.	Poisson image editing	NA:NA:NA	2018
Sing Bing Kang:Matthew Uyttendaele:Simon Winder:Richard Szeliski	Typical video footage captured using an off-the-shelf camcorder suffers from limited dynamic range. This paper describes our approach to generate high dynamic range (HDR) video from an image sequence of a dynamic scene captured while rapidly varying the exposure of each frame. Our approach consists of three parts: automatic exposure control during capture, HDR stitching across neighboring frames, and tonemapping for viewing. HDR stitching requires accurately registering neighboring frames and choosing appropriate pixels for computing the radiance map. We show examples for a variety of dynamic scenes. We also show how we can compensate for scene and camera movement when creating an HDR still from a series of bracketed still photographs.	High dynamic range video	NA:NA:NA:NA	2018
Vladislav Kraevoy:Alla Sheffer:Craig Gotsman	Texture mapping enhances the visual realism of 3D models by adding fine details. To achieve the best results, it is often necessary to force a correspondence between some of the details of the texture and the features of the model.The most common method for mapping texture onto 3D meshes is to use a planar parameterization of the mesh. This, however, does not reflect any special correspondence between the mesh geometry and the texture. The Matchmaker algorithm presented here forces user-defined feature correspondence for planar parameterization of meshes. This is achieved by adding positional constraints to the planar parameterization. Matchmaker allows users to introduce scores of constraints while maintaining a valid one-to-one mapping between the embedding and the 3D surface. Matchmaker's constraint mechanism can be used for other applications requiring parameterization besides texture mapping, such as morphing and remeshing.Matchmaker begins with an unconstrained planar embedding of the 3D mesh generated by conventional methods. It moves the constrained vertices to the required positions by matching a triangulation of these positions to a triangulation of the planar mesh formed by paths between constrained vertices. The matching triangulations are used to generate a new parameterization that satisfies the constraints while minimizing the deviation from the original 3D geometry.	Matchmaker: constructing constrained texture maps	NA:NA:NA	2018
Lifeng Wang:Xi Wang:Xin Tong:Stephen Lin:Shimin Hu:Baining Guo:Heung-Yeung Shum	Significant visual effects arise from surface mesostructure, such as fine-scale shadowing, occlusion and silhouettes. To efficiently render its detailed appearance, we introduce a technique called view-dependent displacement mapping (VDM) that models surface displacements along the viewing direction. Unlike traditional displacement mapping, VDM allows for efficient rendering of self-shadows, occlusions and silhouettes without increasing the complexity of the underlying surface mesh. VDM is based on per-pixel processing, and with hardware acceleration it can render mesostructure with rich visual appearance in real time.	View-dependent displacement mapping	NA:NA:NA:NA:NA:NA:NA	2018
Michael Garland	NA	Session details: Parameterization	NA	2018
Emil Praun:Hugues Hoppe	The traditional approach for parametrizing a surface involves cutting it into charts and mapping these piecewise onto a planar domain. We introduce a robust technique for directly parametrizing a genus-zero surface onto a spherical domain. A key ingredient for making such a parametrization practical is the minimization of a stretch-based measure, to reduce scale-distortion and thereby prevent undersampling. Our second contribution is a scheme for sampling the spherical domain using uniformly subdivided polyhedral domains, namely the tetrahedron, octahedron, and cube. We show that these particular semi-regular samplings can be conveniently represented as completely regular 2D grids, i.e. geometry images. Moreover, these images have simple boundary extension rules that aid many processing operations. Applications include geometry remeshing, level-of-detail, morphing, compression, and smooth surface subdivision.	Spherical parametrization and remeshing	NA:NA	2018
Andrei Khodakovsky:Nathan Litke:Peter Schröder	Good parameterizations are of central importance in many digital geometry processing tasks. Typically the behavior of such processing algorithms is related to the smoothness of the parameterization and how much distortion it contains. Since a parameterization maps a bounded region of the plane to the surface, a parameterization for a surface which is not homeomorphic to a disc must be made up of multiple pieces. We present a novel parameterization algorithm for arbitrary topology surface meshes which computes a globally smooth parameterization with low distortion. We optimize the patch layout subject to criteria such as shape quality and metric distortion, which are used to steer a mesh simplification approach for base complex construction. Global smoothness is achieved through simultaneous relaxation over all patches, with suitable transition functions between patches incorporated into the relaxation procedure. We demonstrate the quality of our parameterizations through numerical evaluation of distortion measures and the excellent rate distortion performance of semi-regular remeshes produced with these parameterizations. The numerical algorithms required to compute the parameterizations are robust and run on the order of minutes even for large meshes.	Globally smooth parameterizations with low distortion	NA:NA:NA	2018
Craig Gotsman:Xianfeng Gu:Alla Sheffer	Parameterization of 3D mesh data is important for many graphics applications, in particular for texture mapping, remeshing and morphing. Closed manifold genus-0 meshes are topologically equivalent to a sphere, hence this is the natural parameter domain for them. Parameterizing a triangle mesh onto the sphere means assigning a 3D position on the unit sphere to each of the mesh vertices, such that the spherical triangles induced by the mesh connectivity are not too distorted and do not overlap. Satisfying the non-overlapping requirement is the most difficult and critical component of this process. We describe a generalization of the method of barycentric coordinates for planar parameterization which solves the spherical parameterization problem, prove its correctness by establishing a connection to spectral graph theory and show how to compute these parameterizations.	Fundamentals of spherical parameterization for 3D meshes	NA:NA:NA	2018
Bruno Lévy	Shape optimization and surface fairing for polygon meshes have been active research areas for the last few years. Existing approaches either require the border of the surface to be fixed, or are only applicable to closed surfaces. In this paper, we propose a new approach, that computes natural boundaries. This makes it possible not only to smooth an existing geometry, but also to extrapolate its shape beyond the existing border. Our approach is based on a global parameterization of the surface and on a minimization of the squared curvatures, discretized on the edges of the surface. The so-constructed surface is an approximation of a minimal energy surface (MES). Using a global parameterization makes it possible to completely decouple the outer fairness (surface smoothness) from the inner fairness (mesh quality). In addition, the parameter space provides the user with a new means of controlling the shape of the surface. When used as a geometry filter, our approach computes a smoothed mesh that is discrete conformal to the original one. This allows smoothing textured meshes without introducing distortions.	Dual domain extrapolation	NA	2018
Henrik Wann Jensen	NA	Session details: Precomputed radiance transfer	NA	2018
Peter-Pike Sloan:Xinguo Liu:Heung-Yeung Shum:John Snyder	Radiance transfer represents how generic source lighting is shadowed and scattered by an object to produce view-dependent appearance. We generalize by rendering transfer at two scales. A macro-scale is coarsely sampled over an object's surface, providing global effects like shadows cast from an arm onto a body. A meso-scale is finely sampled over a small patch to provide local texture. Low-order (25D) spherical harmonics represent low-frequency lighting dependence for both scales. To render, a coefficient vector representing distant source lighting is first transformed at the macro-scale by a matrix at each vertex of a coarse mesh. The resulting vectors represent a spatially-varying hemisphere of lighting incident to the meso-scale. A 4D function, called a radiance transfer texture (RTT), then specifies the surface's meso-scale response to each lighting basis component, as a function of a spatial index and a view direction. Finally, a 25D dot product of the macro-scale result vector with the vector looked up from the RTT performs the correct shading integral. We use an id map to place RTT samples from a small patch over the entire object; only two scalars are specified at high spatial resolution. Results show that bi-scale decomposition makes preprocessing practical and efficiently renders self-shadowing and interreflection effects from dynamic, low-frequency light sources at both scales.	Bi-scale radiance transfer	NA:NA:NA:NA	2018
Ren Ng:Ravi Ramamoorthi:Pat Hanrahan	We present a method, based on pre-computed light transport, for real-time rendering of objects under all-frequency, time-varying illumination represented as a high-resolution environment map. Current techniques are limited to small area lights, with sharp shadows, or large low-frequency lights, with very soft shadows. Our main contribution is to approximate the environment map in a wavelet basis, keeping only the largest terms (this is known as a non-linear approximation). We obtain further compression by encoding the light transport matrix sparsely but accurately in the same basis. Rendering is performed by multiplying a sparse light vector by a sparse transport matrix, which is very fast. For accurate rendering, using non-linear wavelets is an order of magnitude faster than using linear spherical harmonics, the current best technique.	All-frequency shadows using non-linear wavelet lighting approximation	NA:NA:NA	2018
Peter-Pike Sloan:Jesse Hall:John Hart:John Snyder	We compress storage and accelerate performance of precomputed radiance transfer (PRT), which captures the way an object shadows, scatters, and reflects light. PRT records over many surface points a transfer matrix. At run-time, this matrix transforms a vector of spherical harmonic coefficients representing distant, low-frequency source lighting into exiting radiance. Per-point transfer matrices form a high-dimensional surface signal that we compress using clustered principal component analysis (CPCA), which partitions many samples into fewer clusters each approximating the signal as an affine subspace. CPCA thus reduces the high-dimensional transfer signal to a low-dimensional set of per-point weights on a per-cluster set of representative matrices. Rather than computing a weighted sum of representatives and applying this result to the lighting, we apply the representatives to the lighting per-cluster (on the CPU) and weight these results per-point (on the GPU). Since the output of the matrix is lower-dimensional than the matrix itself, this reduces computation. We also increase the accuracy of encoded radiance functions with a new least-squares optimal projection of spherical harmonics onto the hemisphere. We describe an implementation on graphics hardware that performs real-time rendering of glossy objects with dynamic self-shadowing and interreflection without fixing the view or light as in previous work. Our approach also allows significantly increased lighting frequency when rendering diffuse objects and includes subsurface scattering.	Clustered principal components for precomputed radiance transfer	NA:NA:NA:NA	2018
Michiel van de Panne	NA	Session details: Character animation	NA	2018
Tae-hoon Kim:Sang Il Park:Sung Yong Shin	Real-time animation of human-like characters is an active research area in computer graphics. The conventional approaches have, however, hardly dealt with the rhythmic patterns of motions, which are essential in handling rhythmic motions such as dancing and locomotive motions. In this paper, we present a novel scheme for synthesizing a new motion from unlabelled example motions while preserving their rhythmic pattern. Our scheme first captures the motion beats from the example motions to extract the basic movements and their transitions. Based on those data, our scheme then constructs a movement transition graph that represents the example motions. Given an input sound signal, our scheme finally synthesizes a novel motion in an on-line manner while traversing the motion transition graph, which is synchronized with the input sound signal and also satisfies kinematic constraints given explicitly and implicitly. Through experiments, we have demonstrated that our scheme can effectively produce a variety of rhythmic motions.	Rhythmic-motion synthesis based on motion-beat analysis	NA:NA:NA	2018
Okan Arikan:David A. Forsyth:James F. O'Brien	This paper describes a framework that allows a user to synthesize human motion while retaining control of its qualitative properties. The user paints a timeline with annotations --- like walk, run or jump --- from a vocabulary which is freely chosen by the user. The system then assembles frames from a motion database so that the final motion performs the specified actions at specified times. The motion can also be forced to pass through particular configurations at particular times, and to go to a particular position and orientation. Annotations can be painted positively (for example, must run), negatively (for example, may not run backwards) or as a don't-care. The system uses a novel search method, based around dynamic programming at several scales, to obtain a solution efficiently so that authoring is interactive. Our results demonstrate that the method can generate smooth, natural-looking motion.The annotation vocabulary can be chosen to fit the application, and allows specification of composite motions (run and jump simultaneously, for example). The process requires a collection of motion data that has been annotated with the chosen vocabulary. This paper also describes an effective tool, based around repeated use of support vector machines, that allows a user to annotate a large collection of motions quickly and easily so that they may be used with the synthesis algorithm.	Motion synthesis from annotations	NA:NA:NA	2018
Mira Dontcheva:Gary Yngve:Zoran Popović	We introduce an acting-based animation system for creating and editing character animation at interactive speeds. Our system requires minimal training, typically under an hour, and is well suited for rapidly prototyping and creating expressive motion. A real-time motion-capture framework records the user's motions for simultaneous analysis and playback on a large screen. The animator's real-world, expressive motions are mapped into the character's virtual world. Visual feedback maintains a tight coupling between the animator and character. Complex motion is created by layering multiple passes of acting. We also introduce a novel motion-editing technique, which derives implicit relationships between the animator and character. The animator mimics some aspect of the character motion, and the system infers the association between features of the animator's motion and those of the character. The animator modifies the mimic by acting again, and the system maps the changes onto the character. We demonstrate our system with several examples and present the results from informal user studies with expert and novice animators.	Layered acting for character animation	NA:NA:NA	2018
Anthony C. Fang:Nancy S. Pollard	Optimization is a promising way to generate new animations from a minimal amount of input data. Physically based optimization techniques, however, are difficult to scale to complex animated characters, in part because evaluating and differentiating physical quantities becomes prohibitively slow. Traditional approaches often require optimizing or constraining parameters involving joint torques; obtaining first derivatives for these parameters is generally an O(D2) process, where D is the number of degrees of freedom of the character. In this paper, we describe a set of objective functions and constraints that lead to linear time analytical first derivatives. The surprising finding is that this set includes constraints on physical validity, such as ground contact constraints. Considering only constraints and objective functions that lead to linear time first derivatives results in fast per-iteration computation times and an optimization problem that appears to scale well to more complex characters. We show that qualities such as squash-and-stretch that are expected from physically based optimization result from our approach. Our animation system is particularly useful for synthesizing highly dynamic motions, and we show examples of swinging and leaping motions for characters having from 7 to 22 degrees of freedom.	Efficient synthesis of physically valid human motion	NA:NA	2018
Jack Tumblin	NA	Session details: Visualization and printing	NA	2018
Roger D. Hersch:Fabien Collaud:Patrick Emmel	By combining a metallic ink and standard inks, one may create printed images having a dynamic appearance: an image viewed under specular reflection may be considerably different from the same image viewed under non-specular reflection. Patterns which are either dark or hidden become highlighted under specular reflection, yielding interesting visual effects. To create such images, one needs to be able to reproduce at non-specular reflection angles the same colors, by standard inks alone or in combination with a metallic ink. Accurate color prediction models need to be established which model the underlying physical phenomena in a consistent manner. To meet this challenge, we propose two models, one for predicting the reflection spectra of standard inks on coated paper and one for predicting the reflection spectra of a combination of standard inks and a metallic ink. They are enhancements of the classical Clapper-Yule model which models optical dot gain of halftone prints by taking into account lateral scattering within the paper bulk and multiple internal reflections. The models we propose also take into account physical dot gain and ink spreading for standard inks as well as the low reflectance of metallic inks at non-specular reflection angles and the poor adherence of standard inks printed on top of a metallic ink (trapping effect). These models open the way towards color separation of images to be reproduced by combining a metallic ink and standard inks. Several designs printed on an offset press demonstrate their applicability and their benefits for high-end design and security applications.	Reproducing color images with embedded metallic patterns	NA:NA:NA	2018
Bingfeng Zhou:Xifeng Fang	In this paper, we describe the use of threshold modulation to remove the visual artifacts contained in the variable-coefficient error-diffusion algorithm. To obtain a suitable parameter set for the threshold modulation, a cost function used for the search of optimal parameters is designed. An optimal diffusion parameter set, as well as the corresponding threshold modulation strength values, is thus obtained. Experiments over this new set of parameters show that, compared with the original variable-coefficient error-diffusion algorithm, threshold modulation can remove visual anomalies more effectively. The result of the new algorithm is an artifact-free halftoning in the full range of intensities. Fourier analysis of the experimental results further support this conclusion.	Improving mid-tone quality of variable-coefficient error diffusion using threshold modulation	NA:NA	2018
Yiying Tong:Santiago Lombeyda:Anil N. Hirani:Mathieu Desbrun	While 2D and 3D vector fields are ubiquitous in computational sciences, their use in graphics is often limited to regular grids, where computations are easily handled through finite-difference methods. In this paper, we propose a set of simple and accurate tools for the analysis of 3D discrete vector fields on arbitrary tetrahedral grids. We introduce a variational, multiscale decomposition of vector fields into three intuitive components: a divergence-free part, a curl-free part, and a harmonic part. We show how our discrete approach matches its well-known smooth analog, called the Helmotz-Hodge decomposition, and that the resulting computational tools have very intuitive geometric interpretation. We demonstrate the versatility of these tools in a series of applications, ranging from data visualization to fluid and deformable object simulation.	Discrete multiscale vector field decomposition	NA:NA:NA:NA	2018
Tamara Munzner:François Guimbretière:Serdar Tasiran:Li Zhang:Yunhong Zhou	Structural comparison of large trees is a difficult task that is only partially supported by current visualization techniques, which are mainly designed for browsing. We present TreeJuxtaposer, a system designed to support the comparison task for large trees of several hundred thousand nodes. We introduce the idea of "guaranteed visibility", where highlighted areas are treated as landmarks that must remain visually apparent at all times. We propose a new methodology for detailed structural comparison between two trees and provide a new nearly-linear algorithm for computing the best corresponding node from one tree to another. In addition, we present a new rectilinear Focus+Context technique for navigation that is well suited to the dynamic linking of side-by-side views while guaranteeing landmark visibility and constant frame rates. These three contributions result in a system delivering a fluid exploration experience that scales both in the size of the dataset and the number of pixels in the display. We have based the design decisions for our system on the needs of a target audience of biologists who must understand the structural details of many phylogenetic, or evolutionary, trees. Our tool is also useful in many other application domains where tree comparison is needed, ranging from network management to call graph optimization to genealogy.	TreeJuxtaposer: scalable tree comparison using Focus+Context with guaranteed visibility	NA:NA:NA:NA:NA	2018
Emil Praun	NA	Session details: Surfaces	NA	2018
Yutaka Ohtake:Alexander Belyaev:Marc Alexa:Greg Turk:Hans-Peter Seidel	We present a new shape representation, the multi-level partition of unity implicit surface, that allows us to construct surface models from very large sets of points. There are three key ingredients to our approach: 1) piecewise quadratic functions that capture the local shape of the surface, 2) weighting functions (the partitions of unity) that blend together these local shape functions, and 3) an octree subdivision method that adapts to variations in the complexity of the local shape.Our approach gives us considerable flexibility in the choice of local shape functions, and in particular we can accurately represent sharp features such as edges and corners by selecting appropriate shape functions. An error-controlled subdivision leads to an adaptive approximation whose time and memory consumption depends on the required accuracy. Due to the separation of local approximation and local blending, the representation is not global and can be created and evaluated rapidly. Because our surfaces are described using implicit functions, operations such as shape blending, offsets, deformations and CSG are simple to perform.	Multi-level partition of unity implicits	NA:NA:NA:NA:NA	2018
Haeyoung Lee:Mathieu Desbrun:Peter Schröder	We present a progressive encoding technique specifically designed for complex isosurfaces. It achieves better rate distortion performance than all standard mesh coders, and even improves on all previous single rate isosurface coders. Our novel algorithm handles isosurfaces with or without sharp features, and deals gracefully with high topologic and geometric complexity. The inside/outside function of the volume data is progressively transmitted through the use of an adaptive octree, while a local frame based encoding is used for the fine level placement of surface samples. Local patterns in topology and local smoothness in geometry are exploited by context-based arithmetic encoding, allowing us to achieve an average of 6.10 bits per vertex (b/v) at very low distortion. Of this rate only 0.65 b/v are dedicated to connectivity data: this improves by 24% over the best previous single rate isosurface encoder.	Progressive encoding of complex isosurfaces	NA:NA:NA	2018
Thomas W. Sederberg:Jianmin Zheng:Almaz Bakenov:Ahmad Nasri	This paper presents a generalization of non-uniform B-spline surfaces called T-splines. T-spline control grids permit T-junctions, so lines of control points need not traverse the entire control grid. T-splines support many valuable operations within a consistent framework, such as local refinement, and the merging of several B-spline surfaces that have different knot vectors into a single gap-free model. The paper focuses on T-splines of degree three, which are C2 (in the absence of multiple knots). T-NURCCs (Non-Uniform Rational Catmull-Clark Surfaces with T-junctions) are a superset of both T-splines and Catmull-Clark surfaces. Thus, a modeling program for T-NURCCs can handle any NURBS or Catmull-Clark model as special cases. T-NURCCs enable true local refinement of a Catmull-Clark-type control grid: individual control points can be inserted only where they are needed to provide additional control, or to create a smoother tessellation, and such insertions do not alter the limit surface. T-NURCCs use stationary refinement rules and are C2 except at extraordinary points and features.	T-splines and T-NURCCs	NA:NA:NA:NA	2018
Pierre Alliez:David Cohen-Steiner:Olivier Devillers:Bruno Lévy:Mathieu Desbrun	In this paper, we propose a novel polygonal remeshing technique that exploits a key aspect of surfaces: the intrinsic anisotropy of natural or man-made geometry. In particular, we use curvature directions to drive the remeshing process, mimicking the lines that artists themselves would use when creating 3D models from scratch. After extracting and smoothing the curvature tensor field of an input genus-0 surface patch, lines of minimum and maximum curvatures are used to determine appropriate edges for the remeshed version in anisotropic regions, while spherical regions are simply point sampled since there is no natural direction of symmetry locally. As a result our technique generates polygon meshes mainly composed of quads in anisotropic regions, and of triangles in spherical regions. Our approach provides the flexibility to produce meshes ranging from isotropic to anisotropic, from coarse to dense, and from uniform to curvature adapted.	Anisotropic polygonal remeshing	NA:NA:NA:NA:NA	2018
Kavita Bala	NA	Session details: Shadows	NA	2018
Yung-Yu Chuang:Dan B Goldman:Brian Curless:David H. Salesin:Richard Szeliski	In this paper, we describe a method for extracting shadows from one natural scene and inserting them into another. We develop physically-based shadow matting and compositing equations and use these to pull a shadow matte from a source scene in which the shadow is cast onto an arbitrary planar background. We then acquire the photometric and geometric properties of the target scene by sweeping oriented linear shadows (cast by a straight object) across it. From these shadow scans, we can construct a shadow displacement map without requiring camera or light source calibration. This map can then be used to deform the original shadow matte. We demonstrate our approach for both indoor scenes with controlled lighting and for outdoor scenes using natural lighting.	Shadow matting and compositing	NA:NA:NA:NA:NA	2018
Naga K. Govindaraju:Brandon Lloyd:Sung-Eui Yoon:Avneesh Sud:Dinesh Manocha	We present a new algorithm for interactive generation of hard-edged, umbral shadows in complex environments with a moving light source. Our algorithm uses a hybrid approach that combines the image quality of object-precision methods with the efficiencies of image-precision techniques. We present an algorithm for computing a compact potentially visible set (PVS) using levels-of-detail (LODs) and visibility culling. We use the PVSs computed from both the eye and the light in a novel cross-culling algorithm that identifies a reduced set of potential shadow-casters and shadow-receivers. Finally, we use a combination of shadow-polygons and shadow maps to generate shadows. We also present techniques for LOD-selection to minimize possible artifacts arising from the use of LODs. Our algorithm can generate sharp shadow edges and reduces the aliasing in pure shadow map approaches. We have implemented the algorithm on a three-PC system with NVIDIA GeForce 4 cards. We achieve 7--25 frames per second in three complex environments composed of millions of triangles.	Interactive shadow generation in complex environments	NA:NA:NA:NA:NA	2018
Ulf Assarsson:Tomas Akenine-Möller	Most previous soft shadow algorithms have either suffered from aliasing, been too slow, or could only use a limited set of shadow casters and/or receivers. Therefore, we present a strengthened soft shadow volume algorithm that deals with these problems. Our critical improvements include robust penumbra wedge construction, geometry-based visibility computation, and also simplified computation through a four-dimensional texture lookup. This enables us to implement the algorithm using programmable graphics hardware, and it results in images that most often are indistinguishable from images created as the average of 1024 hard shadow images. Furthermore, our algorithm can use both arbitrary shadow casters and receivers. Also, one version of our algorithm completely avoids sampling artifacts which is rare for soft shadow algorithms. As a bonus, the four-dimensional texture lookup allows for small textured light sources, and, even video textures can be used as light sources. Our algorithm has been implemented in pure software, and also using the GeForce FX emulator with pixel shaders. Our software implementation renders soft shadows at 0.5--5 frames per second for the images in this paper. With actual hardware, we expect that our algorithm will render soft shadows in real time. An important performance measure is bandwidth usage. For the same image quality, an algorithm using the accumulated hard shadow images uses almost two orders of magnitude more bandwidth than our algorithm.	A geometry-based soft shadow volume algorithm using graphics hardware	NA:NA	2018
Pradeep Sen:Mike Cammarano:Pat Hanrahan	The most popular techniques for interactive rendering of hard shadows are shadow maps and shadow volumes. Shadow maps work well in regions that are completely in light or in shadow but result in objectionable artifacts near shadow boundaries. In contrast, shadow volumes generate precise shadow boundaries but require high fill rates. In this paper, we propose the method of silhouette maps, in which a shadow depth map is augmented by storing the location of points on the geometric silhouette. This allows the shader to construct a piecewise linear approximation to the true shadow silhouette, improving the visual quality over the piecewise constant approximation of conventional shadow maps. We demonstrate an implementation of our approach running on programmable graphics hardware in real-time.	Shadow silhouette maps	NA:NA:NA	2018
Holly Rushmeier	NA	Session details: Perception and manipulation	NA	2018
Carol O'Sullivan:John Dingliana:Thanh Giang:Mary K. Kaiser	For many systems that produce physically based animations, plausibility rather than accuracy is acceptable. We consider the problem of evaluating the visual quality of animations in which physical parameters have been distorted or degraded, either unavoidably due to real-time frame-rate requirements, or intentionally for aesthetic reasons. To date, no generic means of evaluating or predicting the fidelity, either physical or visual, of the dynamic events occurring in an animation exists. As a first step towards providing such a metric, we present a set of psychophysical experiments that established some thresholds for human sensitivity to dynamic anomalies, including angular, momentum and spatio-temporal distortions applied to simple animations depicting the elastic collision of two rigid objects. In addition to finding significant acceptance thresholds for these distortions under varying conditions, we identified some interesting biases that indicate non-symmetric responses to these distortions (e.g., expansion of the angle between post-collision trajectories was preferred to contraction and increases in velocity were preferred to decreases). Based on these results, we derived a set of probability functions that can be used to evaluate the visual fidelity of a physically based simulation. To illustrate how our results could be used, two simple case studies of simulation levels of detail and constrained dynamics are presented.	Evaluating the visual fidelity of physically based animations	NA:NA:NA:NA	2018
Paul S. A. Reitsma:Nancy S. Pollard	Motion capture data and techniques for blending, editing, and sequencing that data can produce rich, realistic character animation; however, the output of these motion processing techniques sometimes appears unnatural. For example, the motion may violate physical laws or reflect unreasonable forces from the character or the environment. While problems such as these can be fixed, doing so is not yet feasible in real time environments. We are interested in developing ways to estimate perceived error in animated human motion so that the output quality of motion processing techniques can be better controlled to meet user goals.This paper presents results of a study of user sensitivity to errors in animated human motion. Errors were systematically added to human jumping motion, and the ability of subjects to detect these errors was measured. We found that users were able to detect motion with errors, and noted some interesting trends: errors in horizontal velocity were easier to detect than errors in vertical velocity, and added accelerations were easier to detect than added decelerations. On the basis of our results, we propose a perceptually based metric for measuring errors in ballistic human motion.	Perceptual metrics for character animation: sensitivity to errors in ballistic motion	NA:NA	2018
Miguel A. Otaduy:Ming C. Lin	We introduce a novel "sensation preserving" simplification algorithm for faster collision queries between two polyhedral objects in haptic rendering. Given a polyhedral model, we construct a multiresolution hierarchy using " filtered edge collapse", subject to constraints imposed by collision detection. The resulting hierarchy is then used to compute fast contact response for haptic display. The computation model is inspired by human tactual perception of contact information. We have successfully applied and demonstrated the algorithm on a time-critical collision query framework for haptically displaying complex object-object interaction. Compared to existing exact contact query algorithms, we observe noticeable performance improvement in update rates with little degradation in the haptic perception of contacts.	Sensation preserving simplification for haptic rendering	NA:NA	2018
Irfan Essa	NA	Session details: Human bodies	NA	2018
Kolja Kähler:Jörg Haber:Hans-Peter Seidel	Facial reconstruction for postmortem identification of humans from their skeletal remains is a challenging and fascinating part of forensic art. The former look of a face can be approximated by predicting and modeling the layers of tissue on the skull. This work is as of today carried out solely by physical sculpting with clay, where experienced artists invest up to hundreds of hours to craft a reconstructed face model. Remarkably, one of the most popular tissue reconstruction methods bears many resemblances with surface fitting techniques used in computer graphics, thus suggesting the possibility of a transfer of the manual approach to the computer. In this paper, we present a facial reconstruction approach that fits an anatomy-based virtual head model, incorporating skin and muscles, to a scanned skull using statistical data on skull / tissue relationships. The approach has many advantages over the traditional process: a reconstruction can be completed in about an hour from acquired skull data; also, variations such as a slender or a more obese build of the modeled individual are easily created. Last not least, by matching not only skin geometry but also virtual muscle layers, an animatable head model is generated that can be used to form facial expressions beyond the neutral face typically used in physical reconstructions.	Reanimating the dead: reconstruction of expressive faces from skull data	NA:NA:NA	2018
Alex Mohr:Michael Gleicher	Good character animation requires convincing skin deformations including subtleties and details like muscle bulges. Such effects are typically created in commercial animation packages which provide very general and powerful tools. While these systems are convenient and flexible for artists, the generality often leads to characters that are slow to compute or that require a substantial amount of memory and thus cannot be used in interactive systems. Instead, interactive systems restrict artists to a specific character deformation model which is fast and memory efficient but is notoriously difficult to author and can suffer from many deformation artifacts. This paper presents an automated framework that allows character artists to use the full complement of tools in high-end systems to create characters for interactive systems. Our method starts with an arbitrarily rigged character in an animation system. A set of examples is exported, consisting of skeleton configurations paired with the deformed geometry as static meshes. Using these examples, we fit the parameters of a deformation model that best approximates the original data yet remains fast to compute and compact in memory.	Building efficient, accurate character skins from examples	NA:NA	2018
Joel Carranza:Christian Theobalt:Marcus A. Magnor:Hans-Peter Seidel	In free-viewpoint video, the viewer can interactively choose his viewpoint in 3-D space to observe the action of a dynamic real-world scene from arbitrary perspectives. The human body and its motion plays a central role in most visual media and its structure can be exploited for robust motion estimation and efficient visualization. This paper describes a system that uses multi-view synchronized video footage of an actor's performance to estimate motion parameters and to interactively re-render the actor's appearance from any viewpoint.The actor's silhouettes are extracted from synchronized video frames via background segmentation and then used to determine a sequence of poses for a 3D human body model. By employing multi-view texturing during rendering, time-dependent changes in the body surface are reproduced in high detail. The motion capture subsystem runs offline, is non-intrusive, yields robust motion parameter estimates, and can cope with a broad range of motion. The rendering subsystem runs at real-time frame rates using ubiquous graphics hardware, yielding a highly naturalistic impression of the actor. The actor can be placed in virtual environments to create composite dynamic scenes. Free-viewpoint video allows the creation of camera fly-throughs or viewing the action interactively from arbitrary perspectives.	Free-viewpoint video of human actors	NA:NA:NA:NA	2018
Peter Sand:Leonard McMillan:Jovan Popović	We describe a method for the acquisition of deformable human geometry from silhouettes. Our technique uses a commercial tracking system to determine the motion of the skeleton, then estimates geometry for each bone using constraints provided by the silhouettes from one or more cameras. These silhouettes do not give a complete characterization of the geometry for a particular point in time, but when the subject moves, many observations of the same local geometries allow the construction of a complete model. Our reconstruction algorithm provides a simple mechanism for solving the problems of view aggregation, occlusion handling, hole filling, noise removal, and deformation modeling. The resulting model is parameterized to synthesize geometry for new poses of the skeleton. We demonstrate this capability by rendering the geometry for motion sequences that were not included in the original datasets.	Continuous capture of skin deformation	NA:NA:NA	2018
Brett Allen:Brian Curless:Zoran Popović	We develop a novel method for fitting high-resolution template meshes to detailed human body range scans with sparse 3D markers. We formulate an optimization problem in which the degrees of freedom are an affine transformation at each template vertex. The objective function is a weighted combination of three measures: proximity of transformed vertices to the range data, similarity between neighboring transformations, and proximity of sparse markers at corresponding locations on the template and target surface. We solve for the transformations with a non-linear optimizer, run at two resolutions to speed convergence. We demonstrate reconstruction and consistent parameterization of 250 human body models. With this parameterized set, we explore a variety of applications for human body modeling, including: morphing, texture transfer, statistical analysis of shape, model fitting from sparse markers, feature analysis to modify multiple correlated parameters (such as the weight and height of an individual), and transfer of surface detail and animation controls from a template to fitted models.	The space of human body shapes: reconstruction and parameterization from range scans	NA:NA:NA	2018
Brian Curless	NA	Session details: Light fields and visibility	NA	2018
Tommer Leyvand:Olga Sorkine:Daniel Cohen-Or	From-region visibility culling is considered harder than from-point visibility culling, since it is inherently four-dimensional. We present a conservative occlusion culling method based on factorizing the 4D visibility problem into horizontal and vertical components. The visibility of the two components is solved asymmetrically: the horizontal component is based on a parameterization of the ray space, and the visibility of the vertical component is solved by incrementally merging umbrae. The technique is designed so that the horizontal and vertical operations can be efficiently realized together by modern graphics hardware. Similar to image-based from-point methods, we use an occlusion map to encode visibility; however, the image-space occlusion map is in the ray space rather than in the primal space. Our results show that the culling time and the size of the computed potentially visible set depend on the size of the viewcell. For moderate viewcells, conservative occlusion culling of large urban scenes takes less than a second, and the size of the potentially visible set is only about two times larger than the size of the exact visible set.	Ray space factorization for from-region visibility	NA:NA:NA	2018
Sameer Agarwal:Ravi Ramamoorthi:Serge Belongie:Henrik Wann Jensen	We introduce structured importance sampling, a new technique for efficiently rendering scenes illuminated by distant natural illumination given in an environment map. Our method handles occlusion, high-frequency lighting, and is significantly faster than alternative methods based on Monte Carlo sampling. We achieve this speedup as a result of several ideas. First, we present a new metric for stratifying and sampling an environment map taking into account both the illumination intensity as well as the expected variance due to occlusion within the scene. We then present a novel hierarchical stratification algorithm that uses our metric to automatically stratify the environment map into regular strata. This approach enables a number of rendering optimizations, such as pre-integrating the illumination within each stratum to eliminate noise at the cost of adding bias, and sorting the strata to reduce the number of sample rays. We have rendered several scenes illuminated by natural lighting, and our results indicate that structured importance sampling is better than the best previous Monte Carlo techniques, requiring one to two orders of magnitude fewer samples for the same image quality.	Structured importance sampling of environment maps	NA:NA:NA:NA	2018
Vincent Masselus:Pieter Peers:Philip Dutré:Yves D. Willems	We present an image-based technique to relight real objects illuminated by a 4D incident light field, representing the illumination of an environment. By exploiting the richness in angular and spatial variation of the light field, objects can be relit with a high degree of realism.We record photographs of an object, illuminated from various positions and directions, using a projector mounted on a gantry as a moving light source. The resulting basis images are used to create a subset of the full reflectance field of the object. Using this reflectance field, we can create an image of the object, relit with any incident light field and observed from a flxed camera position.To maintain acceptable recording times and reduce the amount of data, we propose an efficient data acquisition method.Since the object can be relit with a 4D incident light field, illumination effects encoded in the light field, such as shafts of shadow or spot light effects, can be realized.	Relighting with 4D incident light fields	NA:NA:NA:NA	2018
Michael Goesele:Xavier Granier:Wolfgang Heidrich:Hans-Peter Seidel	Realistic image synthesis requires both complex and realistic models of real-world light sources and efficient rendering algorithms to deal with them. In this paper, we describe a processing pipeline for dealing with complex light sources from acquisition to global illumination rendering. We carefully design optical filters to guarantee high precision measurements of real-world light sources. We discuss two practically feasible setups that allow us to measure light sources with different characteristics. Finally, we introduce an efficient importance sampling algorithm for our representation that can be used, for example, in conjunction with Photon Maps.	Accurate light source acquisition and rendering	NA:NA:NA:NA	2018
Marc Alexa	NA	Session details: Points	NA	2018
Kavita Bala:Bruce Walter:Donald P. Greenberg	This paper presents a new interactive rendering and display technique for complex scenes with expensive shading, such as global illumination. Our approach combines sparsely sampled shading (points) and analytically computed discontinuities (edges) to interactively generate high-quality images. The edge-and-point image is a new compact representation that combines edges and points such that fast, table-driven interpolation of pixel shading from nearby point samples is possible, while respecting discontinuities.The edge-and-point renderer is extensible, permitting the use of arbitrary shaders to collect shading samples. Shading discontinuities, such as silhouettes and shadow edges, are found at interactive rates. Our software implementation supports interactive navigation and object manipulation in scenes that include expensive lighting effects (such as global illumination) and geometrically complex objects. For interactive rendering we show that high-quality images of these scenes can be rendered at 8--14 frames per second on a desktop PC: a speedup of 20--60 over a ray tracer computing a single sample per pixel.	Combining edges and points for interactive high-quality rendering	NA:NA:NA	2018
Mark Pauly:Richard Keiser:Leif P. Kobbelt:Markus Gross	We present a versatile and complete free-form shape modeling framework for point-sampled geometry. By combining unstructured point clouds with the implicit surface definition of the moving least squares approximation, we obtain a hybrid geometry representation that allows us to exploit the advantages of implicit and parametric surface models. Based on this representation we introduce a shape modeling system that enables the designer to perform large constrained deformations as well as boolean operations on arbitrarily shaped objects. Due to minimum consistency requirements, point-sampled surfaces can easily be re-structured on the fly to support extreme geometric deformations during interactive editing. In addition, we show that strict topology control is possible and sharp features can be generated and preserved on point-sampled objects. We demonstrate the effectiveness of our system on a large set of input models, including noisy range scans, irregular point clouds, and sparsely as well as densely sampled models.	Shape modeling with point-sampled geometry	NA:NA:NA:NA	2018
Bart Adams:Philip Dutré	In this paper we present an algorithm to perform interactive boolean operations on free-form solids bounded by surfels. We introduce a fast inside-outside test to check whether surfels lie within the bounds of another surfel-bounded solid. This enables us to add, subtract and intersect complex solids at interactive rates. Our algorithm is fast both in displaying and constructing the new geometry resulting from the boolean operation.We present a resampling operator to solve problems resulting from sharp edges in the resulting solid. The operator resamples the surfels intersecting with the surface of the other solid. This enables us to represent the sharp edges with great detail.We believe our algorithm to be an ideal tool for interactive editing of free-form solids.	Interactive boolean operations on surfel-bounded solids	NA:NA	2018
Carsten Dachsbacher:Christian Vogelgsang:Marc Stamminger	In this paper we present sequential point trees, a data structure that allows adaptive rendering of point clouds completely on the graphics processor. Sequential point trees are based on a hierarchical point representation, but the hierarchical rendering traversal is replaced by sequential processing on the graphics processor, while the CPU is available for other tasks. Smooth transition to triangle rendering for optimized performance is integrated. We describe optimizations for backface culling and texture adaptive point selection. Finally, we discuss implementation issues and show results.	Sequential point trees	NA:NA:NA	2018
Carol O'Sullivan	NA	Session details: Modeling and simplification	NA	2018
Ignacio Llamas:Byungmoon Kim:Joshua Gargus:Jarek Rossignac:Chris D. Shaw	A free-form deformation that warps a surface or solid may be specified in terms of one or several point-displacement constraints that must be interpolated by the deformation. The Twister approach introduced here, adds the capability to impose an orientation change, adding three rotational constraints, at each displaced point. Furthermore, it solves for a space warp that simultaneously interpolates two sets of such displacement and orientation constraints. With a 6 DoF magnetic tracker in each hand, the user may grab two points on or near the surface of an object and simultaneously drag them to new locations while rotating the trackers to tilt, bend, or twist the shape near the displaced points. Using a new formalism based on a weighted average of screw displacements, Twister computes in realtime a smooth deformation, whose effect decays with distance from the grabbed points, simultaneously interpolating the 12 constraints. It is continuously applied to the shape, providing realtime graphic feedback. The two-hand interface and the resulting deformation are intuitive and hence offer an effective direct manipulation tool for creating or modifying 3D shapes.	Twister: a space-warp operator for the two-handed editing of 3D shapes	NA:NA:NA:NA:NA	2018
Peter Wonka:Michael Wimmer:François Sillion:William Ribarsky	This paper presents a new method for the automatic modeling of architecture. Building designs are derived using split grammars, a new type of parametric set grammar based on the concept of shape. The paper also introduces an attribute matching system and a separate control grammar, which offer the flexibility required to model buildings using a large variety of different styles and design ideas. Through the adaptive nature of the design grammar used, the created building designs can either be generic or adhere closely to a specified goal, depending on the amount of data available.	Instant architecture	NA:NA:NA:NA	2018
Andrew Wilson:Dinesh Manocha	We present an incremental algorithm to compute image-based simplifications of a large environment. We use an optimization-based approach to generate samples based on scene visibility, and from each viewpoint create textured depth meshes (TDMs) using sampled range panoramas of the environment. The optimization function minimizes artifacts such as skins and cracks in the reconstruction. We also present an encoding scheme for multiple TDMs that exploits spatial coherence among different viewpoints. The resulting simplifications, incremental textured depth meshes (ITDMs), reduce preprocessing, storage, rendering costs and visible artifacts. Our algorithm has been applied to large, complex synthetic environments comprising millions of primitives. It is able to render them at 20 -- 40 frames a second on a PC with little loss in visual fidelity.	Simplifying complex environments using incremental textured depth meshes	NA:NA	2018
Xavier Décoret:Frédo Durand:François X. Sillion:Julie Dorsey	We introduce billboard clouds -- a new approach for extreme simplification in the context of real-time rendering. 3D models are simplified onto a set of planes with texture and transparency maps. We present an optimization approach to build a billboard cloud given a geometric error threshold. After computing an appropriate density function in plane space, a greedy approach is used to select suitable representative planes. A good surface approximation is ensured by favoring planes that are "nearly tangent" to the model. This method does not require connectivity information, but instead avoids cracks by projecting primitives onto multiple planes when needed. For extreme simplification, our approach combines the strengths of mesh decimation and image-based impostors. We demonstrate our technique on a large class of models, including smooth manifolds and composite objects.	Billboard clouds for extreme model simplification	NA:NA:NA:NA	2018
Joe Marks	NA	Session details: Reprise of UIST 2003 and I3D 2003	NA	2018
Takeo Igarashi:John F. Hughes	NA	Clothing manipulation	NA:NA	2018
Michael Tsang:George W. Fitzmzurice:Gordon Kurtenbach:Azam Khan:Bill Buxton	We review the Boom Chameleon, a novel input/output device consisting of a flat-panel display mounted on a tracked mechanical armature. The display acts as a physical window into 3D virtual environments, through which a one-to-one mapping between real and virtual space is preserved. The Boom Chameleon is further augmented with a touch-screen and a microphone/speaker combination. We created a 3D annotation application that exploits this unique configuration in order to simultaneously capture viewpoint, voice and gesture information. Results of an informal user study show that the Boom Chameleon annotation facilities have the potential to be an effective, and intuitive system for reviewing 3D designs.	Boom chameleon: simultaneous capture of 3D viewpoint, voice and gesture annotations on a spatially-aware display	NA:NA:NA:NA:NA	2018
NA	The Actuated Workbench is a device that uses magnetic forces to move objects on a table in two dimensions. It is intended for use with existing tabletop tangible interfaces, providing an additional feedback loop for computer output, and helping to resolve inconsistencies that otherwise arise from the computer's inability to move objects on the table.	The actuated workbench: computer-controlled actuation in tabletop tangible interfaces	NA	2018
Christopher Niederauer:Mike Houston:Maneesh Agrawala:Greg Humphreys	NA	Non-invasive interactive visualization of dynamic architectural environments	NA:NA:NA:NA	2018
Benjamin Lok:Samir Naik:Mary Whitton:Frederick P. Brooks	NA	Incorporating dynamic real objects into immersive virtual environments	NA:NA:NA:NA	2018
Michael Gleicher:Hyun Joon Shin:Lucas Kovar:Andrew Jepsen	NA	Snap-together motion: assembling run-time animations	NA:NA:NA:NA	2018
Doug James	NA	Session details: Fluids and smoke	NA	2018
Nick Rasmussen:Duc Quang Nguyen:Willi Geiger:Ronald Fedkiw	In this paper, we present an efficient method for simulating highly detailed large scale participating media such as the nuclear explosions shown in figure 1. We capture this phenomena by simulating the motion of particles in a fluid dynamics generated velocity field. A novel aspect of this paper is the creation of highly detailed three-dimensional turbulent velocity fields at interactive rates using a low to moderate amount of memory. The key idea is the combination of two-dimensional high resolution physically based flow fields with a moderate sized three-dimensional Kolmogorov velocity field tiled periodically in space.	Smoke simulation for large scale phenomena	NA:NA:NA:NA	2018
Bryan E. Feldman:James F. O'Brien:Okan Arikan	This paper describes a method for animating suspended particle explosions. Rather than modeling the numerically troublesome, and largely invisible blast wave, the method uses a relatively stable incompressible fluid model to account for the motion of air and hot gases. The fluid's divergence field is adjusted directly to account for detonations and the generation and expansion of gaseous combustion products. Particles immersed in the fluid track the motion of particulate fuel and soot as they are advected by the fluid. Combustion is modeled using a simple but effective process governed by the particle and fluid systems. The method has enough flexibility to also approximate sprays of burning liquids. This paper includes several demonstrative examples showing air bursts, explosions near obstacles, confined explosions, and burning sprays. Because the method is based on components that allow large time integration steps, it only requires a few seconds of computation per frame for the examples shown.	Animating suspended particle explosions	NA:NA:NA	2018
Adrien Treuille:Antoine McNamara:Zoran Popović:Jos Stam	We describe a method for controlling smoke simulations through user-specified keyframes. To achieve the desired behavior, a continuous quasi-Newton optimization solves for appropriate "wind" forces to be applied to the underlying velocity field throughout the simulation. The cornerstone of our approach is a method to efficiently compute exact derivatives through the steps of a fluid simulation. We formulate an objective function corresponding to how well a simulation matches the user's keyframes, and use the derivatives to solve for force parameters that minimize this function. For animations with several keyframes, we present a novel multiple-shooting approach. By splitting large problems into smaller overlapping subproblems, we greatly speed up the optimization process while avoiding certain local minima.	Keyframe control of smoke simulations	NA:NA:NA:NA	2018
Jos Stam	In this paper we introduce a method to simulate fluid flows on smooth surfaces of arbitrary topology: an effect never seen before. We achieve this by combining a two-dimensional stable fluid solver with an atlas of parametrizations of a Catmull-Clark surface. The contributions of this paper are: (i) an extension of the Stable Fluids solver to arbitrary curvilinear coordinates, (ii) an elegant method to handle cross-patch boundary conditions and (iii) a set of new external forces custom tailored for surface flows. Our techniques can also be generalized to handle other types of processes on surfaces modeled by partial differential equations, such as reaction-diffusion. Some of our simulations allow a user to interactively place densities and apply forces to the surface, then watch their effects in real-time. We have also computed higher resolution animations of surface flows off-line.	Flows on surfaces of arbitrary topology	NA	2018
Yoshinori Dobashi:Tsuyoshi Yamamoto:Tomoyuki Nishita	In computer graphics, most research focuses on creating images. However, there has been much recent work on the automatic generation of sound linked to objects in motion and the relative positions of receivers and sound sources. This paper proposes a new method for creating one type of sound called aerodynamic sound. Examples of aerodynamic sound include sound generated by swinging swords or by wind blowing. A major source of aerodynamic sound is vortices generated in fluids such as air. First, we propose a method for creating sound textures for aerodynamic sound by making use of computational fluid dynamics. Next, we propose a method using the sound textures for real-time rendering of aerodynamic sound according to the motion of objects or wind velocity.	Real-time rendering of aerodynamic sound using sound textures based on computational fluid dynamics	NA:NA:NA	2018
Julie Dorsey	NA	Session details: Scattering and reflectance measurement	NA	2018
Jefferson Y. Han:Ken Perlin	We describe a new technique for measuring the bidirectional texture function (BTF) of a surface that requires no mechanical movement, can measure surfaces in situ under arbitrary lighting conditions, and can be made small, portable and inexpensive. The enabling innovation is the use of a tapered kaleidoscope, which allows a camera to view the same surface sample simultaneously from many directions. Similarly, the surface can be simultaneously illuminated from many directions, using only a single structured light source. We describe the techniques of construction and measurement, and we show experimental results.	Measuring bidirectional texture reflectance with a kaleidoscope	NA:NA	2018
Andrew Gardner:Chris Tchou:Tim Hawkins:Paul Debevec	This paper presents a technique for estimating the spatially-varying reflectance properties of a surface based on its appearance during a single pass of a linear light source. By using a linear light rather than a point light source as the illuminant, we are able to reliably observe and estimate the diffuse color, specular color, and specular roughness of each point of the surface. The reflectometry apparatus we use is simple and inexpensive to build, requiring a single direction of motion for the light source and a fixed camera viewpoint. Our model fitting technique first renders a reflectance table of how diffuse and specular reflectance lobes would appear under moving linear light source illumination. Then, for each pixel we compare its series of intensity values to the tabulated reflectance lobes to determine which reflectance model parameters most closely produce the observed reflectance values. Using two passes of the linear light source at different angles, we can also estimate per-pixel surface normals as well as the reflectance parameters. Additionally our system records a per-pixel height map for the object and estimates its per-pixel translucency. We produce real-time renderings of the captured objects using a custom hardware shading algorithm. We apply the technique to a test object exhibiting a variety of materials as well as to an illuminated manuscript with gold lettering. To demonstrate the technique's accuracy, we compare renderings of the captured models to real photographs of the original objects.	Linear light source reflectometry	NA:NA:NA:NA	2018
Wojciech Matusik:Hanspeter Pfister:Matt Brand:Leonard McMillan	We present a generative model for isotropic bidirectional reflectance distribution functions (BRDFs) based on acquired reflectance data. Instead of using analytical reflectance models, we represent each BRDF as a dense set of measurements. This allows us to interpolate and extrapolate in the space of acquired BRDFs to create new BRDFs. We treat each acquired BRDF as a single high-dimensional vector taken from a space of all possible BRDFs. We apply both linear (subspace) and non-linear (manifold) dimensionality reduction tools in an effort to discover a lower-dimensional representation that characterizes our measurements. We let users define perceptually meaningful parametrization directions to navigate in the reduced-dimension BRDF space. On the low-dimensional manifold, movement along these directions produces novel but valid BRDFs.	A data-driven reflectance model	NA:NA:NA:NA	2018
Norimichi Tsumura:Nobutoshi Ojima:Kayoko Sato:Mitsuhiro Shiraishi:Hideto Shimizu:Hirohide Nabeshima:Syuuichi Akazaki:Kimihiko Hori:Yoichi Miyake	This paper proposes an E-cosmetic function for digital images based on physics and physiologically-based image processing. A practical skin color and texture analysis/synthesis technique is introduced for this E-cosmetic function. Shading on the face is removed by a simple color vector analysis in the optical density domain as an inverse lighting technique. The image without shading is analyzed by a previously introduced technique that extracts hemoglobin and melanin components by independent component analysis. Experimental results using UV-B irradiation and the application of methyl nicotinate on the arms support the physiological validity of the analysis and the effectiveness of the proposed shading removal. We synthesized the way facial images changed due to tanning or alcohol consumption, and compared the synthesized images with images of actual changes in skin color. The comparison shows an excellent match between the synthesized and actual images of changes due to tanning and alcohol consumption. We also proposed a technique to synthesize the change of texture in pigment due to aging or the application of cosmetics. The pyramid-based texture analysis/synthesis technique was used for the spatial processing of texture. Using the proposed technique, we could realistically change the skin color and texture of a 50 year-old woman to that of a 20 year-old woman.	Image-based skin color and texture analysis/synthesis by extracting hemoglobin and melanin information in the skin	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Stephen R. Marschner:Henrik Wann Jensen:Mike Cammarano:Steve Worley:Pat Hanrahan	Light scattering from hair is normally simulated in computer graphics using Kajiya and Kay's classic phenomenological model. We have made new measurements of scattering from individual hair fibers that exhibit visually significant effects not predicted by Kajiya and Kay's model. Our measurements go beyond previous hair measurements by examining out-of-plane scattering, and together with this previous work they show a multiple specular highlight and variation in scattering with rotation about the fiber axis. We explain the sources of these effects using a model of a hair fiber as a transparent elliptical cylinder with an absorbing interior and a surface covered with tilted scales. Based on an analytical scattering function for a circular cylinder, we propose a practical shading model for hair that qualitatively matches the scattering behavior shown in the measurements. In a comparison between a photograph and rendered images, we demonstrate the new model's ability to match the appearance of real hair.	Light scattering from human hair fibers	NA:NA:NA:NA:NA	2018
Bengt-Olaf Schneider	NA	Session details: Hardware and displays	NA	2018
Timo Aila:Ville Miettinen:Petri Nordlund	In causal processes decisions do not depend on future data. Many well-known problems, such as occlusion culling, order-independent transparency and edge antialiasing cannot be properly solved using the traditional causal rendering architectures, because future data may change the interpretation of current events.We propose adding a delay stream between the vertex and pixel processing units. While a triangle resides in the delay stream, subsequent triangles generate occlusion information. As a result, the triangle may be culled by primitives that were submitted after it. We show two-to fourfold efficiency improvements in pixel processing and video memory bandwidth usage in common benchmark scenes. We also demonstrate how the memory requirements of order-independent transparency can be substantially reduced by using delay streams. Finally, we describe how discontinuity edges can be detected in hardware. Previously used heuristics for collapsing samples in adaptive supersampling are thus replaced by connectivity information.	Delay streams for graphics hardware	NA:NA:NA	2018
Tomas Akenine-Möller:Jacob Ström	The mobile phone is one of the most widespread devices with rendering capabilities. Those capabilities have been very limited because the resources on such devices are extremely scarce; small amounts of memory, little bandwidth, little chip area dedicated for special purposes, and limited power consumption. The small display resolutions present a further challenge; the angle subtended by a pixel is relatively large, and therefore reasonably high quality rendering is needed to generate high fidelity images.To increase the mobile rendering capabilities, we propose a new hardware architecture for rasterizing textured triangles. Our architecture focuses on saving memory bandwidth, since an external memory access typically is one of the most energy-consuming operations, and because mobile phones need to use as little power as possible. Therefore, our system includes three new key innovations: I) an inexpensive multisampling scheme that gives relatively high quality at the same cost of previous inexpensive schemes, II) a texture minification system, including texture compression, which gives quality relatively close to trilinear mipmapping at the cost of 1.33 32-bit memory accesses on average, III) a scanline-based culling scheme that avoids a significant amount of z-buffer reads, and that only requires one context. Software simulations show that these three innovations together significantly reduce the memory bandwidth, and thus also the power consumption.	Graphics for the masses: a hardware rasterization architecture for mobile phones	NA:NA	2018
Ramesh Raskar:Jeroen van Baar:Paul Beardsley:Thomas Willwacher:Srinivas Rao:Clifton Forlines	Projectors are currently undergoing a transformation as they evolve from static output devices to portable, environment-aware, communicating systems. An enhanced projector can determine and respond to the geometry of the display surface, and can be used in an ad-hoc cluster to create a self-configuring display. Information display is such a prevailing part of everyday life that new and more flexible ways to present data are likely to have significant impact. This paper examines geometrical issues for enhanced projectors, relating to customized projection for different shapes of display surface, object augmentation, and co-operation between multiple units.We introduce a new technique for adaptive projection on nonplanar surfaces using conformal texture mapping. We describe object augmentation with a hand-held projector, including interaction techniques. We describe the concept of a display created by an ad-hoc cluster of heterogeneous enhanced projectors, with a new global alignment scheme, and new parametric image transfer methods for quadric surfaces, to make a seamless projection. The work is illustrated by several prototypes and applications.	iLamps: geometrically aware and self-configuring projectors	NA:NA:NA:NA:NA:NA	2018
Markus Gross:Stephan Würmlin:Martin Naef:Edouard Lamboray:Christian Spagno:Andreas Kunz:Esther Koller-Meier:Tomas Svoboda:Luc Van Gool:Silke Lang:Kai Strehlke:Andrew Vande Moere:Oliver Staadt	We present blue-c, a new immersive projection and 3D video acquisition environment for virtual design and collaboration. It combines simultaneous acquisition of multiple live video streams with advanced 3D projection technology in a CAVE™-like environment, creating the impression of total immersion. The blue-c portal currently consists of three rectangular projection screens that are built from glass panels containing liquid crystal layers. These screens can be switched from a whitish opaque state (for projection) to a transparent state (for acquisition), which allows the video cameras to "look through" the walls. Our projection technology is based on active stereo using two LCD projectors per screen. The projectors are synchronously shuttered along with the screens, the stereo glasses, active illumination devices, and the acquisition hardware. From multiple video streams, we compute a 3D video representation of the user in real time. The resulting video inlays are integrated into a networked virtual environment. Our design is highly scalable, enabling blue-c to connect to portals with less sophisticated hardware.	blue-c: a spatially immersive display and 3D video portal for telepresence	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Frédo Durand	NA	Session details: Design and depiction	NA	2018
Maneesh Agrawala:Doantam Phan:Julie Heiser:John Haymaker:Jeff Klingner:Pat Hanrahan:Barbara Tversky	We present design principles for creating effective assembly instructions and a system that is based on these principles. The principles are drawn from cognitive psychology research which investigated people's conceptual models of assembly and effective methods to visually communicate assembly information. Our system is inspired by earlier work in robotics on assembly planning and in visualization on automated presentation design. Although other systems have considered presentation and planning independently, we believe it is necessary to address the two problems simultaneously in order to create effective assembly instructions. We describe the algorithmic techniques used to produce assembly instructions given object geometry, orientation, and optional grouping and ordering constraints on the object's parts. Our results demonstrate that it is possible to produce aesthetically pleasing and easy to follow instructions for many everyday objects.	Designing effective step-by-step assembly instructions	NA:NA:NA:NA:NA:NA:NA	2018
Charles Jacobs:Wilmot Li:Evan Schrier:David Bargeron:David Salesin	Grid-based page designs are ubiquitous in commercially printed publications, such as newspapers and magazines. Yet, to date, no one has invented a good way to easily and automatically adapt such designs to arbitrarily-sized electronic displays. The difficulty of generalizing grid-based designs explains the generally inferior nature of on-screen layouts when compared to their printed counterparts, and is arguably one of the greatest remaining impediments to creating on-line reading experiences that rival those of ink on paper. In this work, we present a new approach to adaptive grid-based document layout, which attempts to bridge this gap. In our approach, an adaptive layout style is encoded as a set of grid-based templates that know how to adapt to a range of page sizes and other viewing conditions. These templates include various types of layout elements (such as text, figures, etc.) and define, through constraint-based relationships, just how these elements are to be laid out together as a function of both the properties of the content itself, such as a figure's size and aspect ratio, and the properties of the viewing conditions under which the content is being displayed. We describe an XML-based representation for our templates and content, which maintains a clean separation between the two. We also describe the various parts of our research prototype system: a layout engine for formatting the page; a paginator for determining a globally optimal allocation of content amongst the pages, as well as an optimal pairing of templates with content; and a graphical user interface for interactively creating adaptive templates. We also provide numerous examples demonstrating the capabilities of this prototype, including this paper, itself, which has been laid out with our system.	Adaptive grid-based document layout	NA:NA:NA:NA:NA	2018
Doug DeCarlo:Adam Finkelstein:Szymon Rusinkiewicz:Anthony Santella	In this paper, we describe a non-photorealistic rendering system that conveys shape using lines. We go beyond contours and creases by developing a new type of line to draw: the suggestive contour. Suggestive contours are lines drawn on clearly visible parts of the surface, where a true contour would first appear with a minimal change in viewpoint. We provide two methods for calculating suggestive contours, including an algorithm that finds the zero crossings of the radial curvature. We show that suggestive contours can be drawn consistently with true contours, because they anticipate and extend them. We present a variety of results, arguing that these images convey shape more effectively than contour alone.	Suggestive contours for conveying shape	NA:NA:NA:NA	2018
Robert D. Kalnins:Philip L. Davidson:Lee Markosian:Adam Finkelstein	We describe a way to render stylized silhouettes of animated 3D models with temporal coherence. Coherence is one of the central challenges for non-photorealistic rendering. It is especially difficult for silhouettes, because they may not have obvious correspondences between frames. We demonstrate various coherence effects for stylized silhouettes with a robust working system. Our method runs in real-time for models of moderate complexity, making it suitable for both interactive applications and offline animation.	Coherent stylized silhouettes	NA:NA:NA:NA	2018
James F. O'Brien	NA	Session details: Dynamics	NA	2018
David Baraff:Andrew Witkin:Michael Kass	Deficient cloth-to-cloth collision response is the most serious shortcoming of most cloth simulation systems. Past approaches to cloth-cloth collision have used history to decide whether nearby cloth regions have interpenetrated. The biggest pitfall of history-based methods is that an error anywhere along the way can give rise to persistent tangles. This is a particularly serious issue for production character animation, because characters' bodies routinely self-intersect, for instance in the bend of an elbow or knee, or where the arm or hand rests against the body. Cloth that becomes pinched in these regions is often forced into jagged self-intersections that defeat history-based methods, leaving a tangled mess when the body parts separate. This paper describes a history-free cloth collision response algorithm based on global intersection analysis of cloth meshes at each simulation step. The algorithm resolves tangles that arise during pinching as soon as the surrounding geometry permits, and also resolves tangled initial conditions. The ability to untangle cloth after pinching is not sufficient, because standard cloth-solid collision algorithms handle pinches so poorly that they often give rise to visible flutters and other simulation artifacts during the pinch. As a companion to the global intersection analysis method, we present a cloth-solid collision algorithm called collision flypapering, that eliminates these artifacts. The two algorithms presented have been used together extensively and successfully in a production animation environment.	Untangling cloth	NA:NA:NA	2018
Eran Guendelman:Robert Bridson:Ronald Fedkiw	We consider the simulation of nonconvex rigid bodies focusing on interactions such as collision, contact, friction (kinetic, static, rolling and spinning) and stacking. We advocate representing the geometry with both a triangulated surface and a signed distance function defined on a grid, and this dual representation is shown to have many advantages. We propose a novel approach to time integration merging it with the collision and contact processing algorithms in a fashion that obviates the need for ad hoc threshold velocities. We show that this approach matches the theoretical solution for blocks sliding and stopping on inclined planes with friction. We also present a new shock propagation algorithm that allows for efficient use of the propagation (as opposed to the simultaneous) method for treating contact. These new techniques are demonstrated on a variety of problems ranging from simple test cases to stacking problems with as many as 1000 nonconvex rigid bodies with friction as shown in Figure 1.	Nonconvex rigid bodies with stacking	NA:NA:NA	2018
Doug L. James:Kayvon Fatahalian	We present an approach for precomputing data-driven models of interactive physically based deformable scenes. The method permits real-time hardware synthesis of nonlinear deformation dynamics, including self-contact and global illumination effects, and supports real-time user interaction. We use data-driven tabulation of the system's deterministic state space dynamics, and model reduction to build efficient low-rank parameterizations of the deformed shapes. To support runtime interaction, we also tabulate impulse response functions for a palette of external excitations. Although our approach simulates particular systems under very particular interaction conditions, it has several advantages. First, parameterizing all possible scene deformations enables us to precompute novel reduced coparameterizations of global scene illumination for low-frequency lighting conditions. Second, because the deformation dynamics are precomputed and parameterized as a whole, collisions are resolved within the scene during precomputation so that runtime self-collision handling is implicit. Optionally, the data-driven models can be synthesized on programmable graphics hardware, leaving only the low-dimensional state space dynamics and appearance data models to be computed by the main CPU.	Precomputing interactive dynamic deformable scenes	NA:NA	2018
Jia-chi Wu:Zoran Popović	In this paper we describe a physics-based method for synthesis of bird flight animations. Our method computes a realistic set of wingbeats that enables a bird to follow the specified trajectory. We model the bird as an articulated skeleton with elastically deformable feathers. The bird motion is created by applying joint torques and aerodynamic forces over time in a forward dynamics simulation. We solve for each wingbeat motion separately by optimizing for wingbeat parameters that create the most natural motion. The final animation is constructed by concatenating a series of optimal wingbeats. This detailed bird flight model enables us to produce flight motions of different birds performing a variety of maneuvers including taking off, cruising, rapidly descending, turning, and landing.	Realistic modeling of bird flight animations	NA:NA	2018
Michael McCool	NA	Session details: Computation on GPUs	NA	2018
William R. Mark:R. Steven Glanville:Kurt Akeley:Mark J. Kilgard	The latest real-time graphics architectures include programmable floating-point vertex and fragment processors, with support for data-dependent control flow in the vertex processor. We present a programming language and a supporting system that are designed for programming these stream processors. The language follows the philosophy of C, in that it is a hardware-oriented, general-purpose language, rather than an application-specific shading language. The language includes a variety of facilities designed to support the key architectural features of programmable graphics processors, and is designed to support multiple generations of graphics architectures with different levels of functionality. The system supports both of the major 3D graphics APIs: OpenGL and Direct3D. This paper identifies many of the choices that we faced as we designed the system, and explains why we made the decisions that we did.	Cg: a system for programming graphics hardware in a C-like language	NA:NA:NA:NA	2018
Jens Krüger:Rüdiger Westermann	In this work, the emphasis is on the development of strategies to realize techniques of numerical computing on the graphics chip. In particular, the focus is on the acceleration of techniques for solving sets of algebraic equations as they occur in numerical simulation. We introduce a framework for the implementation of linear algebra operators on programmable graphics processors (GPUs), thus providing the building blocks for the design of more complex numerical algorithms. In particular, we propose a stream model for arithmetic operations on vectors and matrices that exploits the intrinsic parallelism and efficient communication on modern GPUs. Besides performance gains due to improved numerical computations, graphics algorithms benefit from this model in that the transfer of computation results to the graphics processor for display is avoided. We demonstrate the effectiveness of our approach by implementing direct solvers for sparse matrices, and by applying these solvers to multi-dimensional finite difference equations, i.e. the 2D wave equation and the incompressible Navier-Stokes equations.	Linear algebra operators for GPU implementation of numerical algorithms	NA:NA	2018
Jeff Bolz:Ian Farmer:Eitan Grinspun:Peter Schröder	Many computer graphics applications require high-intensity numerical simulation. We show that such computations can be performed efficiently on the GPU, which we regard as a full function streaming processor with high floating-point performance. We implemented two basic, broadly useful, computational kernels: a sparse matrix conjugate gradient solver and a regular-grid multigrid solver. Real time applications ranging from mesh smoothing and parameterization to fluid solvers and solid mechanics can greatly benefit from these, evidence our example applications of geometric flow and fluid simulation running on NVIDIA's GeForce FX.	Sparse matrix solvers on the GPU: conjugate gradients and multigrid	NA:NA:NA:NA	2018
Karl E. Hillesland:Sergey Molinov:Radek Grzeszczuk	Graphics hardware is undergoing a change from fixed-function pipelines to more programmable organizations that resemble general purpose stream processors. In this paper, we show that certain general algorithms, not normally associated with computer graphics, can be mapped to such designs. Specifically, we cast nonlinear optimization as a data streaming process that is well matched to modern graphics processors. Our framework is particularly well suited for solving image-based modeling problems since it can be used to represent a large and diverse class of these problems using a common formulation. We successfully apply this approach to two distinct image-based modeling problems: light field mapping approximation and fitting the Lafortune model to spatial bidirectional reflectance distribution functions. Comparing the performance of the graphics hardware implementation to a CPU implementation, we show more than 5-fold improvement.	Nonlinear optimization framework for image-based modeling on programmable graphics hardware	NA:NA:NA	2018
Martin Isenburg:Stefan Gumhold	Polygonal models acquired with emerging 3D scanning technology or from large scale CAD applications easily reach sizes of several gigabytes and do not fit in the address space of common 32-bit desktop PCs. In this paper we propose an out-of-core mesh compression technique that converts such gigantic meshes into a streamable, highly compressed representation. During decompression only a small portion of the mesh needs to be kept in memory at any time. As full connectivity information is available along the decompression boundaries, this provides seamless mesh access for incremental in-core processing on gigantic meshes. Decompression speeds are CPU-limited and exceed one million vertices and two million triangles per second on a 1.8 GHz Athlon processor.A novel external memory data structure provides our compression engine with transparent access to arbitrary large meshes. This out-of-core mesh was designed to accommodate the access pattern of our region-growing based compressor, which - in return - performs mesh queries as seldom and as local as possible by remembering previous queries as long as needed and by adapting its traversal slightly. The achieved compression rates are state-of-the-art.	Out-of-core compression for gigantic polygon meshes	NA:NA	2018
Thouis R. Jones:Frédo Durand:Mathieu Desbrun	With the increasing use of geometry scanners to create 3D models, there is a rising need for fast and robust mesh smoothing to remove inevitable noise in the measurements. While most previous work has favored diffusion-based iterative techniques for feature-preserving smoothing, we propose a radically different approach, based on robust statistics and local first-order predictors of the surface. The robustness of our local estimates allows us to derive a non-iterative feature-preserving filtering technique applicable to arbitrary "triangle soups". We demonstrate its simplicity of implementation and its efficiency, which make it an excellent solution for smoothing large, noisy, and non-manifold meshes.	Non-iterative, feature-preserving mesh smoothing	NA:NA:NA	2018
Shachar Fleishman:Iddo Drori:Daniel Cohen-Or	We present an anisotropic mesh denoising algorithm that is effective, simple and fast. This is accomplished by filtering vertices of the mesh in the normal direction using local neighborhoods. Motivated by the impressive results of bilateral filtering for image denoising, we adopt it to denoise 3D meshes; addressing the specific issues required in the transition from two-dimensions to manifolds in three dimensions. We show that the proposed method successfully removes noise from meshes while preserving features. Furthermore, the presented algorithm excels in its simplicity both in concept and implementation.	Bilateral mesh denoising	NA:NA:NA	2018
Frédo Durand	NA	Session details: Graphics is fun	NA	2018
Stephane Guy:Cyril Soler	We present an algorithm for rendering faceted colored gemstones in real time, using graphics hardware. Beyond the technical challenge of handling the complex behavior of light in such objects, a real time high quality rendering of gemstones has direct applications in the field of jewelry prototyping, which has now become a standard practice for replacing tedious (and less interactive) wax carving methods. Our solution is based on a number of controlled approximations of the physical phenomena involved when light enters a stone, which permit an implementation based on the most recent -- yet commonly available -- hardware features such as fragment programs, cube-mapping.	Graphics gems revisited: fast and physically-based rendering of gemstones	NA:NA	2018
Roger David Hersch:Sylvain Chosson	We propose a new powerful way of synthesizing moiré images that enables the creation of dynamically moving messages incorporating text, symbols, and color elements. Moiré images appear when superposing a base layer made of replicated base bands and a revealing layer made of a line grating comprising thin transparent lines. Each replicated base band contains the same image, e.g. text or color motifs. Since the base bands and the revealing line grating have similar periods, the revealed moiré image is the image located within each base band, enlarged along one dimension. By considering the formation of the moiré image as a line sampling process, we derive the linear transformation between the base layer and the moiré image. We obtain the geometric layout of the resulting moiré image, i.e. its orientation, size and displacement direction when moving the revealing layer on top of the base layer. Interesting moiré images can be synthesized by applying geometric transformations to both the base and the revealing layers. We propose a mathematical model describing the geometric transformation that a moiré image undergoes, when its base layer and its revealing layer are subject to different freely chosen non-linear geometric transformations. By knowing in advance the layout of a moiré image as a function of the layouts of the base layer and of the revealing layer, we are able to create moiré components running up and down at different speeds and orientations upon translation of the revealing layer. We also derive layer transformations which yield periodic moiré images despite the fact that both the base and the revealing layers are curved. By offering a new means of artistic expression, band moiré images can be used to create new designs and to synthesize visually appealing applications.	Band moiré images	NA:NA	2018
Nicolas Tsingos:Emmanuel Gallo:George Drettakis	We propose a real-time 3D audio rendering pipeline for complex virtual scenes containing hundreds of moving sound sources. The approach, based on auditory culling and spatial level-of-detail, can handle more than ten times the number of sources commonly available on consumer 3D audio hardware, with minimal decrease in audio quality. The method performs well for both indoor and outdoor environments. It leverages the limited capabilities of audio hardware for many applications, including interactive architectural acoustics simulations and automatic 3D voice management for video games.Our approach dynamically eliminates inaudible sources and groups the remaining audible sources into a budget number of clusters. Each cluster is represented by one impostor sound source, positioned using perceptual criteria. Spatial audio processing is then performed only on the impostor sound sources rather than on every original source thus greatly reducing the computational cost.A pilot validation study shows that degradation in audio quality, as well as localization impairment, are limited and do not seem to vary significantly with the cluster budget. We conclude that our real-time perceptual audio rendering pipeline can generate spatialized audio for complex auditory environments without introducing disturbing changes in the resulting perceived soundfield.	Perceptual audio rendering of complex virtual environments	NA:NA:NA	2018
Jun Mitani:Hiromasa Suzuki	We propose a new method for producing unfolded papercraft patterns of rounded toy animal figures from triangulated meshes by means of strip-based approximation. Although in principle a triangulated model can be unfolded simply by retaining as much as possible of its connectivity while checking for intersecting triangles in the unfolded plane, creating a pattern with tens of thousands of triangles is unrealistic. Our approach is to approximate the mesh model by a set of continuous triangle strips with no internal vertices. Initially, we subdivide our mesh into parts corresponding to the features of the model. We segment each part into zonal regions, grouping triangles which are similar topological distances from the part boundary. We generate triangle strips by simplifying the mesh while retaining the borders of the zonal regions and additional cut-lines. The pattern is then created simply by unfolding the set of strips. The distinguishing feature of our method is that we approximate a mesh model by a set of continuous strips, not by other ruled surfaces such as parts of cones or cylinders. Thus, the approximated unfolded pattern can be generated using only mesh operations and a simple unfolding algorithm. Furthermore, a set of strips can be crafted just by bending the paper (without breaking edges) and can represent smooth features of the original mesh models.	Making papercraft toys from meshes using strip-based approximate unfolding	NA:NA	2018
Marc Alexa	NA	Session details: Curves & surfaces	NA	2018
Nina Amenta:Yong Joo Kil	The MLS surface [Levin 2003], used for modeling and rendering with point clouds, was originally defined algorithmically as the output of a particular meshless construction. We give a new explicit definition in terms of the critical points of an energy function on lines determined by a vector field. This definition reveals connections to research in computer vision and computational topology.Variants of the MLS surface can be created by varying the vector field and the energy function. As an example, we define a similar surface determined by a cloud of surfels (points equipped with normals), rather than points.We also observe that some procedures described in the literature to take points in space onto the MLS surface fail to do so, and we describe a simple iterative procedure which does.	Defining point-set surfaces	NA:NA	2018
Lexing Ying:Denis Zorin	We present a smooth surface construction based on the manifold approach of Grimm and Hughes. We demonstrate how this approach can relatively easily produce a number of desirable properties which are hard to achieve simultaneously with polynomial patches, subdivision or variational surfaces. Our surfaces are C∞-continuous with explicit nonsingular C∞ parameterizations, high-order flexible at control vertices, depend linearly on control points, have fixed-size local support for basis functions, and have good visual quality.	A simple manifold-based construction of surfaces of arbitrary smoothness	NA:NA	2018
Thomas W. Sederberg:David L. Cardon:G. Thomas Finnigan:Nicholas S. North:Jianmin Zheng:Tom Lyche	A typical NURBS surface model has a large percentage of superfluous control points that significantly interfere with the design process. This paper presents an algorithm for eliminating such superfluous control points, producing a T-spline. The algorithm can remove substantially more control points than competing methods such as B-spline wavelet decomposition. The paper also presents a new T-spline local refinement algorithm and answers two fundamental open questions on T-spline theory.	T-spline simplification and local refinement	NA:NA:NA:NA:NA:NA	2018
Michael Hofer:Helmut Pottmann	Variational interpolation in curved geometries has many applications, so there has always been demand for geometrically meaningful and efficiently computable splines in manifolds. We extend the definition of the familiar cubic spline curves and splines in tension, and we show how to compute these on parametric surfaces, level sets, triangle meshes, and point samples of surfaces. This list is more comprehensive than it looks, because it includes variational motion design for animation, and allows the treatment of obstacles via barrier surfaces. All these instances of the general concept are handled by the same geometric optimization algorithm, which minimizes an energy of curves on surfaces of arbitrary dimension and codimension.	Energy-minimizing splines in manifolds	NA:NA	2018
Aaron Hertzmann	NA	Session details: Interacting with images	NA	2018
Aseem Agarwala:Mira Dontcheva:Maneesh Agrawala:Steven Drucker:Alex Colburn:Brian Curless:David Salesin:Michael Cohen	We describe an interactive, computer-assisted framework for combining parts of a set of photographs into a single composite picture, a process we call "digital photomontage." Our framework makes use of two techniques primarily: graph-cut optimization, to choose good seams within the constituent images so that they can be combined as seamlessly as possible; and gradient-domain fusion, a process based on Poisson equations, to further reduce any remaining visible artifacts in the composite. Also central to the framework is a suite of interactive tools that allow the user to specify a variety of high-level image objectives, either globally across the image, or locally through a painting-style interface. Image objectives are applied independently at each pixel location and generally involve a function of the pixel values (such as "maximum contrast") drawn from that same location in the set of source images. Typically, a user applies a series of image objectives iteratively in order to create a finished composite. The power of this framework lies in its generality; we show how it can be used for a wide variety of applications, including "selective composites" (for instance, group photos in which everyone looks their best), relighting, extended depth of field, panoramic stitching, clean-plate production, stroboscopic visualization of movement, and time-lapse mosaics.	Interactive digital photomontage	NA:NA:NA:NA:NA:NA:NA:NA	2018
Yin Li:Jian Sun:Chi-Keung Tang:Heung-Yeung Shum	In this paper, we present Lazy Snapping, an interactive image cutout tool. Lazy Snapping separates coarse and fine scale processing, making object specification and detailed adjustment easy. Moreover, Lazy Snapping provides instant visual feedback, snapping the cutout contour to the true object boundary efficiently despite the presence of ambiguous or low contrast edges. Instant feedback is made possible by a novel image segmentation algorithm which combines graph cut with pre-computed over-segmentation. A set of intuitive user interface (UI) tools is designed and implemented to provide flexible control and editing for the users. Usability studies indicate that Lazy Snapping provides a better user experience and produces better segmentation results than the state-of-the-art interactive image cutout tool, Magnetic Lasso in Adobe Photoshop.	Lazy snapping	NA:NA:NA:NA	2018
Carsten Rother:Vladimir Kolmogorov:Andrew Blake	The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for "border matting" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools.	"GrabCut": interactive foreground extraction using iterated graph cuts	NA:NA:NA	2018
Jian Sun:Jiaya Jia:Chi-Keung Tang:Heung-Yeung Shum	In this paper, we formulate the problem of natural image matting as one of solving Poisson equations with the matte gradient field. Our approach, which we call Poisson matting, has the following advantages. First, the matte is directly reconstructed from a continuous matte gradient field by solving Poisson equations using boundary information from a user-supplied trimap. Second, by interactively manipulating the matte gradient field using a number of filtering tools, the user can further improve Poisson matting results locally until he or she is satisfied. The modified local result is seamlessly integrated into the final result. Experiments on many complex natural images demonstrate that Poisson matting can generate good matting results that are not possible using existing matting techniques.	Poisson matting	NA:NA:NA:NA	2018
Julie Dorsey	NA	Session details: 3D texture	NA	2018
Shigeru Owada:Frank Nielsen:Makoto Okabe:Takeo Igarashi	This paper presents an interactive system for designing and browsing volumetric illustrations. Volumetric illustrations are 3D models with internal textures that the user can browse by cutting the models at desired locations. To assign internal textures to a surface mesh, the designer cuts the mesh and provides simple guiding information to specify the correspondence between the cross-section and a reference 2D image. The guiding information is stored with the geometry and used during the synthesis of cross-sectional textures. The key idea is to synthesize a plausible cross-sectional image using a 2D texture-synthesis technique, instead of sampling from a complete 3D RGB volumetric representation directly. This simplifies the design interface and reduces the amount of data, making it possible for non-experts to rapidly design and use volumetric illustrations. We believe that our system can enrich human communications in various domains, such as medicine, biology, and geology.	Volumetric illustration: designing 3D models with internal textures	NA:NA:NA:NA	2018
Robert Jagnow:Julie Dorsey:Holly Rushmeier	We describe the use of traditional stereological methods to synthesize 3D solid textures from 2D images of existing materials. We first illustrate our approach for aggregate materials of spherical particles, and then extend the technique to apply to particles of arbitrary shapes. We demonstrate the effectiveness of the approach with side-by-side comparisons of a real material and a synthetic model with its appearance parameters derived from its physical counterpart. Unlike ad hoc methods for texture synthesis, stereology provides a disciplined, systematic basis for predicting material structure with well-defined assumptions.	Stereological techniques for solid textures	NA:NA:NA	2018
M. Alex O. Vasilescu:Demetri Terzopoulos	This paper introduces a tensor framework for image-based rendering. In particular, we develop an algorithm called TensorTextures that learns a parsimonious model of the bidirectional texture function (BTF) from observational data. Given an ensemble of images of a textured surface, our nonlinear, generative model explicitly represents the multifactor interaction implicit in the detailed appearance of the surface under varying photometric angles, including local (per-texel) reflectance, complex mesostructural self-occlusion, interreflection and self-shadowing, and other BTF-relevant phenomena. Mathematically, TensorTextures is based on multilinear algebra, the algebra of higher-order tensors, hence its name. It is computed through a decomposition known as the N-mode SVD, an extension to tensors of the conventional matrix singular value decomposition (SVD). We demonstrate the application of TensorTextures to the image-based rendering of natural and synthetic textured surfaces under continuously varying viewpoint and illumination conditions.	TensorTextures: multilinear image-based rendering	NA:NA	2018
Yanyun Chen:Xin Tong:Jiaping Wang:Stephen Lin:Baining Guo:Heung-Yeung Shum	We propose a texture function for realistic modeling and efficient rendering of materials that exhibit surface mesostructures, translucency and volumetric texture variations. The appearance of such complex materials for dynamic lighting and viewing directions is expensive to calculate and requires an impractical amount of storage to precompute. To handle this problem, our method models an object as a shell layer, formed by texture synthesis of a volumetric material sample, and a homogeneous inner core. To facilitate computation of surface radiance from the shell layer, we introduce the shell texture function (STF) which describes voxel irradiance fields based on precomputed fine-level light interactions such as shadowing by surface mesostructures and scattering of photons inside the object. Together with a diffusion approximation of homogeneous inner core radiance, the STF leads to fast and detailed raytraced renderings of complex materials.	Shell texture functions	NA:NA:NA:NA:NA:NA	2018
Dani Lischinski	NA	Session details: Photo & video texture	NA	2018
Hui Fang:John C. Hart	We combine existing techniques for shape-from-shading and texture synthesis to create a new tool for texturing objects in photographs. Our approach clusters pixels with similar recovered normals into patches on which texture is synthesized. Distorting the texture based on the recovered normals creates the illusion that the texture adheres to the undulations of the photographed surface. Inconsistencies in the recovered surface are disguised by the graphcut blending of the individually textured patches. Further applications include the generation of detail on manually-shaded painting, extracting and synthesizing a displacement map from a texture swatch, and the embossed transfer of normals from one image to another, which would be difficult to create with current image processing packages.	Textureshop: texture synthesis as a photograph editing tool	NA:NA	2018
Kiran S. Bhat:Steven M. Seitz:Jessica K. Hodgins:Pradeep K. Khosla	This paper presents a novel algorithm for synthesizing and editing video of natural phenomena that exhibit continuous flow patterns. The algorithm analyzes the motion of textured particles in the input video along user-specified flow lines, and synthesizes seamless video of arbitrary length by enforcing temporal continuity along a second set of user-specified flow lines. The algorithm is simple to implement and use. We used this technique to edit video of water-falls, rivers, flames, and smoke.	Flow-based video synthesis and editing	NA:NA:NA:NA	2018
Qing Wu:Yizhou Yu	One significant problem in patch-based texture synthesis is the presence of broken features at the boundary of adjacent patches. The reason is that optimization schemes for patch merging may fail when neighborhood search cannot find satisfactory candidates in the sample texture because of an inaccurate similarity measure. In this paper, we consider both curvilinear features and their deformation. We develop a novel algorithm to perform feature matching and alignment by measuring structural similarity. Our technique extracts a feature map from the sample texture, and produces both a new feature map and texture map. Texture synthesis guided by feature maps can significantly reduce the number of feature discontinuities and related artifacts, and gives rise to satisfactory results.	Feature matching and deformation for texture synthesis	NA:NA	2018
Yanxi Liu:Wen-Chieh Lin:James Hays	A near-regular texture deviates geometrically and photometrically from a regular congruent tiling. Although near-regular textures are ubiquitous in the man-made and natural world, they present computational challenges for state of the art texture analysis and synthesis algorithms. Using regular tiling as our anchor point, and with user-assisted lattice extraction, we can explicitly model the deformation of a near-regular texture with respect to geometry, lighting and color. We treat a deformation field both as a function that acts on a texture and as a texture that is acted upon, and develop a multi-modal framework where each deformation field is subject to analysis, synthesis and manipulation. Using this formalization, we are able to construct simple parametric models to faithfully synthesize the appearance of a near-regular texture and purposefully control its regularity.	Near-regular texture analysis and manipulation	NA:NA:NA	2018
Jovan Popović	NA	Session details: Dynamics & modeling	NA	2018
Mark Carlson:Peter J. Mucha:Greg Turk	We present the Rigid Fluid method, a technique for animating the interplay between rigid bodies and viscous incompressible fluid with free surfaces. We use distributed Lagrange multipliers to ensure two-way coupling that generates realistic motion for both the solid objects and the fluid as they interact with one another. We call our method the rigid fluid method because the simulator treats the rigid objects as if they were made of fluid. The rigidity of such an object is maintained by identifying the region of the velocity field that is inside the object and constraining those velocities to be rigid body motion. The rigid fluid method is straightforward to implement, incurs very little computational overhead, and can be added as a bridge between current fluid simulators and rigid body solvers. Many solid objects of different densities (e.g., wood or lead) can be combined in the same animation.	Rigid fluid: animating the interplay between rigid bodies and fluid	NA:NA:NA	2018
Neil Molino:Zhaosheng Bao:Ron Fedkiw	We propose a virtual node algorithm that allows material to separate along arbitrary (possibly branched) piecewise linear paths through a mesh. The material within an element is fragmented by creating several replicas of the element and assigning a portion of real material to each replica. This results in elements that contain both real material and empty regions. The missing material is contained in another copy (or copies) of this element. Our new virtual node algorithm automatically determines the number of replicas and the assignment of material to each. Moreover, it provides the degrees of freedom required to simulate the partially or fully fragmented material in a fashion consistent with the embedded geometry. This approach enables efficient simulation of complex geometry with a simple mesh, i.e. the geometry need not align itself with element boundaries. It also alleviates many shortcomings of traditional Lagrangian simulation techniques for meshes with changing topology. For example, slivers do not require small CFL time step restrictions since they are embedded in well shaped larger elements. To enable robust simulation of embedded geometry, we propose new algorithms for handling rigid body and self collisions. In addition, we present several mechanisms for influencing and controlling fracture with grain boundaries, prescoring, etc. We illustrate our method for both volumetric and thin-shell simulations.	A virtual node algorithm for changing mesh topology during simulation	NA:NA:NA	2018
Doug L. James:Dinesh K. Pai	We introduce the Bounded Deformation Tree, or BD-Tree, which can perform collision detection with reduced deformable models at costs comparable to collision detection with rigid objects. Reduced deformable models represent complex deformations as linear superpositions of arbitrary displacement fields, and are used in a variety of applications of interactive computer graphics. The BD-Tree is a bounding sphere hierarchy for output-sensitive collision detection with such models. Its bounding spheres can be updated after deformation in any order, and at a cost independent of the geometric complexity of the model; in fact the cost can be as low as one multiplication and addition per tested sphere, and at most linear in the number of reduced deformation coordinates. We show that the BD-Tree is also extremely simple to implement, and performs well in practice for a variety of real-time and complex off-line deformable simulation examples.	BD-tree: output-sensitive collision detection for reduced deformable models	NA:NA	2018
Robert W. Sumner:Jovan Popović	Deformation transfer applies the deformation exhibited by a source triangle mesh onto a different target triangle mesh. Our approach is general and does not require the source and target to share the same number of vertices or triangles, or to have identical connectivity. The user builds a correspondence map between the triangles of the source and those of the target by specifying a small set of vertex markers. Deformation transfer computes the set of transformations induced by the deformation of the source mesh, maps the transformations through the correspondence from the source to the target, and solves an optimization problem to consistently apply the transformations to the target shape. The resulting system of linear equations can be factored once, after which transferring a new deformation to the target mesh requires only a backsubstitution step. Global properties such as foot placement can be achieved by constraining vertex positions. We demonstrate our method by retargeting full body key poses, applying scanned facial deformations onto a digital character, and remapping rigid and non-rigid animation sequences from one mesh onto another.	Deformation transfer for triangle meshes	NA:NA	2018
Maneesh Agrawala	NA	Session details: Identifying & sketching the future	NA	2018
Ramesh Raskar:Paul Beardsley:Jeroen van Baar:Yao Wang:Paul Dietz:Johnny Lee:Darren Leigh:Thomas Willwacher	This paper describes how to instrument the physical world so that objects become self-describing, communicating their identity, geometry, and other information such as history or user annotation. The enabling technology is a wireless tag which acts as a radio frequency identity and geometry (RFIG) transponder. We show how addition of a photo-sensor to a wireless tag significantly extends its functionality to allow geometric operations - such as finding the 3D position of a tag, or detecting change in the shape of a tagged object. Tag data is presented to the user by direct projection using a handheld locale-aware mobile projector. We introduce a novel technique that we call interactive projection to allow a user to interact with projected information e.g. to navigate or update the projected information.The ideas are demonstrated using objects with active radio frequency (RF) tags. But the work was motivated by the advent of unpowered passive-RFID, a technology that promises to have significant impact in real-world applications. We discuss how our current prototypes could evolve to passive-RFID in the future.	RFIG lamps: interacting with a self-describing world via photosensing wireless tags and projectors	NA:NA:NA:NA:NA:NA:NA:NA	2018
J. P. Lewis:Ruth Rosenholtz:Nickson Fong:Ulrich Neumann	Although existing GUIs have a sense of space, they provide no sense of place. Numerous studies report that users misplace files and have trouble wayfinding in virtual worlds despite the fact that people have remarkable visual and spatial abilities. This issue is considered in the human-computer interface field and has been addressed with alternate display/navigation schemes. Our paper presents a fundamentally graphics based approach to this 'lost in hyperspace' problem. Specifically, we propose that spatial display of files is not sufficient to engage our visual skills; scenery (distinctive visual appearance) is needed as well. While scenery (in the form of custom icon assignments) is already possible in current operating systems, few if any users take the time to manually assign icons to all their files. As such, our proposal is to generate visually distinctive icons ("VisualIDs") automatically, while allowing the user to replace the icon if desired. The paper discusses psychological and conceptual issues relating to icons, visual memory, and the necessary relation of scenery to data. A particular icon generation algorithm is described; subjects using these icons in simulated file search and recall tasks show significantly improved performance with little effort. Although the incorporation of scenery in a graphical user interface will introduce many new (and interesting) design problems that cannot be addressed in this paper, we show that automatically created scenery is both beneficial and feasible.	VisualIDs: automatic distinctive icons for desktop interfaces	NA:NA:NA:NA	2018
Matthew Thorne:David Burke:Michiel van de Panne	In this paper we present a novel system for sketching the motion of a character. The process begins by sketching a character to be animated. An animated motion is then created for the character by drawing a continuous sequence of lines, arcs, and loops. These are parsed and mapped to a parameterized set of output motions that further reflect the location and timing of the input sketch. The current system supports a repertoire of 18 different types of motions in 2D and a subset of these in 3D. The system is unique in its use of a cursive motion specification, its ability to allow for fast experimentation, and its ease of use for non-experts.	Motion doodles: an interface for sketching character motion	NA:NA:NA	2018
Joseph J. LaViola, Jr.:Robert C. Zeleznik	We present mathematical sketching, a novel, pen-based, modeless gestural interaction paradigm for mathematics problem solving. Mathematical sketching derives from the familiar pencil-and-paper process of drawing supporting diagrams to facilitate the formulation of mathematical expressions; however, with a mathematical sketch, users can also leverage their physical intuition by watching their hand-drawn diagrams animate in response to continuous or discrete parameter changes in their written formulas. Diagram animation is driven by implicit associations that are inferred, either automatically or with gestural guidance, from mathematical expressions, diagram labels, and drawing elements. The modeless nature of mathematical sketching enables users to switch freely between modifying diagrams or expressions and viewing animations. Mathematical sketching can also support computational tools for graphing, manipulating and solving equations; initial feedback from a small user group of our mathematical sketching prototype application, MathPad2, suggests that it has the potential to be a powerful tool for mathematical problem solving and visualization.	MathPad2: a system for the creation and exploration of mathematical sketches	NA:NA	2018
Doug L. James	NA	Session details: Smoke, water & goop	NA	2018
Raanan Fattal:Dani Lischinski	In this paper we present a new method for efficiently controlling animated smoke. Given a sequence of target smoke states, our method generates a smoke simulation in which the smoke is driven towards each of these targets in turn, while exhibiting natural-looking interesting smoke-like behavior. This control is made possible by two new terms that we add to the standard flow equations: (i) a driving force term that causes the fluid to carry the smoke towards a particular target, and (ii) a smoke gathering term that prevents the smoke from diffusing too much. These terms are explicitly defined by the instantaneous state of the system at each simulation timestep. Thus, no expensive optimization is required, allowing complex smoke animations to be generated with very little additional cost compared to ordinary flow simulations.	Target-driven smoke animation	NA:NA	2018
Antoine McNamara:Adrien Treuille:Zoran Popović:Jos Stam	We describe a novel method for controlling physics-based fluid simulations through gradient-based nonlinear optimization. Using a technique known as the adjoint method, derivatives can be computed efficiently, even for large 3D simulations with millions of control parameters. In addition, we introduce the first method for the full control of free-surface liquids. We show how to compute adjoint derivatives through each step of the simulation, including the fast marching algorithm, and describe a new set of control parameters specifically designed for liquids.	Fluid control using the adjoint method	NA:NA:NA:NA	2018
Frank Losasso:Frédéric Gibou:Ron Fedkiw	We present a method for simulating water and smoke on an unrestricted octree data structure exploiting mesh refinement techniques to capture the small scale visual detail. We propose a new technique for discretizing the Poisson equation on this octree grid. The resulting linear system is symmetric positive definite enabling the use of fast solution methods such as preconditioned conjugate gradients, whereas the standard approximation to the Poisson equation on an octree grid results in a non-symmetric linear system which is more computationally challenging to invert. The semi-Lagrangian characteristic tracing technique is used to advect the velocity, smoke density, and even the level set making implementation on an octree straightforward. In the case of smoke, we have multiple refinement criteria including object boundaries, optical depth, and vorticity concentration. In the case of water, we refine near the interface as determined by the zero isocontour of the level set function.	Simulating water and smoke with an octree data structure	NA:NA:NA	2018
Tolga G. Goktekin:Adam W. Bargteil:James F. O'Brien	This paper describes a technique for animating the behavior of viscoelastic fluids, such as mucus, liquid soap, pudding, toothpaste, or clay, that exhibit a combination of both fluid and solid characteristics. The technique builds upon prior Eulerian methods for animating incompressible fluids with free surfaces by including additional elastic terms in the basic Navier-Stokes equations. The elastic terms are computed by integrating and advecting strain-rate throughout the fluid. Transition from elastic resistance to viscous flow is controlled by von Mises's yield condition, and subsequent behavior is then governed by a quasi-linear plasticity model.	A method for animating viscoelastic fluids	NA:NA:NA	2018
Kavita Bala	NA	Session details: Lighting & sampling	NA	2018
Eric Tabellion:Arnauld Lamorlette	Lighting models used in the production of computer generated feature animation have to be flexible, easy to control, and efficient to compute. Global illumination techniques do not lend themselves easily to flexibility, ease of use, or speed, and have remained out of reach thus far for the vast majority of images generated in this context. This paper describes the implementation and integration of indirect illumination within a feature animation production renderer. For efficiency reasons, we choose to partially solve the rendering equation. We explain how this compromise allows us to speed-up final gathering calculations and reduce noise. We describe an efficient ray tracing strategy and its integration with a micro-polygon based scan line renderer supporting displacement mapping and programmable shaders. We combine a modified irradiance gradient caching technique with an approximate lighting model that enhances caching coherence and provides good scalability to render complex scenes into high-resolution images suitable for film. We describe the tools that are made available to the artists to control indirect lighting in final renders. We show that our approach provides an efficient solution, easy to art direct, that allows animators to enhance considerably the quality of images generated for a large category of production work.	An approximate global illumination system for computer generated films	NA:NA	2018
Ren Ng:Ravi Ramamoorthi:Pat Hanrahan	This paper focuses on efficient rendering based on pre-computed light transport, with realistic materials and shadows under all-frequency direct lighting such an environment maps. The basic difficulty is representation and computation in the 6D space of light direction, view direction, and surface position. While image-based and synthetic methods for real-time rendering have been proposed, they do not scale to high sampling rates with variation of both lighting and viewpoint. Current approaches are therefore limited to lower dimensionality (only lighting or viewpoint variation, not both) or lower sampling rates (low frequency lighting and materials). We propose a new mathematical and computational analysis of pre-computed light transport. We use factored forms, separately pre-computing and representing visibility and material properties. Rendering then requires computing triple product integrals at each vertex, involving the lighting, visibility and BRDF. Our main contribution is a general analysis of these triple product integrals, which are likely to have broad applicability in computer graphics and numerical analysis. We first determine the computational complexity in a number of bases like point samples, spherical harmonics and wavelets. We then give efficient linear and sublinear-time algorithms for Haar wavelets, incorporating non-linear wavelet approximation of lighting and BRDFs. Practically, we demonstrate rendering of images under new lighting and viewing conditions in a few seconds, significantly faster than previous techniques.	Triple product wavelet integrals for all-frequency relighting	NA:NA:NA	2018
Victor Ostromoukhov:Charles Donohue:Pierre-Marc Jodoin	This paper presents a novel method for efficiently generating a good sampling pattern given an importance density over a 2D domain. A Penrose tiling is hierarchically subdivided creating a sufficiently large number of sample points. These points are numbered using the Fibonacci number system, and these numbers are used to threshold the samples against the local value of the importance density. Pre-computed correction vectors, obtained using relaxation, are used to improve the spectral characteristics of the sampling pattern. The technique is deterministic and very fast; the sampling time grows linearly with the required number of samples. We illustrate our technique with importance-based environment mapping, but the technique is versatile enough to be used in a large variety of computer graphics applications, such as light transport calculations, digital halftoning, geometry processing, and various rendering techniques.	Fast hierarchical importance sampling with blue noise properties	NA:NA:NA	2018
Jason Lawrence:Szymon Rusinkiewicz:Ravi Ramamoorthi	High-quality Monte Carlo image synthesis requires the ability to importance sample realistic BRDF models. However, analytic sampling algorithms exist only for the Phong model and its derivatives such as Lafortune and Blinn-Phong. This paper demonstrates an importance sampling technique for a wide range of BRDFs, including complex analytic models such as Cook-Torrance and measured materials, which are being increasingly used for realistic image synthesis. Our approach is based on a compact factored representation of the BRDF that is optimized for sampling. We show that our algorithm consistently offers better efficiency than alternatives that involve fitting and sampling a Lafortune or Blinn-Phong lobe, and is more compact than sampling strategies based on tabulating the full BRDF. We are able to efficiently create images involving multiple measured and analytic BRDFs, under both complex direct lighting and global illumination.	Efficient BRDF importance sampling using a factored representation	NA:NA:NA	2018
Nancy Pollard	NA	Session details: Data driven character animation	NA	2018
Matthew Stone:Doug DeCarlo:Insuk Oh:Christian Rodriguez:Adrian Stere:Alyssa Lees:Chris Bregler	We describe a method for using a database of recorded speech and captured motion to create an animated conversational character. People's utterances are composed of short, clearly-delimited phrases; in each phrase, gesture and speech go together meaningfully and synchronize at a common point of maximum emphasis. We develop tools for collecting and managing performance data that exploit this structure. The tools help create scripts for performers, help annotate and segment performance data, and structure specific messages for characters to use within application contexts. Our animations then reproduce this structure. They recombine motion samples with new speech samples to recreate coherent phrases, and blend segments of speech and motion together phrase-by-phrase into extended utterances. By framing problems for utterance generation and synthesis so that they can draw closely on a talented performance, our techniques support the rapid construction of animated characters with rich and appropriate expression.	Speaking with hands: creating animated conversational characters from recordings of human performance	NA:NA:NA:NA:NA:NA:NA	2018
Alla Safonova:Jessica K. Hodgins:Nancy S. Pollard	Optimization is an appealing way to compute the motion of an animated character because it allows the user to specify the desired motion in a sparse, intuitive way. The difficulty of solving this problem for complex characters such as humans is due in part to the high dimensionality of the search space. The dimensionality is an artifact of the problem representation because most dynamic human behaviors are intrinsically low dimensional with, for example, legs and arms operating in a coordinated way. We describe a method that exploits this observation to create an optimization problem that is easier to solve. Our method utilizes an existing motion capture database to find a low-dimensional space that captures the properties of the desired behavior. We show that when the optimization problem is solved within this low-dimensional subspace, a sparse sketch can be used as an initial guess and full physics constraints can be enabled. We demonstrate the power of our approach with examples of forward, vertical, and turning jumps; with running and walking; and with several acrobatic flips.	Synthesizing physically realistic human motion in low-dimensional, behavior-specific spaces	NA:NA:NA	2018
Keith Grochow:Steven L. Martin:Aaron Hertzmann:Zoran Popović	This paper presents an inverse kinematics system based on a learned model of human poses. Given a set of constraints, our system can produce the most likely pose satisfying those constraints, in real-time. Training the model on different input data leads to different styles of IK. The model is represented as a probability distribution over the space of all possible poses. This means that our IK system can generate any pose, but prefers poses that are most similar to the space of poses in the training data. We represent the probability with a novel model called a Scaled Gaussian Process Latent Variable Model. The parameters of the model are all learned automatically; no manual tuning is required for the learning component of the system. We additionally describe a novel procedure for interpolating between styles.Our style-based IK can replace conventional IK, wherever it is used in computer animation and computer vision. We demonstrate our system in the context of a number of applications: interactive character posing, trajectory keyframing, real-time motion capture with missing markers, and posing from a 2D image.	Style-based inverse kinematics	NA:NA:NA:NA	2018
Katsu Yamane:James J. Kuffner:Jessica K. Hodgins	Even such simple tasks as placing a box on a shelf are difficult to animate, because the animator must carefully position the character to satisfy geometric and balance constraints while creating motion to perform the task with a natural-looking style. In this paper, we explore an approach for animating characters manipulating objects that combines the power of path planning with the domain knowledge inherent in data-driven, constraint-based inverse kinematics. A path planner is used to find a motion for the object such that the corresponding poses of the character satisfy geometric, kinematic, and posture constraints. The inverse kinematics computation of the character's pose resolves redundancy by biasing the solution toward natural-looking poses extracted from a database of captured motions. Having this database greatly helps to increase the quality of the output motion. The computed path is converted to a motion trajectory using a model of the velocity profile. We demonstrate the effectiveness of the algorithm by generating animations across a wide range of scenarios that cover variations in the geometric, kinematic, and dynamic models of the character, the manipulated object, and obstacles in the scene.	Synthesizing animations of human manipulation tasks	NA:NA:NA	2018
Leonard McMillan	NA	Session details: Shape & motion	NA	2018
Christian Theobalt:Irene Albrecht:Jörg Haber:Marcus Magnor:Hans-Peter Seidel	Athletes and coaches in most professional sports make use of high-tech equipment to analyze and, subsequently, improve the athlete's performance. High-speed video cameras are employed, for instance, to record the swing of a golf club or a tennis racket, the movement of the feet while running, and the body motion in apparatus gymnastics. High-tech and high-speed equipment, however, usually implies high-cost as well. In this paper, we present a passive optical approach to capture high-speed motion using multi-exposure images obtained with low-cost commodity still cameras and a stroboscope. The recorded motion remains completely undisturbed by the motion capture process. We apply our approach to capture the motion of hand and ball for a variety of baseball pitches and present algorithms to automatically track the position, velocity, rotation axis, and spin of the ball along its trajectory. To demonstrate the validity of our setup and algorithms, we analyze the consistency of our measurements with a physically based model that predicts the trajectory of a spinning baseball. Our approach can be applied to capture a wide variety of other high-speed objects and activities such as golfing, bowling, or tennis for visualization as well as analysis purposes.	Pitching a baseball: tracking high-speed motion with multi-exposure images	NA:NA:NA:NA:NA	2018
Li Zhang:Noah Snavely:Brian Curless:Steven M. Seitz	We present an end-to-end system that goes from video sequences to high resolution, editable, dynamically controllable face models. The capture system employs synchronized video cameras and structured light projectors to record videos of a moving face from multiple viewpoints. A novel spacetime stereo algorithm is introduced to compute depth maps accurately and overcome over-fitting deficiencies in prior work. A new template fitting and tracking procedure fills in missing data and yields point correspondence across the entire sequence without using markers. We demonstrate a data-driven, interactive method for inverse kinematics that draws on the large set of fitted templates and allows for posing new expressions by dragging surface points directly. Finally, we describe new tools that model the dynamics in the input sequence to enable new animations, created via key-framing or texture-synthesis techniques.	Spacetime faces: high resolution capture for modeling and animation	NA:NA:NA:NA	2018
Lucas Kovar:Michael Gleicher	Large motion data sets often contain many variants of the same kind of motion, but without appropriate tools it is difficult to fully exploit this fact. This paper provides automated methods for identifying logically similar motions in a data set and using them to build a continuous and intuitively parameterized space of motions. To find logically similar motions that are numerically dissimilar, our search method employs a novel distance metric to find "close" motions and then uses them as intermediaries to find more distant motions. Search queries are answered at interactive speeds through a precomputation that compactly represents all possibly similar motion segments. Once a set of related motions has been extracted, we automatically register them and apply blending techniques to create a continuous space of motions. Given a function that defines relevant motion parameters, we present a method for extracting motions from this space that accurately possess new parameters requested by the user. Our algorithm extends previous work by explicitly constraining blend weights to reasonable values and having a run-time cost that is nearly independent of the number of example motions. We present experimental results on a test data set of 37,000 frames, or about ten minutes of motion sampled at 60 Hz.	Automated extraction and parameterization of motions in large data sets	NA:NA	2018
Jason Harrison:Ronald A. Rensink:Michiel van de Panne	In this paper we examine to what extent the lengths of the links in an animated articulated-figure can be changed without the viewer being aware of the change. This is investigated in terms of a framework that emphasizes the role of attention in visual perception. We conducted a set of five experiments to establish bounds for the sensitivity to changes in length as a function of several parameters and the amount of attention available. We found that while length changes of 3% can be perceived when the relevant links are given full attention, changes of over 20% can go unnoticed when attention is not focused in this way. These results provide general guidelines for algorithms that produce or process character motion data and also bring to light some of the potential gains that stand to be achieved with attention-based algorithms.	Obscuring length changes during animated motion	NA:NA:NA	2018
Irfan Essa	NA	Session details: Video-based rendering	NA	2018
Jue Wang:Yingqing Xu:Heung-Yeung Shum:Michael F. Cohen	We describe a system for transforming an input video into a highly abstracted, spatio-temporally coherent cartoon animation with a range of styles. To achieve this, we treat video as a space-time volume of image data. We have developed an anisotropic kernel mean shift technique to segment the video data into contiguous volumes. These provide a simple cartoon style in themselves, but more importantly provide the capability to semi-automatically rotoscope semantically meaningful regions.In our system, the user simply outlines objects on keyframes. A mean shift guided interpolation algorithm is then employed to create three dimensional semantic regions by interpolation between the keyframes, while maintaining smooth trajectories along the time dimension. These regions provide the basis for creating smooth two dimensional edge sheets and stroke sheets embedded within the spatio-temporal video volume. The regions, edge sheets, and stroke sheets are rendered by slicing them at particular times. A variety of styles of rendering are shown. The temporal coherence provided by the smoothed semantic regions and sheets results in a temporally consistent non-photorealistic appearance.	Video tooning	NA:NA:NA:NA	2018
Aseem Agarwala:Aaron Hertzmann:David H. Salesin:Steven M. Seitz	We describe a new approach to rotoscoping --- the process of tracking contours in a video sequence --- that combines computer vision with user interaction. In order to track contours in video, the user specifies curves in two or more frames; these curves are used as keyframes by a computer-vision-based tracking algorithm. The user may interactively refine the curves and then restart the tracking algorithm. Combining computer vision with user interaction allows our system to track any sequence with significantly less effort than interpolation-based systems --- and with better reliability than "pure" computer vision systems. Our tracking algorithm is cast as a spacetime optimization problem that solves for time-varying curve shapes based on an input video sequence and user-specified constraints. We demonstrate our system with several rotoscoped examples. Additionally, we show how these rotoscoped contours can be used to help create cartoon animation by attaching user-drawn strokes to the tracked contours.	Keyframe-based tracking for rotoscoping and animation	NA:NA:NA:NA	2018
Peter Sand:Seth Teller	This paper describes a method for bringing two videos (recorded at different times) into spatiotemporal alignment, then comparing and combining corresponding pixels for applications such as background subtraction, compositing, and increasing dynamic range. We align a pair of videos by searching for frames that best match according to a robust image registration process. This process uses locally weighted regression to interpolate and extrapolate high-likelihood image correspondences, allowing new correspondences to be discovered and refined. Image regions that cannot be matched are detected and ignored, providing robustness to changes in scene content and lighting, which allows a variety of new applications.	Video matching	NA:NA	2018
C. Lawrence Zitnick:Sing Bing Kang:Matthew Uyttendaele:Simon Winder:Richard Szeliski	The ability to interactively control viewpoint while watching a video is an exciting application of image-based rendering. The goal of our work is to render dynamic scenes with interactive viewpoint control using a relatively small number of video cameras. In this paper, we show how high-quality video-based rendering of dynamic scenes can be accomplished using multiple synchronized video streams combined with novel image-based modeling and rendering algorithms. Once these video streams have been processed, we can synthesize any intermediate view between cameras at any time, with the potential for space-time manipulation.In our approach, we first use a novel color segmentation-based stereo algorithm to generate high-quality photoconsistent correspondences across all camera views. Mattes for areas near depth discontinuities are then automatically extracted to reduce artifacts during view synthesis. Finally, a novel temporal two-layer compressed representation that handles matting is developed for rendering at interactive rates.	High-quality video view interpolation using a layered representation	NA:NA:NA:NA:NA	2018
Nina Amenta	NA	Session details: Shape analysis	NA	2018
Yutaka Ohtake:Alexander Belyaev:Hans-Peter Seidel	We propose a simple and effective method for detecting view-and scale-independent ridge-valley lines defined via first- and second-order curvature derivatives on shapes approximated by dense triangle meshes. A high-quality estimation of high-order surface derivatives is achieved by combining multi-level implicit surface fitting and finite difference approximations. We demonstrate that the ridges and valleys are geometrically and perceptually salient surface features, and, therefore, can be potentially used for shape recognition, coding, and quality evaluation purposes.	Ridge-valley lines on meshes via implicit surface fitting	NA:NA:NA	2018
Xinlai Ni:Michael Garland:John C. Hart	Morse theory reveals the topological structure of a shape based on the critical points of a real function over the shape. A poor choice of this real function can lead to a complex configuration of an unnecessarily high number of critical points. This paper solves a relaxed form of Laplace's equation to find a "fair" Morse function with a user-controlled number and configuration of critical points. When the number is minimal, the resulting Morse complex cuts the shape into a disk. Specifying additional critical points at surface features yields a base domain that better represents the geometry and shares the same topology as the original mesh, and can also cluster a mesh into approximately developable patches. We make Morse theory on meshes more robust with teflon saddles and flat edge collapses, and devise a new "intermediate value propagation" multigrid solver for finding fair Morse functions that runs in provably linear time.	Fair morse functions for extracting the topological structure of a surface mesh	NA:NA:NA	2018
Michael Kazhdan:Thomas Funkhouser:Szymon Rusinkiewicz	With recent improvements in methods for the acquisition and rendering of 3D models, the need for retrieval of models has gained prominence in the graphics and vision communities. A variety of methods have been proposed that enable the efficient querying of model repositories for a desired 3D shape. Many of these methods use a 3D model as a query and attempt to retrieve models from the database that have a similar shape.In this paper we consider the implications of anisotropy on the shape matching paradigm. In particular, we propose a novel method for matching 3D models that factors the shape matching equation as the disjoint outer product of anisotropy and geometric comparisons. We provide a general method for computing the factored similarity metric and show how this approach can be applied to improve the matching performance of many existing shape matching methods.	Shape matching and anisotropy	NA:NA:NA	2018
Mark Pauly	NA	Session details: Interactive modeling	NA	2018
Mario Botsch:Leif Kobbelt	We present a freeform modeling framework for unstructured triangle meshes which is based on constraint shape optimization. The goal is to simplify the user interaction even for quite complex freeform or multiresolution modifications. The user first sets various boundary constraints to define a custom tailored (abstract) basis function which is adjusted to a given design task. The actual modification is then controlled by moving one single 9-dof manipulator object. The technique can handle arbitrary support regions and piecewise boundary conditions with smoothness ranging continuously from C0 to C2. To more naturally adapt the modification to the shape of the support region, the deformed surface can be tuned to bend with anisotropic stiffness. We are able to achieve real-time response in an interactive design session even for complex meshes by precomputing a set of scalar-valued basis functions that correspond to the degrees of freedom of the manipulator by which the user controls the modification.	An intuitive framework for real-time freeform modeling	NA:NA	2018
Jianbo Peng:Daniel Kristjansson:Denis Zorin	Volume textures aligned with a surface can be used to add topologically complex geometric detail to objects in an efficient way, while retaining an underlying simple surface structure.Adding a volume texture to a surface requires more than a conventional two-dimensional parameterization: a part of the space surrounding the surface has to be parameterized. Another problem with using volume textures for adding geometric detail is the difficulty in rendering implicitly represented surfaces, especially when they are changed interactively.In this paper we present algorithms for constructing and rendering volume-textured surfaces. We demonstrate a number of interactive operations that these algorithms enable.	Interactive modeling of topologically complex geometric detail	NA:NA:NA	2018
Yizhou Yu:Kun Zhou:Dong Xu:Xiaohan Shi:Hujun Bao:Baining Guo:Heung-Yeung Shum	In this paper, we introduce a novel approach to mesh editing with the Poisson equation as the theoretical foundation. The most distinctive feature of this approach is that it modifies the original mesh geometry implicitly through gradient field manipulation. Our approach can produce desirable and pleasing results for both global and local editing operations, such as deformation, object merging, and smoothing. With the help from a few novel interactive tools, these operations can be performed conveniently with a small amount of user interaction. Our technique has three key components, a basic mesh solver based on the Poisson equation, a gradient field manipulation scheme using local transforms, and a generalized boundary condition representation based on local frames. Experimental results indicate that our framework can outperform previous related mesh editing techniques.	Mesh editing with poisson-based gradient field manipulation	NA:NA:NA:NA:NA:NA:NA	2018
Thomas Funkhouser:Michael Kazhdan:Philip Shilane:Patrick Min:William Kiefer:Ayellet Tal:Szymon Rusinkiewicz:David Dobkin	In this paper, we investigate a data-driven synthesis approach to constructing 3D geometric surface models. We provide methods with which a user can search a large database of 3D meshes to find parts of interest, cut the desired parts out of the meshes with intelligent scissoring, and composite them together in different ways to form new objects. The main benefit of this approach is that it is both easy to learn and able to produce highly detailed geometric models -- the conceptual design for new models comes from the user, while the geometric details come from examples in the database. The focus of the paper is on the main research issues motivated by the proposed approach: (1) interactive segmentation of 3D surfaces, (2) shape-based search to find 3D models with parts matching a query, and (3) composition of parts to form new models. We provide new research contributions on all three topics and incorporate them into a prototype modeling system. Experience with our prototype system indicates that it allows untrained users to create interesting and detailed 3D models.	Modeling by example	NA:NA:NA:NA:NA:NA:NA:NA	2018
Rick Szeliski	NA	Session details: Flash & color	NA	2018
Georg Petschnigg:Richard Szeliski:Maneesh Agrawala:Michael Cohen:Hugues Hoppe:Kentaro Toyama	Digital photography has made it possible to quickly and easily take a pair of images of low-light environments: one with flash to capture detail and one without flash to capture ambient illumination. We present a variety of applications that analyze and combine the strengths of such flash/no-flash image pairs. Our applications include denoising and detail transfer (to merge the ambient qualities of the no-flash image with the high-frequency flash detail), white-balancing (to change the color tone of the ambient image), continuous flash (to interactively adjust flash intensity), and red-eye removal (to repair artifacts in the flash image). We demonstrate how these applications can synthesize new images that are of higher quality than either of the originals.	Digital photography with flash and no-flash image pairs	NA:NA:NA:NA:NA:NA	2018
Elmar Eisemann:Frédo Durand	We enhance photographs shot in dark environments by combining a picture taken with the available light and one taken with the flash. We preserve the ambiance of the original lighting and insert the sharpness from the flash image. We use the bilateral filter to decompose the images into detail and large scale. We reconstruct the image using the large scale of the available lighting and the detail of the flash. We detect and correct flash shadows. This combines the advantages of available illumination and flash photography.	Flash photography enhancement via intrinsic relighting	NA:NA	2018
Ramesh Raskar:Kar-Han Tan:Rogerio Feris:Jingyi Yu:Matthew Turk	We present a non-photorealistic rendering approach to capture and convey shape features of real-world scenes. We use a camera with multiple flashes that are strategically positioned to cast shadows along depth discontinuities in the scene. The projective-geometric relationship of the camera-flash setup is then exploited to detect depth discontinuities and distinguish them from intensity edges due to material discontinuities.We introduce depiction methods that utilize the detected edge features to generate stylized static and animated images. We can highlight the detected features, suppress unnecessary details or combine features from multiple images. The resulting images more clearly convey the 3D structure of the imaged scenes.We take a very different approach to capturing geometric features of a scene than traditional approaches that require reconstructing a 3D model. This results in a method that is both surprisingly simple and computationally efficient. The entire hardware/software setup can conceivably be packaged into a self-contained device no larger than existing digital cameras.	Non-photorealistic camera: depth edge detection and stylized rendering using multi-flash imaging	NA:NA:NA:NA:NA	2018
Anat Levin:Dani Lischinski:Yair Weiss	Colorization is a computer-assisted process of adding color to a monochrome image or movie. The process typically involves segmenting images into regions and tracking these regions across image sequences. Neither of these tasks can be performed reliably in practice; consequently, colorization requires considerable user intervention and remains a tedious, time-consuming, and expensive task.In this paper we present a simple colorization method that requires neither precise image segmentation, nor accurate region tracking. Our method is based on a simple premise; neighboring pixels in space-time that have similar intensities should have similar colors. We formalize this premise using a quadratic cost function and obtain an optimization problem that can be solved efficiently using standard techniques. In our approach an artist only needs to annotate the image with a few color scribbles, and the indicated colors are automatically propagated in both space and time to produce a fully colorized image or sequence. We demonstrate that high quality colorizations of stills and movie clips may be obtained from a relatively modest amount of user input.	Colorization using optimization	NA:NA:NA	2018
Marcus Gross	NA	Session details: Capture from images	NA	2018
David Koller:Michael Turitzin:Marc Levoy:Marco Tarini:Giuseppe Croccia:Paolo Cignoni:Roberto Scopigno	Valuable 3D graphical models, such as high-resolution digital scans of cultural heritage objects, may require protection to prevent piracy or misuse, while still allowing for interactive display and manipulation by a widespread audience. We have investigated techniques for protecting 3D graphics content, and we have developed a remote rendering system suitable for sharing archives of 3D models while protecting the 3D geometry from unauthorized extraction. The system consists of a 3D viewer client that includes low-resolution versions of the 3D models, and a rendering server that renders and returns images of high-resolution models according to client requests. The server implements a number of defenses to guard against 3D reconstruction attacks, such as monitoring and limiting request streams, and slightly perturbing and distorting the rendered images. We consider several possible types of reconstruction attacks on such a rendering server, and we examine how these attacks can be defended against without excessively compromising the interactive experience for non-malicious users.	Protected interactive 3D graphics via remote rendering	NA:NA:NA:NA:NA:NA:NA	2018
Ko Nishino:Shree K. Nayar	The combination of the cornea of an eye and a camera viewing the eye form a catadioptric (mirror + lens) imaging system with a very wide field of view. We present a detailed analysis of the characteristics of this corneal imaging system. Anatomical studies have shown that the shape of a normal cornea (without major defects) can be approximated with an ellipsoid of fixed eccentricity and size. Using this shape model, we can determine the geometric parameters of the corneal imaging system from the image. Then, an environment map of the scene with a large field of view can be computed from the image. The environment map represents the illumination of the scene with respect to the eye. This use of an eye as a natural light probe is advantageous in many relighting scenarios. For instance, it enables us to insert virtual objects into an image such that they appear consistent with the illumination of the scene. The eye is a particularly useful probe when relighting faces. It allows us to reconstruct the geometry of a face by simply waving a light source in front of the face. Finally, in the case of an already captured image, eyes could be the only direct means for obtaining illumination information. We show how illumination computed from eyes can be used to replace a face in an image with another one. We believe that the eye not only serves as a useful tool for relighting but also makes relighting possible in situations where current approaches are hard to use.	Eyes for relighting	NA:NA	2018
Sylvain Paris:Hector M. Briceño:François X. Sillion	Hair is a major feature of digital characters. Unfortunately, it has a complex geometry which challenges standard modeling tools. Some dedicated techniques exist, but creating a realistic hairstyle still takes hours. Complementary to user-driven methods, we here propose an image-based approach to capture the geometry of hair.The novelty of this work is that we draw information from the scattering properties of the hair that are normally considered a hindrance. To do so, we analyze image sequences from a fixed camera with a moving light source. We first introduce a novel method to compute the image orientation of the hairs from their anisotropic behavior. This method is proven to subsume and extend existing work while improving accuracy. This image orientation is then raised into a 3D orientation by analyzing the light reflected by the hair fibers. This part relies on minimal assumptions that have been proven correct in previous work.Finally, we show how to use several such image sequences to reconstruct the complete hair geometry of a real person. Results are shown to illustrate the fidelity of the captured geometry to the original hair. This technique paves the way for a new approach to digital hair generation.	Capture of hair geometry from multiple images	NA:NA:NA	2018
Alex Reche-Martinez:Ignacio Martin:George Drettakis	Reconstructing and rendering trees is a challenging problem due to the geometric complexity involved, and the inherent difficulties of capture. In this paper we propose a volumetric approach to capture and render trees with relatively sparse foliage. Photographs of such trees typically have single pixels containing the blended projection of numerous leaves/branches and background. We show how we estimate opacity values on a recursive grid, based on alphamattes extracted from a small number of calibrated photographs of a tree. This data structure is then used to render billboards attached to the centers of the grid cells. Each billboard is assigned a set of view-dependent textures corresponding to each input view. These textures are generated by approximating coverage masks based on opacity and depth from the camera. Rendering is performed using a view-dependent texturing algorithm. The resulting volumetric tree structure has low polygon count, permitting interactive rendering of realistic 3D trees. We illustrate the implementation of our system on several different real trees, and show that we can insert the resulting model in virtual scenes.	Volumetric reconstruction and interactive rendering of trees from photographs	NA:NA:NA	2018
Eric Saund:David Fleet:Daniel Larner:James Mahoney	This extended abstract reprises our UIST '03 paper on "Perceptually-Supported Image Editing of Text and Graphics." We introduce a novel image editing program, called ScanScribe, that emphasizes easy selection and manipulation of material found in informal, casual documents such as sketches, handwritten notes, whiteboard images, screen snapshots, and scanned documents.	Perceptually-supported image editing of text and graphics	NA:NA:NA:NA	2018
Xiang Cao:Ravin Balakrishnan	A passive wand tracked in 3D using computer vision techniques is explored as a new input mechanism for interacting with large displays. We demonstrate a variety of interaction techniques and visual widgets that exploit the affordances of the wand, resulting in an effective interface for large scale display interaction. The lack of any buttons or other electronics on the wand presents a challenge that we address by developing a set of gestures and postures to track and enable command input.	VisionWand: interaction techniques for large displays using a passive wand tracked in 3D	NA:NA	2018
James Fogarty:Scott E. Hudson	Recent work is beginning to reveal the potential of numerical optimization as an approach to generating interfaces and displays. Optimization-based approaches can often allow a mix of independent goals and constraints to be blended in ways that are difficult to describe algorithmically. While optimization-based techniques appear to offer several potential advantages, further research in this area is hampered by the lack of appropriate tools. Optimization toolkits do exist, but they typically require substantial specialized knowledge because they have been designed for traditional optimization problems.GADGET is an experimental toolkit to support optimization as an approach to interface and display generation. GADGET provides three core abstractions, initializers, iterations, and evaluations. An initializer creates an initial solution to be optimized, based on an existing algorithm or randomly. Iterations are responsible for transforming one potential solution into another, typically using methods that are at least partially random. Finally, evaluations are used for judging the different notions of goodness in a solution. Together with a evaluation standardization framework, support for generic properties integrated with an efficient lazy evaluation framework, and a library of reusable iterations and evaluations, the abstractions provided by GADGET simplify the development of optimization-based approaches to interface and display generation.	GADGET: a toolkit for optimization-based approaches to interface and display generation	NA:NA	2018
Martin Hachet:Pascal Guitton:Patrick Reuter:Florence Tyndiuk	We present the CAT (Control Action Table), a 6 degrees of freedom freestanding input device designed for interaction with Virtual Environments displayed on huge screens. Both isotonic and isometric sensing modes allow the user to easily and efficiently perform 3D interaction techniques. A 2D tablet fixed on the tabletop allows them to perform accurate 2D interaction techniques.	The CAT for efficient 2D and 3D interaction as an alternative to mouse adaptations	NA:NA:NA:NA	2018
Hajime Nagahara:Yasushi Yagi:Masahiko Yachida	Many applications have used a head-mounted display (HMD), such as in virtual and mixed realities and telepresence. However, the field of view (FOV) of commercial HMD systems is too narrow for feeling immersion. In this paper, we propose a super-wide field of view head-mounted display consisting of an ellipsoidal mirror and a hyperboloidal curved mirror. The horizontal FOV of the proposed HMD is 180 degrees and includes the peripheral vision of humans. It increases the reality and immersion for users.	Super wide viewer using catadioptrical optics	NA:NA:NA	2018
Jack Tumblin	NA	Session details: HDR and perception	NA	2018
Rafal Mantiuk:Grzegorz Krawczyk:Karol Myszkowski:Hans-Peter Seidel	Due to rapid technological progress in high dynamic range (HDR) video capture and display, the efficient storage and transmission of such data is crucial for the completeness of any HDR imaging pipeline. We propose a new approach for inter-frame encoding of HDR video, which is embedded in the well-established MPEG-4 video compression standard. The key component of our technique is luminance quantization that is optimized for the contrast threshold perception in the human visual system. The quantization scheme requires only 10--11 bits to encode 12 orders of magnitude of visible luminance range and does not lead to perceivable contouring artifacts. Besides video encoding, the proposed quantization provides perceptually-optimized luminance sampling for fast implementation of any global tone mapping operator using a lookup table. To improve the quality of synthetic video sequences, we introduce a coding scheme for discrete cosine transform (DCT) blocks with high contrast. We demonstrate the capabilities of HDR video in a player, which enables decoding, tone mapping, and applying post-processing effects in real-time. The tone mapping algorithm as well as its parameters can be changed interactively while the video is playing. We can simulate post-processing effects such as glare, night vision, and motion blur, which appear very realistic due to the usage of HDR data.	Perception-motivated high dynamic range video encoding	NA:NA:NA:NA	2018
William A. Stokes:James A. Ferwerda:Bruce Walter:Donald P. Greenberg	In this paper we introduce a new perceptual metric for efficient, high quality, global illumination rendering. The metric is based on a rendering-by-components framework in which the direct, and indirect diffuse, glossy, and specular light transport paths are separately computed and then composited to produce an image. The metric predicts the perceptual importances of the computationally expensive indirect illumination components with respect to image quality. To develop the metric we conducted a series of psychophysical experiments in which we measured and modeled the perceptual importances of the components. An important property of this new metric is that it predicts component importances from inexpensive estimates of the reflectance properties of a scene, and therefore adds negligible overhead to the rendering process. This perceptual metric should enable the development of an important new class of efficient global-illumination rendering systems that can intelligently allocate limited computational resources, to provide high quality images at interactive rates.	Perceptual illumination components: a new approach to efficient, high quality global illumination rendering	NA:NA:NA:NA	2018
Benjamin Watson:Neff Walker:Larry F. Hodges	Level of detail (LOD) is widely used to control visual feedback in interactive applications. LOD control is typically based on perception at threshold -- the conditions in which a stimulus first becomes perceivable. Yet most LOD manipulations are quite perceivable and occur well above threshold. Moreover, research shows that supra-threshold perception differs drastically from perception at threshold. In that case, should supra-threshold LOD control also differ from LOD control at threshold?In two experiments, we examine supra-threshold LOD control in the visual periphery and find that indeed, it should differ drastically from LOD control at threshold. Specifically, we find that LOD must support a task-dependent level of reliable perceptibility. Above that level, perceptibility of LOD control manipulations should be minimized, and detail contrast is a better predictor of perceptibility than detail size. Below that level, perceptibility must be maximized, and LOD should be improved as eccentricity rises or contrast drops. This directly contradicts prevailing threshold-based LOD control schemes, and strongly suggests a reexamination of LOD control for foveal display.	Supra-threshold control of peripheral LOD	NA:NA:NA	2018
Helge Seetzen:Wolfgang Heidrich:Wolfgang Stuerzlinger:Greg Ward:Lorne Whitehead:Matthew Trentacoste:Abhijeet Ghosh:Andrejs Vorozcovs	The dynamic range of many real-world environments exceeds the capabilities of current display technology by several orders of magnitude. In this paper we discuss the design of two different display systems that are capable of displaying images with a dynamic range much more similar to that encountered in the real world. The first display system is based on a combination of an LCD panel and a DLP projector, and can be built from off-the-shelf components. While this design is feasible in a lab setting, the second display system, which relies on a custom-built LED panel instead of the projector, is more suitable for usual office workspaces and commercial applications. We describe the design of both systems as well as the software issues that arise. We also discuss the advantages and disadvantages of the two designs and potential applications for both systems.	High dynamic range display systems	NA:NA:NA:NA:NA:NA:NA:NA	2018
Peter-Pike Sloan	NA	Session details: Large meshes and GPU programming	NA	2018
Frank Losasso:Hugues Hoppe	Rendering throughput has reached a level that enables a novel approach to level-of-detail (LOD) control in terrain rendering. We introduce the geometry clipmap, which caches the terrain in a set of nested regular grids centered about the viewer. The grids are stored as vertex buffers in fast video memory, and are incrementally refilled as the viewpoint moves. This simple framework provides visual continuity, uniform frame rate, complexity throttling, and graceful degradation. Moreover it allows two new exciting real-time functionalities: decompression and synthesis. Our main dataset is a 40GB height map of the United States. A compressed image pyramid reduces the size by a remarkable factor of 100, so that it fits entirely in memory. This compressed data also contributes normal maps for shading. As the viewer approaches the surface, we synthesize grid levels finer than the stored terrain using fractal noise displacement. Decompression, synthesis, and normal-map computations are incremental, thereby allowing interactive flight at 60 frames/sec.	Geometry clipmaps: terrain rendering using nested regular grids	NA:NA	2018
Ian Buck:Tim Foley:Daniel Horn:Jeremy Sugerman:Kayvon Fatahalian:Mike Houston:Pat Hanrahan	In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.	Brook for GPUs: stream computing on graphics hardware	NA:NA:NA:NA:NA:NA:NA	2018
Michael McCool:Stefanus Du Toit:Tiberiu Popa:Bryan Chan:Kevin Moule	An algebra consists of a set of objects and a set of operators that act on those objects. We treat shader programs as first-class objects and define two operators: connection and combination. Connection is functional composition: the outputs of one shader are fed into the inputs of another. Combination concatenates the input channels, output channels, and computations of two shaders. Similar operators can be used to manipulate streams and apply computational kernels expressed as shaders to streams. Connecting a shader program to a stream applies that program to all elements of the stream; combining streams concatenates the record definitions of those streams.In conjunction with an optimizing compiler, these operators can manipulate shader programs in many useful ways, including specialization, without modifying the original source code. We demonstrate these operators in Sh, a metaprogramming shading language embedded in C++.	Shader algebra	NA:NA:NA:NA:NA	2018
Paolo Cignoni:Fabio Ganovelli:Enrico Gobbetti:Fabio Marton:Federico Ponchio:Roberto Scopigno	We describe an efficient technique for out-of-core construction and accurate view-dependent visualization of very large surface models. The method uses a regular conformal hierarchy of tetrahedra to spatially partition the model. Each tetrahedral cell contains a precomputed simplified version of the original model, represented using cache coherent indexed strips for fast rendering. The representation is constructed during a fine-to-coarse simplification of the surface contained in diamonds (sets of tetrahedral cells sharing their longest edge). The construction preprocess operates out-of-core and parallelizes nicely. Appropriate boundary constraints are introduced in the simplification to ensure that all conforming selective subdivisions of the tetrahedron hierarchy lead to correctly matching surface patches. For each frame at runtime, the hierarchy is traversed coarse-to-fine to select diamonds of the appropriate resolution given the view parameters. The resulting system can interatively render high quality views of out-of-core models of hundreds of millions of triangles at over 40Hz (or 70M triangles/s) on current commodity graphics platforms.	Adaptive tetrapuzzles: efficient out-of-core construction and visualization of gigantic multiresolution polygonal models	NA:NA:NA:NA:NA:NA	2018
Hanspeter Pfister	NA	Session details: Lightfield acquisition & display	NA	2018
Kurt Akeley:Simon J. Watt:Ahna Reza Girshick:Martin S. Banks	Typical stereo displays provide incorrect focus cues because the light comes from a single surface. We describe a prototype stereo display comprising two independent fixed-viewpoint volumetric displays. Like autostereoscopic volumetric displays, fixed-viewpoint volumetric displays generate near-correct focus cues without tracking eye position, because light comes from sources at the correct focal distances. (In our prototype, from three image planes at different physical distances.) Unlike autostereoscopic volumetric displays, however, fixed-viewpoint volumetric displays retain the qualities of modern projective graphics: view-dependent lighting effects such as occlusion, specularity, and reflection are correctly depicted; modern graphics processor and 2-D display technology can be utilized; and realistic fields of view and depths of field can be implemented. While not a practical solution for general-purpose viewing, our prototype display is a proof of concept and a platform for ongoing vision research. The design, implementation, and verification of this stereo display are described, including a novel technique of filtering along visual lines using 1-D texture mapping.	A stereo display prototype with multiple focal distances	NA:NA:NA:NA	2018
Wojciech Matusik:Hanspeter Pfister	Three-dimensional TV is expected to be the next revolution in the history of television. We implemented a 3D TV prototype system with real-time acquisition, transmission, and 3D display of dynamic scenes. We developed a distributed, scalable architecture to manage the high computation and bandwidth demands. Our system consists of an array of cameras, clusters of network-connected PCs, and a multi-projector 3D display. Multiple video streams are individually encoded and sent over a broadband network to the display. The 3D display shows high-resolution (1024 × 768) stereoscopic color images for multiple viewpoints without special glasses. We implemented systems with rear-projection and front-projection lenticular screens. In this paper, we provide a detailed overview of our 3D TV system, including an examination of design choices and tradeoffs. We present the calibration and image alignment procedures that are necessary to achieve good image quality. We present qualitative results and some early user feedback. We believe this is the first real-time end-to-end 3D TV system with enough views and resolution to provide a truly immersive 3D experience.	3D TV: a scalable system for real-time acquisition, transmission, and autostereoscopic display of dynamic scenes	NA:NA	2018
Marc Levoy:Billy Chen:Vaibhav Vaish:Mark Horowitz:Ian McDowall:Mark Bolas	Confocal microscopy is a family of imaging techniques that employ focused patterned illumination and synchronized imaging to create cross-sectional views of 3D biological specimens. In this paper, we adapt confocal imaging to large-scale scenes by replacing the optical apertures used in microscopy with arrays of real or virtual video projectors and cameras. Our prototype implementation uses a video projector, a camera, and an array of mirrors. Using this implementation, we explore confocal imaging of partially occluded environments, such as foliage, and weakly scattering environments, such as murky water. We demonstrate the ability to selectively image any plane in a partially occluded environment, and to see further through murky water than is otherwise possible. By thresholding the confocal images, we extract mattes that can be used to selectively illuminate any plane in the scene.	Synthetic aperture confocal imaging	NA:NA:NA:NA:NA:NA	2018
Michael Goesele:Hendrik P. A. Lensch:Jochen Lang:Christian Fuchs:Hans-Peter Seidel	Translucent objects are characterized by diffuse light scattering beneath the object's surface. Light enters and leaves an object at possibly distinct surface locations. This paper presents the first method to acquire this transport behavior for arbitrary inhomogeneous objects. Individual surface points are illuminated in our DISCO measurement facility and the object's impulse response is recorded with a high-dynamic range video camera. The acquired data is resampled into a hierarchical model of the object's light scattering properties. Missing values are consistently interpolated resulting in measurement-based, complete and accurate representations of real translucent objects which can be rendered with various algorithms.	DISCO: acquisition of translucent objects	NA:NA:NA:NA:NA	2018
Michael Garland	NA	Session details: Mesh parameterization	NA	2018
Nathan A. Carr:John C. Hart	Surface painting is a technique that allows a user to paint a texture directly onto a surface, usually with a texture atlas: a 1:1 mapping between the surface and its texture image. Many good automatic texture atlas generation methods exist that evenly distribute texture samples across a surface based on its area and/or curvature, and some are even sensitive to the frequency spectrum of the input texture. However, during the surface painting process, the texture can change non-uniformly and unpredictably and even the atlases are static and can thus fail to reproduce sections of finely painted detail such as surface illustration.We present a new texture atlas algorithm that distributes initial texture samples evenly according to surface area and texture frequency, and, more importantly, maintains this distribution as the texture signal changes during the surface painting process. The running time is further accelerated with a novel GPU implementation of the surface painting process. The redistribution of samples is transparent to the user, resulting in a surface painting system of seemingly unlimited resolution. The atlas construction is local, making it fast enough to run interactively on models containing over 100K faces.	Painting detail	NA:NA	2018
Marco Tarini:Kai Hormann:Paolo Cignoni:Claudio Montani	Standard texture mapping of real-world meshes suffers from the presence of seams that need to be introduced in order to avoid excessive distortions and to make the topology of the mesh compatible to the one of the texture domain. In contrast, cube maps provide a mechanism that could be used for seamless texture mapping with low distortion, but only if the object roughly resembles a cube. We extend this concept to arbitrary meshes by using as texture domain the surface of a polycube whose shape is similar to that of the given mesh. Our approach leads to a seamless texture mapping method that is simple enough to be implemented in currently available graphics hardware.	PolyCube-Maps	NA:NA:NA:NA	2018
Vladislav Kraevoy:Alla Sheffer	Many geometry processing applications, such as morphing, shape blending, transfer of texture or material properties, and fitting template meshes to scan data, require a bijective mapping between two or more models. This mapping, or cross-parameterization, typically needs to preserve the shape and features of the parameterized models, mapping legs to legs, ears to ears, and so on. Most of the applications also require the models to be represented by compatible meshes, i.e. meshes with identical connectivity, based on the cross-parameterization. In this paper we introduce novel methods for shape preserving cross-parameterization and compatible remeshing. Our cross-parameterization method computes a low-distortion bijective mapping between models that satisfies user prescribed constraints. Using this mapping, the remeshing algorithm preserves the user-defined feature vertex correspondence and the shape correlation between the models. The remeshing algorithm generates output meshes with significantly fewer elements compared to previous techniques, while accurately approximating the input geometry. As demonstrated by the examples, the compatible meshes we construct are ideally suitable for morphing and other geometry processing applications.	Cross-parameterization and compatible remeshing of 3D models	NA:NA	2018
John Schreiner:Arul Asirvatham:Emil Praun:Hugues Hoppe	We consider the problem of creating a map between two arbitrary triangle meshes. Whereas previous approaches compose parametrizations over a simpler intermediate domain, we directly create and optimize a continuous map between the meshes. Map distortion is measured with a new symmetric metric, and is minimized during interleaved coarse-to-fine refinement of both meshes. By explicitly favoring low inter-surface distortion, we obtain maps that naturally align corresponding shape elements. Typically, the user need only specify a handful of feature correspondences for initial registration, and even these constraints can be removed during optimization. Our method robustly satisfies hard constraints if desired. Inter-surface mapping is shown using geometric and attribute morphs. Our general framework can also be applied to parametrize surfaces onto simplicial domains, such as coarse meshes (for semi-regular remeshing), and octahedron and toroidal domains (for geometry image remeshing). In these settings, we obtain better parametrizations than with previous specialized techniques, thanks to our fine-grain optimization.	Inter-surface mapping	NA:NA:NA:NA	2018
Emil Praun	NA	Session details: Fixing models	NA	2018
Andrei Sharf:Marc Alexa:Daniel Cohen-Or	Sampling complex, real-world geometry with range scanning devices almost always yields imperfect surface samplings. These "holes" in the surface are commonly filled with a smooth patch that conforms with the boundary. We introduce a context-based method: the characteristics of the given surface are analyzed, and the hole is iteratively filled by copying patches from valid regions of the given surface. In particular, the method needs to determine best matching patches, and then, fit imported patches by aligning them with the surrounding surface. The completion process works top down, where details refine intermediate coarser approximations. To align an imported patch with the existing surface, we apply a rigid transformation followed by an iterative closest point procedure with non-rigid transformations. The surface is essentially treated as a point set, and local implicit approximations aid in measuring the similarity between two point set patches. We demonstrate the method at several point-sampled surfaces, where the holes either result from imperfect sampling during range scanning or manual removal.	Context-based surface completion	NA:NA:NA	2018
Tao Ju	We present a robust method for repairing arbitrary polygon models. The method is guaranteed to produce a closed surface that partitions the space into disjoint internal and external volumes. Given any model represented as a polygon soup, we construct an inside/outside volume using an octree grid, and reconstruct the surface by contouring. Our novel algorithm can efficiently process large models containing millions of polygons and is capable of reproducing sharp features in the original geometry.	Robust repair of polygonal models	NA	2018
Chen Shen:James F. O'Brien:Jonathan R. Shewchuk	This paper describes a method for building interpolating or approximating implicit surfaces from polygonal data. The user can choose to generate a surface that exactly interpolates the polygons, or a surface that approximates the input by smoothing away features smaller than some user-specified size. The implicit functions are represented using a moving least-squares formulation with constraints integrated over the polygons. The paper also presents an improved method for enforcing normal constraints and an iterative procedure for ensuring that the implicit surface tightly encloses the input vertices.	Interpolating and approximating implicit surfaces from polygon soup	NA:NA:NA	2018
James L. Mohler	NA	Keynote and Awards presentations	NA	2018
Bruce Carse	NA	Keynote Speech: George Lucas: A keynote Q&A with the father of digital cinema	NA	2018
Ronen Barzel	NA	Session details: Skin & faces	NA	2018
Doug L. James:Christopher D. Twigg	We extend approaches for skinning characters to the general setting of skinning deformable mesh animations. We provide an automatic algorithm for generating progressive skinning approximations, that is particularly efficient for pseudo-articulated motions. Our contributions include the use of nonparametric mean shift clustering of high-dimensional mesh rotation sequences to automatically identify statistically relevant bones, and robust least squares methods to determine bone transformations, bone-vertex influence sets, and vertex weight values. We use a low-rank data reduction model defined in the undeformed mesh configuration to provide progressive convergence with a fixed number of bones. We show that the resulting skinned animations enable efficient hardware rendering, rest pose editing, and deformable collision detection. Finally, we present numerous examples where skins were automatically generated using a single set of parameter values.	Skinning mesh animations	NA:NA	2018
Dragomir Anguelov:Praveen Srinivasan:Daphne Koller:Sebastian Thrun:Jim Rodgers:James Davis	We introduce the SCAPE method (Shape Completion and Animation for PEople)---a data-driven method for building a human shape model that spans variation in both subject shape and pose. The method is based on a representation that incorporates both articulated and non-rigid deformations. We learn a pose deformation model that derives the non-rigid surface deformation as a function of the pose of the articulated skeleton. We also learn a separate model of variation based on body shape. Our two models can be combined to produce 3D surface models with realistic muscle deformation for different people in different poses, when neither appear in the training set. We show how the model can be used for shape completion --- generating a complete surface mesh given a limited set of markers specifying the target shape. We present applications of shape completion to partial view completion and motion capture animation. In particular, our method is capable of constructing a high-quality animated surface model of a moving person, with realistic muscle deformation, using just a single static scan and a marker motion capture sequence of the person.	SCAPE: shape completion and animation of people	NA:NA:NA:NA:NA:NA	2018
Eftychios Sifakis:Igor Neverov:Ronald Fedkiw	We built an anatomically accurate model of facial musculature, passive tissue and underlying skeletal structure using volumetric data acquired from a living male subject. The tissues are endowed with a highly nonlinear constitutive model including controllable anisotropic muscle activations based on fiber directions. Detailed models of this sort can be difficult to animate requiring complex coordinated stimulation of the underlying musculature. We propose a solution to this problem automatically determining muscle activations that track a sparse set of surface landmarks, e.g. acquired from motion capture marker data. Since the resulting animation is obtained via a three dimensional nonlinear finite element method, we obtain visually plausible and anatomically correct deformations with spatial and temporal coherence that provides robustness against outliers in the motion capture data. Moreover, the obtained muscle activations can be used in a robust simulation framework including contact and collision of the face with external objects.	Automatic determination of facial muscle activations from sparse motion capture marker data	NA:NA:NA	2018
Daniel Vlasic:Matthew Brand:Hanspeter Pfister:Jovan Popović	Face Transfer is a method for mapping videorecorded performances of one individual to facial animations of another. It extracts visemes (speech-related mouth articulations), expressions, and three-dimensional (3D) pose from monocular video or film footage. These parameters are then used to generate and drive a detailed 3D textured face mesh for a target identity, which can be seamlessly rendered back into target footage. The underlying face model automatically adjusts for how the target performs facial expressions and visemes. The performance data can be easily edited to change the visemes, expressions, pose, or even the identity of the target---the attributes are separably controllable. This supports a wide variety of video rewrite and puppetry applications.Face Transfer is based on a multilinear model of 3D face meshes that separably parameterizes the space of geometric variations due to different attributes (e.g., identity, expression, and viseme). Separability means that each of these attributes can be independently varied. A multilinear model can be estimated from a Cartesian product of examples (identities × expressions × visemes) with techniques from statistical analysis, but only after careful preprocessing of the geometric data set to secure one-to-one correspondence, to minimize cross-coupling artifacts, and to fill in any missing examples. Face Transfer offers new solutions to these problems and links the estimated model with a face-tracking algorithm to extract pose, expression, and viseme parameters.	Face transfer with multilinear models	NA:NA:NA:NA	2018
Hanspeter Pfister	NA	Session details: Hardware rendering	NA	2018
Sven Woop:Jörg Schmittler:Philipp Slusallek	Recursive ray tracing is a simple yet powerful and general approach for accurately computing global light transport and rendering high quality images. While recent algorithmic improvements and optimized parallel software implementations have increased ray tracing performance to realtime levels, no compact and programmable hardware solution has been available yet.This paper describes the architecture and a prototype implementation of a single chip, fully programmable Ray Processing Unit (RPU). It combines the flexibility of general purpose CPUs with the efficiency of current GPUs for data parallel computations. This design allows for realtime ray tracing of dynamic scenes with programmable material, geometry, and illumination shaders.Although, running at only 66 MHz the prototype FPGA implementation already renders images at up to 20 frames per second, which in many cases beats the performance of highly optimized software running on multi-GHz desktop CPUs. The performance and efficiency of the proposed architecture is analyzed using a variety of benchmark scenes.	RPU: a programmable ray processing unit for realtime ray tracing	NA:NA:NA	2018
Fabio Pellacini	Programmable shading is a fundamental technique for specifying appearance in 3d environments. While shading architectures provides fast execution of shaders, shader evaluation is today a major cost in the rendering process. In the same manner in which geometric simplification lets us deal with large models, it would be beneficial to have an automatic technique that trades off shader quality for speed.This paper presents such a technique by introducing a framework for the automatic simplification of complex procedural shaders, where a sequence of increasingly simplified shaders is generated starting from an original shader together with ranges for all of its input parameters. Our approach works by applying simplification rules to the code of a shader to generate a series of candidates, whose differences from the original one are measured and used to select the candidate with the smallest error. This procedure is repeated until the last shader is a constant. While this automatic procedure generates high quality simplified shaders, the artist might want to emphasize particular aspects of a shader during simplification. Our framework supports this desire by allowing the user to specify additional rules to be considered during simplification. The term user-configurable simplification comes from this feature of our system.We implemented our algorithm to support the simplification of fragment shaders running on graphics hardware. Our results show that automatic simplification of complex procedural shaders is possible with high quality.	User-configurable automatic shader simplification	NA	2018
Nathaniel Duca:Krzysztof Niski:Jonathan Bilodeau:Matthew Bolitho:Yuan Chen:Jonathan Cohen	We present a new, unified approach to debugging graphics software. We propose a representation of all graphics state over the course of program execution as a relational database, and produce a query-based framework for extracting, manipulating, and visualizing data from all stages of the graphics pipeline. Using an SQL-based query language, the programmer can establish functional relationships among all the data, linking OpenGL state to primitives to vertices to fragments to pixels. Based on the Chromium library, our approach requires no modification to or recompilation of the program to be debugged, and forms a superset of many existing techniques for debugging graphics software.	A relational debugging engine for the graphics pipeline	NA:NA:NA:NA:NA:NA	2018
Fabio Pellacini:Kiril Vidimče:Aaron Lefohn:Alex Mohr:Mark Leone:John Warren	In computer cinematography, the process of lighting design involves placing and configuring lights to define the visual appearance of environments and to enhance story elements. This process is labor intensive and time consuming, primarily because lighting artists receive poor feedback from existing tools: interactive previews have very poor quality, while final-quality images often take hours to render.This paper presents an interactive cinematic lighting system used in the production of computer-animated feature films containing environments of very high complexity, in which surface and light appearances are described using procedural RenderMan shaders. Our system provides lighting artists with high-quality previews at interactive framerates with only small approximations compared to the final rendered images. This is accomplished by combining numerical estimation of surface response, image-space caching, deferred shading, and the computational power of modern graphics hardware.Our system has been successfully used in the production of two feature-length animated films, dramatically accelerating lighting tasks. In our experience interactivity fundamentally changes an artist's workflow, improving both productivity and artistic expressiveness.	Lpics: a hybrid hardware-accelerated relighting engine for computer cinematography	NA:NA:NA:NA:NA:NA	2018
Ioana Boier-Martin	NA	Session details: Mesh manipulation	NA	2018
Matthias Müller:Bruno Heidelberger:Matthias Teschner:Markus Gross	We present a new approach for simulating deformable objects. The underlying model is geometrically motivated. It handles pointbased objects and does not need connectivity information. The approach does not require any pre-processing, is simple to compute, and provides unconditionally stable dynamic simulations.The main idea of our deformable model is to replace energies by geometric constraints and forces by distances of current positions to goal positions. These goal positions are determined via a generalized shape matching of an undeformed rest state with the current deformed state of the point cloud. Since points are always drawn towards well-defined locations, the overshooting problem of explicit integration schemes is eliminated. The versatility of the approach in terms of object representations that can be handled, the efficiency in terms of memory and computational complexity, and the unconditional stability of the dynamic simulation make the approach particularly interesting for games.	Meshless deformations based on shape matching	NA:NA:NA:NA	2018
Yaron Lipman:Olga Sorkine:David Levin:Daniel Cohen-Or	We introduce a rigid motion invariant mesh representation based on discrete forms defined on the mesh. The reconstruction of mesh geometry from this representation requires solving two sparse linear systems that arise from the discrete forms: the first system defines the relationship between local frames on the mesh, and the second encodes the position of the vertices via the local frames. The reconstructed geometry is unique up to a rigid transformation of the mesh. We define surface editing operations by placing user-defined constraints on the local frames and the vertex positions. These constraints are incorporated in the two linear reconstruction systems, and their solution produces a deformed surface geometry that preserves the local differential properties in the least-squares sense. Linear combination of shapes expressed with our representation enables linear shape interpolation that correctly handles rotations. We demonstrate the effectiveness of the new representation with various detail-preserving editing operators and shape morphing.	Linear rotation-invariant coordinates for meshes	NA:NA:NA:NA	2018
Robert W. Sumner:Matthias Zwicker:Craig Gotsman:Jovan Popović	The ability to position a small subset of mesh vertices and produce a meaningful overall deformation of the entire mesh is a fundamental task in mesh editing and animation. However, the class of meaningful deformations varies from mesh to mesh and depends on mesh kinematics, which prescribes valid mesh configurations, and a selection mechanism for choosing among them. Drawing an analogy to the traditional use of skeleton-based inverse kinematics for posing skeletons. we define mesh-based inverse kinematics as the problem of finding meaningful mesh deformations that meet specified vertex constraints.Our solution relies on example meshes to indicate the class of meaningful deformations. Each example is represented with a feature vector of deformation gradients that capture the affine transformations which individual triangles undergo relative to a reference pose. To pose a mesh, our algorithm efficiently searches among all meshes with specified vertex positions to find the one that is closest to some pose in a nonlinear span of the example feature vectors. Since the search is not restricted to the span of example shapes, this produces compelling deformations even when the constraints require poses that are different from those observed in the examples. Furthermore, because the span is formed by a nonlinear blend of the example feature vectors, the blending component of our system may also be used independently to pose meshes by specifying blending weights or to compute multi-way morph sequences.	Mesh-based inverse kinematics	NA:NA:NA:NA	2018
Kun Zhou:Jin Huang:John Snyder:Xinguo Liu:Hujun Bao:Baining Guo:Heung-Yeung Shum	We present a novel technique for large deformations on 3D meshes using the volumetric graph Laplacian. We first construct a graph representing the volume inside the input mesh. The graph need not form a solid meshing of the input mesh's interior; its edges simply connect nearby points in the volume. This graph's Laplacian encodes volumetric details as the difference between each point in the graph and the average of its neighbors. Preserving these volumetric details during deformation imposes a volumetric constraint that prevents unnatural changes in volume. We also include in the graph points a short distance outside the mesh to avoid local self-intersections. Volumetric detail preservation is represented by a quadric energy function. Minimizing it preserves details in a least-squares sense, distributing error uniformly over the whole deformed mesh. It can also be combined with conventional constraints involving surface positions, details or smoothness, and efficiently minimized by solving a sparse linear system.We apply this technique in a 2D curve-based deformation system allowing novice users to create pleasing deformations with little effort. A novel application of this system is to apply nonrigid and exaggerated deformations of 2D cartoon characters to 3D meshes. We demonstrate our system's potential with several examples.	Large mesh deformation using the volumetric graph Laplacian	NA:NA:NA:NA:NA:NA:NA	2018
François X. Sillion	NA	Session details: Illustration and image based modeling	NA	2018
Nelson S.-H. Chu:Chiew-Lan Tai	This paper presents a physically-based method for simulating ink dispersion in absorbent paper for art creation purposes. We devise a novel fluid flow model based on the lattice Boltzmann equation suitable for simulating percolation in disordered media, like paper, in real time. Our model combines the simulations of spontaneous shape evolution and porous media flow under a unified framework. We also couple our physics simulation with simple implicit modeling and image-based methods to render high quality output. We demonstrate the effectiveness of our techniques in a digital paint system and achieve various realistic effects of ink dispersion, including complex flow patterns observed in real artwork, and other special effects.	MoXi: real-time ink dispersion in absorbent paper	NA:NA	2018
Michael Burns:Janek Klawe:Szymon Rusinkiewicz:Adam Finkelstein:Doug DeCarlo	Renderings of volumetric data have become an important data analysis tool for applications ranging from medicine to scientific simulation. We propose a volumetric drawing system that directly extracts sparse linear features, such as silhouettes and suggestive contours, using a temporally coherent seed-and-traverse framework. In contrast to previous methods based on isosurfaces or nonrefractive transparency, producing these drawings requires examining an asymptotically smaller subset of the data, leading to efficiency on large data sets. In addition, the resulting imagery is often more comprehensible than standard rendering styles, since it focuses attention on important features in the data. We test our algorithms on datasets up to 5123, demonstrating interactive extraction and rendering of line drawings in a variety of drawing styles.	Line drawings from volume data	NA:NA:NA:NA:NA	2018
Ce Liu:Antonio Torralba:William T. Freeman:Frédo Durand:Edward H. Adelson	We present motion magnification, a technique that acts like a microscope for visual motion. It can amplify subtle motions in a video sequence, allowing for visualization of deformations that would otherwise be invisible. To achieve motion magnification, we need to accurately measure visual motions, and group the pixels to be modified. After an initial image registration step, we measure motion by a robust analysis of feature point trajectories, and segment pixels based on similarity of position, color, and motion. A novel measure of motion similarity groups even very small motions according to correlation over time, which often relates to physical cause. An outlier mask marks observations not explained by our layered motion model, and those pixels are simply reproduced on the output from the original registered observations.The motion of any selected layer may be magnified by a user-specified amount; texture synthesis fills-in unseen "holes" revealed by the amplified motions. The resulting motion-magnified images can reveal or emphasize small motions in the original sequence, as we demonstrate with deformations in load-bearing structures, subtle motions or balancing corrections of people, and "rigid" structures bending under hand pressure.	Motion magnification	NA:NA:NA:NA:NA	2018
Hongcheng Wang:Qing Wu:Lin Shi:Yizhou Yu:Narendra Ahuja	Tensor approximation is necessary to obtain compact multilinear models for multi-dimensional visual datasets. Traditionally, each multi-dimensional data item is represented as a vector. Such a scheme flattens the data and partially destroys the internal structures established throughout the multiple dimensions. In this paper, we retain the original dimensionality of the data items to more effectively exploit existing spatial redundancy and allow more efficient computation. Since the size of visual datasets can easily exceed the memory capacity of a single machine, we also present an out-of-core algorithm for higher-order tensor approximation. The basic idea is to partition a tensor into smaller blocks and perform tensor-related operations blockwise. We have successfully applied our techniques to three graphics-related data-driven models, including 6D bidirectional texture functions, 7D dynamic BTFs and 4D volume simulation sequences. Experimental results indicate that our techniques can not only process out-of-core data, but also achieve higher compression ratios and quality than previous methods.	Out-of-core tensor approximation of multi-dimensional matrices of visual data	NA:NA:NA:NA:NA	2018
Alla Sheffer	NA	Session details: Meshes I	NA	2018
Diego Nehab:Szymon Rusinkiewicz:James Davis:Ravi Ramamoorthi	Range scanning, manual 3D editing, and other modeling approaches can provide information about the geometry of surfaces in the form of either 3D positions (e.g., triangle meshes or range images) or orientations (normal maps or bump maps). We present an algorithm that combines these two kinds of estimates to produce a new surface that approximates both. Our formulation is linear, allowing it to operate efficiently on complex meshes commonly used in graphics. It also treats high-and low-frequency components separately, allowing it to optimally combine outputs from data sources such as stereo triangulation and photometric stereo, which have different error-vs.-frequency characteristics. We demonstrate the ability of our technique to both recover high-frequency details and avoid low-frequency bias, producing surfaces that are more widely applicable than position or orientation data alone.	Efficiently combining positions and normals for precise 3D geometry	NA:NA:NA:NA	2018
Shachar Fleishman:Daniel Cohen-Or:Cláudio T. Silva	We introduce a robust moving least-squares technique for reconstructing a piecewise smooth surface from a potentially noisy point cloud. We use techniques from robust statistics to guide the creation of the neighborhoods used by the moving least squares (MLS) computation. This leads to a conceptually simple approach that provides a unified framework for not only dealing with noise, but also for enabling the modeling of surfaces with sharp features.Our technique is based on a new robust statistics method for outlier detection: the forward-search paradigm. Using this powerful technique, we locally classify regions of a point-set to multiple outlier-free smooth regions. This classification allows us to project points on a locally smooth region rather than a surface that is smooth everywhere, thus defining a piecewise smooth surface and increasing the numerical stability of the projection operator. Furthermore, by treating the points across the discontinuities as outliers, we are able to define sharp features. One of the nice features of our approach is that it automatically disregards outliers during the surface-fitting phase.	Robust moving least-squares fitting with sharp features	NA:NA:NA	2018
Vitaly Surazhsky:Tatiana Surazhsky:Danil Kirsanov:Steven J. Gortler:Hugues Hoppe	The computation of geodesic paths and distances on triangle meshes is a common operation in many computer graphics applications. We present several practical algorithms for computing such geodesics from a source point to one or all other points efficiently. First, we describe an implementation of the exact "single source, all destination" algorithm presented by Mitchell, Mount, and Papadimitriou (MMP). We show that the algorithm runs much faster in practice than suggested by worst case analysis. Next, we extend the algorithm with a merging operation to obtain computationally efficient and accurate approximations with bounded error. Finally, to compute the shortest path between two given points, we use a lower-bound property of our approximate geodesic algorithm to efficiently prune the frontier of the MMP algorithm. thereby obtaining an exact solution even more quickly.	Fast exact and approximate geodesics on meshes	NA:NA:NA:NA:NA	2018
Tao Ju:Scott Schaefer:Joe Warren	Constructing a function that interpolates a set of values defined at vertices of a mesh is a fundamental operation in computer graphics. Such an interpolant has many uses in applications such as shading, parameterization and deformation. For closed polygons, mean value coordinates have been proven to be an excellent method for constructing such an interpolant. In this paper, we generalize mean value coordinates from closed 2D polygons to closed triangular meshes. Given such a mesh P, we show that these coordinates are continuous everywhere and smooth on the interior of P. The coordinates are linear on the triangles of P and can reproduce linear functions on the interior of P. To illustrate their usefulness, we conclude by considering several interesting applications including constructing volumetric textures and surface deformation.	Mean value coordinates for closed triangular meshes	NA:NA:NA	2018
Wojciech Matusik	NA	Session details: Video & image matting	NA	2018
Morgan McGuire:Wojciech Matusik:Hanspeter Pfister:John F. Hughes:Frédo Durand	Video matting is the process of pulling a high-quality alpha matte and foreground from a video sequence. Current techniques require either a known background (e.g., a blue screen) or extensive user interaction (e.g., to specify known foreground and background elements). The matting problem is generally under-constrained, since not enough information has been collected at capture time. We propose a novel, fully autonomous method for pulling a matte using multiple synchronized video streams that share a point of view but differ in their plane of focus. The solution is obtained by directly minimizing the error in filter-based image formation equations, which are over-constrained by our rich data stream. Our system solves the fully dynamic video matting problem without user assistance: both the foreground and background may be high frequency and have dynamic content, the foreground may resemble the background, and the scene is lit by natural (as opposed to polarized or collimated) illumination.	Defocus video matting	NA:NA:NA:NA:NA	2018
Derek Hoiem:Alexei A. Efros:Martial Hebert	This paper presents a fully automatic method for creating a 3D model from a single photograph. The model is made up of several texture-mapped planar billboards and has the complexity of a typical children's pop-up book illustration. Our main insight is that instead of attempting to recover precise geometry, we statistically model geometric classes defined by their orientations in the scene. Our algorithm labels regions of the input image into coarse categories: "ground", "sky", and "vertical". These labels are then used to "cut and fold" the image into a pop-up model using a set of simple assumptions. Because of the inherent ambiguity of the problem and the statistical nature of the approach, the algorithm is not expected to work on every image. However. it performs surprisingly well for a wide range of scenes taken from a typical person's photo album.	Automatic photo pop-up	NA:NA:NA	2018
Jue Wang:Pravin Bhat:R. Alex Colburn:Maneesh Agrawala:Michael F. Cohen	We present an interactive system for efficiently extracting foreground objects from a video. We extend previous min-cut based image segmentation techniques to the domain of video with four new contributions. We provide a novel painting-based user interface that allows users to easily indicate the foreground object across space and time. We introduce a hierarchical mean-shift preprocess in order to minimize the number of nodes that min-cut must operate on. Within the min-cut we also define new local cost functions to augment the global costs defined in earlier work. Finally, we extend 2D alpha matting methods designed for images to work with 3D video volumes. We demonstrate that our matting approach preserves smoothness across both space and time. Our interactive video cutout system allows users to quickly extract foreground objects from video sequences for use in a variety of applications including compositing onto new backgrounds and NPR cartoon style rendering.	Interactive video cutout	NA:NA:NA:NA:NA	2018
Yin Li:Jian Sun:Heung-Yeung Shum	In this paper, we present a system for cutting a moving object out from a video clip. The cutout object sequence can be pasted onto another video or a background image. To achieve this, we first apply a new 3D graph cut based segmentation approach on the spatial-temporal video volume. Our algorithm partitions watershed presegmentation regions into foreground and background while preserving temporal coherence. Then, the initial segmentation result is refined locally. Given two frames in the video sequence, we specify two respective windows of interest which are then tracked using a bi-directional feature tracking algorithm. For each frame in between these two given frames, the segmentation in each tracked window is refined using a 2D graph cut that utilizes a local color model. Moreover, we provide brush tools for the user to control the object boundary precisely wherever needed. Based on the accurate binary segmentation result, we apply coherent matting to extract the alpha mattes and foreground colors of the object.	Video object cut and paste	NA:NA:NA	2018
Hugues Hoppe	NA	Session details: Meshes II	NA	2018
Gabriel Peyré:Stéphane Mallat	This paper describes the construction of second generation bandelet bases and their application to 3D geometry compression. This new coding scheme is orthogonal and the corresponding basis functions are regular. In our method, surfaces are decomposed in a bandelet basis with a fast bandeletization algorithm that removes the geometric redundancy of orthogonal wavelet coefficients. The resulting transform coding scheme has an error decay that is asymptotically optimal for geometrically regular surfaces. We then use these bandelet bases to perform geometry image and normal map compression. Numerical tests show that for complex surfaces bandelets bring an improvement of 1.5dB to 2dB over state of the art compression schemes.	Surface compression with geometric bandelets	NA:NA	2018
Jingliang Peng:C.-C. Jay Kuo	A new progressive lossless 3D triangular mesh encoder is proposed in this work, which can encode any 3D triangular mesh with an arbitrary topological structure. Given a mesh, the quantized 3D vertices are first partitioned into an octree (OT) structure, which is then traversed from the root and gradually to the leaves. During the traversal, each 3D cell in the tree front is subdivided into eight childcells. For each cell subdivision, both local geometry and connectivity changes are encoded, where the connectivity coding is guided by the geometry coding. Furthermore, prioritized cell subdivision is performed in the tree front to provide better rate-distortion (RD) performance. Experiments show that the proposed mesh coder outperforms the kd-tree algorithm in both geometry and connectivity coding efficiency. For the geometry coding part, the range of improvement is typically around 10%~20%, but may go up to 50%~60% for meshes with highly regular geometry data and/or tight clustering of vertices.	Geometry-guided progressive lossless 3D mesh coding with octree (OT) decomposition	NA:NA	2018
Pierre Alliez:David Cohen-Steiner:Mariette Yvinec:Mathieu Desbrun	In this paper, a novel Delaunay-based variational approach to isotropic tetrahedral meshing is presented. To achieve both robustness and efficiency, we minimize a simple mesh-dependent energy through global updates of both vertex positions and connectivity. As this energy is known to be the ∠1 distance between an isotropic quadratic function and its linear interpolation on the mesh, our minimization procedure generates well-shaped tetrahedra. Mesh design is controlled through a gradation smoothness parameter and selection of the desired number of vertices. We provide the foundations of our approach by explaining both the underlying variational principle and its geometric interpretation. We demonstrate the quality of the resulting meshes through a series of examples.	Variational tetrahedral meshing	NA:NA:NA:NA	2018
Serban D. Porumbescu:Brian Budge:Louis Feng:Kenneth I. Joy	A shell map is a bijective mapping between shell space and texture space that can be used to generate small-scale features on surfaces using a variety of modeling techniques. The method is based upon the generation of an offset surface and the construction of a tetrahedral mesh that fills the space between the base surface and its offset. By identifying a corresponding tetrahedral mesh in texture space, the shell map can be implemented through a straightforward barycentric-coordinate map between corresponding tetrahedra. The generality of shell maps allows texture space to contain geometric objects, procedural volume textures, scalar fields, or other shell-mapped objects.	Shell maps	NA:NA:NA:NA	2018
Maneesh Agrawala	NA	Session details: Perception	NA	2018
Amy A. Gooch:Sven C. Olsen:Jack Tumblin:Bruce Gooch	Visually important image features often disappear when color images are converted to grayscale. The algorithm introduced here reduces such losses by attempting to preserve the salient features of the color image. The Color2Gray algorithm is a 3-step process: 1) convert RGB inputs to a perceptually uniform CIE L*a*b* color space, 2) use chrominance and luminance differences to create grayscale target differences between nearby image pixels, and 3) solve an optimization problem designed to selectively modulate the grayscale representation as a function of the chroma variation of the source image. The Color2Gray results offer viewers salient information missing from previous grayscale image creation methods.	Color2Gray: salience-preserving color removal	NA:NA:NA:NA	2018
Patrick Ledda:Alan Chalmers:Tom Troscianko:Helge Seetzen	Tone mapping operators are designed to reproduce visibility and the overall impression of brightness, contrast and color of the real world onto limited dynamic range displays and printers. Although many tone mapping operators have been published in recent years, no thorough psychophysical experiments have yet been undertaken to compare such operators against the real scenes they are purporting to depict. In this paper, we present the results of a series of psychophysical experiments to validate six frequently used tone mapping operators against linearly mapped High Dynamic Range (HDR) scenes displayed on a novel HDR device. Individual operators address the tone mapping issue using a variety of approaches and the goals of these techniques are often quite different from one another. Therefore, the purpose of this investigation was not simply to determine which is the "best" algorithm, but more generally to propose an experimental methodology to validate such operators and to determine the participants' impressions of the images produced compared to what is visible on a high contrast ratio display.	Evaluation of tone mapping operators using a High Dynamic Range display	NA:NA:NA:NA	2018
Michael F. Deering	A photon accurate model of individual cones in the human eye perceiving images on digital display devices is presented. Playback of streams of pixel video data is modeled as individual photon emission events from within the physical substructure of each display pixel. The thus generated electromagnetic wavefronts are refracted through a four surface model of the human cornea and lens, and diffracted at the pupil. The position, size, shape, and orientation of each of the five million photoreceptor cones in the retina are individually modeled by a new synthetic retina model. Photon absorption events map the collapsing wavefront to photon detection events in a particular cone, resulting in images of the photon counts in the retinal cone array. The custom rendering systems used to generate sequences of these images takes a number of optical and physical properties of the image formation into account, including wavelength dependent absorption in the tissues of the eye, and the motion blur caused by slight movement of the eye during a frame of viewing. The creation of this new model is part of a larger framework for understanding how changes to computer graphics rendering algorithms and changes in image display devices are related to artifacts visible to human viewers.	A photon accurate model of the human eye	NA	2018
Chang Ha Lee:Amitabh Varshney:David W. Jacobs	Research over the last decade has built a solid mathematical foundation for representation and analysis of 3D meshes in graphics and geometric modeling. Much of this work however does not explicitly incorporate models of low-level human visual attention. In this paper we introduce the idea of mesh saliency as a measure of regional importance for graphics meshes. Our notion of saliency is inspired by low-level human visual system cues. We define mesh saliency in a scale-dependent manner using a center-surround operator on Gaussian-weighted mean curvatures. We observe that such a definition of mesh saliency is able to capture what most would classify as visually interesting regions on a mesh. The human-perception-inspired importance measure computed by our mesh saliency operator results in more visually pleasing results in processing and viewing of 3D meshes. compared to using a purely geometric measure of shape. such as curvature. We discuss how mesh saliency can be incorporated in graphics applications such as mesh simplification and viewpoint selection and present examples that show visually appealing results from using mesh saliency.	Mesh saliency	NA:NA:NA	2018
Nancy Pollard	NA	Session details: Motion capture data: interaction and selection	NA	2018
Jackie Assa:Yaron Caspi:Daniel Cohen-Or	Illustrating motion in still imagery for the purpose of summary, abstraction and motion description is important for a diverse spectrum of fields, ranging from arts to sciences. In this paper, we introduce a method that produces an action synopsis for presenting motion in still images. The method carefully selects key poses based on an analysis of a skeletal animation sequence, to facilitate expressing complex motions in a single image or a small number of concise views. Our approach is to embed the high-dimensional motion curve in a low-dimensional Euclidean space, where the main characteristics of the skeletal action are kept. The lower complexity of the embedded motion curve allows a simple iterative method which analyzes the curve and locates significant points, associated with the key poses of the original motion. We present methods for illustrating the selected poses in an image as a means to convey the action. We applied our methods to a variety of motions of human actions given either as 3D animation sequences or as video clips, and generated images that depict their synopsis.	Action synopsis: pose selection and illustration	NA:NA:NA	2018
Meinard Müller:Tido Röder:Michael Clausen	The reuse of human motion capture data to create new, realistic motions by applying morphing and blending techniques has become an important issue in computer animation. This requires the identification and extraction of logically related motions scattered within some data set. Such content-based retrieval of motion capture data, which is the topic of this paper, constitutes a difficult and time-consuming problem due to significant spatio-temporal variations between logically related motions. In our approach, we introduce various kinds of qualitative features describing geometric relations between specified body points of a pose and show how these features induce a time segmentation of motion capture data streams. By incorporating spatio-temporal invariance into the geometric features and adaptive segments, we are able to adopt efficient indexing methods allowing for flexible and efficient content-based retrieval and browsing in huge motion capture databases. Furthermore, we obtain an efficient preprocessing method substantially accelerating the cost-intensive classical dynamic time warping techniques for the time alignment of logically similar motion data streams. We present experimental results on a test data set of more than one million frames, corresponding to 180 minutes of motion. The linearity of our indexing algorithms guarantees the scalability of our results to much larger data sets.	Efficient content-based retrieval of motion capture data	NA:NA:NA	2018
Jinxiang Chai:Jessica K. Hodgins	This paper introduces an approach to performance animation that employs video cameras and a small set of retro-reflective markers to create a low-cost, easy-to-use system that might someday be practical for home use. The low-dimensional control signals from the user's performance are supplemented by a database of pre-recorded human motion. At run time, the system automatically learns a series of local models from a set of motion capture examples that are a close match to the marker locations captured by the cameras. These local models are then used to reconstruct the motion of the user as a full-body animation. We demonstrate the power of this approach with real-time control of six different behaviors using two video cameras and a small set of retro-reflective markers. We compare the resulting animation to animation from commercial motion capture equipment with a full set of markers.	Performance animation from low-dimensional control signals	NA:NA	2018
Victor Brian Zordan:Anna Majkowska:Bill Chiu:Matthew Fast	Human motion capture embeds rich detail and style which is difficult to generate with competing animation synthesis technologies. However, such recorded data requires principled means for creating responses in unpredicted situations, for example reactions immediately following impact. This paper introduces a novel technique for incorporating unexpected impacts into a motion capture-driven animation system through the combination of a physical simulation which responds to contact forces and a specialized search routine which determines the best plausible re-entry into motion library playback following the impact. Using an actuated dynamic model, our system generates a physics-based response while connecting motion capture segments. Our method allows characters to respond to unexpected changes in the environment based on the specific dynamic effects of a given contact while also taking advantage of the realistic movement made available through motion capture. We show the results of our system under various conditions and with varying responses using martial arts motion capture as a testbed.	Dynamic response for motion capture animation	NA:NA:NA:NA	2018
Greg Turk	NA	Session details: Plants	NA	2018
Adam Runions:Martin Fuhrer:Brendan Lane:Pavol Federl:Anne-Gaëlle Rolland-Lagan:Przemyslaw Prusinkiewicz	We introduce a class of biologically-motivated algorithms for generating leaf venation patterns. These algorithms simulate the interplay between three processes: (1) development of veins towards hormone (auxin) sources embedded in the leaf blade; (2) modification of the hormone source distribution by the proximity of veins; and (3) modification of both the vein pattern and source distribution by leaf growth. These processes are formulated in terms of iterative geometric operations on sets of points that represent vein nodes and auxin sources. In addition, a vein connection graph is maintained to determine vein widths. The effective implementation of the algorithms relies on the use of space subdivision (Voronoi diagrams) and time coherence between iteration steps. Depending on the specification details and parameters used, the algorithms can simulate many types of venation patterns, both open (tree-like) and closed (with loops). Applications of the presented algorithms include texture and detailed structure generation for image synthesis purposes, and modeling of morphogenetic processes in support of biological research.	Modeling and visualization of leaf venation patterns	NA:NA:NA:NA:NA:NA	2018
Lifeng Wang:Wenle Wang:Julie Dorsey:Xu Yang:Baining Guo:Heung-Yeung Shum	This paper presents a framework for the real-time rendering of plant leaves with global illumination effects. Realistic rendering of leaves requires a sophisticated appearance model and accurate lighting computation. For leaf appearance we introduce a parametric model that describes leaves in terms of spatially-variant BRDFs and BTDFs. These BRDFs and BTDFs, incorporating analysis of subsurface scattering inside leaf tissues and rough surface scattering on leaf surfaces, can be measured from real leaves. More importantly, this description is compact and can be loaded into graphics hardware for fast run-time shading calculations, which are essential for achieving high frame rates. For lighting computation, we present an algorithm that extends the Precomputed Radiance Transfer (PRT) approach to all-frequency lighting for leaves. In particular, we handle the combined illumination effects due to low-frequency environment light and high-frequency sunlight. This is done by decomposing the local incident radiance of sunlight into direct and indirect components. The direct component, which contains most of the high frequencies, is not pre-computed with spherical harmonics as in PRT; instead it is evaluated on-the-fly using pre-computed light-visibility convolution data. We demonstrate our framework by the rendering of a variety of leaves and assemblies thereof.	Real-time rendering of plant leaves	NA:NA:NA:NA:NA:NA	2018
Takashi Ijiri:Shigeru Owada:Makoto Okabe:Takeo Igarashi	We present a system for modeling flowers in three dimensions quickly and easily while preserving correct botanical structures. We use floral diagrams and inflorescences, which were developed by botanists to concisely describe structural information of flowers. Floral diagrams represent the layout of floral components on a single flower, while inflorescences are arrangements of multiple flowers. Based on these notions, we created a simple user interface that is specially tailored to flower editing, while retaining a maximum variety of generable models. We also provide sketching interfaces to define the geometries of floral components. Separation of structural editing and editing of geometry makes the authoring process more flexible and efficient. We found that even novice users could easily design various flower models using our technique. Our system is an example of application-customized sketching, illustrating the potential power of a sketching interface that is carefully designed for a specific application.	Floral diagrams and inflorescences: interactive flower modeling using botanical structural constraints	NA:NA:NA:NA	2018
Stephen R. Marschner:Stephen H. Westin:Adam Arbree:Jonathan T. Moon	Wood coated with transparent finish has a beautiful and distinctive appearance that is familiar to everyone. Woods with unusual grain patterns. such as tiger, burl, and birdseye figures, have a strikingly unusual directional reflectance that is prized for decorative applications. With new, high resolution measurements of spatially varying BRDFs. we show that this distinctive appearance is due to light scattering that does not conform to the usual notion of anisotropic surface reflection. The behavior can be explained by scattering from the matrix of wood fibers below the surface, resulting in a subsurface highlight that occurs on a cone with an out-of-plane axis. We propose a new shading model component to handle reflection from subsurface fibers, which is combined with the standard diffuse and specular components to make a complete shading model. Rendered results from fits of our model to the measurement data demonstrate that this new model captures the distinctive appearance of wood.	Measuring and modeling the appearance of finished wood	NA:NA:NA:NA	2018
Szymon Rusinkiewicz	NA	Session details: Capturing reality I	NA	2018
Ren Ng	This paper contributes to the theory of photograph formation from light fields. The main result is a theorem that, in the Fourier domain, a photograph formed by a full lens aperture is a 2D slice in the 4D light field. Photographs focused at different depths correspond to slices at different trajectories in the 4D space. The paper demonstrates the utility of this theorem in two different ways. First, the theorem is used to analyze the performance of digital refocusing, where one computes photographs focused at different depths from a single light field. The analysis shows in closed form that the sharpness of refocused photographs increases linearly with directional resolution. Second, the theorem yields a Fourier-domain algorithm for digital refocusing, where we extract the appropriate 2D slice of the light field's Fourier transform, and perform an inverse 2D Fourier transform. This method is faster than previous approaches.	Fourier slice photography	NA	2018
Pradeep Sen:Billy Chen:Gaurav Garg:Stephen R. Marschner:Mark Horowitz:Marc Levoy:Hendrik P. A. Lensch	We present a novel photographic technique called dual photography, which exploits Helmholtz reciprocity to interchange the lights and cameras in a scene. With a video projector providing structured illumination, reciprocity permits us to generate pictures from the viewpoint of the projector, even though no camera was present at that location. The technique is completely image-based, requiring no knowledge of scene geometry or surface properties, and by its nature automatically includes all transport paths, including shadows, inter-reflections and caustics. In its simplest form, the technique can be used to take photographs without a camera; we demonstrate this by capturing a photograph using a projector and a photo-resistor. If the photo-resistor is replaced by a camera, we can produce a 4D dataset that allows for relighting with 2D incident illumination. Using an array of cameras we can produce a 6D slice of the 8D reflectance field that allows for relighting with arbitrary light fields. Since an array of cameras can operate in parallel without interference, whereas an array of light sources cannot, dual photography is fundamentally a more efficient way to capture such a 6D dataset than a system based on multiple projectors and one camera. As an example, we show how dual photography can be used to capture and relight scenes.	Dual photography	NA:NA:NA:NA:NA:NA:NA	2018
Andreas Wenger:Andrew Gardner:Chris Tchou:Jonas Unger:Tim Hawkins:Paul Debevec	We present a technique for capturing an actor's live-action performance in such a way that the lighting and reflectance of the actor can be designed and modified in postproduction. Our approach is to illuminate the subject with a sequence of time-multiplexed basis lighting conditions, and to record these conditions with a high-speed video camera so that many conditions are recorded in the span of the desired output frame interval. We investigate several lighting bases for representing the sphere of incident illumination using a set of discrete LED light sources, and we estimate and compensate for subject motion using optical flow and image warping based on a set of tracking frames inserted into the lighting basis. To composite the illuminated performance into a new background, we include a time-multiplexed matte within the basis. We also show that the acquired data enables time-varying surface normals, albedo, and ambient occlusion to be estimated, which can be used to transform the actor's reflectance to produce both subtle and stylistic effects.	Performance relighting and reflectance transformation with time-multiplexed illumination	NA:NA:NA:NA:NA:NA	2018
Bennett Wilburn:Neel Joshi:Vaibhav Vaish:Eino-Ville Talvala:Emilio Antunez:Adam Barth:Andrew Adams:Mark Horowitz:Marc Levoy	The advent of inexpensive digital image sensors and the ability to create photographs that combine information from a number of sensed images are changing the way we think about photography. In this paper, we describe a unique array of 100 custom video cameras that we have built, and we summarize our experiences using this array in a range of imaging applications. Our goal was to explore the capabilities of a system that would be inexpensive to produce in the future. With this in mind, we used simple cameras, lenses, and mountings, and we assumed that processing large numbers of images would eventually be easy and cheap. The applications we have explored include approximating a conventional single center of projection video camera with high performance along one or more axes, such as resolution, dynamic range, frame rate, and/or large aperture, and using multiple cameras to approximate a video camera with a large synthetic aperture. This permits us to capture a video light field, to which we can apply spatiotemporal view interpolation algorithms in order to digitally simulate time dilation and camera motion. It also permits us to create video sequences using custom non-uniform synthetic apertures.	High performance imaging using large camera arrays	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Heung-Yeung Shum	NA	Session details: Texture synthesis	NA	2018
Sylvain Lefebvre:Hugues Hoppe	We present a texture synthesis scheme based on neighborhood matching, with contributions in two areas: parallelism and control. Our scheme defines an infinite, deterministic, aperiodic texture, from which windows can be computed in real-time on a GPU. We attain high-quality synthesis using a new analysis structure called the Gaussian stack, together with a coordinate upsampling step and a subpass correction approach. Texture variation is achieved by multiresolution jittering of exemplar coordinates. Combined with the local support of parallel synthesis, the jitter enables intuitive user controls including multiscale randomness, spatial modulation over both exemplar and output, feature drag-and-drop, and periodicity constraints. We also introduce synthesis magnification, a fast method for amplifying coarse synthesis results to higher resolution.	Parallel controllable texture synthesis	NA:NA	2018
Wojciech Matusik:Matthias Zwicker:Frédo Durand	We present a system for designing novel textures in the space of textures induced by an input database. We capture the structure of the induced space by a simplicial complex where vertices of the simplices represent input textures. A user can generate new textures by interpolating within individual simplices. We propose a morphable interpolation for textures, which also defines a metric used to build the simplicial complex. To guarantee sharpness in interpolated textures, we enforce histograms of high-frequency content using a novel method for histogram interpolation. We allow users to continuously navigate in the simplicial complex and design new textures using a simple and efficient user interface. We demonstrate the usefulness of our system by integrating it with a 3D texture painting application, where the user interactively designs desired textures.	Texture design using a simplicial complex of morphable textures	NA:NA:NA	2018
Vivek Kwatra:Irfan Essa:Aaron Bobick:Nipun Kwatra	We present a novel technique for texture synthesis using optimization. We define a Markov Random Field (MRF)-based similarity metric for measuring the quality of synthesized texture with respect to a given input sample. This allows us to formulate the synthesis problem as minimization of an energy function, which is optimized using an Expectation Maximization (EM)-like algorithm. In contrast to most example-based techniques that do region-growing, ours is a joint optimization approach that progressively refines the entire texture. Additionally, our approach is ideally suited to allow for controllable synthesis of textures. Specifically, we demonstrate controllability by animating image textures using flow fields. We allow for general two-dimensional flow fields that may dynamically change over time. Applications of this technique include dynamic texturing of fluid animations and texture-based flow visualization.	Texture optimization for example-based synthesis	NA:NA:NA:NA	2018
Robert L. Cook:Tony DeRose	Noise functions are an essential building block for writing procedural shaders in 3D computer graphics. The original noise function introduced by Ken Perlin is still the most popular because it is simple and fast, and many spectacular images have been made with it. Nevertheless, it is prone to problems with aliasing and detail loss. In this paper we analyze these problems and show that they are particularly severe when 3D noise is used to texture a 2D surface. We use the theory of wavelets to create a new class of simple and fast noise functions that avoid these problems.	Wavelet noise	NA:NA	2018
Steve Marschner	NA	Session details: Capturing reality II	NA	2018
Tim Hawkins:Per Einarsson:Paul Debevec	We present a technique for capturing time-varying volumetric data of participating media. A laser sheet is swept repeatedly through the volume, and the scattered light is imaged using a high-speed camera. Each sweep of the laser provides a near-simultaneous volume of density values. We demonstrate rendered animations under changing viewpoint and illumination, making use of measured values for the scattering phase function and albedo.	Acquisition of time-varying participating media	NA:NA:NA	2018
Yichen Wei:Eyal Ofek:Long Quan:Heung-Yeung Shum	In this paper, we propose a novel image-based approach to model hair geometry from images taken at multiple viewpoints. Unlike previous hair modeling techniques that require intensive user interactions or rely on special capturing setup under controlled illumination conditions, we use a handheld camera to capture hair images under uncontrolled illumination conditions. Our multi-view approach is natural and flexible for capturing. It also provides inherent strong and accurate geometric constraints to recover hair models.In our approach, the hair fibers are synthesized from local image orientations. Each synthesized fiber segment is validated and optimally triangulated from all visible views. The hair volume and the visibility of synthesized fibers can also be reliably estimated from multiple views. Flexibility of acquisition, little user interaction, and high quality results of recovered complex hair models are the key advantages of our method.	Modeling hair from multiple views	NA:NA:NA:NA	2018
Aseem Agarwala:Ke Colin Zheng:Chris Pal:Maneesh Agrawala:Michael Cohen:Brian Curless:David Salesin:Richard Szeliski	This paper describes a mostly automatic method for taking the output of a single panning video camera and creating a panoramic video texture (PVT): a video that has been stitched into a single, wide field of view and that appears to play continuously and indefinitely. The key problem in creating a PVT is that although only a portion of the scene has been imaged at any given time, the output must simultaneously portray motion throughout the scene. Like previous work in video textures, our method employs min-cut optimization to select fragments of video that can be stitched together both spatially and temporally. However, it differs from earlier work in that the optimization must take place over a much larger set of data. Thus, to create PVTs, we introduce a dynamic programming step, followed by a novel hierarchical min-cut optimization algorithm. We also use gradient-domain compositing to further smooth boundaries between video fragments. We demonstrate our results with an interactive viewer in which users can interactively pan and zoom on high-resolution PVTs.	Panoramic video textures	NA:NA:NA:NA:NA:NA:NA:NA	2018
Amit Agrawal:Ramesh Raskar:Shree K. Nayar:Yuanzhen Li	Flash images are known to suffer from several problems: saturation of nearby objects, poor illumination of distant objects, reflections of objects strongly lit by the flash and strong highlights due to the reflection of flash itself by glossy surfaces. We propose to use a flash and no-flash (ambient) image pair to produce better flash images. We present a novel gradient projection scheme based on a gradient coherence model that allows removal of reflections and highlights from flash images. We also present a brightness-ratio based algorithm that allows us to compensate for the falloff in the flash image brightness due to depth. In several practical scenarios, the quality of flash/no-flash images may be limited in terms of dynamic range. In such cases, we advocate using several images taken under different flash intensities and exposures. We analyze the flash intensity-exposure space and propose a method for adaptively sampling this space so as to minimize the number of captured images for any given scene. We present several experimental results that demonstrate the ability of our algorithms to produce improved flash images.	Removing photography artifacts using gradient projection and flash-exposure sampling	NA:NA:NA:NA	2018
Chris Bregler	NA	Session details: Image processing	NA	2018
Yuanzhen Li:Lavanya Sharan:Edward H. Adelson	High dynamic range (HDR) imaging is an area of increasing importance, but most display devices still have limited dynamic range (LDR). Various techniques have been proposed for compressing the dynamic range while retaining important visual information. Multi-scale image processing techniques, which are widely used for many image processing tasks, have a reputation of causing halo artifacts when used for range compression. However, we demonstrate that they can work when properly implemented. We use a symmetrical analysis-synthesis filter bank, and apply local gain control to the subbands. We also show that the technique can be adapted for the related problem of "companding", in which an HDR image is converted to an LDR image, and later expanded back to high dynamic range.	Compressing and companding high dynamic range images with subband architectures	NA:NA:NA	2018
Eric P. Bennett:Leonard McMillan	We enhance underexposed, low dynamic range videos by adaptively and independently varying the exposure at each photoreceptor in a post-process. This virtual exposure is a dynamic function of both the spatial neighborhood and temporal history at each pixel. Temporal integration enables us to expand the image's dynamic range while simultaneously reducing noise. Our non-linear exposure variation and denoising filters smoothly transition from temporal to spatial for moving scene elements. Our virtual exposure framework also supports temporally coherent per frame tone mapping. Our system outputs restored video sequences with significantly reduced noise, increased exposure time of dark pixels, intact motion, and improved details.	Video enhancement using per-pixel virtual exposures	NA:NA	2018
Yung-Yu Chuang:Dan B Goldman:Ke Colin Zheng:Brian Curless:David H. Salesin:Richard Szeliski	In this paper, we explore the problem of enhancing still pictures with subtly animated motions. We limit our domain to scenes containing passive elements that respond to natural forces in some fashion. We use a semi-automatic approach, in which a human user segments the scene into a series of layers to be individually animated. Then, a "stochastic motion texture" is automatically synthesized using a spectral method, i.e., the inverse Fourier transform of a filtered noise spectrum. The motion texture is a time-varying 2D displacement map, which is applied to each layer. The resulting warped layers are then recomposited to form the animated frames. The result is a looping video texture created from a single still image, which has the advantages of being more controllable and of generally higher image quality and resolution than a video texture created from a video source. We demonstrate the technique on a variety of photographs and paintings.	Animating pictures with stochastic motion textures	NA:NA:NA:NA:NA:NA	2018
Jian Sun:Lu Yuan:Jiaya Jia:Heung-Yeung Shum	In this paper, we introduce a novel approach to image completion, which we call structure propagation. In our system, the user manually specifies important missing structure information by extending a few curves or line segments from the known to the unknown regions. Our approach synthesizes image patches along these user-specified curves in the unknown region using patches selected around the curves in the known region. Structure propagation is formulated as a global optimization problem by enforcing structure and consistency constraints. If only a single curve is specified, structure propagation is solved using Dynamic Programming. When multiple intersecting curves are specified, we adopt the Belief Propagation algorithm to find the optimal patches. After completing structure propagation, we fill in the remaining unknown regions using patch-based texture synthesis. We show that our approach works well on a number of examples that are challenging to state-of-the-art techniques.	Image completion with structure propagation	NA:NA:NA:NA	2018
Marc Stamminger	NA	Session details: Large models & large displays	NA	2018
Louis Borgeat:Guy Godin:François Blais:Philippe Massicotte:Christian Lahanier	This paper presents a new technique for fast, view-dependent, real-time visualization of large multiresolution geometric models with color or texture information. This method uses geomorphing to smoothly interpolate between geometric patches composing a hierarchical level-of-detail structure, and to maintain seamless continuity between neighboring patches of the model. It combines the advantages of view-dependent rendering with numerous additional features: the high performance rendering associated with static preoptimized geometry, the capability to display at both low and high resolution with minimal artefacts, and a low CPU usage since all the geomorphing is done on the GPU. Furthermore, the hierarchical subdivision of the model into a tree structure can be accomplished according to any spatial or topological criteria. This property is particularly useful in dealing with models with high resolution textures derived from digital photographs. Results are presented for both highly tesselated models (372 million triangles), and for models which also contain large quantities of texture (200 million triangles + 20 GB of compressed texture). The method also incorporates asynchronous out-of-core model management. Performances obtained on commodity hardware are in the range of 50 million geomorphed triangles/second for a benchmark model such as Stanford's St. Matthew dataset.	GoLD: interactive display of huge colored and textured models	NA:NA:NA:NA:NA	2018
Enrico Gobbetti:Fabio Marton	We present an efficient approach for end-to-end out-of-core construction and interactive inspection of very large arbitrary surface models. The method tightly integrates visibility culling and out-of-core data management with a level-of-detail framework. At preprocessing time, we generate a coarse volume hierarchy by binary space partitioning the input triangle soup. Leaf nodes partition the original data into chunks of a fixed maximum number of triangles, while inner nodes are discretized into a fixed number of cubical voxels. Each voxel contains a compact direction dependent approximation of the appearance of the associated volumetric subpart of the model when viewed from a distance. The approximation is constructed by a visibility aware algorithm that fits parametric shaders to samples obtained by casting rays against the full resolution dataset. At rendering time, the volumetric structure, maintained off-core, is refined and rendered in front-to-back order, exploiting vertex programs for GPU evaluation of view-dependent voxel representations, hardware occlusion queries for culling occluded subtrees, and asynchronous I/O for detecting and avoiding data access latencies. Since the granularity of the multiresolution structure is coarse, data management, traversal and occlusion culling cost is amortized over many graphics primitives. The efficiency and generality of the approach is demonstrated with the interactive rendering of extremely complex heterogeneous surface models on current commodity graphics platforms.	Far voxels: a multiresolution framework for interactive rendering of huge complex 3D models on commodity graphics platforms	NA:NA	2018
Sung-Eui Yoon:Peter Lindstrom:Valerio Pascucci:Dinesh Manocha	We present a novel method for computing cache-oblivious layouts of large meshes that improve the performance of interactive visualization and geometric processing algorithms. Given that the mesh is accessed in a reasonably coherent manner, we assume no particular data access patterns or cache parameters of the memory hierarchy involved in the computation. Furthermore, our formulation extends directly to computing layouts of multi-resolution and bounding volume hierarchies of large meshes.We develop a simple and practical cache-oblivious metric for estimating cache misses. Computing a coherent mesh layout is reduced to a combinatorial optimization problem. We designed and implemented an out-of-core multilevel minimization algorithm and tested its performance on unstructured meshes composed of tens to hundreds of millions of triangles. Our layouts can significantly reduce the number of cache misses. We have observed 2--20 times speedups in view-dependent rendering, collision detection, and isocontour extraction without any modification of the algorithms or runtime applications.	Cache-oblivious mesh layouts	NA:NA:NA:NA	2018
Daniel J. Sandin:Todd Margolis:Jinghua Ge:Javier Girado:Tom Peterka:Thomas A. DeFanti	Virtual reality (VR) has long been hampered by the gear needed to make the experience possible; specifically, stereo glasses and tracking devices. Autostereoscopic display devices are gaining popularity by freeing the user from stereo glasses, however few qualify as VR displays. The Electronic Visualization Laboratory (EVL) at the University of Illinois at Chicago (UIC) has designed and produced a large scale, high resolution head-tracked barrier-strip autostereoscopic display system that produces a VR immersive experience without requiring the user to wear any encumbrances. The resulting system, called Varrier, is a passive parallax barrier 35-panel tiled display that produces a wide field of view, head-tracked VR experience. This paper presents background material related to parallax barrier autostereoscopy, provides system configuration and construction details, examines Varrier interleaving algorithms used to produce the stereo images, introduces calibration and testing, and discusses the camera-based tracking subsystem.	The VarrierTM autostereoscopic virtual reality display	NA:NA:NA:NA:NA:NA	2018
John Anderson	NA	Session details: Fluid simulation	NA	2018
Bryan E. Feldman:James F. O'Brien:Bryan M. Klingner	This paper presents a method for animating gases on unstructured tetrahedral meshes to efficiently model the interaction of fluids with irregularly shaped obstacles. Because our discretization scheme parallels that of the standard staggered grid mesh. we are able to combine tetrahedral cells with regular hexahedral cells in a single mesh. This hybrid mesh offers both accuracy near obstacles and efficiency in open regions.	Animating gases with hybrid meshes	NA:NA:NA	2018
Andrew Selle:Nick Rasmussen:Ronald Fedkiw	Vorticity confinement reintroduces the small scale detail lost when using efficient semi-Lagrangian schemes for simulating smoke and fire. However, it only amplifies the existing vorticity, and thus can be insufficient for highly turbulent effects such as explosions or rough water. We introduce a new hybrid technique that makes synergistic use of Lagrangian vortex particle methods and Eulerian grid based methods to overcome the weaknesses of both. Our approach uses vorticity confinement itself to couple these two methods together. We demonstrate that this approach can generate highly turbulent effects unachievable by standard grid based methods, and show applications to smoke, water and explosion simulations.	A vortex particle method for smoke, water and explosions	NA:NA:NA	2018
Jeong-Mo Hong:Chang-Hun Kim	At interfaces between different fluids, properties such as density, viscosity, and molecular cohesion are discontinuous. To animate small-scale details of incompressible viscous multi-phase fluids realistically, we focus on the discontinuities in the state variables that express these properties. Surface tension of both free and bubble surfaces is modeled using the jump condition in the pressure field; and discontinuities in the velocity gradient field. driven by viscosity differences, are also considered. To obtain derivatives of the pressure and velocity fields with sub-grid accuracy, they are extrapolated across interfaces using continuous variables based on physical properties. The numerical methods that we present are easy to implement and do not impact the performance of existing solvers. Small-scale fluid motions, such as capillary instability, breakup of liquid sheets, and bubbly water can all be successfully animated.	Discontinuous fluids	NA:NA	2018
Huamin Wang:Peter J. Mucha:Greg Turk	We present a physically-based method to enforce contact angles at the intersection of fluid free surfaces and solid objects, allowing us to simulate a variety of small-scale fluid phenomena including water drops on surfaces. The heart of this technique is a virtual surface method, which modifies the level set distance field representing the fluid surface in order to maintain an appropriate contact angle. The surface tension that is calculated on the contact line between the solid surface and liquid surface can then capture all interfacial tensions, including liquid-solid, liquid-air and solid-air tensions. We use a simple dynamic contact angle model to select contact angles according to the solid material property, water history, and the fluid front's motion. Our algorithm robustly and accurately treats various drop shape deformations, and handles both flat and curved solid surfaces. Our results show that our algorithm is capable of realistically simulating several small-scale liquid phenomena such as beading and flattened drops, stretched and separating drops, suspended drops on curved surfaces, and capillary action.	Water drops on surfaces	NA:NA:NA	2018
Steven Feiner	NA	Session details: Reprise of UIST and I3D: UIST (user interface software and technology)	NA	2018
Georg Apitz:François Guimbretière	We introduce CrossY, a simple drawing application developed as a benchmark to demonstrate the feasibility of goal-crossing as the basis for a graphical user interface. While crossing was previously identified as a potential substitute for the classic point-and-click interaction, this work is the first to report on the practical aspects of implementing an interface solely based on goal-crossing.	CrossY: a crossing-based drawing application	NA:NA	2018
Tovi Grossman:Daniel Wigdor:Ravin Balakrishnan	Volumetric displays provide interesting opportunities and challenges for 3D interaction and visualization, particularly when used in a highly interactive manner. We explore this area through the design and implementation of techniques for interactive direct manipulation of objects with a 3D volumetric display. Motion tracking of the user's fingers provides for direct gestural interaction with the virtual objects, through manipulations on and around the display's hemispheric enclosure. Our techniques leverage the unique features of volumetric displays, including a 360° viewing volume that enables manipulation from any viewpoint around the display, as well as natural and accurate perception of true depth information in the displayed 3D scene. We demonstrate our techniques within a prototype 3D geometric model building application.	Multi-finger gestural interaction with 3D volumetric displays	NA:NA:NA	2018
Blair MacIntyre:Maribeth Gandy:Steven Dow:Jay David Bolter	In this paper [MacIntyre et al 2004]. we describe The Designer's Augmented Reality Toolkit (DART). DART is built on top of Macromedia Director, a widely used multimedia development environment. We summarize the most significant problems faced by designers working with AR in the real world, and discuss how DART addresses them. Most of DART is implemented in an interpreted scripting language, and can be modified by designers to suit their needs. Our work focuses on supporting early design activities, especially a rapid transition from storyboards to working experience, so that the experiential part of a design can be tested early and often. DART allows designers to specify complex relationships between the physical and virtual worlds, and supports 3D animatic actors (informal, sketch-based content) in addition to more polished content. Designers can capture and replay synchronized video and sensor data, allowing them to work off-site and to test specific parts of their experience more effectively.	DART: a toolkit for rapid design exploration of augmented reality experiences	NA:NA:NA:NA	2018
David Luebke	NA	Session details: I3D (symposium on interactive 3D graphics)	NA	2018
Simon Dobbyn:John Hamill:Keith O'Conor:Carol O'Sullivan	The simulation of large crowds of humans is important in many fields of computer graphics, including real-time applications such as games, as they can breathe life into otherwise static scenes and enhance believability. Although many new games are released each year, it is very unusual to find large-scale crowds populating the environments depicted. Such applications need to deal with having limited resources available at each frame. With many hundreds or thousands of potential virtual humans in a crowd, traditional techniques rapidly become overwhelmed and are not able to sustain an interactive frame-rate. Therefore, simpler approaches to the rendering, animation and behaviour control of the crowds are needed. Additionally, these new approaches must provide for variety, as environments inhabited by carbon-copy clones can be disconcerting and unrealistic.	Geopostors: a real-time geometry/impostor crowd rendering system	NA:NA:NA:NA	2018
Youngihn Kho:Michael Garland	Techniques for interactive deformation of unstructured polygon meshes are of fundamental importance to a host of applications. Most traditional approaches to this problem have emphasized precise control over the deformation being made. However, they are often cumbersome and unintuitive for non-expert users.In this paper, we present an interactive system for deforming unstructured polygon meshes that is very easy to use. The user interacts with the system by sketching curves in the image plane. A single stroke can define a free-form skeleton and the region of the model to be deformed. By sketching the desired deformation of this reference curve, the user can implicitly and intuitively control the deformation of an entire region of the surface. At the same time, the reference curve also provides a basis for controlling additional parameters, such as twist and scaling. We demonstrate that our system can be used to interactively edit a variety of unstructured mesh models with very little effort. Furthermore, we can also use our formulation of the deformation to achieve a natural interpolation between character poses, thus producing simple key framed animations.	Sketching mesh deformations	NA:NA	2018
Fábio Policarpo:Manuel M. Oliveira:João L. D. Comba	We present a technique for mapping relief textures onto arbitrary polygonal models in real time, producing correct self-occlusions, interpenetrations, shadows and per-pixel lighting. The technique uses a pixel-driven formulation based on an efficient ray-height-field intersection implemented on the GPU. It has very low memory requirements, supports extreme close-up views of the surfaces and can be applicable to surfaces undergoing deformation.	Real-time relief mapping on arbitrary polygonal surfaces	NA:NA:NA	2018
Jovan Popovic	NA	Session details: Dynamics of solids	NA	2018
Stephane Redon:Nico Galoppo:Ming C. Lin	Forward dynamics is central to physically-based simulation and control of articulated bodies. We present an adaptive algorithm for computing forward dynamics of articulated bodies: using novel motion error metrics, our algorithm can automatically simplify the dynamics of a multi-body system, based on the desired number of degrees of freedom and the location of external forces and active joint forces. We demonstrate this method in plausible animation of articulated bodies, including a large-scale simulation of 200 animated humanoids and multi-body dynamics systems with many degrees of freedom. The graceful simplification allows us to achieve up to two orders of magnitude performance improvement in several complex benchmarks.	Adaptive dynamics of articulated bodies	NA:NA:NA	2018
Danny M. Kaufman:Timothy Edmunds:Dinesh K. Pai	We describe an efficient algorithm for the simulation of large sets of non-convex rigid bodies. The algorithm finds a simultaneous solution for a multi-body system that is linear in the total number of contacts detected in each iteration. We employ a novel contact model that uses mass, location, and velocity information from all contacts, at the moment of maximum compression, to constrain rigid body velocities. We also develop a new friction model in the configuration space of rigid bodies. These models are used to compute the feasible velocity and the frictional response of each body. Implementation is simple and leads to a fast rigid body simulator that computes steps on the order of seconds for simulations involving over one thousand non-convex objects in high contact configurations.	Fast frictional dynamics for rigid bodies	NA:NA:NA	2018
Mark Pauly:Richard Keiser:Bart Adams:Philip Dutré:Markus Gross:Leonidas J. Guibas	We present a new meshless animation framework for elastic and plastic materials that fracture. Central to our method is a highly dynamic surface and volume sampling method that supports arbitrary crack initiation, propagation, and termination, while avoiding many of the stability problems of traditional mesh-based techniques. We explicitly model advancing crack fronts and associated fracture surfaces embedded in the simulation volume. When cutting through the material, crack fronts directly affect the coupling between simulation nodes, requiring a dynamic adaptation of the nodal shape functions. We show how local visibility tests and dynamic caching lead to an efficient implementation of these effects based on point collocation. Complex fracture patterns of interacting and branching cracks are handled using a small set of topological operations for splitting, merging, and terminating crack fronts. This allows continuous propagation of cracks with highly detailed fracture surfaces, independent of the spatial resolution of the simulation nodes, and provides effective mechanisms for controlling fracture paths. We demonstrate our method for a wide range of materials, from stiff elastic to highly plastic objects that exhibit brittle and/or ductile fracture.	Meshless animation of fracturing solids	NA:NA:NA:NA:NA:NA	2018
Mathieu Desbrun	NA	Session details: Deformable models	NA	2018
Yongning Zhu:Robert Bridson	We present a physics-based simulation method for animating sand. To allow for efficiently scaling up to large volumes of sand, we abstract away the individual grains and think of the sand as a continuum. In particular we show that an existing water simulator can be turned into a sand simulator with only a few small additions to account for inter-grain and boundary friction.We also propose an alternative method for simulating fluids. Our core representation is a cloud of particles, which allows for accurate and flexible surface tracking and advection, but we use an auxiliary grid to efficiently enforce boundary conditions and incompressibility. We further address the issue of reconstructing a surface from particle data to render each frame.	Animating sand as a fluid	NA:NA	2018
Eran Guendelman:Andrew Selle:Frank Losasso:Ronald Fedkiw	We present a novel method for solid/fluid coupling that can treat infinitesimally thin solids modeled by a lower dimensional triangulated surface. Since classical solid/fluid coupling algorithms rasterize the solid body onto the fluid grid, an entirely new approach is required to treat thin objects that do not contain an interior region. Robust ray casting is used to augment a number of interpolation, finite difference and rendering techniques so that fluid does not leak through the triangulated surface. Moreover, we propose a technique for properly enforcing incompressibility so that fluid does not incorrectly compress (and appear to lose mass) near the triangulated surface. This allows for the robust interaction of cloth and shells with thin sheets of water. The proposed method works for both rigid body shells and for deformable manifolds such as cloth, and we present a two way coupling technique that allows the fluid's pressure to affect the solid. Examples illustrate that our method performs well, especially in the difficult case of water and cloth where it produces visually rich interactions between the particle level set method for treating the water/air interface and our newly proposed method for treating the solid/fluid interface. We have implemented the method on both uniform and adaptive octree grids.	Coupling water and smoke to thin deformable and rigid shells	NA:NA:NA:NA	2018
Jernej Barbič:Doug L. James	In this paper, we present an approach for fast subspace integration of reduced-coordinate nonlinear deformable models that is suitable for interactive applications in computer graphics and haptics. Our approach exploits dimensional model reduction to build reduced-coordinate deformable models for objects with complex geometry. We exploit the fact that model reduction on large deformation models with linear materials (as commonly used in graphics) result in internal force models that are simply cubic polynomials in reduced coordinates. Coefficients of these polynomials can be precomputed, for efficient runtime evaluation. This allows simulation of nonlinear dynamics using fast implicit Newmark subspace integrators, with subspace integration costs independent of geometric complexity. We present two useful approaches for generating low-dimensional subspace bases: modal derivatives and an interactive sketching technique. Mass-scaled principal component analysis (mass-PCA) is suggested for dimensionality reduction. Finally, several examples are given from computer animation to illustrate high performance, including force-feedback haptic rendering of a complicated object undergoing large deformations.	Real-Time subspace integration for St. Venant-Kirchhoff deformable models	NA:NA	2018
Naga K. Govindaraju:David Knott:Nitin Jain:Ilknur Kabul:Rasmus Tamstorf:Russell Gayle:Ming C. Lin:Dinesh Manocha	We present a novel algorithm for accurately detecting all contacts, including self-collisions, between deformable models. We precompute a chromatic decomposition of a mesh into non-adjacent primitives using graph coloring algorithms. The chromatic decomposition enables us to check for collisions between non-adjacent primitives using a linear-time culling algorithm. As a result, we achieve higher culling efficiency and significantly reduce the number of false positives. We use our algorithm to check for collisions among complex deformable models consisting of tens of thousands of triangles for cloth modeling and medical simulation. Our algorithm accurately computes all contacts at interactive rates. We observed up to an order of magnitude speedup over prior methods.	Interactive collision detection between deformable models using chromatic decomposition	NA:NA:NA:NA:NA:NA:NA:NA	2018
Henry Fuchs	NA	Session details: Geometry on GPUs	NA	2018
Charles Loop:Jim Blinn	We present a method for resolution independent rendering of paths and bounded regions, defined by quadratic and cubic spline curves, that leverages the parallelism of programmable graphics hardware to achieve high performance. A simple implicit equation for a parametric curve is found in a space that can be thought of as an analog to texture space. The image of a curve's Bézier control points are found in this space and assigned to the control points as texture coordinates. When the triangle(s) corresponding to the Bézier curve control hull are rendered, a pixel shader program evaluates the implicit equation for a pixel's interpolated texture coordinates to determine an inside/outside test for the curve. We extend our technique to handle anti-aliasing of boundaries. We also construct a vector image from mosaics of triangulated Bézier control points and show how to deform such images to create resolution independent texture on three dimensional objects.	Resolution independent curve rendering using programmable graphics hardware	NA:NA	2018
Le-Jeng Shiue:Ian Jones:Jörg Peters	By organizing the control mesh of subdivision in texture memory so that irregularities occur strictly inside independently refinable fragment meshes, all major features of subdivision algorithms can be realized in the framework of highly parallel stream processing. Our implementation of Catmull-Clark subdivision as a GPU kernel in programmable graphics hardware can model features like semi-smooth creases and global boundaries; and a simplified version achieves near-realtime depth-five re-evaluation of moderate-sized subdivision meshes. The approach is easily adapted to other refinement patterns, such as Loop, Doo-Sabin or √3 and it allows for postprocessing with additional shaders.	A realtime GPU subdivision kernel	NA:NA:NA	2018
Michael Guthe:Aákos Balázs:Reinhard Klein	As there is no hardware support neither for rendering trimmed NURBS -- the standard surface representation in CAD -- nor for T-Spline surfaces the usability of existing rendering APIs like OpenGL, where a run-time tessellation is performed on the CPU, is limited to simple scenes. Due to the irregular mesh data structures required for trimming no algorithms exists that exploit the GPU for tessellation. Therefore, recent approaches perform a pretessellation and use level-of-detail techniques. In contrast to a simple API these methods require tedious preparation of the models before rendering and hinder interactive editing. Furthermore, due to the tremendous amount of triangle data smooth zoom-ins from long shot to close-up are not possible, In this paper we show how the trimming region can be defined by a trim-texture that is dynamically adapted to the required resolution and allows for an efficient trimming of surfaces on the GPU. Combining this new method with GPU-based tessellation of cubic rational surfaces allows a new rendering algorithm for arbitrary trimmed NURBS and T-Spline surfaces with prescribed error in screen space on the GPU. The performance exceeds current CPU-based techniques by a factor of up to 1000 and makes real-time visualization of real-world trimmed NURBS and T-Spline models possible on consumer-level graphics cards.	GPU-based trimming and tessellation of NURBS and T-Spline surfaces	NA:NA:NA	2018
John Hable:Jarek Rossignac	By combining depth peeling with a linear formulation of a Boolean expression called Blist, the Blister algorithm renders an arbitrary CSG model of n primitives in at most k steps, where k is the number of depth-layers in the arrangement of the primitives. Each step starts by rendering each primitive to produce candidate surfels on the next depth-layer. Then, it renders the primitives again, one at a time, to classify the candidate surfels against the primitive and to evaluate the Boolean expression directly on the GPU. Since Blist does not expand the CSG expression into a disjunctive (sum-of-products) form, Blister has O(kn) time complexity. We explain the Blist formulation while providing algorithms for CSG-to-Blist conversion and Blist-based parallel surfel classification. We report real-time performance for nontrivial CSG models. On hardware with an 8-bit stencil buffer, we can render all possible CSG expressions with 3909 primitives.	Blister: GPU-based rendering of Boolean combinations of free-form triangulated shapes	NA:NA	2018
George Drettakis	NA	Session details: Transparency & translucency	NA	2018
Craig Donner:Henrik Wann Jensen	This paper introduces a shading model for light diffusion in multi-layered translucent materials. Previous work on diffusion in translucent materials has assumed smooth semi-infinite homogeneous materials and solved for the scattering of light using a dipole diffusion approximation. This approximation breaks down in the case of thin translucent slabs and multi-layered materials. We present a new efficient technique based on multiple dipoles to account for diffusion in thin slabs. We enhance this multipole theory to account for mismatching indices of refraction at the top and bottom of of translucent slabs, and to model the effects of rough surfaces. To model multiple layers, we extend this single slab theory by convolving the diffusion profiles of the individual slabs. We account for multiple scattering between slabs by using a variant of Kubelka-Munk theory in frequency space. Our results demonstrate diffusion of light in thin slabs and multi-layered materials such as paint, paper, and human skin.	Light diffusion in multi-layered translucent materials	NA:NA	2018
Bo Sun:Ravi Ramamoorthi:Srinivasa G. Narasimhan:Shree K. Nayar	We consider real-time rendering of scenes in participating media, capturing the effects of light scattering in fog, mist and haze. While a number of sophisticated approaches based on Monte Carlo and finite element simulation have been developed, those methods do not work at interactive rates. The most common real-time methods are essentially simple variants of the OpenGL fog model. While easy to use and specify, that model excludes many important qualitative effects like glows around light sources, the impact of volumetric scattering on the appearance of surfaces such as the diffusing of glossy highlights, and the appearance under complex lighting such as environment maps. In this paper, we present an alternative physically based approach that captures these effects while maintaining real time performance and the ease-of-use of the OpenGL fog model. Our method is based on an explicit analytic integration of the single scattering light transport equations for an isotropic point light source in a homogeneous participating medium. We can implement the model in modern programmable graphics hardware using a few small numerical lookup tables stored as texture maps. Our model can also be easily adapted to generate the appearances of materials with arbitrary BRDFs, environment map lighting, and precomputed radiance transfer methods, in the presence of participating media. Hence, our techniques can be widely used in real-time rendering.	A practical analytic single scattering model for real time rendering	NA:NA:NA:NA	2018
Chris Wyman	Many interactive applications strive for realistic renderings, but framerate constraints usually limit realism to effects that run efficiently in graphics hardware. One effect largely ignored in such applications is refraction. We introduce a simple, image-space approach to refractions that easily runs on modern graphics cards. Our method requires two passes on a GPU, and allows refraction of a distant environment through two interfaces, compared to current interactive techniques that are restricted to a single interface. Like all image-based algorithms, aliasing can occur in certain circumstances, but the plausible refractions generated with our approach should suffice for many applications.	An approximate image-space approach for interactive refraction	NA	2018
Xin Tong:Jiaping Wang:Stephen Lin:Baining Guo:Heung-Yeung Shum	Many translucent materials consist of evenly-distributed heterogeneous elements which produce a complex appearance under different lighting and viewing directions. For these quasi-homogeneous materials, existing techniques do not address how to acquire their material representations from physical samples in a way that allows arbitrary geometry models to be rendered with these materials. We propose a model for such materials that can be readily acquired from physical samples. This material model can be applied to geometric models of arbitrary shapes, and the resulting objects can be efficiently rendered without expensive subsurface light transport simulation. In developing a material model with these attributes, we capitalize on a key observation about the subsurface scattering characteristics of quasi-homogeneous materials at different scales. Locally, the non-uniformity of these materials leads to inhomogeneous subsurface scattering. For subsurface scattering on a global scale, we show that a lengthy photon path through an even distribution of heterogeneous elements statistically resembles scattering in a homogeneous medium. This observation allows us to represent and measure the global light transport within quasi-homogeneous materials as well as the transfer of light into and out of a material volume through surface mesostructures. We demonstrate our technique with results for several challenging materials that exhibit sophisticated appearance features such as transmission of back illumination through surface mesostructures.	Modeling and rendering of quasi-homogeneous materials	NA:NA:NA:NA:NA	2018
Jehee Lee	NA	Session details: Styles of human motion	NA	2018
Tomohiko Mukai:Shigeru Kuriyama	A common motion interpolation technique for realistic human animation is to blend similar motion samples with weighting functions whose parameters are embedded in an abstract space. Existing methods, however, are insensitive to statistical properties, such as correlations between motions. In addition, they lack the capability to quantitatively evaluate the reliability of synthesized motions. This paper proposes a method that treats motion interpolations as statistical predictions of missing data in an arbitrarily definable parametric space. A practical technique of geostatistics, called universal kriging, is then introduced for statistically estimating the correlations between the dissimilarity of motions and the distance in the parametric space. Our method statistically optimizes interpolation kernels for given parameters at each frame, using a pose distance metric to efficiently analyze the correlation. Motions are accurately predicted for the spatial constraints represented in the parametric space, and they therefore have few undesirable artifacts, if any. This property alleviates the problem of spatial inconsistencies, such as foot-sliding, that are associated with many existing methods. Moreover, numerical estimates for the reliability of predictions enable motions to be adaptively sampled. Since the interpolation kernels are computed with a linear system in real-time, motions can be interactively edited using various spatial controls.	Geostatistical motion interpolation	NA:NA	2018
C. Karen Liu:Aaron Hertzmann:Zoran Popović	This paper presents a novel physics-based representation of realistic character motion. The dynamical model incorporates several factors of locomotion derived from the biomechanical literature, including relative preferences for using some muscles more than others. elastic mechanisms at joints due to the mechanical properties of tendons, ligaments, and muscles, and variable stiffness at joints depending on the task. When used in a spacetime optimization framework, the parameters of this model define a wide range of styles of natural human movement.Due to the complexity of biological motion, these style parameters are too difficult to design by hand. To address this, we introduce Nonlinear Inverse Optimization, a novel algorithm for estimating optimization parameters from motion capture data. Our method can extract the physical parameters from a single short motion sequence. Once captured, this representation of style is extremely flexible: motions can be generated in the same style but performing different tasks, and styles may be edited to change the physical properties of the body.	Learning physics-based motion style with nonlinear inverse optimization	NA:NA:NA	2018
Eugene Hsu:Kari Pulli:Jovan Popović	Style translation is the process of transforming an input motion into a new style while preserving its original content. This problem is motivated by the needs of interactive applications, which require rapid processing of captured performances. Our solution learns to translate by analyzing differences between performances of the same content in input and output styles. It relies on a novel correspondence algorithm to align motions, and a linear time-invariant model to represent stylistic differences. Once the model is estimated with system identification, our system is capable of translating streaming input with simple linear operations at each frame.	Style translation for human motion	NA:NA:NA	2018
Liu Ren:Alton Patrick:Alexei A. Efros:Jessica K. Hodgins:James M. Rehg	In this paper, we investigate whether it is possible to develop a measure that quantifies the naturalness of human motion (as defined by a large database). Such a measure might prove useful in verifying that a motion editing operation had not destroyed the naturalness of a motion capture clip or that a synthetic motion transition was within the space of those seen in natural human motion. We explore the performance of mixture of Gaussians (MoG), hidden Markov models (HMM), and switching linear dynamic systems (SLDS) on this problem. We use each of these statistical models alone and as part of an ensemble of smaller statistical models. We also implement a Naive Bayes (NB) model for a baseline comparison. We test these techniques on motion capture data held out from a database, keyframed motions, edited motions, motions with noise added, and synthetic motion transitions. We present the results as receiver operating characteristic (ROC) curves and compare the results to the judgments made by subjects in a user study.	A data-driven approach to quantifying natural human motion	NA:NA:NA:NA:NA	2018
Julie Dorsey	NA	Session details: Appearance & illumination	NA	2018
Bruce Walter:Sebastian Fernandez:Adam Arbree:Kavita Bala:Michael Donikian:Donald P. Greenberg	Lightcuts is a scalable framework for computing realistic illumination. It handles arbitrary geometry, non-diffuse materials, and illumination from a wide variety of sources including point lights, area lights, HDR environment maps, sun/sky models, and indirect illumination. At its core is a new algorithm for accurately approximating illumination from many point lights with a strongly sublinear cost. We show how a group of lights can be cheaply approximated while bounding the maximum approximation error. A binary light tree and perceptual metric are then used to adaptively partition the lights into groups to control the error vs. cost tradeoff.We also introduce reconstruction cuts that exploit spatial coherence to accelerate the generation of anti-aliased images with complex illumination. Results are demonstrated for five complex scenes and show that lightcuts can accurately approximate hundreds of thousands of point lights using only a few hundred shadow rays. Reconstruction cuts can reduce the number of shadow rays to tens.	Lightcuts: a scalable approach to illumination	NA:NA:NA:NA:NA:NA	2018
Okan Arikan:David A. Forsyth:James F. O'Brien	In this paper we present an approximate method for accelerated computation of the final gathering step in a global illumination algorithm. Our method operates by decomposing the radiance field close to surfaces into separate far- and near-field components that can be approximated individually. By computing surface shading using these approximations, instead of directly querying the global illumination solution, we have been able to obtain rendering time speed ups on the order of 10x compared to previous acceleration methods. Our approximation schemes rely mainly on the assumptions that radiance due to distant objects will exhibit low spatial and angular variation, and that the visibility between a surface and nearby surfaces can be reasonably predicted by simple location and orientation-based heuristics. Motivated by these assumptions, our far-field scheme uses scattered-data interpolation with spherical harmonics to represent spatial and angular variation, and our near-field scheme employs an aggressively simple visibility heuristic. For our test scenes, the errors introduced when our assumptions fail do not result in visually objectionable artifacts or easily noticeable deviation from a ground-truth solution. We also discuss how our near-field approximation can be used with standard local illumination algorithms to produce significantly improved images at only negligible additional cost.	Fast and detailed approximate global illumination by irradiance decomposition	NA:NA:NA	2018
Frédo Durand:Nicolas Holzschuch:Cyril Soler:Eric Chan:François X. Sillion	We present a signal-processing framework for light transport. We study the frequency content of radiance and how it is altered by phenomena such as shading, occlusion, and transport. This extends previous work that considered either spatial or angular dimensions, and it offers a comprehensive treatment of both space and angle.We show that occlusion, a multiplication in the primal, amounts in the Fourier domain to a convolution by the spectrum of the blocker. Propagation corresponds to a shear in the space-angle frequency domain, while reflection on curved objects performs a different shear along the angular frequency axis. As shown by previous work, reflection is a convolution in the primal and therefore a multiplication in the Fourier domain. Our work shows how the spatial components of lighting are affected by this angular convolution.Our framework predicts the characteristics of interactions such as caustics and the disappearance of the shadows of small features. Predictions on the frequency content can then be used to control sampling rates for rendering. Other potential applications include precomputed radiance transfer and inverse rendering.	A frequency analysis of light transport	NA:NA:NA:NA:NA	2018
Yanyun Chen:Lin Xia:Tien-Tsin Wong:Xin Tong:Hujun Bao:Baining Guo:Heung-Yeung Shum	Weathering modeling introduces blemishes such as dirt, rust, cracks and scratches to virtual scenery. In this paper we present a visual stimulation technique that works well for a wide variety of weathering phenomena. Our technique, called γ-ton tracing, is based on a type of aging-inducing particles called γ-tons. Modeling a weathering effect with γ-ton tracing involves tracing a large number of γ-tons through the scene in a way similar to photon tracing and then generating the weathering effect using the recorded γ-ton transport information. With this technique, we can produce weathering effects that are customized to the scene geometry and tailored to the weathering sources. Several effects that are challenging for existing techniques can be readily captured by γ-ton tracing. These include global transport effects. or "stainbleeding". γ-ton tracing also enables visual simulations of complex multi-weathering effects. Lastly γ-ton tracing can generate weathering effects that not only involve texture changes but also large-scale geometry changes. We demonstrate our technique with a variety of examples.	Visual simulation of weathering by γ-ton tracing	NA:NA:NA:NA:NA:NA:NA	2018
David Ebert	NA	Session details: Shape & texture	NA	2018
Takeo Igarashi:Tomer Moscovich:John F. Hughes	We present an interactive system that lets a user move and deform a two-dimensional shape without manually establishing a skeleton or freeform deformation (FFD) domain beforehand. The shape is represented by a triangle mesh and the user moves several vertices of the mesh as constrained handles. The system then computes the positions of the remaining free vertices by minimizing the distortion of each triangle. While physically based simulation or iterative refinement can also be used for this purpose, they tend to be slow. We present a two-step closed-form algorithm that achieves real-time interaction. The first step finds an appropriate rotation for each triangle and the second step adjusts its scale. The key idea is to use quadratic error metrics so that each minimization problem becomes a system of linear equations. After solving the simultaneous equations at the beginning of interaction, we can quickly find the positions of free vertices during interactive manipulation. Our approach successfully conveys a sense of rigidity of the shape, which is difficult in space-warp approaches. With a multiple-point input device, even beginners can easily move, rotate, and deform shapes at will.	As-rigid-as-possible shape manipulation	NA:NA:NA	2018
Andrew Nealen:Olga Sorkine:Marc Alexa:Daniel Cohen-Or	In this paper we present a method for the intuitive editing of surface meshes by means of view-dependent sketching. In most existing shape deformation work, editing is carried out by selecting and moving a handle, usually a set of vertices. Our system lets the user easily determine the handle, either by silhouette selection and cropping, or by sketching directly onto the surface. Subsequently, an edit is carried out by sketching a new, view-dependent handle position or by indirectly influencing differential properties along the sketch. Combined, these editing and handle metaphors greatly simplify otherwise complex shape modeling tasks.	A sketch-based interface for detail-preserving mesh editing	NA:NA:NA:NA	2018
Kun Zhou:Xi Wang:Yiying Tong:Mathieu Desbrun:Baining Guo:Heung-Yeung Shum	We propose a technique, called TextureMontage, to seamlessly map a patchwork of texture images onto an arbitrary 3D model. A texture atlas can be created through the specification of a set of correspondences between the model and any number of texture images. First, our technique automatically partitions the mesh and the images, driven solely by the choice of feature correspondences. Most charts will then be parameterized over their corresponding image planes through the minimization of a distortion metric based on both geometric distortion and texture mismatch across patch boundaries and images. Lastly, a surface texture inpainting technique is used to fill in the remaining charts of the surface with no corresponding texture patches. The resulting texture mapping satisfies the (sparse or dense) user-specified constraints while minimizing the distortion of the texture images and ensuring a smooth transition across the boundaries of different mesh patches. Seamless Texturing of Arbitrary Surfaces From Multiple Images	TextureMontage	NA:NA:NA:NA:NA:NA	2018
Nelson Max	NA	Session details: Ray tracing	NA	2018
Samuli Laine:Timo Aila:Ulf Assarsson:Jaakko Lehtinen:Tomas Akenine-Möller	We present a new, fast algorithm for rendering physically-based soft shadows in ray tracing-based renderers. Our method replaces the hundreds of shadow rays commonly used in stochastic ray tracers with a single shadow ray and a local reconstruction of the visibility function. Compared to tracing the shadow rays. our algorithm produces exactly the same image while executing one to two orders of magnitude faster in the test scenes used. Our first contribution is a two-stage method for quickly determining the silhouette edges that overlap an area light source, as seen from the point to be shaded. Secondly, we show that these partial silhouettes of occluders, along with a single shadow ray, are sufficient for reconstructing the visibility function between the point and the light source.	Soft shadow volumes for ray tracing	NA:NA:NA:NA:NA	2018
Petrik Clarberg:Wojciech Jarosz:Tomas Akenine-Möller:Henrik Wann Jensen	We present a new technique for importance sampling products of complex functions using wavelets. First, we generalize previous work on wavelet products to higher dimensional spaces and show how this product can be sampled on-the-fly without the need of evaluating the full product. This makes it possible to sample products of high-dimensional functions even if the product of the two functions in itself is too memory consuming. Then, we present a novel hierarchical sample warping algorithm that generates high-quality point distributions, which match the wavelet representation exactly. One application of the new sampling technique is rendering of objects with measured BRDFs illuminated by complex distant lighting --- our results demonstrate how the new sampling technique is more than an order of magnitude more efficient than the best previous techniques.	Wavelet importance sampling: efficiently evaluating products of complex functions	NA:NA:NA:NA	2018
Alexander Reshetov:Alexei Soupikov:Jim Hurley	We propose new approaches to ray tracing that greatly reduce the required number of operations while strictly preserving the geometrical correctness of the solution. A hierarchical "beam" structure serves as a proxy for a collection of rays. It is tested against a kd-tree representing the overall scene in order to discard from consideration the sub-set of the kd-tree (and hence the scene) that is guaranteed not to intersect with any possible ray inside the beam. This allows for all the rays inside the beam to start traversing the tree from some node deep inside thus eliminating unnecessary operations. The original beam can be further sub-divided, and we can either continue looking for new optimal entry points for the sub-beams, or we can decompose the beam into individual rays. This is a hierarchical process that can be adapted to the geometrical complexity of a particular view direction allowing for efficient geometric anti-aliasing. By amortizing the cost of partially traversing the tree for all the rays in a beam, up to an order of magnitude performance improvement can be achieved enabling interactivity for complex scenes on ordinary desktop machines.	Multi-level ray tracing algorithm	NA:NA:NA	2018
David Cline:Justin Talbot:Parris Egbert	We present Energy Redistribution (ER) sampling as an unbiased method to solve correlated integral problems. ER sampling is a hybrid algorithm that uses Metropolis sampling-like mutation strategies in a standard Monte Carlo integration setting, rather than resorting to an intermediate probability distribution step. In the context of global illumination, we present Energy Redistribution Path Tracing (ERPT). Beginning with an inital set of light samples taken from a path tracer, ERPT uses path mutations to redistribute the energy of the samples over the image plane to reduce variance. The result is a global illumination algorithm that is conceptually simpler than Metropolis Light Transport (MLT) while retaining its most powerful feature, path mutation. We compare images generated with the new technique to standard path tracing and MLT.	Energy redistribution path tracing	NA:NA:NA	2018
Wolfgang Heidrich	NA	Session details: Precomputed light transport	NA	2018
Kun Zhou:Yaohua Hu:Stephen Lin:Baining Guo:Heung-Yeung Shum	We present a soft shadow technique for dynamic scenes with moving objects under the combined illumination of moving local light sources and dynamic environment maps. The main idea of our technique is to precompute for each scene entity a shadow field that describes the shadowing effects of the entity at points around it. The shadow field for a light source, called a source radiance field (SRF), records radiance from an illuminant as cube maps at sampled points in its surrounding space. For an occluder, an object occlusion field (OOF) conversely represents in a similar manner the occlusion of radiance by an object. A fundamental difference between shadow fields and previous shadow computation concepts is that shadow fields can be precomputed independent of scene configuration. This is critical for dynamic scenes because, at any given instant, the shadow information at any receiver point can be rapidly computed as a simple combination of SRFs and OOFs according to the current scene configuration. Applications that particularly benefit from this technique include large dynamic scenes in which many instances of an entity can share a single shadow field. Our technique enables low-frequency shadowing effects in dynamic scenes in real-time and all-frequency shadows at interactive rates.	Precomputed shadow fields for dynamic scenes	NA:NA:NA:NA:NA	2018
Rui Wang:John Tran:David Luebke	We present a technique, based on precomputed light transport, for interactive rendering of translucent objects under all-frequency environment maps. We consider the complete BSSRDF model proposed by Jensen et al. [2001]. which includes both single and diffuse multiple scattering components. The challenge is how to efficiently precompute all-frequency light transport functions due to subsurface scattering. We apply the two-pass hierarchical technique by Jensen et al. [2002] in the space of non-linearly approximated transport vectors, which allows us to efficiently evaluate transport vectors due to diffuse multiple scattering. We then include an approximated single scattering term in the precomputation, which previous interactive systems have ignored. For an isotropic phase function, this approximation produces a diffuse transport vector per vertex, and is combined with the multiple scattering component. For a general phase function, we introduce a technique from BRDF rendering to factor the phase function using a separable decomposition to allow for view-dependent rendering. We show that our rendering results qualitatively match the appearance of translucent objects, achieving a high level of realism at interactive rates.	All-frequency interactive relighting of translucent objects with single and multiple scattering	NA:NA:NA	2018
Anders Wang Kristensen:Tomas Akenine-Möller:Henrik Wann Jensen	This paper introduces a new method for real-time relighting of scenes illuminated by local light sources. We extend previous work on precomputed radiance transfer for distant lighting to local lighting by introducing the concept of unstructured light clouds. The unstructured light cloud enables a compact representation of local lights in the model and real-time rendering of complex models with full global illumination due to local light sources. We use simplification of lights, and clustered PCA to obtain a compressed representation. When storing only the indirect component of the illumination, we are able to get high quality with only 8-16 lighting coefficients per vertex. Our results demonstrate real-time rendering of scenes with moving lights, dynamic cameras, glossy materials and global illumination.	Precomputed local radiance transfer for real-time lighting design	NA:NA:NA	2018
Kavita Bala	NA	Session details: Sampling and ray tracing	NA	2018
Ingo Wald:Thiago Ize:Andrew Kensler:Aaron Knoll:Steven G. Parker	We present a new approach to interactive ray tracing of moderate-sized animated scenes based on traversing frustum-bounded packets of coherent rays through uniform grids. By incrementally computing the overlap of the frustum with a slice of grid cells, we accelerate grid traversal by more than a factor of 10, and achieve ray tracing performance competitive with the fastest known packet-based kd-tree ray tracers. The ability to efficiently rebuild the grid on every frame enables this performance even for fully dynamic scenes that typically challenge interactive ray tracing systems.	Ray tracing animated scenes using coherent grid traversal	NA:NA:NA:NA:NA	2018
Peter Wonka:Michael Wimmer:Kaichi Zhou:Stefan Maierhofer:Gerd Hesina:Alexander Reshetov	This paper addresses the problem of computing the triangles visible from a region in space. The proposed aggressive visibility solution is based on stochastic ray shooting and can take any triangular model as input. We do not rely on connectivity information, volumetric occluders, or the availability of large occluders, and can therefore process any given input scene. The proposed algorithm is practically memoryless, thereby alleviating the large memory consumption problems prevalent in several previous algorithms. The strategy of our algorithm is to use ray mutations in ray space to cast rays that are likely to sample new triangles. Our algorithm improves the sampling efficiency of previous work by over two orders of magnitude.	Guided visibility sampling	NA:NA:NA:NA:NA:NA	2018
Daniel Dunbar:Greg Humphreys	Sampling distributions with blue noise characteristics are widely used in computer graphics. Although Poisson-disk distributions are known to have excellent blue noise characteristics, they are generally regarded as too computationally expensive to generate in real time. We present a new method for sampling by dart-throwing in O(N log N) time and introduce a novel and efficient variation for generating Poisson-disk distributions in O(N) time and space.	A spatial data structure for fast Poisson-disk sample generation	NA:NA	2018
Johannes Kopf:Daniel Cohen-Or:Oliver Deussen:Dani Lischinski	Well distributed point sets play an important role in a variety of computer graphics contexts, such as anti-aliasing, global illumination, halftoning, non-photorealistic rendering, point-based modeling and rendering, and geometry processing. In this paper, we introduce a novel technique for rapidly generating large point sets possessing a blue noise Fourier spectrum and high visual quality. Our technique generates non-periodic point sets, distributed over arbitrarily large areas. The local density of a point set may be prescribed by an arbitrary target density function, without any preset bound on the maximum density. Our technique is deterministic and tile-based; thus, any local portion of a potentially infinite point set may be consistently regenerated as needed. The memory footprint of the technique is constant, and the cost to generate any local portion of the point set is proportional to the integral over the target density in that area. These properties make our technique highly suitable for a variety of real-time interactive applications, some of which are demonstrated in the paper.Our technique utilizes a set of carefully constructed progressive and recursive blue noise Wang tiles. The use of Wang tiles enables the generation of infinite non-periodic tilings. The progressive point sets inside each tile are able to produce spatially varying point densities. Recursion allows our technique to adaptively subdivide tiles only where high density is required, and makes it possible to zoom into point sets by an arbitrary amount, while maintaining a constant apparent density.	Recursive Wang tiles for real-time blue noise	NA:NA:NA:NA	2018
Yizhou Yu	NA	Session details: Image processing	NA	2018
Ben Weiss	Median filtering is a cornerstone of modern image processing and is used extensively in smoothing and de-noising applications. The fastest commercial implementations (e.g. in Adobe® Photoshop® CS2) exhibit O(r) runtime in the radius of the filter, which limits their usefulness in realtime or resolution-independent contexts. We introduce a CPU-based, vectorizable O(log r) algorithm for median filtering, to our knowledge the most efficient yet developed. Our algorithm extends to images of any bit-depth, and can also be adapted to perform bilateral filtering. On 8-bit data our median filter outperforms Photoshop's implementation by up to a factor of fifty.	Fast median and bilateral filtering	NA	2018
Aude Oliva:Antonio Torralba:Philippe G. Schyns	We present hybrid images, a technique that produces static images with two interpretations, which change as a function of viewing distance. Hybrid images are based on the multiscale processing of images by the human visual system and are motivated by masking studies in visual perception. These images can be used to create compelling displays in which the image appears to change as the viewing distance changes. We show that by taking into account perceptual grouping mechanisms it is possible to build compelling hybrid images with stable percepts at each distance. We show examples in which hybrid images are used to create textures that become visible only when seen up-close, to generate facial expressions whose interpretation changes with viewing distance, and to visualize changes over time within a single picture.	Hybrid images	NA:NA:NA	2018
Scott Schaefer:Travis McPhail:Joe Warren	We provide an image deformation method based on Moving Least Squares using various classes of linear functions including affine, similarity and rigid transformations. These deformations are realistic and give the user the impression of manipulating real-world objects. We also allow the user to specify the deformations using either sets of points or line segments, the later useful for controlling curves and profiles present in the image. For each of these techniques, we provide simple closed-form solutions that yield fast deformations, which can be performed in real-time.	Image deformation using moving least squares	NA:NA:NA	2018
Sylvain Lefebvre:Hugues Hoppe	The traditional approach in texture synthesis is to compare color neighborhoods with those of an exemplar. We show that quality is greatly improved if pointwise colors are replaced by appearance vectors that incorporate nonlocal information such as feature and radiance-transfer data. We perform dimensionality reduction on these vectors prior to synthesis, to create a new appearance-space exemplar. Unlike a texton space, our appearance space is low-dimensional and Euclidean. Synthesis in this information-rich space lets us reduce runtime neighborhood vectors from 5x5 grids to just 4 locations. Building on this unifying framework, we introduce novel techniques for coherent anisometric synthesis, surface texture synthesis directly in an ordinary atlas, and texture advection. Remarkably, we achieve all these functionalities in real-time, or 3 to 4 orders of magnitude faster than prior work.	Appearance-space texture synthesis	NA:NA	2018
Ioana Boier-Martin	NA	Session details: Shape matching and symmetry	NA	2018
Joshua Podolak:Philip Shilane:Aleksey Golovinskiy:Szymon Rusinkiewicz:Thomas Funkhouser	Symmetry is an important cue for many applications, including object alignment, recognition, and segmentation. In this paper, we describe a planar reflective symmetry transform (PRST) that captures a continuous measure of the reflectional symmetry of a shape with respect to all possible planes. This transform combines and extends previous work that has focused on global symmetries with respect to the center of mass in 3D meshes and local symmetries with respect to points in 2D images. We provide an efficient Monte Carlo sampling algorithm for computing the transform for surfaces and show that it is stable under common transformations. We also provide an iterative refinement algorithm to find local maxima of the transform precisely. We use the transform to define two new geometric properties, center of symmetry and principal symmetry axes, and show that they are useful for aligning objects in a canonical coordinate system. Finally, we demonstrate that the symmetry transform is useful for several applications in computer graphics, including shape matching, segmentation of meshes into parts, and automatic viewpoint selection.	A planar-reflective symmetry transform for 3D shapes	NA:NA:NA:NA:NA	2018
Niloy J. Mitra:Leonidas J. Guibas:Mark Pauly	"Symmetry is a complexity-reducing concept [...]; seek it every-where." - Alan J. PerlisMany natural and man-made objects exhibit significant symmetries or contain repeated substructures. This paper presents a new algorithm that processes geometric models and efficiently discovers and extracts a compact representation of their Euclidean symmetries. These symmetries can be partial, approximate, or both. The method is based on matching simple local shape signatures in pairs and using these matches to accumulate evidence for symmetries in an appropriate transformation space. A clustering stage extracts potential significant symmetries of the object, followed by a verification step. Based on a statistical sampling analysis, we provide theoretical guarantees on the success rate of our algorithm. The extracted symmetry graph representation captures important high-level information about the structure of a geometric model which in turn enables a large set of further processing operations, including shape compression, segmentation, consistent editing, symmetrization, indexing for retrieval, etc.	Partial and approximate symmetry detection for 3D geometry	NA:NA:NA	2018
Qi-Xing Huang:Simon Flöry:Natasha Gelfand:Michael Hofer:Helmut Pottmann	We present a system for automatic reassembly of broken 3D solids. Given as input 3D digital models of the broken fragments, we analyze the geometry of the fracture surfaces to find a globally consistent reconstruction of the original object. Our reconstruction pipeline consists of a graph-cuts based segmentation algorithm for identifying potential fracture surfaces, feature-based robust global registration for pairwise matching of fragments, and simultaneous constrained local registration of multiple fragments. We develop several new techniques in the area of geometry processing, including the novel integral invariants for computing multi-scale surface characteristics, registration based on forward search techniques and surface consistency, and a non-penetrating iterated closest point algorithm. We illustrate the performance of our algorithms on a number of real-world examples.	Reassembling fractured objects by geometric matching	NA:NA:NA:NA:NA	2018
Sylvain Lefebvre:Hugues Hoppe	We explore using hashing to pack sparse data into a compact table while retaining efficient random access. Specifically, we design a perfect multidimensional hash function -- one that is precomputed on static data to have no hash collisions. Because our hash function makes a single reference to a small offset table, queries always involve exactly two memory accesses and are thus ideally suited for parallel SIMD evaluation on graphics hardware. Whereas prior hashing work strives for pseudorandom mappings, we instead design the hash function to preserve spatial coherence and thereby improve runtime locality of reference. We demonstrate numerous graphics applications including vector images, texture sprites, alpha channel compression, 3D-parameterized textures, 3D painting, simulation, and collision detection.	Perfect spatial hashing	NA:NA	2018
David Ebert	NA	Session details: Shape modeling and textures	NA	2018
Olga A. Karpenko:John F. Hughes	We introduce SmoothSketch---a system for inferring plausible 3D free-form shapes from visible-contour sketches. In our system, a user's sketch need not be a simple closed curve as in Igarashi's Teddy [1999], but may have cusps and T-junctions, i.e., endpoints of hidden parts of the contour. We follow a process suggested by Williams [1994] for inferring a smooth solid shape from its visible contours: completion of hidden contours, topological shape reconstruction, and smoothly embedding the shape via relaxation. Our main contribution is a practical method to go from a contour drawing to a fairly smooth surface with that drawing as its visible contour. In doing so, we make several technical contributions: • extending Williams' and Mumford's work [Mumford 1994] on figural completion of hidden contours containing T-junctions to contours containing cusps as well, • characterizing a class of visible-contour drawings for which inflation can be proved possible, • finding a topological embedding of the combinatorial surface that Williams creates from the figural completion, and • creating a fairly smooth solid shape by smoothing the topological embedding using a mass-spring system.We handle many kinds of drawings (including objects with holes), and the generated shapes are plausible interpretations of the sketches. The method can be incorporated into any sketch-based free-form modeling interface like Teddy.	SmoothSketch: 3D free-form shapes from complex sketches	NA:NA	2018
Long Quan:Ping Tan:Gang Zeng:Lu Yuan:Jingdong Wang:Sing Bing Kang	In this paper, we propose a semi-automatic technique for modeling plants directly from images. Our image-based approach has the distinct advantage that the resulting model inherits the realistic shape and complexity of a real plant. We designed our modeling system to be interactive, automating the process of shape recovery while relying on the user to provide simple hints on segmentation. Segmentation is performed in both image and 3D spaces, allowing the user to easily visualize its effect immediately. Using the segmented image and 3D data, the geometry of each leaf is then automatically recovered from the multiple views by fitting a deformable leaf model. Our system also allows the user to easily reconstruct branches in a similar manner. We show realistic reconstructions of a variety of plants, and demonstrate examples of plant editing.	Image-based plant modeling	NA:NA:NA:NA:NA:NA	2018
Ryan Schmidt:Cindy Grimm:Brian Wyvill	A method is described for texturing surfaces using decals, images placed on the surface using local parameterizations. Decal parameterizations are generated with a novel O(N log N) discrete approximation to the exponential map which requires only a single additional step in Dijkstra's graph-distance algorithm. Decals are dynamically composited in an interface that addresses many limitations of previous work. Tools for image processing, deformation/feature-matching, and vector graphics are implemented using direct surface interaction. Exponential map decals can contain holes and can also be combined with conformal parameterization to reduce distortion. The exponential map approximation can be computed on any point set, including meshes and sampled implicit surfaces, and is relatively stable under resampling. The decals stick to the surface as it is interactively deformed, allowing the texture to be preserved even if the surface changes topology. These properties make exponential map decals a suitable approach for texturing animated implicit surfaces.	Interactive decal compositing with discrete exponential maps	NA:NA:NA	2018
Pascal Müller:Peter Wonka:Simon Haegler:Andreas Ulmer:Luc Van Gool	CGA shape, a novel shape grammar for the procedural modeling of CG architecture, produces building shells with high visual quality and geometric detail. It produces extensive architectural models for computer games and movies, at low cost. Context sensitive shape rules allow the user to specify interactions between the entities of the hierarchical shape descriptions. Selected examples demonstrate solutions to previously unsolved modeling problems, especially to consistent mass modeling with volumetric shapes of arbitrary orientation. CGA shape is shown to efficiently generate massive urban models with unprecedented level of detail, with the virtual rebuilding of the archaeological site of Pompeii as a case in point.	Procedural modeling of buildings	NA:NA:NA:NA:NA	2018
Alexei Efros	NA	Session details: Image manipulation	NA	2018
Daniel Cohen-Or:Olga Sorkine:Ran Gal:Tommer Leyvand:Ying-Qing Xu	Harmonic colors are sets of colors that are aesthetically pleasing in terms of human visual perception. In this paper, we present a method that enhances the harmony among the colors of a given photograph or of a general image, while remaining faithful, as much as possible, to the original colors. Given a color image, our method finds the best harmonic scheme for the image colors. It then allows a graceful shifting of hue values so as to fit the harmonic scheme while considering spatial coherence among colors of neighboring pixels using an optimization technique. The results demonstrate that our method is capable of automatically enhancing the color "look-and-feel" of an ordinary image. In particular, we show the results of harmonizing the background image to accommodate the colors of a foreground image, or the foreground with respect to the background, in a cut-and-paste setting. Our color harmonization technique proves to be useful in adjusting the colors of an image composed of several parts taken from different sources.	Color harmonization	NA:NA:NA:NA:NA	2018
Jiaya Jia:Jian Sun:Chi-Keung Tang:Heung-Yeung Shum	In this paper, we present a user-friendly system for seamless image composition, which we call drag-and-drop pasting. We observe that for Poisson image editing [Perez et al. 2003] to work well, the user must carefully draw a boundary on the source image to indicate the region of interest, such that salient structures in source and target images do not conflict with each other along the boundary. To make Poisson image editing more practical and easy to use, we propose a new objective function to compute an optimized boundary condition. A shortest closed-path algorithm is designed to search for the location of the boundary. Moreover, to faithfully preserve the object's fractional boundary, we construct a blended guidance field to incorporate the object's alpha matte. To use our system, the user needs only to simply outline a region of interest in the source image, and then drag and drop it onto the target image. Experimental results demonstrate the effectiveness of our "drag-and-drop pasting" system.	Drag-and-drop pasting	NA:NA:NA:NA	2018
Soonmin Bae:Sylvain Paris:Frédo Durand	We introduce a new approach to tone management for photographs. Whereas traditional tone-mapping operators target a neutral and faithful rendition of the input image, we explore pictorial looks by controlling visual qualities such as the tonal balance and the amount of detail. Our method is based on a two-scale non-linear decomposition of an image. We modify the different layers based on their histograms and introduce a technique that controls the spatial variation of detail. We introduce a Poisson correction that prevents potential gradient reversal and preserves detail. In addition to directly controlling the parameters, the user can transfer the look of a model photograph to the picture being edited.	Two-scale tone management for photographic look	NA:NA:NA	2018
Dani Lischinski:Zeev Farbman:Matt Uyttendaele:Richard Szeliski	This paper presents a new interactive tool for making local adjustments of tonal values and other visual parameters in an image. Rather than carefully selecting regions or hand-painting layer masks, the user quickly indicates regions of interest by drawing a few simple brush strokes and then uses sliders to adjust the brightness, contrast, and other parameters in these regions. The effects of the user's sparse set of constraints are interpolated to the entire image using an edge-preserving energy minimization method designed to prevent the propagation of tonal adjustments to regions of significantly different luminance. The resulting system is suitable for adjusting ordinary and high dynamic range images, and provides the user with much more creative control than existing tone mapping algorithms. Our tool is also able to produce a tone mapping automatically, which may serve as a basis for further local adjustments, if so desired. The constraint propagation approach developed in this paper is a general one, and may also be used to interactively control a variety of other adjustments commonly performed in the digital darkroom.	Interactive local adjustment of tonal values	NA:NA:NA:NA	2018
Erum Arif Khan:Erik Reinhard:Roland W. Fleming:Heinrich H. Bülthoff	Photo editing software allows digital images to be blurred, warped or re-colored at the touch of a button. However, it is not currently possible to change the material appearance of an object except by painstakingly painting over the appropriate pixels. Here we present a method for automatically replacing one material with another, completely different material, starting with only a single high dynamic range image as input. Our approach exploits the fact that human vision is surprisingly tolerant of certain (sometimes enormous) physical inaccuracies, while being sensitive to others. By adjusting our simulations to be careful about those aspects to which the human visual system is sensitive, we are for the first time able to demonstrate significant material changes on the basis of a single photograph as input.	Image-based material editing	NA:NA:NA:NA	2018
Leif P. Kobbelt	NA	Session details: Surfaces	NA	2018
Charles Loop:Jim Blinn	We consider the problem of real-time GPU rendering of algebraic surfaces defined by Bézier tetrahedra. These surfaces are rendered directly in terms of their polynomial representations, as opposed to a collection of approximating triangles, thereby eliminating tessellation artifacts and reducing memory usage. A key step in such algorithms is the computation of univariate polynomial coefficients at each pixel; real roots of this polynomial correspond to possibly visible points on the surface. Our approach leverages the strengths of GPU computation and is highly efficient. Furthermore, we compute these coefficients in Bernstein form to maximize the stability of root finding, and to provide shader instances with an early exit test based on the sign of these coefficients. Solving for roots is done using analytic techniques that map well to a SIMD architecture, but limits us to fourth order algebraic surfaces. The general framework could be extended to higher order with numerical root finding.	Real-time GPU rendering of piecewise algebraic surfaces	NA:NA	2018
Anders Adamson:Marc Alexa	A piecewise smooth surface, possibly with boundaries, sharp edges, corners, or other features is defined by a set of samples. The basic idea is to model surface patches, curve segments and points explicitly, and then to glue them together based on explicit connectivity information. The geometry is defined as the set of stationary points of a projection operator, which is generalized to allow modeling curves with samples, and extended to account for the connectivity information. Additional tangent constraints can be used to model shapes with continuous tangents across edges and corners.	Point-sampled cell complexes	NA:NA	2018
Yang Liu:Helmut Pottmann:Johannes Wallner:Yong-Liang Yang:Wenping Wang	In architectural freeform design, the relation between shape and fabrication poses new challenges and requires more sophistication from the underlying geometry. The new concept of conical meshes satisfies central requirements for this application: They are quadrilateral meshes with planar faces, and therefore particularly suitable for the design of freeform glass structures. Moreover, they possess a natural offsetting operation and provide a support structure orthogonal to the mesh. Being a discrete analogue of the network of principal curvature lines, they represent fundamental shape characteristics. We show how to optimize a quad mesh such that its faces become planar, or the mesh becomes even conical. Combining this perturbation with subdivision yields a powerful new modeling tool for all types of quad meshes with planar faces, making subdivision attractive for architecture design and providing an elegant way of modeling developable surfaces.	Geometric modeling with conical meshes and developable surfaces	NA:NA:NA:NA:NA	2018
Kun Zhou:Xin Huang:Xi Wang:Yiying Tong:Mathieu Desbrun:Baining Guo:Heung-Yeung Shum	We introduce mesh quilting, a geometric texture synthesis algorithm in which a 3D texture sample given in the form of a triangle mesh is seamlessly applied inside a thin shell around an arbitrary surface through local stitching and deformation. We show that such geometric textures allow interactive and versatile editing and animation, producing compelling visual effects that are difficult to achieve with traditional texturing methods. Unlike pixel-based image quilting, mesh quilting is based on stitching together 3D geometry elements. Our quilting algorithm finds corresponding geometry elements in adjacent texture patches, aligns elements through local deformation, and merges elements to seamlessly connect texture patches. For mesh quilting on curved surfaces, a critical issue is to reduce distortion of geometry elements inside the 3D space of the thin shell. To address this problem we introduce a low-distortion parameterization of the shell space so that geometry elements can be synthesized even on very curved objects without the visual distortion present in previous approaches. We demonstrate how mesh quilting can be used to generate convincing decorations for a wide range of geometric textures.	Mesh quilting for geometric texture synthesis	NA:NA:NA:NA:NA:NA:NA	2018
Greg Ward	NA	Session details: HDR and systems	NA	2018
Jacob Munkberg:Petrik Clarberg:Jon Hasselgren:Tomas Akenine-Möller	In this paper, we break new ground by presenting algorithms for fixed-rate compression of high dynamic range textures at low bit rates. First, the S3TC low dynamic range texture compression scheme is extended in order to enable compression of HDR data. Second, we introduce a novel robust algorithm that offers superior image quality. Our algorithm can be efficiently implemented in hardware, and supports textures with a dynamic range of over 109:1. At a fixed rate of 8 bits per pixel, we obtain results virtually indistinguishable from uncompressed HDR textures at 48 bits per pixel. Our research can have a big impact on graphics hardware and real-time rendering, since HDR texturing suddenly becomes affordable.	High dynamic range texture compression for graphics hardware	NA:NA:NA:NA	2018
Kimmo Roimela:Tomi Aarnio:Joonas Itäranta	We present a novel compression scheme for high dynamic range textures, targeted for hardware implementation. Our method encodes images at a constant 8 bits per pixel, for a compression ratio of 6:1. We demonstrate that our method achieves good visual fidelity, surpassing DXTC texture compression of RGBE data which is the most practical method on existing graphics hardware. The decoding logic for our method is simple enough to be implemented as part of the texture fetch unit in graphics hardware.	High dynamic range texture compression	NA:NA:NA	2018
Rafał Mantiuk:Alexander Efremov:Karol Myszkowski:Hans-Peter Seidel	To embrace the imminent transition from traditional low-contrast video (LDR) content to superior high dynamic range (HDR) content, we propose a novel backward compatible HDR video compression (HDR MPEG) method. We introduce a compact reconstruction function that is used to decompose an HDR video stream into a residual stream and a standard LDR stream, which can be played on existing MPEG decoders, such as DVD players. The reconstruction function is finely tuned to the content of each HDR frame to achieve strong decorrelation between the LDR and residual streams, which minimizes the amount of redundant information. The size of the residual stream is further reduced by removing invisible details prior to compression using our HDR-enabled filter, which models luminance adaptation, contrast sensitivity, and visual masking based on the HDR content. Designed especially for DVD movie distribution, our HDR MPEG compression method features low storage requirements for HDR content resulting in a 30% size increase to an LDR video sequence. The proposed compression method does not impose restrictions or modify the appearance of the LDR or HDR video. This is important for backward compatibility of the LDR stream with current DVD appearance, and also enables independent fine tuning, tone mapping, and color grading of both streams.	Backward compatible high dynamic range MPEG video compression	NA:NA:NA:NA	2018
David Blythe	We present a system architecture for the 4th generation of PC-class programmable graphics processing units (GPUs). The new pipeline features significant additions and changes to the prior generation pipeline including a new programmable stage capable of generating additional primitives and streaming primitive data to memory, an expanded, common feature set for all of the programmable stages, generalizations to vertex and image memory resources, and new storage formats. We also describe structural modifications to the API, runtime, and shading language to complement the new pipeline. We motivate the design with descriptions of frequently encountered obstacles in current systems. Throughout the paper we present rationale behind prominent design choices and alternatives that were ultimately rejected, drawing on insights collected during a multi-year collaboration with application developers and hardware designers.	The Direct3D 10 system	NA	2018
Holly Rushmeier	NA	Session details: Appearance representation	NA	2018
Jason Lawrence:Aner Ben-Artzi:Christopher DeCoro:Wojciech Matusik:Hanspeter Pfister:Ravi Ramamoorthi:Szymon Rusinkiewicz	Recent progress in the measurement of surface reflectance has created a demand for non-parametric appearance representations that are accurate, compact, and easy to use for rendering. Another crucial goal, which has so far received little attention, is editability: for practical use, we must be able to change both the directional and spatial behavior of surface reflectance (e.g., making one material shinier, another more anisotropic, and changing the spatial "texture maps" indicating where each material appears). We introduce an Inverse Shade Tree framework that provides a general approach to estimating the "leaves" of a user-specified shade tree from high-dimensional measured datasets of appearance. These leaves are sampled 1- and 2-dimensional functions that capture both the directional behavior of individual materials and their spatial mixing patterns. In order to compute these shade trees automatically, we map the problem to matrix factorization and introduce a flexible new algorithm that allows for constraints such as non-negativity, sparsity, and energy conservation. Although we cannot infer every type of shade tree, we demonstrate the ability to reduce multi-gigabyte measured datasets of the Spatially-Varying Bidirectional Reflectance Distribution Function (SVBRDF) into a compact representation that may be edited in real time.	Inverse shade trees for non-parametric material representation and editing	NA:NA:NA:NA:NA:NA:NA	2018
Pieter Peers:Karl vom Berge:Wojciech Matusik:Ravi Ramamoorthi:Jason Lawrence:Szymon Rusinkiewicz:Philip Dutré	Many translucent materials exhibit heterogeneous subsurface scattering, which arises from complex internal structures. The acquisition and representation of these scattering functions is a complex problem that has been only partially addressed in previous techniques. Unlike homogeneous materials, the spatial component of heterogeneous subsurface scattering can vary arbitrarily over surface locations. Storing the spatial component without compression leads to impractically large datasets. In this paper, we address the problem of acquiring and compactly representing the spatial component of heterogeneous subsurface scattering functions. We propose a material model based on matrix factorization that can be mapped onto arbitrary geometry, and, due to its compact form, can be incorporated into most visualization systems with little overhead. We present results of several real-world datasets that are acquired using a projector and a digital camera.	A compact factored representation of heterogeneous subsurface scattering	NA:NA:NA:NA:NA:NA:NA	2018
Jiaping Wang:Xin Tong:Stephen Lin:Minghao Pan:Chao Wang:Hujun Bao:Baining Guo:Heung-Yeung Shum	We present a visual simulation technique called appearance manifolds for modeling the time-variant surface appearance of a material from data captured at a single instant in time. In modeling time-variant appearance, our method takes advantage of the key observation that concurrent variations in appearance over a surface represent different degrees of weathering. By reorganizing these various appearances in a manner that reveals their relative order with respect to weathering degree, our method infers spatial and temporal appearance properties of the material's weathering process that can be used to convincingly generate its weathered appearance at different points in time. Results with natural non-linear reflectance variations are demonstrated in applications such as visual simulation of weathering on 3D models, increasing and decreasing the weathering of real objects, and material transfer with weathering effects.	Appearance manifolds for modeling time-variant appearance of materials	NA:NA:NA:NA:NA:NA:NA:NA	2018
Jinwei Gu:Chien-I Tu:Ravi Ramamoorthi:Peter Belhumeur:Wojciech Matusik:Shree Nayar	For computer graphics rendering, we generally assume that the appearance of surfaces remains static over time. Yet, there are a number of natural processes that cause surface appearance to vary dramatically, such as burning of wood, wetting and drying of rock and fabric, decay of fruit skins, and corrosion and rusting of steel and copper. In this paper, we take a significant step towards measuring, modeling, and rendering time-varying surface appearance. We describe the acquisition of the first time-varying database of 26 samples, encompassing a variety of natural processes including burning, drying, decay, and corrosion. Our main technical contribution is a Space-Time Appearance Factorization (STAF). This model factors space and time-varying effects. We derive an overall temporal appearance variation characteristic curve of the specific process, as well as space-dependent textures, rates, and offsets. This overall temporal curve controls different spatial locations evolve at the different rates, causing spatial patterns on the surface over time. We show that the model accurately represents a variety of phenomena. Moreover, it enables a number of novel rendering applications, such as transfer of the time-varying effect to a new static surface, control to accelerate time evolution in certain areas, extrapolation beyond the acquired sequence, and texture synthesis of time-varying appearance.	Time-varying surface appearance: acquisition, modeling and rendering	NA:NA:NA:NA:NA:NA	2018
Paul Debevec	NA	Session details: Matting & deblurring	NA	2018
Jian Sun:Yin Li:Sing Bing Kang:Heung-Yeung Shum	In this paper, we propose a novel approach to extract mattes using a pair of flash/no-flash images. Our approach, which we call flash matting, was inspired by the simple observation that the most noticeable difference between the flash and no-flash images is the foreground object if the background scene is sufficiently distant. We apply a new matting algorithm called joint Bayesian flash matting to robustly recover the matte from flash/no-flash images, even for scenes in which the foreground and the background are similar or the background is complex. Experimental results involving a variety of complex indoors and outdoors scenes show that it is easy to extract high-quality mattes using an off-the-shelf, flash-equipped camera. We also describe extensions to flash matting for handling more general scenes.	Flash matting	NA:NA:NA:NA	2018
Neel Joshi:Wojciech Matusik:Shai Avidan	We present an algorithm and a system for high-quality natural video matting using a camera array. The system uses high frequencies present in natural scenes to compute mattes by creating a synthetic aperture image that is focused on the foreground object, which reduces the variance of pixels reprojected from the foreground while increasing the variance of pixels reprojected from the background. We modify the standard matting equation to work directly with variance measurements and show how these statistics can be used to construct a trimap that is later upgraded to an alpha matte. The entire process is completely automatic, including an automatic method for focusing the synthetic aperture image on the foreground object and an automatic method to compute the trimap and the alpha matte. The proposed algorithm is very efficient and has a per-pixel running time that is linear in the number of cameras. Our current system runs at several frames per second, and we believe that it is the first system capable of computing high-quality alpha mattes at near real-time rates without the use of active illumination or special backgrounds.	Natural video matting using camera arrays	NA:NA:NA	2018
Rob Fergus:Barun Singh:Aaron Hertzmann:Sam T. Roweis:William T. Freeman	Camera shake during exposure leads to objectionable image blur and ruins many photographs. Conventional blind deconvolution methods typically assume frequency-domain constraints on images, or overly simplified parametric forms for the motion path during camera shake. Real camera motions can follow convoluted paths, and a spatial domain prior can better maintain visually salient image characteristics. We introduce a method to remove the effects of camera shake from seriously blurred images. The method assumes a uniform camera blur over the image and negligible in-plane camera rotation. In order to estimate the blur from the camera shake, the user must specify an image region without saturation effects. We show results for a variety of digital photographs taken from personal photo collections.	Removing camera shake from a single photograph	NA:NA:NA:NA:NA	2018
Ramesh Raskar:Amit Agrawal:Jack Tumblin	In a conventional single-exposure photograph, moving objects or moving cameras cause motion blur. The exposure time defines a temporal box filter that smears the moving object across the image by convolution. This box filter destroys important high-frequency spatial details so that deblurring via deconvolution becomes an ill-posed problem.Rather than leaving the shutter open for the entire exposure duration, we "flutter" the camera's shutter open and closed during the chosen exposure time with a binary pseudo-random sequence. The flutter changes the box filter to a broad-band filter that preserves high-frequency spatial details in the blurred image and the corresponding deconvolution becomes a well-posed problem. We demonstrate that manually-specified point spread functions are sufficient for several challenging cases of motion-blur removal including extremely large motions, textured backgrounds and partial occluders.	Coded exposure photography: motion deblurring using fluttered shutter	NA:NA:NA	2018
Ming Lin	NA	Session details: Fluids	NA	2018
Geoffrey Irving:Eran Guendelman:Frank Losasso:Ronald Fedkiw	We present a new method for the efficient simulation of large bodies of water, especially effective when three-dimensional surface effects are important. Similar to a traditional two-dimensional height field approach, most of the water volume is represented by tall cells which are assumed to have linear pressure profiles. In order to avoid the limitations typically associated with a height field approach, we simulate the entire top surface of the water volume with a state of the art, fully three-dimensional Navier-Stokes free surface solver. Our philosophy is to use the best available method near the interface (in the three-dimensional region) and to coarsen the mesh away from the interface for efficiency. We coarsen with tall, thin cells (as opposed to octrees or AMR), because they maintain good resolution horizontally allowing for accurate representation of bottom topography.	Efficient simulation of large bodies of water by coupling two and three dimensional techniques	NA:NA:NA:NA	2018
Frank Losasso:Tamar Shinar:Andrew Selle:Ronald Fedkiw	The particle level set method has proven successful for the simulation of two separate regions (such as water and air, or fuel and products). In this paper, we propose a novel approach to extend this method to the simulation of as many regions as desired. The various regions can be liquids (or gases) of any type with differing viscosities, densities, viscoelastic properties, etc. We also propose techniques for simulating interactions between materials, whether it be simple surface tension forces or more complex chemical reactions with one material converting to another or two materials combining to form a third. We use a separate particle level set method for each region, and propose a novel projection algorithm that decodes the resulting vector of level set values providing a "dictionary" that translates between them and the standard single-valued level set representation. An additional difficulty occurs since discretization stencils (for interpolation, tracing semi-Lagrangian rays, etc.) cross region boundaries naively combining non-smooth or even discontinuous data. This has recently been addressed via ghost values, e.g. for fire or bubbles. We instead propose a new paradigm that allows one to incorporate physical jump conditions in data "on the fly," which is significantly more efficient for multiple regions especially at triple points or near boundaries with solids.	Multiple interacting liquids	NA:NA:NA:NA	2018
Bryan M. Klingner:Bryan E. Feldman:Nuttapong Chentanez:James F. O'Brien	This paper presents a method for animating fluid using unstructured tetrahedral meshes that change at each time step. We show that meshes that conform well to changing boundaries and that focus computation in the visually important parts of the domain can be generated quickly and reliably using existing techniques. We also describe a new approach to two-way coupling of fluid and rigid bodies that, while general, benefits from remeshing. Overall, the method provides a flexible environment for creating complex scenes involving fluid animation.	Fluid animation with dynamic meshes	NA:NA:NA:NA	2018
Adrien Treuille:Andrew Lewis:Zoran Popović	We present a new model reduction approach to fluid simulation, enabling large, real-time, detailed flows with continuous user interaction. Our reduced model can also handle moving obstacles immersed in the flow. We create separate models for the velocity field and for each moving boundary, and show that the coupling forces may be reduced as well. Our results indicate that surprisingly few basis functions are needed to resolve small but visually important features such as spinning vortices.	Model reduction for real-time fluids	NA:NA:NA	2018
Wojciech Matusik	NA	Session details: Image collections	NA	2018
Noah Snavely:Steven M. Seitz:Richard Szeliski	We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.	Photo tourism: exploring photo collections in 3D	NA:NA:NA	2018
Carsten Rother:Lucas Bordeaux:Youssef Hamadi:Andrew Blake	The paper defines an automatic procedure for constructing a visually appealing collage from a collection of input images. The aim is that the resulting collage should be representative of the collection, summarising its main themes. It is also assembled largely seamlessly, using graph-cut, Poisson blending of alpha-masks, to hide the joins between input images. This paper makes several new contributions. Firstly, we show how energy terms can be included that: encourage the selection of a representative set of images; that are sensitive to particular object classes; that encourage a spatially efficient and seamless layout. Secondly the resulting optimization poses a search problem that, on the face of it, is computationally in-feasible. Rather than attempt an expensive, integrated optimization procedure, we have developed a sequence of optimization steps, from static ranking of images, through region of interest optimization, optimal packing by constraint satisfaction, and lastly graph-cut alpha-expansion. To illustrate the power of AutoCollage, we have used it to create collages of many home photo sets; we also conducted a user study in which AutoCollage outperformed competitive methods.	AutoCollage	NA:NA:NA:NA	2018
Aseem Agarwala:Maneesh Agrawala:Michael Cohen:David Salesin:Richard Szeliski	We present a system for producing multi-viewpoint panoramas of long, roughly planar scenes, such as the facades of buildings along a city street, from a relatively sparse set of photographs captured with a handheld still camera that is moved along the scene. Our work is a significant departure from previous methods for creating multi-viewpoint panoramas, which composite thin vertical strips from a video sequence captured by a translating video camera, in that the resulting panoramas are composed of relatively large regions of ordinary perspective. In our system, the only user input required beyond capturing the photographs themselves is to identify the dominant plane of the photographed scene; our system then computes a panorama automatically using Markov Random Field optimization. Users may exert additional control over the appearance of the result by drawing rough strokes that indicate various high-level goals. We demonstrate the results of our system on several scenes, including urban streets, a river bank, and a grocery store aisle.	Photographing long scenes with multi-viewpoint panoramas	NA:NA:NA:NA:NA	2018
Dan B Goldman:Brian Curless:David Salesin:Steven M. Seitz	We present a method for visualizing short video clips in a single static image, using the visual language of storyboards. These schematic storyboards are composed from multiple input frames and annotated using outlines, arrows, and text describing the motion in the scene. The principal advantage of this storyboard representation over standard representations of video -- generally either a static thumbnail image or a playback of the video clip in its entirety -- is that it requires only a moment to observe and comprehend but at the same time retains much of the detail of the source video. Our system renders a schematic storyboard layout based on a small amount of user interaction. We also demonstrate an interaction technique to scrub through time using the natural spatial dimensions of the storyboard. Potential applications include video editing, surveillance summarization, assembly instructions, composition of graphic novels, and illustration of camera technique for film studies.	Schematic storyboarding for video visualization and editing	NA:NA:NA:NA	2018
Doug L. James	NA	Session details: Motion capture	NA	2018
Paul G. Kry:Dinesh K. Pai	Modifying motion capture to satisfy the constraints of new animation is difficult when contact is involved, and a critical problem for animation of hands. The compliance with which a character makes contact also reveals important aspects of the movement's purpose. We present a new technique called interaction capture, for capturing these contact phenomena. We capture contact forces at the same time as motion, at a high rate, and use both to estimate a nominal reference trajectory and joint compliance. Unlike traditional methods, our method estimates joint compliance without the need for motorized perturbation devices. New interactions can then be synthesized by physically based simulation. We describe a novel position-based linear complementarity problem formulation that includes friction, breaking contact, and the compliant coupling between contacts at different fingers. The technique is validated using data from previous work and our own perturbation-based estimates.	Interaction capture and synthesis	NA:NA	2018
Sang Il Park:Jessica K. Hodgins	During dynamic activities, the surface of the human body moves in many subtle but visually significant ways: bending, bulging, jiggling, and stretching. We present a technique for capturing and animating those motions using a commercial motion capture system and approximately 350 markers. Although the number of markers is significantly larger than that used in conventional motion capture, it is only a sparse representation of the true shape of the body. We supplement this sparse sample with a detailed, actor-specific surface model. The motion of the skin can then be computed by segmenting the markers into the motion of a set of rigid parts and a residual deformation (approximated first as a quadratic transformation and then with radial basis functions). We demonstrate the power of this approach by capturing flexing muscles, high frequency motions, and abrupt decelerations on several actors. We compare these results both to conventional motion capture and skinning and to synchronized video of the actors.	Capturing and animating skin deformation in human motion	NA:NA	2018
Okan Arikan	We present a lossy compression algorithm for large databases of motion capture data. We approximate short clips of motion using Bezier curves and clustered principal component analysis. This approximation has a smoothing effect on the motion. Contacts with the environment (such as foot strikes) have important detail that needs to be maintained. We compress these environmental contacts using a separate, JPEG like compression algorithm and ensure these contacts are maintained during decompression.Our method can compress 6 hours 34 minutes of human motion capture from 1080 MB data into 35.5 MB with little visible degradation. Compression and decompression is fast: our research implementation can decompress at about 1.2 milliseconds/frame, 7 times faster than real-time (for 120 frames per second animation). Our method also yields smaller compressed representation for the same error or produces smaller error for the same compressed size.	Compression of motion capture databases	NA	2018
Kang Hoon Lee:Myung Geol Choi:Jehee Lee	Real time animation of human figures in virtual environments is an important problem in the context of computer games and virtual environments. Recently, the use of large collections of captured motion data has increased realism in character animation. However, assuming that the virtual environment is large and complex, the effort of capturing motion data in a physical environment and adapting them to an extended virtual environment is the bottleneck for achieving interactive character animation and control. We present a new technique for allowing our animated characters to navigate through a large virtual environment, which is constructed using a set of building blocks. The building blocks, called motion patches, can be arbitrarily assembled to create novel environments. Each patch is annotated with motion data, which informs what actions are available for animated characters within the block. The versatility and flexibility of our approach are demonstrated through examples in which multiple characters are animated and controlled at interactive rates in large, complex virtual environments.	Motion patches: building blocks for virtual environments annotated with motion data	NA:NA:NA	2018
Szymon Rusinkiewicz	NA	Session details: Image capture	NA	2018
Li Zhang:Shree Nayar	In order to produce bright images, projectors have large apertures and hence narrow depths of field. In this paper, we present methods for robust scene capture and enhanced image display based on projection defocus analysis. We model a projector's defocus using a linear system. This model is used to develop a novel temporal defocus analysis method to recover depth at each camera pixel by estimating the parameters of its projection defocus kemel in frequency domain. Compared to most depth recovery methods, our approach is more accurate near depth discontinuities. Furthermore, by using a coaxial projector-camera system, we ensure that depth is computed at all camera pixels, without any missing parts. We show that the recovered scene geometry can be used for refocus synthesis and for depth-based image composition. Using the same projector defocus model and estimation technique, we also propose a defocus compensation method that filters a projection image in a spatially-varying, depth-dependent manner to minimize its defocus blur after it is projected onto the scene. This method effectively increases the depth of field of a projector without modifying its optics. Finally, we present an algorithm that exploits projector defocus to reduce the strong pixelation artifacts produced by digital projectors, while preserving the quality of the projected image. We have experimentally verified each of our methods using real scenes.	Projection defocus analysis for scene capture and image display	NA:NA	2018
Sujit Kuthirummal:Shree K. Nayar	In this paper, we present a class of imaging systems, called radial imaging systems, that capture a scene from a large number of view-points within a single image, using a camera and a curved mirror. These systems can recover scene properties such as geometry, reflectance, and texture. We derive analytic expressions that describe the properties of a complete family of radial imaging systems, including their loci of viewpoints, fields of view, and resolution characteristics. We have built radial imaging systems that, from a single image, recover the frontal 3D structure of an object, generate the complete texture map of a convex object, and estimate the parameters of an analytic BRDF model for an isotropic material. In addition, one of our systems can recover the complete geometry of a convex object by capturing only two images. These results show that radial imaging systems are simple, effective, and convenient devices for a wide range of applications in computer graphics and computer vision.	Multiview radial catadioptric imaging for scene capture	NA:NA	2018
Marc Levoy:Ren Ng:Andrew Adams:Matthew Footer:Mark Horowitz	By inserting a microlens array into the optical train of a conventional microscope, one can capture light fields of biological specimens in a single photograph. Although diffraction places a limit on the product of spatial and angular resolution in these light fields, we can nevertheless produce useful perspective views and focal stacks from them. Since microscopes are inherently orthographic devices, perspective views represent a new way to look at microscopic specimens. The ability to create focal stacks from a single photograph allows moving or light-sensitive specimens to be recorded. Applying 3D deconvolution to these focal stacks, we can produce a set of cross sections, which can be visualized using volume rendering. In this paper, we demonstrate a prototype light field microscope (LFM), analyze its optical performance, and show perspective views, focal stacks, and reconstructed volumes for a variety of biological specimens. We also show that synthetic focusing followed by 3D deconvolution is equivalent to applying limited-angle tomography directly to the 4D light field.	Light field microscopy	NA:NA:NA:NA:NA	2018
Shree K. Nayar:Gurunandan Krishnan:Michael D. Grossberg:Ramesh Raskar	We present fast methods for separating the direct and global illumination components of a scene measured by a camera and illuminated by a light source. In theory, the separation can be done with just two images taken with a high frequency binary illumination pattern and its complement. In practice, a larger number of images are used to overcome the optical and resolution limitations of the camera and the source. The approach does not require the material properties of objects and media in the scene to be known. However, we require that the illumination frequency is high enough to adequately sample the global components received by scene points. We present separation results for scenes that include complex interreflections, subsurface scattering and volumetric scattering. Several variants of the separation approach are also described. When a sinusoidal illumination pattern is used with different phase shifts, the separation can be done using just three images. When the computed images are of lower resolution than the source and the camera, smoothness constraints are used to perform the separation using a single image. Finally, in the case of a static scene that is lit by a simple point source, such as the sun, a moving occluder and a video camera can be used to do the separation. We also show several simple examples of how novel images of a scene can be computed from the separation results.	Fast separation of direct and global components of a scene using high frequency illumination	NA:NA:NA:NA	2018
Fabio Pellacini	NA	Session details: Precomputed transfer	NA	2018
Aner Ben-Artzi:Ryan Overbeck:Ravi Ramamoorthi	Current systems for editing BRDFs typically allow users to adjust analytic parameters while visualizing the results in a simplified setting (e.g. unshadowed point light). This paper describes a real-time rendering system that enables interactive edits of BRDFs, as rendered in their final placement on objects in a static scene, lit by direct, complex illumination. All-frequency effects (ranging from near-mirror reflections and hard shadows to diffuse shading and soft shadows) are rendered using a precomputation-based approach. Inspired by real-time relighting methods, we create a linear system that fixes lighting and view to allow real-time BRDF manipulation. In order to linearize the image's response to BRDF parameters, we develop an intermediate curve-based representation, which also reduces the rendering and precomputation operations to 1D while maintaining accuracy for a very general class of BRDFs. Our system can be used to edit complex analytic BRDFs (including anisotropic models), as well as measured reflectance data. We improve on the standard precomputed radiance transfer (PRT) rendering computation by introducing an incremental rendering algorithm that takes advantage of frame-to-frame coherence. We show that it is possible to render reference-quality images while only updating 10% of the data at each frame, sustaining frame-rates of 25-30fps.	Real-time BRDF editing in complex lighting	NA:NA:NA	2018
Weifeng Sun:Amar Mukherjee	We consider real-time rendering of dynamic glossy objects with realistic shadows under distant all-frequency environment lighting. Previous PRT approaches pre-compute light transport for a fixed scene and cannot account for cast shadows on high-glossy objects occluded by dynamic neighbors. In this paper, we extend double/triple product integral to generalized multi-function product integral. We represent shading integral at each vertex as the product integral of multiple functions, involving the lighting, BRDF, local visibility and dynamic occlusions. Our main contribution is a new mathematical representation and analysis of multi-function product integral in the wavelet domain. We show that multi-function product integral in the primal corresponds to the summation of the product of basis coefficients and integral coefficients. We propose a novel generalized Haar integral coefficient theorem to evaluate arbitrary Haar integral coefficients. We present an efficient sub-linear algorithm to render dynamic glossy objects under time-variant all-frequency lighting and arbitrary view conditions in a few seconds on a commodity CPU, orders of magnitude faster than previous techniques. To further accelerate shadow computation, we propose a Just-in-time Radiance Transfer (JRT) technique. JRT is a new generalization to PRT for dynamic scenes. It is compact and flexible, and supports glossy materials. By pre-computing radiance transfer vectors at runtime, we demonstrate rendering dynamic view-dependent all-frequency shadows in real-time.	Generalized wavelet product integral for rendering dynamic glossy objects	NA:NA	2018
Yu-Ting Tsai:Zen-Chung Shih	This paper introduces a new data representation and compression technique for precomputed radiance transfer (PRT). The light transfer functions and light sources are modeled with spherical radial basis functions (SRBFs). A SRBF is a rotation-invariant function that depends on the geodesic distance between two points on the unit sphere. Rotating functions in SRBF representation is as straightforward as rotating the centers of SRBFs. Moreover, high-frequency signals are handled by adjusting the bandwidth parameters of SRBFs. To exploit inter-vertex coherence, the light transfer functions are further classified iteratively into disjoint clusters, and tensor approximation is applied within each cluster. Compared with previous methods, the proposed approach enables real-time rendering with comparable quality under high-frequency lighting environments. The data storage is also more compact than previous all-frequency PRT algorithms.	All-frequency precomputed radiance transfer using spherical radial basis functions and clustered tensor approximation	NA:NA	2018
Zhong Ren:Rui Wang:John Snyder:Kun Zhou:Xinguo Liu:Bo Sun:Peter-Pike Sloan:Hujun Bao:Qunsheng Peng:Baining Guo	Previous methods for soft shadows numerically integrate over many light directions at each receiver point, testing blocker visibility in each direction. We introduce a method for real-time soft shadows in dynamic scenes illuminated by large, low-frequency light sources where such integration is impractical. Our method operates on vectors representing low-frequency visibility of blockers in the spherical harmonic basis. Blocking geometry is modeled as a set of spheres; relatively few spheres capture the low-frequency blocking effect of complicated geometry. At each receiver point, we compute the product of visibility vectors for these blocker spheres as seen from the point. Instead of computing an expensive SH product per blocker as in previous work, we perform inexpensive vector sums to accumulate the log of blocker visibility. SH exponentiation then yields the product visibility vector over all blockers. We show how the SH exponentiation required can be approximated accurately and efficiently for low-order SH, accelerating previous CPU-based methods by a factor of 10 or more, depending on blocker complexity, and allowing real-time GPU implementation.	Real-time soft shadows in dynamic scenes using spherical harmonic exponentiation	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Doug L. James:Jernej Barbič:Dinesh K. Pai	Simulating sounds produced by realistic vibrating objects is challenging because sound radiation involves complex diffraction and interreflection effects that are very perceptible and important. These wave phenomena are well understood, but have been largely ignored in computer graphics due to the high cost and complexity of computing them at audio rates.We describe a new algorithm for real-time synthesis of realistic sound radiation from rigid objects. We start by precomputing the linear vibration modes of an object, and then relate each mode to its sound pressure field, or acoustic transfer function, using standard methods from numerical acoustics. Each transfer function is then approximated to a specified accuracy using low-order multi-pole sources placed near the object. We provide a low-memory, multilevel, randomized algorithm for optimized source placement that is suitable for complex geometries. At runtime, we can simulate new interaction sounds by quickly summing contributions from each mode's equivalent multipole sources. We can efficiently simulate global effects such as interreflection and changes in sound due to listener location. The simulation costs can be dynamically traded-off for sound quality. We present several examples of sound generation from physically based animations.	Precomputed acoustic transfer: output-sensitive, accurate sound generation for geometrically complex vibration sources	NA:NA:NA	2018
Philip Dutré	NA	Session details: Appearance modeling	NA	2018
Kshitiz Garg:Shree K. Nayar	Photorealistic rendering of rain streaks with lighting and viewpoint effects is a challenging problem. Raindrops undergo rapid shape distortions as they fall, a phenomenon referred to as oscillations. Due to these oscillations, the reflection of light by, and the refraction of light through, a falling raindrop produce complex brightness patterns within a single motion-blurred rain streak captured by a camera or observed by a human. The brightness pattern of a rain streak typically includes speckles, multiple smeared highlights and curved brightness contours. In this work, we propose a new model for rain streak appearance that captures the complex interactions between the lighting direction, the viewing direction and the oscillating shape of the drop. Our model builds upon a raindrop oscillation model that has been developed in atmospheric sciences. We have measured rain streak appearances under a wide range of lighting and viewing conditions and empirically determined the oscillation parameters that are dominant in raindrops. Using these parameters, we have rendered thousands of rain streaks to create a database that captures the variations in streak appearance with respect to lighting and viewing directions. We have developed an efficient image-based rendering algorithm that uses our streak database to add rain to a single image or a captured video with moving objects and sources. The rendering algorithm is very simple to use as it only requires a coarse depth map of the scene and the locations and properties of the light sources. We have rendered rain in a wide range of scenarios and the results show that our physically-based rain streak model greatly enhances the visual realism of rendered rain.	Photorealistic rendering of rain streaks	NA:NA	2018
Srinivasa G. Narasimhan:Mohit Gupta:Craig Donner:Ravi Ramamoorthi:Shree K. Nayar:Henrik Wann Jensen	The visual world around us displays a rich set of volumetric effects due to participating media. The appearance of these media is governed by several physical properties such as particle densities, shapes and sizes, which must be input (directly or indirectly) to a rendering algorithm to generate realistic images. While there has been significant progress in developing rendering techniques (for instance, volumetric Monte Carlo methods and analytic approximations), there are very few methods that measure or estimate these properties for media that are of relevance to computer graphics. In this paper, we present a simple device and technique for robustly estimating the properties of a broad class of participating media that can be either (a) diluted in water such as juices, beverages, paints and cleaning supplies, or (b) dissolved in water such as powders and sugar/salt crystals, or (c) suspended in water such as impurities. The key idea is to dilute the concentrations of the media so that single scattering effects dominate and multiple scattering becomes negligible, leading to a simple and robust estimation algorithm. Furthermore, unlike previous approaches that require complicated or separate measurement setups for different types or properties of media, our method and setup can be used to measure media with a complete range of absorption and scattering properties from a single HDR photograph. Once the parameters of the diluted medium are estimated, a volumetric Monte Carlo technique may be used to create renderings of any medium concentration and with multiple scattering. We have measured the scattering parameters of forty commonly found materials, that can be immediately used by the computer graphics community. We can also create realistic images of combinations or mixtures of the original measured materials, thus giving the user a wide flexibility in making realistic images of participating media.	Acquiring scattering properties of participating media by dilution	NA:NA:NA:NA:NA:NA	2018
Tim Weyrich:Wojciech Matusik:Hanspeter Pfister:Bernd Bickel:Craig Donner:Chien Tu:Janet McAndless:Jinho Lee:Addy Ngan:Henrik Wann Jensen:Markus Gross	We have measured 3D face geometry, skin reflectance, and subsurface scattering using custom-built devices for 149 subjects of varying age, gender, and race. We developed a novel skin reflectance model whose parameters can be estimated from measurements. The model decomposes the large amount of measured skin data into a spatially-varying analytic BRDF, a diffuse albedo map, and diffuse subsurface scattering. Our model is intuitive, physically plausible, and -- since we do not use the original measured data -- easy to edit as well. High-quality renderings come close to reproducing real photographs. The analysis of the model parameters for our sample population reveals variations according to subject age, gender, skin type, and external factors (e.g., sweat, cold, or makeup). Using our statistics, a user can edit the overall appearance of a face (e.g., changing skin type and age) or change small-scale features using texture synthesis (e.g., adding moles and freckles). We are making the collected statistics publicly available to the research community for applications in face synthesis and analysis.	Analysis of human faces using a measurement-based skin reflectance model	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Aleksey Golovinskiy:Wojciech Matusik:Hanspeter Pfister:Szymon Rusinkiewicz:Thomas Funkhouser	Detailed surface geometry contributes greatly to the visual realism of 3D face models. However, acquiring high-resolution face geometry is often tedious and expensive. Consequently, most face models used in games, virtual reality, or computer vision look unrealistically smooth. In this paper, we introduce a new statistical technique for the analysis and synthesis of small three-dimensional facial features, such as wrinkles and pores. We acquire high-resolution face geometry for people across a wide range of ages, genders, and races. For each scan, we separate the skin surface details from a smooth base mesh using displaced subdivision surfaces. Then, we analyze the resulting displacement maps using the texture analysis/synthesis framework of Heeger and Bergen, adapted to capture statistics that vary spatially across a face. Finally, we use the extracted statistics to synthesize plausible detail on face meshes of arbitrary subjects. We demonstrate the effectiveness of this method in several applications, including analysis of facial texture in subjects with different ages and genders, interpolation between high-resolution face scans, adding detail to low-resolution face scans, and adjusting the apparent age of faces. In all cases, we are able to re-produce fine geometric details consistent with those observed in high resolution scans.	A statistical model for synthesis of detailed facial geometry	NA:NA:NA:NA:NA	2018
Joe Warren	NA	Session details: Meshes	NA	2018
Adi Levin	We present a modification to subdivision surfaces, which guarantees second-order smoothness everywhere in the surface, including extraordinary points. The idea is to blend the limit surface with a low degree polynomial defined over the characteristic map, in the vicinity of each extraordinary point. We demonstrate our method on Catmull-Clark surfaces, but a similar modification can be applied to other schemes as well. The proposed modification to Catmull-Clark is simple to implement and can be applied to quad meshes of arbitrary topological type, even when extraordinary vertices share edges.	Modified subdivision surfaces with continuous curvature	NA	2018
Ke Wang:Weiwei:Yiying Tong:Mathieu Desbrun:Peter Schröder	Vertex- and face-based subdivision schemes are now routinely used in geometric modeling and computational science, and their primal/dual relationships are well studied. In this paper, we interpret these schemes as defining bases for discrete differential 0- resp. 2-forms, and complete the picture by introducing edge-based subdivision schemes to construct the missing bases for discrete differential 1-forms. Such subdivision schemes map scalar coefficients on edges from the coarse to the refined mesh and are intrinsic to the surface. Our construction is based on treating vertex-, edge-, and face-based subdivision schemes as a joint triple and enforcing that subdivision commutes with the topological exterior derivative. We demonstrate our construction for the case of arbitrary topology triangle meshes. Using Loop's scheme for 0-forms and generalized half-box splines for 2-forms results in a unique generalized spline scheme for 1-forms, easily incorporated into standard subdivision surface codes. We also provide corresponding boundary stencils. Once a metric is supplied, the scalar 1-form coefficients define a smooth tangent vector field on the underlying subdivision surface. Design of tangent vector fields is made particularly easy with this machinery as we demonstrate.	Edge subdivision schemes and the construction of smooth vector fields	NA:NA:NA:NA:NA	2018
Martin Isenburg:Yuanxin Liu:Jonathan Shewchuk:Jack Snoeyink	We show how to greatly accelerate algorithms that compute Delaunay triangulations of huge, well-distributed point sets in 2D and 3D by exploiting the natural spatial coherence in a stream of points. We achieve large performance gains by introducing spatial finalization into point streams: we partition space into regions, and augment a stream of input points with finalization tags that indicate when a point is the last in its region. By extending an incremental algorithm for Delaunay triangulation to use finalization tags and produce streaming mesh output, we compute a billion-triangle terrain representation for the Neuse River system from 11.2 GB of LIDAR data in 48 minutes using only 70 MB of memory on a laptop with two hard drives. This is a factor of twelve faster than the previous fastest out-of-core Delaunay triangulation software.	Streaming computation of Delaunay triangulations	NA:NA:NA:NA	2018
Shen Dong:Peer-Timo Bremer:Michael Garland:Valerio Pascucci:John C. Hart	Resampling raw surface meshes is one of the most fundamental operations used by nearly all digital geometry processing systems. The vast majority of this work has focused on triangular remeshing, yet quadrilateral meshes are preferred for many surface PDE problems, especially fluid dynamics, and are best suited for defining Catmull-Clark subdivision surfaces. We describe a fundamentally new approach to the quadrangulation of manifold polygon meshes using Laplacian eigenfunctions, the natural harmonics of the surface. These surface functions distribute their extrema evenly across a mesh, which connect via gradient flow into a quadrangular base mesh. An iterative relaxation algorithm simultaneously refines this initial complex to produce a globally smooth parameterization of the surface. From this, we can construct a well-shaped quadrilateral mesh with very few extraordinary vertices. The quality of this mesh relies on the initial choice of eigenfunction, for which we describe algorithms and hueristics to efficiently and effectively select the harmonic most appropriate for the intended application.	Spectral surface quadrangulation	NA:NA:NA:NA:NA	2018
Ravi Ramamoorthi	NA	Session details: Light transport	NA	2018
Jonathan T. Moon:Stephen R. Marschner	Simulating multiple scattering correctly is important for accurate rendering of hair. However, a volume of hair is a difficult scene to simulate because scattering from an individual fiber is very structured and forward directed, and because the radiance distributions that arise from many such scattering events remain quite directional. For these reasons, previous methods cannot compute accurate images substantially faster than Monte Carlo path tracing.This paper proposes a new physically accurate method for rendering hair that is based on previous volumetric photon mapping methods. The first pass generates a photon map by tracing particles through the hair geometry, depositing them along paths rather than at scattering events. The second pass ray traces the hair, computing direct illumination and looking up indirect radiance in the photon map. Photons are stored and looked up in 5D position-direction space to allow for the very directional radiance distributions that occur in hair. Together with a new radiance caching method for fibers, our method simulates difficult scattering problems in hair efficiently and with low noise.The new algorithm is validated against path tracing and also compared with a photograph of light scattering in real hair.	Simulating multiple scattering in hair using a photon mapping approach	NA:NA	2018
Mark Meyer:John Anderson	Global illumination provides important visual cues to an animation, however its computational expense limits its use in practice. In this paper, we present an easy to implement technique for accelerating the computation of indirect illumination for an animated sequence using stochastic ray tracing. We begin by computing a quick but noisy solution using a small number of sample rays at each sample location. The variation of these noisy solutions over time is then used to create a smooth basis. Finally, the noisy solutions are projected onto the smooth basis to produce the final solution. The resulting animation has greatly reduced spatial and temporal noise, and a computational cost roughly equivalent to the noisy, low sample computation.	Statistical acceleration for animated global illumination	NA:NA	2018
Bruce Walter:Adam Arbree:Kavita Bala:Donald P. Greenberg	Multidimensional lightcuts is a new scalable method for efficiently rendering rich visual effects such as motion blur, participating media, depth of field, and spatial anti-aliasing in complex scenes. It introduces a flexible, general rendering framework that unifies the handling of such effects by discretizing the integrals into large sets of gather and light points and adaptively approximating the sum of all possible gather-light pair interactions.We create an implicit hierarchy, the product graph, over the gather-light pairs to rapidly and accurately approximate the contribution from hundreds of millions of pairs per pixel while only evaluating a tiny fraction (e.g., 200--1,000). We build upon the techniques of the prior Lightcuts method for complex illumination at a point, however, by considering the complete pixel integrals, we achieve much greater efficiency and scalability.Our example results demonstrate efficient handling of volume scattering, camera focus, and motion of lights, cameras, and geometry. For example, enabling high quality motion blur with 256x temporal sampling requires only a 6.7x increase in shading cost in a scene with complex moving geometry, materials, and illumination.	Multidimensional lightcuts	NA:NA:NA:NA	2018
Miloš Hašan:Fabio Pellacini:Kavita Bala	This paper presents an interactive GPU-based system for cinematic relighting with multiple-bounce indirect illumination from a fixed view-point. We use a deep frame-buffer containing a set of view samples, whose indirect illumination is recomputed from the direct illumination on a large set of gather samples, distributed around the scene. This direct-to-indirect transfer is a linear transform which is particularly large, given the size of the view and gather sets. This makes it hard to precompute, store and multiply with. We address this problem by representing the transform as a set of sparse matrices encoded in wavelet space. A hierarchical construction is used to impose a wavelet basis on the unstructured gather cloud, and an image-based approach is used to map the sparse matrix computations to the GPU. We precompute the transfer matrices using a hierarchical algorithm and a variation of photon mapping in less than three hours on one processor. We achieve high-quality indirect illumination at 10-20 frames per second for complex scenes with over 2 million polygons, with diffuse and glossy materials, and arbitrary direct lighting models (expressed using shaders). We compute per-pixel indirect illumination without the need of irradiance caching or other subsampling techniques.	Direct-to-indirect transfer for cinematic relighting	NA:NA:NA	2018
Marc Alexa	NA	Session details: Shape deformation	NA	2018
Scott Kircher:Michael Garland	Deforming surfaces, such as cloth, can be generated through physical simulation, morphing, and even video capture. Such data is currently very difficult to alter after the generation process is complete, and data generated for one purpose generally cannot be adapted to other uses. Such adaptation would be extremely useful, however. Being able to take cloth captured from a flapping flag and attach it to a character to make a cape, or enhance the wrinkles on a simulated garment, would greatly enhance the usability and re-usability of deforming surface data. In addition, it is often necessary to cleanup or "tweak" simulation results. Doing this by editing each frame individually is a very time consuming and tedious process. Extensive research has investigated how to edit and re-use skeletal motion capture data, but very little has addressed completely non-rigid deforming surfaces. We have developed a novel method that now makes it easy to edit such arbitrary deforming surfaces. Our system enables global signal processing, direct manipulation, multiresolution embossing, and constraint editing on arbitrarily deforming surfaces, such as simulated cloth, motion-captured cloth, morphs, and other animations. The foundation of our method is a novel time-varying multiresolution transform, which adapts to the changing geometry of the surface in a temporally coherent manner.	Editing arbitrarily deforming surface animations	NA:NA	2018
Lin Shi:Yizhou Yu:Nathan Bell:Wei-Wen Feng	In this paper, we present a multigrid technique for efficiently deforming large surface and volume meshes. We show that a previous least-squares formulation for distortion minimization reduces to a Laplacian system on a general graph structure for which we derive an analytic expression. We then describe an efficient multigrid algorithm for solving the relevant equations. Here we develop novel prolongation and restriction operators used in the multigrid cycles. Combined with a simple but effective graph coarsening strategy, our algorithm can outperform other multigrid solvers and the factorization stage of direct solvers in both time and memory costs for large meshes. It is demonstrated that our solver can trade off accuracy for speed to achieve greater interactivity, which is attractive for manipulating large meshes. Our multigrid solver is particularly well suited for a mesh editing environment which does not permit extensive precomputation. Experimental evidence of these advantages is provided on a number of meshes with a wide range of size. With our mesh deformation solver, we also successfully demonstrate that visually appealing mesh animations can be generated from both motion capture data and a single base mesh even when they are inconsistent.	A fast multigrid algorithm for mesh deformation	NA:NA:NA:NA	2018
Wolfram von Funck:Holger Theisel:Hans-Peter Seidel	We present an approach to define shape deformations by constructing and interactively modifying C1 continuous time-dependent divergence-free vector fields. The deformation is obtained by a path line integration of the mesh vertices. This way, the deformation is volume-preserving, free of (local and global) self-intersections, feature preserving, smoothness preserving, and local. Different modeling metaphors support the approach which is able to modify the vector field on-the-fly according to the user input. The approach works at interactive frame rates for moderate mesh sizes, and the numerical integration preserves the volume with a high accuracy.	Vector field based shape deformations	NA:NA:NA	2018
Jin Huang:Xiaohan Shi:Xinguo Liu:Kun Zhou:Li-Yi Wei:Shang-Hua Teng:Hujun Bao:Baining Guo:Heung-Yeung Shum	In this paper we present a general framework for performing constrained mesh deformation tasks with gradient domain techniques. We present a gradient domain technique that works well with a wide variety of linear and nonlinear constraints. The constraints we introduce include the nonlinear volume constraint for volume preservation, the nonlinear skeleton constraint for maintaining the rigidity of limb segments of articulated figures, and the projection constraint for easy manipulation of the mesh without having to frequently switch between multiple viewpoints. To handle nonlinear constraints, we cast mesh deformation as a nonlinear energy minimization problem and solve the problem using an iterative algorithm. The main challenges in solving this nonlinear problem are the slow convergence and numerical instability of the iterative solver. To address these issues, we develop a subspace technique that builds a coarse control mesh around the original mesh and projects the deformation energy and constraints onto the control mesh vertices using the mean value interpolation. The energy minimization is then carried out in the subspace formed by the control mesh vertices. Running in this subspace, our energy minimization solver is both fast and stable and it provides interactive responses. We demonstrate our deformation constraints and subspace deformation technique with a variety of constrained deformation examples.	Subspace gradient domain mesh deformation	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
John Snyder	NA	Session details: Numerical and geometric algorithms and crowds	NA	2018
Richard Szeliski	This paper develops locally adapted hierarchical basis functions for effectively preconditioning large optimization problems that arise in computer graphics applications such as tone mapping, gradient-domain blending, colorization, and scattered data interpolation. By looking at the local structure of the coefficient matrix and performing a recursive set of variable eliminations, combined with a simplification of the resulting coarse level problems, we obtain bases better suited for problems with inhomogeneous (spatially varying) data, smoothness, and boundary constraints. Our approach removes the need to heuristically adjust the optimal number of preconditioning levels, significantly outperforms previously proposed approaches, and also maps cleanly onto data-parallel architectures such as modern GPUs.	Locally adapted hierarchical basis preconditioning	NA	2018
Avneesh Sud:Naga Govindaraju:Russell Gayle:Ilknur Kabul:Dinesh Manocha	We present novel algorithms to perform collision and distance queries among multiple deformable models in dynamic environments. These include inter-object queries between different objects as well as intra-object queries. We describe a unified approach to compute these queries based on N-body distance computation and use properties of the 2nd order discrete Voronoi diagram to perform N-body culling. Our algorithms involve no preprocessing and also work well on models with changing topologies. We can perform all proximity queries among complex deformable models consisting of thousands of triangles in a fraction of a second on a high-end PC. Moreover, our Voronoi-based culling algorithm can improve the performance of separation distance and penetration queries by an order of magnitude.	Fast proximity computation among deformable models using discrete Voronoi diagrams	NA:NA:NA:NA:NA	2018
Pascal Volino:Nadia Magnenat-Thalmann	Robust handling of collisions on non-oriented deformable surfaces requires advanced methods for recovering intersecting surfaces. We present a novel method that resolves intersections between two intersecting surface regions by inducing relative displacements which minimize the length of the intersection contour between them. This method, which does not rely on intersection regions, has a broader application field than existing methods, and its implementation is also much simpler, allowing integration into most existing collision response schemes. We demonstrate the efficiency of this method through examples in the context of cloth simulation.	Resolving surface collisions through intersection contour minimization	NA:NA	2018
Adrien Treuille:Seth Cooper:Zoran Popović	We present a real-time crowd model based on continuum dynamics. In our model, a dynamic potential field simultaneously integrates global navigation with moving obstacles such as other people, efficiently solving for the motion of large crowds without the need for explicit collision avoidance. Simulations created with our system run at interactive rates, demonstrate smooth flow under a variety of conditions, and naturally exhibit emergent phenomena that have been observed in real crowds.	Continuum crowds	NA:NA:NA	2018
Dinesh K. Pai	NA	Session details: Animation	NA	2018
Jue Wang:Steven M. Drucker:Maneesh Agrawala:Michael F. Cohen	We present the "Cartoon Animation Filter", a simple filter that takes an arbitrary input motion signal and modulates it in such a way that the output motion is more "alive" or "animated". The filter adds a smoothed, inverted, and (sometimes) time shifted version of the second derivative (the acceleration) of the signal back into the original signal. Almost all parameters of the filter are automated. The user only needs to set the desired strength of the filter. The beauty of the animation filter lies in its simplicity and generality. We apply the filter to motions ranging from hand drawn trajectories, to simple animations within PowerPoint presentations, to motion captured DOF curves, to video segmentation results. Experimental results show that the filtered motion exhibits anticipation, follow-through, exaggeration and squash-and-stretch effects which are not present in the original input motion data.	The cartoon animation filter	NA:NA:NA:NA	2018
Kevin G. Der:Robert W. Sumner:Jovan Popović	Articulated shapes are aptly described by reduced deformable models that express required shape deformations using a compact set of control parameters. Although sufficient to describe most shape deformations, these control parameters can be ill-suited for animation tasks, particularly when reduced deformable models are inferred automatically from example shapes. Our algorithm provides intuitive and direct control of reduced deformable models similar to a conventional inverse-kinematics algorithm for jointed rigid structures. We present a fully automated pipeline that transforms a set of unarticulated example shapes into a controllable, articulated model. With only a few manipulations, an animator can automatically and interactively pose detailed shapes at rates independent of their geometric complexity.	Inverse kinematics for reduced deformable models	NA:NA:NA	2018
Florence Bertails:Basile Audoly:Marie-Paule Cani:Bernard Querleux:Frédéric Leroy:Jean-Luc Lévêque	Simulating human hair is recognized as one of the most difficult tasks in computer animation. In this paper, we show that the Kirchhoff equations for dynamic, inextensible elastic rods can be used for accurately predicting hair motion. These equations fully account for the nonlinear behavior of hair strands with respect to bending and twisting. We introduce a novel deformable model for solving them: each strand is represented by a Super-Helix, i.e., a piecewise helical rod which is animated using the principles of Lagrangian mechanics. This results in a realistic and stable simulation, allowing large time steps. Our second contribution is an in-depth validation of the Super-Helix model, carried out through a series of experiments based on the comparison of real and simulated hair motions. We show that our model efficiently handles a wide range of hair types with a high level of realism.	Super-helices for predicting the dynamics of natural hair	NA:NA:NA:NA:NA:NA	2018
Sung-Hee Lee:Demetri Terzopoulos	Unlike the human face, the neck has been largely overlooked in the computer graphics literature, this despite its complex anatomical structure and the important role that it plays in supporting the head in balance while generating the controlled head movements that are essential to so many aspects of human behavior. This paper makes two major contributions. First, we introduce a biomechanical model of the human head-neck system. Emulating the relevant anatomy, our model is characterized by appropriate kinematic redundancy (7 cervical vertebrae coupled by 3-DOF joints) and muscle actuator redundancy (72 neck muscles arranged in 3 muscle layers). This anatomically consistent biomechanical model confronts us with a challenging motor control problem, even for the relatively simple task of balancing the mass of the head in gravity atop the cervical spine. Hence, our second contribution is a novel neuromuscular control model for human head animation that emulates the relevant biological motor control mechanisms. Incorporating low-level reflex and high-level voluntary sub-controllers, our hierarchical controller provides input motor signals to the numerous muscle actuators. In addition to head pose and movement, it controls the tone of mutually opposed neck muscles to regulate the stiffness of the head-neck multibody system. Employing machine learning techniques, the neural networks within our neuromuscular controller are trained offline to efficiently generate the online pose and tone control signals necessary to synthesize a variety of autonomous movements for the behavioral animation of the human head and face.	Heads up!: biomechanical modeling and neuromuscular control of the neck	NA:NA	2018
Aaron Hertzmann	NA	Session details: Non-photorealistic rendering	NA	2018
Szymon Rusinkiewicz:Michael Burns:Doug DeCarlo	In fields ranging from technical illustration to mapmaking, artists have developed distinctive visual styles designed to convey both detail and overall shape as clearly as possible. We investigate a non-photorealistic shading model, inspired by techniques for carto-graphic terrain relief, based on dynamically adjusting the effective light position for different areas of the surface. It reveals detail regardless of surface orientation and, by operating at multiple scales, is designed to convey detail at all frequencies simultaneously.	Exaggerated shading for depicting shape and detail	NA:NA:NA	2018
Thomas Luft:Carsten Colditz:Oliver Deussen	We present a simple and efficient method to enhance the perceptual quality of images that contain depth information. Similar to an unsharp mask, the difference between the original depth buffer content and a low-pass filtered copy is utilized to determine information about spatially important areas in a scene. Based on this information we locally enhance the contrast, color, and other parameters of the image. Our technique aims at improving the perception of complex scenes by introducing additional depth cues. The idea is motivated by artwork and findings in the field of neurology, and can be applied to images of any kind, ranging from complex landscape data and technical artifacts, to volume rendering, photograph, and video with depth information.	Image enhancement by unsharp masking the depth buffer	NA:NA:NA	2018
Yingge Qu:Tien-Tsin Wong:Pheng-Ann Heng	This paper proposes a novel colorization technique that propagates color over regions exhibiting pattern-continuity as well as intensity-continuity. The proposed method works effectively on colorizing black-and-white manga which contains intensive amount of strokes, hatching, halftoning and screening. Such fine details and discontinuities in intensity introduce many difficulties to intensity-based colorization methods. Once the user scribbles on the drawing, a local, statistical based pattern feature obtained with Gabor wavelet filters is applied to measure the pattern-continuity. The boundary is then propagated by the level set method that monitors the pattern-continuity. Regions with open boundaries or multiple disjointed regions with similar patterns can be sensibly segmented by a single scribble. With the segmented regions, various colorization techniques can be applied to replace colors, colorize with stroke preservation, or even convert pattern to shading. Several results are shown to demonstrate the effectiveness and convenience of the proposed method.	Manga colorization	NA:NA:NA	2018
Lu Yuan:Jian Sun:Long Quan:Heung-Yeung Shum	Taking satisfactory photos under dim lighting conditions using a hand-held camera is challenging. If the camera is set to a long exposure time, the image is blurred due to camera shake. On the other hand, the image is dark and noisy if it is taken with a short exposure time but with a high camera gain. By combining information extracted from both blurred and noisy images, however, we show in this paper how to produce a high quality image that cannot be obtained by simply denoising the noisy image, or deblurring the blurred image alone. Our approach is image deblurring with the help of the noisy image. First, both images are used to estimate an accurate blur kernel, which otherwise is difficult to obtain from a single blurred image. Second, and again using both images, a residual deconvolution is proposed to significantly reduce ringing artifacts inherent to image deconvolution. Third, the remaining ringing artifacts in smooth image regions are further suppressed by a gain-controlled deconvolution process. We demonstrate the effectiveness of our approach using a number of indoor and outdoor images taken by off-the-shelf hand-held cameras in poor lighting environments.	Image deblurring with blurred/noisy image pairs	NA:NA:NA:NA	2018
Richard Szeliski	NA	Session details: Image analysis & enhancement	NA	2018
Johannes Kopf:Chi-Wing Fu:Daniel Cohen-Or:Oliver Deussen:Dani Lischinski:Tien-Tsin Wong	We present a novel method for synthesizing solid textures from 2D texture exemplars. First, we extend 2D texture optimization techniques to synthesize 3D texture solids. Next, the non-parametric texture optimization approach is integrated with histogram matching, which forces the global statistics of the synthesized solid to match those of the exemplar. This improves the convergence of the synthesis process and enables using smaller neighborhoods. In addition to producing compelling texture mapped surfaces, our method also effectively models the material in the interior of solid objects. We also demonstrate that our method is well-suited for synthesizing textures with a large number of channels per texel.	Solid texture synthesis from 2D exemplars	NA:NA:NA:NA:NA:NA	2018
Jean-François Lalonde:Derek Hoiem:Alexei A. Efros:Carsten Rother:John Winn:Antonio Criminisi	We present a system for inserting new objects into existing photographs by querying a vast image-based object library, pre-computed using a publicly available Internet object database. The central goal is to shield the user from all of the arduous tasks typically involved in image compositing. The user is only asked to do two simple things: 1) pick a 3D location in the scene to place a new object; 2) select an object to insert using a hierarchical menu. We pose the problem of object insertion as a data-driven, 3D-based, context-sensitive object retrieval task. Instead of trying to manipulate the object to change its orientation, color distribution, etc. to fit the new image, we simply retrieve an object of a specified class that has all the required properties (camera pose, lighting, resolution, etc) from our large object library. We present new automatic algorithms for improving object segmentation and blending, estimating true 3D object size and orientation, and estimating scene lighting conditions. We also present an intuitive user interface that makes object insertion fast and simple even for the artistically challenged.	Photo clip art	NA:NA:NA:NA:NA:NA	2018
James Hays:Alexei A. Efros	What can you do with a million images? In this paper we present a new image completion algorithm powered by a huge database of photographs gathered from the Web. The algorithm patches up holes in images by finding similar image regions in the database that are not only seamless but also semantically valid. Our chief insight is that while the space of images is effectively infinite, the space of semantically differentiable scenes is actually not that large. For many image completion tasks we are able to find similar scenes which contain image fragments that will convincingly complete the image. Our algorithm is entirely data-driven, requiring no annotations or labelling by the user. Unlike existing image completion methods, our algorithm can generate a diverse set of results for each input image and we allow users to select among them. We demonstrate the superiority of our algorithm over existing image completion approaches.	Scene completion using millions of photographs	NA:NA	2018
Michiel van de Panne	NA	Session details: Character animation I	NA	2018
Seth Cooper:Aaron Hertzmann:Zoran Popović	This paper describes an approach to building real-time highly-controllable characters. A kinematic character controller is built on-the-fly during a capture session, and updated after each new motion clip is acquired. Active learning is used to identify which motion sequence the user should perform next, in order to improve the quality and responsiveness of the controller. Because motion clips are selected adaptively, we avoid the difficulty of manually determining which ones to capture, and can build complex controllers from scratch while significantly reducing the number of necessary motion samples.	Active learning for real-time motion controllers	NA:NA:NA	2018
James McCann:Nancy Pollard	In game environments, animated character motion must rapidly adapt to changes in player input - for example, if a directional signal from the player's gamepad is not incorporated into the character's trajectory immediately, the character may blithely run off a ledge. Traditional schemes for data-driven character animation lack the split-second reactivity required for this direct control; while they can be made to work, motion artifacts will result. We describe an on-line character animation controller that assembles a motion stream from short motion fragments, choosing each fragment based on current player input and the previous fragment. By adding a simple model of player behavior we are able to improve an existing reinforcement learning method for precalculating good fragment choices. We demonstrate the efficacy of our model by comparing the animation selected by our new controller to that selected by existing methods and to the optimal selection, given knowledge of the entire path. This comparison is performed over real-world data collected from a game prototype. Finally, we provide results indicating that occasional low-quality transitions between motion segments are crucial to high-quality on-line motion generation; this is an important result for others crafting animation systems for directly-controlled characters, as it argues against the common practice of transition thresholding.	Responsive characters from motion fragments	NA:NA	2018
Adrien Treuille:Yongjoon Lee:Zoran Popović	We present a new approach to realtime character animation with interactive control. Given a corpus of motion capture data and a desired task, we automatically compute near-optimal controllers using a low-dimensional basis representation. We show that these controllers produce motion that fluidly responds to several dimensions of user control and environmental constraints in realtime. Our results indicate that very few basis functions are required to create high-fidelity character controllers which permit complex user navigation and obstacle-avoidance tasks.	Near-optimal character animation with continuous control	NA:NA:NA	2018
Jinxiang Chai:Jessica K. Hodgins	In this paper, we present a technique for generating animation from a variety of user-defined constraints. We pose constraint-based motion synthesis as a maximum a posterior (MAP) problem and develop an optimization framework that generates natural motion satisfying user constraints. The system automatically learns a statistical dynamic model from motion capture data and then enforces it as a motion prior. This motion prior, together with user-defined constraints, comprises a trajectory optimization problem. Solving this problem in the low-dimensional space yields optimal natural motion that achieves the goals specified by the user. We demonstrate the effectiveness of this approach by generating whole-body and facial motion from a variety of spatial-temporal constraints.	Constraint-based motion optimization using a statistical dynamic model	NA:NA	2018
Aseem Agarwala	NA	Session details: Image slicing & stretching	NA	2018
Jue Wang:Maneesh Agrawala:Michael F. Cohen	We present Soft Scissors, an interactive tool for extracting alpha mattes of foreground objects in realtime. We recently proposed a novel offline matting algorithm capable of extracting high-quality mattes for complex foreground objects such as furry animals [Wang and Cohen 2007]. In this paper we both improve the quality of our offline algorithm and give it the ability to incrementally update the matte in an online interactive setting. Our realtime system efficiently estimates foreground color thereby allowing both the matte and the final composite to be revealed instantly as the user roughly paints along the edge of the foreground object. In addition, our system can dynamically adjust the width and boundary conditions of the scissoring paint brush to approximately capture the boundary of the foreground object that lies ahead on the scissor's path. These advantages in both speed and accuracy create the first interactive tool for high quality image matting and compositing.	Soft scissors: an interactive tool for realtime high quality matting	NA:NA:NA	2018
Shai Avidan:Ariel Shamir	Effective resizing of images should not only use geometric constraints, but consider the image content as well. We present a simple image operator called seam carving that supports content-aware image resizing for both reduction and expansion. A seam is an optimal 8-connected path of pixels on a single image from top to bottom, or left to right, where optimality is defined by an image energy function. By repeatedly carving out or inserting seams in one direction we can change the aspect ratio of an image. By applying these operators in both directions we can retarget the image to a new size. The selection and order of seams protect the content of the image, as defined by the energy function. Seam carving can also be used for image content enhancement and object removal. We support various visual saliency measures for defining the energy of an image, and can also include user input to guide the process. By storing the order of seams in an image we create multi-size images, that are able to continuously change in real time to fit a given size.	Seam carving for content-aware image resizing	NA:NA	2018
Jian Sun:Lin Liang:Fang Wen:Heung-Yeung Shum	Recently, gradient meshes have been introduced as a powerful vector graphics representation to draw multicolored mesh objects with smooth transitions. Using tools from Abode Illustrator and Corel CorelDraw, a user can manually create gradient meshes even for photo-realistic vector arts, which can be further edited, stylized and animated. In this paper, we present an easy-to-use interactive tool, called optimized gradient mesh, to semi-automatically and quickly create gradient meshes from a raster image. We obtain the optimized gradient mesh by formulating an energy minimization problem. The user can also interactively specify a few vector lines to guide the mesh generation. The resulting optimized gradient mesh is an editable and scalable mesh that otherwise would have taken many hours for a user to manually create.	Image vectorization using optimized gradient meshes	NA:NA:NA:NA	2018
Hui Fang:John C. Hart	Shape deformation is a common practice in digital image editing, but can unrealistically stretch or compress texture detail. We propose an image editing system that decouples feature position from pixel color generation, by resynthesizing texture from the source image to preserve its detail and orientation around a new feature curve location. We introduce a new distortion to patch-based texture synthesis that aligns texture features with image features. A dense correspondence field between source and target images generated by the control curves then guides texture synthesis.	Detail preserving shape deformation in image editing	NA:NA	2018
Carol O'Sullivan	NA	Session details: Squish, bounce and collide	NA	2018
Geoffrey Irving:Craig Schroeder:Ronald Fedkiw	We propose a numerical method for modeling highly deformable nonlinear incompressible solids that conserves the volume locally near each node in a finite element mesh. Our method works with arbitrary constitutive models, is applicable to both passive and active materials (e.g. muscles), and works with simple tetrahedra without the need for multiple quadrature points or stabilization techniques. Although simple linear tetrahedra typically suffer from locking when modeling incompressible materials, our method enforces incompressibility per node (in a one-ring), and we demonstrate that it is free from locking. We correct errors in volume without introducing oscillations by treating position and velocity in separate implicit solves. Finally, we propose a novel method for treating both object contact and self-contact as linear constraints during the incompressible solve, alleviating issues in enforcing multiple possibly conflicting constraints.	Volume conserving finite element simulations of deformable models	NA:NA:NA	2018
Christopher D. Twigg:Doug L. James	Animation techniques for controlling passive simulation are commonly based on an optimization paradigm: the user provides goals a priori, and sophisticated numerical methods minimize a cost function that represents these goals. Unfortunately, for multibody systems with discontinuous contact events these optimization problems can be highly nontrivial to solve, and many-hour offline optimizations, unintuitive parameters, and convergence failures can frustrate end-users and limit usage. On the other hand, users are quite adaptable, and systems which provide interactive feedback via an intuitive interface can leverage the user's own abilities to quickly produce interesting animations. However, the online computation necessary for interactivity limits scene complexity in practice. We introduce Many-Worlds Browsing, a method which circumvents these limits by exploiting the speed of multibody simulators to compute numerous example simulations in parallel (offline and online), and allow the user to browse and modify them interactively. We demonstrate intuitive interfaces through which the user can select among the examples and interactively adjust those parts of the scene that do not match his requirements. We show that using a combination of our techniques, unusual and interesting results can be generated for moderately sized scenes with under an hour of user time. Scalability is demonstrated by sampling much larger scenes using modest offline computations.	Many-worlds browsing for control of multibody dynamics	NA:NA	2018
Xinyu Zhang:Stephane Redon:Minkyoung Lee:Young J. Kim	We present a fast continuous collision detection (CCD) algorithm for articulated models using Taylor models and temporal culling. Our algorithm is a generalization of conservative advancement (CA) from convex models [Mirtich 1996] to articulated models with non-convex links. Given the initial and final configurations of a moving articulated model, our algorithm creates a continuous motion with constant translational and rotational velocities for each link, and checks for interferences between the articulated model under continuous motion and other models in the environment and for self-collisions. If collisions occur, our algorithm reports the first time of contact (TOC) as well as collision witness features. We have implemented our CCD algorithm and applied it to several challenging scenarios including locomotion generation, articulated-body dynamics and character motion planning. Our algorithm can perform CCDs including self-collision detection for articulated models consisting of many links and tens of thousands of triangles in 1.22 ms on average running on a 3.6 GHz Pentium 4 PC. This is an improvement on the performance of prior algorithms of more than an order of magnitude.	Continuous collision detection for articulated models using Taylor models and temporal culling	NA:NA:NA:NA	2018
Adam W. Bargteil:Chris Wojtan:Jessica K. Hodgins:Greg Turk	We present an extension to Lagrangian finite element methods to allow for large plastic deformations of solid materials. These behaviors are seen in such everyday materials as shampoo, dough, and clay as well as in fantastic gooey and blobby creatures in special effects scenes. To account for plastic deformation, we explicitly update the linear basis functions defined over the finite elements during each simulation step. When these updates cause the basis functions to become ill-conditioned, we remesh the simulation domain to produce a new high-quality finite-element mesh, taking care to preserve the original boundary. We also introduce an enhanced plasticity model that preserves volume and includes creep and work hardening/softening. We demonstrate our approach with simulations of synthetic objects that squish, dent, and flow. To validate our methods, we compare simulation results to videos of real materials.	A finite element method for animating large viscoplastic flow	NA:NA:NA:NA	2018
Doug DeCarlo	NA	Session details: Shape depiction and stylization	NA	2018
Hideki Todo:Ken-ichi Anjyo:William Baxter:Takeo Igarashi	Recent progress in non-photorealistic rendering (NPR) has led to many stylized shading techniques that efficiently convey visual information about the objects depicted. Another crucial goal of NPR is to give artists simple and direct ways to express the abstract ideas born of their imaginations. In particular, the ability to add intentional, but often unrealistic, shading effects is indispensable for many applications. We propose a set of simple stylized shading algorithms that allow the user to freely add localized light and shade to a model in a manner that is consistent and seamlessly integrated with conventional lighting techniques. The algorithms provide an intuitive, direct manipulation method based on a paint-brush metaphor, to control and edit the light and shade locally as desired. Our prototype system demonstrates how our method can enhance both the quality and range of applicability of conventional stylized shading for offline animation and interactive applications.	Locally controllable stylized shading	NA:NA:NA:NA	2018
Yunjin Lee:Lee Markosian:Seungyong Lee:John F. Hughes	We describe a GPU-based algorithm for rendering a 3D model as a line drawing, based on the insight that a line drawing can be understood as an abstraction of a shaded image. We thus render lines along tone boundaries or thin dark areas in the shaded image. We extend this notion to the dual: we render highlight lines along thin bright areas and tone boundaries. We combine the lines with toon shading to capture broad regions of tone. The resulting line drawings effectively convey both shape and material cues. The lines produced by the method can include silhouettes. creases, and ridges, along with a generalization of suggestive contours that responds to lighting as well as viewing changes. The method supports automatic level of abstraction, where the size of depicted shape features adjusts appropriately as the camera zooms in or out. Animated models can be rendered in real time because costly mesh curvature calculations are not needed.	Line drawings via abstracted shading	NA:NA:NA:NA	2018
Tilke Judd:Frédo Durand:Edward Adelson	Three-dimensional shape can be drawn using a variety of feature lines, but none of the current definitions alone seem to capture all visually-relevant lines. We introduce a new definition of feature lines based on two perceptual observations. First, human perception is sensitive to the variation of shading, and since shape perception is little affected by lighting and reflectance modification, we should focus on normal variation. Second, view-dependent lines better convey smooth surfaces. From this we define view-dependent curvature as the variation of the surface normal with respect to a viewing screen plane, and apparent ridges as the loci of points that maximize a view-dependent curvature. We present a formal definition of apparent ridges and an algorithm to render line drawings of 3D meshes. We show that our apparent ridges encompass or enhance aspects of several other feature lines.	Apparent ridges for line drawing	NA:NA:NA	2018
Simon Breslav:Karol Szerszen:Lee Markosian:Pascal Barla:Joëlle Thollot	We describe a new way to render 3D scenes in a variety of non-photorealistic styles, based on patterns whose structure and motion are defined in 2D. In doing so, we sacrifice the ability of patterns that wrap onto 3D surfaces to convey shape through their structure and motion. In return, we gain several advantages, chiefly that 2D patterns are more visually abstract - a quality often sought by artists, which explains their widespread use in hand-drawn images. Extending such styles to 3D graphics presents a challenge: how should a 2D pattern move? Our solution is to transform it each frame by a 2D similarity transform that closely follows the underlying 3D shape. The resulting motion is often surprisingly effective, and has a striking cartoon quality that matches the visual style.	Dynamic 2D patterns for shading 3D scenes	NA:NA:NA:NA:NA	2018
Mark Pauly	NA	Session details: Point sets	NA	2018
Benedict J. Brown:Szymon Rusinkiewicz	A key challenge in reconstructing high-quality 3D scans is registering data from different viewpoints. Existing global (multiview) alignment algorithms are restricted to rigid-body transformations, and cannot adequately handle non-rigid warps frequently present in real-world datasets. Moreover, algorithms that can compensate for such warps between pairs of scans do not easily generalize to the multiview case. We present an algorithm for obtaining a globally optimal alignment of multiple overlapping datasets in the presence of low-frequency non-rigid deformations, such as those caused by device nonlinearities or calibration error. The process first obtains sparse correspondences between views using a locally weighted, stability-guaranteeing variant of iterative closest points (ICP). Global positions for feature points are found using a relaxation method, and the scans are warped to their final positions using thin-plate splines. Our framework efficiently handles large datasets---thousands of scans comprising hundreds of millions of samples---for both rigid and non-rigid alignment, with the non-rigid case requiring little overhead beyond rigid-body alignment. We demonstrate that, relative to rigid-body registration, it improves the quality of alignment and better preserves detail in 3D datasets from a variety of scanners exhibiting non-rigid distortion.	Global non-rigid alignment of 3-D scans	NA:NA	2018
Yaron Lipman:Daniel Cohen-Or:David Levin:Hillel Tal-Ezer	We introduce a Locally Optimal Projection operator (LOP) for surface approximation from point-set data. The operator is parameterization free, in the sense that it does not rely on estimating a local normal, fitting a local plane, or using any other local parametric representation. Therefore, it can deal with noisy data which clutters the orientation of the points. The method performs well in cases of ambiguous orientation, e.g., if two folds of a surface lie near each other, and other cases of complex geometry in which methods based upon local plane fitting may fail. Although defined by a global minimization problem, the method is effectively local, and it provides a second order approximation to smooth surfaces. Hence allowing good surface approximation without using any explicit or implicit approximation space. Furthermore, we show that LOP is highly robust to noise and outliers and demonstrate its effectiveness by applying it to raw scanned data of complex shapes.	Parameterization-free projection for geometry reconstruction	NA:NA:NA:NA	2018
Gaël Guennebaud:Markus Gross	In this paper we present a new Point Set Surface (PSS) definition based on moving least squares (MLS) fitting of algebraic spheres. Our surface representation can be expressed by either a projection procedure or in implicit form. The central advantages of our approach compared to existing planar MLS include significantly improved stability of the projection under low sampling rates and in the presence of high curvature. The method can approximate or interpolate the input point set and naturally handles planar point clouds. In addition, our approach provides a reliable estimate of the mean curvature of the surface at no additional cost and allows for the robust handling of sharp features and boundaries. It processes a simple point set as input, but can also take significant advantage of surface normals to improve robustness, quality and performance. We also present an novel normal estimation procedure which exploits the properties of the spherical fit for both direction estimation and orientation propagation. Very efficient computational procedures enable us to compute the algebraic sphere fitting with up to 40 million points per second on latest generation GPUs.	Algebraic point set surfaces	NA:NA	2018
Sagi Katz:Ayellet Tal:Ronen Basri	This paper proposes a simple and fast operator, the "Hidden" Point Removal operator, which determines the visible points in a point cloud, as viewed from a given viewpoint. Visibility is determined without reconstructing a surface or estimating normals. It is shown that extracting the points that reside on the convex hull of a transformed point cloud, amounts to determining the visible points. This operator is general - it can be applied to point clouds at various dimensions, on both sparse and dense point clouds, and on viewpoints internal as well as external to the cloud. It is demonstrated that the operator is useful in visualizing point clouds, in view-dependent reconstruction and in shadow casting.	Direct visibility of point sets	NA:NA:NA	2018
Steve Marschner	NA	Session details: Lighting	NA	2018
Jonathan Ragan-Kelley:Charlie Kilpatrick:Brian W. Smith:Doug Epps:Paul Green:Christophe Hery:Frédo Durand	We present an automated approach for high-quality preview of feature-film rendering during lighting design. Similar to previous work, we use a deep-framebuffer shaded on the GPU to achieve interactive performance. Our first contribution is to generate the deep-framebuffer and corresponding shaders automatically through data-flow analysis and compilation of the original scene. Cache compression reduces automatically-generated deep-framebuffers to reasonable size for complex production scenes and shaders. We also propose a new structure, the indirect framebuffer, that decouples shading samples from final pixels and allows a deep-framebuffer to handle antialiasing, motion blur and transparency efficiently. Progressive refinement enables fast feedback at coarser resolution. We demonstrate our approach in real-world production.	The lightspeed automatic interactive lighting preview system	NA:NA:NA:NA:NA:NA:NA	2018
Miloš Hašan:Fabio Pellacini:Kavita Bala	Rendering complex scenes with indirect illumination, high dynamic range environment lighting, and many direct light sources remains a challenging problem. Prior work has shown that all these effects can be approximated by many point lights. This paper presents a scalable solution to the many-light problem suitable for a GPU implementation. We view the problem as a large matrix of sample-light interactions; the ideal final image is the sum of the matrix columns. We propose an algorithm for approximating this sum by sampling entire rows and columns of the matrix on the GPU using shadow mapping. The key observation is that the inherent structure of the transfer matrix can be revealed by sampling just a small number of rows and columns. Our prototype implementation can compute the light transfer within a few seconds for scenes with indirect and environment illumination, area lights, complex geometry and arbitrary shaders. We believe this approach can be very useful for rapid previewing in applications like cinematic and architectural lighting design.	Matrix row-column sampling for the many-light problem	NA:NA:NA	2018
Xin Sun:Kun Zhou:Yanyun Chen:Stephen Lin:Jiaoying Shi:Baining Guo	We present a technique for interactive relighting in which source radiance, viewing direction, and BRDFs can all be changed on the fly. In handling dynamic BRDFs, our method efficiently accounts for the effects of BRDF modification on the reflectance and incident radiance at a surface point. For reflectance, we develop a BRDF tensor representation that can be factorized into adjustable terms for lighting, viewing, and BRDF parameters. For incident radiance, there exists a non-linear relationship between indirect lighting and BRDFs in a scene, which makes linear light transport frameworks such as PRT unsuitable. To overcome this problem, we introduce precomputed transfer tensors (PTTs) which decompose indirect lighting into precomputable components that are each a function of BRDFs in the scene, and can be rapidly combined at run time to correctly determine incident radiance. We additionally describe a method for efficient handling of high-frequency specular reflections by separating them from the BRDF tensor representation and processing them using precomputed visibility information. With relighting based on PTTs, interactive performance with indirect lighting is demonstrated in applications to BRDF animation and material tuning.	Interactive relighting with dynamic BRDFs	NA:NA:NA:NA:NA:NA	2018
Charles Han:Bo Sun:Ravi Ramamoorthi:Eitan Grinspun	Filtering is critical for representing detail, such as color textures or normal maps, across a variety of scales. While MIP-mapping texture maps is commonplace, accurate normal map filtering remains a challenging problem because of nonlinearities in shading---we cannot simply average nearby surface normals. In this paper, we show analytically that normal map filtering can be formalized as a spherical convolution of the normal distribution function (NDF) and the BRDF, for a large class of common BRDFs such as Lambertian, microfacet and factored measurements. This theoretical result explains many previous filtering techniques as special cases, and leads to a generalization to a broader class of measured and analytic BRDFs. Our practical algorithms leverage a significant body of work that has studied lighting-BRDF convolution. We show how spherical harmonics can be used to filter the NDF for Lambertian and low-frequency specular BRDFs, while spherical von Mises-Fisher distributions can be used for high-frequency materials.	Frequency domain normal map filtering	NA:NA:NA:NA	2018
Takeo Igarashi	NA	Session details: Illustration & sculpture	NA	2018
Jie Xu:Craig S. Kaplan	We present a set of graphical and combinatorial algorithms for designing mazes based on images. The designer traces regions of interest in an image and annotates the regions with style parameters. They can optionally specify a solution path, which provides a rough guide for laying out the maze's actual solution. The system uses novel extensions to well-known maze construction algorithms to build mazes that approximate the tone of the source image, express the desired style in each region, and conform to the user's solution path.	Image-guided maze construction	NA:NA	2018
Paul Asente:Mike Schuster:Teri Pettit	There are many types of illustrations that are easier to create in planar-map-based illustration systems than in the more common stacking-based systems. One weakness shared by all existing planar-map-based systems is that the editability of the drawing is severely hampered once coloring has begun. The paths that define the areas to be filled become divided wherever they intersect, making it difficult or impossible to edit them as a whole. Live Paint is a new metaphor that allows planar-map-based coloring while maintaining all the original paths unchanged. When a user makes a change, the regions and edges defined by the new paths take on fill and stroke attributes from the previous regions and edges. This results in greater editing flexibility and ease of use. Live Paint uses a set of heuristics to match each region and edge in a changed illustration with a region or edge in the previous version, a task that is more difficult than it at first appears. It then transfers fill and stroke attributes accordingly.	Dynamic planar map illustration	NA:NA:NA	2018
Wilmot Li:Lincoln Ritter:Maneesh Agrawala:Brian Curless:David Salesin	We present a system for authoring and viewing interactive cutaway illustrations of complex 3D models using conventions of traditional scientific and technical illustration. Our approach is based on the two key ideas that 1) cuts should respect the geometry of the parts being cut, and 2) cutaway illustrations should support interactive exploration. In our approach, an author instruments a 3D model with auxiliary parameters, which we call "rigging," that define how cutaways of that structure are formed. We provide an authoring interface that automates most of the rigging process. We also provide a viewing interface that allows viewers to explore rigged models using high-level interactions. In particular, the viewer can just select a set of target structures, and the system will automatically generate a cutaway illustration that exposes those parts. We have tested our system on a variety of CAD and anatomical models, and our results demonstrate that our approach can be used to create and view effective interactive cutaway illustrations for a variety of complex objects with little user effort.	Interactive cutaway illustrations of complex 3D models	NA:NA:NA:NA:NA	2018
Tim Weyrich:Jia Deng:Connelly Barnes:Szymon Rusinkiewicz:Adam Finkelstein	We present a system for semi-automatic creation of bas-relief sculpture. As an artistic medium, relief spans the continuum between 2D drawing or painting and full 3D sculpture. Bas-relief (or low relief) presents the unique challenge of squeezing shapes into a nearly-flat surface while maintaining as much as possible the perception of the full 3D scene. Our solution to this problem adapts methods from the tone-mapping literature, which addresses the similar problem of squeezing a high dynamic range image into the (low) dynamic range available on typical display devices. However, the bas-relief medium imposes its own unique set of requirements, such as maintaining small, fixed-size depth discontinuities. Given a 3D model, camera, and a few parameters describing the relative attenuation of different frequencies in the shape, our system creates a relief that gives the illusion of the 3D shape from a given vantage point while conforming to a greatly compressed height.	Digital bas-relief from 3D scenes	NA:NA:NA:NA:NA	2018
James Davis	NA	Session details: Performance capture	NA	2018
Bernd Bickel:Mario Botsch:Roland Angst:Wojciech Matusik:Miguel Otaduy:Hanspeter Pfister:Markus Gross	We present a novel multi-scale representation and acquisition method for the animation of high-resolution facial geometry and wrinkles. We first acquire a static scan of the face including reflectance data at the highest possible quality. We then augment a traditional marker-based facial motion-capture system by two synchronized video cameras to track expression wrinkles. The resulting model consists of high-resolution geometry, motion-capture data, and expression wrinkles in 2D parametric form. This combination represents the facial shape and its salient features at multiple scales. During motion synthesis the motion-capture data deforms the high-resolution geometry using a linear shell-based mesh-deformation method. The wrinkle geometry is added to the facial base mesh using nonlinear energy optimization. We present the results of our approach for performance replay as well as for wrinkle editing.	Multi-scale capture of facial geometry and motion	NA:NA:NA:NA:NA:NA:NA	2018
Ryan White:Keenan Crane:D. A. Forsyth	We capture the shape of moving cloth using a custom set of color markers printed on the surface of the cloth. The output is a sequence of triangle meshes with static connectivity and with detail at the scale of individual markers in both smooth and folded regions. We compute markers' coordinates in space using correspondence across multiple synchronized video cameras. Correspondence is determined from color information in small neighborhoods and refined using a novel strain pruning process. Final correspondence does not require neighborhood information. We use a novel data driven hole-filling technique to fill occluded regions. Our results include several challenging examples: a wrinkled shirt sleeve, a dancing pair of pants, and a rag tossed onto a cup. Finally, we demonstrate that cloth capture is reusable by animating a pair of pants using human motion capture data.	Capturing and animating occluded cloth	NA:NA:NA	2018
Daniel Vlasic:Rolf Adelsberger:Giovanni Vannucci:John Barnwell:Markus Gross:Wojciech Matusik:Jovan Popović	Commercial motion-capture systems produce excellent in-studio reconstructions, but offer no comparable solution for acquisition in everyday environments. We present a system for acquiring motions almost anywhere. This wearable system gathers ultrasonic time-of-flight and inertial measurements with a set of inexpensive miniature sensors worn on the garment. After recording, the information is combined using an Extended Kalman Filter to reconstruct joint configurations of a body. Experimental results show that even motions that are traditionally difficult to acquire are recorded with ease within their natural settings. Although our prototype does not reliably recover the global transformation, we show that the resulting motions are visually similar to the original ones, and that the combined acoustic and intertial system reduces the drift commonly observed in purely inertial systems. Our final results suggest that this system could become a versatile input device for a variety of augmented-reality applications.	Practical motion capture in everyday surroundings	NA:NA:NA:NA:NA:NA:NA	2018
Ramesh Raskar:Hideaki Nii:Bert deDecker:Yuki Hashimoto:Jay Summet:Dylan Moore:Yong Zhao:Jonathan Westhues:Paul Dietz:John Barnwell:Shree Nayar:Masahiko Inami:Philippe Bekaert:Michael Noland:Vlad Branzoi:Erich Bruns	In this paper, we present a high speed optical motion capture method that can measure three dimensional motion, orientation, and incident illumination at tagged points in a scene. We use tracking tags that work in natural lighting conditions and can be imperceptibly embedded in attire or other objects. Our system supports an unlimited number of tags in a scene, with each tag uniquely identified to eliminate marker reacquisition issues. Our tags also provide incident illumination data which can be used to match scene lighting when inserting synthetic elements. The technique is therefore ideal for on-set motion capture or real-time broadcasting of virtual sets. Unlike previous methods that employ high speed cameras or scanning lasers, we capture the scene appearance using the simplest possible optical devices - a light-emitting diode (LED) with a passive binary mask used as the transmitter and a photosensor used as the receiver. We strategically place a set of optical transmitters to spatio-temporally encode the volume of interest. Photosensors attached to scene points demultiplex the coded optical signals from multiple transmitters, allowing us to compute not only receiver location and orientation but also their incident illumination and the reflectance of the surfaces to which the photosensors are attached. We use our untethered tag system, called Prakash, to demonstrate methods of adding special effects to captured videos that cannot be accomplished using pure vision techniques that rely on camera images.	Prakash: lighting aware motion capture using photosensing markers and multiplexed illuminators	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Sing Bing Kang	NA	Session details: Light field & high dynamic range imaging	NA	2018
Eino-Ville Talvala:Andrew Adams:Mark Horowitz:Marc Levoy	The ability of a camera to record a high dynamic range image, whether by taking one snapshot or a sequence, is limited by the presence of veiling glare - the tendency of bright objects in the scene to reduce the contrast everywhere within the field of view. Veiling glare is a global illumination effect that arises from multiple scattering of light inside the camera's body and lens optics. By measuring separately the direct and indirect components of the intra-camera light transport, one can increase the maximum dynamic range a particular camera is capable of recording. In this paper, we quantify the presence of veiling glare and related optical artifacts for several types of digital cameras, and we describe two methods for removing them: deconvolution by a measured glare spread function, and a novel direct-indirect separation of the lens transport using a structured occlusion mask. In the second method, we selectively block the light that contributes to veiling glare, thereby attaining significantly higher signal-to-noise ratios than with deconvolution. Finally, we demonstrate our separation method for several combinations of cameras and realistic scenes.	Veiling glare in high dynamic range imaging	NA:NA:NA:NA	2018
Ahmet Oǧuz Akyüz:Roland Fleming:Bernhard E. Riecke:Erik Reinhard:Heinrich H. Bülthoff	The development of high dynamic range (HDR) imagery has brought us to the verge of arguably the largest change in image display technologies since the transition from black-and-white to color television. Novel capture and display hardware will soon enable consumers to enjoy the HDR experience in their own homes. The question remains, however, of what to do with existing images and movies, which are intrinsically low dynamic range (LDR). Can this enormous volume of legacy content also be displayed effectively on HDR displays? We have carried out a series of rigorous psychophysical investigations to determine how LDR images are best displayed on a state-of-the-art HDR monitor, and to identify which stages of the HDR imaging pipeline are perceptually most critical. Our main findings are: (1) As expected, HDR displays outperform LDR ones. (2) Surprisingly, HDR images that are tone-mapped for display on standard monitors are often no better than the best single LDR exposure from a bracketed sequence. (3) Most importantly of all, LDR data does not necessarily require sophisticated treatment to produce a compelling HDR experience. Simply boosting the range of an LDR image linearly to fit the HDR display can equal or even surpass the appearance of a true HDR image. Thus the potentially tricky process of inverse tone mapping can be largely circumvented.	Do HDR displays support LDR content?: a psychophysical evaluation	NA:NA:NA:NA:NA	2018
Allan G. Rempel:Matthew Trentacoste:Helge Seetzen:H. David Young:Wolfgang Heidrich:Lorne Whitehead:Greg Ward	New generations of display devices promise to provide significantly improved dynamic range over conventional display technology. In the long run, evolving camera technology and file formats will provide high fidelity content for these display devices. In the near term, however, the vast majority of images and video will only be available in low dynamic range formats. In this paper we describe a method for boosting the dynamic range of legacy video and photographs for viewing on high dynamic range displays. Our emphasis is on real-time processing of video streams, such as web streams or the signal from a DVD player. We place particular emphasis on robustness of the method, and its ability to deal with a wide range of content without user adjusted parameters or visible artifacts. The method can be implemented on both graphics hardware and on signal processors that are directly integrated in the HDR displays.	Ldr2Hdr: on-the-fly reverse tone mapping of legacy video and photographs	NA:NA:NA:NA:NA:NA:NA	2018
Andrew Jones:Ian McDowall:Hideshi Yamada:Mark Bolas:Paul Debevec	We describe a set of rendering techniques for an autostereoscopic light field display able to present interactive 3D graphics to multiple simultaneous viewers 360 degrees around the display. The display consists of a high-speed video projector, a spinning mirror covered by a holographic diffuser, and FPGA circuitry to decode specially rendered DVI video signals. The display uses a standard programmable graphics card to render over 5,000 images per second of interactive 3D graphics, projecting 360-degree views with 1.25 degree separation up to 20 updates per second. We describe the system's projection geometry and its calibration process, and we present a multiple-center-of-projection rendering technique for creating perspective-correct images from arbitrary viewpoints around the display. Our projection technique allows correct vertical perspective and parallax to be rendered for any height and distance when these parameters are known, and we demonstrate this effect with interactive raster graphics using a tracking system to measure the viewer's height and distance. We further apply our projection technique to the display of photographed light fields with accurate horizontal and vertical parallax. We conclude with a discussion of the display's visual accommodation performance and discuss techniques for displaying color imagery.	Rendering for an interactive 360° light field display	NA:NA:NA:NA:NA	2018
Tom Funkhouser	NA	Session details: Sketching 3D shapes	NA	2018
Andrew Nealen:Takeo Igarashi:Olga Sorkine:Marc Alexa	This paper presents a system for designing freeform surfaces with a collection of 3D curves. The user first creates a rough 3D model by using a sketching interface. Unlike previous sketching systems, the user-drawn strokes stay on the model surface and serve as handles for controlling the geometry. The user can add, remove, and deform these control curves easily, as if working with a 2D line drawing. The curves can have arbitrary topology; they need not be connected to each other. For a given set of curves, the system automatically constructs a smooth surface embedding by applying functional optimization. Our system provides real-time algorithms for both control curve deformation and the subsequent surface optimization. We show that one can create sophisticated models using this system, which have not yet been seen in previous sketching or functional optimization systems.	FiberMesh: designing freeform surfaces with 3D curves	NA:NA:NA:NA	2018
Tao Ju:Qian-Yi Zhou:Shi-Min Hu	We present a method for modifying the topology of a 3D model with user control. The heart of our method is a guided topology editing algorithm. Given a source model and a user-provided target shape, the algorithm modifies the source so that the resulting model is topologically consistent with the target. Our algorithm permits removing or adding various topological features (e.g., handles, cavities and islands) in a common framework and ensures that each topological change is made by minimal modification to the source model. To create the target shape, we have also designed a convenient 2D sketching interface for drawing 3D line skeletons. As demonstrated in a suite of examples, the use of sketching allows more accurate removal of topological artifacts than previous methods, and enables creative designs with specific topological goals.	Editing the topology of 3D models by sketching	NA:NA:NA	2018
Andrei Sharf:Thomas Lewiner:Gil Shklarski:Sivan Toledo:Daniel Cohen-Or	The reconstruction of a complete watertight model from scan data is still a difficult process. In particular, since scanned data is often incomplete, the reconstruction of the expected shape is an ill-posed problem. Techniques that reconstruct poorly-sampled areas without any user intervention fail in many cases to faithfully reconstruct the topology of the model. The method that we introduce in this paper is topology-aware: it uses minimal user input to make correct decisions at regions where the topology of the model cannot be automatically induced with a reasonable degree of confidence. We first construct a continuous function over a three-dimensional domain. This function is constructed by minimizing a penalty function combining the data points, user constraints, and a regularization term. The optimization problem is formulated in a mesh-independent manner, and mapped onto a specific mesh using the finite-element method. The zero level-set of this function is a first approximation of the reconstructed surface. At complex under-sampled regions, the constraints might be insufficient. Hence, we analyze the local topological stability of the zero level-set to detect weak regions of the surface. These regions are suggested to the user for adding local inside/outside constraints by merely scribbling over a 2D tablet. Each new user constraint modifies the minimization problem, which is solved incrementally. The process is repeated, converging to a topology-stable reconstruction. Reconstructions of models acquired by a structured-light scanner with a small number of scribbles demonstrate the effectiveness of the method.	Interactive topology-aware surface reconstruction	NA:NA:NA:NA:NA	2018
Tai-Pang Wu:Chi-Keung Tang:Michael S. Brown:Heung-Yeung Shum	We present a simple interactive approach to specify 3D shape in a single view using "shape palettes". The interaction is as follows: draw a simple 2D primitive in the 2D view and then specify its 3D orientation by drawing a corresponding primitive on a shape palette. The shape palette is presented as an image of some familiar shape whose local 3D orientation is readily understood and can be easily marked over. The 3D orientation from the shape palette is transferred to the 2D primitive based on the markup. As we will demonstrate, only sparse markup is needed to generate expressive and detailed 3D surfaces. This markup approach can be used to model freehand 3D surfaces drawn in a single view, or combined with image-snapping tools to quickly extract surfaces from images and photographs.	ShapePalettes: interactive normal transfer via sketching	NA:NA:NA:NA	2018
Yuki Mori:Takeo Igarashi	We introduce Plushie, an interactive system that allows nonprofessional users to design their own original plush toys. To design a plush toy, one needs to construct an appropriate two-dimensional (2D) pattern. However, it is difficult for non-professional users to appropriately design a 2D pattern. Some recent systems automatically generate a 2D pattern for a given three-dimensional (3D) model, but constructing a 3D model is itself a challenge. Furthermore, an arbitrary 3D model cannot necessarily be realized as a real plush toy, and the final sewn result can be very different from the original 3D model. We avoid this mismatch by constructing appropriate 2D patterns and applying simple physical simulation to it on the fly during 3D modeling. In this way, the model on the screen is always a good approximation of the final sewn result, which makes the design process much more efficient. We use a sketching interface for 3D modeling and also provide various editing operations tailored for plush toy design. Internally, the system constructs a 2D cloth pattern in such a way that the simulation result matches the user's input stroke. Our goal is to show that relatively simple algorithms can provide fast, satisfactory results to the user whereas the pursuit of optimal layout and simulation accuracy lies outside this paper's scope. We successfully demonstrated that non-professional users could design plush toys or balloon easily using Plushie.	Plushie: an interactive design system for plush toys	NA:NA	2018
Irfan Essa	NA	Session details: Physical simulation	NA	2018
Robert Bridson:Jim Houriham:Marcus Nordenstam	Procedural methods for animating turbulent fluid are often preferred over simulation, both for speed and for the degree of animator control. We offer an extremely simple approach to efficiently generating turbulent velocity fields based on Perlin noise, with a formula that is exactly incompressible (necessary for the characteristic look of everyday fluids), exactly respects solid boundaries (not allowing fluid to flow through arbitrarily-specified surfaces), and whose amplitude can be modulated in space as desired. In addition, we demonstrate how to combine this with procedural primitives for flow around moving rigid objects, vortices, etc.	Curl-noise for procedural fluid flow	NA:NA:NA	2018
Jeong-Mo Hong:Tamar Shinar:Ronald Fedkiw	We model flames and fire using the Navier-Stokes equations combined with the level set method and jump conditions to model the reaction front. Previous works modeled the flame using a combination of propagation in the normal direction and a curvature term which leads to a level set equation that is parabolic in nature and thus overly dissipative and smooth. Asymptotic theory shows that one can obtain more interesting velocities and fully hyperbolic (as opposed to parabolic) equations for the level set evolution. In particular, researchers in the field of detonation shock dynamics (DSD) have derived a set of equations which exhibit characteristic cellular patterns. We show how to make use of the DSD framework in the context of computer graphics simulations of flames and fire to obtain interesting features such as flame wrinkling and cellular patterns.	Wrinkled flames and cellular patterns	NA:NA:NA	2018
Bart Adams:Mark Pauly:Richard Keiser:Leonidas J. Guibas	We present novel adaptive sampling algorithms for particle-based fluid simulation. We introduce a sampling condition based on geometric local feature size that allows focusing computational resources in geometrically complex regions, while reducing the number of particles deep inside the fluid or near thick flat surfaces. Further performance gains are achieved by varying the sampling density according to visual importance. In addition, we propose a novel fluid surface definition based on approximate particle-to-surface distances that are carried along with the particles and updated appropriately. The resulting surface reconstruction method has several advantages over existing methods, including stability under particle resampling and suitability for representing smooth flat surfaces. We demonstrate how our adaptive sampling and distance-based surface reconstruction algorithms lead to significant improvements in time and memory as compared to single resolution particle simulations, without significantly affecting the fluid flow behavior.	Adaptively sampled particle fluids	NA:NA:NA:NA	2018
Rony Goldenthal:David Harmon:Raanan Fattal:Michel Bercovier:Eitan Grinspun	Many textiles do not noticeably stretch under their own weight. Unfortunately, for better performance many cloth solvers disregard this fact. We propose a method to obtain very low strain along the warp and weft direction using Constrained Lagrangian Mechanics and a novel fast projection method. The resulting algorithm acts as a velocity filter that easily integrates into existing simulation code.	Efficient simulation of inextensible cloth	NA:NA:NA:NA:NA	2018
Miklós Bergou:Saurabh Mathur:Max Wardetzky:Eitan Grinspun	We combine the often opposing forces of artistic freedom and mathematical determinism to enrich a given animation or simulation of a surface with physically based detail. We present a process called tracking, which takes as input a rough animation or simulation and enhances it with physically simulated detail. Building on the foundation of constrained Lagrangian mechanics, we propose weak-form constraints for tracking the input motion. This method allows the artist to choose where to add details such as characteristic wrinkles and folds of various thin shell materials and dynamical effects of physical forces. We demonstrate multiple applications ranging from enhancing an artist's animated character to guiding a simulated inanimate object.	TRACKS: toward directable thin shells	NA:NA:NA:NA	2018
Hendrik Lensch	NA	Session details: Appearance capture & editing	NA	2018
Raanan Fattal:Maneesh Agrawala:Szymon Rusinkiewicz	We present a new image-based technique for enhancing the shape and surface details of an object. The input to our system is a small set of photographs taken from a fixed viewpoint, but under varying lighting conditions. For each image we compute a multiscale decomposition based on the bilateral filter and then reconstruct an enhanced image that combines detail information at each scale across all the input images. Our approach does not require any information about light source positions, or camera calibration, and can produce good results with 3 to 5 input images. In addition our system provides a few high-level parameters for controlling the amount of enhancement and does not require pixel-level user input. We show that the bilateral filter is a good choice for our multiscale algorithm because it avoids the halo artifacts commonly associated with the traditional Laplacian image pyramid. We also develop a new scheme for computing our multiscale bilateral decomposition that is simple to implement, fast O(N2 log N) and accurate.	Multiscale shape and detail enhancement from multi-light image collections	NA:NA:NA	2018
Pieter Peers:Naoki Tamura:Wojciech Matusik:Paul Debevec	We propose a novel post-production facial performance relighting system for human actors. Our system uses just a dataset of view-dependent facial appearances with a neutral expression, captured for a static subject using a Light Stage apparatus. For the actual performance, however, a potentially different actor is captured under known, but static, illumination. During post-production, the reflectance field of the reference dataset actor is transferred onto the dynamic performance, enabling image-based relighting of the entire sequence. Our approach makes post-production relighting more practical and could easily be incorporated in a traditional production pipeline since it does not require additional hardware during principal photography. Additionally, we show that our system is suitable for real-time post-production illumination editing.	Post-production facial performance relighting using reflectance transfer	NA:NA:NA:NA	2018
Jan Kautz:Solomon Boulos:Frédo Durand	While measured Bidirectional Texture Functions (BTF) enable impressive realism in material appearance, they offer little control, which limits their use for content creation. In this work, we interactively manipulate BTFs and create new BTFs from flat textures. We present an out-of-core approach to manage the size of BTFs and introduce new editing operations that modify the appearance of a material. These tools achieve their full potential when selectively applied to subsets of the BTF through the use of new selection operators. We further analyze the use of our editing operators for the modification of important visual characteristics such as highlights, roughness, and fuzziness. Results compare favorably to the direct alteration of micro-geometry and reflectances of synthetic reference data.	Interactive editing and modeling of bidirectional texture functions	NA:NA:NA	2018
Fabio Pellacini:Jason Lawrence	We investigate a new approach to editing spatially- and temporally-varying measured materials that adopts a stroke-based workflow. In our system, a user specifies a small number of editing constraints with a 3-D painting interface which are smoothly propagated to the entire dataset through an optimization that enforces similar edits are applied to areas with similar appearance. The sparse nature of this appearance-driven optimization permits the use of efficient solvers, allowing the designer to interactively refine the constraints. We have found this approach supports specifying a wide range of complex edits that would not be easy with existing techniques which present the user with a fixed segmentation of the data. Furthermore, it is independent of the underlying reflectance model and we show edits to both analytic and non-parametric representations in examples from several material databases.	AppWand: editing measured materials using appearance-driven optimization	NA:NA	2018
Pierre Alliez	NA	Session details: Geometry processing I	NA	2018
Jonathan Palacios:Eugene Zhang	Designing rotational symmetries on surfaces is a necessary task for a wide variety of graphics applications, such as surface parameterization and remeshing, painterly rendering and pen-and-ink sketching, and texture synthesis. In these applications, the topology of a rotational symmetry field such as singularities and separatrices can have a direct impact on the quality of the results. In this paper, we present a design system that provides control over the topology of rotational symmetry fields on surfaces. As the foundation of our system, we provide comprehensive analysis for rotational symmetry fields on surfaces and present efficient algorithms to identify singularities and separatrices. We also describe design operations that allow a rotational symmetry field to be created and modified in an intuitive fashion by using the idea of basis fields and relaxation. In particular, we provide control over the topology of a rotational symmetry field by allowing the user to remove singularities from the field or to move them to more desirable locations. At the core of our analysis and design implementations is the observations that N-way rotational symmetries can be described by symmetric N-th order tensors, which allows an efficient vector-based representation that not only supports coherent definitions of arithmetic operations on rotational symmetries but also enables many analysis and design operations for vector fields to be adapted to rotational symmetry fields. To demonstrate the effectiveness of our approach, we apply our design system to pen-and-ink sketching and geometry remeshing.	Rotational symmetry field design on surfaces	NA:NA	2018
Matthew Fisher:Peter Schröder:Mathieu Desbrun:Hugues Hoppe	Tangent vector fields are an essential ingredient in controlling surface appearance for applications ranging from anisotropic shading to texture synthesis and non-photorealistic rendering. To achieve a desired effect one is typically interested in smoothly varying fields that satisfy a sparse set of user-provided constraints. Using tools from Discrete Exterior Calculus, we present a simple and efficient algorithm for designing such fields over arbitrary triangle meshes. By representing the field as scalars over mesh edges (i.e., discrete 1-forms), we obtain an intrinsic, coordinate-free formulation in which field smoothness is enforced through discrete Laplace operators. Unlike previous methods, such a formulation leads to a linear system whose sparsity permits efficient pre-factorization. Constraints are incorporated through weighted least squares and can be updated rapidly enough to enable interactive design, as we demonstrate in the context of anisotropic texture synthesis.	Design of tangent vector fields	NA:NA:NA:NA	2018
François Labelle:Jonathan Richard Shewchuk	The isosurface stuffing algorithm fills an isosurface with a uniformly sized tetrahedral mesh whose dihedral angles are bounded between 10.7° and 164.8°, or (with a change in parameters) between 8.9° and 158.8°. The algorithm is whip fast, numerically robust, and easy to implement because, like Marching Cubes, it generates tetrahedra from a small set of precomputed stencils. A variant of the algorithm creates a mesh with internal grading: on the boundary, where high resolution is generally desired, the elements are fine and uniformly sized, and in the interior they may be coarser and vary in size. This combination of features makes isosurface stuffing a powerful tool for dynamic fluid simulation, large-deformation mechanics, and applications that require interactive remeshing or use objects defined by smooth implicit surfaces. It is the first algorithm that rigorously guarantees the suitability of tetrahedra for finite element methods in domains whose shapes are substantially more challenging than boxes. Our angle bounds are guaranteed by a computer-assisted proof. If the isosurface is a smooth 2-manifold with bounded curvature, and the tetrahedra are sufficiently small, then the boundary of the mesh is guaranteed to be a geometrically and topologically accurate approximation of the isosurface.	Isosurface stuffing: fast tetrahedral meshes with good dihedral angles	NA:NA	2018
Valerio Pascucci:Giorgio Scorzelli:Peer-Timo Bremer:Ajith Mascarenhas	Reeb graphs are a fundamental data structure for understanding and representing the topology of shapes. They are used in computer graphics, solid modeling, and visualization for applications ranging from the computation of similarities and finding defects in complex models to the automatic selection of visualization parameters. We introduce an on-line algorithm that reads a stream of elements (vertices, triangles, tetrahedra, etc.) and continuously maintains the Reeb graph of all elements already reed. The algorithm is robust in handling non-manifold meshes and general in its applicability to input models of any dimension. Optionally, we construct a skeleton-like embedding of the Reeb graph, and/or remove topological noise to reduce the output size. For interactive multi-resolution navigation we also build a hierarchical data structure which allows real-time extraction of approximated Reeb graphs containing all topological features above a given error threshold. Our extensive experiments show both high performance and practical linear scalability for meshes ranging from thousands to hundreds of millions of triangles. We apply our algorithm to the largest, most general, triangulated surfaces available to us, including 3D, 4D and 5D simplicial meshes. To demonstrate one important application we use Reeb graphs to find and highlight topological defects in meshes, including some widely believed to be "clean."	Robust on-line computation of Reeb graphs: simplicity and speed	NA:NA:NA:NA	2018
Kavita Bala	NA	Session details: Light transport	NA	2018
Ivo Ihrke:Gernot Ziegler:Art Tevs:Christian Theobalt:Marcus Magnor:Hans-Peter Seidel	We present a new method for real-time rendering of sophisticated lighting effects in and around refractive objects. It enables us to realistically display refractive objects with complex material properties, such as arbitrarily varying refractive index, inhomogeneous attenuation, as well as spatially-varying anisotropic scattering and reflectance properties. User-controlled changes of lighting positions only require a few seconds of update time. Our method is based on a set of ordinary differential equations derived from the eikonal equation, the main postulate of geometric optics. This set of equations allows for fast casting of bent light rays with the complexity of a particle tracer. Based on this concept, we also propose an efficient light propagation technique using adaptive wavefront tracing. Efficient GPU implementations for our algorithmic concepts enable us to render a combination of visual effects that were previously not reproducible in real-time.	Eikonal rendering: efficient light transport in refractive objects	NA:NA:NA:NA:NA:NA	2018
Jeppe Revall Frisvad:Niels Jørgen Christensen:Henrik Wann Jensen	This paper introduces a theoretical model for computing the scattering properties of participating media and translucent materials. The model takes as input a description of the components of a medium and computes all the parameters necessary to render it. These parameters are the extinction and scattering coefficients, the phase function, and the index of refraction, Our theory is based on a robust generalization of the Lorenz-Mie theory. Previous models using Lorenz-Mie theory have been limited to non-absorbing media with spherical particles such as paints and clouds. Our generalized theory is capable of handling both absorbing host media and non-spherical particles, which significantly extends the classes of media and materials that can be modeled. We use the theory to computer optical properties for different types of ice and ocean water, and we derive a novel appearance model for milk parameterized by the fat and protein contents. Our results show that we are able to match measured scattering properties in cases where the classical Lorez-Mie theory breaks down, and we can compute properties for media that cannot be measured using existing techniques in computer graphics.	Computing the scattering properties of participating media using Lorenz-Mie theory	NA:NA:NA	2018
Carsten Dachsbacher:Marc Stamminger:George Drettakis:Frédo Durand	We reformulate the rendering equation to alleviate the need for explicit visibility computation, thus enabling interactive global illumination on graphics hardware. This is achieved by treating visibility implicitly and propagating an additional quantity, called antiradiance, to compensate for light transmitted extraneously. Our new algorithm shifts visibility computation to simple local iterations by maintaining additional directional antiradiance information with samples in the scene. It is easy to parallelize on a GPU. By correctly treating discretization and filtering, we can compute indirect illumination in scenes with dynamic objects much faster than traditional methods. Our results show interactive update of indirect illumination with moving characters and lights.	Implicit visibility and antiradiance for interactive global illumination	NA:NA:NA:NA	2018
Dhruv Mahajan:Ira Kemelmacher Shlizerman:Ravi Ramamoorthi:Peter Belhumeur	Blockwise or Clustered Principal Component Analysis (CPCA) is commonly used to achieve real-time rendering of shadows and glossy reflections with precomputed radiance transfer (PRT). The vertices or pixels are partitioned into smaller coherent regions, and light transport in each region is approximated by a locally low-dimensional subspace using PCA. Many earlier techniques such as surface light field and reflectance field compression use a similar paradigm. However, there has been no clear theoretical understanding of how light transport dimensionality increases with local patch size, nor of the optimal block size or number of clusters. In this paper, we develop a theory of locally low dimensional light transport, by using Szego's eigenvalue theorem to analytically derive the eigenvalues of the covariance matrix for canonical cases. We show mathematically that for symmetric patches of area A, the number of basis functions for glossy reflections increases linearly with A, while for simple cast shadows, it often increases as √A. These results are confirmed numerically on a number of test scenes. Next, we carry out an analysis of the cost of rendering, trading off local dimensionality and the number of patches, deriving an optimal block size. Based on this analysis, we provide useful practical insights for setting parameters in CPCA and also derive a new adaptive subdivision algorithm. Moreover, we show that rendering time scales sub-linearly with the resolution of the image, allowing for interactive all-frequency relighting of 1024 x 1024 images.	A theory of locally low dimensional light transport	NA:NA:NA:NA	2018
Nina Amenta	NA	Session details: Geometry processing II	NA	2018
Niloy J. Mitra:Leonidas J. Guibas:Mark Pauly	We present a symmetrization algorithm for geometric objects. Our algorithm enhances approximate symmetries of a model while minimally altering its shape. Symmetrizing deformations are formulated as an optimization process that couples the spatial domain with a transformation configuration space, where symmetries can be expressed more naturally and compactly as parametrized point-pair mappings. We derive closed-form solution for the optimal symmetry transformations, given a set of corresponding sample pairs. The resulting optimal displacement vectors are used to drive a constrained deformation model that pulls the shape towards symmetry. We show how our algorithm successfully symmetrizes both the geometry and the discretization of complex 2D and 3D shapes and discuss various applications of such symmetrizing deformations.	Symmetrization	NA:NA:NA	2018
Martin Kilian:Niloy J. Mitra:Helmut Pottmann	We present a novel framework to treat shapes in the setting of Riemannian geometry. Shapes -- triangular meshes or more generally straight line graphs in Euclidean space -- are treated as points in a shape space. We introduce useful Riemannian metrics in this space to aid the user in design and modeling tasks, especially to explore the space of (approximately) isometric deformations of a given shape. Much of the work relies on an efficient algorithm to compute geodesics in shape spaces; to this end, we present a multi-resolution framework to solve the interpolation problem -- which amounts to solving a boundary value problem -- as well as the extrapolation problem -- an initial value problem -- in shape space. Based on these two operations, several classical concepts like parallel transport and the exponential map can be used in shape space to solve various geometric modeling and geometry processing tasks. Applications include shape morphing, shape deformation, deformation transfer, and intuitive shape exploration.	Geometric modeling in shape space	NA:NA:NA	2018
Helmut Pottmann:Yang Liu:Johannes Wallner:Alexander Bobenko:Wenping Wang	The geometric challenges in the architectural design of freeform shapes come mainly from the physical realization of beams and nodes. We approach them via the concept of parallel meshes, and present methods of computation and optimization. We discuss planar faces, beams of controlled height, node geometry, and multilayer constructions. Beams of constant height are achieved with the new type of edge offset meshes. Mesh parallelism is also the main ingredient in a novel discrete theory of curvatures. These methods are applied to the construction of quadrilateral, pentagonal and hexagonal meshes, discrete minimal surfaces, discrete constant mean curvature surfaces, and their geometric transforms. We show how to design geometrically optimal shapes, and how to find a meaningful meshing and beam layout for existing shapes.	Geometry of multi-layer freeform structures for architecture	NA:NA:NA:NA:NA	2018
Patrick Mullen:Alexander McKenzie:Yiying Tong:Mathieu Desbrun	We present a purely Eulerian framework for geometry processing of surfaces and foliations. Contrary to current Eulerian methods used in graphics, we use conservative methods and a variational interpretation, offering a unified framework for routine surface operations such as smoothing, offsetting, and animation. Computations are performed on a fixed volumetric grid without recourse to Lagrangian techniques such as triangle meshes, particles, or path tracing. At the core of our approach is the use of the Coarea Formula to express area integrals over isosurfaces as volume integrals. This enables the simultaneous processing of multiple isosurfaces, while a single interface can be treated as the special case of a dense foliation. We show that our method is a powerful alternative to conventional geometric representations in delicate cases such as the handling of high-genus surfaces, weighted offsetting, foliation smoothing of medical datasets, and incompressible fluid animation.	A variational approach to Eulerian geometry processing	NA:NA:NA:NA	2018
Marc Levoy	NA	Session details: Computational cameras	NA	2018
Francesc Moreno-Noguer:Peter N. Belhumeur:Shree K. Nayar	We present a system for refocusing images and videos of dynamic scenes using a novel, single-view depth estimation method. Our method for obtaining depth is based on the defocus of a sparse set of dots projected onto the scene. In contrast to other active illumination techniques, the projected pattern of dots can be removed from each captured image and its brightness easily controlled in order to avoid under- or over-exposure. The depths corresponding to the projected dots and a color segmentation of the image are used to compute an approximate depth map of the scene with clean region boundaries. The depth map is used to refocus the acquired image after the dots are removed, simulating realistic depth of field effects. Experiments on a wide variety of scenes, including close-ups and live action, demonstrate the effectiveness of our method.	Active refocusing of images and videos	NA:NA:NA	2018
Paul Green:Wenyang Sun:Wojciech Matusik:Frédo Durand	The emergent field of computational photography is proving that, by coupling generalized imaging optics with software processing, the quality and flexibility of imaging systems can be increased. In this paper, we capture and manipulate multiple images of a scene taken with different aperture settings (f-numbers). We design and implement a prototype optical system and associated algorithms to capture four images of the scene in a single exposure, each taken with a different aperture setting. Our system can be used with commercially available DSLR cameras and photographic lenses without modification to either. We leverage the fact that defocus blur is a function of scene depth and f/# to estimate a depth map. We demonstrate several applications of our multi-aperture camera, such as post-exposure editing of the depth of field, including extrapolation beyond the physical limits of the lens, synthetic refocusing, and depth-guided deconvolution.	Multi-aperture photography	NA:NA:NA:NA	2018
Ashok Veeraraghavan:Ramesh Raskar:Amit Agrawal:Ankit Mohan:Jack Tumblin	We describe a theoretical framework for reversibly modulating 4D light fields using an attenuating mask in the optical path of a lens based camera. Based on this framework, we present a novel design to reconstruct the 4D light field from a 2D camera image without any additional refractive elements as required by previous light field cameras. The patterned mask attenuates light rays inside the camera instead of bending them, and the attenuation recoverably encodes the rays on the 2D sensor. Our mask-equipped camera focuses just as a traditional camera to capture conventional 2D photos at full sensor resolution, but the raw pixel values also hold a modulated 4D light field. The light field can be recovered by rearranging the tiles of the 2D Fourier transform of sensor values into 4D planes, and computing the inverse Fourier transform. In addition, one can also recover the full resolution image information for the in-focus parts of the scene. We also show how a broadband mask placed at the lens enables us to compute refocused images at full sensor resolution for layered Lambertian scenes. This partial encoding of 4D ray-space data enables editing of image contents by depth, yet does not require computational recovery of the complete 4D light field.	Dappled photography: mask enhanced cameras for heterodyned light fields and coded aperture refocusing	NA:NA:NA:NA:NA	2018
Anat Levin:Rob Fergus:Frédo Durand:William T. Freeman	A conventional camera captures blurred versions of scene information away from the plane of focus. Camera systems have been proposed that allow for recording all-focus images, or for extracting depth, but to record both simultaneously has required more extensive hardware and reduced spatial resolution. We propose a simple modification to a conventional camera that allows for the simultaneous recovery of both (a) high resolution image information and (b) depth information adequate for semi-automatic extraction of a layered depth representation of the image. Our modification is to insert a patterned occluder within the aperture of the camera lens, creating a coded aperture. We introduce a criterion for depth discriminability which we use to design the preferred aperture pattern. Using a statistical model of images, we can recover both depth information and an all-focus image from single photographs taken with the modified camera. A layered depth map is then extracted, requiring user-drawn strokes to clarify layer assignments in some cases. The resulting sharp image and layered depth map can be combined for various photographic applications, including automatic scene segmentation, post-exposure refocusing, or re-rendering of the scene from an alternate viewpoint.	Image and depth from a conventional camera with a coded aperture	NA:NA:NA:NA	2018
Doug James	NA	Session details: Articulation	NA	2018
Pushkar Joshi:Mark Meyer:Tony DeRose:Brian Green:Tom Sanocki	In this paper we consider the problem of creating and controlling volume deformations used to articulate characters for use in high-end applications such as computer generated feature films. We introduce a method we call harmonic coordinates that significantly improves upon existing volume deformation techniques. Our deformations are controlled using a topologically flexible structure, called a cage, that consists of a closed three dimensional mesh. The cage can optionally be augmented with additional interior vertices, edges, and faces to more precisely control the interior behavior of the deformation. We show that harmonic coordinates are generalized barycentric coordinates that can be extended to any dimension. Moreover, they are the first system of generalized barycentric coordinates that are non-negative even in strongly concave situations, and their magnitude falls off with distance as measured within the cage.	Harmonic coordinates for character articulation	NA:NA:NA:NA:NA	2018
Ilya Baran:Jovan Popović	Animating an articulated 3D character currently requires manual rigging to specify its internal skeletal structure and to define how the input motion deforms its surface. We present a method for animating characters automatically. Given a static character mesh and a generic skeleton, our method adapts the skeleton to the character and attaches it to the surface, allowing skeletal motion data to animate the character. Because a single skeleton can be used with a wide range of characters, our method, in conjunction with a library of motions for a few skeletons, enables a user-friendly animation system for novices and children. Our prototype implementation, called Pinocchio, typically takes under a minute to rig a character on a modern midrange PC.	Automatic rigging and animation of 3D characters	NA:NA	2018
Robert Y. Wang:Kari Pulli:Jovan Popović	Enveloping, or the mapping of skeletal controls to the deformations of a surface, is key to driving realistic animated characters. Despite its widespread use, enveloping still relies on slow or inaccurate deformation methods. We propose a method that is both fast, accurate and example-based. Our technique introduces a rotational regression model that captures common skinning deformations such as muscle bulging, twisting, and challenging areas such as the shoulders. Our improved treatment of rotational quantities is made practical by model reduction that ensures real-time solution of least-squares problems, independent of the mesh size. Our method is significantly more accurate than linear blend skinning and almost as fast, suggesting its use as a replacement for linear blend skinning when examples are available.	Real-time enveloping with rotational regression	NA:NA:NA	2018
Mark Meyer:John Anderson	Many applications in Computer Graphics contain computationally expensive calculations. These calculations are often performed at many points to produce a full solution, even though the subspace of reasonable solutions may be of a relatively low dimension. The calculation of facial articulation and rendering of scenes with global illumination are two example applications that require these sort of computations. In this paper, we present Key Point Subspace Acceleration and Soft Caching, a technique for accelerating these types of computations. Key Point Subspace Acceleration (KPSA) is a statistical acceleration scheme that uses examples to compute a statistical subspace and a set of characteristic key points. The full calculation is then computed only at these key points and these points are used to provide a subspace based estimate of the entire calculation. The soft caching process is an extension to the KPSA technique where the key points are also used to provide a confidence estimate for the KPSA result. In cases with high anticipated error the calculation will then "fail through" to a full evaluation of all points (a cache miss), while frames with low error can use the accelerated statistical evaluation (a cache hit).	Key Point Subspace Acceleration and soft caching	NA:NA	2018
Erik Reinhard	NA	Session details: Perception & color	NA	2018
Roger D. Hersch:Philipp Donzé:Sylvain Chosson	The present contribution aims at creating color images printed with fluorescent inks that are only visible under UV light. The considered fluorescent inks absorb light in the UV wavelength range and reemit part of it in the visible wavelength range. In contrast to normal color printing which relies on the spectral absorption of light by the inks, at low concentration fluorescent inks behave additively, i.e. their light emission spectra sum up. We first analyze to which extent different fluorescent inks can be superposed. Due to the quenching effect, at high concentrations of the fluorescent molecules, the fluorescent effect diminishes. With an ink-jet printer capable of printing pixels at reduced dot sizes, we reduce the concentration of the individual fluorescent inks and are able to create from the blue, red and greenish-yellow inks the new colorants white and magenta. In order to avoid quenching effects, we propose a color halftoning method relying on diagonally oriented pre-computed screen dots, which are printed side by side. For gamut mapping and color separation, we create a 3D representation of the fluorescent ink gamut in CIELAB space by predicting halftone fluorescent emission spectra according to the spectral Neugebauer model. Thanks to gamut mapping and juxtaposed halftoning, we create color images, which are invisible under daylight and have, under UV light, a high resemblance with the original images.	Color images visible under UV light	NA:NA:NA	2018
Ganesh Ramanarayanan:James Ferwerda:Bruce Walter:Kavita Bala	Efficient, realistic rendering of complex scenes is one of the grand challenges in computer graphics. Perceptually based rendering addresses this challenge by taking advantage of the limits of human vision. However, existing methods, based on predicting visible image differences, are too conservative because some kinds of image differences do not matter to human observers. In this paper, we introduce the concept of visual equivalence, a new standard for image fidelity in graphics. Images are visually equivalent if they convey the same impressions of scene appearance, even if they are visibly different. To understand this phenomenon, we conduct a series of experiments that explore how object geometry, material, and illumination interact to provide information about appearance, and we characterize how two kinds of transformations on illumination maps (blurring and warping) affect these appearance attributes. We then derive visual equivalence predictors (VEPs): metrics for predicting when images rendered with transformed illumination maps will be visually equivalent to images rendered with reference maps. We also run a confirmatory study to validate the effectiveness of these VEPs for general scenes. Finally, we show how VEPs can be used to improve the efficiency of two rendering algorithms: Light-cuts and precomputed radiance transfer. This work represents some promising first steps towards developing perceptual metrics based on higher order aspects of visual coding.	Visual equivalence: towards a new standard for image fidelity	NA:NA:NA:NA	2018
Peter Vangorp:Jurgen Laurijssen:Philip Dutré	Visual observation is our principal source of information in determining the nature of objects, including shape, material or roughness. The physiological and cognitive processes that resolve visual input into an estimate of the material of an object are influenced by the illumination and the shape of the object. This affects our ability to select materials by observing them on a point-lit sphere, as is common in current 3D modeling applications. In this paper we present an exploratory psychophysical experiment to study various influences on material discrimination in a realistic setting. The resulting data set is analyzed using a wide range of statistical techniques. Analysis of variance is used to estimate the magnitude of the influence of geometry, and fitted psychometric functions produce significantly diverse material discrimination thresholds across different shapes and materials. Suggested improvements to traditional material pickers include direct visualization on the target object, environment illumination, and the use of discrimination thresholds as a step size for parameter adjustments.	The influence of shape on the perception of material reflectance	NA:NA:NA	2018
Fabio Pellacini	NA	Session details: Sampling	NA	2018
Victor Ostromoukhov	We present a new general-purpose method for fast hierarchical importance sampling with blue-noise properties. Our approach is based on self-similar tiling of the plane or the surface of a sphere with rectifiable polyominoes. Sampling points are associated with polyominoes, one point per polyomino. Each polyomino is recursively subdivided until the desired local density of samples is reached. A numerical code generated during the subdivision process is used for thresholding to accept or reject the sample. The exact position of the sampling point within the polyomino is determined according to a structural index, which indicates the polyomino's local neighborhood. The variety of structural indices and associated sampling point positions are computed during the offline optimization process, and tabulated. Consequently, the sampling itself is extremely fast. The method allows both deterministic and pseudo-non-deterministic sampling. It can be successfully applied in a large variety of graphical applications, where fast sampling with good spectral and visual properties is required. The prime application is rendering.	Sampling with polyominoes	NA	2018
Robert L. Cook:John Halstead:Maxwell Planck:David Ryu	Many renderers perform poorly on scenes that contain a lot of detailed geometry. The load on the renderer can be alleviated by simplification techniques, which create less expensive representations of geometry that is small on the screen. Current simplification techniques for high-quality surface-based rendering tend to work best with element detail (i.e., detail due to the complexity of individual elements) but not as well with aggregate detail (i.e., detail due to the large number of elements). To address this latter type of detail, we introduce a stochastic technique related to some approaches used for point-based renderers. Scenes are rendered by randomly selecting a subset of the geometric elements and altering those elements statistically to preserve the overall appearance of the scene. The amount of simplification can depend on a number of factors, including screen size, motion blur, and depth of field.	Stochastic simplification of aggregate detail	NA:NA:NA:NA	2018
Denis Zorin	NA	Session details: Shape deformation	NA	2018
Robert W. Sumner:Johannes Schmid:Mark Pauly	We present an algorithm that generates natural and intuitive deformations via direct manipulation for a wide range of shape representations and editing scenarios. Our method builds a space deformation represented by a collection of affine transformations organized in a graph structure. One transformation is associated with each graph node and applies a deformation to the nearby space. Positional constraints are specified on the points of an embedded object. As the user manipulates the constraints, a nonlinear minimization problem is solved to find optimal values for the affine transformations. Feature preservation is encoded directly in the objective function by measuring the deviation of each transformation from a true rotation. This algorithm addresses the problem of "embedded deformation" since it deforms space through direct manipulation of objects embedded within it, while preserving the embedded objects' features. We demonstrate our method by editing meshes, polygon soups, mesh animations, and animated particle systems.	Embedded deformation for shape manipulation	NA:NA:NA	2018
Xiaohan Shi:Kun Zhou:Yiying Tong:Mathieu Desbrun:Hujun Bao:Baining Guo	We present mesh puppetry, a variational framework for detail-preserving mesh manipulation through a set of high-level, intuitive, and interactive design tools. Our approach builds upon traditional rigging by optimizing skeleton position and vertex weights in an integrated manner. New poses and animations are created by specifying a few desired constraints on vertex positions, balance of the character, length and rigidity preservation, joint limits, and/or self-collision avoidance. Our algorithm then adjusts the skeleton and solves for the deformed mesh simultaneously through a novel cascading optimization procedure, allowing realtime manipulation of meshes with 50K+ vertices for fast design of pleasing and realistic poses. We demonstrate the potential of our framework through an interactive deformation platform and various applications such as deformation transfer and motion retargeting.	Mesh puppetry: cascading optimization of mesh deformation with inverse kinematics	NA:NA:NA:NA:NA:NA	2018
Alec R. Rivers:Doug L. James	We introduce a simple technique that enables robust approximation of volumetric, large-deformation dynamics for real-time or large-scale offline simulations. We propose Lattice Shape Matching, an extension of deformable shape matching to regular lattices with embedded geometry; lattice vertices are smoothed by convolution of rigid shape matching operators on local lattice regions, with the effective mechanical stiffness specified by the amount of smoothing via region width. Since the naïve method can be very slow for stiff models - per-vertex costs scale cubically with region width - we provide a fast summation algorithm, Fast Lattice Shape Matching (FastLSM), that exploits the inherent summation redundancy of shape matching and can provide large-region matching at constant per-vertex cost. With this approach, large lattices can be simulated in linear time. We present several examples and benchmarks of an efficient CPU implementation, including many dozens of soft bodies simulated at real-time rates on a typical desktop machine.	FastLSM: fast lattice shape matching for robust real-time deformation	NA:NA	2018
Oscar Kin-Chung Au:Hongbo Fu:Chiew-Lan Tai:Daniel Cohen-Or	Handle-based mesh deformation is essentially a nonlinear problem. To allow scalability, the original deformation problem can be approximately represented by a compact set of control variables. We show the direct relation between the locations of handles on the mesh and the local rigidity under deformation, and introduce the notion of handle-aware rigidity. Then, we present a reduced model whose control variables are intelligently distributed across the surface, respecting the rigidity information and the geometry. Specifically, for each handle, the control variables are the transformations of the isolines of a harmonic scalar field representing the deformation propagation from that handle. The isolines constitute a virtual skeletal structure similar to the bones in skinning deformation, thus correctly capturing the low-frequency shape deformation. To interpolate the transformations from the isolines to the original mesh, we design a method which is local, linear and geometry-dependent. This novel interpolation scheme and the transformation-based reduced domain allow each iteration of the nonlinear solver to be fully computed over the reduced domain. This makes the per-iteration cost dependent on only the number of isolines and enables compelling deformation of highly detailed shapes at interactive rates. In addition, we show how the handle-driven isolines provide an efficient means for deformation transfer without full shape correspondence.	Handle-aware isolines for scalable shape editing	NA:NA:NA:NA	2018
Weiwei Xu:Kun Zhou:Yizhou Yu:Qifeng Tan:Qunsheng Peng:Baining Guo	Many graphics applications, including computer games and 3D animated films, make heavy use of deforming mesh sequences. In this paper, we generalize gradient domain editing to deforming mesh sequences. Our framework is keyframe based. Given sparse and irregularly distributed constraints at unevenly spaced keyframes, our solution first adjusts the meshes at the keyframes to satisfy these constraints, and then smoothly propagate the constraints and deformations at keyframes to the whole sequence to generate new deforming mesh sequence. To achieve convenient keyframe editing, we have developed an efficient alternating least-squares method. It harnesses the power of subspace deformation and two-pass linear methods to achieve high-quality deformations. We have also developed an effective algorithm to define boundary conditions for all frames using handle trajectory editing. Our deforming mesh editing framework has been successfully applied to a number of editing scenarios with increasing complexity, including footprint editing, path editing, temporal filtering, handle-based deformation mixing, and spacetime morphing.	Gradient domain editing of deforming mesh sequences	NA:NA:NA:NA:NA:NA	2018
Hanspeter Pfister	NA	Session details: Image-based modeling	NA	2018
Pascal Müller:Gang Zeng:Peter Wonka:Luc Van Gool	This paper describes algorithms to automatically derive 3D models of high visual quality from single facade images of arbitrary resolutions. We combine the procedural modeling pipeline of shape grammars with image analysis to derive a meaningful hierarchical facade subdivision. Our system gives rise to three exciting applications: urban reconstruction based on low resolution oblique aerial imagery, reconstruction of facades based on higher resolution ground-based imagery, and the automatic derivation of shape grammar rules from facade images to build a rule base for procedural modeling technology.	Image-based procedural modeling of facades	NA:NA:NA:NA	2018
Anton van den Hengel:Anthony Dick:Thorsten Thormählen:Ben Ward:Philip H. S. Torr	VideoTrace is a system for interactively generating realistic 3D models of objects from video---models that might be inserted into a video game, a simulation environment, or another video sequence. The user interacts with VideoTrace by tracing the shape of the object to be modelled over one or more frames of the video. By interpreting the sketch drawn by the user in light of 3D information obtained from computer vision techniques, a small number of simple 2D interactions can be used to generate a realistic 3D model. Each of the sketching operations in VideoTrace provides an intuitive and powerful means of modelling shape from video, and executes quickly enough to be used interactively. Immediate feedback allows the user to model rapidly those parts of the scene which are of interest and to the level of detail required. The combination of automated and manual reconstruction allows VideoTrace to model parts of the scene not visible, and to succeed in cases where purely automated approaches would fail.	VideoTrace: rapid interactive scene modelling from video	NA:NA:NA:NA:NA	2018
Ping Tan:Gang Zeng:Jingdong Wang:Sing Bing Kang:Long Quan	In this paper, we propose an approach for generating 3D models of natural-looking trees from images that has the additional benefit of requiring little user intervention. While our approach is primarily image-based, we do not model each leaf directly from images due to the large leaf count, small image footprint, and widespread occlusions. Instead, we populate the tree with leaf replicas from segmented source images to reconstruct the overall tree shape. In addition, we use the shape patterns of visible branches to predict those of obscured branches. We demonstrate our approach on a variety of trees.	Image-based tree modeling	NA:NA:NA:NA:NA	2018
Boris Neubert:Thomas Franken:Oliver Deussen	We present a method for producing 3D tree models from input photographs with only limited user intervention. An approximate voxel-based tree volume is estimated using image information. The density values of the voxels are used to produce initial positions for a set of particles. Performing a 3D flow simulation, the particles are traced downwards to the tree basis and are combined to form twigs and branches. If possible, the trunk and the first-order branches are determined in the input photographs and are used as attractors for particle simulation. The geometry of the tree skeleton is produced using botanical rules for branch thicknesses and branching angles. Finally, leaves are added. Different initial seeds for particle simulation lead to a variety, yet similar-looking branching structures for a single set of photographs.	Approximate image-based tree-modeling using particle flows	NA:NA:NA	2018
Greg Humphreys	NA	Session details: Graphics architecture	NA	2018
Pedro V. Sander:Diego Nehab:Joshua Barczak	We present novel algorithms that optimize the order in which triangles are rendered, to improve post-transform vertex cache efficiency as well as for view-independent overdraw reduction. The resulting triangle orders perform on par with previous methods, but are orders magnitude faster to compute. The improvements in processing speed allow us to perform the optimization right after a model is loaded, when more information on the host hardware is available. This allows our vertex cache optimization to often outperform other methods. In fact, our algorithms can even be executed interactively, allowing for re-optimization in case of changes to geometry or topology, which happen often in CAD/CAM applications. We believe that most real-time rendering applications will immediately benefit from these new results.	Fast triangle reordering for vertex locality and reduced overdraw	NA:NA:NA	2018
Tim Weyrich:Simon Heinzle:Timo Aila:Daniel B. Fasnacht:Stephan Oetiker:Mario Botsch:Cyril Flaig:Simon Mall:Kaspar Rohrer:Norbert Felber:Hubert Kaeslin:Markus Gross	We present a novel architecture for hardware-accelerated rendering of point primitives. Our pipeline implements a refined version of EWA splatting, a high quality method for antialiased rendering of point sampled representations. A central feature of our design is the seamless integration of the architecture into conventional, OpenGL-like graphics pipelines so as to complement triangle-based rendering. The specific properties of the EWA algorithm required a variety of novel design concepts including a ternary depth test and using an on-chip pipelined heap data structure for making the memory accesses of splat primitives more coherent. In addition, we developed a computationally stable evaluation scheme for perspectively corrected splats. We implemented our architecture both on reconfigurable FPGA boards and as an ASIC prototype, and we integrated it into an OpenGL-like software implementation. Our evaluation comprises a detailed performance analysis using scenes of varying complexity.	A hardware architecture for surface splatting	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Kun Zhou:Xin Huang:Weiwei Xu:Baining Guo:Heung-Yeung Shum	We present an algorithm for interactive deformation of subdivision surfaces, including displaced subdivision surfaces and subdivision surfaces with geometric textures. Our system lets the user directly manipulate the surface using freely-selected surface points as handles. During deformation the control mesh vertices are automatically adjusted such that the deforming surface satisfies the handle position constraints while preserving the original surface shape and details. To best preserve surface details, we develop a gradient domain technique that incorporates the handle position constraints and detail preserving objectives into the deformation energy. For displaced subdivision surfaces and surfaces with geometric textures, the deformation energy is highly nonlinear and cannot be handled with existing iterative solvers. To address this issue, we introduce a shell deformation solver, which replaces each numerically unstable iteration step with two stable mesh deformation operations. Our deformation algorithm only uses local operations and is thus suitable for GPU implementation. The result is a real-time deformation system running orders of magnitude faster than the state-of-the-art multigrid mesh deformation solver. We demonstrate our technique with a variety of examples, including examples of creating visually pleasing character animations in real-time by driving a subdivision surface with motion capture data.	Direct manipulation of subdivision surfaces on GPUs	NA:NA:NA:NA:NA	2018
Jon Hasselgren:Thomas Akenine-Möller	Culling techniques have always been a central part of computer graphics, but graphics hardware still lack efficient and flexible support for culling. To improve the situation, we introduce the programmable culling unit, which is as flexible as the fragment program unit and capable of quickly culling entire blocks of fragments. Furthermore, it is very easy for the developer to use the PCU as culling programs can be automatically derived from fragment programs containing a discard instruction. Our PCU can be integrated into an existing fragment program unit with a modest hardware overhead of only about 10%. Using the PCU, we have observed shader speedups between 1.4 and 2.1 for relevant scenes.	PCU: the programmable culling unit	NA:NA	2018
Ramesh Raskar	NA	Session details: Big images	NA	2018
Johannes Kopf:Matt Uyttendaele:Oliver Deussen:Michael F. Cohen	We present a system to capture and view "Gigapixel images": very high resolution, high dynamic range, and wide angle imagery consisting of several billion pixels each. A specialized camera mount, in combination with an automated pipeline for alignment, exposure compensation, and stitching, provide the means to acquire Gigapixel images with a standard camera and lens. More importantly, our novel viewer enables exploration of such images at interactive rates over a network, while dynamically and smoothly interpolating the projection between perspective and curved projections, and simultaneously modifying the tone-mapping to ensure an optimal view of the portion of the scene being viewed.	Capturing and viewing gigapixel images	NA:NA:NA:NA	2018
Aseem Agarwala	We describe a hierarchical approach to improving the efficiency of gradient-domain compositing, a technique that constructs seamless composites by combining the gradients of images into a vector field that is then integrated to form a composite. While gradient-domain compositing is powerful and widely used, it suffers from poor scalability. Computing an n pixel composite requires solving a linear system with n variables; solving such a large system quickly overwhelms the main memory of a standard computer when performed for multi-megapixel composites, which are common in practice. In this paper we show how to perform gradient-domain compositing approximately by solving an O(p) linear system, where p is the total length of the seams between image regions in the composite; for typical cases, p is O(√n). We achieve this reduction by transforming the problem into a space where much of the solution is smooth, and then utilize the pattern of this smoothness to adaptively subdivide the problem domain using quadtrees. We demonstrate the merits of our approach by performing panoramic stitching and image region copy-and-paste in significantly reduced time and memory while achieving visually identical results.	Efficient gradient-domain compositing using quadtrees	NA	2018
Raanan Fattal	In this paper we propose a new method for upsampling images which is capable of generating sharp edges with reduced input-resolution grid-related artifacts. The method is based on a statistical edge dependency relating certain edge features of two different resolutions, which is generically exhibited by real-world images. While other solutions assume some form of smoothness, we rely on this distinctive edge dependency as our prior knowledge in order to increase image resolution. In addition to this relation we require that intensities are conserved; the output image must be identical to the input image when downsampled to the original resolution. Altogether the method consists of solving a constrained optimization problem, attempting to impose the correct edge relation and conserve local intensities with respect to the low-resolution input image. Results demonstrate the visual importance of having such edge features properly matched, and the method's capability to produce images in which sharp edges are successfully reconstructed.	Image upsampling via imposed edge statistics	NA	2018
Johannes Kopf:Michael F. Cohen:Dani Lischinski:Matt Uyttendaele	Image analysis and enhancement tasks such as tone mapping, colorization, stereo depth, and photomontage, often require computing a solution (e.g., for exposure, chromaticity, disparity, labels) over the pixel grid. Computational and memory costs often require that a smaller solution be run over a downsampled image. Although general purpose upsampling methods can be used to interpolate the low resolution solution to the full resolution, these methods generally assume a smoothness prior for the interpolation. We demonstrate that in cases, such as those above, the available high resolution input image may be leveraged as a prior in the context of a joint bilateral upsampling procedure to produce a better high resolution solution. We show results for each of the applications above and compare them to traditional upsampling methods.	Joint bilateral upsampling	NA:NA:NA:NA	2018
Markus Gross	NA	Session details: Fluids	NA	2018
Paul W. Cleary:Soon Hyoung Pyo:Mahesh Prakash:Bon Ki Koo	We present a discrete particle based method capable of creating very realistic animations of bubbles in fluids. It allows for the generation (nucleation) of bubbles from gas dissolved in the fluid, the motion of the discrete bubbles including bubble collisions and drag interactions with the liquid which could be undergoing complex free surface motion, the formation and motion of coupled foams and the final dissipation of bubbles. This allows comprehensive simulations of dynamic bubble behavior. The underlying fluid simulation is based on the mesh-free Smoothed Particle Hydrodynamics method. Each particle representing the liquid contains an amount of dissolved gas. Gas is transferred from the continuum fluid model to the discrete bubble model at nucleation sites on the surface of solid bodies. The rate of gas transport to the nucleation sites controls the rate of bubble generation, producing very natural time variations in bubble numbers. Rising bubbles also grow by gathering more gas from the surrounding liquid as they move. This model contains significant bubble scale physics and allows, in principle, the capturing of many important processes that cannot be directly modeled by traditional methods. The method is used here to realistically animate the pouring of a glass of beer, starting with a stream of fresh beer entering the glass, the formation of a dense cloud of bubbles, which rise to create a good head as the beer reaches the top of the glass.	Bubbling and frothing liquids	NA:NA:NA:NA	2018
Byungmoon Kim:Yingjie Liu:Ignacio Llamas:Xiangmin Jiao:Jarek Rossignac	Liquid and gas interactions often produce bubbles that stay for a long time without bursting on the surface, making a dry foam structure. Such long lasting bubbles simulated by the level set method can suffer from a small but steady volume error that accumulates to a visible amount of volume change. We propose to address this problem by using the volume control method. We track the volume change of each connected region, and apply a carefully computed divergence that compensates undesired volume changes. To compute the divergence, we construct a mathematical model of the volume change, choose control strategies that regulate the modeled volume error, and establish methods to compute the control gains that provide robust and fast reduction of the volume error, and (if desired) the control of how the volume changes over time.	Simulation of bubbles in foam with the volume control method	NA:NA:NA:NA:NA	2018
Cem Yuksel:Donald H. House:John Keyser	We present a new method for the real-time simulation of fluid surface waves and their interactions with floating objects. The method is based on the new concept of wave particles, which offers a simple, fast, and unconditionally stable approach to wave simulation. We show how graphics hardware can be used to convert wave particles to a height field surface, which is warped horizontally to account for local wave-induced flow. The method is appropriate for most fluid simulation situations that do not involve significant global flow. It is demonstrated to work well in constrained areas, including wave reflections off of boundaries, and in unconstrained areas, such as an ocean surface. Interactions with floating objects are easily integrated by including wave forces on the objects and wave generation due to object motion. Theoretical foundations and implementation details are provided, and experiments demonstrate that we achieve plausible realism. Timing studies show that the method is scalable to allow simulation of wave interaction with several hundreds of objects at real-time rates.	Wave particles	NA:NA:NA	2018
Christopher Batty:Florence Bertails:Robert Bridson	Physical simulation has emerged as a compelling animation technique, yet current approaches to coupling simulations of fluids and solids with irregular boundary geometry are inefficient or cannot handle some relevant scenarios robustly. We propose a new variational approach which allows robust and accurate solution on relatively coarse Cartesian grids, allowing possibly orders of magnitude faster simulation. By rephrasing the classical pressure projection step as a kinetic energy minimization, broadly similar to modern approaches to rigid body contact, we permit a robust coupling between fluid and arbitrary solid simulations that always gives a well-posed symmetric positive semi-definite linear system. We provide several examples of efficient fluid-solid interaction and rigid body coupling with sub-grid cell flow. In addition, we extend the framework with a new boundary condition for free-surface flow, allowing fluid to separate naturally from solids.	A fast variational framework for accurate solid-fluid coupling	NA:NA:NA	2018
Maneesh Agrawala	NA	Session details: Video processing	NA	2018
Kalyan Sunkavalli:Wojciech Matusik:Hanspeter Pfister:Szymon Rusinkiewicz	We describe a method for converting time-lapse photography captured with outdoor cameras into Factored Time-Lapse Video (FTLV): a video in which time appears to move faster (i.e., lapsing) and where data at each pixel has been factored into shadow, illumination, and reflectance components. The factorization allows a user to easily relight the scene, recover a portion of the scene geometry (normals), and to perform advanced image editing operations. Our method is easy to implement, robust, and provides a compact representation with good reconstruction characteristics. We show results using several publicly available time-lapse sequences.	Factored time-lapse video	NA:NA:NA:NA	2018
Eric P. Bennett:Leonard McMillan	We present methods for generating novel time-lapse videos that address the inherent sampling issues that arise with traditional photographic techniques. Starting with video-rate footage as input, our post-process downsamples the source material into a time-lapse video and provides user controls for retaining, removing, and resampling events. We employ two techniques for selecting and combining source frames to form the output. First, we present a non-uniform sampling method, based on dynamic programming, which optimizes the sampling of the input video to match the user's desired duration and visual objectives. We present multiple error metrics for this optimization, each resulting in different sampling characteristics. To complement the non-uniform sampling, we present the virtual shutter, a non-linear filtering technique that synthetically extends the exposure time of time-lapse frames.	Computational time-lapse video	NA:NA	2018
Jiawen Chen:Sylvain Paris:Frédo Durand	We present a new data structure---the bilateral grid, that enables fast edge-aware image processing. By working in the bilateral grid, algorithms such as bilateral filtering, edge-aware painting, and local histogram equalization become simple manipulations that are both local and independent. We parallelize our algorithms on modern GPUs to achieve real-time frame rates on high-definition video. We demonstrate our method on a variety of applications such as image editing, transfer of photographic look, and contrast enhancement of medical images.	Real-time edge-aware image processing with the bilateral grid	NA:NA:NA	2018
Adrien Bousseau:Fabrice Neyret:Joëlle Thollot:David Salesin	In this paper, we present a method for creating watercolor-like animation, starting from video as input. The method involves two main steps: applying textures that simulate a watercolor appearance; and creating a simplified, abstracted version of the video to which the texturing operations are applied. Both of these steps are subject to highly visible temporal artifacts, so the primary technical contributions of the paper are extensions of previous methods for texturing and abstraction to provide temporal coherence when applied to video sequences. To maintain coherence for textures, we employ texture advection along lines of optical flow. We furthermore extend previous approaches by incorporating advection in both forward and reverse directions through the video, which allows for minimal texture distortion, particularly in areas of disocclusion that are otherwise highly problematic. To maintain coherence for abstraction, we employ mathematical morphology extended to the temporal domain, using filters whose temporal extents are locally controlled by the degree of distortions in the optical flow. Together, these techniques provide the first practical and robust approach for producing watercolor animations from video, which we demonstrate with a number of examples.	Video watercolorization using bidirectional texture advection	NA:NA:NA:NA	2018
Okan Arikan	NA	Session details: Character animation II	NA	2018
KangKang Yin:Kevin Loken:Michiel van de Panne	Physics-based simulation and control of biped locomotion is difficult because bipeds are unstable, underactuated, high-dimensional dynamical systems. We develop a simple control strategy that can be used to generate a large variety of gaits and styles in real-time, including walking in all directions (forwards, backwards, sideways, turning), running, skipping, and hopping. Controllers can be authored using a small number of parameters, or their construction can be informed by motion capture data. The controllers are applied to 2D and 3D physically-simulated character models. Their robustness is demonstrated with respect to pushes in all directions, unexpected steps and slopes, and unexpected variations in kinematic and dynamic parameters. Direct transitions between controllers are demonstrated as well as parameterized control of changes in direction and speed. Feedback-error learning is applied to learn predictive torque models, which allows for the low-gain control that typifies many natural motions as well as producing smoother simulated motion.	SIMBICON: simple biped locomotion control	NA:NA:NA	2018
Alla Safonova:Jessica K. Hodgins	Many compelling applications would become feasible if novice users had the ability to synthesize high quality human motion based only on a simple sketch and a few easily specified constraints. We approach this problem by representing the desired motion as an interpolation of two time-scaled paths through a motion graph. The graph is constructed to support interpolation and pruned for efficient search. We use an anytime version of A* search to find a globally optimal solution in this graph that satisfies the user's specification. Our approach retains the natural transitions of motion graphs and the ability to synthesize physically realistic variations provided by interpolation. We demonstrate the power of this approach by synthesizing optimal or near optimal motions that include a variety of behaviors in a single motion.	Construction and optimal search of interpolated motion graphs	NA:NA	2018
Kwang Won Sok:Manmyung Kim:Jehee Lee	Physically based simulation of human motions is an important issue in the context of computer animation, robotics and biomechanics. We present a new technique for allowing our physically-simulated planar biped characters to imitate human behaviors. Our contribution is twofold. We developed an optimization method that transforms any (either motion-captured or kinematically synthesized) biped motion into a physically-feasible, balance-maintaining simulated motion. Our optimization method allows us to collect a rich set of training data that contains stylistic, personality-rich human behaviors. Our controller learning algorithm facilitates the creation and composition of robust dynamic controllers that are learned from training data. We demonstrate a planar articulated character that is dynamically simulated in real time, equipped with an integrated repertoire of motor skills, and controlled interactively to perform desired motions.	Simulating biped behaviors from human motion data	NA:NA:NA	2018
Aseem Agarwala	NA	Session details: Image collections and video	NA	2018
Huamin Wang:Yonatan Wexler:Eyal Ofek:Hugues Hoppe	We reduce transmission bandwidth and memory space for images by factoring their repeated content. A transform map and a condensed epitome are created such that all image blocks can be reconstructed from transformed epitome patches. The transforms may include affine deformation and color scaling to account for perspective and tonal variations across the image. The factored representation allows efficient random-access through a simple indirection, and can therefore be used for real-time texture mapping without expansion in memory. Our scheme is orthogonal to traditional image compression, in the sense that the epitome is amenable to further compression such as DXT. Moreover it allows a new mode of progressivity, whereby generic features appear before unique detail. Factoring is also effective across a collection of images, particularly in the context of image-based rendering. Eliminating redundant content lets us include textures that are several times as large in the same memory space.	Factoring repeated content within and among images	NA:NA:NA:NA	2018
Noah Snavely:Rahul Garg:Steven M. Seitz:Richard Szeliski	When a scene is photographed many times by different people, the viewpoints often cluster along certain paths. These paths are largely specific to the scene being photographed, and follow interesting regions and viewpoints. We seek to discover a range of such paths and turn them into controls for image-based rendering. Our approach takes as input a large set of community or personal photos, reconstructs camera viewpoints, and automatically computes orbits, panoramas, canonical views, and optimal paths between views. The scene can then be interactively browsed in 3D using these controls or with six degree-of-freedom free-viewpoint control. As the user browses the scene, nearby views are continuously selected and transformed, using control-adaptive reprojection techniques.	Finding paths through the world's photos	NA:NA:NA:NA	2018
Michael Rubinstein:Ariel Shamir:Shai Avidan	Video, like images, should support content aware resizing. We present video retargeting using an improved seam carving operator. Instead of removing 1D seams from 2D images we remove 2D seam manifolds from 3D space-time volumes. To achieve this we replace the dynamic programming method of seam carving with graph cuts that are suitable for 3D volumes. In the new formulation, a seam is given by a minimal cut in the graph and we show how to construct a graph such that the resulting cut is a valid seam. That is, the cut is monotonic and connected. In addition, we present a novel energy criterion that improves the visual quality of the retargeted images and videos. The original seam carving operator is focused on removing seams with the least amount of energy, ignoring energy that is introduced into the images and video by applying the operator. To counter this, the new criterion is looking forward in time - removing seams that introduce the least amount of energy into the retargeted result. We show how to encode the improved criterion into graph cuts (for images and video) as well as dynamic programming (for images). We apply our technique to images and videos and present results of various applications.	Improved seam carving for video retargeting	NA:NA:NA	2018
Alex Rav-Acha:Pushmeet Kohli:Carsten Rother:Andrew Fitzgibbon	We introduce a new representation for video which facilitates a number of common editing tasks. The representation has some of the power of a full reconstruction of 3D surface models from video, but is designed to be easy to recover from a priori unseen and uncalibrated footage. By modelling the image-formation process as a 2D-to-2D transformation from an object's texture map to the image, modulated by an object-space occlusion mask, we can recover a representation which we term the "unwrap mosaic". Many editing operations can be performed on the unwrap mosaic, and then re-composited into the original sequence, for example resizing objects, repainting textures, copying/cutting/pasting objects, and attaching effects layers to deforming objects.	Unwrap mosaics: a new representation for video editing	NA:NA:NA:NA	2018
Marc Olano	NA	Session details: Parallelism	NA	2018
Larry Seiler:Doug Carmean:Eric Sprangle:Tom Forsyth:Michael Abrash:Pradeep Dubey:Stephen Junkins:Adam Lake:Jeremy Sugerman:Robert Cavin:Roger Espasa:Ed Grochowski:Toni Juan:Pat Hanrahan	This paper presents a many-core visual computing architecture code named Larrabee, a new software rendering pipeline, a manycore programming model, and performance analysis for several applications. Larrabee uses multiple in-order x86 CPU cores that are augmented by a wide vector processor unit, as well as some fixed function logic blocks. This provides dramatically higher performance per watt and per unit of area than out-of-order CPUs on highly parallel workloads. It also greatly increases the flexibility and programmability of the architecture as compared to standard GPUs. A coherent on-die 2nd level cache allows efficient inter-processor communication and high-bandwidth local data access by CPU cores. Task scheduling is performed entirely with software in Larrabee, rather than in fixed function logic. The customizable software graphics rendering pipeline for this architecture uses binning in order to reduce required memory bandwidth, minimize lock contention, and increase opportunities for parallelism relative to standard GPUs. The Larrabee native programming model supports a variety of highly parallel applications that use irregular data structures. Performance analysis on those applications demonstrates Larrabee's potential for a broad range of parallel computation.	Larrabee: a many-core x86 architecture for visual computing	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Qiming Hou:Kun Zhou:Baining Guo	We present BSGP, a new programming language for general purpose computation on the GPU. A BSGP program looks much the same as a sequential C program. Programmers only need to supply a bare minimum of extra information to describe parallel processing on GPUs. As a result, BSGP programs are easy to read, write, and maintain. Moreover, the ease of programming does not come at the cost of performance. A well-designed BSGP compiler converts BSGP programs to kernels and combines them using optimally allocated temporary streams. In our benchmark, BSGP programs achieve similar or better performance than well-optimized CUDA programs, while the source code complexity and programming time are significantly reduced. To test BSGP's code efficiency and ease of programming, we implemented a variety of GPU applications, including a highly sophisticated X3D parser that would be extremely difficult to develop with existing GPU programming languages.	BSGP: bulk-synchronous GPU programming	NA:NA:NA	2018
Li-Yi Wei	Sampling is important for a variety of graphics applications include rendering, imaging, and geometry processing. However, producing sample sets with desired efficiency and blue noise statistics has been a major challenge, as existing methods are either sequential with limited speed, or are parallel but only through pre-computed datasets and thus fall short in producing samples with blue noise statistics. We present a Poisson disk sampling algorithm that runs in parallel and produces all samples on the fly with desired blue noise properties. Our main idea is to subdivide the sample domain into grid cells and we draw samples concurrently from multiple cells that are sufficiently far apart so that their samples cannot conflict one another. We present a parallel implementation of our algorithm running on a GPU with constant cost per sample and constant number of computation passes for a target number of samples. Our algorithm also works in arbitrary dimension, and allows adaptive sampling from a user-specified importance field. Furthermore, our algorithm is simple and easy to implement, and runs faster than existing techniques.	Parallel Poisson disk sampling	NA	2018
Michael Kazhdan:Hugues Hoppe	We introduce a new tool to solve the large linear systems arising from gradient-domain image processing. Specifically, we develop a streaming multigrid solver, which needs just two sequential passes over out-of-core data. This fast solution is enabled by a combination of three techniques: (1) use of second-order finite elements (rather than traditional finite differences) to reach sufficient accuracy in a single V-cycle, (2) temporally blocked relaxation, and (3) multi-level streaming to pipeline the restriction and prolongation phases into single streaming passes. A key contribution is the extension of the B-spline finite-element method to be compatible with the forward-difference gradient representation commonly used with images. Our streaming solver is also efficient for in-memory images, due to its fast convergence and excellent cache behavior. Remarkably, it can outperform spatially adaptive solvers that exploit application-specific knowledge. We demonstrate seamless stitching and tone-mapping of gigapixel images in about an hour on a notebook PC.	Streaming multigrid for gradient-domain operations on large images	NA:NA	2018
Miguel Otaduy	NA	Session details: Noisy collisions	NA	2018
Sung-Hee Lee:Demetri Terzopoulos	Spline joints are a novel class of joints that can model general scleronomic constraints for multibody dynamics based on the minimal-coordinates formulation. The main idea is to introduce spline curves and surfaces in the modeling of joints: We model 1-DOF joints using splines on SE(3), and construct multi-DOF joints as the product of exponentials of splines in Euclidean space. We present efficient recursive algorithms to compute the derivatives of the spline joint, as well as geometric algorithms to determine optimal parameters in order to achieve the desired joint motion. Our spline joints can be used to create interesting new simulated mechanisms for computer animation and they can more accurately model complex biomechanical joints such as the knee and shoulder.	Spline joints for multibody dynamics	NA:NA	2018
David Harmon:Etienne Vouga:Rasmus Tamstorf:Eitan Grinspun	Robust treatment of complex collisions is a challenging problem in cloth simulation. Some state of the art methods resolve collisions iteratively, invoking a fail-safe when a bound on iteration count is exceeded. The best-known fail-safe rigidifies the contact region, causing simulation artifacts. We present a fail-safe that cancels impact but not sliding motion, considerably reducing artificial dissipation. We equip the proposed fail-safe with an approximation of Coulomb friction, allowing finer control of sliding dissipation.	Robust treatment of simultaneous collisions	NA:NA:NA:NA	2018
Nicolas Bonneel:George Drettakis:Nicolas Tsingos:Isabelle Viaud-Delmon:Doug James	Audio rendering of impact sounds, such as those caused by falling objects or explosion debris, adds realism to interactive 3D audiovisual applications, and can be convincingly achieved using modal sound synthesis. Unfortunately, mode-based computations can become prohibitively expensive when many objects, each with many modes, are impacted simultaneously. We introduce a fast sound synthesis approach, based on short-time Fourier Tranforms, that exploits the inherent sparsity of modal sounds in the frequency domain. For our test scenes, this "fast mode summation" can give speedups of 5--8 times compared to a time-domain solution, with slight degradation in quality. We discuss different reconstruction windows, affecting the quality of impact sound "attacks". Our Fourier-domain processing method allows us to introduce a scalable, real-time, audio processing pipeline for both recorded and modal sounds, with auditory masking and sound source clustering. To avoid abrupt computation peaks, such as during the simultaneous impacts of an explosion, we use crossmodal perception results on audiovisual synchrony to effect temporal scheduling. We also conducted a pilot perceptual user evaluation of our method. Our implementation results show that we can treat complex audiovisual scenes in real time with high quality.	Fast modal sounds with scalable frequency-domain synthesis	NA:NA:NA:NA:NA	2018
Christopher D. Twigg:Doug L. James	Physically based simulation of rigid body dynamics is commonly done by time-stepping systems forward in time. In this paper, we propose methods to allow time-stepping rigid body systems back-ward in time. Unfortunately, reverse-time integration of rigid bodies involving frictional contact is mathematically ill-posed, and can lack unique solutions. We instead propose time-reversed rigid body integrators that can sample possible solutions when unique ones do not exist. We also discuss challenges related to dissipation-related energy gain, sensitivity to initial conditions, stacking, constraints and articulation, rolling, sliding, skidding, bouncing, high angular velocities, rapid velocity growth from micro-collisions, and other problems encountered when going against the usual flow of time.	Backward steps in rigid body simulation	NA:NA	2018
Karen Liu	NA	Session details: Characters	NA	2018
Rachel McDonnell:Michéal Larkin:Simon Dobbyn:Steven Collins:Carol O'Sullivan	When simulating large crowds, it is inevitable that the models and motions of many virtual characters will be cloned. However, the perceptual impact of this trade-off has never been studied. In this paper, we consider the ways in which an impression of variety can be created and the perceptual consequences of certain design choices. In a series of experiments designed to test people's perception of variety in crowds, we found that clones of appearance are far easier to detect than motion clones. Furthermore, we established that cloned models can be masked by color variation, random orientation, and motion. Conversely, the perception of cloned motions remains unaffected by the model on which they are displayed. Other factors that influence the ability to detect clones were examined, such as proximity, model type and characteristic motion. Our results provide novel insights and useful thresholds that will assist in creating more realistic, heterogeneous crowds.	Clone attack! Perception of crowd variety	NA:NA:NA:NA:NA	2018
Chris Hecker:Bernd Raabe:Ryan W. Enslow:John DeWeese:Jordan Maynard:Kees van Prooijen	Character animation in video games---whether manually keyframed or motion captured---has traditionally relied on codifying skeletons early in a game's development, and creating animations rigidly tied to these fixed skeleton morphologies. This paper introduces a novel system for animating characters whose morphologies are unknown at the time the animation is created. Our authoring tool allows animators to describe motion using familiar posing and key-framing methods. The system records the data in a morphology-independent form, preserving both the animation's structural relationships and its stylistic information. At runtime, the generalized data are applied to specific characters to yield pose goals that are supplied to a robust and efficient inverse kinematics solver. This system allows us to animate characters with highly varying skeleton morphologies that did not exist when the animation was authored, and, indeed, may be radically different than anything the original animator envisioned.	Real-time motion retargeting to highly varied user-created morphologies	NA:NA:NA:NA:NA:NA	2018
Michael Kass:John Anderson	Oscillatory motion is ubiquitous in computer graphics, yet existing animation techniques are ill-suited to its authoring. We introduce a new type of spline for this purpose, known as a "Wiggly Spline." The spline generalizes traditional piecewise cubics when its resonance and damping are set to zero, but creates oscillatory animation when its resonance and damping are changed. The spline provides a combination of direct manipulation and physical realism. To create overlapped and propagating motion, we generate phase shifts of the Wiggly Spline, and use these to control appropriate degrees of freedom in a model. The phase shifts can be created directly by procedural techniques or through a paint-like interface. A further option is to derive the phase shifts statistically by analyzing a time-series of a simulation. In this case, the Wiggly Spline makes it possible to canonicalize a simulation, generalize it by providing frequency and damping controls and control it through direct manipulation.	Animating oscillatory motion with overlap: wiggly splines	NA:NA	2018
Xiaohan Shi:Kun Zhou:Yiying Tong:Mathieu Desbrun:Hujun Bao:Baining Guo	In this paper we present an approach to enrich skeleton-driven animations with physically-based secondary deformation in real time. To achieve this goal, we propose a novel, surface-based deformable model that can interactively emulate the dynamics of both low-and high-frequency volumetric effects. Given a surface mesh and a few sample sequences of its physical behavior, a set of motion parameters of the material are learned during an off-line preprocessing step. The deformable model is then applicable to any given skeleton-driven animation of the surface mesh. Additionally, our dynamic skinning technique can be entirely implemented on GPUs and executed with great efficiency. Thus, with minimal changes to the conventional graphics pipeline, our approach can drastically enhance the visual experience of skeleton-driven animations by adding secondary deformation in real time.	Example-based dynamic skinning in real time	NA:NA:NA:NA:NA:NA	2018
Bruce Walter	NA	Session details: Hair and realistic rendering	NA	2018
Sylvain Paris:Will Chang:Oleg I. Kozhushnyan:Wojciech Jarosz:Wojciech Matusik:Matthias Zwicker:Frédo Durand	We accurately capture the shape and appearance of a person's hairstyle. We use triangulation and a sweep with planes of light for the geometry. Multiple projectors and cameras address the challenges raised by the reflectance and intricate geometry of hair. We introduce the use of structure tensors to infer the hidden geometry between the hair surface and the scalp. Our triangulation approach affords substantial accuracy improvement and we are able to measure elaborate hair geometry including complex curls and concavities. To reproduce the hair appearance, we capture a six-dimensional reflectance field. We introduce a new reflectance interpolation technique that leverages an analytical reflectance model to alleviate cross-fading artifacts caused by linear methods. Our results closely match the real hairstyles and can be used for animation.	Hair photobooth: geometric and photometric acquisition of real hairstyles	NA:NA:NA:NA:NA:NA:NA	2018
Jonathan T. Moon:Bruce Walter:Steve Marschner	Previous research has shown that a global multiple scattering simulation is needed to achieve physically realistic renderings of hair, particularly light-colored hair with low absorption. However, previous methods have either sacrificed accuracy or have been too computationally expensive for practical use. In this paper we describe a physically based, volumetric rendering method that computes multiple scattering solutions, including directional effects, much faster than previous accurate methods. Our two-pass method first traces light paths through a volumetric representation of the hair, contributing power to a 3D grid of spherical harmonic coefficients that store the directional distribution of scattered radiance everywhere in the hair volume. Then, in a ray tracing pass, multiple scattering is computed by integrating the stored radiance against the scattering functions of visible fibers using an efficient matrix multiplication. Single scattering is computed using conventional direct illumination methods. In our comparisons the new method produces quality similar to that of the best previous methods, but computes multiple scattering more than 10 times faster.	Efficient multiple scattering in hair using spherical harmonics	NA:NA:NA	2018
Arno Zinke:Cem Yuksel:Andreas Weber:John Keyser	When rendering light colored hair, multiple fiber scattering is essential for the right perception of the overall hair color. In this context, we present a novel technique to efficiently approximate multiple fiber scattering for a full head of human hair or a similar fiber based geometry. In contrast to previous ad-hoc approaches, our method relies on the physically accurate concept of the Bidirectional Scattering Distribution Functions and gives physically plausible results with no need for parameter tweaking. We show that complex scattering effects can be approximated very well by using aggressive simplifications based on this theoretical model. When compared to unbiased Monte-Carlo path tracing, our approximations preserve photo-realism in most settings but with rendering times at least two-orders of magnitude lower. Time and space complexity are much lower compared to photon mapping-based techniques and we can even achieve realistic results in real-time on a standard PC with consumer graphics hardware.	Dual scattering approximation for fast multiple scattering in hair	NA:NA:NA:NA	2018
Toshiya Hachisuka:Wojciech Jarosz:Richard Peter Weistroffer:Kevin Dale:Greg Humphreys:Matthias Zwicker:Henrik Wann Jensen	We present a new adaptive sampling strategy for ray tracing. Our technique is specifically designed to handle multidimensional sample domains, and it is well suited for efficiently generating images with effects such as soft shadows, motion blur, and depth of field. These effects are problematic for existing image based adaptive sampling techniques as they operate on pixels, which are possibly noisy results of a Monte Carlo ray tracing process. Our sampling technique operates on samples in the multidimensional space given by the rendering equation and as a consequence the value of each sample is noise-free. Our algorithm consists of two passes. In the first pass we adaptively generate samples in the multidimensional space, focusing on regions where the local contrast between samples is high. In the second pass we reconstruct the image by integrating the multidimensional function along all but the image dimensions. We perform a high quality anisotropic reconstruction by determining the extent of each sample in the multidimensional space using a structure tensor. We demonstrate our method on scenes with a 3 to 5 dimensional space, including soft shadows, motion blur, and depth of field. The results show that our method uses fewer samples than Mittchell's adaptive sampling technique while producing images with less noise.	Multidimensional adaptive sampling and reconstruction for ray tracing	NA:NA:NA:NA:NA:NA:NA	2018
Sumanta Pattanaik	NA	Session details: Real time rendering (mostly)	NA	2018
Thomas Annen:Zhao Dong:Tom Mertens:Philippe Bekaert:Hans-Peter Seidel:Jan Kautz	Shadow computation in dynamic scenes under complex illumination is a challenging problem. Methods based on precomputation provide accurate, real-time solutions, but are hard to extend to dynamic scenes. Specialized approaches for soft shadows can deal with dynamic objects but are not fast enough to handle more than one light source. In this paper, we present a technique for rendering dynamic objects under arbitrary environment illumination, which does not require any precomputation. The key ingredient is a fast, approximate technique for computing soft shadows, which achieves several hundred frames per second for a single light source. This allows for approximating environment illumination with a sparse collection of area light sources and yields real-time frame rates.	Real-time, all-frequency shadows in dynamic scenes	NA:NA:NA:NA:NA:NA	2018
Xin Sun:Kun Zhou:Eric Stollnitz:Jiaoying Shi:Baining Guo	We present a new technique for interactive relighting of dynamic refractive objects with complex material properties. We describe our technique in terms of a rendering pipeline in which each stage runs entirely on the GPU. The rendering pipeline converts surfaces to volumetric data, traces the curved paths of photons as they refract through the volume, and renders arbitrary views of the resulting radiance distribution. Our rendering pipeline is fast enough to permit interactive updates to lighting, materials, geometry, and viewing parameters without any precomputation. Applications of our technique include the visualization of caustics, absorption, and scattering while running physical simulations or while manipulating surfaces in real time.	Interactive relighting of dynamic refractive objects	NA:NA:NA:NA:NA	2018
Kun Zhou:Zhong Ren:Stephen Lin:Hujun Bao:Baining Guo:Heung-Yeung Shum	We present a real-time algorithm called compensated ray marching for rendering of smoke under dynamic low-frequency environment lighting. Our approach is based on a decomposition of the input smoke animation, represented as a sequence of volumetric density fields, into a set of radial basis functions (RBFs) and a sequence of residual fields. To expedite rendering, the source radiance distribution within the smoke is computed from only the low-frequency RBF approximation of the density fields, since the high-frequency residuals have little impact on global illumination under low-frequency environment lighting. Furthermore, in computing source radiances the contributions from single and multiple scattering are evaluated at only the RBF centers and then approximated at other points in the volume using an RBF-based interpolation. A slice-based integration of these source radiances along each view ray is then performed to render the final image. The high-frequency residual fields, which are a critical component in the local appearance of smoke, are compensated back into the radiance integral during this ray march to generate images of high detail. The runtime algorithm, which includes both light transfer simulation and ray marching, can be easily implemented on the GPU, and thus allows for real-time manipulation of viewpoint and lighting, as well as interactive editing of smoke attributes such as extinction cross section, scattering albedo, and phase function. Only moderate preprocessing time and storage is needed. This approach provides the first method for real-time smoke rendering that includes single and multiple scattering while generating results comparable in quality to offline algorithms like ray tracing.	Real-time smoke rendering using compensated ray marching	NA:NA:NA:NA:NA:NA	2018
Jaakko Lehtinen:Matthias Zwicker:Emmanuel Turquin:Janne Kontkanen:Frédo Durand:François X. Sillion:Timo Aila	We introduce a meshless hierarchical representation for solving light transport problems. Precomputed radiance transfer (PRT) and finite elements require a discrete representation of illumination over the scene. Non-hierarchical approaches such as per-vertex values are simple to implement, but lead to long precomputation. Hierarchical bases like wavelets lead to dramatic acceleration, but in their basic form they work well only on flat or smooth surfaces. We introduce a hierarchical function basis induced by scattered data approximation. It is decoupled from the geometric representation, allowing the hierarchical representation of illumination on complex objects. We present simple data structures and algorithms for constructing and evaluating the basis functions. Due to its hierarchical nature, our representation adapts to the complexity of the illumination, and can be queried at different scales. We demonstrate the power of the new basis in a novel precomputed direct-to-indirect light transport algorithm that greatly increases the complexity of scenes that can be handled by PRT approaches.	A meshless hierarchical representation for light transport	NA:NA:NA:NA:NA:NA:NA	2018
Jason Lawrence	NA	Session details: Faces & reflectance	NA	2018
Tommer Leyvand:Daniel Cohen-Or:Gideon Dror:Dani Lischinski	When human raters are presented with a collection of shapes and asked to rank them according to their aesthetic appeal, the results often indicate that there is a statistical consensus among the raters. Yet it might be difficult to define a succinct set of rules that capture the aesthetic preferences of the raters. In this work, we explore a data-driven approach to aesthetic enhancement of such shapes. Specifically, we focus on the challenging problem of enhancing the aesthetic appeal (or the attractiveness) of human faces in frontal photographs (portraits), while maintaining close similarity with the original. The key component in our approach is an automatic facial attractiveness engine trained on datasets of faces with accompanying facial attractiveness ratings collected from groups of human raters. Given a new face, we extract a set of distances between a variety of facial feature locations, which define a point in a high-dimensional "face space". We then search the face space for a nearby point with a higher predicted attractiveness rating. Once such a point is found, the corresponding facial distances are embedded in the plane and serve as a target to define a 2D warp field which maps the original facial features to their adjusted locations. The effectiveness of our technique was experimentally validated by independent rating experiments, which indicate that it is indeed capable of increasing the facial attractiveness of most portraits that we have experimented with.	Data-driven enhancement of facial attractiveness	NA:NA:NA:NA	2018
Dmitri Bitouk:Neeraj Kumar:Samreen Dhillon:Peter Belhumeur:Shree K. Nayar	In this paper, we present a complete system for automatic face replacement in images. Our system uses a large library of face images created automatically by downloading images from the internet, extracting faces using face detection software, and aligning each extracted face to a common coordinate system. This library is constructed off-line, once, and can be efficiently accessed during face replacement. Our replacement algorithm has three main stages. First, given an input image, we detect all faces that are present, align them to the coordinate system used by our face library, and select candidate face images from our face library that are similar to the input face in appearance and pose. Second, we adjust the pose, lighting, and color of the candidate face images to match the appearance of those in the input image, and seamlessly blend in the results. Third, we rank the blended candidate replacements by computing a match distance over the overlap region. Our approach requires no 3D model, is fully automatic, and generates highly plausible results across a wide range of skin tones, lighting conditions, and viewpoints. We show how our approach can be used for a variety of applications including face de-identification and the creation of appealing group photographs from a set of images. We conclude with a user study that validates the high quality of our replacement results, and a discussion on the current limitations of our system.	Face swapping: automatically replacing faces in photographs	NA:NA:NA:NA:NA	2018
Xiaobo An:Fabio Pellacini	We present an intuitive and efficient method for editing the appearance of complex spatially-varying datasets, such as images and measured materials. In our framework, users specify rough adjustments that are refined interactively by enforcing the policy that similar edits are applied to spatially-close regions of similar appearance. Rather than proposing a specific user interface, our method allows artists to quickly and imprecisely specify the initial edits with any method or workflow they feel most comfortable with. An energy optimization formulation is used to propagate the initial rough adjustments to the final refined ones by enforcing the editing policy over all pairs of points in the dataset. We show that this formulation is equivalent to solving a large linear system defined by a dense matrix. We derive an approximate algorithm to compute such a solution interactively by taking advantage of the inherent structure of the matrix. We demonstrate our approach by editing images, HDR radiance maps, and measured materials. Finally, we show that our framework generalizes prior methods while providing significant improvements in generality, robustness and efficiency.	AppProp: all-pairs appearance-space edit propagation	NA:NA	2018
Jiaping Wang:Shuang Zhao:Xin Tong:John Snyder:Baining Guo	We present a new technique for the visual modeling of spatiallyvarying anisotropic reflectance using data captured from a single view. Reflectance is represented using a microfacet-based BRDF which tabulates the facets' normal distribution (NDF) as a function of surface location. Data from a single view provides a 2D slice of the 4D BRDF at each surface point from which we fit a partial NDF. The fitted NDF is partial because the single view direction coupled with the set of light directions covers only a portion of the "half-angle" hemisphere. We complete the NDF at each point by applying a novel variant of texture synthesis using similar, overlapping partial NDFs from other points. Our similarity measure allows azimuthal rotation of partial NDFs, under the assumption that reflectance is spatially redundant but the local frame may be arbitrarily oriented. Our system includes a simple acquisition device that collects images over a 2D set of light directions by scanning a linear array of LEDs over a flat sample. Results demonstrate that our approach preserves spatial and directional BRDF details and generates a visually compelling match to measured materials.	Modeling anisotropic surface reflectance with example-based microfacet synthesis	NA:NA:NA:NA:NA	2018
Eitan Grinspun	NA	Session details: Shape analysis	NA	2018
Hongbo Fu:Daniel Cohen-Or:Gideon Dror:Alla Sheffer	Humans usually associate an upright orientation with objects, placing them in a way that they are most commonly seen in our surroundings. While it is an open challenge to recover the functionality of a shape from its geometry alone, this paper shows that it is often possible to infer its upright orientation by analyzing its geometry. Our key idea is to reduce the two-dimensional (spherical) orientation space to a small set of orientation candidates using functionality-related geometric properties of the object, and then determine the best orientation using an assessment function of several functional geometric attributes defined with respect to each candidate. Specifically we focus on obtaining the upright orientation for man-made objects that typically stand on some flat surface (ground, floor, table, etc.), which include the vast majority of objects in our everyday surroundings. For these types of models orientation candidates can be defined according to static equilibrium. For each candidate, we introduce a set of discriminative attributes linking shape to function. We learn an assessment function of these attributes from a training set using a combination of Random Forest classifier and Support Vector Machine classifier. Experiments demonstrate that our method generalizes well and achieves about 90% prediction accuracy for both a 10-fold cross-validation over the training set and a validation with an independent test set.	Upright orientation of man-made objects	NA:NA:NA:NA	2018
Mark Pauly:Niloy J. Mitra:Johannes Wallner:Helmut Pottmann:Leonidas J. Guibas	We introduce a computational framework for discovering regular or repeated geometric structures in 3D shapes. We describe and classify possible regular structures and present an effective algorithm for detecting such repeated geometric patterns in point- or meshbased models. Our method assumes no prior knowledge of the geometry or spatial location of the individual elements that define the pattern. Structure discovery is made possible by a careful analysis of pairwise similarity transformations that reveals prominent lattice structures in a suitable model of transformation space. We introduce an optimization method for detecting such uniform grids specifically designed to deal with outliers and missing elements. This yields a robust algorithm that successfully discovers complex regular structures amidst clutter, noise, and missing geometry. The accuracy of the extracted generating transformations is further improved using a novel simultaneous registration method in the spatial domain. We demonstrate the effectiveness of our algorithm on a variety of examples and show applications to compression, model repair, and geometry synthesis.	Discovering structural regularity in 3D geometry	NA:NA:NA:NA:NA	2018
Oscar Kin-Chung Au:Chiew-Lan Tai:Hung-Kuo Chu:Daniel Cohen-Or:Tong-Yee Lee	Extraction of curve-skeletons is a fundamental problem with many applications in computer graphics and visualization. In this paper, we present a simple and robust skeleton extraction method based on mesh contraction. The method works directly on the mesh domain, without pre-sampling the mesh model into a volumetric representation. The method first contracts the mesh geometry into zero-volume skeletal shape by applying implicit Laplacian smoothing with global positional constraints. The contraction does not alter the mesh connectivity and retains the key features of the original mesh. The contracted mesh is then converted into a 1D curve-skeleton through a connectivity surgery process to remove all the collapsed faces while preserving the shape of the contracted mesh and the original topology. The centeredness of the skeleton is refined by exploiting the induced skeleton-mesh mapping. In addition to producing a curve skeleton, the method generates other valuable information about the object's geometry, in particular, the skeleton-vertex correspondence and the local thickness, which are useful for various applications. We demonstrate its effectiveness in mesh segmentation and skinning animation.	Skeleton extraction by mesh contraction	NA:NA:NA:NA:NA	2018
Tamal K. Dey:Kuiyu Li:Jian Sun:David Cohen-Steiner	Many applications such as topology repair, model editing, surface parameterization, and feature recognition benefit from computing loops on surfaces that wrap around their 'handles' and 'tunnels'. Computing such loops while optimizing their geometric lengths is difficult. On the other hand, computing such loops without considering geometry is easy but may not be very useful. In this paper we strike a balance by computing topologically correct loops that are also geometrically relevant. Our algorithm is a novel application of the concepts from topological persistence introduced recently in computational topology. The usability of the computed loops is demonstrated with some examples in feature identification and topology simplification.	Computing geometry-aware handle and tunnel loops in 3D models	NA:NA:NA:NA	2018
Adam Bargteil	NA	Session details: Jiggly fluids	NA	2018
Avi Robinson-Mosher:Tamar Shinar:Jon Gretarsson:Jonathan Su:Ronald Fedkiw	We propose a novel solid/fluid coupling method that treats the coupled system in a fully implicit manner making it stable for arbitrary time steps, large density ratios, etc. In contrast to previous work in computer graphics, we derive our method using a simple back-of-the-envelope approach which lumps the solid and fluid momenta together, and which we show exactly conserves the momentum of the coupled system. Notably, our method uses the standard Cartesian fluid discretization and does not require (moving) conforming tetrahedral meshes or ALE frameworks. Furthermore, we use a standard Lagrangian framework for the solid, thus supporting arbitrary solid constitutive models, both implicit and explicit time integration, etc. The method is quite general, working for smoke, water, and multiphase fluids as well as both rigid and deformable solids, and both volumes and thin shells. Rigid shells and cloth are handled automatically without special treatment, and we support fully one-sided discretizations without leaking. Our equations are fully symmetric, allowing for the use of fast solvers, which is a natural result of properly conserving momentum. Finally, for simple explicit time integration of rigid bodies, we show that our equations reduce to form similar to previous work via a single block Gaussian elimination operation, but that this approach scales poorly, i.e. as though four spatial dimensions rather than three.	Two-way coupling of fluids to rigid and deformable solids and shells	NA:NA:NA:NA:NA	2018
Chris Wojtan:Greg Turk	We introduce a method for efficiently animating a wide range of deformable materials. We combine a high resolution surface mesh with a tetrahedral finite element simulator that makes use of frequent re-meshing. This combination allows for fast and detailed simulations of complex elastic and plastic behavior. We significantly expand the range of physical parameters that can be simulated with a single technique, and the results are free from common artifacts such as volume-loss, smoothing, popping, and the absence of thin features like strands and sheets. Our decision to couple a high resolution surface with low-resolution physics leads to efficient simulation and detailed surface features, and our approach to creating the tetrahedral mesh leads to an order-of-magnitude speedup over previous techniques in the time spent re-meshing. We compute masses, collisions, and surface tension forces on the scale of the fine mesh, which helps avoid visual artifacts due to the differing mesh resolutions. The result is a method that can simulate a large array of different material behaviors with high resolution features in a short amount of time.	Fast viscoelastic behavior with thin features	NA:NA	2018
Jeong-Mo Hong:Ho-Young Lee:Jong-Chul Yoon:Chang-Hun Kim	We propose a hybrid method for simulating multiphase fluids such as bubbly water. The appearance of subgrid visual details is improved by incorporating a new bubble model based on smoothed particle hydrodynamics (SPH) into an Eulerian grid-based simulation that handles background flows of large bodies of water and air. To overcome the difficulty in simulating small bubbles in the context of the multiphase flows on a coarse grid, we heuristically model the interphase properties of water and air by means of the interactions between bubble particles. As a result, we can animate lively motion of bubbly water with small scale details efficiently.	Bubbles alive	NA:NA:NA:NA	2018
Toon Lenaerts:Bart Adams:Philip Dutré	This paper presents the simulation of a fluid flowing through a porous deformable material. We introduce the physical principles governing porous flow, expressed by the Law of Darcy, into the Smoothed Particle Hydrodynamics (SPH) framework for simulating fluids and deformable objects. Contrary to previous SPH approaches, we simulate porous flow at a macroscopic scale, making abstraction of individual pores or cavities inside the material. Thus, the number of computational elements is kept low, while at the same time realistic simulations can be achieved. Our algorithm models the changing behavior of the wet material as well as the full two-way coupling between the fluid and the porous material. This enables various new effects, such as the simulation of sponge-like elastic bodies and water-absorbing sticky cloth.	Porous flow in particle-based fluid simulations	NA:NA:NA	2018
Theodore Kim:Nils Thürey:Doug James:Markus Gross	We present a novel wavelet method for the simulation of fluids at high spatial resolution. The algorithm enables large- and small-scale detail to be edited separately, allowing high-resolution detail to be added as a post-processing step. Instead of solving the Navier-Stokes equations over a highly refined mesh, we use the wavelet decomposition of a low-resolution simulation to determine the location and energy characteristics of missing high-frequency components. We then synthesize these missing components using a novel incompressible turbulence function, and provide a method to maintain the temporal coherence of the resulting structures. There is no linear system to solve, so the method parallelizes trivially and requires only a few auxiliary arrays. The method guarantees that the new frequencies will not interfere with existing frequencies, allowing animators to set up a low resolution simulation quickly and later add details without changing the overall fluid motion.	Wavelet turbulence for fluid simulation	NA:NA:NA:NA	2018
Yizhou Yu	NA	Session details: Texture	NA	2018
Charles Han:Eric Risser:Ravi Ramamoorthi:Eitan Grinspun	Example-based texture synthesis algorithms have gained widespread popularity for their ability to take a single input image and create a perceptually similar non-periodic texture. However, previous methods rely on single input exemplars that can capture only a limited band of spatial scales. For example, synthesizing a continent-like appearance at a variety of zoom levels would require an impractically high input resolution. In this paper, we develop a multiscale texture synthesis algorithm. We propose a novel example-based representation, which we call an exemplar graph, that simply requires a few low-resolution input exemplars at different scales. Moreover, by allowing loops in the graph, we can create infinite zooms and infinitely detailed textures that are impossible with current example-based methods. We also introduce a technique that ameliorates inconsistencies in the user's input, and show that the application of this method yields improved interscale coherence and higher visual quality. We demonstrate optimizations for both CPU and GPU implementations of our method, and use them to produce animations with zooming and panning at multiple scales, as well as static gigapixel-sized images with features spanning many spatial scales.	Multiscale texture synthesis	NA:NA:NA:NA	2018
Li-Yi Wei:Jianwei Han:Kun Zhou:Hujun Bao:Baining Guo:Heung-Yeung Shum	The quality and speed of most texture synthesis algorithms depend on a 2D input sample that is small and contains enough texture variations. However, little research exists on how to acquire such sample. For homogeneous patterns this can be achieved via manual cropping, but no adequate solution exists for inhomogeneous or globally varying textures, i.e. patterns that are local but not stationary, such as rusting over an iron statue with appearance conditioned on varying moisture levels. We present inverse texture synthesis to address this issue. Our inverse synthesis runs in the opposite direction with respect to traditional forward synthesis: given a large globally varying texture, our algorithm automatically produces a small texture compaction that best summarizes the original. This small compaction can be used to reconstruct the original texture or to re-synthesize new textures under user-supplied controls. More important, our technique allows real-time synthesis of globally varying textures on a GPU, where the texture memory is usually too small for large textures. We propose an optimization framework for inverse texture synthesis, ensuring that each input region is properly encoded in the output compaction. Our optimization process also automatically computes orientation fields for anisotropic textures containing both low- and high-frequency regions, a situation difficult to handle via existing techniques.	Inverse texture synthesis	NA:NA:NA:NA:NA:NA	2018
Kenshi Takayama:Makoto Okabe:Takashi Ijiri:Takeo Igarashi	We present a method for representing solid objects with spatially-varying oriented textures by repeatedly pasting solid texture exemplars. The underlying concept is to extend the 2D texture patch-pasting approach of lapped textures to 3D solids using a tetrahedral mesh and 3D texture patches. The system places texture patches according to the user-defined volumetric tensor fields over the mesh to represent oriented textures. We have also extended the original technique to handle nonhomogeneous textures for creating solid models whose textural patterns change gradually along the depth fields. We identify several texture types considering the amount of anisotropy and spatial variation and provide a tailored user interface for each. With our simple framework, large-scale realistic solid models can be created easily with little memory and computational cost. We demonstrate the effectiveness of our approach with several examples including trees, fruits, and vegetables.	Lapped solid textures: filling a model with anisotropic textures	NA:NA:NA:NA	2018
Alexander Goldberg:Matthias Zwicker:Frédo Durand	Programmable graphics hardware makes it possible to generate procedural noise textures on the fly for interactive rendering. However, filtering and antialiasing procedural noise involves a tradeoff between aliasing artifacts and loss of detail. In this paper we present a technique, targeted at interactive applications, that provides high-quality anisotropic filtering for noise textures. We generate noise tiles directly in the frequency domain by partitioning the frequency domain into oriented subbands. We then compute weighted sums of the subband textures to accurately approximate noise with a desired spectrum. This allows us to achieve high-quality anisotropic filtering. Our approach is based solely on 2D textures, avoiding the memory overhead of techniques based on 3D noise tiles. We devise a technique to compensate for texture distortions to generate uniform noise on arbitrary meshes. We develop a GPU-based implementation of our technique that achieves similar rendering performance as state-of-the-art algorithms for procedural noise. In addition, it provides anisotropic filtering and achieves superior image quality.	Anisotropic noise	NA:NA:NA	2018
Wojciech Matusik	NA	Session details: Computational photography & display	NA	2018
Chia-Kai Liang:Tai-Hsu Lin:Bing-Yi Wong:Chi Liu:Homer H. Chen	In this paper, we present a system including a novel component called programmable aperture and two associated post-processing algorithms for high-quality light field acquisition. The shape of the programmable aperture can be adjusted and used to capture light field at full sensor resolution through multiple exposures without any additional optics and without moving the camera. High acquisition efficiency is achieved by employing an optimal multiplexing scheme, and quality data is obtained by using the two post-processing algorithms designed for self calibration of photometric distortion and for multi-view depth estimation. View-dependent depth maps thus generated help boost the angular resolution of light field. Various post-exposure photographic effects are given to demonstrate the effectiveness of the system and the quality of the captured light field.	Programmable aperture photography: multiplexed light field acquisition	NA:NA:NA:NA:NA	2018
Ramesh Raskar:Amit Agrawal:Cyrus A. Wilson:Ashok Veeraraghavan	Glare arises due to multiple scattering of light inside the camera's body and lens optics and reduces image contrast. While previous approaches have analyzed glare in 2D image space, we show that glare is inherently a 4D ray-space phenomenon. By statistically analyzing the ray-space inside a camera, we can classify and remove glare artifacts. In ray-space, glare behaves as high frequency noise and can be reduced by outlier rejection. While such analysis can be performed by capturing the light field inside the camera, it results in the loss of spatial resolution. Unlike light field cameras, we do not need to reversibly encode the spatial structure of the ray-space, leading to simpler designs. We explore masks for uniform and non-uniform ray sampling and show a practical solution to analyze the 4D statistics without significantly compromising image resolution. Although diffuse scattering of the lens introduces 4D low-frequency glare, we can produce useful solutions in a variety of common scenarios. Our approach handles photography looking into the sun and photos taken without a hood, removes the effect of lens smudges and reduces loss of contrast due to camera body reflections. We show various applications in contrast enhancement and glare manipulation.	Glare aware photography: 4D ray sampling for reducing glare effects of camera lenses	NA:NA:NA:NA	2018
Oliver Cossairt:Shree Nayar:Ravi Ramamoorthi	We present a novel image-based method for compositing real and synthetic objects in the same scene with a high degree of visual realism. Ours is the first technique to allow global illumination and near-field lighting effects between both real and synthetic objects at interactive rates, without needing a geometric and material model of the real scene. We achieve this by using a light field interface between real and synthetic components---thus, indirect illumination can be simulated using only two 4D light fields, one captured from and one projected onto the real scene. Multiple bounces of interreflections are obtained simply by iterating this approach. The interactivity of our technique enables its use with time-varying scenes, including dynamic objects. This is in sharp contrast to the alternative approach of using 6D or 8D light transport functions of real objects, which are very expensive in terms of acquisition and storage and hence not suitable for real-time applications. In our method, 4D radiance fields are simultaneously captured and projected by using a lens array, video camera, and digital projector. The method supports full global illumination with restricted object placement, and accommodates moderately specular materials. We implement a complete system and show several example scene compositions that demonstrate global illumination effects between dynamic real and synthetic objects. Our implementation requires a single point light source and dark background.	Light field transfer: global illumination between real and synthetic objects	NA:NA:NA	2018
Martin Fuchs:Ramesh Raskar:Hans-Peter Seidel:Hendrik P. A. Lensch	Traditional flat screen displays present 2D images. 3D and 4D displays have been proposed making use of lenslet arrays to shape a fixed outgoing light field for horizontal or bidirectional parallax. In this article, we present different designs of multi-dimensional displays which passively react to the light of the environment behind. The prototypes physically implement a reflectance field and generate different light fields depending on the incident illumination, for example light falling through a window. We discretize the incident light field using an optical system, and modulate it with a 2D pattern, creating a flat display which is view and illumination-dependent. It is free from electronic components. For distant light and a fixed observer position, we demonstrate a passive optical configuration which directly renders a 4D reflectance field in the real-world illumination behind it. We further propose an optical setup that allows for projecting out different angular distributions depending on the incident light direction. Combining multiple of these devices we build a display that renders a 6D experience, where the incident 2D illumination influences the outgoing light field, both in the spatial and in the angular domain. Possible applications of this technology are time-dependent displays driven by sunlight, object virtualization and programmable light benders / ray blockers without moving parts.	Towards passive 6D reflectance field displays	NA:NA:NA:NA	2018
Karol Myszkowski	NA	Session details: Perception & hallucination	NA	2018
Mashhuda Glencross:Gregory J. Ward:Francho Melendez:Caroline Jay:Jun Liu:Roger Hubbold	Capturing detailed surface geometry currently requires specialized equipment such as laser range scanners, which despite their high accuracy, leave gaps in the surfaces that must be reconciled with photographic capture for relighting applications. Using only a standard digital camera and a single view, we present a method for recovering models of predominantly diffuse textured surfaces that can be plausibly relit and viewed from any angle under any illumination. Our multiscale shape-from-shading technique uses diffuse-lit/flash-lit image pairs to produce an albedo map and textured height field. Using two lighting conditions enables us to subtract one from the other to estimate albedo. In the absence of a flash-lit image of a surface for which we already have a similar exemplar pair, we approximate both albedo and diffuse shading images using histogram matching. Our depth estimation is based on local visibility. Unlike other depth-from-shading approaches, all operations are performed on the diffuse shading image in image space, and we impose no constant albedo restrictions. An experimental validation shows our method works for a broad range of textured surfaces, and viewers are frequently unable to identify our results as synthetic in a randomized presentation. Furthermore, in side-by-side comparisons, subjects found a rendering of our depth map equally plausible to one generated from a laser range scan. We see this method as a significant advance in acquiring surface detail for texturing using a standard digital camera, with applications in architecture, archaeological reconstruction, games and special effects.	A perceptually validated model for surface depth hallucination	NA:NA:NA:NA:NA:NA	2018
Ganesh Ramanarayanan:Kavita Bala:James A. Ferwerda	Aggregates of individual objects, such as forests, crowds, and piles of fruit, are a common source of complexity in computer graphics scenes. When viewing an aggregate, observers attend less to individual objects and focus more on overall properties such as numerosity, variety, and arrangement. Paradoxically, rendering and modeling costs increase with aggregate complexity, exactly when observers are attending less to individual objects. In this paper we take some first steps to characterize the limits of visual coding of aggregates to efficiently represent their appearance in scenes. We describe psychophysical experiments that explore the roles played by the geometric and material properties of individual objects in observers' abilities to discriminate different aggregate collections. Based on these experiments we derive metrics to predict when two aggregates have the same appearance, even when composed of different objects. In a follow-up experiment we confirm that these metrics can be used to predict the appearance of a range of realistic aggregates. Finally, as a proof-of-concept we show how these new aggregate perception metrics can be applied to simplify scenes by allowing substitution of geometrically simpler aggregates for more complex ones without changing appearance.	Perception of complex aggregates	NA:NA:NA	2018
Hamilton Y. Chong:Steven J. Gortler:Todd Zickler	Motivated by perceptual principles, we derive a new color space in which the associated metric approximates perceived distances and color displacements capture relationships that are robust to spectral changes in illumination. The resulting color space can be used with existing image processing algorithms with little or no change to the methods.	A perception-based color space for illumination-invariant image processing	NA:NA:NA	2018
Ming-Te Chi:Tong-Yee Lee:Yingge Qu:Tien-Tsin Wong	Illusory motion in a still image is a fascinating research topic in the study of human motion perception. Physiologists and psychologists have attempted to understand this phenomenon by constructing simple, color repeated asymmetric patterns (RAP) and have found several useful rules to enhance the strength of illusory motion. Based on their knowledge, we propose a computational method to generate self-animating images. First, we present an optimized RAP placement on streamlines to generate illusory motion for a given static vector field. Next, a general coloring scheme for RAP is proposed to render streamlines. Furthermore, to enhance the strength of illusion and respect the shape of the region, a smooth vector field with opposite directional flow is automatically generated given an input image. Examples generated by our method are shown as evidence of the illusory effect and the potential applications for entertainment and design purposes.	Self-animating images: illusory motion using repeated asymmetric patterns	NA:NA:NA:NA	2018
Mark Carlson	NA	Session details: Hair, rods & cloth	NA	2018
Miklós Bergou:Max Wardetzky:Stephen Robinson:Basile Audoly:Eitan Grinspun	We present a discrete treatment of adapted framed curves, parallel transport, and holonomy, thus establishing the language for a discrete geometric model of thin flexible rods with arbitrary cross section and undeformed configuration. Our approach differs from existing simulation techniques in the graphics and mechanics literature both in the kinematic description---we represent the material frame by its angular deviation from the natural Bishop frame---as well as in the dynamical treatment---we treat the centerline as dynamic and the material frame as quasistatic. Additionally, we describe a manifold projection method for coupling rods to rigid-bodies and simultaneously enforcing rod inextensibility. The use of quasistatics and constraints provides an efficient treatment for stiff twisting and stretching modes; at the same time, we retain the dynamic bending of the centerline and accurately reproduce the coupling between bending and twisting modes. We validate the discrete rod model via quantitative buckling, stability, and coupled-mode experiments, and via qualitative knot-tying comparisons.	Discrete elastic rods	NA:NA:NA:NA:NA	2018
Andrew Selle:Michael Lentine:Ronald Fedkiw	Our goal is to simulate the full hair geometry, consisting of approximately one hundred thousand hairs on a typical human head. This will require scalable methods that can simulate every hair as opposed to only a few guide hairs. Novel to this approach is that the individual hair/hair interactions can be modeled with physical parameters (friction, static attraction, etc.) at the scale of a single hair as opposed to clumped or continuum interactions. In this vein, we first propose a new altitude spring model for preventing collapse in the simulation of volumetric tetrahedra, and we show that it is also applicable both to bending in cloth and torsion in hair. We demonstrate that this new torsion model for hair behaves in a fashion similar to more sophisticated models with significantly reduced computational cost. For added efficiency, we introduce a semi-implicit discretization of standard springs that makes them truly linear in multiple spatial dimensions and thus unconditionally stable without requiring Newton-Raphson iteration. We also simulate complex hair/hair interactions including sticking and clumping behavior, collisions with objects (e.g. head and shoulders) and self-collisions. Notably, in line with our goal to simulate the full head of hair, we do not generate any new hairs at render time.	A mass spring model for hair simulation	NA:NA:NA	2018
Jonathan M. Kaldor:Doug L. James:Steve Marschner	Knitted fabric is widely used in clothing because of its unique and stretchy behavior, which is fundamentally different from the behavior of woven cloth. The properties of knits come from the nonlinear, three-dimensional kinematics of long, inter-looping yarns, and despite significant advances in cloth animation we still do not know how to simulate knitted fabric faithfully. Existing cloth simulators mainly adopt elastic-sheet mechanical models inspired by woven materials, focusing less on the model itself than on important simulation challenges such as efficiency, stability, and robustness. We define a new computational model for knits in terms of the motion of yarns, rather than the motion of a sheet. Each yarn is modeled as an inextensible, yet otherwise flexible, B-spline tube. To simulate complex knitted garments, we propose an implicit-explicit integrator, with yarn inextensibility constraints imposed using efficient projections. Friction among yarns is approximated using rigid-body velocity filters, and key yarn-yarn interactions are mediated by stiff penalty forces. Our results show that this simple model predicts the key mechanical properties of different knits, as demonstrated by qualitative comparisons to observed deformations of actual samples in the laboratory, and that the simulator can scale up to substantial animations with complex dynamic motion.	Simulating knitted cloth at the yarn level	NA:NA:NA	2018
Elliot English:Robert Bridson	We present a new discretization for the physics-based animation of developable surfaces. Constrained to not deform at all in-plane but free to bend out-of-plane, these are an excellent approximation for many materials, including most cloth, paper, and stiffer materials. Unfortunately the conforming (geometrically continuous) discretizations used in graphics break down in this limit. Our nonconforming approach solves this problem, allowing us to simulate surfaces with zero in-plane deformation as a hard constraint. However, it produces discontinuous meshes, so we further couple this with a "ghost" conforming mesh for collision processing and rendering. We also propose a new second order accurate constrained mechanics time integration method that greatly reduces the numerical damping present in the usual first order methods used in graphics, for virtually no extra cost and sometimes significant speed-up.	Animating developable surfaces using nonconforming elements	NA:NA	2018
Ramesh Raskar	NA	Session details: Tone & color	NA	2018
Zeev Farbman:Raanan Fattal:Dani Lischinski:Richard Szeliski	Many recent computational photography techniques decompose an image into a piecewise smooth base layer, containing large scale variations in intensity, and a residual detail layer capturing the smaller scale details in the image. In many of these applications, it is important to control the spatial scale of the extracted details, and it is often desirable to manipulate details at multiple scales, while avoiding visual artifacts. In this paper we introduce a new way to construct edge-preserving multi-scale image decompositions. We show that current basedetail decomposition techniques, based on the bilateral filter, are limited in their ability to extract detail at arbitrary scales. Instead, we advocate the use of an alternative edge-preserving smoothing operator, based on the weighted least squares optimization framework, which is particularly well suited for progressive coarsening of images and for multi-scale detail extraction. After describing this operator, we show how to use it to construct edge-preserving multi-scale decompositions, and compare it to the bilateral filter, as well as to other schemes. Finally, we demonstrate the effectiveness of our edge-preserving decompositions in the context of LDR and HDR tone mapping, detail enhancement, and other applications.	Edge-preserving decompositions for multi-scale tone and detail manipulation	NA:NA:NA:NA	2018
Rafał Mantiuk:Scott Daly:Louis Kerofsky	We propose a tone mapping operator that can minimize visible contrast distortions for a range of output devices, ranging from e-paper to HDR displays. The operator weights contrast distortions according to their visibility predicted by the model of the human visual system. The distortions are minimized given a display model that enforces constraints on the solution. We show that the problem can be solved very efficiently by employing higher order image statistics and quadratic programming. Our tone mapping technique can adjust image or video content for optimum contrast visibility taking into account ambient illumination and display characteristics. We discuss the differences between our method and previous approaches to the tone mapping problem.	Display adaptive tone mapping	NA:NA:NA	2018
Tunç Ozan Aydin:Rafał Mantiuk:Karol Myszkowski:Hans-Peter Seidel	The diversity of display technologies and introduction of high dynamic range imagery introduces the necessity of comparing images of radically different dynamic ranges. Current quality assessment metrics are not suitable for this task, as they assume that both reference and test images have the same dynamic range. Image fidelity measures employed by a majority of current metrics, based on the difference of pixel intensity or contrast values between test and reference images, result in meaningless predictions if this assumption does not hold. We present a novel image quality metric capable of operating on an image pair where both images have arbitrary dynamic ranges. Our metric utilizes a model of the human visual system, and its central idea is a new definition of visible distortion based on the detection and classification of visible changes in the image structure. Our metric is carefully calibrated and its performance is validated through perceptual experiments. We demonstrate possible applications of our metric to the evaluation of direct and inverse tone mapping operators as well as the analysis of the image appearance on displays with various characteristics.	Dynamic range independent image quality assessment	NA:NA:NA:NA	2018
Eugene Hsu:Tom Mertens:Sylvain Paris:Shai Avidan:Frédo Durand	White balance is a crucial step in the photographic pipeline. It ensures the proper rendition of images by eliminating color casts due to differing illuminants. Digital cameras and editing programs provide white balance tools that assume a single type of light per image, such as daylight. However, many photos are taken under mixed lighting. We propose a white balance technique for scenes with two light types that are specified by the user. This covers many typical situations involving indoor/outdoor or flash/ambient light mixtures. Since we work from a single image, the problem is highly underconstrained. Our method recovers a set of dominant material colors which allows us to estimate the local intensity mixture of the two light types. Using this mixture, we can neutralize the light colors and render visually pleasing images. Our method can also be used to achieve post-exposure relighting effects.	Light mixture estimation for spatially varying white balance	NA:NA:NA:NA:NA	2018
Hendrik Lensch	NA	Session details: Deblurring & dehazing	NA	2018
Anat Levin:Peter Sand:Taeg Sang Cho:Frédo Durand:William T. Freeman	Object motion during camera exposure often leads to noticeable blurring artifacts. Proper elimination of this blur is challenging because the blur kernel is unknown, varies over the image as a function of object velocity, and destroys high frequencies. In the case of motions along a 1D direction (e.g. horizontal) we show that these challenges can be addressed using a camera that moves during the exposure. Through the analysis of motion blur as space-time integration, we show that a parabolic integration (corresponding to constant sensor acceleration) leads to motion blur that is invariant to object velocity. Thus, a single deconvolution kernel can be used to remove blur and create sharp images of scenes with objects moving at different speeds, without requiring any segmentation and without knowledge of the object speeds. Apart from motion invariance, we prove that the derived parabolic motion preserves image frequency content nearly optimally. That is, while static objects are degraded relative to their image from a static camera, a reliable reconstruction of all moving objects within a given velocities range is made possible. We have built a prototype camera and present successful deblurring results over a wide variety of human motions.	Motion-invariant photography	NA:NA:NA:NA:NA	2018
Raanan Fattal	In this paper we present a new method for estimating the optical transmission in hazy scenes given a single input image. Based on this estimation, the scattered light is eliminated to increase scene visibility and recover haze-free scene contrasts. In this new approach we formulate a refined image formation model that accounts for surface shading in addition to the transmission function. This allows us to resolve ambiguities in the data by searching for a solution in which the resulting shading and transmission functions are locally statistically uncorrelated. A similar principle is used to estimate the color of the haze. Results demonstrate the new method abilities to remove the haze layer as well as provide a reliable transmission estimate which can be used for additional applications such as image refocusing and novel view synthesis.	Single image dehazing	NA	2018
Qi Shan:Jiaya Jia:Aseem Agarwala	We present a new algorithm for removing motion blur from a single image. Our method computes a deblurred image using a unified probabilistic model of both blur kernel estimation and unblurred image restoration. We present an analysis of the causes of common artifacts found in current deblurring methods, and then introduce several novel terms within this probabilistic model that are inspired by our analysis. These terms include a model of the spatial randomness of noise in the blurred image, as well a new local smoothness prior that reduces ringing artifacts by constraining contrast in the unblurred image wherever the blurred image exhibits low contrast. Finally, we describe an effficient optimization scheme that alternates between blur kernel estimation and unblurred image restoration until convergence. As a result of these steps, we are able to produce high quality deblurred results in low computation time. We are even able to produce results of comparable quality to techniques that require additional input images beyond a single blurry photograph, and to methods that require additional hardware.	High-quality motion deblurring from a single image	NA:NA:NA	2018
Lu Yuan:Jian Sun:Long Quan:Heung-Yeung Shum	Ringing is the most disturbing artifact in the image deconvolution. In this paper, we present a progressive inter-scale and intra-scale non-blind image deconvolution approach that significantly reduces ringing. Our approach is built on a novel edge-preserving deconvolution algorithm called bilateral Richardson-Lucy (BRL) which uses a large spatial support to handle large blur. We progressively recover the image from a coarse scale to a fine scale (inter-scale), and progressively restore image details within every scale (intra-scale). To perform the inter-scale deconvolution, we propose a joint bilateral Richardson-Lucy (JBRL) algorithm so that the recovered image in one scale can guide the deconvolution in the next scale. In each scale, we propose an iterative residual deconvolution to progressively recover image details. The experimental results show that our progressive deconvolution can produce images with very little ringing for large blur kernels.	Progressive inter-scale and intra-scale non-blind image deconvolution	NA:NA:NA:NA	2018
Bruno Levy	NA	Session details: Folding & unfolding surfaces	NA	2018
Martin Kilian:Simon Flöry:Zhonggui Chen:Niloy J. Mitra:Alla Sheffer:Helmut Pottmann	Fascinating and elegant shapes may be folded from a single planar sheet of material without stretching, tearing or cutting, if one incorporates curved folds into the design. We present an optimization-based computational framework for design and digital reconstruction of surfaces which can be produced by curved folding. Our work not only contributes to applications in architecture and industrial design, but it also provides a new way to study the complex and largely unexplored phenomena arising in curved folding.	Curved folding	NA:NA:NA:NA:NA:NA	2018
Helmut Pottmann:Alexander Schiftner:Pengbo Bo:Heinz Schmiedhofer:Wenping Wang:Niccolo Baldassini:Johannes Wallner	Motivated by applications in architecture and manufacturing, we discuss the problem of covering a freeform surface by single curved panels. This leads to the new concept of semi-discrete surface representation, which constitutes a link between smooth and discrete surfaces. The basic entity we are working with is the developable strip model. It is the semi-discrete equivalent of a quad mesh with planar faces, or a conjugate parametrization of a smooth surface. We present a B-spline based optimization framework for efficient computing with D-strip models. In particular we study conical and circular models, which semi-discretize the network of principal curvature lines, and which enjoy elegant geometric properties. Together with geodesic models and cylindrical models they offer a rich source of solutions for surface panelization problems.	Freeform surfaces from single curved panels	NA:NA:NA:NA:NA:NA:NA	2018
Boris Springborn:Peter Schröder:Ulrich Pinkall	We present a new algorithm for conformal mesh parameterization. It is based on a precise notion of discrete conformal equivalence for triangle meshes which mimics the notion of conformal equivalence for smooth surfaces. The problem of finding a flat mesh that is discretely conformally equivalent to a given mesh can be solved efficiently by minimizing a convex energy function, whose Hessian turns out to be the well known cot-Laplace operator. This method can also be used to map a surface mesh to a parameter domain which is flat except for isolated cone singularities, and we show how these can be placed automatically in order to reduce the distortion of the parameterization. We present the salient features of the theory and elaborate the algorithms with a number of examples.	Conformal equivalence of triangle meshes	NA:NA:NA	2018
Yaron Lipman:David Levin:Daniel Cohen-Or	We introduce Green Coordinates for closed polyhedral cages. The coordinates are motivated by Green's third integral identity and respect both the vertices position and faces orientation of the cage. We show that Green Coordinates lead to space deformations with a shape-preserving property. In particular, in 2D they induce conformal mappings, and extend naturally to quasi-conformal mappings in 3D. In both cases we derive closed-form expressions for the coordinates, yielding a simple and fast algorithm for cage-based space deformation. We compare the performance of Green Coordinates with those of Mean Value Coordinates and Harmonic Coordinates and show that the advantage of the shape-preserving property is not achieved at the expense of speed or simplicity. We also show that the new coordinates extend the mapping in a natural analytic manner to the exterior of the cage, allowing the employment of partial cages.	Green Coordinates	NA:NA:NA	2018
Thomas W. Sederberg:G. Thomas Finnigan:Xin Li:Hongwei Lin:Heather Ipson	This paper addresses the long-standing problem of the unavoidable gaps that arise when expressing the intersection of two NURBS surfaces using conventional trimmed-NURBS representation. The solution converts each trimmed NURBS into an untrimmed T-Spline, and then merges the untrimmed T-Splines into a single, watertight model. The solution enables watertight fillets of NURBS models, as well as arbitrary feature curves that do not have to follow iso-parameter curves. The resulting T-Spline representation can be exported without error as a collection of NURBS surfaces.	Watertight trimmed NURBS	NA:NA:NA:NA:NA	2018
Adrien Treuille	NA	Session details: Humans	NA	2018
Taesoo Kwon:Kang Hoon Lee:Jehee Lee:Shigeo Takahashi	Animating a crowd of characters is an important problem in computer graphics. The latest techniques enable highly realistic group motions to be produced in feature animation films and video games. However, interactive methods have not emerged yet for editing the existing group motion of multiple characters. We present an approach to editing group motion as a whole while maintaining its neighborhood formation and individual moving trajectories in the original animation as much as possible. The user can deform a group motion by pinning or dragging individuals. Multiple group motions can be stitched or merged to form a longer or larger group motion while avoiding collisions. These editing operations rely on a novel graph structure, in which vertices represent positions of individuals at specific frames and edges encode neighborhood formations and moving trajectories. We employ a shape-manipulation technique to minimize the distortion of relative arrangements among adjacent vertices while editing the graph structure. The usefulness and flexibility of our approach is demonstrated through examples in which the user creates and edits complex crowd animations interactively using a collection of group motion clips.	Group motion editing	NA:NA:NA:NA	2018
KangKang Yin:Stelian Coros:Philippe Beaudoin:Michiel van de Panne	Modeling the large space of possible human motions requires scalable techniques. Generalizing from example motions or example controllers is one way to provide the required scalability. We present techniques for generalizing a controller for physics-based walking to significantly different tasks, such as climbing a large step up, or pushing a heavy object. Continuation methods solve such problems using a progressive sequence of problems that trace a path from an existing solved problem to the final desired-but-unsolved problem. Each step in the continuation sequence makes progress towards the target problem while further adapting the solution. We describe and evaluate a number of choices in applying continuation methods to adapting walking gaits for tasks involving interaction with the environment. The methods have been successfully applied to automatically adapt a regular cyclic walk to climbing a 65cm step, stepping over a 55cm sill, pushing heavy furniture, walking up steep inclines, and walking on ice. The continuation path further provides parameterized solutions to these problems.	Continuation methods for adapting simulated skills	NA:NA:NA:NA	2018
Marco da Silva:Yeuhi Abe:Jovan Popović	Animating natural human motion in dynamic environments is difficult because of complex geometric and physical interactions. Simulation provides an automatic solution to parts of this problem, but it needs control systems to produce lifelike motions. This paper describes the systematic computation of controllers that can reproduce a range of locomotion styles in interactive simulations. Given a reference motion that describes the desired style, a derived control system can reproduce that style in simulation and in new environments. Because it produces high-quality motions that are both geometrically and physically consistent with simulated surroundings, interactive animation systems could begin to use this approach along with more established kinematic methods.	Interactive simulation of stylized human locomotion	NA:NA:NA	2018
Shinjiro Sueda:Andrew Kaufman:Dinesh K. Pai	We describe an automatic technique for generating the motion of tendons and muscles under the skin of a traditionally animated character. This is achieved by integrating the traditional animation pipeline with a novel biomechanical simulator capable of dynamic simulation with complex routing constraints on muscles and tendons. We also describe an algorithm for computing the activation levels of muscles required to track the input animation. We demonstrate the results with several animations of the human hand.	Musculotendon simulation for hand animation	NA:NA:NA	2018
Srinivasa Narasimhan	NA	Session details: Shape acquisition	NA	2018
Benedict J. Brown:Corey Toler-Franklin:Diego Nehab:Michael Burns:David Dobkin:Andreas Vlachopoulos:Christos Doumas:Szymon Rusinkiewicz:Tim Weyrich	Although mature technologies exist for acquiring images, geometry, and normals of small objects, they remain cumbersome and time-consuming for non-experts to employ on a large scale. In an archaeological setting, a practical acquisition system for routine use on every artifact and fragment would open new possibilities for archiving, analysis, and dissemination. We present an inexpensive system for acquiring all three types of information, and associated metadata, for small objects such as fragments of wall paintings. The acquisition system requires minimal supervision, so that a single, non-expert user can scan at least 10 fragments per hour. To achieve this performance, we introduce new algorithms to robustly and automatically align range scans, register 2-D scans to 3-D geometry, and compute normals from 2-D scans. As an illustrative application, we present a novel 3-D matching algorithm that efficiently searches for matching fragments using the scanned geometry.	A system for high-volume acquisition and matching of fresco fragments: reassembling Theran wall paintings	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Dror Aiger:Niloy J. Mitra:Daniel Cohen-Or	We introduce 4PCS, a fast and robust alignment scheme for 3D point sets that uses wide bases, which are known to be resilient to noise and outliers. The algorithm allows registering raw noisy data, possibly contaminated with outliers, without pre-filtering or denoising the data. Further, the method significantly reduces the number of trials required to establish a reliable registration between the underlying surfaces in the presence of noise, without any assumptions about starting alignment. Our method is based on a novel technique to extract all coplanar 4-points sets from a 3D point set that are approximately congruent, under rigid transformation, to a given set of coplanar 4-points. This extraction procedure runs in roughly O(n2 + k) time, where n is the number of candidate points and k is the number of reported 4-points sets. In practice, when noise level is low and there is sufficient overlap, using local descriptors the time complexity reduces to O(n + k). We also propose an extension to handle similarity and affine transforms. Our technique achieves an order of magnitude asymptotic acceleration compared to common randomized alignment techniques. We demonstrate the robustness of our algorithm on several sets of multiple range scans with varying degree of noise, outliers, and extent of overlap.	4-points congruent sets for robust pairwise surface registration	NA:NA:NA	2018
Thorsten Thormählen:Hans-Peter Seidel	A semi-automatic approach is presented that enables the generation of a high-quality 3D model of a static object from an image sequence that was taken by a moving, uncalibrated consumer camera. A bounding box is placed around the object, and orthographic projections onto the sides of the bounding box are automatically generated out of the image sequence. These ortho-images can be imported as background maps in the orthographic views (e.g., the top, side, and front view) of any modeling package. Modelers can now use these ortho-images to guide their modeling by tracing the shape of the object over the ortho-images. This greatly improves the accuracy and efficiency of the manual modeling process. An additional advantage over existing semi-automatic systems is that modelers can use the modeling package that they are trained in and can thereby increase their productivity by applying the advanced modeling features the package offers. The results presented show that accurate 3D models can even be generated for translucent or specular surfaces, and the approach is therefore still applicable in cases where today's fully automatic image-based approaches or laser scanners would fail.	3D-modeling by ortho-image generation from image sequences	NA:NA	2018
Matthias B. Hullin:Martin Fuchs:Ivo Ihrke:Hans-Peter Seidel:Hendrik P. A. Lensch	The quality of a 3D range scan should not depend on the surface properties of the object. Most active range scanning techniques, however, assume a diffuse reflector to allow for a robust detection of incident light patterns. In our approach we embed the object into a fluorescent liquid. By analyzing the light rays that become visible due to fluorescence rather than analyzing their reflections off the surface, we can detect the intersection points between the projected laser sheet and the object surface for a wide range of different materials. For transparent objects we can even directly depict a slice through the object in just one image by matching its refractive index to the one of the embedding liquid. This enables a direct sampling of the object geometry without the need for computational reconstruction. This way, a high-resolution 3D volume can be assembled simply by sweeping a laser plane through the object. We demonstrate the effectiveness of our light sheet range scanning approach on a set of objects manufactured from a variety of materials and material mixes, including dark, translucent and transparent objects.	Fluorescent immersion range scanning	NA:NA:NA:NA:NA	2018
Olga Sorkine	NA	Session details: NPR & deformation	NA	2018
Forrester Cole:Aleksey Golovinskiy:Alex Limpaecher:Heather Stoddart Barros:Adam Finkelstein:Thomas Funkhouser:Szymon Rusinkiewicz	This paper presents the results of a study in which artists made line drawings intended to convey specific 3D shapes. The study was designed so that drawings could be registered with rendered images of 3D models, supporting an analysis of how well the locations of the artists' lines correlate with other artists', with current computer graphics line definitions, and with the underlying differential properties of the 3D surface. Lines drawn by artists in this study largely overlapped one another (75% are within 1mm of another line), particularly along the occluding contours of the object. Most lines that do not overlap contours overlap large gradients of the image intensity, and correlate strongly with predictions made by recent line drawing algorithms in computer graphics. 14% were not well described by any of the local properties considered in this study. The result of our work is a publicly available data set of aligned drawings, an analysis of where lines appear in that data set based on local properties of 3D models, and algorithms to predict where artists will draw lines for new scenes.	Where do people draw lines?	NA:NA:NA:NA:NA:NA:NA	2018
Wai-Man Pang:Yingge Qu:Tien-Tsin Wong:Daniel Cohen-Or:Pheng-Ann Heng	This paper presents an optimization-based halftoning technique that preserves the structure and tone similarities between the original and the halftone images. By optimizing an objective function consisting of both the structure and the tone metrics, the generated halftone images preserve visually sensitive texture details as well as the local tone. It possesses the blue-noise property and does not introduce annoying patterns. Unlike the existing edge-enhancement halftoning, the proposed method does not suffer from the deficiencies of edge detector. Our method is tested on various types of images. In multiple experiments and the user study, our method consistently obtains the best scores among all tested methods.	Structure-aware halftoning	NA:NA:NA:NA:NA	2018
Tobias Ritschel:Kaleigh Smith:Matthias Ihrke:Thorsten Grosch:Karol Myszkowski:Hans-Peter Seidel	We present a new approach for enhancing local scene contrast by unsharp masking over arbitrary surfaces under any form of illumination. Our adaptation of a well-known 2D technique to 3D interactive scenarios is designed to aid viewers in tasks like understanding complex or detailed geometric models, medical visualization and navigation in virtual environments. Our holistic approach enhances the depiction of various visual cues, including gradients from surface shading, surface reflectance, shadows, and highlights, to ease estimation of viewpoint, lighting conditions, shapes of objects and their world-space organization. Motivated by recent perceptual findings on 3D aspects of the Cornsweet illusion, we create scene coherent enhancements by treating cues in terms of their 3D context; doing so has a stronger effect than approaches that operate in a 2D image context and also achieves temporal coherence. We validate our unsharp masking in 3D with psychophysical experiments showing that the enhanced images are perceived to have better contrast and are preferred over unenhanced originals. Our operator runs at real-time rates on a GPU and the effect is easily controlled interactively within the rendering pipeline.	3D unsharp masking for scene coherent enhancement	NA:NA:NA:NA:NA:NA	2018
Wei-Wen Feng:Byung-Uck Kim:Yizhou Yu	Achieving intuitive control of animated surface deformation while observing a specific style is an important but challenging task in computer graphics. Solutions to this task can find many applications in data-driven skin animation, computer puppetry, and computer games. In this paper, we present an intuitive and powerful animation interface to simultaneously control the deformation of a large number of local regions on a deformable surface with a minimal number of control points. Our method learns suitable deformation subspaces from training examples, and generate new deformations on the fly according to the movements of the control points. Our contributions include a novel deformation regression method based on kernel Canonical Correlation Analysis (CCA) and a Poisson-based translation solving technique for easy and fast deformation control based on examples. Our run-time algorithm can be implemented on GPUs and can achieve a few hundred frames per second even for large datasets with hundreds of training examples.	Real-time data driven deformation using kernel canonical correlation analysis	NA:NA:NA	2018
Matthias Zwicker	NA	Session details: Painting & sketching	NA	2018
Alexandrina Orzan:Adrien Bousseau:Holger Winnemöller:Pascal Barla:Joëlle Thollot:David Salesin	We describe a new vector-based primitive for creating smooth-shaded images, called the diffusion curve. A diffusion curve partitions the space through which it is drawn, defining different colors on either side. These colors may vary smoothly along the curve. In addition, the sharpness of the color transition from one side of the curve to the other can be controlled. Given a set of diffusion curves, the final image is constructed by solving a Poisson equation whose constraints are specified by the set of gradients across all diffusion curves. Like all vector-based primitives, diffusion curves conveniently support a variety of operations, including geometry-based editing, keyframe animation, and ready stylization. Moreover, their representation is compact and inherently resolution-independent. We describe a GPU-based implementation for rendering images defined by a set of diffusion curves in realtime. We then demonstrate an interactive drawing system for allowing artists to create artworks using diffusion curves, either by drawing the curves in a freehand style, or by tracing existing imagery. The system is simple and intuitive: we show results created by artists after just a few minutes of instruction. Furthermore, we describe a completely automatic conversion process for taking an image and turning it into a set of diffusion curves that closely approximate the original image content.	Diffusion curves: a vector representation for smooth-shaded images	NA:NA:NA:NA:NA:NA	2018
James McCann:Nancy S. Pollard	We present an image editing program which allows artists to paint in the gradient domain with real-time feedback on megapixel-sized images. Along with a pedestrian, though powerful, gradient-painting brush and gradient-clone tool, we introduce an edge brush designed for edge selection and replay. These brushes, coupled with special blending modes, allow users to accomplish global lighting and contrast adjustments using only local image manipulations --- e.g. strengthening a given edge or removing a shadow boundary. Such operations would be tedious in a conventional intensity-based paint program and hard for users to get right in the gradient domain without real-time feedback. The core of our paint program is a simple-to-implement GPU multigrid method which allows integration of megapixel-sized full-color gradient fields at over 20 frames per second on modest hardware. By way of evaluation, we present example images produced with our program and characterize the iteration time and convergence rate of our integration method.	Real-time gradient-domain painting	NA:NA	2018
Yoshinori Dobashi:Katsutoshi Kusumoto:Tomoyuki Nishita:Tsuyoshi Yamamoto	Clouds play an important role for creating realistic images of outdoor scenes. In order to generate realistic clouds, many methods have been developed for modeling and animating clouds. One of the most effective approaches for synthesizing realistic clouds is to simulate cloud formation processes based on the atmospheric fluid dynamics. Although this approach can create realistic clouds, the resulting shapes and motion depend on many simulation parameters and the initial status. Therefore, it is very difficult to adjust those parameters so that the clouds form the desired shapes. This paper addresses this problem and presents a method for controlling the simulation of cloud formation. In this paper, we focus on controlling cumuliform cloud formation. The user specifies the overall shape of the clouds. Then, our method automatically adjusts parameters during the simulation in order to generate clouds forming the specified shape. Our method can generate realistic clouds while their shapes closely match to the desired shape.	Feedback control of cumuliform cloud formation based on computational fluid dynamics	NA:NA:NA:NA	2018
Yotam Gingold:Denis Zorin	We present a system for free-form surface modeling that allows a user to modify a shape by changing its rendered, shaded image using stroke-based drawing tools. User input is translated into a set of tangent and positional constraints on the surface. A new shape, whose rendered image closely approximates user input, is computed using an efficient and stable surface optimization procedure. We demonstrate how several types of free-form surface edits which may be difficult to cast in terms of standard deformation approaches can be easily performed using our system.	Shading-based surface editing	NA:NA	2018
Jehee Lee	NA	Session details: Performance capture	NA	2018
Sang Il Park:Jessica K. Hodgins	In this paper, we present a data-driven technique for synthesizing skin deformation from skeletal motion. We first create a database of dynamic skin deformations by recording the motion of the surface of the skin with a very large set of motion capture markers. We then build a statistical model of the deformations by dividing them into two parts: static and dynamic. Static deformations are modeled as a function of pose. Dynamic deformations are caused by the actions of the muscles as they move the joints and the inertia of muscles and fat. We approximate these effects by fitting a set of dynamic equations to the pre-recorded data. We demonstrate the viability of this approach by generating skin deformations from the skeletal motion of an actor. We compare the generated animation both to synchronized video of the actor and to ground truth animation created directly from the large marker set.	Data-driven modeling of skin and muscle deformation	NA:NA	2018
Daniel Vlasic:Ilya Baran:Wojciech Matusik:Jovan Popović	Details in mesh animations are difficult to generate but they have great impact on visual quality. In this work, we demonstrate a practical software system for capturing such details from multi-view video recordings. Given a stream of synchronized video images that record a human performance from multiple viewpoints and an articulated template of the performer, our system captures the motion of both the skeleton and the shape. The output mesh animation is enhanced with the details observed in the image silhouettes. For example, a performance in casual loose-fitting clothes will generate mesh animations with flowing garment motions. We accomplish this with a fast pose tracking method followed by nonrigid deformation of the template to fit the silhouettes. The entire process takes less than sixteen seconds per frame and requires no markers or texture cues. Captured meshes are in full correspondence making them readily usable for editing operations including texturing, deformation transfer, and deformation model learning.	Articulated mesh animation from multi-view silhouettes	NA:NA:NA:NA	2018
Edilson de Aguiar:Carsten Stoll:Christian Theobalt:Naveed Ahmed:Hans-Peter Seidel:Sebastian Thrun	This paper proposes a new marker-less approach to capturing human performances from multi-view video. Our algorithm can jointly reconstruct spatio-temporally coherent geometry, motion and textural surface appearance of actors that perform complex and rapid moves. Furthermore, since our algorithm is purely meshbased and makes as few as possible prior assumptions about the type of subject being tracked, it can even capture performances of people wearing wide apparel, such as a dancer wearing a skirt. To serve this purpose our method efficiently and effectively combines the power of surface- and volume-based shape deformation techniques with a new mesh-based analysis-through-synthesis framework. This framework extracts motion constraints from video and makes the laser-scan of the tracked subject mimic the recorded performance. Also small-scale time-varying shape detail is recovered by applying model-guided multi-view stereo to refine the model surface. Our method delivers captured performance data at high level of detail, is highly versatile, and is applicable to many complex types of scenes that could not be handled by alternative marker-based or marker-free recording techniques.	Performance capture from sparse multi-view video	NA:NA:NA:NA:NA:NA	2018
Derek Bradley:Tiberiu Popa:Alla Sheffer:Wolfgang Heidrich:Tamy Boubekeur	A lot of research has recently focused on the problem of capturing the geometry and motion of garments. Such work usually relies on special markers printed on the fabric to establish temporally coherent correspondences between points on the garment's surface at different times. Unfortunately, this approach is tedious and prevents the capture of off-the-shelf clothing made from interesting fabrics. In this paper, we describe a marker-free approach to capturing garment motion that avoids these downsides. We establish temporally coherent parameterizations between incomplete geometries that we extract at each timestep with a multiview stereo algorithm. We then fill holes in the geometry using a template. This approach, for the first time, allows us to capture the geometry and motion of unpatterned, off-the-shelf garments made from a range of different fabrics.	Markerless garment capture	NA:NA:NA:NA:NA	2018
Claudio Silva	NA	Session details: Procedural modeling & design	NA	2018
Floraine Grabler:Maneesh Agrawala:Robert W. Sumner:Mark Pauly	Tourist maps are essential resources for visitors to an unfamiliar city because they visually highlight landmarks and other points of interest. Yet, hand-designed maps are static representations that cannot adapt to the needs and tastes of the individual tourist. In this paper we present an automated system for designing tourist maps that selects and highlights the information that is most important to tourists. Our system determines the salience of map elements using bottom-up vision-based image analysis and top-down web-based information extraction techniques. It then generates a map that emphasizes the most important elements, using a combination of multiperspective rendering to increase visibility of streets and landmarks, and cartographic generalization techniques such as simplification, deformation, and displacement to emphasize landmarks and de-emphasize less important buildings. We show a number of automatically generated tourist maps of San Francisco and compare them to existing automated and manual approaches.	Automatic generation of tourist maps	NA:NA:NA:NA	2018
Wilmot Li:Maneesh Agrawala:Brian Curless:David Salesin	We present a system for creating and viewing interactive exploded views of complex 3D models. In our approach, a 3D input model is organized into an explosion graph that encodes how parts explode with respect to each other. We present an automatic method for computing explosion graphs that takes into account part hierarchies in the input models and handles common classes of interlocking parts. Our system also includes an interface that allows users to interactively explore our exploded views using both direct controls and higher-level interaction modes.	Automated generation of interactive 3D exploded view diagrams	NA:NA:NA:NA	2018
Markus Lipp:Peter Wonka:Michael Wimmer	We introduce a real-time interactive visual editing paradigm for shape grammars, allowing the creation of rulebases from scratch without text file editing. In previous work, shape-grammar based procedural techniques were successfully applied to the creation of architectural models. However, those methods are text based, and may therefore be difficult to use for artists with little computer science background. Therefore the goal was to enable a visual work-flow combining the power of shape grammars with traditional modeling techniques. We extend previous shape grammar approaches by providing direct and persistent local control over the generated instances, avoiding the combinatorial explosion of grammar rules for modifications that should not affect all instances. The resulting visual editor is flexible: All elements of a complex state-of-the-art grammar can be created and modified visually.	Interactive visual editing of grammars for procedural architecture	NA:NA:NA	2018
