Chun-Chia Chiu:Yi-Hsiang Lo:Wei-Ting Ruan:Cheng-Han Yang:Ruen-Rone Lee:Hung-Kuo Chu	Scribble art is a kind of illustrative drawing. Artists use continuous lines to convey the impression of an image or concept of a design. Unlike conventional line drawings such as sketching and hatching that commonly comprise of short and straight line segments, scribble artists aim at depicting the image with long and continuous curves. In this work, we study a typical curve pattern, circular scribble that appears most frequently in the artworks. Circular lines are drawn in either clockwise or counter-clockwise direction with varying radius in the circular scribble arts. The artists delicately trace along a seemingly random path and control the size and orientation of circular line pattern to depict a subject of their artwork. The main challenges lie in producing smooth transition between grayscale levels and preserving dominant image features using continuous loops and intersections of a circular scribble. Thus, the creation of circular scribble art is skill-demanding and time-consuming. In order to facilitate such process, we introduce a systematic approach to automatically synthesize circular scribble arts from images by tracing along a virtual path using solely a single continuous circular scribble with varying radius and orientation. We have tested our approach using a wide range of images and generate visually pleasing circular scribble arts (see Figure 1).	Continuous circular scribble arts	NA:NA:NA:NA:NA:NA	2015
Yuki Koyama:Daisuke Sakamoto:Takeo Igarashi	Exploring various visual designs by tweaking parameters is a common practice when designing digital content. For example, if we want to clean up a photo for use at the top of a web page, we adjust the design parameters---brightness, contrast, saturation, etc.---to explore which combination of parameters provides the best result. Similar situations can be found anywhere in computer graphics applications, such as tweaking shader parameters for game development.	Crowd-powered parameter analysis for computational design exploration	NA:NA:NA	2015
Xiang 'Anthony' Chen:Stelian Coros:Jennifer Mankoff:Scott E. Hudson	One powerful aspect of 3D printing is its ability to extend, repair, or more generally modify everyday objects. However, nearly all existing work implicitly assumes that whole objects are to be printed from scratch. Designing objects as extensions or enhancements of existing ones is a laborious process in most of today's 3D authoring tools. This paper presents a framework for 3D printing to augment existing objects that covers a wide range of attachment options. We illustrate the framework through three exemplar attachment techniques - print-over, print-to-affix, and print-through. We implemented these techniques in Encore, a design tool that supports a range of analysis with visualization for users to explore design options and tradeoffs among these metrics. Encore also generates 3D models for production, addressing issues such as support jigs and contact geometry between the attached part and the original object.	Encore: 3D printed augmentation of everyday objects with printed-over, affixed and interlocked attachments	NA:NA:NA:NA	2015
Kazutaka Nakashima:Takeo Igarashi	Construction of a free-form 3D surface model is still difficult. However, in our point of view, construction of a simple voxel model is relatively easy because it can be built with blocks. Even small children can build a voxel model. We present a method to convert a voxel model into a free-form surface model in order to facilitate construction of surface models.	Extraction of a smooth surface from voxels preserving sharp creases	NA:NA	2015
Chengcheng Tang:Xiang Sun:Alexandra Gomes:Johannes Wallner:Helmut Pottmann	We solve the form-finding problem for polyhedral meshes in a way which combines form, function and fabrication; taking care of user-specified constraints like boundary interpolation, planarity of faces, statics, panel size and shape, enclosed volume, and cost. Our main application is the interactive modeling of meshes for architectural and industrial design. Our approach can be described as guided exploration of the constraint space whose algebraic structure is simplified by introducing auxiliary variables and ensuring that constraints are at most quadratic.	Form-finding with polyhedral meshes made simple	NA:NA:NA:NA:NA	2015
Rukmini Goswami:Tim Tregubov:Lorie Loeb	Attention is a limited resource that intrinsically dictates our perceptions, memories, and behaviors. Further, visuospatial attention correlates highly with user engagement, heart rate, and arousal [El-Nasr et al. 2010]. Artists and interactive game designers strive to capture and direct attention, yet even in the most carefully crafted graphic narratives viewer eye paths -- a proxy for attention -- vary up to 20 percent [McCloud 1994; Jain et al. 2012]. Our aim is to use attentional measures to enrich graphic novel narratives. FrameShift uses eye tracking to measure reader attention and changes text and visual elements later on in the story accordingly. We have built an extensible framework for using attention to introduce perceptual changes in narratives. We use attention as an indirect method for interactions and introduce shiftable frame nodes that change readers' belief states over time.	FrameShift: shift your attention, shift the story	NA:NA:NA	2015
EunJin Kim:Hyeon-Jeong Suk	In the process of editorial design, a harmonious match between a picture and a solid color is often essential to achieve a high quality of a graphical art work. Color as such is a compelling cue to elicit emotional responses and thus can enhance the emotional quality of an image. Tools and methods have been developed to automatize the color selection process, and a noticeable progress has been achieved to extract perceptually dominant colors of an image. However, little attention has been paid to the emotional characteristics of selected colors, and it has been highly relying on the color designers' manual judgments. In this study, we propose a computational method that creates a color that enhances both aesthetic and affective quality of an image, and call it a theme color.	Hue extraction and tone match: generating a theme color to enhance the emotional quality of an image	NA:NA	2015
Azusa Mama:Yuki Morimoto:Katsuto Nakajima	Modeling 3D trees is a major theme in the field of computer graphics [Steven et al. 2012]. However, there has been little research on generating illustrations of trees [Yu-Sheng et al. 2012]. One of the ways to generate them is to render their 3D models. However, it is difficult to obtain the characteristic flat representation of illustrations because of the concentration of foliage in the central part of the tree. We present a system to generate a wide variety of tree illustrations by controlling the density of branches, the shape of canopy, and the overlap of flowers and leaves (Fig. 1).	Interactive tree illustration generation system	NA:NA:NA	2015
Raf Ramakers:Kashyap Todi:Kris Luyten	We present PaperPulse, a design and fabrication approach that enables designers without a technical background to produce standalone interactive paper artifacts by augmenting them with electronics. With PaperPulse designers overlay pre-designed visual elements with interactive widgets and specify functional relations between them using a logic demonstration and recording approach, called Pulsation. When the design is finished, PaperPulse generates layered electronic circuit designs, code that can be deployed on a microcontroller, and instructions for assembly.	PaperPulse: an integrated approach for embedding electronics in paper designs	NA:NA:NA	2015
Yuki Igarashi:Jun Mitani	Patchwork is a well-known type of needlework that involves sewing pieces of fabric into a larger design. It is commonly used to form quilts, but can also be used to make bags, wall hangings, cushion covers, and other items. Larger designs are usually based on repeating patterns, which are built up using different shapes. Professional patchwork designers design original patterns; however, novice users usually use geometric patterns or off-the-shelf patterns for each piece; this is because it is difficult for novices to design patterns while visualizing the resulting larger fabric.	Patchy: an interactive patchwork design system	NA:NA	2015
Shinji Mizuno:Marino Isoda:Rei Ito:Mei Okamoto:Momoko Kondo:Saya Sugiura:Yuki Nakatani:Motomi Hirose	Drawing on a sketchbook is one of the most familiar arts and people of all ages can enjoy it. Thus a lot of CG applications on which a user can create 2D and 3DCG images with drawing operations have been developed [Kondo et al. 2013]. On the other hand, dancing is also familiar to many people. Thus a digital content that is a mixture of drawing and dancing could be very attractive.	Sketch dance stage	NA:NA:NA:NA:NA:NA:NA:NA	2015
Daria Tsoupikova:Scott Rettberg:Roderick Coover:Arthur Nishimoto	We describe the first virtual reality art performance Hearts and Minds: The Interrogations Project developed using a novel method for direct output of the Unity-based virtual reality projects into CAVE2™ [Febretti et al. 2013] environment. This work incorporates original research, technological innovation and creative arts in an adaptation of veterans' testimonies detailing US military interrogations and abuses of prisoners in Iraq during the American counter-insurgency campaign in the early 2000s. It presents a debate focusing on interrogation methods, torture and its consequences, and Post Traumatic Stress Disorder experienced by solders who have participated in such acts. This work was developed at the Electronic Visualization Lab in Chicago through a unique international collaboration between artists, scientists, and researchers from five different Universities. The methods developed for this project allow hands-on education of virtual reality by letting students create their own virtual environments and exhibit them in the CAVE2 quickly.	The battle for hearts and minds: interrogation and torture in the age of war	NA:NA:NA:NA	2015
Rebecca Kleinberger	Our voice is an important part of our individuality but the relationship we have with our own voice is not obvious. We don't hear it the same way others do, and our brain treats it differently from any other sound we hear [Houde et al. 2002]. Yet its sonority is highly linked to our body and mind, and deeply connected with how we are perceived by society and how we see ourselves. The V3 system (Vocal Vibrations Visualization) offers a interactive visualization of vocal vibration patterns. We developed the hexauscultation mask, a head set sensor that measures bioacoustic signals from the voice at 6 points of the face and throat. Those signals are sent and processed to offer a real-time visualization of the relative vibration intensities at the 6 measured points. This system can be used in various situations such as vocal training, tool design for the deaf community, design of HCI for speech disorder treatment and prosody acquisition but also simply for personal vocal exploration.	V3: an interactive real-time visualization of vocal vibrations	NA	2015
Shogo Fukushima:Takeshi Naemura	When we snap strings playing with a CMOS camera, the strings seems to vibrate in a wobbly slow motion pattern. Because a CMOS sensor scans one line of video in sequence, fast moving objects are distorted during the scanning sequence. The morphing and distorting are called a rolling shutter effect, which is considered to be an artistic photographic techniques like strip photography and slit-scan photography. However, the effect can only be seen on a camera finder or a PC screen; the guitar player and audience are quite unlikely to notice it by the naked eye.	Wobble strings: spatially divided stroboscopic effect for augmenting wobbly motion of stringed instruments	NA:NA	2015
Sang-won Leigh:Harshit Agrawal:Pattie Maes	We present a drone-based drawing system where a user's sketch on a desk is transformed across scale and time, and transferred onto a larger canvas at a distance in real-time. Various spatio-temporal transformations like scaling, mirroring, time stretching, recording and playing back over time, and simultaneously drawing at multiple locations allow for creating various artistic effects. The unrestricted motion of the drone promises scalability and a huge potential as an artistic medium.	Z-drawing: a flying agent system for computer-assisted drawing	NA:NA:NA	2015
Katsutoshi Masai:Yuta Sugiura:Masa Ogata:Katsuhiro Suzuki:Fumihiko Nakamura:Sho Shimamura:Kai Kunze:Masahiko Inami:Maki Sugimoto	Facial expression is a powerful way for us to exchange information nonverbally. They can give us insights into how people feel and think. There are a number of works related to facial expression detection in computer vision. However, most works focus on camera-based systems installed in the environment. With this method, it is difficult to track user's face if user moves constantly. Moreover, user's facial expression can be recognized at only a limited place.	AffectiveWear: toward recognizing facial expression	NA:NA:NA:NA:NA:NA:NA:NA:NA	2015
Hugo Talbot:Frederick Roy:Stéphane Cotin	Cryotherapy is a rapidly growing minimally invasive technique for the treatment of different kinds of tumors, such as breast cancer, renal and prostate cancer. Several hollow needles are percutaneously inserted in the target area under image guidance and a gas (usually argon) is then decompressed inside the needles. Based on the Thompson-Joule principle, the temperature drops drown and a ball of ice crystals forms around the tip of each needle. Radiologists rely on the geometry of this iceball (273 K), visible on computer tomographic (CT) or magnetic resonance (MR) images, to assess the status of the ablation. However, cellular death only occurs when the temperature falls below 233 K. The complexity of the procedure therefore resides in planning the optimal number, position and orientation of the needles required to treat the tumor, while avoiding any damage to the surrounding healthy tissues.	Augmented reality for cryoablation procedures	NA:NA:NA	2015
Jun Nishida:Hikaru Takatori:Kosuke Sato:Kenji Suzuki	Understanding and perceiving the world from a child's perspective is a very important key not only to design products and architecture, but also to remind staff who work closely with children, such as hospitals and kindergartens. Ida et al. investigated the universality of devices and architecture in public spaces by recording videos through a hand-held camera positioned at a child's eye level [Ida et al. 2010]. In this study, we propose a novel wearable suit called CHILDHOOD that virtually realizes a child's eye and hand movements by attaching a viewpoint translator and hand exoskeletons (Figure 1a). We hypothesized that virtualizing a child's body size by transforming our own body while preserving embodied interactions with actual surroundings would provide an augmented experience of a child's perspective. This could assist designers in evaluating product accessibility through their own body interactions in real time. In addition, augmented child experience can help staff and parents remember how children feel and touch the world.	CHILDHOOD: wearable suit for augmented child experience	NA:NA:NA:NA	2015
Mark Bolas:Ashok Kuruvilla:Shravani Chintalapudi:Fernando Rabelo:Vangelis Lympouridis:Christine Barron:Evan Suma:Catalina Matamoros:Cristina Brous:Alicja Jasina:Yawen Zheng:Andrew Jones:Paul Debevec:David Krum	There is rapidly growing interest in the creation of rendered environments and content for tracked head-mounted stereoscopic displays for virtual reality. Currently, the most popular approaches include polygonal environments created with game engines, as well as 360 degree spherical cameras used to capture live action video. These tools were not originally designed to leverage the more complex visual cues available in VR when users laterally shift viewpoints, manually interact with models, and employ stereoscopic vision. There is a need for a fresh look at graphics techniques that can capitalize upon the unique affordances that make VR so compelling.	Creating near-field VR using stop motion characters and a touch of light-field rendering	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2015
Liang-Chen Wu:Jia-Ye Li:Yu-Hsuan Huang:Ming Ouhyoung	In making 3D animation with traditional method, we usually edit 3D objects in 3-dimension space on the screen; therefore, we have to use input devices to edit and to observe 3D models. However, those processes can be improved. With the improvement in gesture recognition nowadays, virtual information operations are no longer confined to the mouse and keyboard. We can use the recognized gestures to apply to difficult operations in editing model motion. And for observing 3D model, we would use head tracking from external devices to improve it. It would be easy to observe the interactive results without complicated operation because the system will accurately map the real world head movements.	First-person view animation editing utilizing video see-through augmented reality	NA:NA:NA:NA	2015
Michael Saenz:Joshua Strunk:Kelly Maset:Jinsil Harwaryoung Seo:Erica Malone	We present FlexAR, a kinetic tangible augmented reality [Billinghurst,2008] application for anatomy education. Anatomy has been taught traditionally in two dimensions, particularly for those in non-medical fields such as artists. Medical students gain hands-on experience through cadaver dissection [[Winkelmann, 2007]. However, with dissection becoming less practical, researchers have begun evaluating techniques for teaching anatomy through technology.	FlexAR: anatomy education through kinetic tangible augmented reality	NA:NA:NA:NA:NA	2015
Nazim Haouchine:Alexandre Bilger:Jeremie Dequidt:Stephane Cotin	The considerable advances in Computer Vision for hand and finger tracking made it possible to have several sorts of interactions in Augmented Reality systems (AR), such as object grasping, object translation or surface deformation [Chun and Höllerer 2013]. However, no method has yet considered interaction than involves topological changes of the augmented model (like mesh cutting).	Fracture in augmented reality	NA:NA:NA:NA	2015
Toshiaki Nakasu:Tsukasa Ike:Kazunori Imoto:Yasunobu Yamauchi	In maintenance of electric power control panels, a worker has to do a lot of manual work such as pushing buttons and turning on/off selector switches. Therefore, a hands-free gesture operating system is needed. Tsukada [Tsukada et al. 2002] proposed a gesture operating system using an acceleration sensor and switches. Although it is a simple task to control a home appliance by gesture, users have to use both gesture and switch on/off to perform more complicated tasks such as controlling and recording documents in maintenance work. Therefore, the system becomes complicated. We propose a novel switch-less assist system for maintenance work with a simple structure that recognizes gesture using only an acceleration sensor. Ike [Ike et al. 2014] proposed a hand gesture operating system that enables users to control a TV remotely by adopting "Tapping" as a click signal. The system recognizes tapping by detecting a pulse-like acceleration pattern corresponding to a micro collision generated by tapping. However, it is difficult to recognize tapping because maintenance work includes many micro collisions generated by touching things. We adopt "Tapping & Finger up", i.e., tapping fingers and turning up a finger, gestures that rarely occur in maintenance work, and design a gesture system enabling users to perform maintenance tasks and gesture operation seamlessly. Our system helps users do maintenance work easily and intuitively without interrupting work.	Hands-free gesture operation for maintenance work using finger-mounted acceleration sensor	NA:NA:NA:NA	2015
Bruno Marques:Nazim Haouchine:Rosalie Plantefeve:Stephane Cotin	Minimally invasive surgery (MIS) is a recent surgical technique where the surgeon does not interact directly with the patient's organs. In contrast to open surgery, the surgeon manipulates the organs through instruments inserted in the patient's abdominal cavity while observing the organ from a display showing the video stream captured by an endoscopic camera. While the benefits of MIS for patients are clearly claimed, performing these operations remains very challenging for the surgeons, due to the loss of depth perception caused by this indirect manipulation. To tackle this limitation, the research community suggests to use augmented reality (AR) during the procedure [Haouchine et al. 2013]. The objective towards the use of AR during surgery is to be able to overlay the 3D model of the organ (that can be obtained from a pre-operative scan of the patient) onto the video stream. Surgical AR made considerable advances and reached a certain maturity in the estimation of tumors and vessels localisation. Howerver, very few studies have investigated depth perception and visualization of internal structures [Lerotic et al. 2007], which is considered by surgeons as a very sensitive issue. This study suggests a method to compensate the loss of depth perception while enhancing organ vessels and tumors to surgeons. This method relies on a combination of contour rendering technique and adaptive alpha blending to effectively perceive the vessels and tumors depth. In addition, this technique is designed to achieve real-time to satisfy the requirements of clinical routines, and has been tested on real human surgery.	Improving depth perception during surgical augmented reality	NA:NA:NA:NA	2015
Prashanth Bollam:Eesha Gothwal:GBCS Tejaswi, V:Shailesh Kumar:Soumyajit Deb	The recent boom in computing capabilities of mobile devices has led to the introduction of Virtual Reality into the mobile ecosystem. We demonstrate a framework for the Samsung Gear VR headset that allows developers to create a totally immersive AR & VR experience with no need for interfacing with external devices or cables thereby making it a truly autonomous mobile VR experience. The significant benefits of this system over existing ones are - a fully hands free experience where hands could be used for gesture based input, the ability to use the Head Mounted Display (HMD) sensor for improved head and positional tracking and automatic peer to peer network creation for communication between phones. The most important factor in our system is to provide an intuitive way to interact with virtual objects in AR and VR. And users should be able to switch from AR to VR world and vice versa seamlessly.	Mobile collaborative augmented reality with real-time AR/VR switching	NA:NA:NA:NA:NA	2015
Xing Zhang:Umur A Ciftci:Lijun Yin	In recent years, Virtual Reality (VR) has become a new media to provide users an immersive experience. Events happening in the VR connect closer to our emotions as compared to other interfaces. The emotion variations are reflected as our facial expressions. However, the current VR systems concentrate on "giving" information to the user, yet ignore "receiving" emotional status from the user, while this information definitely contributes to the media content rating and the user experience. On the other hand, traditional controllers become difficult to use due to the obscured view point. Hand and head gesture based control is an option [Cruz-Neira et al. 1993]. However, certain sensor devices need to be worn to assure control accuracy and users are easy to feel tired. Although face tracking achieves accurate result in both 2D and 3D scenarios, the current state-of-the-art systems cannot work when half of the face is occluded by the VR headset because the shape model is trained by data from the whole face.	Mouth gesture based emotion awareness and interaction in virtual reality	NA:NA:NA	2015
Toshikazu Ohshima:Shun Kawaguchi:Yuma Tanaka	The MR Coral Sea is a mixed reality tiny aquarium. The intent of the system is to play with little virtual fish. A player can interact with the virtual fish via Coral Display, an interactive device with multisensory physical feedback. When a player presents his or her hand above the device, fish bait appears on the palm, and the virtual fish come to eat it. The device provides the user with a feeling of spatial existence through illumination and vibration.	MR coral sea evolved: mixed reality aquarium with physical MR displays	NA:NA:NA	2015
Yong Yi Lee:Junho Choi:Yong Hwi Kim:Jong Hun Lee:Moon Gu Son:Bilal Ahmed:Kwan H. Lee	Traditional museums have shown interest in exhibiting a meaningful representation of cultural heritage. However, existing stereotypical exhibition fails to attract the visitors' interest continuously as it provides only static and non-interactive contents and transmits information unilaterally. Recently, high performance measurement techniques have rapidly developed to a degree that allows for the realistic digitization of cultural heritage. Based on this digitized cultural heritage, dynamic and interactive content, such as 3D video and augmented reality, have been made to improve the immersion of visitors. In spite of these attempts, the sense of artificiality is still a challenge because most existing methods demonstrate their content via screen displays.	RiSE: reflectance transformation imaging in spatial augmented reality for exhibition of cultural heritage	NA:NA:NA:NA:NA:NA:NA	2015
Masasuke Yasumoto:Takehiro Teraoka	"Shadow Shooter" is a VR shooter game that uses the "e-Yumi 3D" bow interface and real physical interactive content that changes a 360-degree all-around view in a room into virtual game space (Figure 1). This system was constructed by developing our previous interactive "Light Shooter" content based on "The Electric Bow Interface" [Yasumoto and Ohta 2013]. Shadow Shooter expands the virtual game space to all the walls in a room just as in Jones' "Room Alive" [Jones et al. 2014]; however, it does not require large-scale equipment such as multiple projectors. It only requires the e-Yumi 3D device that consists of a real bow's components added to Willis's interface with a mobile projector [Willis et al. 2013]. Thus, we constructed a unique device for Shadow Shooter that easily changes the 360-degree all-around view into a virtual game space.	Shadow shooter: 360-degree all-around virtual 3d interactive content	NA:NA	2015
Paul Debevec:Greg Downing:Mark Bolas:Hsuen-Yueh Peng:Jules Urbach	Todays most compelling virtual reality experiences shift the users viewpoint within the virtual environment based on input from a head-tracking system, giving a compelling sense of motion parallax. While this is straightforward for computer generated scenes, photographic VR content generally does not provide motion parallax in response to head motion. Even 360° stereo panoramas, which offer separated left and right views, fail to allow the vantage point to change in response to head motion.	Spherical light field environment capture for virtual reality using a motorized pan/tilt head and offset camera	NA:NA:NA:NA:NA	2015
Stefano Scheggi:Leonardo Meli:Claudio Pacchierotti:Domenico Prattichizzo	The complexity of the world around us is creating a demand for novel interfaces that will simplify and enhance the way we interact with the environment. The recently unveiled Android Wear operating system addresses this demand by providing a modern system for all those companies that are now developing wearable devices, also known as "wearables". Wearability of robotic devices will enable novel forms of human intention recognition through haptic signals and novel forms of communication between humans and robots. Specifically, wearable haptics will enable devices to communicate with humans during their interaction with the environment they share. Wearable haptic technology have been introduced in our everyday life by Sony. In 1997 its DualShock controller for PlayStation revolutionized the gaming industry by introducing a simple but effective vibrotactile feedback. More recently, Apple unveiled the Apple Watch, which embeds a linear actuator that can make the watch vibrate. It is used whenever the wearer receives an alert or notification, or to communicate with other Apple Watch owners.	Touch the virtual reality: using the leap motion controller for hand tracking and wearable tactile devices for immersive haptic rendering	NA:NA:NA:NA	2015
Seunghyun Woo:Daeyun An:Jongmin Oh:Gibeom Hong	Due to various features being available in the vehicle such as multimedia, the dashboard has become rather complicated. Therefore, an increased need for HMI(Human Machine Interface) research has arisen in the design creation process. However, there are issues such as design changes occurring even after the design is selected due to the initial evaluation being too simple to cover all of the requirements. Designers do not consider carefully HMI the during sketching phase and issues with designs are discovered too far along in the process. This study suggests an HMI simulation tool system based on projection to pre-evaluate an HMI prior to selecting specifications through virtual function implementation. This system evaluates each function of centerfacia through quantitative criteria such as performance time and distraction time. As a result, the objective of the system is to quickly analyze and validate designs through virtual means and find interface issues with a quantitative method.	WAOH: virtual automotive HMI evaluation tool	NA:NA:NA:NA	2015
Nobuki Yoda:Takeo Igarashi	In 2D game graphics, textures are packed into a single texture called a sprite sheet in order to achieve efficient rendering. The sprite sheet can be compressed to save memory by using various compression methods such as block-based compressions and 16 bpp (bits per pixel) tone reduction. These methods are not without some problems, though. Block-based compressions are GPU-dependent, and high-quality compressions such as ASTC [Nystad et al. 2012] are often unavailable on mobile devices. 16 bpp tone reduction--often used with dithering--can create undesirable noise when it is scaled up (Figure 1c).	Decomposition of 32 bpp into 16 bpp textures with alpha	NA:NA	2015
Shaohui Jiao:Xiaofeng Tong:Eric Li:Wenlong Li	Fur simulation is crucial in many graphic applications since it can greatly enhance the realistic visual effect of virtual objects, e.g. animal avatars. However, due to its high computational cost of massive fur strands processing and motion complexity, dynamic fur is regarded as a challenging task, especially on the mobile platforms with low computing power. In order to support real-time fur rendering in mobile applications, we propose a novel method called textured offset surfaces (TOS). In particular, the furry surface is represented by a set of offset surfaces, as shown in Figure 1(a). The offset surfaces are shifted outwards from the original mesh. Each offset surface is textured with scattering density (red rectangles in Figure 1(a)) to implicitly represent the fur geometry, whose value can be changed by texture warping to simulate the fur animation. In order to achieve high quality anisotropic illumination result, as shown in Figure 1(b), Kajiya/Banks lighting model is employed in the rendering phase.	Dynamic fur on mobile using textured offset surfaces	NA:NA:NA:NA	2015
Kai-Wen Liu:I-Peng Lin:Shih-Wei Sun:Wen-Huang Cheng:Xiaoniu Su-Chu Hsu	Interaction with virtual objects among different devices attracts lots of attention recently. LuminAR [Linder and Maes] was developed for a portable and compact projector-camera system for interactive displaying. THAW [Leigh et al.] was proposed to use a back-facing camera of a smartphone to assist the interactive displaying. RealSense [Lin et al.] was adopted the built-in compass sensor on a mobile device to calibrate the relative position among different mobile devices. However, the complex calibration process of LuminAR [Linder and Maes] and THAW [Leigh et al.] limited the applications. On the other hand, as addressed by the authors, once the users with mobile devices using RealSense [Lin et al.] move larger than 15°, the positioning relationship cannot be kept stable. Therefore, in this paper, a 3D positioning scheme is proposed based on the built-in gyro sensor on a mobile device for effective and intuitive calibration and allow users to freely move the mobile devices with a natural user experience.	G-spacing: a gyro sensor based relative 3D space positioning scheme	NA:NA:NA:NA:NA	2015
Jinhong Park:Minkyu Kim:Sunho Ki:Youngduke Seo:Chulho Shin	Although the mobile industry has recently begun trending towards high quality graphics content, it is still difficult to satisfy this trend due to performance, power and thermal issue of GPU/CPU in mobile application processor.	Half frame forwarding: frame-rate up conversion for tiled rendering GPU	NA:NA:NA:NA:NA	2015
Antoinette Leanna Bumatay:Jinsil Hwaryoung Seo	Stress is physical response that affects everyone in varying degrees. Throughout history, people have developed various practices to help cope with stress. Many of these practices focus on bringing awareness to the body and breath. Studies have shown that mindfulness meditation and paced breathing are effective tools for stress management [Brown, 2005].	Mobile haptic system design to evoke relaxation through paced breathing	NA:NA	2015
Ravi Krishnaswamy	Engineering documents e.g. 'blueprints' are one of the traditional forms of paper based information moving more to the digital realm. With mobile and the evolution of GPUs on mobile, there are tremendous opportunities for applications that view and interact with engineering documents.	Performance and precision: mobile solutions for high quality engineering drawings	NA	2015
Kristian Sons:Felix Klein:Jan Sutter:Philipp Slusallek	Graphics hardware has become ubiquitous: Integrated into CPUs and into mobile devices and recently even embedded into cars. With the advent of WebGL, accelerated graphics is finally accessible from within the web browser. However, still the capabilities of GPUs are almost exclusively exploited by the video game industry, where experts produce specialized content for game engines.	The XML3D architecture	NA:NA:NA:NA	2015
Nobuhisa Hanamitsu:Kanata Nakamura:Mhd Yamen Saraiji:Kouta Minamizawa:Susumu Tachi	Twech is a mobile platform that enables users to share visuo-tactile experience and search other experiences for tactile data. User can record and share visuo-tactile experiences by using a visuo-tactile recording and displaying attachment for smartphone, allows the user to instantly such as tweet, and re-experience shared data such as visuo-motor coupling. Further, Twech's search engine finds similar other experiences, which were scratched material surfaces, communicated with animals or other experiences, for uploaded tactile data by using search engine is based on deep learning that ware expanded for recognizing tactile materials. Twech provides a sharing and finding haptic experiences and users re-experience uploaded visual-tactile data from cloud server.	Twech: a mobile platform to search and share visuo-tactile experiences	NA:NA:NA:NA:NA	2015
Masasuke Yasumoto:Takehiro Teraoka	Various studies have been done on the combined use of mobile devices. Ohta's Pinch [Ohta and Tanaka 2012] and Leigh's THAW [Leigh et al. 2014] are representative studies. However, they have certain limitations; Pinch cannot dynamically correspond to the positional relations of the devices, and THAW cannot recognize the devices' spatial positional relations. We constructed VISTouch so that it does not require a particular kind of external sensor, and it enables multiple mobile devices to dynamically obtain other devices' relative positions in real time. We summarize VISTouch in this paper.	VISTouch	NA:NA	2015
Haruki Sato:Tatsunori Hirai:Tomoyasu Nakano:Masataka Goto:Shigeo Morishima	This paper presents a system that can automatically add a soundtrack to a video clip by replacing and concatenating an existing song's musical bars considering a user's preference. Since a soundtrack makes a video clip attractive, adding a soundtrack to a clip is one of the most important processes in video editing. To make a video clip more attractive, an editor of the clip tends to add a soundtrack considering its timing and climax. For example, editors often add chorus sections to the climax of the clip by replacing and concatenating musical bars in an existing song. However, in the process, editors should take naturalness of rearranged soundtrack into account. Therefore, editors have to decide how to replace musical bars in a song considering its timing, climax, and naturalness of rearranged soundtrack simultaneously. In this case, editors are required to optimize the soundtrack by listening to the rearranged result as well as checking the naturalness and synchronization between the result and the video clip. However, this repetitious work is time-consuming. [Feng et al. 2010] proposed an automatic soundtrack addition method. However, since this method automatically adds soundtrack with data-driven approach, this method cannot consider timing and climax which a user prefers.	A music video authoring system synchronizing climax of video clips and music via rearrangement of musical bars	NA:NA:NA:NA:NA	2015
Ergun Akleman:Siran Liu:Donald House	In this work, we present a simple mathematical approach to art directed shader development. We have tested this approach over two semesters in an introductory level graduate rendering & shading class at Texas A&M University. The students in the class each chose an artist's style to mimic, and then easily created rendered images strongly resembling that style (see Figures 1). The method provides shader developers an intuitive process, giving them a high level of visual control in the creation of stylized depictions.	Art directed rendering & shading using control images	NA:NA:NA	2015
Hiroki Kagiyama:Masahide Kawai:Daiki Kuwahara:Takuya Kato:Shigeo Morishima	In movie and video game productions, synthesizing subtle eye and corresponding head movements of CG character is essential to make a content dramatic and impressive. However, to complete them costs a lot of time and labors because they often have to be made by manual operations of skilled artists.	Automatic synthesis of eye and head animation according to duration and point of gaze	NA:NA:NA:NA:NA	2015
Shugo Yamaguchi:Chie Furusawa:Takuya Kato:Tsukasa Fukusato:Shigeo Morishima	Anime designers often paint actual sceneries to serve as background images based on photographs to complement characters. As painting background scenery is time consuming and cost ineffective, there is a high demand for techniques that can convert photographs into anime styled graphics. Previous approaches for this purpose, such as Image Quilting [Efros and Freeman 2001] transferred a source texture onto a target photograph. These methods synthesized corresponding source patches with the target elements in a photograph, and correspondence was achieved through nearest-neighbor search such as PatchMatch [Barnes et al. 2009]. However, the nearest-neighbor patch is not always the most suitable patch for anime transfer because photographs and anime background images differ in color and texture. For example, real-world color need to be converted into specific colors for anime; further, the type of brushwork required to realize an anime effect, is different for different photograph elements (e.g. sky, mountain, grass). Thus, to get the most suitable patch, we propose a method, wherein we establish global region correspondence before local patch match. In our proposed method, BGMaker, (1) we divide the real and anime images into regions; (2) then, we automatically acquire correspondence between each region on the basis of color and texture features, and (3) search and synthesize the most suitable patch within the corresponding region. Our primary contribution in this paper is a method for automatically acquiring correspondence between target regions and source regions of different color and texture, which allows us to generate an anime background image while preserving the details of the source image.	BGMaker: example-based anime background image creation from a photograph	NA:NA:NA:NA:NA	2015
Siran Liu:Ergun Akleman	In this work, we have developed an approach to include global illumination effects into Chinese Paintings (see Figure 1). Our method provides a robust approach to represent tone and value in a way similar to how Chinese Ink-and-Brush is painted. The method, especially, supports reflection, shadow, atmospheric, depth and weathering effects. Using the method, we can recapture the aesthetic of irregularity in shapes and forms commonly seen in Chinese Painting. We also arrange composition in 3D to obtain multi-camera imagee that matches the compositions in Chinese painting. We also included cinematic lighting aesthetic in 3D Chinese painting to enhance mood and storytelling.	Chinese ink and brush painting with reflections	NA:NA	2015
Jonah Friedman:Andrew C. Jones	In 3D production for commercials, television, and film, ID mattes are commonly used to modify rendered images without re-rendering. ID mattes are bitmap images used to isolate specific objects, or multiple objects, such as all of the buttons on a shirt. Many 3D pipelines are built to provide compositors with ID mattes in addition to beauty renders to allow flexibility.	Fully automatic ID mattes with support for motion blur and transparency	NA:NA	2015
Benjamin Knowles:Oleg Fryazinov	With the increasing quality of real-time graphics it is vital to make sure assets move in a convincing manner otherwise the players immersion can be broken. Grass is an important area as it can move substantially and often takes up a large portion of screen space in games. Animation of grass is a subject to academic research [Fernando 2004; Perbet and Cani 2001] as well as a technology which is implemented in a number of video games. The list includes, but is not limited to, games such as Far Cry 4, Battlefield 4, Dear Esther and Unigine Valley. Comparing video games assets with reality, it can be seen that the current methods have a number of problems which decrease the realism of the resulting grass animation. These problems include: 1) the visible planar nature of grass geometry and 2) problems with the grass movement which include over-connectivity of grass blades in respect to their neighbours, no obvious wind direction and exaggerated swaying motions. In this paper we propose to increase realism of the grass by focusing on its movement. The main contributions of this work are: 1) Distinguishing ambient and directional components of the wind and 2) The method for calculating directional wind by using a grayscale map and wind vector. The grass was implemented with vertex shaders in line with the majority of methods described in academic literature (e.g. [Fernando 2004]) and implemented in modern games.	Increasing realism of animated grass in real-time game environments	NA:NA	2015
Seungbae Bang:Byungkuk Choi:Roger Blanco I Ribera:Meekyoung Kim:Sung-Hee Lee:Junyong Noh	Skeleton-driven animation is a widespread technique, which is frequently used in film and video game productions to animate 3D characters. The process of preparing characters for skeletal animation is referred to as character rigging. Commercial applications such as Maya or 3DS Max provide many tool that support this process, including the 'joint tool' and the 'paint skin weights tool'. Most of these tools are difficult to use for novice users. Even for professional artists, it requires many hours of intensive effort.	Interactive rigging	NA:NA:NA:NA:NA:NA	2015
Simon Pabst:Hansung Kim:Lukáš Polok:Viorela Ila:Ted Waine:Adrian Hilton:Jeff Clifford	Modern digital film production uses large quantities of data captured on-set, such as videos, digital photographs, LIDAR scans, spherical photography and many other sources to create the final film frames. The processing and management of this massive amount of heterogeneous data consumes enormous resources. We propose an integrated pipeline for 2D/3D data registration aimed at film production, based around the prototype application Jigsaw. It allows users to efficiently manage and process various data types from digital photographs to 3D point clouds. A key step in the use of multi-modal 2D/3D data for content production is the registration into a common coordinate frame (match moving). 3D geometric information is reconstructed from 2D data and registered to the reference 3D models using 3D feature matching [Kim and Hilton 2014]. We present several highly efficient and robust approaches to this problem. Additionally, we have developed and integrated a fast algorithm for incremental marginal covariance calculation [Ila et al. 2015]. This allows us to estimate and visualize the 3D reconstruction error directly on-set, where insufficient coverage or other problems can be addressed right away. We describe the fast hybrid multi-core and GPU accelerated techniques that let us run these algorithms on a laptop. Jigsaw has been used and evaluated in several major digital film productions and significantly reduced the time and work required to manage and process on-set data.	Jigsaw: multi-modal big data management in digital film production	NA:NA:NA:NA:NA:NA:NA	2015
Daniel Camozzato:Leandro Dihl:Ivan Silveira:Fernando Marson:Soraia R. Musse	Computer graphics applications require models which are crafted by hand, requiring skill and time. In architectural applications an automated system that converts 2D floor plan images into 3D building models can be used to lower the modeling cost, and in video games procedural algorithms can be used to generate content, including cities, buildings and floor plans. One motivation to generate floor plans for predetermined building exteriors is that building generators often create only a façade without the interior. Another motivation is in planning real-world layouts, often tackled as an optimization problem (e.g. [Merrell et al. 2010] and [Peng et al. 2014]). The state of the art in [Merrell et al. 2010] uses machine learning and stochastic optimization to generate realistic layouts. However, it is unsuitable in our case because the exterior appears as a result of the layout. Our approach generates floor plans using both the building exterior and user requisites as constraints. The proposed method [Camozzato et al. 2015] handles a variety of image styles and building shapes, and the run time remains low (around 1 ms versus a 30 s optimization reported by [Merrell et al. 2010]).	Procedural floor plan generation from building sketches	NA:NA:NA:NA:NA	2015
Chun-Kai Huang:Yi-Ling Chen:I-Chao Shen:Bing-Yu Chen	We introduce an interactive method suitable for retargeting both 3D objects and scenes under a general framework. Initially, an input object or scene is decomposed into a collection of constituent components embraced by corresponding control bounding volumes which capture the intra-structures of the object or the semantic groupings of the objects in the scene. The overall retargeting is accomplished through a constrained optimization by manipulating the control bounding volumes. Without inferring the intricate dependencies between the components, we define a minimal set of constraints that maintain the spatial arrangement and connectivity between the components to regularize valid retargeting results. The default retargeting behavior can then be easily altered by additional semantic constraints imposed by users.	Retargeting 3D objects and scenes	NA:NA:NA:NA	2015
Yu Wang:Marc Olano	We present a framework for modeling solid-fluid phase change. Our framework is physically-motivated, with geometric constraints applied to define rigid dynamics using shape matching. In each simulation step, particle positions are updated using an extended SPH solver where they are treated as fluid. Then a geometric constraint is computed based on current particle configuration, which consists of an optimal translation and an optimal rotation. Our approach differs from methods such as [Carlson et al. 2004] in that we solve rigid dynamics by using a stable geometric constraint [Müller et al. 2005] embedded in a fluid simulator.	Rigid fluid	NA:NA	2015
Katsuhisa Kanazawa:Ryoma Tanabe:Tomoaki Moriya:Tokiichiro Takahashi	Realistic representation of nature scenes is one of the most challenging areas in computer graphics community. There are important factors to synthesize realistic scenes in 3D CG which are decayed materials such as dead trees, weathered statues, rusty metals and so on. We are interested in the methodology for simulating its decaying processes. In this paper, we propose a simple method for rust aging simulation based on a probabilistic cellular automaton model taking into account object's geometries.	Rust aging simulation considering object's geometries	NA:NA:NA:NA	2015
I Chiang:Po-Han Lin:Yuan-Hung Chang:Ming Ouhyoung	Synthesizing competitive interactions between two avatars in a physics-based simulation remains challenging. Most previous works rely on reusing motion capture data. They also need an offline preprocessing step to either build motion graphs or perform motion analysis. On the other hand, an online motion synthesis algorithm [Hämäläinen et al. 2014] can produce physically plausible motions including balance recovery and dodge projectiles without prior data. They use a kd-tree sequential Monte Carlo sampler to optimize the joint angle trajectories. We extend their approach and propose a new objective function to create two-character animations in a close-range combat. The principles of attack and defense are designed according to fundamental theory of Chinese martial arts. Instead of following a series of fixed Kung Fu forms, our method gives 3D avatars the freedom to explore diverse movements and through pruning can finally evolve an optimal way for fighting.	Synthesizing close combat using sequential Monte Carlo	NA:NA:NA:NA	2015
Jaehwan Kim:JongYoul Park:Kyoung Park	Unsupervised matting, whose goal is to extract interesting foreground components from arbitrary and natural background regions without any additional information of the contents of the corresponding scenes, plays an important role in many computer vision and graphics applications. Especially, the precisely extracted object images from the matting process can be useful for automatic generation of large-scale annotated training sets with more accuracy, as well as for improving the performance of a variety of applications including content-based image retrieval. However, unsupervised matting problem is intrinsically ill-posed so that it is hard to generate a perfect segmented object matte from a given image without any prior knowledge. This additional information is usually fed by means of a trimap which is a rough pre-segmented image consisting of three subregions of foreground, background and unknown. When such matting process is applied to object collections in a large-scale image set, the requirement for manually specifying every trimap for each of independent input images can be a serious drawback definitely. Recently, automatic detection of salient object regions in images has been widely researched in computer vision tasks including image segmentation, object recognition and so on. Although there are many different types of proposal measures in methodology under the common perceptual assumption of a salient region standing out its surrounding neighbors and capturing the attention of a human observer, most final saliency maps having lots of noises are not sufficient to take advantage of the consequent computational processes of highly accurate low-level representation of images.	UnAMT: unsupervised adaptive matting tool for large-scale object collections	NA:NA:NA	2015
Naoki Nozawa:Daiki Kuwahara:Shigeo Morishima	A reconstruction of a human face shape from a single image is an important theme for criminal investigation such as recognition of suspected people from surveillance cameras with only a few frames. It is, however, still difficult to recover a face shape from a non-frontal face image. Method using shading cues on a face depends on the lighting circumstance and cannot be adapted to images in which shadows occurs, for example [Kemelmacher et al. 2011]. On the other hand, [Blanz et al. 2004] reconstructed a shape by 3D Morphable Model (3DMM) only with facial feature points. This method, however, requires the pose-wise correspondences of vertices in the model to feature points of input image because a face contour cannot be seen when the facial direction is not the front. In this paper, we propose a method which can reconstruct a facial shape from a non-frontal face image only with a single general correspondence table. Our method searches for the correspondences of points on a facial contour in the iterative reconstruction process, and makes the reconstruction simple and stable.	3D face reconstruction from a single non-frontal face image	NA:NA:NA	2015
Toshihiko Yamasaki:Yusuke Nakano:Kiyoharu Aizawa	3D printing is becoming a more common technology and has a growing number of applications. Although 3D compression algorithms have been studied in the computer graphics (CG) community for decades, the quality of the compressed 3D models are discussed only in the CG space. In this paper, we discuss the relationship between the PSNR of the compressed 3D models and the human perception to the printed objects. We conducted subjective evaluation by inviting 13 people and found that there is a clear linear relationship between them. Such a quality perception model is useful for estimating the printing quality of the compressed 3D models and deciding reasonable compression parameters.	A prediction model on 3D model compression and its printed quality based on subjective study	NA:NA:NA	2015
Toru Kawanabe:Tomoko Hashida	In recent years, there has been rapid development of techniques for superimposing virtual information on real-world scenes and changing the appearance of actual scenes in arbitrary ways. We are particularly interested in means of arbitrarily changing the appearance of real-world scenes without the use of physical interfaces such as glasses or other devices worn by the user. In this paper, we refer to such means as spatial displays. Typical examples of spatial displays include a system that can change the transparency or physical properties of buildings [Rekimoto, 2012] and a system that projects video images [Raskar, 2001]. However, those systems have restrictions such as requiring some kind of physical interface between the user and the scene or not being usable in a well-lit environment. Taking a different approach, we turned our attention to a natural phenomenon referred to as heat haze, in which the appearance of objects is altered by changes in the refractive index of air caused by differences in temperature distribution. We propose the atmoRefractor, a system that can generate and control heat haze on a small scale without an additional physical interface such as lenses. That locally controllable heat haze effect can be used to direct attention by changing the appearance of certain parts of scenes.	atmoRefractor: spatial display by controlling heat haze	NA:NA	2015
Tony Tung	Consumer RGBD sensors are becoming ubiquitous and can be found in many devices such as laptops (e.g., Intel's RealSense) or tablets (e.g., Google Tango, Structure, etc.). They have become popular in graphics, vision, and HCI communities as they enable numerous applications such as 3D capture, gesture recognition, virtual fitting, etc. Nowadays, common sensors can deliver a stream of color images and depth maps in VGA resolution at 30 fps. While the color image is usually of sufficient quality for visualization, depth information (represented as a point cloud) is usually too sparse and noisy for readable rendering.	Augmented dynamic shape for live high quality rendering	NA	2015
Nobuhiko Mukai:Naoki Mita:Youngha Chang	[Hong et al. 2008] proposed a hybrid method of Eulerian grids and Lagrangian particles to represent small-scale bubbles in large-scale water. In the simulation, bubbles rise freely in the water; however, bubble seeds are set at random at the bottom and they disappear as soon as they arrive at the water surface. [Patkar et al. 2013] also proposed a hybrid Lagrangian-Eulerian framework to visualize both small and large scale bubbles. A bubble that exits the spout of a water dispenser flows up in the water with changing its shape; however, they did not simulate the rupture process of bubble. Then, [Mukai et al. 2012] tried a rupture simulation by using MPS method; however, it did not consider the high density ratio of the water to the air so that the bubble ruptured gradually. Therefore, this paper proposes a method of bubble rupture simulation, where a wave is generated after a bubble has ruptured rapidly, by considering the high density ratio of the water to the air.	Bubble rupture simulation by considering high density ratio	NA:NA:NA	2015
Yajie Yan:Tao Ju:David Letscher:Erin Chambers	Medial axis is a classical shape descriptor that is widely used in computer graphics, computer vision, and pattern recognition. Defined elegantly as the locus of points with multiple nearest neighbors on the object boundary, the medial axis preserves both the structure and topology of the object in a compact form - a geometry that has one lower dimension than the object itself.	Burning the medial axis	NA:NA:NA:NA	2015
Hisashi Watanabe:Toshiya Fujii:Tatsuya Nakamura:Tsuguhiro Korenaga	It is a common philosophical question as to whether your blue is the same as my blue. The two-tone striped dress shown in Figure 1, which attracted a lot of attention on the Internet, gave us a clear answer: "No." Some people see the dress as blue and black, whereas others insist it's white and gold. So your blue can be my white. Why is it that people looking at the same picture perceive totally different color combinations?	Color perception difference: white and gold, or black and blue?	NA:NA:NA:NA	2015
Yang Kang:Chi Xu:Shujin Lin:Songhua Xu:Xiaonan Luo:Qiang Chen	Sketching is a natural human practice. With the popularity of multi-touch tablets and styluses, sketching has become a more popular means of human-computer interaction. However, accurately recognizing sketches is rather challenging, especially when they are drawn by non-professionals. Therefore, automatic sketch understanding has attracted much research attention. To tackle the problem, we propose to segment sketch drawings before analyzing the semantic meanings of sketches for the purpose of developing a sketch-based 3D model retrieval system.	Component segmentation of sketches used in 3D model retrieval	NA:NA:NA:NA:NA:NA	2015
Afsaneh Rafighi:Sahand Seifi:Oscar Meruvia-Pastor	This paper presents a novel method for automatic registration of video streams originated from two depth-sensing cameras. The system consists of a sender and receiver, in which the sender obtains the streams from two RGBD sensors placed arbitrarily around a room and produces a unified scene as a registered point cloud. A conventional method to support a multi-depth sensor system is through calibration. However, calibration methods are time consuming and require the use of external markers prior to streaming. If the cameras are moved, calibration has to be repeated. The motivation of this work is to facilitate the use of RGBD sensors for non-expert users, so that cameras need not to be calibrated, and if cameras are moved, the system will automatically recover the alignment of the video streams. DeReEs [Seifi et al. 2014], a new registration algorithm, is used, since it is fast and successful in registering scenes with small overlapping sections.	Continuous and automatic registration of live RGBD video streams with partial overlapping views	NA:NA:NA	2015
Michelle Holloway:Tao Ju:Cindy Grimm	In clinical practice, when a subject is imaged (i.e. CT scan or MRI) the result is a 3D image of volumetric data. In order to study the organ, bone, or other object of interest, this data needs to be segmented to obtain a 3D model that can be used in any number of down stream applications. When used for treatment planning these segmentations need to not only be accurate but also produced quickly to avoid health risks. Automatic segmentation methods are becoming more reliable but many experts in the scientific community still rely on time consuming manual segmentation.	Contour guided surface deformation for volumetric segmentation	NA:NA:NA	2015
Peihong Guo:Ergun Akleman:He Ying:Xiaoning Wang:Wei Liu	In this work, we present some of the unexpected observations resulted from our recent research. We, recently, needed to identify a small number of important critical points, i.e. minimum, maximum and saddle points, on a given manifold mesh surface. All critical points on a manifold triangular mesh can be identified using discrete Gaussian curvature, which is given as ki = 2π − Σj θi,j where ki is vertex defect (the discrete Gaussian curvature) of the vertex i and θi,j is the corner of the vertex in the triangle j. A very useful property coming with vertex defect is the discrete version of Gauss-Bonnet theorem: the sum of all vertex defects is always constant as Σi ki = 2π(2−2g) where g is the genus of the mesh. Any vertex with a non-zero vertex defect is really an critical point of the surface. However, identification of interesting critical points is hard with vertex defect alone. As it can be seen in Figure 1(a), even we ignore vertex defects that are small, too many vertices are still chosen and this information is not really useful to make any conclusion of the shape of the surface.	Critical points with discrete Morse theory	NA:NA:NA:NA:NA	2015
Nahomi Maki:Kazuhisa Yanaka	Various colors, such as in a prism, are observed in properly cut diamond even under white light because of dispersion. Properly-cut diamond brings about scintillation when viewing angle is changed, because total reflection inside a diamond tends to occur frequently due to the large refractive index. Moreover, strong rainbow colors are seen because of high dispersion ratio.	Display of diamond dispersion using wavelength-division rendering and integral photography	NA:NA	2015
Slim Ouni:Guillaume Gris	One main concern of audiovisual speech research is the intelligibility of audiovisual speech (i.e., talking head). In fact, lip reading is crucial for challenged population as hard of hearing people. For audiovisual synthesis and animation, this suggests that one should pay careful attention to modeling the region of the face that participates actively during speech. Above all, a facial animation system needs extremely good representations of lip motion and deformation in order to achieve realism and effective communication.	Dynamic realistic lip animation using a limited number of control points	NA:NA	2015
Byeongjun Choi:Woong Seo:Insung Ihm	In the ray-tracing community, the surface-area heuristic (SAH) has been employed as a de facto standard strategy for building a high-quality kd-tree. Aiming to improve both time and space efficiency of the conventional SAH-based kd-tree in ray tracing, we propose to use an extended kd-tree representation for which an effective tree-construction algorithm is provided. Our experiments with several test scenes revealed that the presented kd-tree scheme significantly reduced the memory requirement for representing the tree structure, while also increasing the overall frame rate for rendering.	Enhancing time and space efficiency of kd-tree for ray-tracing static scenes	NA:NA:NA	2015
Hisataka Suzuki:Rex Hsieh:Ryotaro Tsuda:Akihiko Shirai	In recent years, 3D technology has become so widespread that the technology alone no longer fascinates the viewers. To achieve further technical innovations on display experience, we should explore the limitations of 3D devices.	ExPixel FPGA: multiplex hidden imagery for HDMI video sources	NA:NA:NA:NA	2015
Yoichi Ochiai:Kota Kumagai:Takayuki Hoshi:Jun Rekimoto:Satoshi Hasegawa:Yoshio Hayasaki	We envision a laser-induced plasma technology in general applications for public use. If laser-induced plasma aerial images were made available, many useful applications such as spatial aerial AR, aerial user interfaces, volumetric images could be produced. This would be a highly effective display for the expression of three-dimensional information. Volumetric expression has considerable merit because the content scale corresponds to the human body; therefore, this technology could be usefully applied to wearable materials and spatial user interactions. Further, laser focusing technology can add an additional dimension to conventional projection technology, which is designed for surface mapping, while laser focusing technology is capable of volumetric mapping. This technology can be effectively used in real-world-oriented user interfaces.	Fairy lights in femtoseconds: aerial and volumetric graphics rendered by focused femtosecond laser combined with computational holographic fields	NA:NA:NA:NA:NA:NA	2015
Jérémy Levallois:David Coeurjolly:Jacques-Olivier Lachaud	During a snowfall, the snow crystals accumulate on the ground and gradually form a complex porous medium constituted of air, water vapour, ice and sometimes liquid water. This ground-lying snow transforms with time, depending on the physical parameters of the environment. The main purpose of the digitalSnow project is to provide efficient computational tools to study the metamorphism of real snow microstructures from 3D images acquired using X tomography techniques. We design 3D image-based numerical models than can simulate the shape evolution of the snow microstructure during its metamorphism. As a key measurement, (mean) curvature of snow microstructure boundary plays a crucial role in metamorphosis equations (mostly driven by mean curvature flow). In our previous work, we have proposed robust 2D curvature and 3D mean and principal curvatures estimators using integral invariants. In short, curvature quantities are estimated using a spherical convolution kernel with given radius R applied on point surfaces [Coeurjolly et al. 2014]. The specific aspect of these estimators is that they are defined on (isothetic) digital surfaces (boundary of shape in Z3). Tailored for this digital model, these estimators allow us to mathematically prove their multigrid convergence, i.e. for a class of mathematical shapes (e.g. C3-boundary and bounded positive curvature), the estimated quantity converges to the underlying Euclidean one when shapes are digitized on grids with gridstep tending to zero. In this work, we propose to use the radius R of our curvature estimators as a scale-space parameter to extract features on digital shapes. Many feature estimators exist in the literature, either on point clouds or meshes ("ridge-valley", threshold on principal curvatures, spectral analysis from Laplacian matrix eigenvalues, . . . ). In the context of objects in Z3 and using our robust curvature estimator, we define a new feature extraction approach on which theoretical results can be proven in the multigrid framework.	Feature extraction on digital snow microstructures	NA:NA:NA	2015
A. Andreadis:R. Gregor:I. Sipiran:P. Mavridis:G. Papaioannou:T. Schreck	The problem of object restoration from eroded fragments where large parts could be missing is of high relevance in archaeology. Manual restoration is possible and common in practice but it is a tedious and error-prone process, which does not scale well. Solutions for specific parts of the problem have been proposed but a complete reassembly and repair pipeline is absent from the bibliography. We propose a shape restoration pipeline consisting of appropriate methods for automatic fragment reassembly and shape completion. We demonstrate the effectiveness of our approach using real-world fractured objects.	Fractured 3D object restoration and completion	NA:NA:NA:NA:NA:NA	2015
Caigui Jiang:Chengcheng Tang:Jun Wang:Johannes Wallner:Helmut Pottmann	In freeform architecture and fabrication aware design, repetitive geometry is a very important contribution to the reduction of production costs. This poster addresses two closely related geometric rationalizations of freeform surfaces with repetitive elements: freeform honeycomb structures defined as torsion-free structures where the walls of cells meet at 120 degrees, and Lobel frames formed by equilateral triangles. There turns out to be an interesting duality between these two structures, and this poster discusses the geometric relation, computation, modeling as well as applications of them.	Freeform honeycomb structures and lobel frames	NA:NA:NA:NA:NA	2015
Antoine Toisoul:Abhijeet Ghosh	We present a novel approach for image based relighting using the lighting controls available in a regular room. We employ individual light sources available in the room such as windows and house lights as basis lighting conditions. We further optimize the projection of a desired lighting environment into the sparse room lighting basis in order to closely approximate the target lighting environment with the given lighting basis. We achieve plausible relit results that compare favourably with ground truth relighting with dense sampling of the reflectance field.	Image based relighting using room lighting basis	NA:NA	2015
Daniel Rakita:Tomislav Pejsa:Bilge Mutlu:Michael Gleicher	Motion-captured performances seldom include eye gaze, because capturing this motion requires eye tracking technology that is not typically part of a motion capture setup. Yet having eye gaze information is important, as it tells us what the actor was attending to during capture and it adds to the expressivity of their performance.	Inferring gaze shifts from captured body motion	NA:NA:NA:NA	2015
Hiroki Yamamoto:Hajime Kajita:Hanyuool Kim:Naoya Koizumi:Takeshi Naemura	In design process and medical visualization, e.g. CT/MRI cross-sectional images, exterior and interior images can help users to understand the overall shape of volumetric objects. For this purpose, displays need to provide both vertical and horizontal images at the same time. To display cross-sectional images, an LCD display [Cassinelli et al. 2009] and image projection [Nagakura et al. 2006] have been proposed. Although these displays could show internal images of volumetric objects, seamless crossing of internal and external images cannot be realized since the images are limited to physical displays.	Mid-air plus: a 2.5 D cross-sectional mid-air display with transparency control	NA:NA:NA:NA:NA	2015
Takuya Kato:Akira Kato:Naomi Okamura:Taro Kanai:Ryo Suzuki:Yuko Shirai	Trees have been a pillar of our lives not just for human but for all the species living in the earth. Despite of its blessings for our lives, the heaps of problems around forestry have not been solved. One of the major problems in this field is that most of the forest are not been sorted into an organized database. Detailed natural data have never been provided even in famous map applications, Google earth for instance, induced from its difficulty. The forest database has been demanded in many regions as it provides beneficial information for both industrial and environmental aspects. It even helps many divisions such as CG animations to simulate not only a tree itself but also the mountain or the forest as a whole depending on given natural conditions.	Musasabi: 2D/3D intuitive and detailed visualization system for the forest	NA:NA:NA:NA:NA:NA	2015
Beibei Wang:Xiangxu Meng:Tamy Boubekeur	Point-Based Global Illumination (PBGI) [2008] is a popular rendering method in special effects and motion picture productions. This algorithm provides a diffuse global illumination solution by caching radiance in a mesh-less hierarchical data structure during a pre-process, while solving for visibility over this cache, at rendering time and for each receiver, using microbuffers, which are localized depth and color buffer inspired from real time rendering environments. As a result, noise free ambient occlusion, indirect soft shadows and color bleeding effects are computed efficiently for high resolution image output and in a temporally coherent fashion. We propose an evolution of this method to address the case of non-diffuse inter-reflections and refractions using wavelets instead of spherical harmonics (see Fig. 1). We also propose a new importance-driven adaptive microbuffer model to capture accurately incoming radiance at a point. Furthermore, we evaluate outgoing radiance using a fast wavelet radiance product, containing the memory footprint by encoding hierarchically the wavelets tree.	Non-diffuse effects for point-based global illumination	NA:NA:NA	2015
Hajime Kajita:Naoya Koizumi:Takeshi Naemura	Mid-air imaging has the advantage of expression along the depth direction. For example, MARIO [1], a mid-air display, can form an image in the depth range of 30 cm by physically moving the light source display. Multi-layered mid-air images can be displayed at various depths, but such multi-layered images are transparent and experience color mixture due to the addition of light from the light source displays. It is difficult to see the front of transparent images because they have no occlusion expression.	OpaqueLusion: opaque mid-air images using dynamic mask for occlusion expression	NA:NA:NA	2015
Christian Hafner:Przemyslaw Musialski:Thomas Auzinger:Michael Wimmer:Leif Kobbelt	Keyboard percussion instruments such as xylophones and glockenspiels are composed of an arrangement of bars. These are varied in some of their geometrical properties---typically the length---in order to influence their acoustic behavior. Most instruments in this family do not deviate from simple geometrical shapes, since designing the natural frequency spectrum of complex shapes usually involves a pain-staking trial-and-error process and has been reserved to gifted artisans or professional manufacturers.	Optimization of natural frequencies for fabrication-aware shape modeling	NA:NA:NA:NA:NA	2015
Junichi Sugita:Tokiichiro Takahashi	Many people have been familiar with subtractive color model based on pigment color compositing since their early childhood. However, the RGB color space is not comprehensible for children due to additive color compositing. In the RGB color space, the resulting mixture color is often different from colors viewer expected. CMYK is a well-known subtractive color space, but its three primal colors are not familiar. Kubelka-Munk model (KM model in short) simulates pigment compositing as well as paint-like appearance by physically-based simulation. However, it is difficult to use KM model because of many simulation parameters.	Paint-like compositing based on RYB color model	NA:NA	2015
Naoki Hashimoto:Koki Kosaka	We propose a photometric compensation for projecting arbitrary images on practical surfaces of our everyday life. Although many previous proposals have achieved fine compensation at their experimental environments [Nayar et al. 2003], they cannot support practical targets including high-contrast texture. In order to adapt to such situation, we need a time-consuming iterative processing with camera feedback. Even though the iterative processing is applied, we cannot obtain fine compensation because no camera pixels of a projector-camera system (procam) correspond perfectly to the pixels of the projector [Mihara et al. 2014].	Photometric compensation for practical and complex textures	NA:NA	2015
Takefumi Hiraki:Issei Takahashi:Shotaro Goto:Shogo Fukushima:Takeshi Naemura	Forming images by using a swarm of mobile robots has emerged as a new platform for computer entertainment. Each robot has colored lighting, and the swarm represents various abstract patterns by using the lighting and the locomotion.	Phygital field: integrated field with visible images and robot swarm controlled by invisible images	NA:NA:NA:NA:NA	2015
Ari Rapkin Blenkhorn	The glory is a colorful atmospheric phenomenon which resembles a small circular rainbow on the front surface of a cloudbank. It is most frequently seen from aircraft when the observer is directly between the sun and the clouds. Glories are also sometimes seen by skydivers looking down through thin cloud layers. They are always centered around the shadow of the observer's head (or camera).	Real-time rendering of atmospheric glories	NA	2015
Hiroyuki Kubo:Kohe Tokoi:Yasuhiro Mukaigawa	To synthesize realistic translucent materials in computer graphics, it is necessary to simulate the effect of subsurface scattering. In previous works, several methods are proposed for rendering such materials in real-time. The screen space subsurface scattering (SSSS) is developed by Jimenez et al. [2009], yet the speed of rendering is not very practical for low-end computational environment, because screen space techniques require huge number of texture samplings. We previously propose a curvature-based shading method [Kubo et al. 2010] which approximates the effect of subsurface scattering according to the curvature. Since the curvature is determined by the surface shape of neighbors, it is not able to compute the effect of scattering light from the behind of the object. In this paper, we propose a novel shading method depending on the translucency magnitude which represents the significance of the subsurface scattering effect. According to the translucency magnitude, we modulate the reflectance to imitate the effect of subsurface scattering. Since this modulation is very simple to compute, we are able to render translucent materials in real-time not only in high-end workstations but also low-end mobile devices.	Real-time rendering of subsurface scattering according to translucency magnitude	NA:NA:NA	2015
Kang Zhang:Wuyi Yu:Mary Manhein:Warren Waggenspack:Xin Li	Geometric restoration that composes 3D fragmented pieces into the original complete object is an important computer graphics and geometric processing problem. Automatic and effective restoration has applications in many fields such as archeological reconstruction, digital heritage archiving, forensic evidence processing, to name a few. For example, archaeologists reconstruct ceramic fragments (sherds) into complete pots in order to analyze the information of the ancient society. Forensic scientists reassemble skull fragments into complete skull for face reconstruction and body identification. In both of these problems we need to solve a composition of digitized thin-shell fragments with different shapes, sizes, and resolutions. This problem remains very challenging.	Reassembling 3D thin shells using integrated template guidance and fracture region matching	NA:NA:NA:NA:NA	2015
Keita Sekijima:Hiroya Tanaka	Digital materials are discrete elements such as LEGO Blocks that it can be a kind of reconfigurable 3D matters. There are two advantages of using digital material rather than a continuous material. Firstly, it is easy to change the form after shaping by assembling and disassembling the elements. Secondly, There is never that the error of the part impacts the whole form in the shaping because the elements can be connected exactly by the joint system. There are many researches of digital material focus on the modular connection by press fitting or bonding. Such a digital material can't be assembled and disassembled smoothly after shaped. In our research, we designed the digital material "Kelvin Block" (figure 1a) that specialized in smoothly reconfiguring, and we developed the machine "3D Assembler" (figure 1b) to arrange Kelvin Blocks automatically. The size of Kelvin Block is 40mmx40mmx40mm that is optimized to the volume of the joint system.	Reconfigurable three-dimensional prototype system using digital materials	NA:NA	2015
Francisco Inácio:Jan P. Springer	Maintaining a high steady frame rate is an important aspect in interactive real-time graphics. It is mainly influenced by the number of objects and the number of lights to be processed for a 3d scene. The upper-bound effort for rendering a scene is then defined by the number of objects times the number of lights, i. e. O(NO · NL). Deferred shading reduces this upper bound to the number of objects plus the number of lights, i. e. O(NO + NL), by separating the rendering process into two phases: geometry processing and lighting evaluation. The geometry processing rasterizes all objects but only retains visible fragments in a G-Buffer for the current viewpoint. The lighting evaluation then only needs to process those surviving fragments to compute the final image (for the current viewpoint). Unfortunately, this approach not only trades computational effort for memory but also requires the re-creation of the G-Buffer every time the viewpoint changes. Additionally, transparent objects cannot be encoded into a G-Buffer and must be separately processed. Post-rendering 3d warping [Mark et al. 1997] is one particular technique that allows to create images from G-Buffer information for new viewpoints. However, this only works with sufficient fragment information. Objects not encoded in the G-Buffer, because they were not visible from the original viewpoint, will create visual artifacts at discontinuities between objects. We propose fragment-history volumes (FHV) to create novel viewpoints from a discrete representation of the entire scene using current graphics hardware and present an initial performance comparison.	Reducing geometry-processing overhead for novel viewpoint creation	NA:NA	2015
Fumiya Narita:Shunsuke Saito:Takuya Kato:Tsukasa Fukusato:Shigeo Morishima	Dressing virtual characters is necessary for many applications, while modeling clothing is a significant bottleneck. Therefore, it has been proposed that the idea of Garment Transfer for transfer-ring clothing model from one character to another character [Brouet et al. 2012]. In recent years, this idea has been extended to be applicable between characters in various poses and shapes [Narita et al. 2014]. However, texture design of clothing is not preserved in their method since they deform the source clothing model to fit the target body (see Figure 1(a)(c)).	Texture preserving garment transfer	NA:NA:NA:NA:NA	2015
Paul Kilgo:Jerry Tessendorf	A Monte Carlo multiple scattering technique for participating media is extended. Validation against an experimentally well-studied optics problem is discussed. Designing initial paths for a numerical integration of Feynman path integrals is posed. A plot of the resulting integration is discussed.	Toward validation of a Monte Carlo rendering technique	NA:NA	2015
Caleb Brose:Martin Thuo:Jeremy W. Sheaffer	We present a system for tracking the movement and deformation of drops of water in free fall and collision. Our data comes from a high-speed camera which records 60,000 frames per second. The data is noisy, and is compromised by an unfortunate camera angle and poor lighting which contribute to caustics, reflections, and shadows in the image. Given an input video, we apply techniques from image processing, computer vision and computational geometry to track the the droplet's position and shape. While our tool could monitor the movement of transparent fluids in a more general environment, our data specifically depicts water colliding with hydrophobic materials. The output of our processing is used by materials scientists to better our understanding of the interactions between water and hydrophobic surfaces. These interactions have direct application in the materials engineering of next generation printing technologies.	Tracking water droplets under descent and deformation	NA:NA:NA	2015
Xueming Yu:Shanhe Wang:Jay Busch:Thai Phan:Tracy McSheery:Mark Bolas:Paul Debevec	High-end facial performance capture solutions typically use head-mounted camera systems which provide one or more close-up video streams of each actor's performance. These provide clear views of each actor's performance, but can be bulky, uncomfortable, get in the way of sight lines, and prevent actors from getting close to each other. To address this, we propose a virtual head-mounted camera system: an array of cameras placed around around the performance capture volume which automatically track zoomed-in, sharply focussed, high-resolution views of the each actor's face from a multitude of directions. The resulting imagery can be used in conjunction with body motion capture data to derive nuanced facial performances without head-mounted cameras.	Virtual headcam: pan/tilt mirror-based facial performance tracking	NA:NA:NA:NA:NA:NA:NA	2015
