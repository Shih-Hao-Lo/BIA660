Bing-Yu Chen:Shih-Chiang Dai:Shuen-Huei Guan:Tomoyuki Nishita	In this extended abstract, we present a system that allows the user to animate character images in 3D space by applying an existed 3D character model with motion data. The character model with skeleton rigged is used as a template model to fit the silhouette of the character image. After assigning some corresponding points between the character image and template model, the system then fits the model to the image and transfer the colors and patterns of the image to the model as the textures. Finally, the user can apply any motion data to animate the fitted 3D character model in 3D space.	Animating character images in 3D space	NA:NA:NA:NA	2009
Fu-Chung Huang:Yu-Mei Chen:Tse-Hsien Wang:Bing-Yu Chen:Shuen-Huei Guan	Speech animation is traditionally considered as important but tedious work for most applications, because the muscles on the face are complex and dynamically interacting. In this paper, we introduce a framework for synthesizing a 3D lip-sync speech animation by a given speech sequence and its corresponding texts. We first identify the representative key-lip-shapes from a training video that are important for blend-shapes and guiding the artist to create corresponding 3D key-faces (lips). The training faces in the video are then cross-mapped to the crafted key-faces to construct the Dominated Animeme Models (DAM) for each kind of phoneme. Considering the coarticulation effects in animation control signals from the cross-mapped training faces, the DAM computes two functions: polynomial-fitted animeme shape functions and corresponding dominance weighting functions. Finally, given a novel speech sequence and its corresponding texts, a lip-sync speech animation can be synthesized in a short time with the DAM.	Animating lip-sync speech faces by dominated animeme models	NA:NA:NA:NA:NA	2009
Nozomi Kugimoto:Rui Miyazono:Kosuke Omori:Takeshi Fujimura:Shinichi Furuya:Haruhiro Katayose:Hiroyoshi Miwa:Noriko Nagata	Technologies recreating piano performance in the form of CG animation are eagerly anticipated by people working in various fields, such as content production, music education, etc. Nonetheless, much of the past research has dealt with the mechanical finger movements in piano practice support systems and performance support GUIs, etc. and there has been little research recreating the reality of finger movements. We are promoting research into the analysis and CG expression of realistic and natural piano fingering. This paper describes the following aspects of this research program: (i) measurement of piano fingering using motion capture technology, (ii) generation of a CG animation of fingering using offline/realtime rendering, and (iii) automatic generation of fingering using optimized algorithms. And finally we will introduce examples in which the fingering data created in (i) is used in TV animation.	CG animation for piano performance	NA:NA:NA:NA:NA:NA:NA:NA	2009
Shinsuke Nakamura:Masashi Shiraishi:Shigeo Morishima:Mayu Okumura:Yasushi Makihara:Yasushi Yagi	Characteristics of human motion, such as walking, running or jumping vary from person to person. Differences in human motion enable people to identify oneself or a friend. However, it is challenging to generate animation where individual characters exhibit characteristic motion using computer graphics. Our goal is to construct a system that synthesizes characteristic gait animation automatically. As a result, when crowd animation is generated for instance, the motion with the variation can be made using our system. In our system, we first acquire a silhouette image as input data using a video camera. Second, we extract gait feature from single view silhouette. Finally we automatically synthesize 3D gait animation using the method blending a small number of motion data [KOVAR, L et al 2003].This blending weight is estimated using the gait feature automatically.	Characteristic gait animation synthesis from single view silhouette	NA:NA:NA:NA:NA:NA	2009
Takeshi Miura:Kazutaka Mitobe:Takaaki Kaiga:Takashi Yukawa:Toshiyuki Taniguchi:Hideo Tamamoto	In the field of dance motion analysis, development of the technique for extraction of characteristic postures peculiar to each dance number is needed [Hachimura 2006]; extracted postures can be used as the indexes for the retrieval of motion data. In this study, the authors suggest a novel method for extraction of characteristic postures from the motion data of a dance number; the information of uniqueness of the dance number given by the statistical analysis of a database including motion data of plural dance numbers is used in the extraction process.	Extraction of characteristic postures in a dance by statistical analysis of a database of motion data	NA:NA:NA:NA:NA:NA	2009
Yohei Shimotori:Shiori Sugimoto:Shigeo Morishima	Shadows in 2D Anime play a significant role for expressing symbolic visual effects such as the character's position and shape. However, animators frequently can't draw detailed shadows according to their intentions because of time constraints and a lack of skilled animators. For solving this problem, we have developed a system that can generate shadows automatically. Our system provides simple shadows and shadows on the water by applying Simplification Filter and Water Mapping Filter. Also, our system only requires inputs of the 2D character animation layers generally composed in the Anime industry. Consequently, our system enables animators to intuitively produce Anime-like shadow animation in a short time.	Directable anime-like shadow based on water mapping filter	NA:NA:NA	2009
Ryo Takamizawa:Takanori Suzuki:Hiroyuki Kubo:Akinobu Maejima:Shigeo Morishima	MoCap-based facial expression synthesis techniques have been applied to provide CG character with expressive and accurate facial expressions [Deng et al. 2006: Lau et al. 2007]. The representative performance of these techniques depends on the variety of captured facial expressions. It is also difficult to guess what expressions are needed to synthesize expressive face before capture. Therefore, much MoCap data are required to construct a subspace employing dimensional compression techniques, and then the space enables us to synthesize expressions with linear-combination of basis vectors of the space. However, it is hard work to take much facial MoCap data to obtain expressive result.	Expressive facial subspace construction from key face selection	NA:NA:NA:NA:NA	2009
James D. Edge:Adrian Hilton:Philip Jackson	We describe a method for the synthesis of visual speech movements using a hybrid unit selection/model-based approach. Speech lip movements are captured using a 3D stereo face capture system and split up into phonetic units. A dynamic parameterisation of this data is constructed which maintains the relationship between lip shapes and velocities; within this parameterisation a model of how lips move is built and is used in the animation of visual speech movements from speech audio input. The mapping from audio parameters to lip movements is disambiguated by selecting only the most similar stored phonetic units to the target utterance during synthesis. By combining properties of model-based synthesis (e.g., HMMs, neural nets) with unit selection we improve the quality of our speech synthesis.	Model-based synthesis of visual speech movements from 3D video	NA:NA:NA	2009
Hiroto Yarimizu:Yasushi Ishibashi:Hiroyuki Kubo:Akinobu Maejima:Shigeo Morishima	Muscle-based facial animation [Lee et al. 1995] is one of the best approaches to realize facial expressions of characters. However, this approach does not consider the personal variation in facial tissue model such as skin thickness. So personal character in emotional expression can not be reflected in this model.	Muscle-based facial animation considering fat layer structure captured by MRI	NA:NA:NA:NA:NA	2009
Takashi Tokizaki:Yuuichi Tazaki:Hironori Mitake:Shoichi Hasegawa	Interactive applications such as Video Games require characters, which generate motions corresponding to user's interaction. Motion capture is an effective technique to reproduce realistic motion. However, to produce a motion which is appropriate to the operation of the user, a lot of motions must be prepared and one of the motions which is suitable for the user's operation must be selected and played. Because the user's operation changes the motion trajectory, unexpected contact to objects may happen. The amount of change on a trajectory depends on not only the trajectory of motion but also internal tensions of skeletal muscles - co-contraction level, when a person put one's hand down on a table or collides with an object. [Hogan 1984] proposed that reaching motion of human is supposed to be generated by spring damper characteristics of muscles dragging to the virtual trajectory. Human controls not only trajectories of motions but also spring-damper characteristics of muscles by changing co-contraction levels. Realistic character motions contacting to objects can be generated easily with virtual trajectory tracking control which is integrated to physics engines for character motions.	Pliant motion: integration of virtual trajectory control into LCP based physics engines	NA:NA:NA:NA	2009
D. Kravtsov:O. Fryazinov:V. Adzhiev:A. Pasko:P. Comninos	The modern world of computer graphics is mostly dominated by polygonal models. Due to their scalability and ease of rendering such models have various applications in a wide range of fields. Unfortunately some shape modelling and animation problems can hardly be overcome using polygonal models only. For example, dramatic changes of the shape (involving change of topology) or metamorphosis between different shapes can not be performed easily. The Function Representation (FRep) [Pasko et al. 1995] allows us to overcome some of the problems and simplify the process of the major model modification. Our system is based on a hybrid modelling concept, where polygonal and FRep models are combined together and can be evaluated in near-real or real time. It allows us to: • produce animations involving dramatic changes of the shape (e.g. metamorphosis, viscoelastic behaviour, character modifications etc) in short times (Fig. 1) • interactively create complex shapes with changing topology (Fig. 2) and specified level of detail (LOD) • integrate existing animated polygonal models and FRep models within a single model	Polygonal-functional hybrids for computer animation and games	NA:NA:NA:NA:NA	2009
Jianfeng Xu:Haruhisa Kato:Akio Yoneyama	This poster presents a motion retrieval algorithm, which searches the motions in the same category as a query's (known as logically similar motions) in a motion capture database. The challenge is that logically similar motions may not be numerically similar due to the motion variations [Müller et al. 2005]. In this poster, we propose a novel short-term feature that extracts both symbolized representation and continuous features from joint velocities in a motion clip, which is employed to effectively retrieve logically similar motions to the query. Although symbolized representation of human motion has been studied [Müller et al. 2005], our approach is different in that we consider temporal correlation instead of Müller's spatial relationship. Moreover, not only symbolized representation (dynamic pattern) but also continuous features (average speed) are extracted in our short-term feature. Furthermore, our method is more friendly to novices as it requires no prior knowledge to determine features. Our experiments demonstrate that our algorithm greatly improves the performance compared to two conventional methods.	Retrieval of motion capture data based on short-term feature extraction	NA:NA:NA	2009
Katsutoki Hamana:Hiroshi Mori:Atsushi Nakano:Junichi Hoshino	In recent years, many entertainment systems have relied on the progress of interaction technology to create characters that act autonomously. To show these lifelike characters, it is important for them to perform various actions, such as daily actions, reflex actions that require reacting to input from a user, perceiving actions where the character perceives an object and reacts to it, and actions based on personalities or feelings. This results in the problem of complex action planning. A character has to carry out the actions listed, keep schedules and maintain a personality, and react flexibly to user interaction, while still maintaining story flow. In this paper, we propose the Story Engine, which can execute various actions in multiple characters.	Story engine for interactive characters	NA:NA:NA:NA	2009
Evan Tice:Tim Tregubov:Kate Schnippering:Yoon-Ki Park:Ray diCiaccio:Max Friedman:Jennifer Huang:Justin Slick:Giulia Siccardo:Jessica Glago:Stephanie Trudeau:Daniel Gobaud:Daniel Garcia:Craig Slagel:Lorie Loeb	With glaciers melting, sea levels rising and natural disasters---such as hurricanes and cyclones---intensifying, climate change is a growing concern. While innovations in renewable energy are critical, research shows that changing energy use behavior has become increasingly important in the fight against global warming. GreenLite Dartmouth focuses on changing behavior by making energy conservation a priority for students by creating both an intellectual and emotional connection between daily actions and their adverse effects on the environment. We combine computer graphics, art, engineering, sociology, environmental science, systems-thinking and behavioral psychology to turn real-time energy use data into a meaningful interactive display. GreenLite employs innovative methods for displaying complex data using interactivity, storytelling, animation, competition and goal-setting. Appealing animated information-display and "mood" algorithms put data into context to make it meaningful. We incorporate a system of digital energy meters, a custom database, computational analysis, 2D and 3D animations, interactive design and a game-engine to spur behavior change and, hopefully, reverse the course of climate change.	GreenLite Dartmouth: unplug or the polar bear gets it	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2009
Saba Hashem Kawas	With today's increased interest in advanced digital networking and social online communities, many higher educational institutes have been exploring online three-dimensional virtual environments as a new medium for distance learning. These technologies provide multiple features for online interaction and collaboration, a visual cyberspace for text and voice communication, the ability to manipulate multimedia in real time with a group of individuals, and the opportunity to link users to Web sites.	H-link 3D: hyper-learning interface and navigational toolkit in 3D virtual worlds experimental interface design for cobalt, a Croquet metaverse	NA	2009
Akira Nakayasu:Kiyoshi Tomimatsu	Himawari is a sunflower robot composed of mechanical parts and electronic parts, such as servo motors, LEDs and shape-memory alloy actuators, that reacts, slowly and fluidly, facing and communicating to humans in front of it.	Himawari: a plant robot	NA:NA	2009
Benjamin Raynal:Xavier Gouchet:Venceslas Biri:Vincent Nozick	Designing a program that is not a tool of artistic creation, but a creator itself have been a real challenge for both digital artists and researchers. The most famous program of artistic creation is AARON [Cohen], which is in continual development since 1973 by its creator, Harold Cohen. Unfortunately, AARON cannot learn new styles or imagery by its own, each new capability must be hand-coded by Harold Cohen. Roxame [Berger], another artistic creation program created in 2001 by Pierre Berger, is based on artificial intelligence, and have its own style, emerging from both the artistic preferences of the user, and a stochastic process. This style can evolve and is refined at each work.	IArtist: a self learning computer artist	NA:NA:NA:NA	2009
Takuji Narumi:Tomohiro Akagawa:Young Ah Seong:Michitaka Hirose	To change the spatial structures of our living spaces, we usually take an architectural approach. However once such structures have been constructed, re-configuring them incurs substantial costs. Reserving the flexibility to change spatial design is essential for the effective use of spaces. Our research aims to make spatial design flexible. New characteristics are added to an existing space using information technology, so the relationship between the space and people within changes.	Thermotaxis	NA:NA:NA:NA	2009
Max Grosse:Gordon Wetzstein:Oliver Bimber:Anselm Grundhöfer	With adaptive coded aperture projection, we present solutions for taking projectors to the next level. By placing a programmable liquid crystal array at a projectors aperture plane we show how the depth of field (DOF) of a projection can be greatly enhanced. This allows focussed imagery to be shown on complex screens with varying distances to the projectors focal plane, such as projection domes as in planetariums or cylindrical canvases as in IMAX theaters. We demonstrate that adaptive apertures outperform previous methods of projector defocus compensation for objective lenses with static apertures. In addition, our adaptive apertures can perform the type of temporal contrast enhancement employed by common auto-iris projection lenses, and also produce high-quality depixelated images. The latter is beneficial for close-view displays with limited resolution, such as rear-projected TV sets.	Adaptive coded aperture projection	NA:NA:NA:NA	2009
Martha Carrer:Cruz Gabriel	The objective of this paper/presentation is to describe the potentialities of Mobile Tagging as a tool for increasing and spreading the effects of Mixed Realities. In this sense, we will start introducing the main concepts and some examples of Mixed Realities followed by the concepts and examples of Mobile Tagging, showing that they are connected and benefit each other.	Mobile tagging and mixed realities	NA:NA	2009
Norihiro Nakamura:Yoshiyuki Kokojima:Yasunobu Yamauchi	Resolution-independent rendering is important for many applications such as text rendering and rendering vector objects. This theme has attracted interest in recent years owing to the growing popularity of Flash and SVG-based applications [Loop and Blinn 2005]. We had previously presented a fast rendering method using a stencil buffer for deformable vector objects [Kokojima et al.]. One of the advantages of this method is that retriangulation is unnecessary when vector objects deform interactively. However, [Kokojima et al.] only deal with rendering vector objects on a flat surface.	Rendering of vector objects on curved surface using pivot triangle primitives	NA:NA:NA	2009
Takehito Teraguchi:Hiromasa Yamashita:Ken Masamune:Takeyoshi Dohi:Hongen Liao	Three-dimensional (3-D) displays have got a lot of attention because they have a much higher sense of realism and are more intuitive than 2-D displays. In particular, autostereoscopic displays are suitable for everyday use because they can be observed from an arbitrary viewpoint without supplementary glasses or tracking devices. Integral Videography (IV) is one of the methods for autostereoscopic animated images that extends Integral Photography (IP) to animation. IP/IV uses a combination of a lens array and a number of calculated elemental images with different perspectives. In particular, IV with a depth of several meters can be applied in many areas. However, most IV reports have an image depth of only several centimeters. Only a small deviation of a lens from its designed position would result in several degrees of deviation of the light ray from the back of the lens. We have developed the static autostereoscopic image by projecting the light sources from an object onto a photographic film through the lens-array [Liao et al. 2005]. In this study, we obtained animated IV of 1 m image depth with less distortion using our method to correct IV images. The method is technically unique.	Three-dimensional auto-stereoscopic animated image with a long viewing distance using high-precision image correction	NA:NA:NA:NA:NA	2009
Ilya Rosenberg:Ken Perlin:Charles Hendee:Alex Grau:Nadim Awad	Multi-touch input has been an active area of research for over two decades but has always suffered from the absence of an easily available high quality touch input device. For this reason, exciting user interfaces developed in the lab have appeared on CNN, but not on everyone's desk, computer screens, table-tops, walls and floors. What has been needed - and lacking - is a better mousetrap; an inexpensive, flexible and sensitive touch imaging technology.	The UnMousePad: the future of touch sensing	NA:NA:NA:NA:NA	2009
Wei-Chung Cheng:Jih-Fon Huang	This work demonstrates a novel channel for a smart display to interact with its user for enhancing the viewing experience. By using a wearable electro-oculography (EOG) circuit, the saccadic eye movements can be detected so that the user's viewing mode can be determined. Different gamut settings are used in the "fixation mode" versus "saccade mode," such that the fast eye movement induced artifacts can be suppressed. Furthermore, by analyzing the saccade patterns, we can determine the user is in the "image viewing" mode or "text reading" mode. Then different brightness, contrast, saturation settings can be assigned accordingly and automatically to improve the user's comfort level.	A saccade-contingent display for suppressing color breakup	NA:NA	2009
Jussi Huhtala:Ari-Heikki Sarjanoja:Jani Mäntyjärvi:Minna Isomursu:Jonna Häkkilä	Mobile devices have limitations compared to PCs due to their inferior computing power and small screens, but a successful design of animated transitions can hide processing delays and make the user experience smoother. In this paper, we describe the design of animated transitions and present a user study on how they are perceived.	Mobile screen transition animations	NA:NA:NA:NA:NA	2009
Danny Rado:Daniel F. Keefe	Despite the many challenges understanding how best to interact with large format displays, they are becoming increasingly popular for data analysis tasks in a variety of domains, including scientific, information, and geo-visualization. (Figure 1a shows a relatively small, 60" display; even larger, wall-size displays are also popular.) In order to make the most effective use of the full display, users typically stand and walk around in these environments. In fact, this physical navigation has been shown to be beneficial in data analysis tasks [1]. Since immobile input devices, such as mice, keyboards, or pen-tablets, do not naturally support interaction "on the move", new interactive techniques are needed to facilitate fluid interaction across a range of distances when working with large-format displays. We believe body-centric 3D, gestural input is particularly promising in this regard. Our work investigates techniques for reliable menu selection based upon these ideas, introducing new 3D input strategies for controlling menus. Our work builds upon previous techniques, such as rapMenu [3], which uses rotational hand movements and finger pinches to control menus from a distance.	rAir flow menus: toward reliable 3D gestural input for radial marking menus	NA:NA	2009
Kensuke Takada:Kyoko Higurashi:Tatsuhiko Suzuki:Misako Ota:Tetsuaki Baba:Kumiko Kushiyama	"Thermo-Pict" is a design apparatus produced by applying temperature visualization technology linked to an information display with the use of a thermograph sheet. Thermography is used to visualize the surface temperature of objects through their depiction as colors. This technology has been used primarily in the medical and research fields. Thermography display colors come in a wide range of hues and brightness that enables quick visualization of any object's surface temperature distribution. Use of this technology will be attempted as a tool in the production of design displays. [Fig. 1]	Temperature design display device to use peltier elements and liquid crystal thermograph sheet "Thermo-Pict"	NA:NA:NA:NA:NA:NA	2009
Celambarasan Ramasamy:Donald H. House:Andrew T. Duchowski:Brian Daugherty	This poster will analyze the feasibility of eye tracking as a tool for helping filmmakers to make decisions in a stereoscopic film production. In a conventional dialogue driven shot it is fairly easy to predict where the audiences would be looking. However, for visually complex shots it is not so obvious. In this case, eye tracking can be used as a tool to observe the gaze pattern of the audience to identify the regions of interest in the frame. This information could be used to budget the resources for the shot. It can also be used to identify elements that distract the audience from the flow of the movie. This technique could be used to help filmmakers to make more informed decisions during the film making process. We analyzed a student produced stereoscopic film using this technique. In our study, a number of subjects were asked to watch the film and their gaze data was recorded.	Using eye tracking to analyze stereoscopic filmmaking	NA:NA:NA:NA	2009
Yi-Ting Cheng:Virginia Tzeng:Yu Liang:Chuan-Chang Wang:Bing-Yu Chen:Yung-Yu Chuang:Ming Ouhyoung	The development in digital technologies and the widespread Web 2.0 concept have made many digital videos accessible. Editing and modifying digital videos have become an interesting and important topic. In this paper, we present a system for face replacement in video. Most digital processing software can perform face replacement only when the poses for the source and target faces are similar, and the manipulation process with those software is often time-consuming and labor-intensive. While previous work [Blanz et al. 2004] focuses on image face replacement, our system performs face replacement in video by constructing 3D models for both target and source faces and swapping them accordingly. 3D face models are created by fitting 3D morphable models [Blanz et al. 1999] and the input is reduced to two pictures for the face to be placed in.	3D-model-based face replacement in video	NA:NA:NA:NA:NA:NA:NA	2009
Che-Hua Yeh:Pei-Ruu Shih:Yin-Tzu Lin:Kuan-Ting Liu:Huang-Ming Chang:Ming Ouhyoung	This poster presents experimental results of three face recognition methods -- Support Vector Machine (SVM), Local Binary Pattern (LBP)-based, and Sparse Represented-based Classification (SRC). We will show the experimental results based on AR face database and on home photos. The experiments show that the three algorithms can achieve over 85% recognition rate in AR database. However, the recognition rate is extremely reduced in home photos. SVM and SRC-based method encounter challenges of selecting training model while LBP-based method encounters the challenge of merging over scattered clusters. Our goal is to improve the accuracy and efficiency especially in home photos based on the three methods.	A comparison of three methods of face recognition for home photos	NA:NA:NA:NA:NA:NA	2009
Jörn Loviscach	All of today's digital cameras record the date and time at which a photo has been taken; some cameras also record the geographical position. This work proposes yet another augmentation: to record temperatures at different spots picked by the user in the image. This has many applications for both family life and professional engineering: Was the water in the swimming pool heated? Was last Saturday night's party fever really a fever? On the serious side, an engineer may record the temperature of different chips on a printed circuit board or document heat loss due to bad building insulation.	Augmenting a camera with a thermometer	NA	2009
Yuji Morimoto:Yuichi Taguchi:Takeshi Naemura	Colorization is the process of adding color to monochrome images and video. It is used to increase the visual appeal of images such as old black and white photos, classic movies, and scientific visualizations. Since colorizing grayscale images involves assigning three-dimensional (RGB) pixel values to an image whose elements are characterized by one feature (luminance) only, the colorization problem does not have a unique solution. Hence, human interaction is typically required in the colorization process. Although existing colorization methods attempt to minimize the amount of user intervention, they require users to manually sellect a similar image to the target image or input a set of color seeds for different regions of the target image. In this paper, we present an entirely automatic colorization method using multiple images collected from the Web. The method generates various and natural colorized images from an input monochrome image by using the information of the scene structure.	Automatic colorization of grayscale images using multiple images on the web	NA:NA:NA	2009
Tongbo Chen:Abhijeet Ghosh:Paul Debevec	Separation of the diffuse and specular components of observed reflectance has been an active area of research in computer graphics and vision, with major applications in reflectance modeling and scene analysis. Traditionally, researchers have investigated diffusespecular separation under point or directional illumination conditions while employing polarization and, in the case of dielectric materials, color space analysis techniques. Recently, Ma et al. [2007] introduced a technique for estimating high quality diffuse and specular normals and albedo maps (see Fig. 1, (a) & (d)) of a specular object using polarized spherical gradient illumination. However, the employed polarization technique imposes view-point restriction, and results in insufficient light levels for performance capture with high speed acquisition. Hence, in this work, we look into an alternate diffuse-specular separation technique for spherical gradients based on a data-driven reflectance model. Traditional separation techniques based on color space analysis focus on removing specular reflections from the observation for scene analysis [Mallick et al. 2005]. In contrast, we focus on obtaining high quality estimates of both the diffuse and the specular reflectance components.	Data-driven diffuse-specular separation of spherical gradient illumination	NA:NA:NA	2009
Mary Hudachek-Buswell:Catherine Matos:Michael Stewart	In this presentation, the restoration of images blurred by atmospheric turbulence is examined. The proposal uses a new class of approximations to blurring operators representing Gaussian blur. The Toeplitz matrix representing the blur is transformed into a Cauchy-like (CL) matrix using the FFT. In addition to the CL structure, the transformed matrix has a rank structure. In particular, the off-diagonal blocks have low rank. This class of matrices can be approximated quickly, and the structure can be exploited for fast image restoration.	Deblurring with rank-structured inverse approximations	NA:NA:NA	2009
Nari Kim:Jong-Chul Yoon:In-Kwon Lee	In this work, we present an image based virtual dress up system according to user input model and garment image. At 'Registration' step, we asked the user manually setting the skeleton structure and matte out the alphamap from the image. Next step, our method automatically deforms the garment image corresponding to model's body. For the boundary fitting, our method uniformly sampled contour points and solves the optimization function. To enhance the more realistic scene, we reconstruct the 2D mesh to the 3D mesh according to a human's standard body shape. For the lighting effect we estimate the light position by using luminance value with the detected face region. Previous 3D scanner based virtual dress up system has expensive cost and under locational limitation issues, but our system integrates various image processing techniques and introduced an easy-to-use system for the general users. We present that our system produces a visually plausible and well-fitted virtual dress up results in a practical and usable way.	Image-based dress up system	NA:NA:NA	2009
Yingen Xiong:Xianglin Wang:Marius Tico:Chia-Kai Liang:Kari Pulli	We present the design and implementation of a mobile imaging system for high resolution panoramic image creation. The system comprises the following components: automatic camera motion tracking and high resolution image capturing, image registration on spherical manifold, image warping, image labeling, and image blending.	Panoramic imaging system for mobile devices	NA:NA:NA:NA:NA	2009
Kei Utsugi:Takuma Shibahara:Takafumi Koike:Takeshi Naemura	Seam carving is an image processing operator for content-aware image resizing [Avidan and Shamir 2007]. It generates an energy map from gradient intensity of pixels and searches for seams, which are vertical or horizontal continuous paths of pixels that run through local minimum energy areas. Removing or inserting pixels along a seam enables users to shrink or enlarge pictures by a wide range, while still retaining all details of the image.	Proportional constraint for seam carving	NA:NA:NA:NA	2009
Susan M. Munn:Jeff B. Pelz	Our portable video-based monocular eye tracker contains a headgear with two cameras that capture videos of the observer's right eye and the scene from the observer's perspective (Figure 1a). With this eye tracker, we typically obtain a position -- that represents the observer's point of regard (POR) -- in each frame of the scene video (Figure 1b without bottom left box). These POR positions are in the image coordinate system of the scene camera, which moves with the observer's head. Therefore, these POR positions do not tell us where the person is looking in an exocentric reference frame. Currently, the videos are analyzed manually by examining each frame. In short, we aim to automatically determine how long the observer spends fixating specific objects in the scene and in what order these objects are fixated.	Ray tracing to get 3D fixations on VOIs from portable eye tracker videos	NA:NA	2009
Naoki Kawai	Vector plot is a frequently used method for illustrating vector fields used in applications such as scientific visualization. Although the method is easy to implement and the resulting image captures the original vector field well, the streamlines are often positioned too closely or too sparsely to one another due to sources and sinks of the original vector field. This results in unevenness of visual density over the entire region, and some previous researches have treated the problem. Mebarki et al [1] proposed the improved strategy that the maximum vacant region should be given priority for a new streamline, but the results still lack uniformity. Other related works [2][3] suggested that both tapering streamlines and controlling intensity improve the visual uniformity of streamlines. We propose another approach for making streamlines look uniform with dotted and broken lines instead of tapering or intensity control. The results are binary images and consist of fixed width streamlines which preserve uniformity.	Uniform looking vector plot with streamline fragmentation	NA	2009
Chung-Lin Wen:Yu-Ting Wong:Bing-Yu Chen:Yoichi Sato	In this extended abstract, we propose a novel approach for video segmentation by utilizing motion information. Recently, graph-cutbased segmentation methods became popular in this domain but most of them dealt with color information only. Those methods possibly fail if there are regions similar in color between foreground and background. Unfortunately, it is usually hard to avoid, especially when objects are filmed under a natural environment. For instance, Figure 1(a) shows a result of graph cut with a small smoothness weighting, and hence some background regions are incorrectly labeled. On the contrary, if a larger smoothness weighting is used, some background regions near the foreground will be merged as shown in Figure 1(b). To improve those drawbacks, we propose a method based on both of color and motion information to conduct the segmentation. The method is useful because foreground and background usually have different motion patterns as shown in Figure 1(c).	Video segmentation with motion smoothness	NA:NA:NA:NA	2009
Alice Boit:Thomas Geimer:Jörn Loviscach	Graphical passwords [Suo et al. 2005] address vital problems of textual passwords: Users pick from a limited vocabulary; machine-generated passwords are hard to memorize. Graphical input, however, faces "shoulder surfing," as bystanders can watch the screen. Current solutions to this problem tend to impose high cognitive loads. We propose an easy-to-handle approach.	A random cursor matrix to hide graphical password input	NA:NA:NA	2009
Alexandros Zotos:Katerina Mania:Nick Mourkoussis	In order to economize on rendering computation, selective rendering guides high level of detail to specific regions of a synthetic scene and lower quality to the remaining scene, without compromising the level of information transmitted. Scene regions that have been rendered in low and high quality can be combined to form one complete scene. We propose a novel selective rendering approach which is task and gaze-independent, simulating cognitive creation of spatial hypotheses. Scene objects are rendered in varying quality (polygon count) according to how they are associated with the context (schema) of the scene.	A selective rendering algorithm based on memory schemas	NA:NA:NA	2009
Matthew Hirsch:Douglas Lanman:Ramesh Raskar:Henry Holtzman	We present a BiDirectional screen capable of both imaging and display, that uses an LCD as a spatial light modulator to support seamless transition from on-screen multi-touch interactions to off-screen hover-based gestures.	BiDi screen: depth and lighting aware interaction and display	NA:NA:NA:NA	2009
Jinha Lee:Yasuaki Kakehi:Takeshi Naemura	In this paper, we propose a novel block-shaped tangible interface named Bloxel (see Figure 1). A Bloxel is a translucent cubical block that glows in full color and communicates with the neighboring Bloxels through high-speed flickers.	Bloxels: glowing blocks as volumetric pixels	NA:NA:NA	2009
André Maximo:Maria Paula Saba:Luiz Velho	Introduction and Related Work Tabletop and tangible interfaces have become common in recent years. Technology trends in this area can be found in commercial products, such as Apple's iPhone™ and Microsoft Surface™, as well as in research ventures, such as Reactable and Perceptive Pixel initiatives. Nevertheless, natural human computer interfaces (HCI) to support this hardware technology are still non-intuitive.	collecTable: a natural interface for music collections	NA:NA:NA	2009
Andrew Bragdon:David H. Laidlaw	We explore the design of a multi-view interaction metaphor for 3D visualization in the CAVE. We then present the results of a formative evaluation of a "Wizard of Oz" [Kelley 1984] prototype. Although there has been significant prior work on 2D and 3D desktop applications utilizing multiple views, little prior work exists for multi-view systems in immersive virtual environments such as the CAVE, despite the clear advantages enjoyed by desktop analogues. Immersive 3D environments pose unique challenges for such a system. Since the contents of such views are themselves 3D, it is unclear whether users will be able to easily read views independently of one another, as in a naive implementation they might become intermingled; even in a system that is conscious of this problem, some vantage points may cause depth ambiguity problems which make it difficult to read each view. In addition, interaction techniques for controlling and managing such views must be explored. Thus, formative empirical testing is warranted to determine the viability of such a system.	The design and evaluation of a lightweight multi-view interaction metaphor for 3D visualization in the CAVE	NA:NA	2009
Mi Sun Lee:Mi-Gi Han:Joo-Youn Park:Su-e Park	Online spaces are being transformed into new social spaces with a variety of interpersonal relationships and social activities. Especially, cyber spaces based on three dimensions show various cross-cultural social relationships and activities compared with cyber spaces based on two dimensions. These phenomena have different characteristics, depending on users' cultural backgrounds. Relating to social issues in online spaces, many preliminary studies have been conducted. Especially, impressions have been considered important subjects related with social networks. In spite of that, sufficient cross-cultural research related with impressions in online spaces has not been conducted, especially based on 3-D cyber spaces. Therefore, the main goals of this study were to extract 3-D cyber factors formatting perceptional impressions and compare those factors based on cultural differences. In the preliminary research, we identified six impressions dimensions in 3-D cyber space: F1.Cheerful, F2.Logical, F3.Violent, F4.Selfish, F5.Warm, and F6.Seclusive (Lee, Kim & Park, 2009). In order to achieve our goal, first, we selected two countries considering Hofstede's culture dimensions (e.g. Power Distance, Individualism versus Collectivism, Masculinity versus Femininity, Uncertainty Avoidance) (Hofstede, 2005). Korea and America have very different cultural characteristics in terms of Hofstede's culture dimensions (Hofstede & Bond, 1984). Secondly, we conducted in-depth individual interviews. For these interviews, we recruited interviewees as actual users of 3-D cyber spaces (Second Life); depending on the frequency uses and interpersonal relations contained therein, we selected eight Korean participants and eight American participants. Before conducting interviews, we recorded normal lives of participants within a three-day span, for two hours of each day. Then, we conducted the survey to each participant seeing the video clips of others' virtual lives for the purpose of analyzing others' preserved impressions. In-depth interviews were conducted in 3-D cyber space using actual voices. The interview consisted of two parts of questions: 1) What are the factors relating with your perceived impressions?; and 2) If you help an avatar on the video clip before you saw to make clear his/her impression, how will you help? All interviews were recorded as video and audio clips. After collecting data, we analyzed data based on Grounded theory (Strauss, 1990) recognized qualitative research methods. First of all, we accurately transcribed all voice data to text data, and then separated data to minimal units of meaning considering interviewees' intentions. Finally, we extracted properties and grouped properties during axial coding. As a result, Factors formatting perceptional impression in 3-D cyber space was derived with distinction by Korean and American users. These derived factors were linguistic, visual, behavioral, relational, inner-environmental, and outer-environment. Of these, linguistic factors (106, 43%) and behavioral factors (57, 23%) were the most derived. Further, looking at the visual factors, the number of derived factors was similar among Korean and American users. Alternatively, we looked at the detailed factors derived with distinction by Korean and American users. The factors derived by Korean users included exposure degree of clothes, thickness of clothes, while the factors derived by American users included color of clothes and types of avatar. In conclusion, this study has theoretical and empirical significance. The theoretical significance, through the cultural differences research, is to understand how each intercultural impression provided role elements in Korea cultures and American cultures and to understand how the impression provided difference elements. Therefore, more extensive future research on the dimensions of the intercultural impression formation mechanism was proposed, based on this study. The empirical significance was to offer impression dimensions-related elements in 3D gaming to developers and designers in the development of related systems; furthermore, as the results provide data of how elements affect impression in intercultural perception and how in each dimension, the system will be able to provide a basis about impression formation elements in intercultural context.	Factors formatting perceptional impression in 3-D cyber spaces: a cross-cultural study of Korean and American users	NA:NA:NA:NA	2009
Piotr Dalka:Andrzej Czyzewski	The main goal of each HCI application is to make working with a computer as natural, intuitive and effective as possible. One of the main areas of applications of new human-computer interfaces is making possible to use computers for people with permanent or temporal motor disabilities in an efficient way. There are two main types of such solutions [Aggarwal and Cai 1999]. The first group utilizes devices mounted directly on the user's body. Applications in the second group are contactless and they use remote sensors only, therefore they are much more comfortable for a user. Amongst contactless solutions, vision-based human-computer interfaces are the most promising ones. They utilize cameras and image processing algorithms to detect signs and gestures made by a user and execute configured actions. The most common vision-based applications employ eye and hand tracking [Shin and Chun 2007].	LipMouse: novel multimodal human-computer interaction interface	NA:NA	2009
Fumitaka Ozaki:Takuo Imbe:Shin Kiyasu:Yuta Sugiura:Yusuke Mizukami:Shuichi Ishibashi:Maki Sugimoto:Masahiko Inami:Adrian D. Cheok:Naohito Okude:Masahiko Inakage	MYGLOBE is an interactive map media which allows us to share our cognitive maps. This map grows up with our own activities and shows our subjective view of the city by emphasizing roads or landmarks frequently used. Users can bring up their own city in the device by actually walking in the city, and also share their own maps with each other and discover unknown places. Present map services such as Google maps and Google Earth, provide mash-up tools which allow us to create our own favorite place on the map easily. We can use hand held GPS devices to make our own travel route and navigate to destination places. MYGLOBE allows us to not only tag their favorite places on the map but also change the shape of the map itself. Instead of an accurate geographic map, MYGLOBE provides maps reflecting the user's individual experiments and the view of the city. It can also be used as a communication tool to share the life history with your friends. MYGLOBE will enhance your city experience.	MYGLOBE: cognitive map as communication media	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2009
Duksu Kim:Jae-Pil Heo:Sung-eui Yoon	Collision detection between deformable models is one of fundamental tools of various applications including games. Collision detection can be classified into two categories: discrete and continuous collision detection methods. Discrete collision detection (DCD) has been demonstrated to show the interactive performance by using bounding volume hierarchies (BVHs). However, some colliding primitives may be missed since DCD methods find intersecting primitives only at discrete time steps. This issue can be a very serious problem in physical based simulation, CAD/CAM applications and etc. On the other hand, continuous collision detection (CCD) identifies the first time of contact of colliding primitives during a time interval between two discrete time steps.	PCCD: parallel continuous collision detection	NA:NA:NA	2009
Sho Kamuro:Kouta Minamizawa:Naoki Kawakami:Susumu Tachi	We propose a pen-shaped handheld haptic display that allows haptic interactions with virtual environments by generating kinesthetic sensations on the user's fingers; the user's movements are not restricted since the device does not have mechanical linkages. Unlike conventional haptic displays that provide vibrations, which are not representative of tactile sensation, our proposed device, named "Pen de Touch" (Figure 1), provides kinesthetic sensations to the muscles in the user's fingers.	Pen de Touch	NA:NA:NA:NA	2009
Toki Takeda:Kazuhiko Yamamoto:Reiji Tsuruno:Taketoshi Ushiama	The recent popularity of small mobile devices such as cellular phones, digital cameras, and game device has made it increasingly convenient to carry them with us as we go about our daily lives. However, the increased miniaturization and functionality range of these devices can make it hard to access and utilize their contents. For instance, when using a device with a small display screen, it is often impossible to display an entire area of interest in a single view and the existence of numerous buttons can make it difficult to manipulate the device. In this study, we propose a novel interface that allows intuitive browsing of the content of mobile devices using an actual map.	Actual map based interface for browsing content on mobile devices	NA:NA:NA:NA	2009
Christian Schulze:Laurens Nienhaus:Jörn Loviscach	Painting in a 3D geobrowser is interesting both for artistic uses such as virtual graffiti and for commercial applications such as architectural sketches. It seems straightforward to turn Google Earth into a 3D painting tool: Store the 3D mouse data that the software returns and construct polylines from them. However, this approach has two vexing drawbacks: First, when the user paints past an edge, the end of the stroke will be placed at an incorrect depth, see Figure 1; second, it is not possible to sketch in mid-air, for instance to indicate a planned height extension of a building.	Sketch-based annotations in Google Earth	NA:NA:NA	2009
Andrzej Czyzewski:Piotr Odya:Agnieszka Grabkowska:Michal Grabkowski:Bozena Kostek	Dyslexia (dysgraphia) therapy is often boring for children and, what even worse, its results can be unsatisfactory. Hence, many therapists insist on development new methods which would be more interesting for young patients. The Smart Pen is such a tool. It is designed for supporting the therapy of developmental dyslexia, with particular regard to dysgraphia.	Smart pen: new multimodal computer control tool for dyslexia therapy	NA:NA:NA:NA:NA	2009
Yasuko Hayashi:Kensei Jo:Yasuaki Kakehi:Takeshi Naemura	Generally, people use a keyboards as an interface for text input. However, unlike handwritten characters, typed characters are identical no matter who types them. To create variations in the appearance of typed characters, we usually decorate characters by changing fonts. For example, we change the font size, font color and boldness of the characters and the spacing between characters. However, due to these decorations, people must handle a few input processes such as checking the select menu. To improve current conditions, some research suggests methods of applying the user's unconscious actions while typing to decorate characters, using a laptop with a built-in acceleration sensor or body-worn electronic equipment [Iwasaki et al. 2009][Wang et al. 2004][TypeTrace 2006].	TypeTile: a keyboard system that decorates characters depending on the way of typing	NA:NA:NA:NA	2009
Kai-Yin Cheng:Ko-Yuan Chou:Sheng-Jie Luo:Bing-Yu Chen	Due to the development in digital technologies, people now can easily retain their valuable memory by taking pictures through digital cameras. The cheap digital storage also encourages people to take lots of photos as they want. However, due to the tremendous amount of digital photos, it is not easy for people to browse all of them. Therefore, some techniques are proposed to help people to enjoy the photos, although it may be difficult for some people to arrange a time slot to watch them intentionally. Hence, in this extended abstract, we propose a system, which can utilize the large number of photos as the program themes (background), so that people will not notice the synthesized background while they are working, but the program themes may still be able to remind their good memory when they taking a short rest.	Utilizing photos as program themes	NA:NA:NA:NA	2009
Tatsuya Ishikawa:Shoichi Hasegawa	Recently, hobby robots such as pet robots and humanoid robots for entertainment are spreading and becoming more and more familiar each day. Compared to robots such as HONDA's ASIMO, a hobby robot is much cheaper, less rigid and has far less precision in measuring and controlling the angle of its own joints. For this reason, although we can assign joint angle to a key frame, assigned posture cannot be taken like computer graphics. Therefore, to ensure the robot moves as desired, we need to actually look at the robot operating while adjusting key frames respectively. By this, the margin of error in the joint angle and distortion in mechanism can be avoided. For CG animation, the animator observes the animation in real-time and when a problem is encountered, the problem in the key frame is corrected by slowing the animation down or by examining each frame. However, with robots, sudden stopping of ambulatory action makes the robot fall down. Moreover, the error in joint angle and distortion in mechanism are different between operation and geostationary state because of dynamic influences.	Virtual stroboscope for robot motion design	NA:NA	2009
Stephan Wenger:Marcus Magnor:Christophe Morisset:Wolfgang Steffen	Distant astrophysical objects like planetary nebulae can normally only be observed from a single point of view, which makes deducing plausible 3D models a hard task that usually involves a lot of manual work [Nadeau et al. 2001]. However, additional physical assumptions can be used in order to estimate the missing depth information. In previous work [Wenger et al. 2009], a certain axial symmetry was assumed which is present in many planetary nebulae, so that tomographic methods could be used for the reconstruction. However, this assumption obviously fails for many of the most complex and interesting objects in question, and it only leads to unambiguous results as long as no absorption occurs within the nebula.	3D reconstruction of planetary nebulae using hybrid models	NA:NA:NA:NA	2009
Hiroki Fujishiro:Takanori Suzuki:Shinya Nakano:Akinobu Mejima:Shigeo Morishima	Everyone is interested in being more attractive. Leyvand et al have been proposed the method which can enhances of facial attractiveness into a photograph [Leyvand et al. 2008]. However, their method can't synthesize more attractive facial expression than that of an input face photograph. Meanwhile, amateur subjects' facial expressions often become unnatural when they act in front of the camera in experimental environments. At such a situation, a natural expression can be synthesized without performance skills.	A natural smile synthesis from an artificial smile	NA:NA:NA:NA:NA	2009
Kentaro Yamanaka:Shinsuke Nakamura:Shota Kobayashi:Akane Yano:Masashi Shiraishi:Shigeo Morishima	This paper presents a new methodology for constructing a skin deformation model using MRI and generating accurate skin deformations based on the model. Many methods to generate skin deformations have been proposed and they are classified into three main types. The first type is anatomically based modeling. Anatomically accurate deformations can be reconstructed but computation time is long and controlling generated motion is difficult. In addition, modeling whole body is very difficult. The second is skeleton-subspace deformation (SSD). SSD is easy to implement and fast to compute so it is the most common technique today. However, accurate skin deformations can't be easily realized with SSD. The last type consists of data-driven approaches including example-based methods. In order to construct our model from MRI images, we employ an example-based method. Using examples obtained from medical images, skin deformations can be modeled related to skeleton motions. Retargeting generated motions to other characters is generally difficult with this kind of methods. Kurihara and Miyata realize accurate skin deformations from CT images [Kurihara et al. 2004], but it doesn't mention the possibility of retargeting. With our model, however, generated deformations can be retargeted. Once the model is constructed, accurate skin deformations are easily generated applying our model to a skin mesh. In our experiment, we construct a skin deformation model which reconstructs pronosupination, rotational movement of forearm, and we use range scan data as a skin mesh to apply our model and generate accurate skin deformations.	Accurate skin deformation model of forearm using MRI	NA:NA:NA:NA:NA:NA	2009
Satoko Kasai:Shigeo Morishima	The presence of CG character is essential in anime and movie contents recently. Especially, personality and aging factor are also important in character modeling. However, the modeling CG character is based on hand-made process yet, so it costs huge amount of money and labor to give one character several variations. About a facial animation, muscle based process or blend shape based process is very popular in contents production, however, in case of considering aging mechanism on face skin and bone, the different model of each age has to be constructed for every character.	Aging model of human face by averaging geometry and filtering texture in database	NA:NA	2009
Ming Tang	This poster presents a range of urban simulation techniques and multidisciplinary research across Architecture design, urban design, interactive design and game design, as well as visual effects. Through a series of design experiments using City Generator, the 3D urban simulation tool, I explored an integrated set of design methods, such as genetic computation, and real time simulation with game engine. The complexity of 3D urban form is procedurally controlled by 2D GIS (Geography Information System) data input and digital elevation model (DEM), which allows designers to efficiently shape urban growth in a preferred direction.	City generator: GIS driven genetic evolution in urban simulation	NA	2009
Lu Liu:Tao Ju	Describing shapes is an important task in graphics and vision. A simple, concise descriptor that captures the essential shape properties of an object would greatly facilitate computer-based understanding of the object and applications such as matching and segmentation. For this reason, medial axes (MA) has become a popular shape descriptor since its introduction by Blum [Blum 1967]. The MA of an N-D object is an (N -- 1)-D geometry centered within the object. For example, the MA of a 2D object consists of medial curves at elongated parts, whereas the MA of a 3D object consists of medial surfaces describing the protrusions on the object.	Defining and computing multi-dimensional skeletons	NA:NA	2009
Sebastian Pena Serna:Andre Stork	Dynamic simplicial meshes will enable real time unconstrained deformation of triangular and tetrahedral meshes regardless of topological limitations, by means of combining a quality measure with complex topological operations and smoothing techniques, which automatically improve and maintain the quality of the mesh, aiming at mesh and geometry processing applications.	Dynamic simplicial meshes	NA:NA	2009
David Nilosek:Karl Walli	Automated synthetic terrain and architecture generation is now becoming feasible with calibrated camera remote sensing. This poster implements computer vision techniques that have recently become popular to extract "structure from motion" (SfM) of a calibrated camera with respect to a target. This process will build off of Microsoft's popular "PhotoSynth" technique and apply it to geographic scenes imaged from an airborne platform. Additionally, it will be augmented with new features to increase the fidelity of the 3D structure for realistic scene modeling. This includes the generation of both sparse and dense point clouds useful for synthetic macro/micro-scene reconstruction.	Aerial scene synthesis from images	NA:NA	2009
Jingyuan Huang:Stephen Mann:Bill Cowan	Müller et al. introduced CGA shape, a shape grammar for procedural modeling of architecture [Müller et al. 2006b], which they applied to Mayan archaeological site in Xkipché [Müller et al. 2006a]. Inspired by this application of shape grammars to archaeology, we built a simple reconstruction application that uses a shape grammar to build 3D Inca sites from 2D plans. The application can help users to create a quick 3D overview of an Inca site that they are studying.	Inca reconstruction using shape grammar	NA:NA:NA	2009
Yoshiki Mizushima:Shuhei Nomura:Genki Umeizumi:Noriko Nagata:Yoshiyuki Sakaguchi	The need for rendering woven fabrics arises frequently in computer graphics[N Adabala, N Magnenat-Thalmann, G Fei 2003]. Woven fabrics have a specific appearance, luster, and transparency. We have proposed a BRDF/BTDF model using the Henyey-Greenstein function and an algorithm for the real-time rendering of woven fabrics based on the texture look-up table[Uno et al. 2008]. However, in order to make the model more accurate, the microlevel BRDF/BTDF is necessary. The objective of this study is to express a more detailed texture of the cloth by creating a 3D model that especially takes into consideration the "twisted structure" of the yarn in the fine woven structure of the cloth and by making the rendering algorithm more precise.	Lace curtain: modeling and rendering of woven structures using BRDF/BTDF: production of a catalog of curtain animations	NA:NA:NA:NA:NA	2009
E. Riegel:T. Indinger:N. A. Adams	Within computational fluid dynamics (CFD) the Navier-Stokes (NS) equations are traditionally used to describe the physical properties of the fluid. An alternative approach to classical discretizations for the numerical solution of the Navier-Stokes equations, such as Finite-Difference and Finite-Volume schemes, is provided by the Lattice-Boltzmann equations [Benzi et al. 1992], [Chen and G. 1998]. The Lattice-Boltzmann method (LBM) uses a Cartesian grid for propagating and relaxing a discrete velocity distribution function on a lattice at discrete time steps. Usually a very large number of cells is necessary to obtain an accurate prediction of the macroscopic scales for pressure and velocity. However, due to the simple formulation of the underlying algorithm this method is well suited for parallelization and hardware acceleration using general purpose graphical processing units (GPGPU). LBM is used in engineering software for example to compute the aerodynamic drag of a car to improve its efficiency. Therefore LBM has a big practical importance. Improving the performance of a CFD simulation gives the engineers more time and better feedback during the engineering process leading to more efficient engineering processes and more efficient engineering products. Moreover a strong acceleration in simulation performance makes a new quality of physical simulation technology available for desktop computer software like entertainment and content creation software.	Numerical simulation of fluid flow on complex geometries using the Lattice-Boltzmann method and CUDA-enabled GPUs	NA:NA:NA	2009
Yu-Bin Yang:Jin-Jie Lin	In content-based 3D mesh retrieval, graph-based structure is one of the most important shape descriptors. The early work on this issue can be found in [Hilaga et al. 2001], in which a 3D mesh representation, Multiresolutional Reeb Graphs (MRGs), was proposed. Since then, many Reeb-Graph based descriptors have been designed to simplify and represent 3D meshes. However, most of those descriptors are only suitable for graph-based topology matching, which is time-consuming and unreliable. To address this issue, we propose a novel approach to representing 3D mesh by using a tree-like structure, Attributed Root Trees (ARTs). The advantages of our method are three-fold: (1) Tree matching is easier and efficient than graph matching; (2) The representation well reserves topological information of a 3D mesh, thus topology matching can be easily completed; (3) It is possible to perform multi-resolution matching on ARTs.	Representing 3D mesh with attributed root trees	NA:NA	2009
Ross Sowell:Lu Liu:Tao Ju:Cindy Grimm:Christopher Abraham:Garima Gokhroo:Daniel Low	MRI and CT scanners have long been used to produce three-dimensional samplings of anatomy elements for use in medical visualization and analysis. Physicians often need to construct surfaces representing the anatomical shape in order to conduct treatment, such as radiating a tumor. Traditionally, this is done by a time-consuming process in which an experienced physician marks a series of parallel contours that outline the object of interest.	User studies on the feasibility of oblique contouring	NA:NA:NA:NA:NA:NA:NA	2009
Trishul Mallikarjuna	This document introduces a conceptually novel, simple and ordinarily robust computer-vision-based method of extracting musical beats from regular physical gestures of a performer, implemented in VisiBeat: a grid-based percussion system on the Max/MSP/Jitter platform for collaborative interactive music.	A visual beat detection system for grid-based interactive percussion and synchronization	NA	2009
M. Cicconet:I. Paterman:P. Carvalho:L. Velho	Hardware based musical instruments are, in general, from the performer point of view, merely copies of real physical instruments. They do not provide facilities for being played, especially for musically untrained people.	The blues machine	NA:NA:NA:NA	2009
Sergio Krakowski:Luiz Velho:Francois Pachet	In this work, we address to the problem of making the machine listen and react to the musician in an improvisation situation with the purpose of generating high-quality music.	Pandeiro funk: experiments on rhythm-based interaction	NA:NA:NA	2009
Moohyun Cha:Jaikyung Lee:Byungil Choi:Hyokwang Lee:Soonhung Han	In order to simulate and visualize natural phenomena, especially fluid behavior such as smoke and fire, many novel studies have recently been conducted. Usually these methods use CFD (computational fluid dynamics), which calculate Navier-Stokes equations in real-time to generate realistic fluid motion and interactions, as well as high-performance GPU technologies. We proposed a new approach to the visual simulation of fluid flow by combining the use of pre-calculated CFD data with the real-time processing of such data. As the domain-specialized CFD solver predicts detailed fluid dynamics to an accuracy of a guaranteed error range, we could provide nearly actual behaviors of a fire-driven fluid flow. Moreover, this CFD data includes physical quantities such as temperature distribution, which can provide useful information to the training evaluation process. However, the data-driven method requires appropriate data processing techniques to create and manage large data sets. In this study, we developed a firefighter training simulator to demonstrate our proposed methods and explore related research issues.	A data-driven visual simulation of fire phenomena	NA:NA:NA:NA:NA	2009
Ippei Takauchi:Masatoshi Ochiai:Hiromu Saito:Ryo Asakura:Motofumi Hattori	In this paper, the authors discuss how to make 3DCG animations of flowers, wings, and cloths etc. which are modeled by surfaces. These 3DCG animations are obtained based on the numerical simulation for the Newtonian dynamics equations of surfaces. These equations are obtained from the potential functions (the energy functions) of the surfaces.	Desired deformation of continuum surfaces in 3DCG animation by time varying stable forms: application to make animations of flowers, wings, cloths etc.	NA:NA:NA:NA:NA	2009
Kazuhiko Yamamoto	There has been growing interest in the sound generating technique based on physics from the motions of 3d graphics objects. In recent work several methods have been proposed to physically simulate these audio events natably using modal synthesis [K. van den Doel et al. 2001] or finite element method [O'Brien et al. 2002]. However, in these mesh-based method, it needs complicated operation to preprocess, for example, to generate the computational mesh for 3d object, and to create the system's matrices, etc...	The framework of sound rendering for particle-based physics	NA	2009
Alejandro L. Garcia:Alice A. Carter:J. Courtney Granner:David Chai	"Physics for Animation Artists" is a joint project by the Department of Physics and the Animation/Illustration Program at San Jose State University to develop a physics curriculum specifically for art majors planning to enter the animation industry.	Physics for animation artists	NA:NA:NA:NA	2009
Biswarup Choudhury:Pisith Hao:Sharat Chandran	Amongst all atmospheric phenomena, rain is probably the most commonly used effect to create realistic immersive virtual environments, and to set the mood in movie storytelling. Although not immediately obvious, the beauty of rain emanates from the interplay of the involved light-matter interaction, generating effects of refraction and reflection, coupled with scattering effects. At the core, rain consists of water droplets under the influence of gravity. Current state of the art methods of generating rain are either computationally burdening, or not realistic enough. The key idea we introduce in this paper is to consider these droplets as transparent objects in the environment matting (EM) framework. This enables careful preprocessing to discover the light transport phenomena. We end up with a free-viewpoint real-time technique of simulating realistic droplets and rain in novel environments.	Real-time droplet modeling using color-space environment matting	NA:NA:NA	2009
Adrien Herubel:Venceslas Biri:Farchad Bidgolirad	In computer graphics, physically-based global illumination algorithms such as photon-mapping [2001] have a linear progression between complexity and quality. To a given quality, rendering time scales linearly with computer performances. With Moore's law call in question and increasing demand in quality, those algorithms need more and more optimisations.	Autonomous lighting agents in global illumination	NA:NA:NA	2009
Graham Fyffe	Image-based relighting is a powerful technique for synthesizing images of a scene under novel illumination conditions, based on a set of input photographs. While successful relighting methods exist, they either require many photographs [Debevec et al. 2000], or operate on a limited class of materials or illumination conditions [Ma et al. 2007][Ramamoorthi 2006].	Cosine lobe based relighting from gradient illumination photographs	NA	2009
Hiroyuki Kubo:Mai Hariu:Shuhei Wemler:Shigeo Morishima	Simulating sub-surface scattering is one of the most effective ways to realistically synthesize translucent materials such as marble, milk and human skin. In previous work, the method developed by Jensen et al. [2002] improved significantly on the speed of the simulation, yet still cannot produce real-time rendering. Thus, we have developed a simple local illumination model which mimics the presence of a subsurface scattering effect. Furthermore, this approach is easy implementable on the GPU and doesn't require any complicated pre-processing as is often the case in this area of research [Mertens et al. 2003].	Curvature-dependent local illumination approximation for translucent materials	NA:NA:NA:NA	2009
Greg Nichols:Chris Wyman	Area light sources are common in the real world, and thus important in realistic images. However, interactive rendering with area light sources is challenging, as each surface in a scene can receive light from every point in the area light. This problem is similar in nature to the rendering of single-bounce indirect illumination, and can be addressed with similar techniques.	Direct illumination from dynamic area lights	NA:NA	2009
Sajid Farooq:J. Paul Siebert	Gaussian Projection is an algorithm based on the Gaussian Pyramid, that can render point-sampled-geometry - obtained from stereo data in the form of range images - without polygonization, and at full native resolution. Gaussian Projection makes use of the GPU to perform efficient and fast multi-resolution rendering of point-based data, with automatic hole-filling (scattered point interpolation), and without any preprocessing other than Image Pyramid generation.	Gaussian projection: a novel PBR algorithm for real-time rendering	NA:NA	2009
Cyril Crassin:Fabrice Neyret:Sylvain Lefebvre:Miguel Sainz:Elmar Eisemann	Voxel representations are commonly used for scientific data visualization, but also for many special effects involving complex or fuzzy data (e.g., clouds, smoke, foam). Since voxel rendering permits better and easier filtering than triangle-based representations it is also an efficient high-quality choice for complex meshes (with several triangles per pixel) and detailed geometric data (e.g., boats in Pirates of the Caribbean).	Beyond triangles: gigavoxels effects in video games	NA:NA:NA:NA:NA	2009
Borom Tunwattanapong:Paul Debevec	We present a technique for relighting an image such that different areas of the image are illuminated with different combinations of lighting directions. The key idea is to capture illumination data using a lighting apparatus system such as Hawkins et al. [2004], calculate radial basis function interpolation of light constraints specified by users and render the calculated illumination result in realtime using GPU. The application can simulate the result of unnatural lighting conditions, for example, the image of a whole face lit from per pixel view dependence reflection angles or from gazing angles (see Fig. 1, a). The application can also render a high-resolution result at 1920 x 1080 in three to four minutes.	Interactive lighting manipulation application on GPU	NA:NA	2009
Kenshi Takayama:Takeo Igarashi	In our previous work of lapped solid textures [Takayama et al. 2008], layered (or 'type 1-b') texture exemplars were used to create solid textured models such as strata and cakes. However, no methods have been proposed so far to synthesize this kind of texture automatically. This poster proposes an extension of Kopf et al.'s method [2007] to synthesize such layered solid textures from single 2D exemplars.	Layered solid texture synthesis from a single 2D exemplar	NA:NA	2009
Seungju Bang:Kyoungju Park	In oriental paintings, artists have developed a unique style that exploits the effects of the dispersion of the ink, composed of soot and glue, onto absorbent paper. These artists produce these effects purposely by manipulating the ink concentration, stroke speed and brush angle. We describe these artistic styles of oriental painting and show how we render a single image in the oriental painting style. Our work is to take a single image as input and produce an oriental brushwork-like image automatically as a result. This nonphotorealistic rendering do not have richness of physical painting system such as 3-D brushes but reproduce artistic style of paintings to an image with a speed. In recent years, oriental brushwork rendering have been applied for 3-D objects e.g. Non-photorealistic rendering in Chinese painting of animals by Yeh and Ouhyoung 2002 and stereo images e.g. Humanistic oriental art created using automated computer pocessing and non-photorealistic rendering by Cheok et al. 2007 that require geometric details and depth information, but our method proposes a system that reproduce the styles and effects of oriental paintings from single 2-D image by using a set of image processing techniques. Figure 1 shows an input image and the reproduced oriental paintings. We focus on reproducing the stroke drawing and artistic shades that are essential in conventional oriental paintings. Some artistic styles of painting exploit the variety of lines and deep shades at once, giving a charming feel to the oriental brushwork. One general technique is "gu-ru law"Figure 2(a), in which artists draw tufted thin lines for boundaries and geometric details, and brush the interior with abstract but contrasting shades of ink. Another artistic style shades the whole objects instead of drawing its boundary with lines Figure 2(b), and paints near boundary regions with highly contrasting intensities. Our overall framework consists of two modules: stroke drawing and artistic shading Figure 3. These modules, essential to oriental painting, are generated using a set of techniques. Abstraction: Given an input image, we simplify it for geometric abstraction and color quantization using bilateral filtering. The abstracted image is then used as the input for our stroke drawing and artistic shading modules. Stroke drawing: We extract and draw representative lines for boundaries and geometric details. Line extraction includes selecting feature points, discarding small features, and clustering and linking similar points. Next, the spline curves are fitted to the linked points after the linked points are once again subdivided into several clusters according to their size. Line fitting produces smooth curves of appropriate length and curvature, similar to human-drawn strokes. Then, the curves' thickness is defined as proportional to the curvatures. Artistic shading: While oriental ink interacts with absorbent paper, ink disperses on the paper as water flows, the concentration of ink leaves bright-and-dark shades, and the residual ink disperses along the direction of the paper's fibers. We reproduce these effects by following steps. First, filtering over time is used to reproduce the ink dispersion effects when water spreads and stops, by applying blurring and sharpening filters consecutively. Next, we reproduce the bright and dark shades that arise from irregular ink concentration by stretching lightness contrast nonlinearly based on intensity values from blurred images. Finally, we reproduce the local patterns due to dispersion along the paper's fibers by using texture masks that are similar to textures of absorbent paper fibers. Composition: We compose the final results by combining stroke drawing and artistic shading, and by adding textures of absorbent papers for realistic effects. We produce the final oriental painting rendering as a composite of the results of stroke drawing and artistic shading from a single 2-D image. Figure 3 shows that strokes drawing; (a) is a map of edge by canny's edge filter, (b) is for clustering, linking sampled features and discarding some groups which have few features, (c) presents a thickness in each group, artistic shading; (d) Filtering overtime from input image, (e) contrast stretching, and (f) applying the mask of paper-fiber.	Oriental stylization with strokes and shades	NA:NA	2009
Yoon-Seok Choi:In-Kwon Lee:Bon-Ki Koo	This study describes a fully automated system for generating caricature images by using a painterly rendering method. This system transforms photos into caricature images automatically. A few similar approaches have been proposed by other researchers including [Gooch et al. 2004] and [Liang et al. 2002]. These methods, however, did not produce satisfying results, as the caricatures produced did not resemble handiwork. By simulating the brush strokes used by painters [Park et al. 2006], we reproduced the brush painting technique and incorporated it into our caricature system.	Painterly caricature maker	NA:NA:NA	2009
Tae-Joon Kim:Bochang Moon:Duksu Kim:Sung-Eui Yoon	Bounding volume hierarchies (BVHs) are widely used to accelerate the performance of various geometric and graphics applications. These applications include ray tracing, collision detection, visibility queries, dynamic simulation, and motion planning. These applications typically precompute BVHs of input models and traverse the BVHs at runtime in order to perform intersection or culling tests.	RACBVHs: random-accessible compressed bounding volume hierarchies	NA:NA:NA:NA	2009
Masashi Baba:Naoki Asada	Nowadays metallic paints are used in many situations. Although a lot of industrial products are painted because of the significant appearance, the reflection of the metallic paint is very complex and it is difficult to generate a photo-realistic image of a particular metallic paint. Recently, Rump et al. [Rump et al. 2008] proposed a method to acquire the reflectance and to generate photo-realistic images of metallic paints. They used BTF to capture and represent flakes in metallic paints, however, it is hard to capture and to store. In this paper, we propose a simple model to express the metallic paints including the sparkling effect of the flakes in metallic paints.	Reflection model of metallic paints for reflectance acquisition	NA:NA	2009
Jan-Phillip Tiesel:Christoph W. Borst	We describe an efficient single-pass rendering approach for composable 3D volumetric lenses. Composing rendering effects by intersecting multiple 3D lenses is a logical and intuitive extension of the Magic Lens metaphor and volumetric lenses. However, 3D lens composition was once considered intractable and recent multi-lens approaches require a number of passes that can grow exponentially with lens count. They generally involve substantial per-frame data structure generation or advanced techniques such as depth peeling. In contrast, we summarize a simple and effective technique that renders intersecting lenses of various shapes and effects in a single pass, does not require the maintenance of costly data structures, and can easily be incorporated into existing real-time rendering systems. It also supports more flexibility in the way complex lens effects are combined by using shade tree concepts to build composite shader programs.	Single-pass rendering of composable volumetric lens effects	NA:NA	2009
Kuntee Viriyothai:Paul Debevec	We present a technique for sampling the light probe image using variance minimization. The technique modifies median cut algorithm for light probe sampling [Debevec 2005] so that the variance of each region is minimized. The algorithm is fast, efficient, and easy to implement.	Variance minimization light probe sampling	NA:NA	2009
Christian Lipski:Christian Linz:Kai Berger:Marcus Magnor	We present an image-based rendering system to viewpoint-navigate through space and time of complex real-world, dynamic scenes. Our approach accepts unsynchronized, uncalibrated multi-video footage as input. Inexpensive, consumer-grade camcorders suffice to acquire arbitrary scenes, e.g., in the outdoors, without elaborate recording setup procedures. Instead of scene depth estimation, layer segmentation, or 3D reconstruction, our approach is based on dense image correspondences, treating view interpolation uniformly in space and time: spatial viewpoint navigation, slow motion, and freeze-and-rotate effects can all be created in the same fashion. Acquisition simplification, generalization to difficult scenes, and space-time symmetric interpolation amount to a widely applicable Virtual Video Camera system.	Virtual video camera: image-based viewpoint navigation through space and time	NA:NA:NA:NA	2009
Phoebe Coleman:Bill Elder	A 3D medical animation that visually describes the role of the Cystic Fibrosis Transmembrane conductance Regulator, which is directed by the code dictated by the Cystic Fibrosis gene in chromosome number seven.	The journey of the Cystic Fibrosis gene	NA:NA	2009
Takashi Nariya:Young Ah Seong:Tomoko Hashida:Takeshi Naemura	Global warming is one of many urgent problems we face and a positive individual attitude to the environment is necessary. The goal of our project is to raise awareness to the carbon dioxide (CO2) surrounding people which is a main factor causing global warming.	Spatio-temporal sensing and visualizing of CO2	NA:NA:NA:NA	2009
Alejandro Aguilar-Sierra	Modern technology allows to improve teaching methologies by allowing interactivity and to involve more senses in the learning process at an affordable price. In particular, visual learning has proved to be a very important way to understand otherwise elusive scientific principles. Our Visualization Laboratory for Earth Sciences is aiming to be a visual learning environment for Earth Science graduate students and at the same time to be a training platform for Computer Science undergraduates specializing in Graphics Application Programming. The importance of visual learning, specifically for Earth Sciences, has been examined previously [McGrath and Brown 2005], [Reynolds 2005].	Visualization laboratory for Earth Sciences: a multidisciplinary visual learning environment	NA	2009
Lisa Blum:Wolfgang Broll:Stefan Müller	Fascinated by a stunning variety of corals and fishes or mysterious wrecks more and more people are attracted by snorkeling and diving adventures. Virtual Reality scenarios like the virtual oceanarium [Froehlich 2000] try to satisfy this interest by allowing for discovery of underwater worlds in a riskfree and comfortable way, but a realistic feeling of diving is never achieved by virtual submarine worlds.	Augmented reality under water	NA:NA:NA	2009
Bruno Fernandes:Joaquin Fernández	Augmented Reality (AR) techniques have been applied to many application areas; however, there is still research that needs to be conducted on the best way to interact with AR content. Since hands are our main means of interaction with objects in real life, it would be natural for AR interfaces to allow free hand interaction with virtual objects. We present a system that tracks the 2D position of the user's hands on a tabletop surface, allowing the user to move, rotate and resize the virtual objects over this surface. Our implementation is based on a computer vision tracking system that processes the video stream of a single usb camera.	Bare hand interaction in tabletop augmented reality	NA:NA	2009
Hiroshi Mori:Kazuhito Shiratori:Tomoyuki Fujieda:Jun'ichi Hoshino	We propose the wellness entertainment system Versatile Training Field (VTF). In this system, we use the flexible foot interface as the input device. The system enables the user to move and jump freely in VR space by exaggerated movement corresponding to walking or jumping on the mini trampoline of the flexible foot interface. Improvements in exercise motivation and support for continuous exercise are achieved in our system, since it is possible to enjoy strolling through a virtual space, which is usually difficult to experience, by exercising on the mini trampoline without injury to the user's joints.	Flexible foot interface for versatile training field	NA:NA:NA:NA	2009
Takafumi Aoki:Hironori Mitake:Shoichi Hasegawa:Makoto Sato	We propose a new method for Symmetrical Haptic Interaction System with Virtual Creatures (VCs) in Mixed Reality. It's achieved by small and light haptic interface and Reactive VCs with touch sensations. People can touch VCs directly by fingers and watch their reaction. And VCs also can touch us directly.	Haptic ring: touching virtual creatures in mixed reality environments	NA:NA:NA:NA	2009
Daniel Makoto Tokunaga:Ricardo Nakamura:Romero Tori	The digital games industry is always looking for innovation in user experience. A new trend in this field is the use of Augmented Reality (AR) techniques for intuitive, novel game interfaces [Bernardes Jr et al. 2008]. Among the several technologies related to AR, video avatars are one of the most attractive for games, because they allow the insertion of the game player's image in the game. Furthermore, the availability of commodity stereo cameras makes it feasible to employ 3D video avatars in consumer games. However, when a non-photorealistic rendering style is required for design reasons (often the case for games), the use of a conventional video avatar, even with 3D information, results in visual inconsistencies. This work presents a new approach to enable nonphotorealistic rendering of a real-time 3D video avatar. The project builds upon a 3D video avatar system designed for teleconferencing over Internet 2 for educational purposes, extending it with this new rendering method. The expected result is a reduction in the visual and cognitive mismatch between player image and synthetic environment, allowing for a more immersive experience.	Non-photorealistic 3D video-avatar	NA:NA:NA	2009
Sheila Tejada	The mixed-reality game Robot RockStars combines a massively-interactive videogame controlled by multiple WiiGuitar players with a herd of Pleo and AIBO wireless robots as back-up singers, which can both react and affect gameplay. Attendees can affect gameplay by interacting with the robots or with mobile wireless devices. The main innovation lies in this project with the novelty of creating open-source, mixed-reality, plug-n-play tools that we build and the challenge for applications, such as the mixed-reality game Robot RockStars, to be constructed with them, allowing technology to go beyond what is currently possible for massively interactive collaboration between people and agents.	Robot rockstars: a mixed-reality game	NA	2009
Ryo Kishi:Yasuaki Kakehi:Takeshi Naemura	In this paper, we present a novel persistence of vision display named SteganoScan. SteganoScan is a stick shaped display device with a single line of LEDs that emits light in full color and shows 2D images without any screen.	SteganoScan: persistence of vision display with pixel-level visible light communication projector	NA:NA:NA	2009
Junfeng Yao:Xiaobiao Xie:Ming Zhang:Hui Zhang:Andy Ju An Wang	In recent years, virtual reality is experiencing a rapid development, which is also applied in plant morphology simulation. With a variety dynamic process of blooming, you are not only able to create a virtual landscape but to decorate a virtual space. In the internet, people can enjoy flowering anytime and anywhere to promote flower exhibitions and business. This paper focuses on the botanical characteristics of blooming process and creates a vivid effect based on Bezier curves and surfaces theory.	A 3-D flowering simulation based on botany characteristics and random generation algorithm	NA:NA:NA:NA:NA	2010
Shunsuke Matsuyama:Hironori Mitake:Shoichi Hasegawa	Recent progress of interactive techniques brought intuitive and physical interaction with characters in entertainment field such as console games. Conventional motion generation method requires preparing an enormous number of motion patterns in order to implement various reactions of characters. Therefore, for an easy way of implementation of various reactions, virtual creatures with sensorimotor models will become useful. Virtual creatures [Mitake et al. 2007] are characters with sensorimotor models generate motion in physics simulation environment.	A development environment for designing interactive characters with sensorimotor models	NA:NA:NA	2010
Aria Shahingohar:Roy Eagleson	The simulation of needle insertion is an important research area that has many applications in robotic and image guided brachytherapy cancer treatment, biopsies, and neurosurgery. Modeling of soft tissue plays an important role in the needle insertion simulation, but the use of Finite Element Method is complicated due to the need for remeshing in the neighbourhood of the needle tip. We are proposing to use a meshfree method for the tissue deformation modeling, in which new tissue nodes are added on the needle shaft as the needle is inserted into the tissue. In addition, we have utilized Nvidia's CUDA technology to accelerate the methods used in our framework.	A framework for GPU accelerated needle insertion simulation using meshfree methods	NA:NA	2010
Hirofumi Suda:Kentaro Yamanaka:Shigeo Morishima	We propose a skinning technique to improve expressive power of Skeleton Subspace Deformation (SSD) by adding the influence of the shape of skeletons to the deformation result by post-processing.	A skinning technique considering the shape of human skeletons	NA:NA:NA	2010
Michael Berger:Gregor Hofer:Hiroshi Shimodaira	Facial animation is difficult to do convincingly. The movements of the face are complex and subtle, and we are innately attuned to faces. It is particularly difficult and labor-intensive to accurately synchronize faces with speech. A technology-based solution to this problem is automated facial animation. There are various ways to automate facial animation, each of which drives a face from some input sequence. In performance-driven animation, the input sequence may be either facial motion capture or video of a face. In automatic lip-syncing, the input is audio (and possibly a text transcript), resulting in facial animation synchronized with that audio. In audio-visual text-to-speech synthesis (AVTTS), only text is input, and synchronous auditory and visual speech are synthesized.	Carnival: a modular framework for automated facial animation	NA:NA:NA	2010
D. Kravtsov:O. Fryazinov:V. Adzhiev:A. Pasko:P. Comninos	Polygonal models are widely used in computer animation. Static polygonal models are commonly animated using an underlying skeleton controlling the deformation of the mesh. This technique known as skeletal animation allows the artist to produce complex animation sequences in a relatively easy way. However, performing complex transitions between arbitrary animated meshes remains a challenging problem. There is a set of established techniques to perform metamorphosis (3D morphing) between static 3D meshes [Lazarus and Verroust 1998], but most of these can not be easily applied to animated meshes. The approach presented in this poster allows us to produce with great ease metamorphosing transitions between animated meshes of arbitrary topology using polygonal functional hybrids [Kravtsov et al. 2010].	Controlled metamorphosis of animated meshes using polygonal-functional hybrids	NA:NA:NA:NA:NA	2010
Hiroaki Gohara:Shiori Sugimoto:Shigeo Morishima	In anime production, some key-frames are drawn by artist precisely and then a great number of in-betweening frames are drawn by assistants' hands. However, it is seriously time-consuming and skilled work to draw many characters especially including face rotation. In this paper, we propose an automatic in-betweening technique for rotating face of hand drawn character only from a front image and a diagonal image (Fig.1). Baxter [2009] represented generating in-betweening using image morphing technique. However, their approach doesn't consider reflecting the artist's style and touch. Accordingly, we represent reflecting style and touch using morphing technique trained by his own database and introduced especially to generate a rotational in-betweening faces. This database contains center of gravity of each part (right eye, left eye, nose, mouth, eyebrow) and the contours on the facial image.	Data driven in-betweening for hand drawn rotating face	NA:NA:NA	2010
Michael J. Gourlay	Modeling continuous media such as fluids remains an elusive goal for interactive simulations. Fluids are particularly challenging because of the complexity imparted by the non-linear equations of motion, and the difficulty in creating stable simulations that retain spatial detail.	Fluid-body simulations using vortex particle operations	NA	2010
Dung A. Nguyen:Zhaoyang Wang	A 3D motion capturing and reconstructing system at high speed is presented. The system utilizes the fringe projection technique with one modified DLP projector, one camera and a computing unit to provide real-time reconstruction of forty-two 3D frames per second with the relative accuracy of 1/5000.	High speed 3D shape and motion capturing system	NA:NA	2010
Meredith McLendon:Ann McNamara:Tim McLaughlin:Ravindra Dwivedi	A digital creature's performance can be thought of as a combination of specifically defined motion and form; a combination that allows the viewer to comprehend the creature's action and intent. Computer graphics offers a variety of methods for defining motion including key-frame animation, data-driven action, rule-based and physically-based motion. However, all of these methods can be complex and time-consuming to implement. Essentially, most computer animation methods force the animator to think about motion at a low-level of abstraction. To create animation tools that simplify the process of creating expressive motion, we need to allow animators to work at a high-level of abstraction. We need determine the minimal elements of form and motion that visually communicate a maximal amount of information about an actor's identity or intentions. By attaching small reflective objects to joint pivot locations and recording at high contrast [Johansson 1973] developed a method for isolating motion from form as a collection of particles, now commonly known as a Point-Light Display (PLD). Manipulating this minimized visual information can even affect the perceived gender of PLD walkers. Cutting [1978] found that exaggerating the movement of points representing the hips and shoulders can bias gender recognition. The goal of our study was to investigate whether viewers use similar visual information to recognize expressive characteristics in animal motion PLDs as when viewing full representations and discover how it might be possible to use that visual information to influence the viewer's perception.	Lions and tigers and bears: investigating cues for expressive creature motion	NA:NA:NA:NA	2010
Gregor Hofer:Korin Richmond:Michael Berger	Talking computer animated characters are a common sight in video games and movies. Although doing the mouth animation by hand gives the best results it is not always feasible because of cost or time constraints. Therefore producing lip animation automatically is highly desirable. The problem can therefore be phrased as mapping from speech to lip animation or in other words as an acoustic inversion. In our work we propose a solution that takes a sequence of input frames of speech and maps it directly to an output sequence of animation frames. The key point is that there is no need for phonemes or visemes which cuts one step in the usual lip synchronization process.	Lip synchronization by acoustic inversion	NA:NA:NA	2010
Sriranjan Rasakatla:K. Madhava Krishna:Bipin Indurkhya	The Modular Legged robotic system [1] "Mod-Leg" presented here has been bio-inspired from a Snake's vertebrae and a caterpillar's legged structure. The system can be configured to a 4-legged robotic dog, a hexapod, a caterpillar and a Snake robot. This robot's novel design achieves compliance to the terrain using a combination of legs and electronically actuated universal spine. A unique simulator has been designed for this purpose. Some of the things we learned while developing this robotic system have been presented below.	"Mod-Leg" a modular legged robotic system	NA:NA:NA	2010
Adriana Schulz:Marcelo Cicconet:Luiz Velho	The one-way road leading music to motion has many manifestations. Choreographers build dance movements to match music features, subway passengers tap their feet following their iPod song hits, and the number one rule at any party is: if you wanna dance, dance to the music!	Motion scoring	NA:NA:NA	2010
Takeshi Miura:Kazutaka Mitobe:Takaaki Kaiga:Takashi Yukawa:Toshiyuki Taniguchi:Hideo Tamamoto:Noboru Yoshimura	It has been recognized that a technique to divide a raw motion-capture data stream of a dance into segments on the time axis is needed [Sonoda 2008]. In particular, the extraction of the higher-level information such as the hierarchical segmentation-structure is a subject of growing interest at the present time. In this study, the authors attempt to develop a method to segment dance motion in a multi-level style, namely in a hierarchical fashion.	Multi-level segmentation of dance motion by piecewise regression	NA:NA:NA:NA:NA:NA:NA	2010
Shoji Kunitomo:Shinsuke Nakamura:Shigeo Morishima	Realistic drape and motion of virtual clothing is now possible by using an up-to-date cloth simulator, but it is even difficult and time consuming to adjust and tune many parameters to achieve an authentic looking of a real particular fabric. Bhat et al. [2003] proposed a way to estimate the parameters from the video data of real fabrics. However, this projects structured light patterns on the fabrics, so it might not be possible to estimate the accurate value of the parameters if fabrics have colors and textures. In addition to the structured light patterns, they use a motion capture system to track how the fabrics move. In this paper, we will introduce a new method using only a motion capture system by attaching a few markers on fabric surface without any other devices. Moreover, animators can easily estimate the parameters of many kinds of fabrics with this method. Authentic looking and motion of simulated fabrics are realized by minimizing error function between captured motion data and synthetic motion considering both static and dynamic cloth features.	Optimization of cloth simulation parameters by considering static and dynamic features	NA:NA:NA	2010
Fiona Rivera:Phil Watten:Patrick Holroyd:Felix Beacher:Katerina Mania:Hugo Critchley	This research concentrates on providing high fidelity animation, only achievable with offline rendering solutions, for interactive fMRI-based experiments. Virtual characters are well established within the film, game and research worlds, yet much remains to be learned about which design, stylistic or behavioural factors combine to make a believable character. The definition of believability depends on context. When designing and implementing characters for entertainment, the concern is making believable characters that the audience will engage with. When using virtual characters in experiments, the aim is to create characters and synthetic spaces that people respond to in a similar manner to their real world counterparts. Research has shown that users show empathy for virtual characters. However, uncanny valley effects -- ie dips in user impressions -- can arise: behavioural fidelity expectations increase alongside increases in visual fidelity and vice versa. Often, characters used within virtual environments tend to be of fairly low fidelity due to technological constraints including rendering in real-time (Garau et al. 2003). This problem is addressed here by using non-linear playback and compositing of pre-rendered high fidelity sequences.	Real-time compositing framework for interactive stereo fMRI displays	NA:NA:NA:NA:NA:NA	2010
Cheng-Te Li:Hsun-Ping Hsieh:Tsung-Ting Kuo:Shou-De Lin	The goal of crowd simulation is to produce potential collective behaviors by simulating the movement process of a number of characters or agents. Some famous models are proposed to simulate crowd, including social force (e.g. [Helbing 2000]), cellular automata (e.g. [Chenny 2004]), and rule-based models (e.g. [Reynolds 1987]). Others use physiological (e.g. locomotion, energy level) and psychological (e.g. impatience, personality attributes) traits of agents to trigger heterogeneous behaviors [Pelechano 2007]. However, existing approaches do not consider the real-world social interactions among agents, and thus are unable to produce social-dependent scenarios. In this work, we propose to leverage the underlying social network, which captures social relationships among agents, for crowd simulation. A novel social-network-based framework, SocioCrowd, is developed (figure 1(a)) shows the virtual world). Based on SocioCrowd, we simulate three social-based scenarios, including community-guided flocking, following leading persons, and spatio-social information spreading. They display certain real-world social behaviors which are hardly modeled by existing methods. To lift the performance, our SocioCrowd is implemented by pure Java with GPU programming in ways of GSGL and JCUDA.	SocioCrowd: a social-network-based framework for crowd simulation	NA:NA:NA:NA	2010
Nobuhiko Mukai:Kentaro Ito:Masashi Nakagawa:Makoto Kosugi	One of the most challenging issues of computer graphics is to represent the behavior of fluid. Visualizing the fluid behavior requires to solve Navier-Stokes equations, which take huge amount of time so that some researches use many super computers for the simulation, and others utilize the GPU performance. The common fluid is Newtonian that can be described by a single constant value of viscosity, and there are many researches related to Newtonian. On the other hand, there is another type of fluid called non-Newtonian that cannot be described easily, and one of non-Newtonians is viscoelactic fluid. Viscoelastic fluid has the characteristics of both viscosity of fluid and elasticity of solid, and it is difficult to represent the behavior of viscoelastic fluid. [Goktekin et al. 2004] represented the behavior of viscoelastic fluid. His technique is based on Eulerian methods and added elastic terms to Navier-stokes equations, which govern fluid behavior. [Clavet et al. 2005] used particle method for representing fluid behavior. Particle method can represent fine behavior of the fluid such as rain drops, fountains, clay manipulation. Their researches could visualize many types of behavior of viscoelastic fluid, however, they cannot represent the spinnability, which has three characteristics: 1) it stretches very thin as if it is a string, 2) the radius is getting smaller gradually from the both ends and the center part has the least radius, and 3) it shrinks rapidly as if it is a rubber.	Spinnability simulation of viscoelastic fluid	NA:NA:NA:NA	2010
Daniele Federico:Damien Fagnou:Tom Reed	The creation of animation clips and the tweaking of existing character animation is often a tedious, time-consuming task, especially in large-scale CG productions. Traditionally these tasks were achieved by animators manually changing multiple keyframe values for all the relevant animation rig controls. Starting with the idea that a single animated object (e.g. a rig control) essentially defines a time-varying curve in 3D space - where the control points are defined by the keyframe values of the translation channels - we introduce a modeling approach for deforming these conceptual 3D curves. We will talk about our current implementation based on FFDs (Free Form Deformations) as described in [Sederberg and Parry 1986], but we strongly believe the same approach can have many other usages (e.g. modeling tools, collisions and obstacle avoidance).	Warping the space around an animated object	NA:NA:NA	2010
Paul D. Solt	Creating digital artwork requires a lot of time, talent, and effort from artists and programmers. It takes artists hours to design pleasing artwork and programmers even more time as they develop and debug complex graphics shaders. One way to aid in the creation of complex art is to use evolutionary computing called genetic programming. Genetic programming can be used to create mathematical expressions that can be rendered as an image. The image can be used as a texture in a 3D scene or as a starting point for additional artwork.	Artwork evolution	NA	2010
Young-Mi Kim:Jong-Soo Choi	This paper is about the study on an artwork, a black-and white drawing that has been expressed through a digital algorithm. Black-white drawings were popular during the Chosun era (1392--1910) reigned by kings and officials. The Oriental fine art, pursuing harmony with nature, is expressed in a moderate and restrained way, hence anyone would find it very soft and thus readily acceptable. Unlike the western paintings that fill the canvus to the very full, the oriental paintings treat even the blank space as a part making up a balanced painting. This artwork features Daegum, the decent traditional musical instrument which used to be played in loyal palaces or guest rooms of prestigious officials' residences, and a bamboo which was a frequent motive of gentlemen's paintings in the past. Daegum and the bamboo, expressed in a modern style in this work, make people appreciate the life that is full and rich. So, one can say they have been used here to make this "well-being art."	Breathe brush	NA:NA	2010
Daniel Tauber	I present a custom software for typing experiences that opposed to linear word processing renders visible individual writing styles on a personal computer using responsive typography in order to achieve a unique and personal representation of text analogous to handwriting.	Digital writing ductus: a visual representation of individual writing styles	NA	2010
Young-Mi Kim:Jong-Soo Choi	During the old days in the orient, people used to wipe cymbidium leaves or painted cymbidium for mental training by having a cymbidium always by their side. Through the act of wiping cymbidium leaves with utmost care, a cymbidium instilled with ancient philosophical ideas is visualized, and just as God breathed life into human nostrils and created a living life form, if a breath is breathed into a cymbidium flower, a cymbidium flower with an excellent fragrance is visualized. This work is an interactive visualization of an oriental cymbidium using modern technology which our oriental ancestors painted for mental training.	Ink-and-wash painting oriental cymbidium drawn with the tip of the fingers	NA:NA	2010
Özge Samanci:Anuj Tewari	In the digital era, the comics medium is transported from print to computer screen. Current digital comics (web comics or online comics) are confined to computer screen and use the affordances of digital medium in a limited way. GPS Comics: Seeing thru Walls is a GPS based comics story that expands the comic canvas and explores the idea of location-based comics. In Seeing thru Walls, in order to receive the meaning in a comic frame the player must experience a sensory detail (a smell, sound, breeze or an object) in her surroundings in the physical world. The concept of location-based comics is an unexplored idea and gives artists new meaning making strategies.	GPS comics: seeing thru walls	NA:NA	2010
Cem Sina Cetin	Graphic artists have a wide variety of applications to use for digital painting. Although each application has its own solution to enhance the user experience, most of them rely on the same standard feature; a single brush, which is completely dependent on user input for location. Although this is required for a fully controlled painting process, making small changes on this feature yields unpredictable results. My proposal for an alternate brush paradigm is using multiple brushes (as seen in the application "PD Particles"), which are not completely under control but rather moving within trajectories with random deviations, simultaneously. The trajectories are defined by controllable parameters and the user input. Since the rate of obedience to user input is dependant on the parameters, users can define the rate of deviation and thus switch between finger painting and generative painting, without changing the set of tools.	Musophobia	NA	2010
Tomoko Hashida:Yasuaki Kakehi:Takeshi Naemura	Drawing tools using digital technology can stimulate creativity in people. For example, the Wacky Brush tool in KidPix can produce effects (such as a line of dripping paint or a line of shapes) that cannot be obtained using ordinary paper and brushes [Hickman 1991]. This feature makes it easy for people to draw pictures having a combination of patterns. Such software, however, has so far been used only with electronic displays such as LCDs and PDPs. In this paper, we propose a mechanism that would allow the user to draw such pictures while using paper as a canvas instead of electronic displays. With this mechanism, a variety of patterns can be made to appear along lines traced out by the user by moving an electronic paint brush over paper. The advantages of using paper in this way include a high degree of freedom in shape and size as well as portability.	Photochromic canvas drawing with patterned light	NA:NA:NA	2010
Russell Deaton	Code (computer software and the technologies that it enables) is changing fundamentally how human beings interact with each other, and think about themselves and the world. It is a medium through which artists are increasingly expressing themselves. Code can serve as the tool which the artist uses to produce their work, or more interestingly, the artist takes the role of programmer and designs and implements an algorithm that generates the work of art. Thus, the artist's ideas are filtered and constrained through the filter of code, whose limitations and capabilities shape and inform the consequent artistic vision. For example, Casey Reas through his {Software} Structures takes verbal descriptions of processes to produce visual components and turns them into programs[Reas 2009]. In what follows, a Turing-universal model of some natural and manmade phenomena, Self-Assembly, is adapted to the automatic creation of visual art.	Self-assembled art	NA	2010
Shiho Hirayama:Yasuaki Kakehi	From childhood, we often play with bubbles. We find various aesthetic elements in a series of actions of soap bubbles: appearing, expanding, floating, bursting and disappearing. This time, we utilize the movements of soap bubbles as a pixel of an image and propose a novel interactive substantial display named "Shaboned Display." (see Figure 1)	Shaboned display: an interactive substantial display using soap bubbles	NA:NA	2010
Akira Nakayasu:Kiyoshi Tomimatsu	Recently there has been demand for display equipment capable of advanced expressions in spatial design. For example, there is the Adobe Interactive Wall at Union Square (New York City, 2007), and the Zero Energy Media Wall of greenPIX (Beijing, 2008) using LEDs placed on the whole facade. The simple display of information contents is becoming insufficient, and more appealing spatial designs combining information content with interactive art expression are becoming more important. In this paper, we propose a shape memory alloy motion display (SMD), a novel piece of display equipment taking advantage of the existence of an actual object. Then, we introduce an interactive art work plant based on SMD technology.	SMA motion display: plant	NA:NA	2010
Junfeng Yao:Xiaobiao Xie:Fengchun Lin:Xufa Ji:Xiaoyan Lin:Andy Ju An Wang	Recently, Xiamen University and Flying Information Technology Co., Ltd worked together and completed the development of The Online Custom-Built WEB3D Middleware System for Arts and Crafts, which will perform as a product 3D design and display center, its main features include the product demonstration background change, 3-Dimension design, 3-Dimension product display, product component reorganization and product material replacement.	The online customer-built WEB3D middleware system for arts and crafts	NA:NA:NA:NA:NA:NA	2010
Yuka Kubo:Hiroyuki Shindo:Koichi Hirota	For more than 1,300 years, beauty portraits have continued to be painted in Japan and virtually all have been stylized to a very unrealistic extent.	The Orikao Method: 3D scene reconstruction from Japanese beauty portraits	NA:NA:NA	2010
Robert B. Trempe, Jr	"24X7ATPHL: Codify" is an investigation into the novel usage of time-based animation software and procedural modeling as a method for visualizing time-based quantitative data via the construction of a qualitative, two-dimensional rendering. Treated as an experiment in the extrusion and aggregation of time-based qualitative instances, "24X7ATPHL: Codify" slows down and composites the accumulated information of seven days traffic (customer pickup and drop off) at an international airport; visualizing information in such a way as to not only notate the generations and changes in patterns, but also to show the beauty that can be found in data while unlocking the emergent potential for design. "Codify" makes use of the accumulation of NURBS geometries as a methodology for understanding the specific conditions of movement created by the interaction of existing architecture and user, the results of which are currently being used to develop everything from the design of several furniture pieces to that of a new cladding system for the Philadelphia International Airport.	[email protected]: Codify	NA	2010
Amy Martin:Wendy Ju	Bloom uses the metaphor of a desktop plant to remove task management from the already overloaded inbox and into a more human environment. When tasks in the inbox are starred, the email information is sent to an external touchscreen that then grows a flower for that specific task. The flower is activated on touch and the text of the email is displayed. Plucking the flower---touching, holding, then flicking the flower---removes that item from the task list. A large number of tools exist for managing tasks. Bloom is different in that it uses an organic, passive metaphor for visual display. Instead of having a series of piling text, whether in physical or digital form, Bloom does not visually overwhelm. A single task is as visually appealing as fifty. Additionally, although numerous email visualizations also exist much of this work has to do with overall inbox visualization and/or the display of relationships [1]. There is also precedence in using metaphor to visualize email as seen in Kjen Wilkens' Mail Garden. Bloom is distinct in both its focus on task management and our intent at full integration with existing email systems.	Bloom: an interactive, organic visualization of starred emails	NA:NA	2010
Yuki Igarashi	Line stone decoration is popular with young people. They enjoy applying line stone decorations to personal goods, such as notebook PCs, mobile phones and digital cameras. However, novices often find line stone decorations difficult to design, as the user must consider stroke length, stone width, and stone spacing. Hence, many people employ off-the-shelf design sheets or have their items decorated by an in-store professional. We have developed an interactive designing editor for line stone decoration. The user interactively draws freeform strokes on the canvas, as shown in Figure 1. The system then automatically generates a virtual line stone image (Fig. 1(b)), in which none of the stones overlap. Various off-line methods have been proposed for designing such decorations (e.g., tile mosaics [Hausner 2001]). Using our system, the user can create the designs interactively at the computer. Our system also creates a physical stencil pattern to help novice users to construct real line stone (Fig. 1 (c), (d)).	DECO: a designing editor for line stone decoration	NA	2010
Yuki Igarashi:Hiromasa Suzuki	Shape-matching toys are popular items for infants, and consist of boxes with many holes in different shapes along with corresponding blocks of the same shapes. To play with the toy, an infant finds and inserts a block matching the shape of a particular hole. It is difficult to design new shape-matching toys based on existing blocks. We assume that the user performs such design as shown in Fig. 1 (e) based on existing building blocks like those shown in Fig. 1 (a). The construction of the toy body can be roughly divided into three steps: gather the parts, lay them out on a wooden board and trace them using a pencil, and saw the wooden board. This manual method is straightforward, but errors cannot be rectified and it is also unsuitable for mass production. Accordingly, we propose the use of a laser cutter (e.g., Commax Laser System) or a cutting plotter (e.g., Craft ROBO). Today, services are available that allow the user to send a vector dataset to a company and have the corresponding wooden board returned to them.	Designing a new toy to fit other toy pieces: a shape-matching toy design based on existing building blocks	NA:NA	2010
Koh Sueda:Kazushi Kotani:Jun Rekimoto	Easy-Tagging Cam (or ETC) is a digital image recording system equipped with multiple shutter buttons. This system enables users to capture and tag photographs simultaneously. This function allows the user to be set free from tagging tasks. The users enable to develop re-useable photo storage continuously. This system also utilizes a life-log system thereby aiding the easy retrieval of information.	Easy-Tagging Cam: using social tagging to augment memory	NA:NA:NA	2010
Hiroki Yamada	Flower arrangement is one of famous traditional arts in Japan, and being enjoyed across the world now. People have been created the atmosphere in a room or represented one's mind by flower arrangement.	Floral melody: flower arrangement as music interface	NA	2010
Robert B. Trempe, Jr	"How Would You Like To Live" is a graphical articulation manifest from user sensory "wishes" supplied by an architectural client building a new home. It was crafted to help the designer in under-standing the needs of the client through emergent, patterned, non 1:1 results. Through the use of a parametrically-driven procedural network with parametric inputs supplied by the client, a graphical "depiction" of the user's hopes, dreams, and senses towards the occupation of domestic space was generated.	How would you like to live?	NA	2010
Ming Tabg:Jonathon Anderson	The name "Math-Morph" combines the notion of "mathematic" with the notion of "morphology. This project focuses on the study of "mathematic" as an embedded variability of spatial arrangement with procedural model. The influence of digital media and information technology on architectural education and practice is increasingly evident. Digital technology has reconditioned the design process that establishes new processes and techniques of fabrication. This reconditioning has influenced how we operate as architects. Today, architectural design and building construction are increasingly aided by and dependent on digital technology. These technologies allow architects to foresee the appearance and predict the performance of proposed buildings. Mathmorph proposes an interdisciplinary research in digital fabrication of unconventional 3D forms on a conceptual design level in order to explore their features in interacting with people and their potentials of being used as architectural forms. It describes an experimental approach which facilitates 3D form generation, visualization and fabrication.	Mathmorph	NA:NA	2010
Rebecca Findlay	Pixel Pusher represents a humorous symbol for all people who work with pixels on a daily basis such as teachers, students, or some other creative in the field relating them to the rigorous, time-consuming labor of a construction worker.	Rebecca Findlay's Pixel Pusher: poster abstract	NA	2010
Sophia Sobers	This architectural system explores the idea of using a parametric interface that reacts and changes based on user input while reproducing a series of affects (defined in psychology as the experience of emotion or feeling) on the user. The affects are predetermined, based on real world examples, and the system is designed in accordance. The overall premise for this project is to explore how tangible affects can be represented through parameters where the results are only visualized through the computer.	Reactive architecture	NA	2010
Kumiko Kushiyama:Tetsuaki Baba:Kouki Doi:Shinji Sasada	"Thermo-Pict neo" is a design apparatus produced by applying temperature visualization technology linked to an information display with the use of a thermograph sheet. Thermography is used to visualize the surface temperature of objects through their depiction as colors. This technology has been used primarily in the medical and research fields. Thermography display colors come in a wide range of hues and brightness that enables quick visualization of any object's surface temperature distribution. Use of this technology will be attempted as a tool in the production of design displays. [Fig.1]	Temperature design display device to use peltier elements and liquid crystal thermograph sheet: "Thermo-Pict neo"	NA:NA:NA:NA	2010
Hiroki Yamada	In this paper, the authors propose Tiny Dreamy Stories, which uses a traditional paper book as an interface to experience digital contents, so that it can keep the affordances of paper books while adding electronic augmentation. The aim of this study is to achieve both highly computer-supported contents and natural interface, e.g., highly efficient combination of physical and digital world. With Tiny Dreamy Stories, every person (especially who is not good at operating computers) can enjoy rich digital contents just by flipping pages.	Tiny Dreamy Stories: interactive paper book capable of changing the storylines	NA	2010
Jieun Kim:Carole Bouchard:Jean-Francois Omhover:Ameziane Aoussat	The TRENDS European project aimed at developing an image and text retrieval engine in order to support the activity of the designers in the early stages of their design process [TRENS 2007]. The study of the designers' activity has led us to the production of an image database in which designers will find inspirational material. A content-based image search engine has been elaborated, starting from recommendations taken from the methodology employed by the designers in their activity, to end with a complete system incorporating image retrieval technologies and various tools to extract relevant information from these images.	TRENDS: a content-based information retrieval system for designers	NA:NA:NA:NA	2010
Mary Huang	The design of typefaces is founded upon principles from the days of metal type, when creating individual fonts was a laborious process and constrained by physical requirements. Most digital type design follows those same conventions, even though fonts are now drawn with vectors and pixels. Fonts are still largely based on historical references and are created in the context of publishing.	TYPEFACE	NA	2010
Hwan-Soo Yoo:Seong-Whan Kim	We propose an Agritainment (agriculture with entertainment) framework, where users can learn how to cultivate plants and to breed livestock. To make an agricultural training joyful, we implement 3D collaborative space for training agricultural experience, which transforms monotonous training experience into realistic experience. Technical details include (1) multi-user networking, (2) realistic plant grow-up modeling, and (3) story-telling approach for immersive experience.	Agritainment: 3D collaborative space for training agricultural experience with entertainment elements	NA:NA	2010
Liliana Vega:Griselda Ledezma:Anayeli Hidalgo:Eduardo Ruiz:Omar Pinto:Ricardo Quintero:Leopoldo Zepeda	Recent results in educational research suggest the benefits of creating learning atmospheres in which students actively engage with the material as well as other classmates [1]. The idea of creating such an environment using a multiplayer mobile game represents a natural extension of the ubiquitous audio guides offered by most museums today.	Basic elements on game design for interactive museum exhibitions	NA:NA:NA:NA:NA:NA:NA	2010
David Bartle:Sam Rossoff:David Whittaker:Bruce Gooch:Kim Kerns:Jenny MacSween	Therapies that help restore abilities in individuals with brain damage are being investigated to help individuals with FAS. These methods focus on rehabilitation and exercises for the brain which improve specific cognitive capacities. We present Cognitive Carnival, a computer game therapy based on cognitive exercises, designed to improve the child's motivation and engagement of the tasks. Three minigames were developed, each based on improving one of three cognitive prinicples: executive function, continuous performance, and working memory. These minigames will be used in controlled therapy sessions with neuropsychologists for children with FAS to determine their effectiveness as a rehabilitative tool. Fetal Alcohol Syndrome (FAS) is a disorder that is caused by the ingestion of alcohol during pregnancy. Alcohol is a teratogen (substance that is toxic to the developing brain) and can result in abnormal brain development (brain damage). Children with FAS are faced with numerous obstacles, including significant problems with executive functions, attention, memory, and language. These conditions impede children with FAS from succeeding in school and living normal lives. There is estimated to be 0.5 to 2.0 children diagnosed with FAS per 1,000 births in the United States during the 1980's and 1990's [May and Gossage 2001]. It especially prevalent in remote communities. There is no cure. However, therapies that help restore abilities in individuals with brain damage are being explored to help individuals with FAS. These methods focus on rehabilitiation by means of an intervention by psychological professionals. The therapies are able to leverage the brain's plasticity to improve cognitive function [Neu 2002]. While adult brains show low levels of plasticity, children have more neurons and their brains continue to grow into their early 20's. Consequently, neurogenesis can be leveraged by supervised mental exercise. Classical therapy involves a trained therapist visiting local school, often times for a single child, to administer the therapy. The therapy itself consists of a set of exercises which the child preforms. This model of therapy, while effective, is inefficient and often times impractical for many areas. Additionally, the therapy has no builtin reward system and often times the therapist will offer the child candy or a small prize which provides little engagement with the tasks themselves. The minigames are intended for a controlled environment where a child with FAS is supervised by a neuroscientist. Over a course of weeks, the child will play each minigame, progressing through the difficulty levels as their abilities increase. A game-based therapy has multiple advantages over traditional exercises. Games tend to be more engaging than paper exercises. They also can accommodate built-in reward and motivation systems, instead of requiring the alternative of real-world incentives as the sole motivation for completing the tasks. Possibly the the most significant advantage is the ability to easily distribute the system using the Internet. This allows it to easily reach remote areas, where FAS is prevalent. Our therapy targeted three cognitive abilities: continuous performance, working memory, and executive function. Four minigames were created, each embodying at least one of the fundamental cognitive abilities affected by FAS. We decided to divide the therapy into minigames as each minigame could focus primarily on a single cognitive principle. This narrowed focus allowed neuropsychologists to measure progress on particular abilities. This also allowed for separate starting ability parameters for the children. The minigames focused on three cognitive principles typically impaired by FAS: continuous performance, working memory, and executive function. Continuous performance is the ability to sustain a consistent focus to an ongoing task continuing over an extended time period. Children without this ability may be at a major disadvantage in learning settings to their healthy counterparts. Working memory is the ability to temporarily store and manipulate information. It is an important basis for complex cognitive processes. Executive function is the ability to plan, problem solve, and make decisions. Each minigame has built-in difficulty levels, and the ability to manually adjust parameters for an individually-tailored therapy. Levels are then subdivided into repeated trials of the same parameters. The user progresses through each trial, receiving positive and negative feedback as appropriate, according to his or her performance. Upon the completion of a session, the user is presented with his or her results and a progress plot of their recent trials.	Cognitive games as therapy for children with FAS	NA:NA:NA:NA:NA:NA	2010
James R. Geraci:Erek R. Speed	Our work focuses on the area of using a high level language to improve program productivity, performance and portability. In general, this has been an area of intense research. There are a number of previous efforts including ZPL [Chamberlain and et al 2004], X10/Fortress/Chapel from IBM/SUN/Cray [Weiland 2007], Intel's CT/RapidMind [McCool 2006] and parallel VSIPL++ [Lebak and et al 2005] to name a few. However, while these languages do great things in simplifying parallel implementation of code, extensions beyond that are limited. The primary exception to this is VSIPL++ which implements several high level functions useful to the signal processing community. While most of these languages can be used to implement graphics or game related algorithms if necessary, none of them attempt to provide a platform that makes such development particularly easy. On the other hand, high level engines such as Renderman and Unreal provide the wanted abstractions but with little or no guarantees about extensibility, portability, or parallel performance. Our research focuses on adapting the parallel VSIPL++ API from the signal processing community to the graphics and game development environment.	Improving program productivity, performance and portability through a high level language for graphics and game development	NA:NA	2010
Madhuri Koushik:Eun Jung Lee:Laura Pieroni:Emily Sun:Chun-Wei Yeh	The goal of the project was to design an integrated system for the California Academy of Sciences that combined new technology (iPads in our case) with a social-networking based website to promote educational learning geared towards middle-school students. The experience begins when museum visitors create profiles on the California Academy of Sciences website. Initially they are able to personalize a limited number of characteristics of their avatars. Once they visit the museum, they play mini-games on iPad kiosks to accumulate points on their accounts. We developed five different educational mini-games, focusing on the areas of climate change, astronomy, evolution, and the food chain. The points gained on the iPad mini-games can then be redeemed at home by returning to the California Academy of Sciences website. Accessing the website from home allows the user to further personalize an avatar, learn more facts, and compare their scores on the mini-games and their avatar with those of their peers. Points can be redeemed to upgrade the avatar's available attributes and attires. By extending the museum experience to home and through increased level of social network interaction, learning is reinforced over a longer period of time. In a user testing session with 50 students of the target demographic age, 72% said they would be interested in redeeming their points online. They also had the opportunity to write a fact that they learned from the game. 67% of students that played the camouflage game (n=12) were able to state a fact that they learned from the game. Utilizing new technologies like the iPad is an opportunity to increase the number of initial users that create profiles on a new educational socialnetworking game website. Future research can focus on determining the extent of the educational effect of a system like this. For example, how might the experience of learning at the museum, enforcing that learning at home, and repeating through return visits to the website affect retention of facts?	iPad mini-games connected to an educational social networking website	NA:NA:NA:NA:NA	2010
Tetsuaki Baba:Kumiko Kushiyama:Kouki Doi	Today, many researchers reports studies about haptic, tactile or tangible art and entertainment. Particularly about temperature sensation, few interaction system has ever been presented because of it does not have good responsiveness. In this study, we shall design the video game interaction system that uses temperature sensation to users. First of all we investigate the relation of the rapidity of temperature change and user response time by using prototyped controller. Our game controller can offer temperature to users dynamically according to game situations. As a result, It was able to propose a basis of interaction system to take the temperate sensation to the game interaction.	ThermoGame: video game interaction system that offers dynamic temperature sensation to users	NA:NA:NA	2010
Kotaro Takahashi:Tomohito Yamamoto	For providing high presence, many kinds of display system have been developed [Hughes et al. 2005]. Typical examples are 3-dimensional display and multi-channel surround speaker system. Moreover, 3D movie such as "Avatar" or 3D TV have been brought to the market. However, these high realistic displays for visual, sound, or both, were usually composed of fixed and expensive equipment.	3D audio-visual display using mobile devices	NA:NA	2010
Mike Roberts:Mario Costa Sousa:Joseph Ross Mitchell	We present a novel GPU level set segmentation algorithm that is both work-efficient and step-efficient. Our algorithm has O(log n) step-complexity, in contrast to previous GPU algorithms [Lefohn et al. 2004; Jeong et al. 2009] which have O(n) step-complexity. Moreover our algorithm limits the active computational domain to the minimal set of changing elements by examining both the temporal and spatial derivatives of the level set field. We apply our algorithm to 3D medical images (Figure 1) and demonstrate that our algorithm reduces the total number of processed level set field elements by 16x and is 14x faster than previous GPU algorithms with no reduction in segmentation accuracy.	A work-efficient GPU algorithm for level set segmentation	NA:NA:NA	2010
Budirijanto Purnomo:Norman Rubin:Michael Houston	Modern GPUs have been shown to be highly efficient machines for data-parallel applications such as graphics, image, video processing, or physical simulation applications. For example, a single ATI Radeon™ HD 5870 GPU has a theoretical peak of 2.72 teraflops (1012 floating-point operations per second) with a video memory bandwidth of 153.6 GB/s. While it is not difficult to port CPU algorithms to run on GPUs, it is extremely challenging to optimize the algorithms to achieve teraflops performance on GPUs. Only a select few expert engineers with the application domain expertise, a deep understanding of the modern GPU architecture, and an intimate knowledge of shader compiler optimization can program GPUs close to their optimal capabilities. Many developers are content with several folds of improvements rather than one or several orders of magnitude acceleration compared to their optimized CPU implementations.	ATI Stream Profiler: a tool to optimize an OpenCL kernel on ATI Radeon GPUs	NA:NA:NA	2010
Yoshiharu Momonoi:Masahiro Sekine:Tatsuo Saishu:Yasunobu Yamauchi	We have proposed a flatbed autostereoscopic display using the one-dimensional (1-D) integral imaging (II) method [Hirayama 2006]. 1-D cylindrical lens array (lenticular sheet) is used in the 1D-II display, making it possible to observe a three-dimensional (3-D) image with the horizontal parallax ray. The flatbed autostereoscopic display system brought about a more effective stereoscopic experience than the conventional upright display. In the flatbed display configuration, observers perceive displayed objects as if they exist on a table, because it has real depth matching with a horizontal plane and uses bird's-eye view configuration.	Birds-eye view ray scan system for flatbed autostereoscopic displays	NA:NA:NA:NA	2010
William C. Thibault	Immersive multi-projector displays with dozens of projectors are becoming easier to build as projection technology proliferates. We envision scenarios such as a classroom of students with individual projectors, and informal groups of people with projector-equipped mobile devices, in which computer-generated imagery can be generated and displayed in real time. Ideally, the resolution of the multi-projector display should grow with the number of projectors, and support arbitrarily wide displays (to 4π steradians).	Camera-based calibration for scalable immersive rendering	NA	2010
Sriranjan Rasakatla:Kashyap Kompella:Krishna Koundinya	Here we present our idea of using a cell-phone (the Neo Freerunner) for tracking a Car's location using GPS and measuring the road's quality using the accelerometer in the cell-phone. Neo-Freerunner is an open source Linux phone by Open Moko Inc. The phone can run many flavors of linux like Android, Qt, SHR etc. Here the implementation was done in SHR.	Car tracking and vibration test rig using Neo-Freerunner	NA:NA:NA	2010
Kazuhisa Yanaka:Akifumi Momose:Masahiko Yoda	Chroma keying is a well-known technique for mixing two images in which a specific color of the foreground image is made transparent. When this technology is applied to integral photography (IP) images, each of which is a textured image in which images taken from hundreds of angles are integrated, it is very useful. IP is an ideal 3D display method because parallax in all directions can be obtained without the need for wearing special glasses. Moreover, it needs only simple hardware consisting of an LCD and a fly's eye lens. In particular, in the case of the extended fractional view (EFV) method [1][2], an inexpensive ready-made fly's eye lens can be used. Animation is also possible by displaying frames successively. However, creation of IP images is computationally intensive because multi-viewpoint rendering, in which a large number of images are observed from hundreds of viewpoints, is necessary. Therefore, we developed a chroma keying technology to reduce the processing time. By creating the foreground IP images and the background IP images separately and combining them later, the processing time could be reduced greatly, especially when the background was stationary.	Chroma keying between integral photography images	NA:NA:NA	2010
Douglas Lanman:Matthew Hirsch:Yunhee Kim:Ramesh Raskar	We optimize the performance of automultiscopic barrier-based displays, constructed by stacking a pair of LCD panels. To date, such displays have conventionally employed heuristically-determined parallax barriers, containing a fixed array of slits or pinholes, to provide view-dependent imagery. While recent methods adapt barriers to one or more viewers, we show that both layers can be adapted to the multi-view content as well. The resulting content-adaptive parallax barriers increase display brightness and frame rate. We prove that any 4D light field created by dual-stacked LCDs is the tensor product of two 2D mask functions. Thus, a pair of 1D masks only achieves a rank-1 approximation of a 2D light field. We demonstrate higher-rank approximations using temporal multiplexing.	Content-adaptive parallax barriers for automultiscopic 3D display	NA:NA:NA:NA	2010
Kip Haynes:Jacquelyn Morie:Eric Chance	Second Life (SL) is a popular 3D online virtual world designed for human interaction (also known as a MUVE, or multi-user virtual environment). It typically supports 60--70 thousand concurrent users. The assets and physical environments within SL are easy to create and use, and the environments themselves are very much part of the human interaction experience. However, the typical means of accessing SL is through a single computer screen, which lessens the immersion that is inherent in such a rich 3D world. Because of this, the SL virtual world is a good candidate for adaptation to large scale immersive displays such as a CAVE™ or other multi projector systems.	I want my virtual friends to be life size!: adapting Second Life to multi-screen projected environments	NA:NA:NA	2010
Shunsuke Yoshida:Sumio Yano:Hiroshi Ando	A tabletop is a useful shared space for diverse collaborative tasks. If the tabletop is considered to be interface, then expression through visual sensation, especially 3D images, is an important way to engage the principal human sense. Many 3D displays that can be observed from any direction have been proposed in recent years. However, some techniques force to wear special glasses and restrict the positions from which 3D images can be viewed [Kitamura et al. 2001]. Other glasses-free 3D displays employ obstructive apparatus on the table [Jones et al. 2007].	Implementation of a tabletop 3D display based on light field reproduction	NA:NA:NA	2010
Sriranjan Rasakatla	Many sensors like the laser range finder, stereo vision cameras which help in building a depth perception of the world around it in 3D are very costly. Here I present the designs and prototypes of few 3D perception sensors which have been built low cost using components off the shelf. These perception sensors use structured infrared light projection. The design is miniature compared to other 3D sensors like LIDAR, Laser scanner and Time of flight cameras.	Low cost 3D perception sensors	NA	2010
Hideaki Nii:James Teh Keng Soon:Adrian David Cheok	This paper describes the design of a parallax based Moving Slit Light Field Display (MSLFD). A MSLFD shows multi parallax images, by using vertical slits in an opaque cylinder surrounding multiple static flat panel displays. It allows viewers looking towards the cylinder to see an image from any position. Currently, various forms of 3D display have been developed and flat panel 3D display has been in practical use for some time. But commercial 3D displays only show a stereogram. On the other hand, Light Field Displays have been developed for displaying the dense ray information of the space, [Endo et al. 2005; Jones et al. 2007]. These displays use a "Parallax barrier" to control the ray direction to the observer. It shows parallax images without the use of an eye glass. We describe a system to reduce the size of the display by using two dimensional Organic Light- Emitting Diodes (OLED) and a rounding slit. OLEDs can act as the dense light source array and it can be controlled line by line. This system proposes a method to synchronize the movement of OLED's line and movement of slits. It can show many images in multi orientation (Figure 1(a)). In this paper we explain the principal method of design and how to expand the resolution and views of a MSLFD.	Moving Slit Light Field Display	NA:NA:NA	2010
Ishtiaq Rasool Khan	Two-layer encoding schemes for HDR images (and video) can not only reduce the storage requirements, but more importantly they can also ensure backward compatibility during transition from LDR to HDR age. The first layer is a tone-mapped LDR image, which can be shown on existing displays. The second layer is another LDR image and contains the residual information lost in tone-mapping, which can be used by HDR applications.	A backward compatible HDR encoding scheme	NA	2010
Chun-Te Wu:Wei-Hao Huang:Chih-Hao Liu:Wei-Jia Huang:Kai-Che Liu:Ludovic J. Angot	The new data structure, the bilateral grid, was presented by Jiawen et al. to make bilateral filter algorithm become simple implementation. Based on the data structure, the GPU CUDA-based optimization is proposed to have more efficiency in using GPU shared memory and massive multithreading. Meanwhile, a commercial application, the video 2d to 3d conversion which was presented by Ludovic et al. is also re-designed by applying the proposed CUDA-based bilateral grid three times to obtain better 3D quality in real-time. Depth map are created and modified by adjusting bilateral grid parameters.	A real-time video 2D-to-3D with the bilateral grid	NA:NA:NA:NA:NA:NA	2010
Masaru Tsuchida:Toru Takahashi:Koichi Ito:Takahito Kawanishi:Junji Yamato:Takafumi Aoki	In the digital archiving for cultural heritage preservation, in the medical field, and in some industrial fields, high-fidelity reproduction of color, gloss, texture, and shape are very important. Multiband or full-spectrum imaging technology is a solution for accurate color reproduction. Although several types of multi band camera systems have been developed [Yamaguchi 2000, Tominaga 2000, Helling 2004, Hashimoto 2008], all of them are multi-shot systems and they cannot take images of moving objects. Ohsawa et al. [2004] have developed a six-band HDTV camera system. However, the system requires very expensive customized equipment. In order to make multiband technology pervasive, equipment costs must be reduced and the systems have to be able to take images of moving objects. To meet these requirements, we developed a novel multiband image capturing system that combines multiband and stereo imaging techniques. This system can acquire both spectral color information and depth information at the same time. In this paper, we focus on the generation of six-band images from a pair of stereo image.	A stereo one-shot multi-band camera system for accurate color reproduction	NA:NA:NA:NA:NA:NA	2010
Christian Lipski:Christian Linz:Marcus Magnor	Over the last decade, considerable progress has been made on the so-called early vision problems. We present an optical flow algorithm for image morphing that incorporates recent advances in feature matching, energy minimization, stereo vision and image segmentation. At the core of our flow estimation we use Efficient Belief Propagation for energy minimization. While state-of-the-art algorithms only work on thumbnail-sized images, our novel feature downsampling scheme in combination with a simple, yet efficient data term compression can cope with high-resolution data. The incorporation of SIFT features into data term computation further resolves matching ambiguities, making long-range flows possible. We detect occluded areas by evaluating the symmetry of the flow fields, we further apply Geodesic matting to automatically inpaint these regions.	Belief propagation optical flow for high-resolution image morphing	NA:NA:NA	2010
Stavros Papastavrou:Demetris Hadjiachilleos:Georgios Stylianou	The identification of a bank note's value is a non-trivial task for the blind and the visually impaired. A popular approach adopted by many countries in order to facilitate the visually impaired, is the impression of a high-contrast, large-print region on their bank notes. Additionally, an approach used to facilitate the blind population is the impression of unique tactile marks on bank notes. However, even when tactile marks or different sizes (e.g. Euros) are used, blind and visually impaired people have practical difficulties in identifying them.	Blind-folded recognition of bank notes on the mobile phone	NA:NA:NA	2010
Xuan Dong:Yi (Amy) Pang:Jiangtao (Gene) Wen	We describe a novel and effective video enhancement algorithm for low lighting video. The algorithm works by first inverting the input low-lighting video and then applying an image de-haze algorithm on the inverted input. To facilitate faster computation and improve temporal consistency, correlations between temporally neighboring frames are utilized. Simulations using naive implementations of the algorithm show good enhancement results and 2x speed-up as compared with frame-wise enhancement algorithms, with further improvements in both quality and speed possible.	Fast efficient algorithm for enhancement of low lighting video	NA:NA:NA	2010
Lange Benoit:Rodriguez Nancy	Until now computer graphic researchers have tried to solve visualization problems introduced by the size of meshes. Modern tools produce large models and hardware is not able to render them in full resolution. For example, the digital Michelangelo project extracted a model with more than one billion polygons. One can notice hardware has become more and more powerful but meshes have also become more and more complex. To solve this issue, people have worked on many solutions. We can find solutions based on space subdivision, or based on visibility of objects like the use of a Z-buffer. But in 1976, Clark [Clark 1976] introduces the level of detail concept (LOD). The principle of LOD is the construction of several versions of the same 3D model at different resolutions. This is achieved by removing some object features. Luebke provides in [Luebke 1997] a very complete survey of LOD algorithms. The main issue with the simplification is that the mesh does not preserve appearance of the original mesh. Indeed, important features tend to disappear. For example, with the Quadric Error Metrics (QEM) algorithms and the cow mesh, the tail, horn and other characteristic points merge with the mesh at a low resolution. Our approach allows the simplified mesh to preserve important details.	LOD +: augmenting LOD with skeletons	NA:NA	2010
Shiro Ozawa:Takao Abe:Noriyuki Naruto:Toshihiro Nakae:Makoto Nakamura:Naoya Miyashita:Mitsunori Hirano:Kazuhiko Tanaka	In surface computing, one of the most important requirements is tracking an object placed on the surface and manipulating information related to that object. To recognize objects, the most popular technique is marker tracking using techniques such as RFID, tag-like TarckMate[Kumpf 2009] and so on. The issues with marker tracking are the effort required to paste the tag and the existence of objects that are difficult to mark with a tag. To recognize objects without tags, feature point tracking on the image plane is one of the most effective ways in the area of the computer vision[Lowe 2004]. Unfortunately it is difficult to extract features from images taken through the frosted glass that is often used in surface computing. In addition, one cannot extract the feature points from objects without strong texture. In this paper, we present a marker-less object recognition system using multi channel silhouettes and quantized polar coordinates.	Marker-less object recognition for surface computing	NA:NA:NA:NA:NA:NA:NA:NA	2010
Erich Marth:Guillermo Marcus	With the introduction of H.264, the complexity on video encoders has increased dramatically. As hardware based encoding solutions profit from the strict sequential design and already feature real time capabilities for high definition material, software solutions lack most of the encoding performance. More precisely, the performance of software encoders is limited due to the computation power of encoding system as well as the high level of codec-intern dependencies. As a consequence, software encoders supporting high definition needs are very rare.	Parallelization of the x264 encoder using OpenCL	NA:NA	2010
Eriko Kimura:Naoki Kawai:Kazunori Miyata	We have proposed a method called Bump Mapping onto Real Objects (BMRO)[1] for displaying the appearance of curved surface on flat media. The method converts normal vectors of modeled curved surface into directions of grooves by which anisotropic reflection occurs for displaying a curved surface. Although curved surfaces can appear on media by BMRO, it is still insufficient for practical use because the streamlines used for a pattern of grooves are often placed too closely or too sparsely to one another due to the vector plot employed for generating them. The simplest solution to avoid the non-uniformity is to divide the entire region into regular square or hexagonal cells and to fill each cell with parallel lines in a given direction instead of tracing the direction field strictly with streamlines, but this improvement causes aliasing to noticeably appear at the edges and ridges of the original model. In this article, we propose an improvement on generating cells that reduces aliasing for BMRO and makes it practical for industrial applications.	Practical 3D decoration on flat media with anisotropic reflection	NA:NA:NA	2010
Matthew Trentacoste:Rafal Mantiuk:Wolfgang Heidrich	The image quality of a digital viewfinder is considerably lower than that of a through-the-lens optical system. While the sensor may be capable of capturing 10 or 20 megapixels, the screen of the viewfinder is typically constrained to resolutions under 1 megapixel. The limited resolution makes it impossible to discern all the small details of the captured image. Small blurs and noise that are present in the full-size image can render the image unusable for certain tasks, yet these artifacts may be too small to be discernible in the downsampled version shown on the camera viewfinder.	Quality-preserving image downsizing	NA:NA:NA	2010
Zhengguo Li:Susanto Rahardja:Shiqian Wu:Zijian Zhu:Shoulie Xie	It is known that a high dynamic range (HDR) image can be produced by sequentially capturing a set of low dynamic range (LDR) images with different exposure times [Debevec and Malik 1997]. However, ghosting artifacts could be produced via this method when there are moving objects in a scene. In this poster, a similarity index is first introduced for such LDR images by using intensity mapping functions (IMFs) among them. The index is then applied to detect moving objects such that ghosting artifacts are removed from the eventual HDR image. The details are given as below.	Robust movement detection based on a new similarity index for HDR imaging	NA:NA:NA:NA:NA	2010
Thang M. Hoang	Fringe projection profilometry (FPP) is one of the most commonly used non-contact methods for retrieving the three-dimensional (3D) shape information of objects. In reality, the nonlinearity mostly caused by the gamma effect of digital otpic system, includes both projector and camera, gives inevitable intensity changes, which dramatically reduce the measurement accuracy. In this poster, a robust and simple scheme to eliminate the intensity nonlinearity induced by gamma effect. Firstly, by using phase shifting techniques, the gamma value involved in the measurement system can be detected accurately. Then, a gamma encoding process is applied to the system for future actual 3D shape measurements. With the proposed technique, high accuracy of measurement can be achieved with the traditional three-step phase-shifting algorithm.	Simple gamma correction for fringe projection profilometry system	NA	2010
Sun-Young Lee:Jong-Chul Yoon:In-Kwon Lee	Existing video matting approaches determine the alpha matte sequence frame-by-frame, which lead to flickering near the boundary of the foreground region. We reduce this effect by considering video data as a spatio-temporal cube, and extending a robust matting algorithm to a 3D solver. Our results demonstrate consistent and visually pleasing alpha mattes, and tend to preserve temporal coherence better than previous techniques.	Temporally coherent video matting	NA:NA:NA	2010
Jean-Charles Bazin:Soonkee Chung:Roger Blanco Ribera:Quang Pham:Inso Kweon	This paper introduces the new concept of virtual face sculpting. Given the images of a human face and a statue face (cf Fig 1-a and b), the goal of this application is to sculpt a virtual statue (cf Fig 1-c) as if the human face was sculpted on the statue. This problem is complicated and must face some important difficulties. For example, the virtual sculpture must verify the color and texture consistency of the original statue. Moreover, the structure of the human face must also not be modified, otherwise the person will not be recognizable.	Virtual face sculpting	NA:NA:NA:NA:NA	2010
Koki Nagano:Takeru Utsugi:Mika Hirano:Takeo Hamada:Akihiko Shirai:Masayuki Nakajima	We have enabled the superimposition of multiplexed images on the same screen at the same time with tangible and stable equipment. Our multiplex images can be seen by wearing special configured polarized glasses, and the image projection method is designed to be based on current 3D stereoscopic technology, which is now prevalent and making rapid progress, thus high compatibility with current contents industries is retained. Therefore our system enables the wide range of applications with new expressions and can easily be put into production.	A new "multiplex content" displaying system compatible with current 3D projection technology	NA:NA:NA:NA:NA:NA	2010
Seiya Matsuda:Tomohito Yamamoto	Recently, 3D movies such as "Avatar" have been popular because they can provide hyper reality. Most of these movies require special facility such as IMAX 3D. Therefore it has been difficult to introduce these movies to home environment. However, 3D TV for individual use has already been developed and is expected to become popular in a few years. In such situation, the demand of 3D contents for those systems will be higher and higher. However it is very difficult to create such contents because it requires exclusive tool, high technique and much cost.	A web system for creating and sharing 3D auditory contents	NA:NA	2010
Yuki Hirobe:Shinobu Kuroki:Katsunari Sato:Takumi Yoshida:Kouta Minamizawa:Susumu Tachi	Previously, pictures were painted using tools such as crayons or even by hand. Surfaces such as canvases or walls, provided the tactile sensations of the drawing surface while painting. However, this tactile experience has got lost because of advances in computer graphics software. Besides, a conventional multi-touch interface [1] can not provide tactile sensation. We propose a novel interactive painting interface called "Colorful Touch Palette" that may help us to rediscover our creativity. The user can touch the canvas having the electrode, select or blend tactile textures of their choice, draw a line, and experience the tactile sensations of painting as shown in Figure 1. Various tactile textures can be created by blending textures as paints. This interface can be used to design complex spatial tactile patterns for haptic-friendly products. Moreover, this system can be potentially used to create novel tactile paintings.	Colorful Touch Palette	NA:NA:NA:NA:NA:NA	2010
Yoshihiro Kuroda:Hirotoshi Ashida:Masataka Imura:Yoshiyuki Kagiyama:Osamu Oshiro	Liquid absorption affects the behavior of objects. Rain absorbed in the barrage can weaken its structure and cause the dam failure. A wet sponge ball bounces differently from a dry one. Porous media is a material that has internal pore space and is able to absorb liquid (e.g. a sponge or soil). Liquid absorption changes not only geometrical properties, e.g. volume, but also mechanical properties, e.g. elasticity. The aim of this study is to physically model the structural change of a porous media due to liquid absorption. Previous studies have focused on liquid flow inside the media[Lenaerts et al. 2008]. In contrast, this paper proposes a porous model that is able to simulate elastic change in a real sponge.	Force reflecting porous media with dynamic elasticity change	NA:NA:NA:NA:NA	2010
Michal Lech:Bozena Kostek	Nowadays, one of the main focuses of the Human-computer interaction area is controlling computers by gestures. Various gesture types provide means of controlling user interfaces and applications. However, most of them involve the front-facing camera and the user's gestures are recognized often from the static background. In addition, colorful gloves, gloves with motion sensors or infrared diodes are often used for this purpose.	Gesture controlled interactive whiteboard based on SVM and fuzzy logic	NA:NA	2010
Stephen David Beck:Shantenu Jha:Brygg Ullmer:Chris Branton:Sharath Maddineni	Laptop Orchestras (LOs) have recently become a very popular mode of musical expression. They engage groups of performers to use ordinary laptop computers as instruments and sound sources in the performance of specially created music software. By using an orchestral metaphor, LOs provide an engaging and challenging environment to experiment with human-computer interaction, network and machine latency, and sound/signal processing. While the LOs at Princeton and Stanford are perhaps the best known, LOs have now been established at many universities in the US and UK, and as private ensembles around the world. Perhaps the biggest challenge for LOs is the distribution, management and control of software across heterogeneous collections of networked computers. Software must be stored and distributed from a central repository, but launched on individual laptops immediately before performance. Each "composition" consists of unique combinations of software, user interfaces, and physical devices. Moreover, performers in a Laptop Orchestra can have a complex array of application layers to manage and launch before the start of a specific piece's performance. For example, one work written for our LO requires a bluetooth middleware application which reads gestures from a Wii-mote and converts them into OpenSoundControl [Wright and Freed 1997] messages to be forwarded to a custom Max application, all of which must be launched and configured before any performance. Combine this with the rapid turnaround from one composition to the next during a concert performance, and the problem of preparing members of the laptop orchestra with the appropriate tools for each piece becomes daunting. The GRENDL project leverages proven grid computing frameworks and approaches the Laptop Orchestra as a distributed computing platform for interactive computer music. This allows us to readily distribute software to each laptop in the orchestra depending on the laptop's internal configuration, its role in the composition, and the player assigned to that computer. Using the SAGA framework [Goodale et al.], GRENDL is able to run pre-distribution scripts on a master computer, distribute software to client computers, launch post-distribution scripts on the master computer and launch application scripts on client computers that in turn manage application environments for each composition. SAGA, the Simple API for Grid Applications, is a distributed computing middleware used to distribute, manage and process grid-based applications, typically for scientific research problems in such diverse fields as numerical relativity, computational fluid dynamics and materials science. Its functionality and stability are well regarded within the computational science community and SAGA has become a standard API for grid computing. Our initial experiments have demonstrated that SAGA can be used successfully in a concert environment. The Laptop Orchestra of Louisiana (LOLs) debut concert on April 14, 2010 used a prototype version of GRENDL to manage two of the seven works performed, and GRENDL worked flawlessly. GRENDL proposes to go further than just applying SAGA to the LO environment. We will use tangible and physical objects [Ullmer et al. 2008] to represent individuals, resources, roles and compositions such that GRENDL knows how to distribute software appropriate to the LO's environment, the individuals performing and their role in the composition. By using RFID-embeded objects [Ullmer et al. 2010], master and client computers determine who is at which computers, and what is being performed. Just as a music librarian knows where to place parts for each composition on which music stands, GRENDL will know where to send software and how to launch that software for each composition, laptop and performer. Extending SAGA to work with tangibles and in novel runtime environments, will require extensions to SAGA -- support for new interfaces and instruments -- as well as require some performance engineering in order for commands to be processed with lower latency than "traditional" distributed systems are designed to tolerate. The trans-disciplinary nature of GRENDL provides potential to shed new light on existing challenges in computational and computer science. The LO setting presents a unique perspective from which to investigate topics such as time-sensitive and dynamic job scheduling, latency-bound interaction, and effective user interfaces for grid computing environments. Some of the first iteration interaction technologies have been developed for distributed computational science applications, and some of what is learned through GRENDL will likely be applicable in that area.	GRENDL: grid enabled distribution and control for Laptop Orchestras	NA:NA:NA:NA:NA	2010
M. Cicconet:L. Velho:P. Carvalho:G. Cabral	Interfacing with the guitar using the audio signal is one of the oldest problems in Computer Music, and advances in the area were astonishing. In our days it is possible to simulate a huge range of amplifiers, apply many filter effects and evaluate the pitch of a plucked string robustly, to mention a few useful applications.	Guitar-leading band	NA:NA:NA:NA	2010
Ji-Hye An:Su-Jin Lee	Considerations of interface design have been limited to the senses of sight and hearing. However, as the sense of touch, such as haptics, began to be applied to equipment, new interaction has emerged. Due to the integrated nature of people (Goldstein, 2002), it is important for a new system that added tactile stimuli to correctly analyze and understand users' experiences. This study analyzes integrated cross modality user experiences from devices providing information on the senses of sight, hearing, and touch.	How people tend to organize sensory information into unified wholes in haptic phone?: focusing on cross modality interaction	NA:NA	2010
Kai Uwe Barthel:Sebastian Müller:David Backstein:Dirk Neumann:Klaus Jung	Internet image search systems mostly use words from the context of the web page containing the image as keywords. The performance of these search systems is rather poor, as the search systems neither know the intention of the searching user nor the semantic relationships of these images. Content-based image retrieval (CBIR) systems rely on the assumption that similar images share similar visual features. Despite intense research efforts, the results of CBIR systems have not reached the performance of text based search engines. The main problem of CBIR systems is the semantic gap between the content that can be described with low-level visual features and the description of image content that humans use with high-level semantic concepts. Some image retrieval systems have combined the keyword and the content-based visual search approach. However with this approach many images may be found that semantically do not match. In addition semantically similar images that visually look different cannot be found at all.	Image retrieval using collaborative filtering and visual navigation	NA:NA:NA:NA:NA	2010
Paulo F. U. Gotardo:Alan Price	Our research explores the use of real-time computer vision techniques and a pair of standard computer cameras to provide 3D human body awareness in an inexpensive, immersive environment system, Fig. 1. The goal is to enhance the user experience of immersion in a virtual scene that is displayed by a 3D screen. We combine stereo vision and stereo projection to allow for both the user and the virtual scene to become aware of each others 3D presence as part of a single, integrated 3D space.	Integrated space: authoring in an immersive environment with 3D body tracking	NA:NA	2010
Jae-Hee Park:Tackdon Han	Multi-touch sensing exists in a number of applications and is presently used in personal computing devices (i.e. laptops and desktop computers), mobile touch screens, kiosks, Interactive wall displays (i.e. subway station map), ATMS, and any display requiring an interactive platform. Current multi-touch sensing methods use capacitive and or resistive based touchscreens both which are expensive and difficult to make. Infrared based touchscreens is being studied as an alternative method that is effective and low-cost solution of producing equal results particularly with large interactive displays.	LLP+: multi-touch sensing using cross plane infrared laser light for interactive based displays	NA:NA	2010
Norbert Győrbíró:Henry Larkin:Michael Cohen	To remember important information, we often take pictures and arrange them into collections. Photos can also be gathered and organized via personal lifelogs and social media websites which may include contextual metadata such as location, participants, rating, and even emotional tags. However, memories and connections between places, events, and people can be difficult to recollect. Memory recall in our brain can depend on several factors: emotional level, context variability, loss of information during encoding, etc. As time passes, memories are gradually forgotten or become altered, e.g. due to collision with newly encoded information [Yi Chen 2010].	Long-term memory retention and recall of collected personal memories	NA:NA:NA	2010
Chun-Yu Tsai:Hung-Jung Lin:Tzu-Hao Kuo:Kai-Yin Cheng:I-Chao Shen:Bing-Yu Chen:Rung-Huei Liang	Hearing is one of human's five senses. In our daily life, we usually guess where we are and the surrounding conditions not only by the visual feedbacks of the surrounded scene, but also by environmental sounds. For example, subway stations usually hint people the door closing by an urgent sound. In Taiwan, the garbage trucks usually broadcast one special song, and people can judge whether the car is coming. Similarly, we usually can recognize our familiar people only by hearing the sounds they generated without actually seeing them. For example, John usually bats basketball while entering the room. Hence, before he enters the room, the familiar sounds is heard, and can be recognized. Moreover, through the sense of hearing, people can only use their peripheral attention to quickly know where they are and what happens.	MusicSpace: you "play" the music	NA:NA:NA:NA:NA:NA:NA	2010
Anusha Withana:Rika Matsui:Maki Sugimoto:Kentaro Harada:Masa Inakage	With recent advancements in digital photography, data storage and network technologies, publishing and sharing of digital images in Internet has been drastically increased. High popularity and growth of internet image libraries such as Flickr and Picasa are good examples for these trends. In order to enable easy browsing and searching, online storages store meta-data in the form of keywords to describe images. Meta-data could be a general description of the image, a specific tag or an annotation to describe spatial information within the image. In order to ease and improve the efficiency of tagging process, image processing and analysis algorithms has been combined to manual tagging systems [Yang et al. 2009]. Furthermore, meta-data can be used to create interesting presentations of images. Specially in exhibition displays, automated slide shows and digital photo frames, meta data are used to group related images and present them under different categories. However, presentations generated by most of the existing systems are limited to sequential displaying of individual images as correlated groups. In this paper we present a system that composite digital images in a narrative fashion utilizing objective and subjective tagging information. Proposed system can extract a region from one image and composite it into another image according to available meta-data. As a proof of concept we created a right to left continuous image panning application using Adobe Flash which combines different images according to their similarities and composite them together to create a narrative image presentation.	Narrative image composition using objective and subjective tagging	NA:NA:NA:NA:NA	2010
Toshihiro Nakae:Shiro Ozawa:Naoya Miyashita	We propose O-Link, a system that allows us to convey our experience by binding digital videos to a real object in order to facilitate intergenerational communications. We focus on two factors in designing our interface to build a closer relationship between grandparents and grandchildren.	O-Link: augmented object system for intergenerational communication	NA:NA:NA	2010
Frank Steinicke:Gerd Bruder:Scott Kuhl	In computer graphics one is often concerned with representing 3D objects on 2D displays, which provide often only a limited display field of view (DFOV) to the observer. Usually, planar geometric projections, in particular linear perspective projections, are applied, which make use of a straightforward mapping of graphical entities in a 3D view frustum to a 2D image plane. Corresponding to the DFOV introduced for computer screens, the aperture angle of the virtual camera is often denoted as geometric field of view (GFOV) [Kjelldahl and Prime 1995]. Projections of virtual objects on a computer screen are affected by the interplay between the GFOV that is used to render the scene, and the DFOV (see Figure 1). In this context, only little research has been conducted to identify perspective projections that appear realistic to users. Instead, graphics designers and developers often choose GFOVs that vary significantly from the DFOV [Steinicke et al. 2009].	Perception of perspective distortions of man-made virtual objects	NA:NA:NA	2010
Marcelo Cicconet:Paulo Cezar Carvalho	Computer hardware and music softwares have evolved to such a level that, in our days, it is possible to compose high quality music using only a simple laptop equipped with the proper applications.	Playing the QWERTY keyboard	NA:NA	2010
Dane M. Coffey:Daniel F. Keefe	Advances in high-performance (supercomputer) simulations are revolutionizing biomedical research. Figure 1 shows a visualiazation of data from a cutting-edge computational fluid dynamics (CFD) simulation of blood flow through a replacement heart valve. Our collaborators in medical device design hope to use these data as part of a new approach to redesigning the valve hinging mechanism, ultimately improving the longevity of these devices. Biomedical engineers face significant challenges in exploring and understanding these data.	Shadow WIM: a multi-touch, dynamic world-in-miniature interface for exploring biomedical data	NA:NA	2010
Kristian Gohlke:Michael Hlatky:Jörn Loviscach	During extended sessions with a graphical user interface (GUI), users often apply a small set of commands with high frequency. A majority of direct manipulation tasks on a GUI are carried out using the mouse, particularly when keyboard shortcuts are not provided or the user is not familiar with them. Thus, to invoke a certain command, the user is required to aim the mouse pointer at a given onscreen widget and click with the mouse. If the overall task requires a user to click on the same widget repeatedly as part of a sequence of different interleaved micro-tasks, the overall performance suffers, as each point-and-click action requires a considerable amount of time for correctly aiming at the respective control.	TapShot: screenshot snippets as GUI shortcuts	NA:NA:NA	2010
Yoichi Ochiai	Visible Breadboard is the Breadboard like interface which shows voltages of each and every hole by full color LED and enable us to make wiring by tracing with finger tips. Users could insert electrical material into the holes and make a circuit on this device. Users could understand what is happening in the circuit and correct the connections with finger tracing.	The visible electricity device: visible breadboard	NA	2010
Kumiko Kushiyama:Tetsuaki Baba:Kouki Doi:Shinji Sasada	"Thermo-Paradox" is a thermal design display device to use the thermal tactile Physiological illusions that can interactively present patterns of warm and cool temperatures. The technological success of a compact 80-pixel, 9-inch thermal display allows text information to be conveyed by temperature, which has never before been achieved, and the device compactness increases the degree of freedom in presentation methods. We propose this unprecedented tactile expression as a device that can display thermal images that interactively match a visual image, using the tactile Paradoxical sensation produced by the ability to control the temperature of each pixel. (Fig. 1)	Thermal design display device to use the thermal tactile illusions: "Thermo-Paradox"	NA:NA:NA:NA	2010
Pierre Rouanet:Pierre-Yves Oudeyer:David Filliat	Social robots are drawing an increasing interest both in scientific and economic communities and one of the main issues is the need to provide these robots with the ability to interact easily and naturally with humans. We believe that the interaction issues may have a very strong impact on the whole system and should be given more attention. Current research however focus mainly on the the visual perception and/or machine learning issues (see for example Steels and Kaplan [1]). We think that by focusing on the users and on the interface we can help them provide the learning system with very high quality learning examples.	Using mediator objects to easily and robustly teach visual objects to a robot	NA:NA:NA	2010
Patricia Codyre	Information and communication technologies (ITC) offer innovative ways to improve health services and systems. Integration of eHealth into the daily life of rural health-care workers is fast becoming a reality in developing countries. Computermediated communication systems can be used to bridge the gap between doctors in underserved regions with local shortages of medical expertise and medical specialists. eHealth for primary health-care includes applications that directly support disease prevention, patient diagnosis and patient management and care.	Using innovative ehealth interventions in a local health care context	NA	2010
Tanasai Sucontphunt:Zhigang Deng:Ulrich Neumann	Modeling a 3D face toward a specific person is a tedious and painstaking task even for skilled artists. Crafting a 3D cartoon-style or a 3D fiction-creature face to reflect a specific person likeness is even more challenging. For example, creating an ogre that keeps the actor/actress likeness or constructing a 3D avatar that reflects the person identity is an intensive process that involves high artistic skills to convey the human identity on the monster geometries. This work presents an automatic 3D face modeling system that transfers a target 3D human face identity (likeness) to any 3D character faces. This system can be used for a broad variety of a 3D face modeling such as an early stage 3D character face design or a individualized 3D avatars creation.	3D human face identity transfer using deformation gradient	NA:NA:NA	2010
Mohammed Yousef:Ahmed Hashem:Hassan Saad:Amr Gamal:Osama Galal:Khaled F. Hussain	Digital Content Creation (DCC) Applications (e.g. Blender, Autodesk 3ds Max) have long been used for the creation and editing of digital content. Due to current advancement in the field, the need for controlled automated work forced these applications to add support for small programming languages that gave power to artists without diving into many details. With time these languages developed into more mature languages and were used for more complex tasks (driving physics simulations, controlling particle systems, or even game engines). For long, these languages have been interpreted, embedded within the applications, lagging the UIs or incomparable with real programming languages (regarding Completeness, Expressiveness, Extensibility and Abstractions). Two approaches were used to implement those languages. Either build them from scratch (like MaxScript), or use an existing popular language and write a set of extensions to it and embed it (like Blender and Python). In practice, both those solutions suffer, the first method produces languages lacking being real, competitive languages and generally very inefficient, the second method has problems arising from not being dedicated in first place for that kind of applications so, they lack expressiveness facilities (like dedicated constructs) that support that particular domain, also it's very hard to optimize these languages for specific DCC situations.	A scripting language for Digital Content Creation applications	NA:NA:NA:NA:NA:NA	2010
Wael Abdelrahman:Sara Farag	Segmenting 3D meshes into distinct components is vital and necessary for more efficient processing and usability. The smaller segments are usually easier to process and can be associated with with semantics or geometric features. This can be used in 3D parametrization, 3D database creation, animation, deformation transfer and many other 3D graphics applications. However, automating such a process is challenging due to the variety and complexity of the input.	Automated 3D mesh segmentation using 2D footprints	NA:NA	2010
Olusola O. Aina:Jian Jun Zhang	Physically-based facial animation (FA) techniques are notoriously difficult to create, reuse, and art-direct. We address these shortcomings by proposing a rig-builder that automatically generates bony and soft-tissue substructures for any given head model. In an earlier work, [Aina 2009] presented a method for fitting a generic skull to any given head model as a first step toward automated rig-building. Here, we outline work done since, and give an overview of a method for creating muscles of facial expression (mimic muscles), and other soft-tissues in the gap between a given head model and a fitted generic skull.	Automatic muscle generation for physically-based facial animation	NA:NA	2010
Ergun Akleman:Jianer Chen:Yen-Lin Chen:Qing Xing	Any arbitrary twist of the edges of an extended graph rotation system induces a cyclic weaving on the corresponding surface [Akleman et al. 2009]. This recent theoretical result allows us to study generalized versions of textile weaving structures as cyclic weaving structures on arbitrary surfaces. In this work, we extend the study to twill weaving, which is used in fabrics such as denim or gabardine. Biaxial twill is a textile weave in which the weft (filling) threads pass over and under two consecutive warp threads and each row is obtained from the row above it by a shift of 1 unit to the right or to the left. The shift operation creates the characteristic diagonal pattern that makes the twill fabric visually appealing.	Cyclic twill-woven objects	NA:NA:NA:NA	2010
Kazuhiko Yamamoto:Toki Takeda:Ryoichi Ando:Syota Kawano	In this study, we propose a novel system to create vivid animated 3d creature model from user's freeform 2d stroke. The most famous technique that construct 3d model from user's 2d sketch is Teddy system[Igarashi et al. 1999]. Past teddy like system create 3D mesh from user sketched 2d-shape, only what is needed is the stroke. These approaches allow a lot of freedom of creativity to us. However, there are also the cases we need to build the identification or determine the character of contents such like most games. In these cases, although it proper to design or control the contents in advance by the developers, the past methods are too much freely to control their contents. To address this, our approach enables end-users to collaborate with product developer's design without decreasing the freedom of creativity. It creates 3d animated creature from the combination of user's freeform stroke and the primitive models that are created by contents' designer beforehand. The generated creature depend on the user's inspiration, so obtained variations are infinte, but don't lack of the identification of the contents which is designed by the creators. Furthermore, our approach makes us to animate the generated model as if it is alive much easier than past methods.	Darwin's Lake: sketch-based creature creation system enables users to collaborate with contents designers	NA:NA:NA:NA	2010
Jae-Pil Heo:Duksu Kim:Joon-Kyung Seong:Jeong-Mo Hong:Min Tang:Sung-Eui Yoon	Simulating complex phenomena such as fracture requires collision detection (CD) methods to avoid any inter-collisions among deforming models and self-collisions (i.e. intra-collisions) within each deforming model. CD is typically the main computational bottleneck of simulating such complex phenomena.	FASTCD: fracturing-aware stable collision detection	NA:NA:NA:NA:NA:NA	2010
Nuno Goncalves:Ana Catarina Nogueira	Reflectors attract the attention of people since they reflect discontinuous images of the world and often provide unexpected information of a non-direct field of view. This is why reflections still have a lot of research attention in rendering of images in computer graphics, computer vision and optics, amongst other fields.	Faster accurate reflections throught quadric mirrors	NA:NA	2010
Yasuyuki Tomita:Reiji Tsuruno	We present a new method for making wave animation from still water image. In our method, users can control the behavior of wave in the water surface intuitively and interactively. After we simulate the wave using a Spectral Method [Tessendorf 1999], we have the water surface corresponding to the projection system of static images. Previous works for animating water surface. Chuang et al [Chuang and Goldman 2005] proposed a method for generating an animating of picture using displacement mapping and warping, however, those methods are only effective for gentle and calm water surfaces. Contrarily, our method is adaptively used for large scale waves of water height field.	Motion texture animation of water surface	NA:NA	2010
Sara Farag:Wael Abdelrahman	Simulation of the interactions with deformable models is important in many applications such as medical training and tissue engineering. To physically model the 3D object, both the inner and outer segments need to be considered. This implies dealing with different materials and hence different deformation behavior. Thus, a physically-based simulation needs to augment the behavior of embedded materials when the materials are in direct physical contact, and produce a plausible net result in both visual and haptic cues.	Physical modeling of heterogeneous embedded deformable object deformation	NA:NA	2010
James Gregson:Zheng Wang	Reconstructing scanned geometry is an important operation in geometry processing. Volumetric algorithms reconstruct the object volume by transforming range images into global coordinates and using scanline algorithms to build a scalar field that can be isocontoured to obtain the surface [Curless and Levoy 1996].	Rapid surface and volume mesh generation from depth-augmented visual hulls	NA:NA	2010
Craig Reynolds	This poster describes an abstract computation model of the evolution of camouflage in nature. Evolution is represented by genetic programming. Camouflage patterns are represented by procedural texture synthesis. A 2D environment is represented by a supplied photo. A predator is represented by a human's visual perception, interacting through a graphical user interface.	Using interactive evolution to discover camouflage patterns	NA	2010
Ku-Jin Kim:Jung-Eun Lee:Nakhoon Baek	We present an interactive algorithm to compute Voronoi diagrams for protein molecules. In the research area of biochemistry, a molecule is generally represented as a set of 3D spheres with various radii. In this paper, we propose a method to compute Voronoi diagrams for a set of spheres in the 3D discrete domain. We achieved interactive construction of Voronoi diagrams through our adaptive subdivision scheme and massively parallel processing supported by current graphics hardware.	Voronoi diagram computation for protein molecules using graphics hardware	NA:NA:NA	2010
Pedro Santos:Thomas Gierlinger:Rafael Huff:Martin Ritz:André Stork	In the real world, the ratio between full brightness of the sun and complete darkness is in the range of 2.000.000.000:1. However today's projection display technology is limited to contrast ratios of approximately 10.000:1. This hinders a convincing simulation and presentation of lighting effects in professional markets such as car styling, architecture and industrial design. At the same time, High Dynamic Range Imaging (HDRI) has been developed as a new field of research resulting in breakthroughs in image based lighting. What is missing today are interactive visualisation systems that fully support HDR material and light information from the acquisition stage right through the processing stage to the display stage. Current software systems do exist to simulate the effect of light sources in virtual scenes. However, they require specialist training, they are complex to use, they cannot operate in real-time, often requiring modification and recalibration. Current systems also do not support HDRI. This means that not only do they lack the ability to simulate real lighting conditions, e.g. the position and intensity of the sun, cloudcover, but also the behaviour of materials in various light conditions.	A full HDR pipeline from acquisition to projection	NA:NA:NA:NA:NA	2010
Sudarshanram Shetty:Mike Bailey	This paper introduces a layered model for rendering human teeth, to be used in photorealistic rendering of humans for games and animations. While the lighting responses of teeth have been studied in the dental industry ([Joiner 2004], [Brodbelt et al 1981], [Zijp and ten Bosch 1993]) for the production of realistic-looking dentures, to our knowledge this is the first study of its type in computer graphics for the production of realistic renderings. Human teeth exhibit translucency and are characterized by complex light interaction. From a rendering perspective, we make use of a bank of sliders (Figure 2) to vary optical properties at runtime and achieve desired rendering effects, including aging effects as shown in Figure 1. We also make use of hand drawn distribution maps for different layers rather than texture maps to achieve diffuse coloring.	A physical rendering model for human teeth	NA:NA	2010
Shailen Agrawal:Subodh Kumar	Ambient occlusion has been tackled in many different ways to inculcate realism into renderings. Ambient occlusion is a crude approximation to global illumination. But performing a full global illumination in real-time has turned out to be computationally expensive. Combined with local rendering models, ambient occlusion can produce renderings which have increased realism.	Approximate ambient occlusion for dynamic scenes using the GPU	NA:NA	2010
Lesley Northam:Joe Istead:Craig Kaplan	Hand-drawn sketches often depict geometry, colour and texture using loose and roughly drawn lines. Many automated sketching algorithms focus on accurately depicting salient details using pen-and-ink drawings. The approach of Hertzmann et. al. [2000] sketches the contours and silhouettes of 3D meshes, while the interactive algorithm of Kalnins et. al. [2002] renders decorative lines with artistic brushes and suggestions. Other 2D algorithms render Sobel and Canny edges with artistic brushes [Orzan et al. 2007].	Artistic sketching with a painterly rendering algorithm	NA:NA:NA	2010
Borom Tunwattanapong:Abhijeet Ghosh:Paul Debevec	Traditional image-based relighting technique requires capturing a dense set of lighting directions surrounding the object and uses the linearity of light transport property together with the illumination data of the target environment to relight an object [Debevec et al. 2000]. However, this can be a very data intensive process because such datasets typically involve photographing hundreds of lighting directions. It is also difficult to modify or edit the data in post-production environments because the data is high dimensional. Adjustment has to be made in several dimensions in order to add artistic effects to the result. Difficulty in acquisition process is also one of the main problems. The capturing process typically lasts long enough to only be suitable for static objects. In this poster, we present a relighting technique which greatly reduces the number of images required for relighting, and still generate realistic results. We combine spherical harmonics with point lights to achieve efficient image based relighting. Spherical harmonics can efficiently capture smooth low frequency illumination [Ramamoorthi and Hanrahan 2001] while point lights capture high frequency directional illumination. Combining both techniques, we create relighting results which have both low and high frequency illumination data. This technique also benefits the acquisition process by reducing the number of required photographs which results in shorter capture time. In addition, fewer dimensions of the data can potentially simplify modification or editing of reflectance data.	Combining spherical harmonics and point-source illumination for efficient image-based relighting	NA:NA:NA	2010
Kyoji Matsusima:Masaki Nakamura:Sumio Nakahara:Ichiroh Kanaya	James Cameron's Avatar pioneered 3-D films in practical meaning. All audiences of the movie were happy even though their eye points were fixed when they were watching the movie. However, there are certain area that fixed eye points are not acceptable. This is why the authors focus on Computational Holography (see Fig. 1).	Computational holography: the real 3-D by fast wave-field rendering in ultra high resolution	NA:NA:NA:NA	2010
Ole Gulbrandsen	Sharply separating a diffuse surface into a light and dark side often results in unwanted details. Combining normals from the actual surface with the normals from a simplified surface we get better control of the dark side.	Controlling the dark side in toon shading	NA	2010
Tomohito Hattori:Hiroyuki Kubo:Shigeo Morishima	This paper discusses an approach for computing the ambient occlusion by curvature depended approximation of occlusion. Ambient occlusion is widely used to improve the realism of fast lighting simulation. The ambient occlusion is defined as follows.	Curvature depended local illumination approximation of ambient occlusion	NA:NA:NA	2010
Jeong-ho Ahn:Jong-Chul Yoon:In-Kwon Lee	When artists draw a picture of photorealistic scene in an image, they describe only specific parts that represent characteristic features carefully, but they express the parts about less important region roughly. In study about Non-photorealistic rendering, image abstraction research reflects such artist's character. Thus, methods about image abstraction commonly preserve image features and flatten non-feature area. Recently, Kyprianidis et al. [2009] introduced Anisotropic Kuwahara Filtering (AKF) which generates feature preserved image abstraction using the smoothed structure tensor. However since they used only color information to defining anisotropic ratio, different regions that have similar color are conquered by each other unintentionally. Hence, we propose the depth-based AKF method that considers not only color, but also depth to generate image abstraction where boundary feateures are effectively preserved.	Depth-based Anisotropic Kuwahara Filtering	NA:NA:NA	2010
Andrew Cox:Jan Kautz	Real-time applications require simple techniques that enable predictable high performance rendering, but illumination methods for GPUs tend either to be complex and fragile on the one hand or very limited and offering poor visual quality on the other. The addition of a plausible global look to an application's lighting impacts users' perceptions of its realism, and on the spectrum of existing approaches, screen space ambient occlusion (SSAO) lies at the simplest and most predictable end. We have extended screen space ambient occlusion by replacing its depth buffer comparisons with a sampling of a volumetric discretization of the scene, while still evaluating it in an image-space post-process and thus retaining its predictable performance. We show improvements in quality over depth buffer based alternatives at a reasonable additional cost.	Dynamic ambient occlusion from volumetric proxies	NA:NA	2010
Roger Hoang:Steve Koepnick:Joseph D. Mahsman:Matthew Sgambati:Cody J. White:Daniel S. Coming	Real time global illumination (GI) is a difficult and steadily researched area. Advances in the field could potentially benefit virtual reality applications by increasing users' sense of presence. In immersive virtual environments (IVE) like CAVEs, applications must support perspective-corrected stereoscopic rendering. Dmitriev et al. [2004] performed GI in a CAVE using Precomputed Radiance Transfer, which requires a static scene. Mortensen et al. [2007] also performed GI in a CAVE using Virtual Light Fields which did not allow moving lights or geometry. We present our attempt to find GI techniques that support dynamic lights and scene geometry in our 6-sided CAVE-like IVE (DRIVE6). We implemented two separate illumination techniques: GPU photon mapping (GPM) and multiresolution splatting for indirect illumination (MSII). Each technique makes trade-offs between image quality and speed, and appropriate use of each depends on the needs of the application. Anecdotal evidence suggests that these techniques increase the sense of presence, warranting formal study. A user study is planned.	Exploring global illumination for virtual reality	NA:NA:NA:NA:NA:NA	2010
Xuehui Liu:Xiaoguang Hao:Mengcheng Huang:Fang Liu:Mingquan Zhou:Hanqiu Sun:En-Hua Wu	Soft shadow generation is a challenging problem in realistic rendering. Previous methods using shadow map or shadow volume work well for point light sources but are difficult to be extended to area lights. This paper presents a new method for fast soft shadow generation under dynamic area light sources. Our algorithm encodes the depth distribution of the scene into a coarse depth grid in a preliminary pass from the light point of view. In the second pass, the scene is rendered from the camera viewpoint to capture the frontmost layer. During deferred shading, the area light is sampled and the irradiance of each shaded pixel is accumulated along the ray. Experimental results demonstrate high quality soft shadows with interactive performance for dynamic scenes and lighting.	Fast soft shadow by depth peeling	NA:NA:NA:NA:NA:NA:NA	2010
Mohammad Obaid:Ramakrishnan Mukundan:Mark Billinghurst	Facial caricature drawing exaggerates physical face features for a comical effect, and can create an entertaining, humorous, and cartoon-like description of a person's face. Recently, example-based approaches have been introduced to generate facial sketches. Most of these approaches exaggerate the caricature appearance by altering the overall facial shape based on capturing artists' exaggeration-prototypes. Rare attempts have been made to alter and control the facial expressions of the generated caricatures. Moreover, example-based approaches learn how to generate artistic sketch styles through a training phase with prototypes from artist sketches. One of the limitations to these systems is that they require a lot of manual work with a large number of training prototypes drawn by artists. In addition, the final appearance of the caricature can only be limited to the prototypes used in the training phase.	Generating and rendering expressive caricatures	NA:NA:NA	2010
Patrick Cozzi:Frank Stoner	Accurately rendering an ellipsoid is a fundamental problem for virtual globes in GIS and aerospace applications where the Earth's standard reference surface is non-spherical. The traditional approach of tessellating an ellipsoid into triangles and rendering via rasterization has several drawbacks [Miller and Gaskins 2009]. Geodetic grid tessellations oversample at the poles (2a), which leads to shading artifacts and ineffective culling. Tessellations based on subdividing an inscribed platonic solid lead to problematic triangles crossing the International Date Line and poles (2b).	GPU ray casting of virtual globes	NA:NA	2010
Christian Linz:Christian Lipski:Marcus A. Magnor	Multi-image interpolation in space and time has recently received considerable attention. Typically, the interpolated image is synthesized by adaptively blending several forward-warped images. Blending itself is a low-pass filtering operation: the interpolated images are prone to blurring and ghosting artifacts as soon as the underlying correspondence fields are imperfect. We address both issues and propose a multi-image interpolation algorithm that avoids blending. Instead, our algorithm decides for each pixel in the synthesized view from which input image to sample. Combined with a symmetrical long-range optical flow formulation for correspondence field estimation, our approach yields crisp interpolated images without ghosting artifacts.	Multi-image interpolation based on graph-cuts and symmetric optical flow	NA:NA:NA	2010
Lei Ma:Shuangjiu Xiao:Xubo Yang	We presented an multi-interfaces image based method to simulate the refraction and related light effects in real time on a normal graphic card. The multi-interfaces based representation of the refractor is obtained by hiring depth peeling ideas. This leads to significantly better results than two interfaces refraction where only the front and back face of the object was captured.	Multi-interfaces based refractive rendering	NA:NA:NA	2010
Daniel M. Tokunaga:Cléber G. Corrêa:Ricardo Nakamura:Fátima L. S. Nunes:Romero Tori	Spatial visualization of virtual contents appears to be, with the appearance of stereoscopic displays, the next step for increasing immersion in visual output. This kind of visualization can be used to facilitate the understanding of complex informations, like anatomic structures. One approach currently adopted for this purpose is the use of non-photorealisitic rendering (NPR), like proposed by Tietjen, Isenberg and Preim [2005]. However, this NPR style, which conventionally tries to simulate a 2D illustration, fused with the the stereoscopic 3D visualization can break the 3D perception of the virtual contents. This work aims to study the 3D perception of virtual contents represented using NPR techniques, in order to evaluate the influence of NPR in the 3D perception when used with stereoscopic information visualization. The stereoscopic NPR visualization was applied in VIDA, a system for the study of anatomic structures that enables the stereoscopic visualization and interaction with virtual objects [Tori et al. 2009], in order to proceed the user tests, the figure 1c show the system used in the test.	Non-photorealistic rendering in stereoscopic 3D visualization	NA:NA:NA:NA:NA	2010
Martin Eisemann:Elmar Eisemann:Hans-Peter Seidel:Marcus Magnor	We present a system to automatically construct high resolution images from an unordered set of low resolution photos. It consists of an automatic preprocessing step to establish correspondences between any given photos. The user may then choose one image and the algorithm automatically creates a higher resolution result, several octaves larger up to the desired resolution. Our recursive creation scheme allows to transfer specific details at subpixel positions of the original image. It adds plausible details to regions not covered by any of the input images and eases the acquisition for large scale panoramas spanning different resolution levels.	Photo zoom: high resolution from unordered image collections	NA:NA:NA:NA	2010
Mahdi MohammadBagher:Jan Kautz:Nicolas Holzschuch:Cyril Soler	We present an algorithm for computing Percentage-Closer Soft Shadows inside a screen-space rendering loop. Our algorithm is faster than traditional soft shadows based on percentage closer filtering, while providing soft shadows of similar visual quality. It combines naturally with a deferred shading pipeline, making it an ideal choice for video games. This algorithm is not only faster, but allows the use of larger shadow maps without dramatically affecting the rendering speed.	Screen-space Percentage-Closer Soft Shadows	NA:NA:NA:NA	2010
Tom Cuypers:Se Baek Oh:Tom Haber:Philippe Bekaert:Ramesh Raskar	Diffraction is a common phenomenon in nature when dealing with small scale occluders. It can be observed on biological surfaces, such as feathers and butterfly wings, and man--made objects like rainbow holograms. In acoustics, the effect of diffraction is even more significant due to the much longer wavelength of sound waves. In order to simulate effects such as interference and diffraction within a ray--based framework, the phase of light or sound waves needs to be integrated.	WBSDF for simulating wave effects of light and audio	NA:NA:NA:NA:NA	2010
Benjamin P. DeLillo	For more than a decade, rich 3D content on the web has only been available via external, often proprietary, browser plugins. However, a new standard has emerged to change this. WebGL, currently under development by the Khronos Group, is a standard specification for javascript bindings to OpenGL [Khronos 2009]. In September 2009 WebGL support made its way to development builds of Firefox 3.7. Since this time, WebGL has gained greater traction and visibility within developer communities. Although impressive demonstrations of WebGL are available [Vukicevic 2009], we believed that the creation of a development library would help kickstart interest in the creation of new applications.	WebGLU development library for WebGL	NA	2010
Stefan Elsen	WorldSeed introduces a fractal architecture that allows to generate and render full scale planets in real-time. Similar to existing concepts (e.g. by Szeliskit and Terzopoulos [1989] [1]) WorldSeed uses self-similar fractal subdivision to generate landscape detail. By expanding the concepts introduced by Bokeloh and Wand [2006] [2] to use triangular patches rather than rectangles, WorldSeed is capable of generating relatively distortion free spherical surfaces. 64bit integer seeds are used to generate consistent worlds, similar to a concept suggest by Teong Joo Ong et al. [2005] [3]. WorldSeed will be an integral part of a virtual reality application within the CodeVenture research project to teach basic modeling and programming skills to teenagers [4].	WorldSeed	NA	2010
Tai-Wei Kan:Chin-Hung Teng	The field of Augmented Reality (AR) has grown and progressed remarkably in recent years and many useful AR applications have been developed focusing on different areas such as game and education. However, most of these AR systems are designed for closed applications with limited number of users and restricted 3D contents. They are inappropriate for public environment with diverse 3D contents due to the following issues: (1) Limited number of markers. To ensure recognition accuracy, the number of markers is typically limited. (2) Marker registration. Pervasive AR systems often require a registration process each time a new marker is included in the systems. (3) Content management. Traditional AR systems are closed systems with all of their contents stored in a system server. This mechanism is inefficient for public systems with a huge volume of 3D contents. (4) Special Markers. The markers of traditional AR systems are often designed with particular patterns. They are not public or universal patterns.	A framework for multifunctional Augmented Reality based on 2D barcodes	NA:NA	2010
Hiroki Nishino	Markers are widely used for camera-based interaction. Yet, most of the marker tracking methods have considerable limitations in shapes and designs; they are not usually visually meaningful to the users. Such an issue on visually communicative designs can be very important to provide visual cues in a mobile/pervasive environment where a user must first notice a marker and understand its meaning before initiating interaction, unlike in an immersive environment with a head-mounted-display that keeps displaying information on the detected markers.	A shape-free, designable 6-DoF marker tracking method	NA	2010
Tej Tadi:Patrick Salamin:Frederic Vexo:Daniel Thalmann:Olaf Blanke	Over the years, different approaches have been explored to build effective learning methods in virtual reality but the design of effective 3D manipulation techniques still remains an important research problem. To this end, it is important to quantify behavioral and brain mechanisms underlying the geometrical mappings of the body with the environment and external objects, both within the virtual environments (VE), the real world and relative to each other. The successful mapping of such interactions entails the study of fundamental components of these interactions, such as the origin of the visuo-spatial perspective (1PP, 3PP) and how they contribute to the user's performance in the virtual environments. Here, we report data using a novel set-up exposing participants - during free navigation - with a scene view from either 3PP or the habitual first-person perspective (1PP).	Brain activity underlying third person and first person perspective training in virtual environments	NA:NA:NA:NA:NA	2010
Takashi Kajinami:Oribe Hayashi:Takuji Narumi:Tomohiro Tanikawa:Michitaka Hirose	In our research, we aim to construct an interactive exhibition system for museums to convey the background information about its exhibit, which today's museums need. Museums have to preserve their exhibits, and that was a limitation on the exhibition form. They cannot hold a quite new type of exhibition because it might jeopardize their exhibits. So they cannot do more than the exhibition with conventional display cases and panels, in other words a passive exhibition to convey the background information. Digital technologies untie them from this limitation. We can convey the background information effectively using CG in exhibitions, without jeopardizing real exhibits.	Digital display case: the exhibition sysytem for conveying the background information	NA:NA:NA:NA:NA	2010
Megha Davalath:Mat Sanford:Anton Agana:Ann McNamara:Frederic Parke	3D Immersive visualization systems provide a novel platform to present complex datasets and virtual environments (VEs). The objective of the research presented here is to compare user-interaction and performance between two immersive displays: a low-cost, tiled, multi-screen immersive visualization system and a more expensive, continuous, immersive visualization facility. The low cost system is designed using off-the-shelf components and constructed by arranging LCD displays in a tiled hemispherical layout. The expensive system is a Rockwell-Collins semi-rigid, rear projected, continuous curved screen. With the low cost paradigm, seams are introduced into the image where the displays are tiled. We hypothesize that the tiled system presents an equivalent visual experience, despite the seams introduced by connecting the screens. Both systems will be tested through psychophysical experimentation designed to measure aspects of human performance. Proving our hypothesis will impact lower budget organizations, currently unable to afford such displays, by providing an opportunity to work with lower cost immersive visualization systems at no sacrifice to user-experience.	Evaluating performance in immersive displays	NA:NA:NA:NA:NA	2010
Tae-Joon Kim:Yongyoung Byun:Yongjin Kim:Bochang Moon:Seungyong Lee:Sung-Eui Yoon	Ray tracing and collision detection are widely used for providing high-quality visualizations and user interactions. In these algorithms, we need to detect intersecting primitives between two input objects (e.g., a ray and a 3D object in ray tracing and two 3D objects in collision detection). In order to efficiently detect these intersecting primitives, hierarchical traversal and culling by using bounding volume hierarchies (BVHs) are commonly used.	HCCMeshes: hierarchical-culling oriented compact meshes	NA:NA:NA:NA:NA:NA	2010
Takuji Narumi:Takashi Kajinami:Tomohiro Tanikawa:Michitaka Hirose	So far, gustatory information has rarely been studied in relation to computers, even though there are lots of studies on visual, auditory, haptic and olfactory information. This scarcity of research on gustatory information has several reasons. One reason is that gustatory sensation is based on chemical signals, whose functions have not been fully understood yet. Another reason is that perception of gustatory sensation is affected by other factors, such as vision, olfaction, thermal sensation, and memories. Thus, complexity of cognition mechanism for gustatory sensation as described above makes it difficult to build up a gustatory display.	Meta cookie	NA:NA:NA:NA	2010
Tsouknidas Nikolaos:Tomimatsu Kiyoshi	Advancements in mobile technology have recently contributed to the surfacing of viable mobile augmented reality applications. Still, the main problem of mobile AR, as with all implementations of augmented reality, is the accurate and robust registration of the live camera feed and the digital contents (e.g. images, video, 3D models). So far, mobile AR applications make use of GPS and marker technology (fiducials) to solve this problem (e.g. Sekai Camera, Layar, AR-toolkit, Unifeye). The disadvantages are that, firstly, GPS can only guess the position of the device within a 5 to 10 meter radius, is subjected to weather changes, and does not work indoors. Secondly, although marker registration is very accurate, a marker has to be printed and visible by the camera in order to work.	QR-code calibration for mobile augmented reality applications: linking a unique physical location to the digital world	NA:NA	2010
Justin Ehrlich	The application of virtual reality is becoming ever more important as technology reaches new heights allowing virtual environments (VE) complete with global illumination. One successful application of virtual environments is educational interventions meant to treat individuals with autism spectrum disorder (ASD) since VEs induces pretense and presence, without the social fear of failing in the real world. Pretense and presence improves the user's ability to learn social skills and enhances perception-taking capabilities. Because of the lack of conclusive research of improving presence of individuals with ASD in a VE, this study evaluated ways of enhancing presence to improve a VE intervention for individuals with ASD. In the field of computer science visual realism, new research has surfaced linking illumination realism to presence. Since this research was limited to Neurologically Typical (NT) individuals (those without ASD), and because generalization is particularly important to individuals with ASD, this study targeted these individuals. Further, since head mounted displays (HMD) are impractical for widespread delivery of a VE intervention application and since the literature is inconclusive about the effect of VEs without HMDs on presence, this study used standard desktop displays. This work measured the extent to which visual realism induces the presence of a VE intervention, enumerated the specific characteristics of rendering that promote the sense of presence and the ability to generalize, and statistically verified the enhanced outcomes from using these techniques. After conducting a between-group study with 24 individuals with ASD, illumination realism was found to have a positive effect (effect size=0.6) on the presence felt by these individuals. This work contributes to the field of visualization and special education by providing empirical evidence supporting the claim that illumination realism increases the presence felt by users with ASD when interacting with a PVE.	The effect of desktop illumination realism on a user's sense of presence in a virtual learning environment	NA	2010
Woong Choi:Takahiro Fukumori:Kohei Furukawa:Kozaburo Hachimura:Takanobu Nishiura:Keiji Yano	Recently, extensive research has been undertaken on digital archiving of cultural properties in the field of cultural heritage. These investigations have examined the processes of recording and preserving both tangible and intangible materials through the use of digital technologies.	Virtual Yamahoko parade in virtual Kyoto	NA:NA:NA:NA:NA:NA	2010
Santi Fort	This research project is conducted by a consortium of European industrial and academic partners that include companies like: Technicolor, Digital Projection, DTS Europe, Doremi, Mediapro, Creative Wokers (CREW) and Datasat, and research centers: Barcelona Media, Joaneum research, University of Hasselt, University of Reading and Fraunhoffer. It is aimed to research, develop and demonstrate novel forms of compelling entertainment experiences based on new technologies for the capture, production, networked distribution and display of three-dimensional sound and images. 2020 3D Media research project is co-funded by the European Commission under the Seventh Framework Programme (FP7--ICT).	2020 3D media: new directions in immersive entertainment	NA	2010
Ippei Takauchi:Yuta Hara:Hiromu Saito:Ryo Asakura:Motofumi Hattori	It is becoming an important field in computer art to visualize High Dimensional Manifolds. [Banchoff 1990] [Kusabuka and AlgorithmicArt 2006] In this research, we will find the dynamics equation which express deforming motion of the α dimensional manifold r, and visualize many interesting motions of the deformed manifold r(t) by CG animation.	A mathematical model of deforming manifolds and their visualizations by CG animation	NA:NA:NA:NA:NA	2010
Dilip Banerjee:John Gross:Pradeep Reddy Gaddam:Marc Olano:William Hess:Judith Terrill:Terence Griffin:John Hagedorn:John Kelso:Steve Satterfield	In order to move away from the current prescriptive design methods towards performance based methods for the design of structures under fire, we need validated computer models. The next section describes our approach for modeling and analysis.	An integrated interactive visualization and analysis environment to study the impact of fire on building structures	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2010
Toshiki Takeuchi:Takuji Narumi:Kunihiro Nishimura:Tomohiro Tanikawa:Michitaka Hirose	Logging images, voices, etc. of one's daily life is called "lifelog". Recently, it is done research by many researchers because of rapid increasing information with a highly information-oriented society and increasing capacity and lowering price of logging device.	Forecast and visualization of future expenditure with logging and analyzing receipts	NA:NA:NA:NA:NA	2010
S. D. Laycock:M. B. Stocks:S. Hayward	A haptic feedback device enables a user to manipulate three dimensional structures and feel forces contained within complex data-sets such as those resulting from computational biology. However, as the data-set grows in size it becomes difficult to ensure that the user can easily interact with every part of it. One could scale the data-set down to fit into the haptic workspace, however, this could result in important features being missed. A secondary problem is enabling the user to select points efficiently within the three dimensional data set, where the perception of depth can be difficult. In this paper we present novel techniques to rapidly navigate large and complex data-sets with a haptic feedback device, whilst still permitting accurate and fast selection of points in three dimensional space. We have applied these techniques as part of software dedicated to studying the response of biomolecules to externally applied forces using elastic network models.	Navigation and exploration of large data-sets using a haptic feedback device	NA:NA:NA	2010
Shantanu H. Joshi:Ian Bowman:Robin Jennings:David Hasson:Zhizhong Liu:Arthur W. Toga:John D. Van Horn	Large scale neuroimaging data archival protocols are gradually becoming ubiquitous in both research as well as clinical settings. Current user-database interfaces are limited to textual searches and often require data-specific knowledge for performing queries. This is proving to be an obstacle for researchers who wish to obtain a holistic view of the data before designing pilot neuroscientific studies or even formulating statistical hypotheses. Instead of providing a restricted, unidimensional view of the data, we seek to place a multi-dimensional view of the entire neurodatabase at the user's disposal. With the aim of visual navigation of complete neuro-repositories, we introduce the concept of brain meta-spaces. The meta-space models the implicit nonlinear manifold where the neurological data resides, and encodes pair-wise dissimilarities between all individuals in a population. Additionally, the novelty in our approach lies in the user ability to simultaneously view and interact with many brains at once but doing so in a vast meta-space that encodes (dis)similarity in morphometry.	Visual mining of neuro-metaspaces	NA:NA:NA:NA:NA:NA:NA	2010
Kunihiro Nishimura:Jun'ichi Nakano:Tomohiro Tanikawa:Michitaka Hirose	The concept of this study is to collect "ant's eye view" to generate "bird's eye view". When we can collect large number of ant's eye views, we can integrate them and can generate bird's eye view. The idea of this study is an assumption that we can grasp both whole view and situations at multiple places when we can see real-time-report from various points. To achieve this idea, we focus on lifelog technology. Using a wearable computer or small devices and sensors, it is easy to get our daily-life data. We can record our photos, sounds, positions, and so on. It will be lifelog data. When we can collect multiple people's lifelog data, we can utilize them much more. In this study, we propose a visualization method for multiple people's life log data. The lifelog data is uploaded to the server, and the viewer visualizes the data that provides us to see the whole view of the data. We combined position information and sensor information of remote places, and visualized these data.	Visualization of multiple people's lifelog: collecting "Ant's-eye view" to generate "Bird's-eye view"	NA:NA:NA:NA	2010
Pedro Cruz:Penousal Machado	This is an information visualization project that narrates the decline of the British, French, Portuguese and Spanish empires during the 19th and 20th centuries. These empires were the main maritime empires in terms of land area during the referred centuries [Wikipedia]. The land area of the empires and its former colonies is continuously represented in the simulation. The size of the empires varies during the simulation as they gain, or lose, territories. The graphic representation forms were selected to attain a narrative that depicts the volatility, instability and dynamics of the expansion and decline of the empires. Furthermore, the graphic representation also aims at emphasizing the contrast between their maximum and current size, and portraying the contemporary heritage and legacy of the empires.	Visualizing empires decline	NA:NA	2010
Vipin Patel:GBCS Tejaswi Vinnakota:Soumyajit Deb:Manjunatha R. Rao	NA	A 3D animation and effects framework for mobile devices	NA:NA:NA:NA	2014
Tomokazu Ishikawa:Kento Okazaki:Masanori Kakimoto:Tomoyuki Nishita	NA	A video summarization technique of animation products according to film comic format	NA:NA:NA:NA	2014
Masaki Sato:Jun Kobayashi:Tomoaki Moriya:Yuki Morimoto:Tokiichiro Takahashi	NA	An icicle generation model based on the SPH method	NA:NA:NA:NA:NA	2014
Sophie Jörg:Alison E. Leonard:Sabarish Babu:Kara Gundersen:Dhaval Parmar:Kevin Boggs:Shaundra Bryant Daily	NA	Character animation and embodiment in teaching computational thinking	NA:NA:NA:NA:NA:NA:NA	2014
Rubaiat Habib Kazi:Fanny Chevalier:Tovi Grossman:Shengdong Zhao:George Fitzmaurice	NA	DRACO: sketching animated drawings with kinetic textures	NA:NA:NA:NA:NA	2014
Changgu Kang:Leonard Yoon:Min Seok Do:Sung-Hee Lee	NA	Environment-adaptive contact poses for virtual characters	NA:NA:NA:NA	2014
Takuya Kato:Shunsuke Saito:Masahide Kawai:Tomoyori Iwao:Akinobu Maejima:Shigeo Morishima	NA	Example-based blendshape sculpting with expression individuality	NA:NA:NA:NA:NA:NA	2014
Or Avrahamy:Mark Shovman	NA	From pain to happiness: interpolating meaningful gait patterns	NA:NA	2014
Syuhei Sato:Yoshinori Dobashi:Kei Iwasaki:Hiroyuki Ochiai:Tsuyoshi Yamamoto:Tomoyuki Nishita	NA	Generating various flow fields using principal component analysis	NA:NA:NA:NA:NA:NA	2014
Naoya Iwamoto:Shigeo Morishima	NA	Material parameter editing system for volumetric simulation models	NA:NA	2014
Todd Keeler:Robert Bridson	NA	Ocean waves animation using boundary integral equations and explicit mesh tracking	NA:NA	2014
Chie Furusawa:Tsukasa Fukusato:Narumi Okada:Tatsunori Hirai:Shigeo Morishima	NA	Quasi 3D rotation for hand-drawn characters	NA:NA:NA:NA:NA	2014
Felix Herbst:Alexander Schulze	NA	Real-time approximation of convincing spider behaviour	NA:NA	2014
Rina Tanaka:Hiroshi Mori:Fubito Toyama:Kenji Shoji	NA	Real-time avatar motion synthesis by replacing low confidence joint poses	NA:NA:NA:NA	2014
Kakuto Goto:Naoya Iwamoto:Shunsuke Saito:Shigeo Morishima	NA	The efficient and robust sticky viscoelastic material simulation	NA:NA:NA:NA	2014
Gerry Chan:Anthony Whitehead:Avi Parush	We examined the effects of personality pairings on enjoyment within two video game scenarios. It was hypothesized that one would enjoy playing with another person who possesses a personality type similar to their own and in a game scenario that matched their personality type. Results showed that cooperative pairings particularly enjoyed playing together. Implications for game design could include a personality survey to maximize enjoyment.	An evaluation of personality type pairings to improve video game enjoyment	NA:NA:NA	2014
Fuka Nojiri:Yasuaki Kakehi	NA	BelliesWave: color and shape changing pixels using bilayer rubber membranes	NA:NA	2014
Man Zhang:Jun Mitani:Yoshihiro Kanamori:Yukio Fukui	NA	Blocklizer: interactive design of stable mini block artwork	NA:NA:NA:NA	2014
Momoko Okazaki:Ken Nakagaki:Yasuaki Kakehi	NA	metamoCrochet: augmenting crocheting with bi-stable color changing inks	NA:NA:NA	2014
Takaki Kimura:Yasuaki Kakehi	NA	MOSS-xels: slow changing pixels using the shape of racomitrium canescens	NA:NA	2014
Stefan Petrovski:Panos Parthenios:Aineias Oikonomou:Katerina Mania	NA	Music as an interventional design tool for urban designers	NA:NA:NA:NA	2014
Laura K. Murphy:Philip Galanter	NA	Stylized trees and landscapes	NA:NA	2014
Michael Kuetemeyer:Anula Shetty	NA	Time Lens	NA:NA	2014
Akira Nakayasu	NA	Waving tentacles: a system and method for controlling a SMA actuator	NA	2014
Jinsil Hwaryoung Seo:James Storey:John Chavez:Diana Reyna:Jinkyo Suh:Michelle Pine	NA	ARnatomy: tangible AR app for learning gross anatomy	NA:NA:NA:NA:NA:NA	2014
Corrie Colombero:Andy Hunsucker:Pui Mo:Monét Rouse	NA	Augmented reality theater experience	NA:NA:NA:NA	2014
Hikari Tono:Saki Sakaguchi:Mitsunori Matsushita	This paper proposes a method for hiding information inside of an actual object and viewing the hidden information as shadows selectively by rotating the object. Our proposed system creates shadows by using the properties of the polarizing plates and the 1/2 wave-length boards. The goal of our research is to realize a novel method of information hiding.	Basic study on creation of invisible shadows by using infrared lights and polarizers	NA:NA:NA	2014
Tobias Alexander Franke	NA	Interactive relighting of arbitrary rough surfaces	NA	2014
Evangelia Mavromihelaki:Jessica Eccles:Neil Harrison:Hugo Critchley:Katerina Mania	NA	Cyberball3D+ for fMRI: implementing neuroscientific gaming	NA:NA:NA:NA:NA	2014
Yuta Ueda:Karin Iwazaki:Mina Shibasaki:Yusuke Mizushina:Masahiro Furukawa:Hideaki Nii:Kouta Minamizawa:Susumu Tachi	NA	HaptoMIRAGE: mid-air autostereoscopic display for seamless interaction with mixed reality environments	NA:NA:NA:NA:NA:NA:NA:NA	2014
Ann McNamara:Laura Murphy:Conrad Egan	This work in progress is investigating new ways to manage visual clutter in Augmented Reality (AR) applications through the use of eye tracking.	Investigating the use of eye-tracking for view management	NA:NA:NA	2014
Naoki Hashimoto:Akane Tashiro:Hisanori Saito:Satoshi Ogawa	NA	Multifocal projection for dynamic multiple objects	NA:NA:NA:NA	2014
Daisuke Kobayashi:Naoki Hashimoto	NA	Spatial augmented reality by using depth-based object tracking	NA:NA	2014
Chin-chia Tung:Tsung-Hua Li:Hong-Shiang Lin:Ming Ouhyoung	NA	Cage-based deformation transfer using mass spring system	NA:NA:NA:NA	2014
Masahiro Fujisaki:Daiki Kuwahara:Taro Nakamura:Akinobu Maejima:Takayoshi Yamashita:Shigeo Morishima	NA	Facial fattening and slimming simulation considering skull structure	NA:NA:NA:NA:NA:NA	2014
Chen Liu:Yong-Liang Yang:Ya-Hsuan Lee:Hung-Kuo Chu	NA	Image-based paper pop-up design	NA:NA:NA:NA	2014
Michal Smolik:Vaclav Skala	NA	In-core and out-core memory fast parallel triangulation algorithm for large data sets in E2 and E3	NA:NA	2014
Syed Altaf Ganihar:Shreyas Joshi:Shankar Shetty:Uma Mudenagudi	In this paper we propose to address the problem of 3D object categorization. We model the 3D object as a 2D Riemannian manifold and propose metric tensor and Christoffel symbols as a novel set of features. The proposed set of features capture the local and global geometry of 3D objects by exploiting the positional dependence of the features. The categorization of 3D objects is carried out using polynomial kernel SVM classifier. The effectiveness of the proposed framework is demonstrated on 3D objects obtained from different datasets and achieve comparable results.	Metric tensor and Christoffel symbols based 3D object categorization	NA:NA:NA:NA	2014
Ai Mizokawa:Taro Nakamura:Akinobu Maejima:Shigeo Morishima	NA	Photorealistic facial image from monochrome pencil sketch	NA:NA:NA:NA	2014
C. Antonio Sánchez:Sidney Fels	NA	PolyMerge: a fast approach for hex-dominant mesh generation	NA:NA	2014
Xuaner Zhang:Lam Yuk Wong	Without physically trying on a garment, online clothes shoppers are unable to decide the best size or color, and therefore are more likely to purchase clothes that do not fit. A solution to this online fit problem is virtual fitting. Using 3D modeling to help customers visualize how garments look on them requires garment simulation in real time. This paper proposes an innovative approach that utilizes machine learning to meet this real-time requirement and can be applied to virtual fitting systems.	Virtual fitting: real-time garment simulation for online shopping	NA:NA	2014
Koharu Horishita:Syuhei Tsutsumi:Saki Sakaguchi:Mitsunori Matsushita	This paper proposes a novel display that is in harmony with its surroundings. Our proposed display presents information using the different shades of color that can be generated in fur.	A nonluminous display using fur to represent different shades of color	NA:NA:NA:NA	2014
Yuto Uehara:Shinji Mizuno	NA	A virtual 3D photocopy system	NA:NA	2014
Wataru Wakita:Hiromi T. Tanaka	NA	An unconstrained tactile rendering with tablet device based on time-series haptic sensing with bilateral control	NA:NA	2014
Tomohiro Amemiya:Hiroaki Gomi	NA	Buru-Navi3: movement instruction using illusory pulled sensation created by thumb-sized vibrator	NA:NA	2014
Chi-Chiang Huang:Rong-Hao Liang:Liwei Chan:Bing-Yu Chen	NA	Dart-It: interacting with a remote display by throwing your finger touch	NA:NA:NA:NA	2014
Edgar Flores:Sidney Fels	NA	Design of a robotic face for studies on facial perception	NA:NA	2014
Matt Adcock:Bruce Thomas:Chris Gunn:Ross Smith	NA	Enabling physical telework with spatial augmented reality	NA:NA:NA:NA	2014
MHD Yamen Saraiji:Yusuke Mizushina:Charith Lasantha Fernando:Masahiro Furukawa:Youichi Kamiyama:Kouta Minamizawa:Susumu Tachi	NA	Enforced telexistence	NA:NA:NA:NA:NA:NA:NA	2014
Mon-Chu Chen:Yi-Ching Huang:Kuan-Ying Wu	NA	Gaze-based drawing assistant	NA:NA:NA	2014
Sota Suzuki:Haruto Suzuki:Mie Sato	NA	Grasping a virtual object with a bare hand	NA:NA:NA	2014
Naoya Maeda:Maki Sugimoto	NA	Pathfinder vision: tele-operation robot interface for supporting future prediction using stored past images	NA:NA	2014
Michelle Holloway:Cindy Grimm:Ruth West:Ross Sowell	NA	A guided approach to segmentation of volumetric data	NA:NA:NA:NA	2014
Janelle Arita:Jinsil Hwaryoung Seo:Stephen Aldriedge	NA	Soft tangible interaction design with tablets for young children	NA:NA:NA	2014
Kevin Fan:Yuta Sugiura:Kouta Minamizawa:Sohei Wakisaka:Masahiko Inami:Naotaka Fujii	NA	Ubiquitous substitutional reality: re-experiencing the past in immersion	NA:NA:NA:NA:NA:NA	2014
Leonardo Meli:Stefano Scheggi:Claudio Pacchierotti:Domenico Prattichizzo	NA	Wearable haptics and hand tracking via an RGB-D camera for immersive tactile experiences	NA:NA:NA:NA	2014
Ishtiaq Rasool Khan	NA	A new quantization scheme for HDR two-layer encoding schemes	NA	2014
Masahide Kawai:Tomoyori Iwao:Akinobu Maejima:Shigeo Morishima	NA	Automatic deblurring for facial image based on patch synthesis	NA:NA:NA:NA	2014
Yusuke Sekikawa:Sang-won Leigh:Koichiro Suzuki	We propose Coded Lens, a novel system for lensless photography. The system does not require highly calibrated optics, but instead, utilizes a coded aperture for guiding lights. Compressed sensing (CS) is used to reconstruct scene from the raw image obtained through the coded aperture. Experimenting with synthetic and real scenes, we show the applicability of the technique and also demonstrate additional functionality such as changing focus programmatically. We believe this will lead to a more compact, cheaper and even versatile imaging systems.	Coded Lens: using coded aperture for low-cost and versatile imaging	NA:NA:NA	2014
Yong-Ho Lee:In-Kwon Lee	NA	Color correction algorithm based on local similarity of stereo images	NA:NA	2014
Yasin Nazzar:Jonathan Bouchard:James J. Clark	NA	Detection of stereo window violation in 3D movies	NA:NA:NA	2014
Shunya Kawamura:Tsukasa Fukusato:Tatsunori Hirai:Shigeo Morishima	NA	Efficient video viewing system for racquet sports with automatic summarization focusing on rally scenes	NA:NA:NA:NA	2014
Hisataka Suzuki:Rex Hsieh:Akihiko Shirai	NA	ExPixel: PixelShader for multiplex-image hiding in consumer 3D flat panels	NA:NA:NA	2014
Morgane Rivière:Makoto Okabe	The vectorization process transforms an image in the algebraic representation of if its contours. In the case of hand-drawn cartoons, the pen stroke made by the artist defines such a contour. That's why drawing can be interpreted as a series of junctions between nodes, or, in other words, as a topological graph. Many softwares tackle this subject (Adobe Live Trace, Win-Topo...). We will focus here on a program developed in 2013 by a team of researchers from the ETH Zurich and Disney Studios [Noris et al. 2013]. Their method was very efficient for solving ambiguous cases in the drawing, however their implementation was very slow: the vectorization of a 2048x2048 cartoon could need more than 3 minutes of computation. We have improved their method achieving interactive speeds.	Extraction of a cartoon's topology	NA:NA	2014
Takahiro Fuji:Tsukasa Fukusato:Shoto Sasaki:Taro Masuda:Tatsunori Hirai:Shigeo Morishima	NA	Face retrieval system by similarity of impression based on hair attribute	NA:NA:NA:NA:NA:NA	2014
Yuji Aramaki:Yusuke Matsui:Toshihiko Yamasaki:Kiyoharu Aizawa	NA	Interactive segmentation for manga	NA:NA:NA:NA	2014
Judith E. Fan:Daniel Yamins:James DiCarlo:Nicholas B. Turk-Browne	NA	Mapping core similarity among visual objects across image modalities	NA:NA:NA:NA	2014
Jérémy Riviere:Pieter Peers:Abhijeet Ghosh	We present two approaches for acquiring spatially varying reflectance of planar samples using a mobile device. For samples with rough specular BRDF, we propose to employ the back camera and flash pair on any typical mobile device for freeform handheld reflectance acquisition using dense backscattering measurements under flash illumination. For samples with highly specular BRDF, we instead employ a 10" tablet for illuminating the sample with extended illumination while employing the front camera for reflectance acquisition. With this setup, we also exploit the tablet's LCD screen polarization for diffuse-specular separation.	Mobile surface reflectometry	NA:NA:NA	2014
Ding Chen:Ryuuki Sakamoto	NA	Optimizing infinite homography for bullet-time effect	NA:NA	2014
Shunsuke Saito:Ryuuki Sakamoto:Shigeo Morishima	NA	Patch-based fast image interpolation in spatial and temporal direction	NA:NA:NA	2014
Paul Olczak:Jack Tumblin	NA	Photometric camera calibration: precise, labless, and automated with AutoLum	NA:NA	2014
Kouta Takeuchi:Shinya Shimizu:Kensaku Fujii:Akira Kojima:Keita Takahashi:Toshiaki Fujii	NA	Scene-independent super-resolution for plenoptic cameras	NA:NA:NA:NA:NA:NA	2014
Hsin-Wei Wang:Ming-Wei Chang:Hong-Shiang Lin:Ming Ouhyoung	NA	Segmentation based stereo matching using color grouping	NA:NA:NA:NA	2014
Pin-Hua Lu:Chien-Wen Chu:I-Chen Lin	NA	Stereoscopic architectural image inpainting	NA:NA:NA	2014
Joan Sol Roo:Christian Richardt	NA	Temporally coherent video de-anaglyph	NA:NA	2014
Gregor Miller:Sidney Fels	NA	VisionGL: towards an API for integrating vision and graphics	NA:NA	2014
Krzysztof Zieliński:Yi-Ting Tsai:Ming Ouhyoung	NA	Yet another vector representation for images using eikonal surfaces	NA:NA:NA	2014
Liana Manukyan:Antonio Martins:Sophie A. Montandon:Michel Bessant:Michel C. Milinkovitch	NA	A versatile high-resolution scanning system and its application to statistical analysis of lizards' skin colour time-evolution	NA:NA:NA:NA:NA	2014
Kim Jaeyoung:Kang Byongsue:Rhee Shinyoung:Kim Byengwol:Yun Hyeonjin:Sung Junghwan	NA	Bitcube: the new kind of physical programming interface with embodied programming	NA:NA:NA:NA:NA:NA	2014
Shaohui Jiao:Haitao Wang:Mingcai Zhou:Xun Sun:Tao Hong	NA	Efficient sub-pixel based light field reconstruction on integral imaging display	NA:NA:NA:NA:NA	2014
Shunsuke Yoshida	NA	Implementations toward interactive glasses-free tabletop 3D display	NA	2014
Reza Qarehbaghi:Hao Jiang:Bozena Kaminska	NA	Nano-Media: multi-channel full color image with embedded covert information display	NA:NA:NA	2014
Yoichi Ochiai:Takayuki Hoshi:Jun Rekimoto	NA	Pixie dust: graphics generated by levitated and animated objects in computational acoustic-potential field	NA:NA:NA	2014
Hisham Bedri:Micha Feigin:Michael Everett:Ivan Filho:Gregory L. Charvat:Ramesh Raskar	Seeing around corners, in the dark, and through smoke is difficult without specialized sensors[Velten et al. 2012], and so far impossible with a mobile phone. We use an active audio system to sense objects around occluders. Current techniques perform passive localization of sound sources with a microphone array, however, we demonstrate that with one microphone and one speaker pair, such as the ones found in mobile phones, it is possible to sense the specular reflection of silent objects such as mannequins around occluding objects. We demonstrate this technique by sensing a mannequin occluded by a wall.	Seeing around corners with a mobile phone?: synthetic aperture audio imaging	NA:NA:NA:NA:NA:NA	2014
Kazuhisa Yanaka	NA	Simple projection-type integral photography system using single projector and fly's eye lens	NA	2014
F. Ferreira:M. Cabral:O. Belloc:G. Miller:C. Kurashima:R. de Deus Lopes:I. Stavness:J. Anacleto:M. Zuffo:S. Fels	NA	Spheree: a 3D perspective-corrected interactive spherical scalable display	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2014
Jefferson Amstutz:Scott Shaw:Lee Butler	NA	Visually programming GPUs in VSL	NA:NA:NA	2014
Ungyeon Yang:Ki-Hong Kim	NA	Wearable display for visualization of 3D objects at your fingertips	NA:NA	2014
Benjamin Bruneau:Matthias Segui Serera	NA	2D additive and dynamic shadows	NA:NA	2014
Yasunari Ikeda:Issei Fujishiro:Toru Matsuoka	NA	An object space approach to shadowing for hair-shaped objects	NA:NA:NA	2014
Jin-Woo Kim:Jung-Min Kim:MinWoo Lee:Tack-Don Han	NA	Asynchronous BVH reconstruction on CPU-GPU hybrid architecture	NA:NA:NA:NA	2014
George Alex Koulieris:George Drettakis:Douglas Cunningham:Nikolaos Sidorakis:Katerina Mania	NA	Context-aware material selective rendering for mobile graphics	NA:NA:NA:NA:NA	2014
Christoph Müller:Fabian Gärtner	NA	Cross-compiled 3D web applications: problems and solutions	NA:NA	2014
Yusuke Tokuyoshi:Tiago da Silva:Takashi Kanai	NA	Directionality-aware rectilinear texture warped shadow maps	NA:NA:NA	2014
Youyou Wang:Ozgur Gonen:Ergun Akleman	NA	Global illumination for 2D artworks with vector field rendering	NA:NA:NA	2014
Midori Okamoto:Shohei Adachi:Hiroaki Ukaji:Kazuki Okami:Shigeo Morishima	NA	Measured curvature-dependent reflectance function for synthesizing translucent materials in real-time	NA:NA:NA:NA:NA	2014
Ryohei Tanaka:Yuki Morimoto:Hideki Todo:Tokiichiro Takahashi	NA	Parametric stylized highlight for character animation based on 3D scene data	NA:NA:NA:NA	2014
Xi M. Chen:Timothy Lambert:Eric Penner	NA	Pre-integrated deferred subsurface scattering	NA:NA:NA	2014
Pu Wang:Diana Bicazan:Abhijeet Ghosh	We present an approach for realistic rerendering of landscape photographs. We first extract a view dependent depth map from single input landscape images by examining global and local pixel color distributions and demonstrate application in novel viewpoint renderings. For relighting, we assume diffuse reflectance and relight landscapes by estimating the irradiance due the sky in the input photograph. Finally, we also take into account specular reflections on water surfaces which are common in landscape photography and demonstrate relighting of scenes with still water.	Rerendering landscape photographs	NA:NA:NA	2014
Takashi Ejiri:Yuki Morimoto:Tokiichiro Takahashi	NA	Shading approach for artistic stroke thickness using 2D light position	NA:NA:NA	2014
Adrian Jarabo:Julio Marco:Adolfo Munoz:Raul Buisan:Wojciech Jarosz:Diego Gutierrez	NA	Theory and analysis of transient rendering	NA:NA:NA:NA:NA:NA	2014
Lukas Hermanns:Tobias Alexander Franke	NA	Screen space cone tracing for glossy reflections	NA:NA	2014
Andrew K. Ho:Mark A. Nicosia:Angela Dietsch:William Pearson:Jana Rieger:Nancy Solomon:Maureen Stone:Yoko Inamoto:Eiichi Saitoh:Sheldon Green:Sidney Fels	NA	3D dynamic visualization of swallowing from multi-slice computed tomography	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2014
Takefumi Hayashi:Narihito Naoe:Naho Komatsubara:Kenji Sumiya:Kay Yonezawa	NA	Development of cultural capital content using ultra-high resolution images	NA:NA:NA:NA:NA	2014
Abir Al-Hajri:Matthew Fong:Gregor Miller:Sidney Fels	NA	How personal video navigation history can be visualized	NA:NA:NA:NA	2014
Daiki Matsumoto:Yusuke Matsui:Toshihiko Yamasaki:Kiyoharu Aizawa:Takanori Katagiri	NA	IllustStyleMap: visualization of illustrations based on similarity of drawing style of authors	NA:NA:NA:NA:NA	2014
Chuong Nguyen:David Lovell:Rolf Oberprieler:Debbie Jennings:Matt Adcock:Eleanor Gates-Stuart:John La Salle	NA	Natural-color 3D insect models for education, entertainment, biosecurity and science	NA:NA:NA:NA:NA:NA:NA	2014
Donald Madden:Andrew Scanlon:Yunxian Zhou:Tae Eun Choe:Martin Smith	We introduce a distributed augmented reality framework for aerial video which uses CPU/GPU acceleration to correct sensor metadata errors, create a geo-referenced scene model registered to the video, overlay important data, and stream to multiple web clients in order to improve situational awareness during real-time missions.	Real time video overlays	NA:NA:NA:NA:NA	2014
Amol Mahurkar:Ameya Joshi:Naren Nallapareddy:Pradyumna Reddy:Micha Feigin:Achuta Kadambi:Ramesh Raskar	NA	Selective visualization of anomalies in fundus images via sparse and low rank decomposition	NA:NA:NA:NA:NA:NA:NA	2014
Chun-Chia Chiu:Yi-Hsiang Lo:Wei-Ting Ruan:Cheng-Han Yang:Ruen-Rone Lee:Hung-Kuo Chu	Scribble art is a kind of illustrative drawing. Artists use continuous lines to convey the impression of an image or concept of a design. Unlike conventional line drawings such as sketching and hatching that commonly comprise of short and straight line segments, scribble artists aim at depicting the image with long and continuous curves. In this work, we study a typical curve pattern, circular scribble that appears most frequently in the artworks. Circular lines are drawn in either clockwise or counter-clockwise direction with varying radius in the circular scribble arts. The artists delicately trace along a seemingly random path and control the size and orientation of circular line pattern to depict a subject of their artwork. The main challenges lie in producing smooth transition between grayscale levels and preserving dominant image features using continuous loops and intersections of a circular scribble. Thus, the creation of circular scribble art is skill-demanding and time-consuming. In order to facilitate such process, we introduce a systematic approach to automatically synthesize circular scribble arts from images by tracing along a virtual path using solely a single continuous circular scribble with varying radius and orientation. We have tested our approach using a wide range of images and generate visually pleasing circular scribble arts (see Figure 1).	Continuous circular scribble arts	NA:NA:NA:NA:NA:NA	2015
Yuki Koyama:Daisuke Sakamoto:Takeo Igarashi	Exploring various visual designs by tweaking parameters is a common practice when designing digital content. For example, if we want to clean up a photo for use at the top of a web page, we adjust the design parameters---brightness, contrast, saturation, etc.---to explore which combination of parameters provides the best result. Similar situations can be found anywhere in computer graphics applications, such as tweaking shader parameters for game development.	Crowd-powered parameter analysis for computational design exploration	NA:NA:NA	2015
Xiang 'Anthony' Chen:Stelian Coros:Jennifer Mankoff:Scott E. Hudson	One powerful aspect of 3D printing is its ability to extend, repair, or more generally modify everyday objects. However, nearly all existing work implicitly assumes that whole objects are to be printed from scratch. Designing objects as extensions or enhancements of existing ones is a laborious process in most of today's 3D authoring tools. This paper presents a framework for 3D printing to augment existing objects that covers a wide range of attachment options. We illustrate the framework through three exemplar attachment techniques - print-over, print-to-affix, and print-through. We implemented these techniques in Encore, a design tool that supports a range of analysis with visualization for users to explore design options and tradeoffs among these metrics. Encore also generates 3D models for production, addressing issues such as support jigs and contact geometry between the attached part and the original object.	Encore: 3D printed augmentation of everyday objects with printed-over, affixed and interlocked attachments	NA:NA:NA:NA	2015
Kazutaka Nakashima:Takeo Igarashi	Construction of a free-form 3D surface model is still difficult. However, in our point of view, construction of a simple voxel model is relatively easy because it can be built with blocks. Even small children can build a voxel model. We present a method to convert a voxel model into a free-form surface model in order to facilitate construction of surface models.	Extraction of a smooth surface from voxels preserving sharp creases	NA:NA	2015
Chengcheng Tang:Xiang Sun:Alexandra Gomes:Johannes Wallner:Helmut Pottmann	We solve the form-finding problem for polyhedral meshes in a way which combines form, function and fabrication; taking care of user-specified constraints like boundary interpolation, planarity of faces, statics, panel size and shape, enclosed volume, and cost. Our main application is the interactive modeling of meshes for architectural and industrial design. Our approach can be described as guided exploration of the constraint space whose algebraic structure is simplified by introducing auxiliary variables and ensuring that constraints are at most quadratic.	Form-finding with polyhedral meshes made simple	NA:NA:NA:NA:NA	2015
Rukmini Goswami:Tim Tregubov:Lorie Loeb	Attention is a limited resource that intrinsically dictates our perceptions, memories, and behaviors. Further, visuospatial attention correlates highly with user engagement, heart rate, and arousal [El-Nasr et al. 2010]. Artists and interactive game designers strive to capture and direct attention, yet even in the most carefully crafted graphic narratives viewer eye paths -- a proxy for attention -- vary up to 20 percent [McCloud 1994; Jain et al. 2012]. Our aim is to use attentional measures to enrich graphic novel narratives. FrameShift uses eye tracking to measure reader attention and changes text and visual elements later on in the story accordingly. We have built an extensible framework for using attention to introduce perceptual changes in narratives. We use attention as an indirect method for interactions and introduce shiftable frame nodes that change readers' belief states over time.	FrameShift: shift your attention, shift the story	NA:NA:NA	2015
EunJin Kim:Hyeon-Jeong Suk	In the process of editorial design, a harmonious match between a picture and a solid color is often essential to achieve a high quality of a graphical art work. Color as such is a compelling cue to elicit emotional responses and thus can enhance the emotional quality of an image. Tools and methods have been developed to automatize the color selection process, and a noticeable progress has been achieved to extract perceptually dominant colors of an image. However, little attention has been paid to the emotional characteristics of selected colors, and it has been highly relying on the color designers' manual judgments. In this study, we propose a computational method that creates a color that enhances both aesthetic and affective quality of an image, and call it a theme color.	Hue extraction and tone match: generating a theme color to enhance the emotional quality of an image	NA:NA	2015
Azusa Mama:Yuki Morimoto:Katsuto Nakajima	Modeling 3D trees is a major theme in the field of computer graphics [Steven et al. 2012]. However, there has been little research on generating illustrations of trees [Yu-Sheng et al. 2012]. One of the ways to generate them is to render their 3D models. However, it is difficult to obtain the characteristic flat representation of illustrations because of the concentration of foliage in the central part of the tree. We present a system to generate a wide variety of tree illustrations by controlling the density of branches, the shape of canopy, and the overlap of flowers and leaves (Fig. 1).	Interactive tree illustration generation system	NA:NA:NA	2015
Raf Ramakers:Kashyap Todi:Kris Luyten	We present PaperPulse, a design and fabrication approach that enables designers without a technical background to produce standalone interactive paper artifacts by augmenting them with electronics. With PaperPulse designers overlay pre-designed visual elements with interactive widgets and specify functional relations between them using a logic demonstration and recording approach, called Pulsation. When the design is finished, PaperPulse generates layered electronic circuit designs, code that can be deployed on a microcontroller, and instructions for assembly.	PaperPulse: an integrated approach for embedding electronics in paper designs	NA:NA:NA	2015
Yuki Igarashi:Jun Mitani	Patchwork is a well-known type of needlework that involves sewing pieces of fabric into a larger design. It is commonly used to form quilts, but can also be used to make bags, wall hangings, cushion covers, and other items. Larger designs are usually based on repeating patterns, which are built up using different shapes. Professional patchwork designers design original patterns; however, novice users usually use geometric patterns or off-the-shelf patterns for each piece; this is because it is difficult for novices to design patterns while visualizing the resulting larger fabric.	Patchy: an interactive patchwork design system	NA:NA	2015
Shinji Mizuno:Marino Isoda:Rei Ito:Mei Okamoto:Momoko Kondo:Saya Sugiura:Yuki Nakatani:Motomi Hirose	Drawing on a sketchbook is one of the most familiar arts and people of all ages can enjoy it. Thus a lot of CG applications on which a user can create 2D and 3DCG images with drawing operations have been developed [Kondo et al. 2013]. On the other hand, dancing is also familiar to many people. Thus a digital content that is a mixture of drawing and dancing could be very attractive.	Sketch dance stage	NA:NA:NA:NA:NA:NA:NA:NA	2015
Daria Tsoupikova:Scott Rettberg:Roderick Coover:Arthur Nishimoto	We describe the first virtual reality art performance Hearts and Minds: The Interrogations Project developed using a novel method for direct output of the Unity-based virtual reality projects into CAVE2™ [Febretti et al. 2013] environment. This work incorporates original research, technological innovation and creative arts in an adaptation of veterans' testimonies detailing US military interrogations and abuses of prisoners in Iraq during the American counter-insurgency campaign in the early 2000s. It presents a debate focusing on interrogation methods, torture and its consequences, and Post Traumatic Stress Disorder experienced by solders who have participated in such acts. This work was developed at the Electronic Visualization Lab in Chicago through a unique international collaboration between artists, scientists, and researchers from five different Universities. The methods developed for this project allow hands-on education of virtual reality by letting students create their own virtual environments and exhibit them in the CAVE2 quickly.	The battle for hearts and minds: interrogation and torture in the age of war	NA:NA:NA:NA	2015
Rebecca Kleinberger	Our voice is an important part of our individuality but the relationship we have with our own voice is not obvious. We don't hear it the same way others do, and our brain treats it differently from any other sound we hear [Houde et al. 2002]. Yet its sonority is highly linked to our body and mind, and deeply connected with how we are perceived by society and how we see ourselves. The V3 system (Vocal Vibrations Visualization) offers a interactive visualization of vocal vibration patterns. We developed the hexauscultation mask, a head set sensor that measures bioacoustic signals from the voice at 6 points of the face and throat. Those signals are sent and processed to offer a real-time visualization of the relative vibration intensities at the 6 measured points. This system can be used in various situations such as vocal training, tool design for the deaf community, design of HCI for speech disorder treatment and prosody acquisition but also simply for personal vocal exploration.	V3: an interactive real-time visualization of vocal vibrations	NA	2015
Shogo Fukushima:Takeshi Naemura	When we snap strings playing with a CMOS camera, the strings seems to vibrate in a wobbly slow motion pattern. Because a CMOS sensor scans one line of video in sequence, fast moving objects are distorted during the scanning sequence. The morphing and distorting are called a rolling shutter effect, which is considered to be an artistic photographic techniques like strip photography and slit-scan photography. However, the effect can only be seen on a camera finder or a PC screen; the guitar player and audience are quite unlikely to notice it by the naked eye.	Wobble strings: spatially divided stroboscopic effect for augmenting wobbly motion of stringed instruments	NA:NA	2015
Sang-won Leigh:Harshit Agrawal:Pattie Maes	We present a drone-based drawing system where a user's sketch on a desk is transformed across scale and time, and transferred onto a larger canvas at a distance in real-time. Various spatio-temporal transformations like scaling, mirroring, time stretching, recording and playing back over time, and simultaneously drawing at multiple locations allow for creating various artistic effects. The unrestricted motion of the drone promises scalability and a huge potential as an artistic medium.	Z-drawing: a flying agent system for computer-assisted drawing	NA:NA:NA	2015
Katsutoshi Masai:Yuta Sugiura:Masa Ogata:Katsuhiro Suzuki:Fumihiko Nakamura:Sho Shimamura:Kai Kunze:Masahiko Inami:Maki Sugimoto	Facial expression is a powerful way for us to exchange information nonverbally. They can give us insights into how people feel and think. There are a number of works related to facial expression detection in computer vision. However, most works focus on camera-based systems installed in the environment. With this method, it is difficult to track user's face if user moves constantly. Moreover, user's facial expression can be recognized at only a limited place.	AffectiveWear: toward recognizing facial expression	NA:NA:NA:NA:NA:NA:NA:NA:NA	2015
Hugo Talbot:Frederick Roy:Stéphane Cotin	Cryotherapy is a rapidly growing minimally invasive technique for the treatment of different kinds of tumors, such as breast cancer, renal and prostate cancer. Several hollow needles are percutaneously inserted in the target area under image guidance and a gas (usually argon) is then decompressed inside the needles. Based on the Thompson-Joule principle, the temperature drops drown and a ball of ice crystals forms around the tip of each needle. Radiologists rely on the geometry of this iceball (273 K), visible on computer tomographic (CT) or magnetic resonance (MR) images, to assess the status of the ablation. However, cellular death only occurs when the temperature falls below 233 K. The complexity of the procedure therefore resides in planning the optimal number, position and orientation of the needles required to treat the tumor, while avoiding any damage to the surrounding healthy tissues.	Augmented reality for cryoablation procedures	NA:NA:NA	2015
Jun Nishida:Hikaru Takatori:Kosuke Sato:Kenji Suzuki	Understanding and perceiving the world from a child's perspective is a very important key not only to design products and architecture, but also to remind staff who work closely with children, such as hospitals and kindergartens. Ida et al. investigated the universality of devices and architecture in public spaces by recording videos through a hand-held camera positioned at a child's eye level [Ida et al. 2010]. In this study, we propose a novel wearable suit called CHILDHOOD that virtually realizes a child's eye and hand movements by attaching a viewpoint translator and hand exoskeletons (Figure 1a). We hypothesized that virtualizing a child's body size by transforming our own body while preserving embodied interactions with actual surroundings would provide an augmented experience of a child's perspective. This could assist designers in evaluating product accessibility through their own body interactions in real time. In addition, augmented child experience can help staff and parents remember how children feel and touch the world.	CHILDHOOD: wearable suit for augmented child experience	NA:NA:NA:NA	2015
Mark Bolas:Ashok Kuruvilla:Shravani Chintalapudi:Fernando Rabelo:Vangelis Lympouridis:Christine Barron:Evan Suma:Catalina Matamoros:Cristina Brous:Alicja Jasina:Yawen Zheng:Andrew Jones:Paul Debevec:David Krum	There is rapidly growing interest in the creation of rendered environments and content for tracked head-mounted stereoscopic displays for virtual reality. Currently, the most popular approaches include polygonal environments created with game engines, as well as 360 degree spherical cameras used to capture live action video. These tools were not originally designed to leverage the more complex visual cues available in VR when users laterally shift viewpoints, manually interact with models, and employ stereoscopic vision. There is a need for a fresh look at graphics techniques that can capitalize upon the unique affordances that make VR so compelling.	Creating near-field VR using stop motion characters and a touch of light-field rendering	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2015
Liang-Chen Wu:Jia-Ye Li:Yu-Hsuan Huang:Ming Ouhyoung	In making 3D animation with traditional method, we usually edit 3D objects in 3-dimension space on the screen; therefore, we have to use input devices to edit and to observe 3D models. However, those processes can be improved. With the improvement in gesture recognition nowadays, virtual information operations are no longer confined to the mouse and keyboard. We can use the recognized gestures to apply to difficult operations in editing model motion. And for observing 3D model, we would use head tracking from external devices to improve it. It would be easy to observe the interactive results without complicated operation because the system will accurately map the real world head movements.	First-person view animation editing utilizing video see-through augmented reality	NA:NA:NA:NA	2015
Michael Saenz:Joshua Strunk:Kelly Maset:Jinsil Harwaryoung Seo:Erica Malone	We present FlexAR, a kinetic tangible augmented reality [Billinghurst,2008] application for anatomy education. Anatomy has been taught traditionally in two dimensions, particularly for those in non-medical fields such as artists. Medical students gain hands-on experience through cadaver dissection [[Winkelmann, 2007]. However, with dissection becoming less practical, researchers have begun evaluating techniques for teaching anatomy through technology.	FlexAR: anatomy education through kinetic tangible augmented reality	NA:NA:NA:NA:NA	2015
Nazim Haouchine:Alexandre Bilger:Jeremie Dequidt:Stephane Cotin	The considerable advances in Computer Vision for hand and finger tracking made it possible to have several sorts of interactions in Augmented Reality systems (AR), such as object grasping, object translation or surface deformation [Chun and Höllerer 2013]. However, no method has yet considered interaction than involves topological changes of the augmented model (like mesh cutting).	Fracture in augmented reality	NA:NA:NA:NA	2015
Toshiaki Nakasu:Tsukasa Ike:Kazunori Imoto:Yasunobu Yamauchi	In maintenance of electric power control panels, a worker has to do a lot of manual work such as pushing buttons and turning on/off selector switches. Therefore, a hands-free gesture operating system is needed. Tsukada [Tsukada et al. 2002] proposed a gesture operating system using an acceleration sensor and switches. Although it is a simple task to control a home appliance by gesture, users have to use both gesture and switch on/off to perform more complicated tasks such as controlling and recording documents in maintenance work. Therefore, the system becomes complicated. We propose a novel switch-less assist system for maintenance work with a simple structure that recognizes gesture using only an acceleration sensor. Ike [Ike et al. 2014] proposed a hand gesture operating system that enables users to control a TV remotely by adopting "Tapping" as a click signal. The system recognizes tapping by detecting a pulse-like acceleration pattern corresponding to a micro collision generated by tapping. However, it is difficult to recognize tapping because maintenance work includes many micro collisions generated by touching things. We adopt "Tapping & Finger up", i.e., tapping fingers and turning up a finger, gestures that rarely occur in maintenance work, and design a gesture system enabling users to perform maintenance tasks and gesture operation seamlessly. Our system helps users do maintenance work easily and intuitively without interrupting work.	Hands-free gesture operation for maintenance work using finger-mounted acceleration sensor	NA:NA:NA:NA	2015
Bruno Marques:Nazim Haouchine:Rosalie Plantefeve:Stephane Cotin	Minimally invasive surgery (MIS) is a recent surgical technique where the surgeon does not interact directly with the patient's organs. In contrast to open surgery, the surgeon manipulates the organs through instruments inserted in the patient's abdominal cavity while observing the organ from a display showing the video stream captured by an endoscopic camera. While the benefits of MIS for patients are clearly claimed, performing these operations remains very challenging for the surgeons, due to the loss of depth perception caused by this indirect manipulation. To tackle this limitation, the research community suggests to use augmented reality (AR) during the procedure [Haouchine et al. 2013]. The objective towards the use of AR during surgery is to be able to overlay the 3D model of the organ (that can be obtained from a pre-operative scan of the patient) onto the video stream. Surgical AR made considerable advances and reached a certain maturity in the estimation of tumors and vessels localisation. Howerver, very few studies have investigated depth perception and visualization of internal structures [Lerotic et al. 2007], which is considered by surgeons as a very sensitive issue. This study suggests a method to compensate the loss of depth perception while enhancing organ vessels and tumors to surgeons. This method relies on a combination of contour rendering technique and adaptive alpha blending to effectively perceive the vessels and tumors depth. In addition, this technique is designed to achieve real-time to satisfy the requirements of clinical routines, and has been tested on real human surgery.	Improving depth perception during surgical augmented reality	NA:NA:NA:NA	2015
Prashanth Bollam:Eesha Gothwal:GBCS Tejaswi, V:Shailesh Kumar:Soumyajit Deb	The recent boom in computing capabilities of mobile devices has led to the introduction of Virtual Reality into the mobile ecosystem. We demonstrate a framework for the Samsung Gear VR headset that allows developers to create a totally immersive AR & VR experience with no need for interfacing with external devices or cables thereby making it a truly autonomous mobile VR experience. The significant benefits of this system over existing ones are - a fully hands free experience where hands could be used for gesture based input, the ability to use the Head Mounted Display (HMD) sensor for improved head and positional tracking and automatic peer to peer network creation for communication between phones. The most important factor in our system is to provide an intuitive way to interact with virtual objects in AR and VR. And users should be able to switch from AR to VR world and vice versa seamlessly.	Mobile collaborative augmented reality with real-time AR/VR switching	NA:NA:NA:NA:NA	2015
Xing Zhang:Umur A Ciftci:Lijun Yin	In recent years, Virtual Reality (VR) has become a new media to provide users an immersive experience. Events happening in the VR connect closer to our emotions as compared to other interfaces. The emotion variations are reflected as our facial expressions. However, the current VR systems concentrate on "giving" information to the user, yet ignore "receiving" emotional status from the user, while this information definitely contributes to the media content rating and the user experience. On the other hand, traditional controllers become difficult to use due to the obscured view point. Hand and head gesture based control is an option [Cruz-Neira et al. 1993]. However, certain sensor devices need to be worn to assure control accuracy and users are easy to feel tired. Although face tracking achieves accurate result in both 2D and 3D scenarios, the current state-of-the-art systems cannot work when half of the face is occluded by the VR headset because the shape model is trained by data from the whole face.	Mouth gesture based emotion awareness and interaction in virtual reality	NA:NA:NA	2015
Toshikazu Ohshima:Shun Kawaguchi:Yuma Tanaka	The MR Coral Sea is a mixed reality tiny aquarium. The intent of the system is to play with little virtual fish. A player can interact with the virtual fish via Coral Display, an interactive device with multisensory physical feedback. When a player presents his or her hand above the device, fish bait appears on the palm, and the virtual fish come to eat it. The device provides the user with a feeling of spatial existence through illumination and vibration.	MR coral sea evolved: mixed reality aquarium with physical MR displays	NA:NA:NA	2015
Yong Yi Lee:Junho Choi:Yong Hwi Kim:Jong Hun Lee:Moon Gu Son:Bilal Ahmed:Kwan H. Lee	Traditional museums have shown interest in exhibiting a meaningful representation of cultural heritage. However, existing stereotypical exhibition fails to attract the visitors' interest continuously as it provides only static and non-interactive contents and transmits information unilaterally. Recently, high performance measurement techniques have rapidly developed to a degree that allows for the realistic digitization of cultural heritage. Based on this digitized cultural heritage, dynamic and interactive content, such as 3D video and augmented reality, have been made to improve the immersion of visitors. In spite of these attempts, the sense of artificiality is still a challenge because most existing methods demonstrate their content via screen displays.	RiSE: reflectance transformation imaging in spatial augmented reality for exhibition of cultural heritage	NA:NA:NA:NA:NA:NA:NA	2015
Masasuke Yasumoto:Takehiro Teraoka	"Shadow Shooter" is a VR shooter game that uses the "e-Yumi 3D" bow interface and real physical interactive content that changes a 360-degree all-around view in a room into virtual game space (Figure 1). This system was constructed by developing our previous interactive "Light Shooter" content based on "The Electric Bow Interface" [Yasumoto and Ohta 2013]. Shadow Shooter expands the virtual game space to all the walls in a room just as in Jones' "Room Alive" [Jones et al. 2014]; however, it does not require large-scale equipment such as multiple projectors. It only requires the e-Yumi 3D device that consists of a real bow's components added to Willis's interface with a mobile projector [Willis et al. 2013]. Thus, we constructed a unique device for Shadow Shooter that easily changes the 360-degree all-around view into a virtual game space.	Shadow shooter: 360-degree all-around virtual 3d interactive content	NA:NA	2015
Paul Debevec:Greg Downing:Mark Bolas:Hsuen-Yueh Peng:Jules Urbach	Todays most compelling virtual reality experiences shift the users viewpoint within the virtual environment based on input from a head-tracking system, giving a compelling sense of motion parallax. While this is straightforward for computer generated scenes, photographic VR content generally does not provide motion parallax in response to head motion. Even 360° stereo panoramas, which offer separated left and right views, fail to allow the vantage point to change in response to head motion.	Spherical light field environment capture for virtual reality using a motorized pan/tilt head and offset camera	NA:NA:NA:NA:NA	2015
Stefano Scheggi:Leonardo Meli:Claudio Pacchierotti:Domenico Prattichizzo	The complexity of the world around us is creating a demand for novel interfaces that will simplify and enhance the way we interact with the environment. The recently unveiled Android Wear operating system addresses this demand by providing a modern system for all those companies that are now developing wearable devices, also known as "wearables". Wearability of robotic devices will enable novel forms of human intention recognition through haptic signals and novel forms of communication between humans and robots. Specifically, wearable haptics will enable devices to communicate with humans during their interaction with the environment they share. Wearable haptic technology have been introduced in our everyday life by Sony. In 1997 its DualShock controller for PlayStation revolutionized the gaming industry by introducing a simple but effective vibrotactile feedback. More recently, Apple unveiled the Apple Watch, which embeds a linear actuator that can make the watch vibrate. It is used whenever the wearer receives an alert or notification, or to communicate with other Apple Watch owners.	Touch the virtual reality: using the leap motion controller for hand tracking and wearable tactile devices for immersive haptic rendering	NA:NA:NA:NA	2015
Seunghyun Woo:Daeyun An:Jongmin Oh:Gibeom Hong	Due to various features being available in the vehicle such as multimedia, the dashboard has become rather complicated. Therefore, an increased need for HMI(Human Machine Interface) research has arisen in the design creation process. However, there are issues such as design changes occurring even after the design is selected due to the initial evaluation being too simple to cover all of the requirements. Designers do not consider carefully HMI the during sketching phase and issues with designs are discovered too far along in the process. This study suggests an HMI simulation tool system based on projection to pre-evaluate an HMI prior to selecting specifications through virtual function implementation. This system evaluates each function of centerfacia through quantitative criteria such as performance time and distraction time. As a result, the objective of the system is to quickly analyze and validate designs through virtual means and find interface issues with a quantitative method.	WAOH: virtual automotive HMI evaluation tool	NA:NA:NA:NA	2015
Nobuki Yoda:Takeo Igarashi	In 2D game graphics, textures are packed into a single texture called a sprite sheet in order to achieve efficient rendering. The sprite sheet can be compressed to save memory by using various compression methods such as block-based compressions and 16 bpp (bits per pixel) tone reduction. These methods are not without some problems, though. Block-based compressions are GPU-dependent, and high-quality compressions such as ASTC [Nystad et al. 2012] are often unavailable on mobile devices. 16 bpp tone reduction--often used with dithering--can create undesirable noise when it is scaled up (Figure 1c).	Decomposition of 32 bpp into 16 bpp textures with alpha	NA:NA	2015
Shaohui Jiao:Xiaofeng Tong:Eric Li:Wenlong Li	Fur simulation is crucial in many graphic applications since it can greatly enhance the realistic visual effect of virtual objects, e.g. animal avatars. However, due to its high computational cost of massive fur strands processing and motion complexity, dynamic fur is regarded as a challenging task, especially on the mobile platforms with low computing power. In order to support real-time fur rendering in mobile applications, we propose a novel method called textured offset surfaces (TOS). In particular, the furry surface is represented by a set of offset surfaces, as shown in Figure 1(a). The offset surfaces are shifted outwards from the original mesh. Each offset surface is textured with scattering density (red rectangles in Figure 1(a)) to implicitly represent the fur geometry, whose value can be changed by texture warping to simulate the fur animation. In order to achieve high quality anisotropic illumination result, as shown in Figure 1(b), Kajiya/Banks lighting model is employed in the rendering phase.	Dynamic fur on mobile using textured offset surfaces	NA:NA:NA:NA	2015
Kai-Wen Liu:I-Peng Lin:Shih-Wei Sun:Wen-Huang Cheng:Xiaoniu Su-Chu Hsu	Interaction with virtual objects among different devices attracts lots of attention recently. LuminAR [Linder and Maes] was developed for a portable and compact projector-camera system for interactive displaying. THAW [Leigh et al.] was proposed to use a back-facing camera of a smartphone to assist the interactive displaying. RealSense [Lin et al.] was adopted the built-in compass sensor on a mobile device to calibrate the relative position among different mobile devices. However, the complex calibration process of LuminAR [Linder and Maes] and THAW [Leigh et al.] limited the applications. On the other hand, as addressed by the authors, once the users with mobile devices using RealSense [Lin et al.] move larger than 15°, the positioning relationship cannot be kept stable. Therefore, in this paper, a 3D positioning scheme is proposed based on the built-in gyro sensor on a mobile device for effective and intuitive calibration and allow users to freely move the mobile devices with a natural user experience.	G-spacing: a gyro sensor based relative 3D space positioning scheme	NA:NA:NA:NA:NA	2015
Jinhong Park:Minkyu Kim:Sunho Ki:Youngduke Seo:Chulho Shin	Although the mobile industry has recently begun trending towards high quality graphics content, it is still difficult to satisfy this trend due to performance, power and thermal issue of GPU/CPU in mobile application processor.	Half frame forwarding: frame-rate up conversion for tiled rendering GPU	NA:NA:NA:NA:NA	2015
Antoinette Leanna Bumatay:Jinsil Hwaryoung Seo	Stress is physical response that affects everyone in varying degrees. Throughout history, people have developed various practices to help cope with stress. Many of these practices focus on bringing awareness to the body and breath. Studies have shown that mindfulness meditation and paced breathing are effective tools for stress management [Brown, 2005].	Mobile haptic system design to evoke relaxation through paced breathing	NA:NA	2015
Ravi Krishnaswamy	Engineering documents e.g. 'blueprints' are one of the traditional forms of paper based information moving more to the digital realm. With mobile and the evolution of GPUs on mobile, there are tremendous opportunities for applications that view and interact with engineering documents.	Performance and precision: mobile solutions for high quality engineering drawings	NA	2015
Kristian Sons:Felix Klein:Jan Sutter:Philipp Slusallek	Graphics hardware has become ubiquitous: Integrated into CPUs and into mobile devices and recently even embedded into cars. With the advent of WebGL, accelerated graphics is finally accessible from within the web browser. However, still the capabilities of GPUs are almost exclusively exploited by the video game industry, where experts produce specialized content for game engines.	The XML3D architecture	NA:NA:NA:NA	2015
Nobuhisa Hanamitsu:Kanata Nakamura:Mhd Yamen Saraiji:Kouta Minamizawa:Susumu Tachi	Twech is a mobile platform that enables users to share visuo-tactile experience and search other experiences for tactile data. User can record and share visuo-tactile experiences by using a visuo-tactile recording and displaying attachment for smartphone, allows the user to instantly such as tweet, and re-experience shared data such as visuo-motor coupling. Further, Twech's search engine finds similar other experiences, which were scratched material surfaces, communicated with animals or other experiences, for uploaded tactile data by using search engine is based on deep learning that ware expanded for recognizing tactile materials. Twech provides a sharing and finding haptic experiences and users re-experience uploaded visual-tactile data from cloud server.	Twech: a mobile platform to search and share visuo-tactile experiences	NA:NA:NA:NA:NA	2015
Masasuke Yasumoto:Takehiro Teraoka	Various studies have been done on the combined use of mobile devices. Ohta's Pinch [Ohta and Tanaka 2012] and Leigh's THAW [Leigh et al. 2014] are representative studies. However, they have certain limitations; Pinch cannot dynamically correspond to the positional relations of the devices, and THAW cannot recognize the devices' spatial positional relations. We constructed VISTouch so that it does not require a particular kind of external sensor, and it enables multiple mobile devices to dynamically obtain other devices' relative positions in real time. We summarize VISTouch in this paper.	VISTouch	NA:NA	2015
Haruki Sato:Tatsunori Hirai:Tomoyasu Nakano:Masataka Goto:Shigeo Morishima	This paper presents a system that can automatically add a soundtrack to a video clip by replacing and concatenating an existing song's musical bars considering a user's preference. Since a soundtrack makes a video clip attractive, adding a soundtrack to a clip is one of the most important processes in video editing. To make a video clip more attractive, an editor of the clip tends to add a soundtrack considering its timing and climax. For example, editors often add chorus sections to the climax of the clip by replacing and concatenating musical bars in an existing song. However, in the process, editors should take naturalness of rearranged soundtrack into account. Therefore, editors have to decide how to replace musical bars in a song considering its timing, climax, and naturalness of rearranged soundtrack simultaneously. In this case, editors are required to optimize the soundtrack by listening to the rearranged result as well as checking the naturalness and synchronization between the result and the video clip. However, this repetitious work is time-consuming. [Feng et al. 2010] proposed an automatic soundtrack addition method. However, since this method automatically adds soundtrack with data-driven approach, this method cannot consider timing and climax which a user prefers.	A music video authoring system synchronizing climax of video clips and music via rearrangement of musical bars	NA:NA:NA:NA:NA	2015
Ergun Akleman:Siran Liu:Donald House	In this work, we present a simple mathematical approach to art directed shader development. We have tested this approach over two semesters in an introductory level graduate rendering & shading class at Texas A&M University. The students in the class each chose an artist's style to mimic, and then easily created rendered images strongly resembling that style (see Figures 1). The method provides shader developers an intuitive process, giving them a high level of visual control in the creation of stylized depictions.	Art directed rendering & shading using control images	NA:NA:NA	2015
Hiroki Kagiyama:Masahide Kawai:Daiki Kuwahara:Takuya Kato:Shigeo Morishima	In movie and video game productions, synthesizing subtle eye and corresponding head movements of CG character is essential to make a content dramatic and impressive. However, to complete them costs a lot of time and labors because they often have to be made by manual operations of skilled artists.	Automatic synthesis of eye and head animation according to duration and point of gaze	NA:NA:NA:NA:NA	2015
Shugo Yamaguchi:Chie Furusawa:Takuya Kato:Tsukasa Fukusato:Shigeo Morishima	Anime designers often paint actual sceneries to serve as background images based on photographs to complement characters. As painting background scenery is time consuming and cost ineffective, there is a high demand for techniques that can convert photographs into anime styled graphics. Previous approaches for this purpose, such as Image Quilting [Efros and Freeman 2001] transferred a source texture onto a target photograph. These methods synthesized corresponding source patches with the target elements in a photograph, and correspondence was achieved through nearest-neighbor search such as PatchMatch [Barnes et al. 2009]. However, the nearest-neighbor patch is not always the most suitable patch for anime transfer because photographs and anime background images differ in color and texture. For example, real-world color need to be converted into specific colors for anime; further, the type of brushwork required to realize an anime effect, is different for different photograph elements (e.g. sky, mountain, grass). Thus, to get the most suitable patch, we propose a method, wherein we establish global region correspondence before local patch match. In our proposed method, BGMaker, (1) we divide the real and anime images into regions; (2) then, we automatically acquire correspondence between each region on the basis of color and texture features, and (3) search and synthesize the most suitable patch within the corresponding region. Our primary contribution in this paper is a method for automatically acquiring correspondence between target regions and source regions of different color and texture, which allows us to generate an anime background image while preserving the details of the source image.	BGMaker: example-based anime background image creation from a photograph	NA:NA:NA:NA:NA	2015
Siran Liu:Ergun Akleman	In this work, we have developed an approach to include global illumination effects into Chinese Paintings (see Figure 1). Our method provides a robust approach to represent tone and value in a way similar to how Chinese Ink-and-Brush is painted. The method, especially, supports reflection, shadow, atmospheric, depth and weathering effects. Using the method, we can recapture the aesthetic of irregularity in shapes and forms commonly seen in Chinese Painting. We also arrange composition in 3D to obtain multi-camera imagee that matches the compositions in Chinese painting. We also included cinematic lighting aesthetic in 3D Chinese painting to enhance mood and storytelling.	Chinese ink and brush painting with reflections	NA:NA	2015
Jonah Friedman:Andrew C. Jones	In 3D production for commercials, television, and film, ID mattes are commonly used to modify rendered images without re-rendering. ID mattes are bitmap images used to isolate specific objects, or multiple objects, such as all of the buttons on a shirt. Many 3D pipelines are built to provide compositors with ID mattes in addition to beauty renders to allow flexibility.	Fully automatic ID mattes with support for motion blur and transparency	NA:NA	2015
Benjamin Knowles:Oleg Fryazinov	With the increasing quality of real-time graphics it is vital to make sure assets move in a convincing manner otherwise the players immersion can be broken. Grass is an important area as it can move substantially and often takes up a large portion of screen space in games. Animation of grass is a subject to academic research [Fernando 2004; Perbet and Cani 2001] as well as a technology which is implemented in a number of video games. The list includes, but is not limited to, games such as Far Cry 4, Battlefield 4, Dear Esther and Unigine Valley. Comparing video games assets with reality, it can be seen that the current methods have a number of problems which decrease the realism of the resulting grass animation. These problems include: 1) the visible planar nature of grass geometry and 2) problems with the grass movement which include over-connectivity of grass blades in respect to their neighbours, no obvious wind direction and exaggerated swaying motions. In this paper we propose to increase realism of the grass by focusing on its movement. The main contributions of this work are: 1) Distinguishing ambient and directional components of the wind and 2) The method for calculating directional wind by using a grayscale map and wind vector. The grass was implemented with vertex shaders in line with the majority of methods described in academic literature (e.g. [Fernando 2004]) and implemented in modern games.	Increasing realism of animated grass in real-time game environments	NA:NA	2015
Seungbae Bang:Byungkuk Choi:Roger Blanco I Ribera:Meekyoung Kim:Sung-Hee Lee:Junyong Noh	Skeleton-driven animation is a widespread technique, which is frequently used in film and video game productions to animate 3D characters. The process of preparing characters for skeletal animation is referred to as character rigging. Commercial applications such as Maya or 3DS Max provide many tool that support this process, including the 'joint tool' and the 'paint skin weights tool'. Most of these tools are difficult to use for novice users. Even for professional artists, it requires many hours of intensive effort.	Interactive rigging	NA:NA:NA:NA:NA:NA	2015
Simon Pabst:Hansung Kim:Lukáš Polok:Viorela Ila:Ted Waine:Adrian Hilton:Jeff Clifford	Modern digital film production uses large quantities of data captured on-set, such as videos, digital photographs, LIDAR scans, spherical photography and many other sources to create the final film frames. The processing and management of this massive amount of heterogeneous data consumes enormous resources. We propose an integrated pipeline for 2D/3D data registration aimed at film production, based around the prototype application Jigsaw. It allows users to efficiently manage and process various data types from digital photographs to 3D point clouds. A key step in the use of multi-modal 2D/3D data for content production is the registration into a common coordinate frame (match moving). 3D geometric information is reconstructed from 2D data and registered to the reference 3D models using 3D feature matching [Kim and Hilton 2014]. We present several highly efficient and robust approaches to this problem. Additionally, we have developed and integrated a fast algorithm for incremental marginal covariance calculation [Ila et al. 2015]. This allows us to estimate and visualize the 3D reconstruction error directly on-set, where insufficient coverage or other problems can be addressed right away. We describe the fast hybrid multi-core and GPU accelerated techniques that let us run these algorithms on a laptop. Jigsaw has been used and evaluated in several major digital film productions and significantly reduced the time and work required to manage and process on-set data.	Jigsaw: multi-modal big data management in digital film production	NA:NA:NA:NA:NA:NA:NA	2015
Daniel Camozzato:Leandro Dihl:Ivan Silveira:Fernando Marson:Soraia R. Musse	Computer graphics applications require models which are crafted by hand, requiring skill and time. In architectural applications an automated system that converts 2D floor plan images into 3D building models can be used to lower the modeling cost, and in video games procedural algorithms can be used to generate content, including cities, buildings and floor plans. One motivation to generate floor plans for predetermined building exteriors is that building generators often create only a façade without the interior. Another motivation is in planning real-world layouts, often tackled as an optimization problem (e.g. [Merrell et al. 2010] and [Peng et al. 2014]). The state of the art in [Merrell et al. 2010] uses machine learning and stochastic optimization to generate realistic layouts. However, it is unsuitable in our case because the exterior appears as a result of the layout. Our approach generates floor plans using both the building exterior and user requisites as constraints. The proposed method [Camozzato et al. 2015] handles a variety of image styles and building shapes, and the run time remains low (around 1 ms versus a 30 s optimization reported by [Merrell et al. 2010]).	Procedural floor plan generation from building sketches	NA:NA:NA:NA:NA	2015
Chun-Kai Huang:Yi-Ling Chen:I-Chao Shen:Bing-Yu Chen	We introduce an interactive method suitable for retargeting both 3D objects and scenes under a general framework. Initially, an input object or scene is decomposed into a collection of constituent components embraced by corresponding control bounding volumes which capture the intra-structures of the object or the semantic groupings of the objects in the scene. The overall retargeting is accomplished through a constrained optimization by manipulating the control bounding volumes. Without inferring the intricate dependencies between the components, we define a minimal set of constraints that maintain the spatial arrangement and connectivity between the components to regularize valid retargeting results. The default retargeting behavior can then be easily altered by additional semantic constraints imposed by users.	Retargeting 3D objects and scenes	NA:NA:NA:NA	2015
Yu Wang:Marc Olano	We present a framework for modeling solid-fluid phase change. Our framework is physically-motivated, with geometric constraints applied to define rigid dynamics using shape matching. In each simulation step, particle positions are updated using an extended SPH solver where they are treated as fluid. Then a geometric constraint is computed based on current particle configuration, which consists of an optimal translation and an optimal rotation. Our approach differs from methods such as [Carlson et al. 2004] in that we solve rigid dynamics by using a stable geometric constraint [Müller et al. 2005] embedded in a fluid simulator.	Rigid fluid	NA:NA	2015
Katsuhisa Kanazawa:Ryoma Tanabe:Tomoaki Moriya:Tokiichiro Takahashi	Realistic representation of nature scenes is one of the most challenging areas in computer graphics community. There are important factors to synthesize realistic scenes in 3D CG which are decayed materials such as dead trees, weathered statues, rusty metals and so on. We are interested in the methodology for simulating its decaying processes. In this paper, we propose a simple method for rust aging simulation based on a probabilistic cellular automaton model taking into account object's geometries.	Rust aging simulation considering object's geometries	NA:NA:NA:NA	2015
I Chiang:Po-Han Lin:Yuan-Hung Chang:Ming Ouhyoung	Synthesizing competitive interactions between two avatars in a physics-based simulation remains challenging. Most previous works rely on reusing motion capture data. They also need an offline preprocessing step to either build motion graphs or perform motion analysis. On the other hand, an online motion synthesis algorithm [Hämäläinen et al. 2014] can produce physically plausible motions including balance recovery and dodge projectiles without prior data. They use a kd-tree sequential Monte Carlo sampler to optimize the joint angle trajectories. We extend their approach and propose a new objective function to create two-character animations in a close-range combat. The principles of attack and defense are designed according to fundamental theory of Chinese martial arts. Instead of following a series of fixed Kung Fu forms, our method gives 3D avatars the freedom to explore diverse movements and through pruning can finally evolve an optimal way for fighting.	Synthesizing close combat using sequential Monte Carlo	NA:NA:NA:NA	2015
Jaehwan Kim:JongYoul Park:Kyoung Park	Unsupervised matting, whose goal is to extract interesting foreground components from arbitrary and natural background regions without any additional information of the contents of the corresponding scenes, plays an important role in many computer vision and graphics applications. Especially, the precisely extracted object images from the matting process can be useful for automatic generation of large-scale annotated training sets with more accuracy, as well as for improving the performance of a variety of applications including content-based image retrieval. However, unsupervised matting problem is intrinsically ill-posed so that it is hard to generate a perfect segmented object matte from a given image without any prior knowledge. This additional information is usually fed by means of a trimap which is a rough pre-segmented image consisting of three subregions of foreground, background and unknown. When such matting process is applied to object collections in a large-scale image set, the requirement for manually specifying every trimap for each of independent input images can be a serious drawback definitely. Recently, automatic detection of salient object regions in images has been widely researched in computer vision tasks including image segmentation, object recognition and so on. Although there are many different types of proposal measures in methodology under the common perceptual assumption of a salient region standing out its surrounding neighbors and capturing the attention of a human observer, most final saliency maps having lots of noises are not sufficient to take advantage of the consequent computational processes of highly accurate low-level representation of images.	UnAMT: unsupervised adaptive matting tool for large-scale object collections	NA:NA:NA	2015
Naoki Nozawa:Daiki Kuwahara:Shigeo Morishima	A reconstruction of a human face shape from a single image is an important theme for criminal investigation such as recognition of suspected people from surveillance cameras with only a few frames. It is, however, still difficult to recover a face shape from a non-frontal face image. Method using shading cues on a face depends on the lighting circumstance and cannot be adapted to images in which shadows occurs, for example [Kemelmacher et al. 2011]. On the other hand, [Blanz et al. 2004] reconstructed a shape by 3D Morphable Model (3DMM) only with facial feature points. This method, however, requires the pose-wise correspondences of vertices in the model to feature points of input image because a face contour cannot be seen when the facial direction is not the front. In this paper, we propose a method which can reconstruct a facial shape from a non-frontal face image only with a single general correspondence table. Our method searches for the correspondences of points on a facial contour in the iterative reconstruction process, and makes the reconstruction simple and stable.	3D face reconstruction from a single non-frontal face image	NA:NA:NA	2015
Toshihiko Yamasaki:Yusuke Nakano:Kiyoharu Aizawa	3D printing is becoming a more common technology and has a growing number of applications. Although 3D compression algorithms have been studied in the computer graphics (CG) community for decades, the quality of the compressed 3D models are discussed only in the CG space. In this paper, we discuss the relationship between the PSNR of the compressed 3D models and the human perception to the printed objects. We conducted subjective evaluation by inviting 13 people and found that there is a clear linear relationship between them. Such a quality perception model is useful for estimating the printing quality of the compressed 3D models and deciding reasonable compression parameters.	A prediction model on 3D model compression and its printed quality based on subjective study	NA:NA:NA	2015
Toru Kawanabe:Tomoko Hashida	In recent years, there has been rapid development of techniques for superimposing virtual information on real-world scenes and changing the appearance of actual scenes in arbitrary ways. We are particularly interested in means of arbitrarily changing the appearance of real-world scenes without the use of physical interfaces such as glasses or other devices worn by the user. In this paper, we refer to such means as spatial displays. Typical examples of spatial displays include a system that can change the transparency or physical properties of buildings [Rekimoto, 2012] and a system that projects video images [Raskar, 2001]. However, those systems have restrictions such as requiring some kind of physical interface between the user and the scene or not being usable in a well-lit environment. Taking a different approach, we turned our attention to a natural phenomenon referred to as heat haze, in which the appearance of objects is altered by changes in the refractive index of air caused by differences in temperature distribution. We propose the atmoRefractor, a system that can generate and control heat haze on a small scale without an additional physical interface such as lenses. That locally controllable heat haze effect can be used to direct attention by changing the appearance of certain parts of scenes.	atmoRefractor: spatial display by controlling heat haze	NA:NA	2015
Tony Tung	Consumer RGBD sensors are becoming ubiquitous and can be found in many devices such as laptops (e.g., Intel's RealSense) or tablets (e.g., Google Tango, Structure, etc.). They have become popular in graphics, vision, and HCI communities as they enable numerous applications such as 3D capture, gesture recognition, virtual fitting, etc. Nowadays, common sensors can deliver a stream of color images and depth maps in VGA resolution at 30 fps. While the color image is usually of sufficient quality for visualization, depth information (represented as a point cloud) is usually too sparse and noisy for readable rendering.	Augmented dynamic shape for live high quality rendering	NA	2015
Nobuhiko Mukai:Naoki Mita:Youngha Chang	[Hong et al. 2008] proposed a hybrid method of Eulerian grids and Lagrangian particles to represent small-scale bubbles in large-scale water. In the simulation, bubbles rise freely in the water; however, bubble seeds are set at random at the bottom and they disappear as soon as they arrive at the water surface. [Patkar et al. 2013] also proposed a hybrid Lagrangian-Eulerian framework to visualize both small and large scale bubbles. A bubble that exits the spout of a water dispenser flows up in the water with changing its shape; however, they did not simulate the rupture process of bubble. Then, [Mukai et al. 2012] tried a rupture simulation by using MPS method; however, it did not consider the high density ratio of the water to the air so that the bubble ruptured gradually. Therefore, this paper proposes a method of bubble rupture simulation, where a wave is generated after a bubble has ruptured rapidly, by considering the high density ratio of the water to the air.	Bubble rupture simulation by considering high density ratio	NA:NA:NA	2015
Yajie Yan:Tao Ju:David Letscher:Erin Chambers	Medial axis is a classical shape descriptor that is widely used in computer graphics, computer vision, and pattern recognition. Defined elegantly as the locus of points with multiple nearest neighbors on the object boundary, the medial axis preserves both the structure and topology of the object in a compact form - a geometry that has one lower dimension than the object itself.	Burning the medial axis	NA:NA:NA:NA	2015
Hisashi Watanabe:Toshiya Fujii:Tatsuya Nakamura:Tsuguhiro Korenaga	It is a common philosophical question as to whether your blue is the same as my blue. The two-tone striped dress shown in Figure 1, which attracted a lot of attention on the Internet, gave us a clear answer: "No." Some people see the dress as blue and black, whereas others insist it's white and gold. So your blue can be my white. Why is it that people looking at the same picture perceive totally different color combinations?	Color perception difference: white and gold, or black and blue?	NA:NA:NA:NA	2015
Yang Kang:Chi Xu:Shujin Lin:Songhua Xu:Xiaonan Luo:Qiang Chen	Sketching is a natural human practice. With the popularity of multi-touch tablets and styluses, sketching has become a more popular means of human-computer interaction. However, accurately recognizing sketches is rather challenging, especially when they are drawn by non-professionals. Therefore, automatic sketch understanding has attracted much research attention. To tackle the problem, we propose to segment sketch drawings before analyzing the semantic meanings of sketches for the purpose of developing a sketch-based 3D model retrieval system.	Component segmentation of sketches used in 3D model retrieval	NA:NA:NA:NA:NA:NA	2015
Afsaneh Rafighi:Sahand Seifi:Oscar Meruvia-Pastor	This paper presents a novel method for automatic registration of video streams originated from two depth-sensing cameras. The system consists of a sender and receiver, in which the sender obtains the streams from two RGBD sensors placed arbitrarily around a room and produces a unified scene as a registered point cloud. A conventional method to support a multi-depth sensor system is through calibration. However, calibration methods are time consuming and require the use of external markers prior to streaming. If the cameras are moved, calibration has to be repeated. The motivation of this work is to facilitate the use of RGBD sensors for non-expert users, so that cameras need not to be calibrated, and if cameras are moved, the system will automatically recover the alignment of the video streams. DeReEs [Seifi et al. 2014], a new registration algorithm, is used, since it is fast and successful in registering scenes with small overlapping sections.	Continuous and automatic registration of live RGBD video streams with partial overlapping views	NA:NA:NA	2015
Michelle Holloway:Tao Ju:Cindy Grimm	In clinical practice, when a subject is imaged (i.e. CT scan or MRI) the result is a 3D image of volumetric data. In order to study the organ, bone, or other object of interest, this data needs to be segmented to obtain a 3D model that can be used in any number of down stream applications. When used for treatment planning these segmentations need to not only be accurate but also produced quickly to avoid health risks. Automatic segmentation methods are becoming more reliable but many experts in the scientific community still rely on time consuming manual segmentation.	Contour guided surface deformation for volumetric segmentation	NA:NA:NA	2015
Peihong Guo:Ergun Akleman:He Ying:Xiaoning Wang:Wei Liu	In this work, we present some of the unexpected observations resulted from our recent research. We, recently, needed to identify a small number of important critical points, i.e. minimum, maximum and saddle points, on a given manifold mesh surface. All critical points on a manifold triangular mesh can be identified using discrete Gaussian curvature, which is given as ki = 2π − Σj θi,j where ki is vertex defect (the discrete Gaussian curvature) of the vertex i and θi,j is the corner of the vertex in the triangle j. A very useful property coming with vertex defect is the discrete version of Gauss-Bonnet theorem: the sum of all vertex defects is always constant as Σi ki = 2π(2−2g) where g is the genus of the mesh. Any vertex with a non-zero vertex defect is really an critical point of the surface. However, identification of interesting critical points is hard with vertex defect alone. As it can be seen in Figure 1(a), even we ignore vertex defects that are small, too many vertices are still chosen and this information is not really useful to make any conclusion of the shape of the surface.	Critical points with discrete Morse theory	NA:NA:NA:NA:NA	2015
Nahomi Maki:Kazuhisa Yanaka	Various colors, such as in a prism, are observed in properly cut diamond even under white light because of dispersion. Properly-cut diamond brings about scintillation when viewing angle is changed, because total reflection inside a diamond tends to occur frequently due to the large refractive index. Moreover, strong rainbow colors are seen because of high dispersion ratio.	Display of diamond dispersion using wavelength-division rendering and integral photography	NA:NA	2015
Slim Ouni:Guillaume Gris	One main concern of audiovisual speech research is the intelligibility of audiovisual speech (i.e., talking head). In fact, lip reading is crucial for challenged population as hard of hearing people. For audiovisual synthesis and animation, this suggests that one should pay careful attention to modeling the region of the face that participates actively during speech. Above all, a facial animation system needs extremely good representations of lip motion and deformation in order to achieve realism and effective communication.	Dynamic realistic lip animation using a limited number of control points	NA:NA	2015
Byeongjun Choi:Woong Seo:Insung Ihm	In the ray-tracing community, the surface-area heuristic (SAH) has been employed as a de facto standard strategy for building a high-quality kd-tree. Aiming to improve both time and space efficiency of the conventional SAH-based kd-tree in ray tracing, we propose to use an extended kd-tree representation for which an effective tree-construction algorithm is provided. Our experiments with several test scenes revealed that the presented kd-tree scheme significantly reduced the memory requirement for representing the tree structure, while also increasing the overall frame rate for rendering.	Enhancing time and space efficiency of kd-tree for ray-tracing static scenes	NA:NA:NA	2015
Hisataka Suzuki:Rex Hsieh:Ryotaro Tsuda:Akihiko Shirai	In recent years, 3D technology has become so widespread that the technology alone no longer fascinates the viewers. To achieve further technical innovations on display experience, we should explore the limitations of 3D devices.	ExPixel FPGA: multiplex hidden imagery for HDMI video sources	NA:NA:NA:NA	2015
Yoichi Ochiai:Kota Kumagai:Takayuki Hoshi:Jun Rekimoto:Satoshi Hasegawa:Yoshio Hayasaki	We envision a laser-induced plasma technology in general applications for public use. If laser-induced plasma aerial images were made available, many useful applications such as spatial aerial AR, aerial user interfaces, volumetric images could be produced. This would be a highly effective display for the expression of three-dimensional information. Volumetric expression has considerable merit because the content scale corresponds to the human body; therefore, this technology could be usefully applied to wearable materials and spatial user interactions. Further, laser focusing technology can add an additional dimension to conventional projection technology, which is designed for surface mapping, while laser focusing technology is capable of volumetric mapping. This technology can be effectively used in real-world-oriented user interfaces.	Fairy lights in femtoseconds: aerial and volumetric graphics rendered by focused femtosecond laser combined with computational holographic fields	NA:NA:NA:NA:NA:NA	2015
Jérémy Levallois:David Coeurjolly:Jacques-Olivier Lachaud	During a snowfall, the snow crystals accumulate on the ground and gradually form a complex porous medium constituted of air, water vapour, ice and sometimes liquid water. This ground-lying snow transforms with time, depending on the physical parameters of the environment. The main purpose of the digitalSnow project is to provide efficient computational tools to study the metamorphism of real snow microstructures from 3D images acquired using X tomography techniques. We design 3D image-based numerical models than can simulate the shape evolution of the snow microstructure during its metamorphism. As a key measurement, (mean) curvature of snow microstructure boundary plays a crucial role in metamorphosis equations (mostly driven by mean curvature flow). In our previous work, we have proposed robust 2D curvature and 3D mean and principal curvatures estimators using integral invariants. In short, curvature quantities are estimated using a spherical convolution kernel with given radius R applied on point surfaces [Coeurjolly et al. 2014]. The specific aspect of these estimators is that they are defined on (isothetic) digital surfaces (boundary of shape in Z3). Tailored for this digital model, these estimators allow us to mathematically prove their multigrid convergence, i.e. for a class of mathematical shapes (e.g. C3-boundary and bounded positive curvature), the estimated quantity converges to the underlying Euclidean one when shapes are digitized on grids with gridstep tending to zero. In this work, we propose to use the radius R of our curvature estimators as a scale-space parameter to extract features on digital shapes. Many feature estimators exist in the literature, either on point clouds or meshes ("ridge-valley", threshold on principal curvatures, spectral analysis from Laplacian matrix eigenvalues, . . . ). In the context of objects in Z3 and using our robust curvature estimator, we define a new feature extraction approach on which theoretical results can be proven in the multigrid framework.	Feature extraction on digital snow microstructures	NA:NA:NA	2015
A. Andreadis:R. Gregor:I. Sipiran:P. Mavridis:G. Papaioannou:T. Schreck	The problem of object restoration from eroded fragments where large parts could be missing is of high relevance in archaeology. Manual restoration is possible and common in practice but it is a tedious and error-prone process, which does not scale well. Solutions for specific parts of the problem have been proposed but a complete reassembly and repair pipeline is absent from the bibliography. We propose a shape restoration pipeline consisting of appropriate methods for automatic fragment reassembly and shape completion. We demonstrate the effectiveness of our approach using real-world fractured objects.	Fractured 3D object restoration and completion	NA:NA:NA:NA:NA:NA	2015
Caigui Jiang:Chengcheng Tang:Jun Wang:Johannes Wallner:Helmut Pottmann	In freeform architecture and fabrication aware design, repetitive geometry is a very important contribution to the reduction of production costs. This poster addresses two closely related geometric rationalizations of freeform surfaces with repetitive elements: freeform honeycomb structures defined as torsion-free structures where the walls of cells meet at 120 degrees, and Lobel frames formed by equilateral triangles. There turns out to be an interesting duality between these two structures, and this poster discusses the geometric relation, computation, modeling as well as applications of them.	Freeform honeycomb structures and lobel frames	NA:NA:NA:NA:NA	2015
Antoine Toisoul:Abhijeet Ghosh	We present a novel approach for image based relighting using the lighting controls available in a regular room. We employ individual light sources available in the room such as windows and house lights as basis lighting conditions. We further optimize the projection of a desired lighting environment into the sparse room lighting basis in order to closely approximate the target lighting environment with the given lighting basis. We achieve plausible relit results that compare favourably with ground truth relighting with dense sampling of the reflectance field.	Image based relighting using room lighting basis	NA:NA	2015
Daniel Rakita:Tomislav Pejsa:Bilge Mutlu:Michael Gleicher	Motion-captured performances seldom include eye gaze, because capturing this motion requires eye tracking technology that is not typically part of a motion capture setup. Yet having eye gaze information is important, as it tells us what the actor was attending to during capture and it adds to the expressivity of their performance.	Inferring gaze shifts from captured body motion	NA:NA:NA:NA	2015
Hiroki Yamamoto:Hajime Kajita:Hanyuool Kim:Naoya Koizumi:Takeshi Naemura	In design process and medical visualization, e.g. CT/MRI cross-sectional images, exterior and interior images can help users to understand the overall shape of volumetric objects. For this purpose, displays need to provide both vertical and horizontal images at the same time. To display cross-sectional images, an LCD display [Cassinelli et al. 2009] and image projection [Nagakura et al. 2006] have been proposed. Although these displays could show internal images of volumetric objects, seamless crossing of internal and external images cannot be realized since the images are limited to physical displays.	Mid-air plus: a 2.5 D cross-sectional mid-air display with transparency control	NA:NA:NA:NA:NA	2015
Takuya Kato:Akira Kato:Naomi Okamura:Taro Kanai:Ryo Suzuki:Yuko Shirai	Trees have been a pillar of our lives not just for human but for all the species living in the earth. Despite of its blessings for our lives, the heaps of problems around forestry have not been solved. One of the major problems in this field is that most of the forest are not been sorted into an organized database. Detailed natural data have never been provided even in famous map applications, Google earth for instance, induced from its difficulty. The forest database has been demanded in many regions as it provides beneficial information for both industrial and environmental aspects. It even helps many divisions such as CG animations to simulate not only a tree itself but also the mountain or the forest as a whole depending on given natural conditions.	Musasabi: 2D/3D intuitive and detailed visualization system for the forest	NA:NA:NA:NA:NA:NA	2015
Beibei Wang:Xiangxu Meng:Tamy Boubekeur	Point-Based Global Illumination (PBGI) [2008] is a popular rendering method in special effects and motion picture productions. This algorithm provides a diffuse global illumination solution by caching radiance in a mesh-less hierarchical data structure during a pre-process, while solving for visibility over this cache, at rendering time and for each receiver, using microbuffers, which are localized depth and color buffer inspired from real time rendering environments. As a result, noise free ambient occlusion, indirect soft shadows and color bleeding effects are computed efficiently for high resolution image output and in a temporally coherent fashion. We propose an evolution of this method to address the case of non-diffuse inter-reflections and refractions using wavelets instead of spherical harmonics (see Fig. 1). We also propose a new importance-driven adaptive microbuffer model to capture accurately incoming radiance at a point. Furthermore, we evaluate outgoing radiance using a fast wavelet radiance product, containing the memory footprint by encoding hierarchically the wavelets tree.	Non-diffuse effects for point-based global illumination	NA:NA:NA	2015
Hajime Kajita:Naoya Koizumi:Takeshi Naemura	Mid-air imaging has the advantage of expression along the depth direction. For example, MARIO [1], a mid-air display, can form an image in the depth range of 30 cm by physically moving the light source display. Multi-layered mid-air images can be displayed at various depths, but such multi-layered images are transparent and experience color mixture due to the addition of light from the light source displays. It is difficult to see the front of transparent images because they have no occlusion expression.	OpaqueLusion: opaque mid-air images using dynamic mask for occlusion expression	NA:NA:NA	2015
Christian Hafner:Przemyslaw Musialski:Thomas Auzinger:Michael Wimmer:Leif Kobbelt	Keyboard percussion instruments such as xylophones and glockenspiels are composed of an arrangement of bars. These are varied in some of their geometrical properties---typically the length---in order to influence their acoustic behavior. Most instruments in this family do not deviate from simple geometrical shapes, since designing the natural frequency spectrum of complex shapes usually involves a pain-staking trial-and-error process and has been reserved to gifted artisans or professional manufacturers.	Optimization of natural frequencies for fabrication-aware shape modeling	NA:NA:NA:NA:NA	2015
Junichi Sugita:Tokiichiro Takahashi	Many people have been familiar with subtractive color model based on pigment color compositing since their early childhood. However, the RGB color space is not comprehensible for children due to additive color compositing. In the RGB color space, the resulting mixture color is often different from colors viewer expected. CMYK is a well-known subtractive color space, but its three primal colors are not familiar. Kubelka-Munk model (KM model in short) simulates pigment compositing as well as paint-like appearance by physically-based simulation. However, it is difficult to use KM model because of many simulation parameters.	Paint-like compositing based on RYB color model	NA:NA	2015
Naoki Hashimoto:Koki Kosaka	We propose a photometric compensation for projecting arbitrary images on practical surfaces of our everyday life. Although many previous proposals have achieved fine compensation at their experimental environments [Nayar et al. 2003], they cannot support practical targets including high-contrast texture. In order to adapt to such situation, we need a time-consuming iterative processing with camera feedback. Even though the iterative processing is applied, we cannot obtain fine compensation because no camera pixels of a projector-camera system (procam) correspond perfectly to the pixels of the projector [Mihara et al. 2014].	Photometric compensation for practical and complex textures	NA:NA	2015
Takefumi Hiraki:Issei Takahashi:Shotaro Goto:Shogo Fukushima:Takeshi Naemura	Forming images by using a swarm of mobile robots has emerged as a new platform for computer entertainment. Each robot has colored lighting, and the swarm represents various abstract patterns by using the lighting and the locomotion.	Phygital field: integrated field with visible images and robot swarm controlled by invisible images	NA:NA:NA:NA:NA	2015
Ari Rapkin Blenkhorn	The glory is a colorful atmospheric phenomenon which resembles a small circular rainbow on the front surface of a cloudbank. It is most frequently seen from aircraft when the observer is directly between the sun and the clouds. Glories are also sometimes seen by skydivers looking down through thin cloud layers. They are always centered around the shadow of the observer's head (or camera).	Real-time rendering of atmospheric glories	NA	2015
Hiroyuki Kubo:Kohe Tokoi:Yasuhiro Mukaigawa	To synthesize realistic translucent materials in computer graphics, it is necessary to simulate the effect of subsurface scattering. In previous works, several methods are proposed for rendering such materials in real-time. The screen space subsurface scattering (SSSS) is developed by Jimenez et al. [2009], yet the speed of rendering is not very practical for low-end computational environment, because screen space techniques require huge number of texture samplings. We previously propose a curvature-based shading method [Kubo et al. 2010] which approximates the effect of subsurface scattering according to the curvature. Since the curvature is determined by the surface shape of neighbors, it is not able to compute the effect of scattering light from the behind of the object. In this paper, we propose a novel shading method depending on the translucency magnitude which represents the significance of the subsurface scattering effect. According to the translucency magnitude, we modulate the reflectance to imitate the effect of subsurface scattering. Since this modulation is very simple to compute, we are able to render translucent materials in real-time not only in high-end workstations but also low-end mobile devices.	Real-time rendering of subsurface scattering according to translucency magnitude	NA:NA:NA	2015
Kang Zhang:Wuyi Yu:Mary Manhein:Warren Waggenspack:Xin Li	Geometric restoration that composes 3D fragmented pieces into the original complete object is an important computer graphics and geometric processing problem. Automatic and effective restoration has applications in many fields such as archeological reconstruction, digital heritage archiving, forensic evidence processing, to name a few. For example, archaeologists reconstruct ceramic fragments (sherds) into complete pots in order to analyze the information of the ancient society. Forensic scientists reassemble skull fragments into complete skull for face reconstruction and body identification. In both of these problems we need to solve a composition of digitized thin-shell fragments with different shapes, sizes, and resolutions. This problem remains very challenging.	Reassembling 3D thin shells using integrated template guidance and fracture region matching	NA:NA:NA:NA:NA	2015
Keita Sekijima:Hiroya Tanaka	Digital materials are discrete elements such as LEGO Blocks that it can be a kind of reconfigurable 3D matters. There are two advantages of using digital material rather than a continuous material. Firstly, it is easy to change the form after shaping by assembling and disassembling the elements. Secondly, There is never that the error of the part impacts the whole form in the shaping because the elements can be connected exactly by the joint system. There are many researches of digital material focus on the modular connection by press fitting or bonding. Such a digital material can't be assembled and disassembled smoothly after shaped. In our research, we designed the digital material "Kelvin Block" (figure 1a) that specialized in smoothly reconfiguring, and we developed the machine "3D Assembler" (figure 1b) to arrange Kelvin Blocks automatically. The size of Kelvin Block is 40mmx40mmx40mm that is optimized to the volume of the joint system.	Reconfigurable three-dimensional prototype system using digital materials	NA:NA	2015
Francisco Inácio:Jan P. Springer	Maintaining a high steady frame rate is an important aspect in interactive real-time graphics. It is mainly influenced by the number of objects and the number of lights to be processed for a 3d scene. The upper-bound effort for rendering a scene is then defined by the number of objects times the number of lights, i. e. O(NO · NL). Deferred shading reduces this upper bound to the number of objects plus the number of lights, i. e. O(NO + NL), by separating the rendering process into two phases: geometry processing and lighting evaluation. The geometry processing rasterizes all objects but only retains visible fragments in a G-Buffer for the current viewpoint. The lighting evaluation then only needs to process those surviving fragments to compute the final image (for the current viewpoint). Unfortunately, this approach not only trades computational effort for memory but also requires the re-creation of the G-Buffer every time the viewpoint changes. Additionally, transparent objects cannot be encoded into a G-Buffer and must be separately processed. Post-rendering 3d warping [Mark et al. 1997] is one particular technique that allows to create images from G-Buffer information for new viewpoints. However, this only works with sufficient fragment information. Objects not encoded in the G-Buffer, because they were not visible from the original viewpoint, will create visual artifacts at discontinuities between objects. We propose fragment-history volumes (FHV) to create novel viewpoints from a discrete representation of the entire scene using current graphics hardware and present an initial performance comparison.	Reducing geometry-processing overhead for novel viewpoint creation	NA:NA	2015
Fumiya Narita:Shunsuke Saito:Takuya Kato:Tsukasa Fukusato:Shigeo Morishima	Dressing virtual characters is necessary for many applications, while modeling clothing is a significant bottleneck. Therefore, it has been proposed that the idea of Garment Transfer for transfer-ring clothing model from one character to another character [Brouet et al. 2012]. In recent years, this idea has been extended to be applicable between characters in various poses and shapes [Narita et al. 2014]. However, texture design of clothing is not preserved in their method since they deform the source clothing model to fit the target body (see Figure 1(a)(c)).	Texture preserving garment transfer	NA:NA:NA:NA:NA	2015
Paul Kilgo:Jerry Tessendorf	A Monte Carlo multiple scattering technique for participating media is extended. Validation against an experimentally well-studied optics problem is discussed. Designing initial paths for a numerical integration of Feynman path integrals is posed. A plot of the resulting integration is discussed.	Toward validation of a Monte Carlo rendering technique	NA:NA	2015
Caleb Brose:Martin Thuo:Jeremy W. Sheaffer	We present a system for tracking the movement and deformation of drops of water in free fall and collision. Our data comes from a high-speed camera which records 60,000 frames per second. The data is noisy, and is compromised by an unfortunate camera angle and poor lighting which contribute to caustics, reflections, and shadows in the image. Given an input video, we apply techniques from image processing, computer vision and computational geometry to track the the droplet's position and shape. While our tool could monitor the movement of transparent fluids in a more general environment, our data specifically depicts water colliding with hydrophobic materials. The output of our processing is used by materials scientists to better our understanding of the interactions between water and hydrophobic surfaces. These interactions have direct application in the materials engineering of next generation printing technologies.	Tracking water droplets under descent and deformation	NA:NA:NA	2015
Xueming Yu:Shanhe Wang:Jay Busch:Thai Phan:Tracy McSheery:Mark Bolas:Paul Debevec	High-end facial performance capture solutions typically use head-mounted camera systems which provide one or more close-up video streams of each actor's performance. These provide clear views of each actor's performance, but can be bulky, uncomfortable, get in the way of sight lines, and prevent actors from getting close to each other. To address this, we propose a virtual head-mounted camera system: an array of cameras placed around around the performance capture volume which automatically track zoomed-in, sharply focussed, high-resolution views of the each actor's face from a multitude of directions. The resulting imagery can be used in conjunction with body motion capture data to derive nuanced facial performances without head-mounted cameras.	Virtual headcam: pan/tilt mirror-based facial performance tracking	NA:NA:NA:NA:NA:NA:NA	2015
