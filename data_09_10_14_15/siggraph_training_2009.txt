Bing-Yu Chen:Shih-Chiang Dai:Shuen-Huei Guan:Tomoyuki Nishita	In this extended abstract, we present a system that allows the user to animate character images in 3D space by applying an existed 3D character model with motion data. The character model with skeleton rigged is used as a template model to fit the silhouette of the character image. After assigning some corresponding points between the character image and template model, the system then fits the model to the image and transfer the colors and patterns of the image to the model as the textures. Finally, the user can apply any motion data to animate the fitted 3D character model in 3D space.	Animating character images in 3D space	NA:NA:NA:NA	2009
Fu-Chung Huang:Yu-Mei Chen:Tse-Hsien Wang:Bing-Yu Chen:Shuen-Huei Guan	Speech animation is traditionally considered as important but tedious work for most applications, because the muscles on the face are complex and dynamically interacting. In this paper, we introduce a framework for synthesizing a 3D lip-sync speech animation by a given speech sequence and its corresponding texts. We first identify the representative key-lip-shapes from a training video that are important for blend-shapes and guiding the artist to create corresponding 3D key-faces (lips). The training faces in the video are then cross-mapped to the crafted key-faces to construct the Dominated Animeme Models (DAM) for each kind of phoneme. Considering the coarticulation effects in animation control signals from the cross-mapped training faces, the DAM computes two functions: polynomial-fitted animeme shape functions and corresponding dominance weighting functions. Finally, given a novel speech sequence and its corresponding texts, a lip-sync speech animation can be synthesized in a short time with the DAM.	Animating lip-sync speech faces by dominated animeme models	NA:NA:NA:NA:NA	2009
Nozomi Kugimoto:Rui Miyazono:Kosuke Omori:Takeshi Fujimura:Shinichi Furuya:Haruhiro Katayose:Hiroyoshi Miwa:Noriko Nagata	Technologies recreating piano performance in the form of CG animation are eagerly anticipated by people working in various fields, such as content production, music education, etc. Nonetheless, much of the past research has dealt with the mechanical finger movements in piano practice support systems and performance support GUIs, etc. and there has been little research recreating the reality of finger movements. We are promoting research into the analysis and CG expression of realistic and natural piano fingering. This paper describes the following aspects of this research program: (i) measurement of piano fingering using motion capture technology, (ii) generation of a CG animation of fingering using offline/realtime rendering, and (iii) automatic generation of fingering using optimized algorithms. And finally we will introduce examples in which the fingering data created in (i) is used in TV animation.	CG animation for piano performance	NA:NA:NA:NA:NA:NA:NA:NA	2009
Shinsuke Nakamura:Masashi Shiraishi:Shigeo Morishima:Mayu Okumura:Yasushi Makihara:Yasushi Yagi	Characteristics of human motion, such as walking, running or jumping vary from person to person. Differences in human motion enable people to identify oneself or a friend. However, it is challenging to generate animation where individual characters exhibit characteristic motion using computer graphics. Our goal is to construct a system that synthesizes characteristic gait animation automatically. As a result, when crowd animation is generated for instance, the motion with the variation can be made using our system. In our system, we first acquire a silhouette image as input data using a video camera. Second, we extract gait feature from single view silhouette. Finally we automatically synthesize 3D gait animation using the method blending a small number of motion data [KOVAR, L et al 2003].This blending weight is estimated using the gait feature automatically.	Characteristic gait animation synthesis from single view silhouette	NA:NA:NA:NA:NA:NA	2009
Takeshi Miura:Kazutaka Mitobe:Takaaki Kaiga:Takashi Yukawa:Toshiyuki Taniguchi:Hideo Tamamoto	In the field of dance motion analysis, development of the technique for extraction of characteristic postures peculiar to each dance number is needed [Hachimura 2006]; extracted postures can be used as the indexes for the retrieval of motion data. In this study, the authors suggest a novel method for extraction of characteristic postures from the motion data of a dance number; the information of uniqueness of the dance number given by the statistical analysis of a database including motion data of plural dance numbers is used in the extraction process.	Extraction of characteristic postures in a dance by statistical analysis of a database of motion data	NA:NA:NA:NA:NA:NA	2009
Yohei Shimotori:Shiori Sugimoto:Shigeo Morishima	Shadows in 2D Anime play a significant role for expressing symbolic visual effects such as the character's position and shape. However, animators frequently can't draw detailed shadows according to their intentions because of time constraints and a lack of skilled animators. For solving this problem, we have developed a system that can generate shadows automatically. Our system provides simple shadows and shadows on the water by applying Simplification Filter and Water Mapping Filter. Also, our system only requires inputs of the 2D character animation layers generally composed in the Anime industry. Consequently, our system enables animators to intuitively produce Anime-like shadow animation in a short time.	Directable anime-like shadow based on water mapping filter	NA:NA:NA	2009
Ryo Takamizawa:Takanori Suzuki:Hiroyuki Kubo:Akinobu Maejima:Shigeo Morishima	MoCap-based facial expression synthesis techniques have been applied to provide CG character with expressive and accurate facial expressions [Deng et al. 2006: Lau et al. 2007]. The representative performance of these techniques depends on the variety of captured facial expressions. It is also difficult to guess what expressions are needed to synthesize expressive face before capture. Therefore, much MoCap data are required to construct a subspace employing dimensional compression techniques, and then the space enables us to synthesize expressions with linear-combination of basis vectors of the space. However, it is hard work to take much facial MoCap data to obtain expressive result.	Expressive facial subspace construction from key face selection	NA:NA:NA:NA:NA	2009
James D. Edge:Adrian Hilton:Philip Jackson	We describe a method for the synthesis of visual speech movements using a hybrid unit selection/model-based approach. Speech lip movements are captured using a 3D stereo face capture system and split up into phonetic units. A dynamic parameterisation of this data is constructed which maintains the relationship between lip shapes and velocities; within this parameterisation a model of how lips move is built and is used in the animation of visual speech movements from speech audio input. The mapping from audio parameters to lip movements is disambiguated by selecting only the most similar stored phonetic units to the target utterance during synthesis. By combining properties of model-based synthesis (e.g., HMMs, neural nets) with unit selection we improve the quality of our speech synthesis.	Model-based synthesis of visual speech movements from 3D video	NA:NA:NA	2009
Hiroto Yarimizu:Yasushi Ishibashi:Hiroyuki Kubo:Akinobu Maejima:Shigeo Morishima	Muscle-based facial animation [Lee et al. 1995] is one of the best approaches to realize facial expressions of characters. However, this approach does not consider the personal variation in facial tissue model such as skin thickness. So personal character in emotional expression can not be reflected in this model.	Muscle-based facial animation considering fat layer structure captured by MRI	NA:NA:NA:NA:NA	2009
Takashi Tokizaki:Yuuichi Tazaki:Hironori Mitake:Shoichi Hasegawa	Interactive applications such as Video Games require characters, which generate motions corresponding to user's interaction. Motion capture is an effective technique to reproduce realistic motion. However, to produce a motion which is appropriate to the operation of the user, a lot of motions must be prepared and one of the motions which is suitable for the user's operation must be selected and played. Because the user's operation changes the motion trajectory, unexpected contact to objects may happen. The amount of change on a trajectory depends on not only the trajectory of motion but also internal tensions of skeletal muscles - co-contraction level, when a person put one's hand down on a table or collides with an object. [Hogan 1984] proposed that reaching motion of human is supposed to be generated by spring damper characteristics of muscles dragging to the virtual trajectory. Human controls not only trajectories of motions but also spring-damper characteristics of muscles by changing co-contraction levels. Realistic character motions contacting to objects can be generated easily with virtual trajectory tracking control which is integrated to physics engines for character motions.	Pliant motion: integration of virtual trajectory control into LCP based physics engines	NA:NA:NA:NA	2009
D. Kravtsov:O. Fryazinov:V. Adzhiev:A. Pasko:P. Comninos	The modern world of computer graphics is mostly dominated by polygonal models. Due to their scalability and ease of rendering such models have various applications in a wide range of fields. Unfortunately some shape modelling and animation problems can hardly be overcome using polygonal models only. For example, dramatic changes of the shape (involving change of topology) or metamorphosis between different shapes can not be performed easily. The Function Representation (FRep) [Pasko et al. 1995] allows us to overcome some of the problems and simplify the process of the major model modification. Our system is based on a hybrid modelling concept, where polygonal and FRep models are combined together and can be evaluated in near-real or real time. It allows us to: • produce animations involving dramatic changes of the shape (e.g. metamorphosis, viscoelastic behaviour, character modifications etc) in short times (Fig. 1) • interactively create complex shapes with changing topology (Fig. 2) and specified level of detail (LOD) • integrate existing animated polygonal models and FRep models within a single model	Polygonal-functional hybrids for computer animation and games	NA:NA:NA:NA:NA	2009
Jianfeng Xu:Haruhisa Kato:Akio Yoneyama	This poster presents a motion retrieval algorithm, which searches the motions in the same category as a query's (known as logically similar motions) in a motion capture database. The challenge is that logically similar motions may not be numerically similar due to the motion variations [Müller et al. 2005]. In this poster, we propose a novel short-term feature that extracts both symbolized representation and continuous features from joint velocities in a motion clip, which is employed to effectively retrieve logically similar motions to the query. Although symbolized representation of human motion has been studied [Müller et al. 2005], our approach is different in that we consider temporal correlation instead of Müller's spatial relationship. Moreover, not only symbolized representation (dynamic pattern) but also continuous features (average speed) are extracted in our short-term feature. Furthermore, our method is more friendly to novices as it requires no prior knowledge to determine features. Our experiments demonstrate that our algorithm greatly improves the performance compared to two conventional methods.	Retrieval of motion capture data based on short-term feature extraction	NA:NA:NA	2009
Katsutoki Hamana:Hiroshi Mori:Atsushi Nakano:Junichi Hoshino	In recent years, many entertainment systems have relied on the progress of interaction technology to create characters that act autonomously. To show these lifelike characters, it is important for them to perform various actions, such as daily actions, reflex actions that require reacting to input from a user, perceiving actions where the character perceives an object and reacts to it, and actions based on personalities or feelings. This results in the problem of complex action planning. A character has to carry out the actions listed, keep schedules and maintain a personality, and react flexibly to user interaction, while still maintaining story flow. In this paper, we propose the Story Engine, which can execute various actions in multiple characters.	Story engine for interactive characters	NA:NA:NA:NA	2009
Evan Tice:Tim Tregubov:Kate Schnippering:Yoon-Ki Park:Ray diCiaccio:Max Friedman:Jennifer Huang:Justin Slick:Giulia Siccardo:Jessica Glago:Stephanie Trudeau:Daniel Gobaud:Daniel Garcia:Craig Slagel:Lorie Loeb	With glaciers melting, sea levels rising and natural disasters---such as hurricanes and cyclones---intensifying, climate change is a growing concern. While innovations in renewable energy are critical, research shows that changing energy use behavior has become increasingly important in the fight against global warming. GreenLite Dartmouth focuses on changing behavior by making energy conservation a priority for students by creating both an intellectual and emotional connection between daily actions and their adverse effects on the environment. We combine computer graphics, art, engineering, sociology, environmental science, systems-thinking and behavioral psychology to turn real-time energy use data into a meaningful interactive display. GreenLite employs innovative methods for displaying complex data using interactivity, storytelling, animation, competition and goal-setting. Appealing animated information-display and "mood" algorithms put data into context to make it meaningful. We incorporate a system of digital energy meters, a custom database, computational analysis, 2D and 3D animations, interactive design and a game-engine to spur behavior change and, hopefully, reverse the course of climate change.	GreenLite Dartmouth: unplug or the polar bear gets it	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2009
Saba Hashem Kawas	With today's increased interest in advanced digital networking and social online communities, many higher educational institutes have been exploring online three-dimensional virtual environments as a new medium for distance learning. These technologies provide multiple features for online interaction and collaboration, a visual cyberspace for text and voice communication, the ability to manipulate multimedia in real time with a group of individuals, and the opportunity to link users to Web sites.	H-link 3D: hyper-learning interface and navigational toolkit in 3D virtual worlds experimental interface design for cobalt, a Croquet metaverse	NA	2009
Akira Nakayasu:Kiyoshi Tomimatsu	Himawari is a sunflower robot composed of mechanical parts and electronic parts, such as servo motors, LEDs and shape-memory alloy actuators, that reacts, slowly and fluidly, facing and communicating to humans in front of it.	Himawari: a plant robot	NA:NA	2009
Benjamin Raynal:Xavier Gouchet:Venceslas Biri:Vincent Nozick	Designing a program that is not a tool of artistic creation, but a creator itself have been a real challenge for both digital artists and researchers. The most famous program of artistic creation is AARON [Cohen], which is in continual development since 1973 by its creator, Harold Cohen. Unfortunately, AARON cannot learn new styles or imagery by its own, each new capability must be hand-coded by Harold Cohen. Roxame [Berger], another artistic creation program created in 2001 by Pierre Berger, is based on artificial intelligence, and have its own style, emerging from both the artistic preferences of the user, and a stochastic process. This style can evolve and is refined at each work.	IArtist: a self learning computer artist	NA:NA:NA:NA	2009
Takuji Narumi:Tomohiro Akagawa:Young Ah Seong:Michitaka Hirose	To change the spatial structures of our living spaces, we usually take an architectural approach. However once such structures have been constructed, re-configuring them incurs substantial costs. Reserving the flexibility to change spatial design is essential for the effective use of spaces. Our research aims to make spatial design flexible. New characteristics are added to an existing space using information technology, so the relationship between the space and people within changes.	Thermotaxis	NA:NA:NA:NA	2009
Max Grosse:Gordon Wetzstein:Oliver Bimber:Anselm Grundhöfer	With adaptive coded aperture projection, we present solutions for taking projectors to the next level. By placing a programmable liquid crystal array at a projectors aperture plane we show how the depth of field (DOF) of a projection can be greatly enhanced. This allows focussed imagery to be shown on complex screens with varying distances to the projectors focal plane, such as projection domes as in planetariums or cylindrical canvases as in IMAX theaters. We demonstrate that adaptive apertures outperform previous methods of projector defocus compensation for objective lenses with static apertures. In addition, our adaptive apertures can perform the type of temporal contrast enhancement employed by common auto-iris projection lenses, and also produce high-quality depixelated images. The latter is beneficial for close-view displays with limited resolution, such as rear-projected TV sets.	Adaptive coded aperture projection	NA:NA:NA:NA	2009
Martha Carrer:Cruz Gabriel	The objective of this paper/presentation is to describe the potentialities of Mobile Tagging as a tool for increasing and spreading the effects of Mixed Realities. In this sense, we will start introducing the main concepts and some examples of Mixed Realities followed by the concepts and examples of Mobile Tagging, showing that they are connected and benefit each other.	Mobile tagging and mixed realities	NA:NA	2009
Norihiro Nakamura:Yoshiyuki Kokojima:Yasunobu Yamauchi	Resolution-independent rendering is important for many applications such as text rendering and rendering vector objects. This theme has attracted interest in recent years owing to the growing popularity of Flash and SVG-based applications [Loop and Blinn 2005]. We had previously presented a fast rendering method using a stencil buffer for deformable vector objects [Kokojima et al.]. One of the advantages of this method is that retriangulation is unnecessary when vector objects deform interactively. However, [Kokojima et al.] only deal with rendering vector objects on a flat surface.	Rendering of vector objects on curved surface using pivot triangle primitives	NA:NA:NA	2009
Takehito Teraguchi:Hiromasa Yamashita:Ken Masamune:Takeyoshi Dohi:Hongen Liao	Three-dimensional (3-D) displays have got a lot of attention because they have a much higher sense of realism and are more intuitive than 2-D displays. In particular, autostereoscopic displays are suitable for everyday use because they can be observed from an arbitrary viewpoint without supplementary glasses or tracking devices. Integral Videography (IV) is one of the methods for autostereoscopic animated images that extends Integral Photography (IP) to animation. IP/IV uses a combination of a lens array and a number of calculated elemental images with different perspectives. In particular, IV with a depth of several meters can be applied in many areas. However, most IV reports have an image depth of only several centimeters. Only a small deviation of a lens from its designed position would result in several degrees of deviation of the light ray from the back of the lens. We have developed the static autostereoscopic image by projecting the light sources from an object onto a photographic film through the lens-array [Liao et al. 2005]. In this study, we obtained animated IV of 1 m image depth with less distortion using our method to correct IV images. The method is technically unique.	Three-dimensional auto-stereoscopic animated image with a long viewing distance using high-precision image correction	NA:NA:NA:NA:NA	2009
Ilya Rosenberg:Ken Perlin:Charles Hendee:Alex Grau:Nadim Awad	Multi-touch input has been an active area of research for over two decades but has always suffered from the absence of an easily available high quality touch input device. For this reason, exciting user interfaces developed in the lab have appeared on CNN, but not on everyone's desk, computer screens, table-tops, walls and floors. What has been needed - and lacking - is a better mousetrap; an inexpensive, flexible and sensitive touch imaging technology.	The UnMousePad: the future of touch sensing	NA:NA:NA:NA:NA	2009
Wei-Chung Cheng:Jih-Fon Huang	This work demonstrates a novel channel for a smart display to interact with its user for enhancing the viewing experience. By using a wearable electro-oculography (EOG) circuit, the saccadic eye movements can be detected so that the user's viewing mode can be determined. Different gamut settings are used in the "fixation mode" versus "saccade mode," such that the fast eye movement induced artifacts can be suppressed. Furthermore, by analyzing the saccade patterns, we can determine the user is in the "image viewing" mode or "text reading" mode. Then different brightness, contrast, saturation settings can be assigned accordingly and automatically to improve the user's comfort level.	A saccade-contingent display for suppressing color breakup	NA:NA	2009
Jussi Huhtala:Ari-Heikki Sarjanoja:Jani Mäntyjärvi:Minna Isomursu:Jonna Häkkilä	Mobile devices have limitations compared to PCs due to their inferior computing power and small screens, but a successful design of animated transitions can hide processing delays and make the user experience smoother. In this paper, we describe the design of animated transitions and present a user study on how they are perceived.	Mobile screen transition animations	NA:NA:NA:NA:NA	2009
Danny Rado:Daniel F. Keefe	Despite the many challenges understanding how best to interact with large format displays, they are becoming increasingly popular for data analysis tasks in a variety of domains, including scientific, information, and geo-visualization. (Figure 1a shows a relatively small, 60" display; even larger, wall-size displays are also popular.) In order to make the most effective use of the full display, users typically stand and walk around in these environments. In fact, this physical navigation has been shown to be beneficial in data analysis tasks [1]. Since immobile input devices, such as mice, keyboards, or pen-tablets, do not naturally support interaction "on the move", new interactive techniques are needed to facilitate fluid interaction across a range of distances when working with large-format displays. We believe body-centric 3D, gestural input is particularly promising in this regard. Our work investigates techniques for reliable menu selection based upon these ideas, introducing new 3D input strategies for controlling menus. Our work builds upon previous techniques, such as rapMenu [3], which uses rotational hand movements and finger pinches to control menus from a distance.	rAir flow menus: toward reliable 3D gestural input for radial marking menus	NA:NA	2009
Kensuke Takada:Kyoko Higurashi:Tatsuhiko Suzuki:Misako Ota:Tetsuaki Baba:Kumiko Kushiyama	"Thermo-Pict" is a design apparatus produced by applying temperature visualization technology linked to an information display with the use of a thermograph sheet. Thermography is used to visualize the surface temperature of objects through their depiction as colors. This technology has been used primarily in the medical and research fields. Thermography display colors come in a wide range of hues and brightness that enables quick visualization of any object's surface temperature distribution. Use of this technology will be attempted as a tool in the production of design displays. [Fig. 1]	Temperature design display device to use peltier elements and liquid crystal thermograph sheet "Thermo-Pict"	NA:NA:NA:NA:NA:NA	2009
Celambarasan Ramasamy:Donald H. House:Andrew T. Duchowski:Brian Daugherty	This poster will analyze the feasibility of eye tracking as a tool for helping filmmakers to make decisions in a stereoscopic film production. In a conventional dialogue driven shot it is fairly easy to predict where the audiences would be looking. However, for visually complex shots it is not so obvious. In this case, eye tracking can be used as a tool to observe the gaze pattern of the audience to identify the regions of interest in the frame. This information could be used to budget the resources for the shot. It can also be used to identify elements that distract the audience from the flow of the movie. This technique could be used to help filmmakers to make more informed decisions during the film making process. We analyzed a student produced stereoscopic film using this technique. In our study, a number of subjects were asked to watch the film and their gaze data was recorded.	Using eye tracking to analyze stereoscopic filmmaking	NA:NA:NA:NA	2009
Yi-Ting Cheng:Virginia Tzeng:Yu Liang:Chuan-Chang Wang:Bing-Yu Chen:Yung-Yu Chuang:Ming Ouhyoung	The development in digital technologies and the widespread Web 2.0 concept have made many digital videos accessible. Editing and modifying digital videos have become an interesting and important topic. In this paper, we present a system for face replacement in video. Most digital processing software can perform face replacement only when the poses for the source and target faces are similar, and the manipulation process with those software is often time-consuming and labor-intensive. While previous work [Blanz et al. 2004] focuses on image face replacement, our system performs face replacement in video by constructing 3D models for both target and source faces and swapping them accordingly. 3D face models are created by fitting 3D morphable models [Blanz et al. 1999] and the input is reduced to two pictures for the face to be placed in.	3D-model-based face replacement in video	NA:NA:NA:NA:NA:NA:NA	2009
Che-Hua Yeh:Pei-Ruu Shih:Yin-Tzu Lin:Kuan-Ting Liu:Huang-Ming Chang:Ming Ouhyoung	This poster presents experimental results of three face recognition methods -- Support Vector Machine (SVM), Local Binary Pattern (LBP)-based, and Sparse Represented-based Classification (SRC). We will show the experimental results based on AR face database and on home photos. The experiments show that the three algorithms can achieve over 85% recognition rate in AR database. However, the recognition rate is extremely reduced in home photos. SVM and SRC-based method encounter challenges of selecting training model while LBP-based method encounters the challenge of merging over scattered clusters. Our goal is to improve the accuracy and efficiency especially in home photos based on the three methods.	A comparison of three methods of face recognition for home photos	NA:NA:NA:NA:NA:NA	2009
Jörn Loviscach	All of today's digital cameras record the date and time at which a photo has been taken; some cameras also record the geographical position. This work proposes yet another augmentation: to record temperatures at different spots picked by the user in the image. This has many applications for both family life and professional engineering: Was the water in the swimming pool heated? Was last Saturday night's party fever really a fever? On the serious side, an engineer may record the temperature of different chips on a printed circuit board or document heat loss due to bad building insulation.	Augmenting a camera with a thermometer	NA	2009
Yuji Morimoto:Yuichi Taguchi:Takeshi Naemura	Colorization is the process of adding color to monochrome images and video. It is used to increase the visual appeal of images such as old black and white photos, classic movies, and scientific visualizations. Since colorizing grayscale images involves assigning three-dimensional (RGB) pixel values to an image whose elements are characterized by one feature (luminance) only, the colorization problem does not have a unique solution. Hence, human interaction is typically required in the colorization process. Although existing colorization methods attempt to minimize the amount of user intervention, they require users to manually sellect a similar image to the target image or input a set of color seeds for different regions of the target image. In this paper, we present an entirely automatic colorization method using multiple images collected from the Web. The method generates various and natural colorized images from an input monochrome image by using the information of the scene structure.	Automatic colorization of grayscale images using multiple images on the web	NA:NA:NA	2009
Tongbo Chen:Abhijeet Ghosh:Paul Debevec	Separation of the diffuse and specular components of observed reflectance has been an active area of research in computer graphics and vision, with major applications in reflectance modeling and scene analysis. Traditionally, researchers have investigated diffusespecular separation under point or directional illumination conditions while employing polarization and, in the case of dielectric materials, color space analysis techniques. Recently, Ma et al. [2007] introduced a technique for estimating high quality diffuse and specular normals and albedo maps (see Fig. 1, (a) & (d)) of a specular object using polarized spherical gradient illumination. However, the employed polarization technique imposes view-point restriction, and results in insufficient light levels for performance capture with high speed acquisition. Hence, in this work, we look into an alternate diffuse-specular separation technique for spherical gradients based on a data-driven reflectance model. Traditional separation techniques based on color space analysis focus on removing specular reflections from the observation for scene analysis [Mallick et al. 2005]. In contrast, we focus on obtaining high quality estimates of both the diffuse and the specular reflectance components.	Data-driven diffuse-specular separation of spherical gradient illumination	NA:NA:NA	2009
Mary Hudachek-Buswell:Catherine Matos:Michael Stewart	In this presentation, the restoration of images blurred by atmospheric turbulence is examined. The proposal uses a new class of approximations to blurring operators representing Gaussian blur. The Toeplitz matrix representing the blur is transformed into a Cauchy-like (CL) matrix using the FFT. In addition to the CL structure, the transformed matrix has a rank structure. In particular, the off-diagonal blocks have low rank. This class of matrices can be approximated quickly, and the structure can be exploited for fast image restoration.	Deblurring with rank-structured inverse approximations	NA:NA:NA	2009
Nari Kim:Jong-Chul Yoon:In-Kwon Lee	In this work, we present an image based virtual dress up system according to user input model and garment image. At 'Registration' step, we asked the user manually setting the skeleton structure and matte out the alphamap from the image. Next step, our method automatically deforms the garment image corresponding to model's body. For the boundary fitting, our method uniformly sampled contour points and solves the optimization function. To enhance the more realistic scene, we reconstruct the 2D mesh to the 3D mesh according to a human's standard body shape. For the lighting effect we estimate the light position by using luminance value with the detected face region. Previous 3D scanner based virtual dress up system has expensive cost and under locational limitation issues, but our system integrates various image processing techniques and introduced an easy-to-use system for the general users. We present that our system produces a visually plausible and well-fitted virtual dress up results in a practical and usable way.	Image-based dress up system	NA:NA:NA	2009
Yingen Xiong:Xianglin Wang:Marius Tico:Chia-Kai Liang:Kari Pulli	We present the design and implementation of a mobile imaging system for high resolution panoramic image creation. The system comprises the following components: automatic camera motion tracking and high resolution image capturing, image registration on spherical manifold, image warping, image labeling, and image blending.	Panoramic imaging system for mobile devices	NA:NA:NA:NA:NA	2009
Kei Utsugi:Takuma Shibahara:Takafumi Koike:Takeshi Naemura	Seam carving is an image processing operator for content-aware image resizing [Avidan and Shamir 2007]. It generates an energy map from gradient intensity of pixels and searches for seams, which are vertical or horizontal continuous paths of pixels that run through local minimum energy areas. Removing or inserting pixels along a seam enables users to shrink or enlarge pictures by a wide range, while still retaining all details of the image.	Proportional constraint for seam carving	NA:NA:NA:NA	2009
Susan M. Munn:Jeff B. Pelz	Our portable video-based monocular eye tracker contains a headgear with two cameras that capture videos of the observer's right eye and the scene from the observer's perspective (Figure 1a). With this eye tracker, we typically obtain a position -- that represents the observer's point of regard (POR) -- in each frame of the scene video (Figure 1b without bottom left box). These POR positions are in the image coordinate system of the scene camera, which moves with the observer's head. Therefore, these POR positions do not tell us where the person is looking in an exocentric reference frame. Currently, the videos are analyzed manually by examining each frame. In short, we aim to automatically determine how long the observer spends fixating specific objects in the scene and in what order these objects are fixated.	Ray tracing to get 3D fixations on VOIs from portable eye tracker videos	NA:NA	2009
Naoki Kawai	Vector plot is a frequently used method for illustrating vector fields used in applications such as scientific visualization. Although the method is easy to implement and the resulting image captures the original vector field well, the streamlines are often positioned too closely or too sparsely to one another due to sources and sinks of the original vector field. This results in unevenness of visual density over the entire region, and some previous researches have treated the problem. Mebarki et al [1] proposed the improved strategy that the maximum vacant region should be given priority for a new streamline, but the results still lack uniformity. Other related works [2][3] suggested that both tapering streamlines and controlling intensity improve the visual uniformity of streamlines. We propose another approach for making streamlines look uniform with dotted and broken lines instead of tapering or intensity control. The results are binary images and consist of fixed width streamlines which preserve uniformity.	Uniform looking vector plot with streamline fragmentation	NA	2009
Chung-Lin Wen:Yu-Ting Wong:Bing-Yu Chen:Yoichi Sato	In this extended abstract, we propose a novel approach for video segmentation by utilizing motion information. Recently, graph-cutbased segmentation methods became popular in this domain but most of them dealt with color information only. Those methods possibly fail if there are regions similar in color between foreground and background. Unfortunately, it is usually hard to avoid, especially when objects are filmed under a natural environment. For instance, Figure 1(a) shows a result of graph cut with a small smoothness weighting, and hence some background regions are incorrectly labeled. On the contrary, if a larger smoothness weighting is used, some background regions near the foreground will be merged as shown in Figure 1(b). To improve those drawbacks, we propose a method based on both of color and motion information to conduct the segmentation. The method is useful because foreground and background usually have different motion patterns as shown in Figure 1(c).	Video segmentation with motion smoothness	NA:NA:NA:NA	2009
Alice Boit:Thomas Geimer:Jörn Loviscach	Graphical passwords [Suo et al. 2005] address vital problems of textual passwords: Users pick from a limited vocabulary; machine-generated passwords are hard to memorize. Graphical input, however, faces "shoulder surfing," as bystanders can watch the screen. Current solutions to this problem tend to impose high cognitive loads. We propose an easy-to-handle approach.	A random cursor matrix to hide graphical password input	NA:NA:NA	2009
Alexandros Zotos:Katerina Mania:Nick Mourkoussis	In order to economize on rendering computation, selective rendering guides high level of detail to specific regions of a synthetic scene and lower quality to the remaining scene, without compromising the level of information transmitted. Scene regions that have been rendered in low and high quality can be combined to form one complete scene. We propose a novel selective rendering approach which is task and gaze-independent, simulating cognitive creation of spatial hypotheses. Scene objects are rendered in varying quality (polygon count) according to how they are associated with the context (schema) of the scene.	A selective rendering algorithm based on memory schemas	NA:NA:NA	2009
Matthew Hirsch:Douglas Lanman:Ramesh Raskar:Henry Holtzman	We present a BiDirectional screen capable of both imaging and display, that uses an LCD as a spatial light modulator to support seamless transition from on-screen multi-touch interactions to off-screen hover-based gestures.	BiDi screen: depth and lighting aware interaction and display	NA:NA:NA:NA	2009
Jinha Lee:Yasuaki Kakehi:Takeshi Naemura	In this paper, we propose a novel block-shaped tangible interface named Bloxel (see Figure 1). A Bloxel is a translucent cubical block that glows in full color and communicates with the neighboring Bloxels through high-speed flickers.	Bloxels: glowing blocks as volumetric pixels	NA:NA:NA	2009
André Maximo:Maria Paula Saba:Luiz Velho	Introduction and Related Work Tabletop and tangible interfaces have become common in recent years. Technology trends in this area can be found in commercial products, such as Apple's iPhone™ and Microsoft Surface™, as well as in research ventures, such as Reactable and Perceptive Pixel initiatives. Nevertheless, natural human computer interfaces (HCI) to support this hardware technology are still non-intuitive.	collecTable: a natural interface for music collections	NA:NA:NA	2009
Andrew Bragdon:David H. Laidlaw	We explore the design of a multi-view interaction metaphor for 3D visualization in the CAVE. We then present the results of a formative evaluation of a "Wizard of Oz" [Kelley 1984] prototype. Although there has been significant prior work on 2D and 3D desktop applications utilizing multiple views, little prior work exists for multi-view systems in immersive virtual environments such as the CAVE, despite the clear advantages enjoyed by desktop analogues. Immersive 3D environments pose unique challenges for such a system. Since the contents of such views are themselves 3D, it is unclear whether users will be able to easily read views independently of one another, as in a naive implementation they might become intermingled; even in a system that is conscious of this problem, some vantage points may cause depth ambiguity problems which make it difficult to read each view. In addition, interaction techniques for controlling and managing such views must be explored. Thus, formative empirical testing is warranted to determine the viability of such a system.	The design and evaluation of a lightweight multi-view interaction metaphor for 3D visualization in the CAVE	NA:NA	2009
Mi Sun Lee:Mi-Gi Han:Joo-Youn Park:Su-e Park	Online spaces are being transformed into new social spaces with a variety of interpersonal relationships and social activities. Especially, cyber spaces based on three dimensions show various cross-cultural social relationships and activities compared with cyber spaces based on two dimensions. These phenomena have different characteristics, depending on users' cultural backgrounds. Relating to social issues in online spaces, many preliminary studies have been conducted. Especially, impressions have been considered important subjects related with social networks. In spite of that, sufficient cross-cultural research related with impressions in online spaces has not been conducted, especially based on 3-D cyber spaces. Therefore, the main goals of this study were to extract 3-D cyber factors formatting perceptional impressions and compare those factors based on cultural differences. In the preliminary research, we identified six impressions dimensions in 3-D cyber space: F1.Cheerful, F2.Logical, F3.Violent, F4.Selfish, F5.Warm, and F6.Seclusive (Lee, Kim & Park, 2009). In order to achieve our goal, first, we selected two countries considering Hofstede's culture dimensions (e.g. Power Distance, Individualism versus Collectivism, Masculinity versus Femininity, Uncertainty Avoidance) (Hofstede, 2005). Korea and America have very different cultural characteristics in terms of Hofstede's culture dimensions (Hofstede & Bond, 1984). Secondly, we conducted in-depth individual interviews. For these interviews, we recruited interviewees as actual users of 3-D cyber spaces (Second Life); depending on the frequency uses and interpersonal relations contained therein, we selected eight Korean participants and eight American participants. Before conducting interviews, we recorded normal lives of participants within a three-day span, for two hours of each day. Then, we conducted the survey to each participant seeing the video clips of others' virtual lives for the purpose of analyzing others' preserved impressions. In-depth interviews were conducted in 3-D cyber space using actual voices. The interview consisted of two parts of questions: 1) What are the factors relating with your perceived impressions?; and 2) If you help an avatar on the video clip before you saw to make clear his/her impression, how will you help? All interviews were recorded as video and audio clips. After collecting data, we analyzed data based on Grounded theory (Strauss, 1990) recognized qualitative research methods. First of all, we accurately transcribed all voice data to text data, and then separated data to minimal units of meaning considering interviewees' intentions. Finally, we extracted properties and grouped properties during axial coding. As a result, Factors formatting perceptional impression in 3-D cyber space was derived with distinction by Korean and American users. These derived factors were linguistic, visual, behavioral, relational, inner-environmental, and outer-environment. Of these, linguistic factors (106, 43%) and behavioral factors (57, 23%) were the most derived. Further, looking at the visual factors, the number of derived factors was similar among Korean and American users. Alternatively, we looked at the detailed factors derived with distinction by Korean and American users. The factors derived by Korean users included exposure degree of clothes, thickness of clothes, while the factors derived by American users included color of clothes and types of avatar. In conclusion, this study has theoretical and empirical significance. The theoretical significance, through the cultural differences research, is to understand how each intercultural impression provided role elements in Korea cultures and American cultures and to understand how the impression provided difference elements. Therefore, more extensive future research on the dimensions of the intercultural impression formation mechanism was proposed, based on this study. The empirical significance was to offer impression dimensions-related elements in 3D gaming to developers and designers in the development of related systems; furthermore, as the results provide data of how elements affect impression in intercultural perception and how in each dimension, the system will be able to provide a basis about impression formation elements in intercultural context.	Factors formatting perceptional impression in 3-D cyber spaces: a cross-cultural study of Korean and American users	NA:NA:NA:NA	2009
Piotr Dalka:Andrzej Czyzewski	The main goal of each HCI application is to make working with a computer as natural, intuitive and effective as possible. One of the main areas of applications of new human-computer interfaces is making possible to use computers for people with permanent or temporal motor disabilities in an efficient way. There are two main types of such solutions [Aggarwal and Cai 1999]. The first group utilizes devices mounted directly on the user's body. Applications in the second group are contactless and they use remote sensors only, therefore they are much more comfortable for a user. Amongst contactless solutions, vision-based human-computer interfaces are the most promising ones. They utilize cameras and image processing algorithms to detect signs and gestures made by a user and execute configured actions. The most common vision-based applications employ eye and hand tracking [Shin and Chun 2007].	LipMouse: novel multimodal human-computer interaction interface	NA:NA	2009
Fumitaka Ozaki:Takuo Imbe:Shin Kiyasu:Yuta Sugiura:Yusuke Mizukami:Shuichi Ishibashi:Maki Sugimoto:Masahiko Inami:Adrian D. Cheok:Naohito Okude:Masahiko Inakage	MYGLOBE is an interactive map media which allows us to share our cognitive maps. This map grows up with our own activities and shows our subjective view of the city by emphasizing roads or landmarks frequently used. Users can bring up their own city in the device by actually walking in the city, and also share their own maps with each other and discover unknown places. Present map services such as Google maps and Google Earth, provide mash-up tools which allow us to create our own favorite place on the map easily. We can use hand held GPS devices to make our own travel route and navigate to destination places. MYGLOBE allows us to not only tag their favorite places on the map but also change the shape of the map itself. Instead of an accurate geographic map, MYGLOBE provides maps reflecting the user's individual experiments and the view of the city. It can also be used as a communication tool to share the life history with your friends. MYGLOBE will enhance your city experience.	MYGLOBE: cognitive map as communication media	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2009
Duksu Kim:Jae-Pil Heo:Sung-eui Yoon	Collision detection between deformable models is one of fundamental tools of various applications including games. Collision detection can be classified into two categories: discrete and continuous collision detection methods. Discrete collision detection (DCD) has been demonstrated to show the interactive performance by using bounding volume hierarchies (BVHs). However, some colliding primitives may be missed since DCD methods find intersecting primitives only at discrete time steps. This issue can be a very serious problem in physical based simulation, CAD/CAM applications and etc. On the other hand, continuous collision detection (CCD) identifies the first time of contact of colliding primitives during a time interval between two discrete time steps.	PCCD: parallel continuous collision detection	NA:NA:NA	2009
Sho Kamuro:Kouta Minamizawa:Naoki Kawakami:Susumu Tachi	We propose a pen-shaped handheld haptic display that allows haptic interactions with virtual environments by generating kinesthetic sensations on the user's fingers; the user's movements are not restricted since the device does not have mechanical linkages. Unlike conventional haptic displays that provide vibrations, which are not representative of tactile sensation, our proposed device, named "Pen de Touch" (Figure 1), provides kinesthetic sensations to the muscles in the user's fingers.	Pen de Touch	NA:NA:NA:NA	2009
Toki Takeda:Kazuhiko Yamamoto:Reiji Tsuruno:Taketoshi Ushiama	The recent popularity of small mobile devices such as cellular phones, digital cameras, and game device has made it increasingly convenient to carry them with us as we go about our daily lives. However, the increased miniaturization and functionality range of these devices can make it hard to access and utilize their contents. For instance, when using a device with a small display screen, it is often impossible to display an entire area of interest in a single view and the existence of numerous buttons can make it difficult to manipulate the device. In this study, we propose a novel interface that allows intuitive browsing of the content of mobile devices using an actual map.	Actual map based interface for browsing content on mobile devices	NA:NA:NA:NA	2009
Christian Schulze:Laurens Nienhaus:Jörn Loviscach	Painting in a 3D geobrowser is interesting both for artistic uses such as virtual graffiti and for commercial applications such as architectural sketches. It seems straightforward to turn Google Earth into a 3D painting tool: Store the 3D mouse data that the software returns and construct polylines from them. However, this approach has two vexing drawbacks: First, when the user paints past an edge, the end of the stroke will be placed at an incorrect depth, see Figure 1; second, it is not possible to sketch in mid-air, for instance to indicate a planned height extension of a building.	Sketch-based annotations in Google Earth	NA:NA:NA	2009
Andrzej Czyzewski:Piotr Odya:Agnieszka Grabkowska:Michal Grabkowski:Bozena Kostek	Dyslexia (dysgraphia) therapy is often boring for children and, what even worse, its results can be unsatisfactory. Hence, many therapists insist on development new methods which would be more interesting for young patients. The Smart Pen is such a tool. It is designed for supporting the therapy of developmental dyslexia, with particular regard to dysgraphia.	Smart pen: new multimodal computer control tool for dyslexia therapy	NA:NA:NA:NA:NA	2009
Yasuko Hayashi:Kensei Jo:Yasuaki Kakehi:Takeshi Naemura	Generally, people use a keyboards as an interface for text input. However, unlike handwritten characters, typed characters are identical no matter who types them. To create variations in the appearance of typed characters, we usually decorate characters by changing fonts. For example, we change the font size, font color and boldness of the characters and the spacing between characters. However, due to these decorations, people must handle a few input processes such as checking the select menu. To improve current conditions, some research suggests methods of applying the user's unconscious actions while typing to decorate characters, using a laptop with a built-in acceleration sensor or body-worn electronic equipment [Iwasaki et al. 2009][Wang et al. 2004][TypeTrace 2006].	TypeTile: a keyboard system that decorates characters depending on the way of typing	NA:NA:NA:NA	2009
Kai-Yin Cheng:Ko-Yuan Chou:Sheng-Jie Luo:Bing-Yu Chen	Due to the development in digital technologies, people now can easily retain their valuable memory by taking pictures through digital cameras. The cheap digital storage also encourages people to take lots of photos as they want. However, due to the tremendous amount of digital photos, it is not easy for people to browse all of them. Therefore, some techniques are proposed to help people to enjoy the photos, although it may be difficult for some people to arrange a time slot to watch them intentionally. Hence, in this extended abstract, we propose a system, which can utilize the large number of photos as the program themes (background), so that people will not notice the synthesized background while they are working, but the program themes may still be able to remind their good memory when they taking a short rest.	Utilizing photos as program themes	NA:NA:NA:NA	2009
Tatsuya Ishikawa:Shoichi Hasegawa	Recently, hobby robots such as pet robots and humanoid robots for entertainment are spreading and becoming more and more familiar each day. Compared to robots such as HONDA's ASIMO, a hobby robot is much cheaper, less rigid and has far less precision in measuring and controlling the angle of its own joints. For this reason, although we can assign joint angle to a key frame, assigned posture cannot be taken like computer graphics. Therefore, to ensure the robot moves as desired, we need to actually look at the robot operating while adjusting key frames respectively. By this, the margin of error in the joint angle and distortion in mechanism can be avoided. For CG animation, the animator observes the animation in real-time and when a problem is encountered, the problem in the key frame is corrected by slowing the animation down or by examining each frame. However, with robots, sudden stopping of ambulatory action makes the robot fall down. Moreover, the error in joint angle and distortion in mechanism are different between operation and geostationary state because of dynamic influences.	Virtual stroboscope for robot motion design	NA:NA	2009
Stephan Wenger:Marcus Magnor:Christophe Morisset:Wolfgang Steffen	Distant astrophysical objects like planetary nebulae can normally only be observed from a single point of view, which makes deducing plausible 3D models a hard task that usually involves a lot of manual work [Nadeau et al. 2001]. However, additional physical assumptions can be used in order to estimate the missing depth information. In previous work [Wenger et al. 2009], a certain axial symmetry was assumed which is present in many planetary nebulae, so that tomographic methods could be used for the reconstruction. However, this assumption obviously fails for many of the most complex and interesting objects in question, and it only leads to unambiguous results as long as no absorption occurs within the nebula.	3D reconstruction of planetary nebulae using hybrid models	NA:NA:NA:NA	2009
Hiroki Fujishiro:Takanori Suzuki:Shinya Nakano:Akinobu Mejima:Shigeo Morishima	Everyone is interested in being more attractive. Leyvand et al have been proposed the method which can enhances of facial attractiveness into a photograph [Leyvand et al. 2008]. However, their method can't synthesize more attractive facial expression than that of an input face photograph. Meanwhile, amateur subjects' facial expressions often become unnatural when they act in front of the camera in experimental environments. At such a situation, a natural expression can be synthesized without performance skills.	A natural smile synthesis from an artificial smile	NA:NA:NA:NA:NA	2009
Kentaro Yamanaka:Shinsuke Nakamura:Shota Kobayashi:Akane Yano:Masashi Shiraishi:Shigeo Morishima	This paper presents a new methodology for constructing a skin deformation model using MRI and generating accurate skin deformations based on the model. Many methods to generate skin deformations have been proposed and they are classified into three main types. The first type is anatomically based modeling. Anatomically accurate deformations can be reconstructed but computation time is long and controlling generated motion is difficult. In addition, modeling whole body is very difficult. The second is skeleton-subspace deformation (SSD). SSD is easy to implement and fast to compute so it is the most common technique today. However, accurate skin deformations can't be easily realized with SSD. The last type consists of data-driven approaches including example-based methods. In order to construct our model from MRI images, we employ an example-based method. Using examples obtained from medical images, skin deformations can be modeled related to skeleton motions. Retargeting generated motions to other characters is generally difficult with this kind of methods. Kurihara and Miyata realize accurate skin deformations from CT images [Kurihara et al. 2004], but it doesn't mention the possibility of retargeting. With our model, however, generated deformations can be retargeted. Once the model is constructed, accurate skin deformations are easily generated applying our model to a skin mesh. In our experiment, we construct a skin deformation model which reconstructs pronosupination, rotational movement of forearm, and we use range scan data as a skin mesh to apply our model and generate accurate skin deformations.	Accurate skin deformation model of forearm using MRI	NA:NA:NA:NA:NA:NA	2009
Satoko Kasai:Shigeo Morishima	The presence of CG character is essential in anime and movie contents recently. Especially, personality and aging factor are also important in character modeling. However, the modeling CG character is based on hand-made process yet, so it costs huge amount of money and labor to give one character several variations. About a facial animation, muscle based process or blend shape based process is very popular in contents production, however, in case of considering aging mechanism on face skin and bone, the different model of each age has to be constructed for every character.	Aging model of human face by averaging geometry and filtering texture in database	NA:NA	2009
Ming Tang	This poster presents a range of urban simulation techniques and multidisciplinary research across Architecture design, urban design, interactive design and game design, as well as visual effects. Through a series of design experiments using City Generator, the 3D urban simulation tool, I explored an integrated set of design methods, such as genetic computation, and real time simulation with game engine. The complexity of 3D urban form is procedurally controlled by 2D GIS (Geography Information System) data input and digital elevation model (DEM), which allows designers to efficiently shape urban growth in a preferred direction.	City generator: GIS driven genetic evolution in urban simulation	NA	2009
Lu Liu:Tao Ju	Describing shapes is an important task in graphics and vision. A simple, concise descriptor that captures the essential shape properties of an object would greatly facilitate computer-based understanding of the object and applications such as matching and segmentation. For this reason, medial axes (MA) has become a popular shape descriptor since its introduction by Blum [Blum 1967]. The MA of an N-D object is an (N -- 1)-D geometry centered within the object. For example, the MA of a 2D object consists of medial curves at elongated parts, whereas the MA of a 3D object consists of medial surfaces describing the protrusions on the object.	Defining and computing multi-dimensional skeletons	NA:NA	2009
Sebastian Pena Serna:Andre Stork	Dynamic simplicial meshes will enable real time unconstrained deformation of triangular and tetrahedral meshes regardless of topological limitations, by means of combining a quality measure with complex topological operations and smoothing techniques, which automatically improve and maintain the quality of the mesh, aiming at mesh and geometry processing applications.	Dynamic simplicial meshes	NA:NA	2009
David Nilosek:Karl Walli	Automated synthetic terrain and architecture generation is now becoming feasible with calibrated camera remote sensing. This poster implements computer vision techniques that have recently become popular to extract "structure from motion" (SfM) of a calibrated camera with respect to a target. This process will build off of Microsoft's popular "PhotoSynth" technique and apply it to geographic scenes imaged from an airborne platform. Additionally, it will be augmented with new features to increase the fidelity of the 3D structure for realistic scene modeling. This includes the generation of both sparse and dense point clouds useful for synthetic macro/micro-scene reconstruction.	Aerial scene synthesis from images	NA:NA	2009
Jingyuan Huang:Stephen Mann:Bill Cowan	Müller et al. introduced CGA shape, a shape grammar for procedural modeling of architecture [Müller et al. 2006b], which they applied to Mayan archaeological site in Xkipché [Müller et al. 2006a]. Inspired by this application of shape grammars to archaeology, we built a simple reconstruction application that uses a shape grammar to build 3D Inca sites from 2D plans. The application can help users to create a quick 3D overview of an Inca site that they are studying.	Inca reconstruction using shape grammar	NA:NA:NA	2009
Yoshiki Mizushima:Shuhei Nomura:Genki Umeizumi:Noriko Nagata:Yoshiyuki Sakaguchi	The need for rendering woven fabrics arises frequently in computer graphics[N Adabala, N Magnenat-Thalmann, G Fei 2003]. Woven fabrics have a specific appearance, luster, and transparency. We have proposed a BRDF/BTDF model using the Henyey-Greenstein function and an algorithm for the real-time rendering of woven fabrics based on the texture look-up table[Uno et al. 2008]. However, in order to make the model more accurate, the microlevel BRDF/BTDF is necessary. The objective of this study is to express a more detailed texture of the cloth by creating a 3D model that especially takes into consideration the "twisted structure" of the yarn in the fine woven structure of the cloth and by making the rendering algorithm more precise.	Lace curtain: modeling and rendering of woven structures using BRDF/BTDF: production of a catalog of curtain animations	NA:NA:NA:NA:NA	2009
E. Riegel:T. Indinger:N. A. Adams	Within computational fluid dynamics (CFD) the Navier-Stokes (NS) equations are traditionally used to describe the physical properties of the fluid. An alternative approach to classical discretizations for the numerical solution of the Navier-Stokes equations, such as Finite-Difference and Finite-Volume schemes, is provided by the Lattice-Boltzmann equations [Benzi et al. 1992], [Chen and G. 1998]. The Lattice-Boltzmann method (LBM) uses a Cartesian grid for propagating and relaxing a discrete velocity distribution function on a lattice at discrete time steps. Usually a very large number of cells is necessary to obtain an accurate prediction of the macroscopic scales for pressure and velocity. However, due to the simple formulation of the underlying algorithm this method is well suited for parallelization and hardware acceleration using general purpose graphical processing units (GPGPU). LBM is used in engineering software for example to compute the aerodynamic drag of a car to improve its efficiency. Therefore LBM has a big practical importance. Improving the performance of a CFD simulation gives the engineers more time and better feedback during the engineering process leading to more efficient engineering processes and more efficient engineering products. Moreover a strong acceleration in simulation performance makes a new quality of physical simulation technology available for desktop computer software like entertainment and content creation software.	Numerical simulation of fluid flow on complex geometries using the Lattice-Boltzmann method and CUDA-enabled GPUs	NA:NA:NA	2009
Yu-Bin Yang:Jin-Jie Lin	In content-based 3D mesh retrieval, graph-based structure is one of the most important shape descriptors. The early work on this issue can be found in [Hilaga et al. 2001], in which a 3D mesh representation, Multiresolutional Reeb Graphs (MRGs), was proposed. Since then, many Reeb-Graph based descriptors have been designed to simplify and represent 3D meshes. However, most of those descriptors are only suitable for graph-based topology matching, which is time-consuming and unreliable. To address this issue, we propose a novel approach to representing 3D mesh by using a tree-like structure, Attributed Root Trees (ARTs). The advantages of our method are three-fold: (1) Tree matching is easier and efficient than graph matching; (2) The representation well reserves topological information of a 3D mesh, thus topology matching can be easily completed; (3) It is possible to perform multi-resolution matching on ARTs.	Representing 3D mesh with attributed root trees	NA:NA	2009
Ross Sowell:Lu Liu:Tao Ju:Cindy Grimm:Christopher Abraham:Garima Gokhroo:Daniel Low	MRI and CT scanners have long been used to produce three-dimensional samplings of anatomy elements for use in medical visualization and analysis. Physicians often need to construct surfaces representing the anatomical shape in order to conduct treatment, such as radiating a tumor. Traditionally, this is done by a time-consuming process in which an experienced physician marks a series of parallel contours that outline the object of interest.	User studies on the feasibility of oblique contouring	NA:NA:NA:NA:NA:NA:NA	2009
Trishul Mallikarjuna	This document introduces a conceptually novel, simple and ordinarily robust computer-vision-based method of extracting musical beats from regular physical gestures of a performer, implemented in VisiBeat: a grid-based percussion system on the Max/MSP/Jitter platform for collaborative interactive music.	A visual beat detection system for grid-based interactive percussion and synchronization	NA	2009
M. Cicconet:I. Paterman:P. Carvalho:L. Velho	Hardware based musical instruments are, in general, from the performer point of view, merely copies of real physical instruments. They do not provide facilities for being played, especially for musically untrained people.	The blues machine	NA:NA:NA:NA	2009
Sergio Krakowski:Luiz Velho:Francois Pachet	In this work, we address to the problem of making the machine listen and react to the musician in an improvisation situation with the purpose of generating high-quality music.	Pandeiro funk: experiments on rhythm-based interaction	NA:NA:NA	2009
Moohyun Cha:Jaikyung Lee:Byungil Choi:Hyokwang Lee:Soonhung Han	In order to simulate and visualize natural phenomena, especially fluid behavior such as smoke and fire, many novel studies have recently been conducted. Usually these methods use CFD (computational fluid dynamics), which calculate Navier-Stokes equations in real-time to generate realistic fluid motion and interactions, as well as high-performance GPU technologies. We proposed a new approach to the visual simulation of fluid flow by combining the use of pre-calculated CFD data with the real-time processing of such data. As the domain-specialized CFD solver predicts detailed fluid dynamics to an accuracy of a guaranteed error range, we could provide nearly actual behaviors of a fire-driven fluid flow. Moreover, this CFD data includes physical quantities such as temperature distribution, which can provide useful information to the training evaluation process. However, the data-driven method requires appropriate data processing techniques to create and manage large data sets. In this study, we developed a firefighter training simulator to demonstrate our proposed methods and explore related research issues.	A data-driven visual simulation of fire phenomena	NA:NA:NA:NA:NA	2009
Ippei Takauchi:Masatoshi Ochiai:Hiromu Saito:Ryo Asakura:Motofumi Hattori	In this paper, the authors discuss how to make 3DCG animations of flowers, wings, and cloths etc. which are modeled by surfaces. These 3DCG animations are obtained based on the numerical simulation for the Newtonian dynamics equations of surfaces. These equations are obtained from the potential functions (the energy functions) of the surfaces.	Desired deformation of continuum surfaces in 3DCG animation by time varying stable forms: application to make animations of flowers, wings, cloths etc.	NA:NA:NA:NA:NA	2009
Kazuhiko Yamamoto	There has been growing interest in the sound generating technique based on physics from the motions of 3d graphics objects. In recent work several methods have been proposed to physically simulate these audio events natably using modal synthesis [K. van den Doel et al. 2001] or finite element method [O'Brien et al. 2002]. However, in these mesh-based method, it needs complicated operation to preprocess, for example, to generate the computational mesh for 3d object, and to create the system's matrices, etc...	The framework of sound rendering for particle-based physics	NA	2009
Alejandro L. Garcia:Alice A. Carter:J. Courtney Granner:David Chai	"Physics for Animation Artists" is a joint project by the Department of Physics and the Animation/Illustration Program at San Jose State University to develop a physics curriculum specifically for art majors planning to enter the animation industry.	Physics for animation artists	NA:NA:NA:NA	2009
Biswarup Choudhury:Pisith Hao:Sharat Chandran	Amongst all atmospheric phenomena, rain is probably the most commonly used effect to create realistic immersive virtual environments, and to set the mood in movie storytelling. Although not immediately obvious, the beauty of rain emanates from the interplay of the involved light-matter interaction, generating effects of refraction and reflection, coupled with scattering effects. At the core, rain consists of water droplets under the influence of gravity. Current state of the art methods of generating rain are either computationally burdening, or not realistic enough. The key idea we introduce in this paper is to consider these droplets as transparent objects in the environment matting (EM) framework. This enables careful preprocessing to discover the light transport phenomena. We end up with a free-viewpoint real-time technique of simulating realistic droplets and rain in novel environments.	Real-time droplet modeling using color-space environment matting	NA:NA:NA	2009
Adrien Herubel:Venceslas Biri:Farchad Bidgolirad	In computer graphics, physically-based global illumination algorithms such as photon-mapping [2001] have a linear progression between complexity and quality. To a given quality, rendering time scales linearly with computer performances. With Moore's law call in question and increasing demand in quality, those algorithms need more and more optimisations.	Autonomous lighting agents in global illumination	NA:NA:NA	2009
Graham Fyffe	Image-based relighting is a powerful technique for synthesizing images of a scene under novel illumination conditions, based on a set of input photographs. While successful relighting methods exist, they either require many photographs [Debevec et al. 2000], or operate on a limited class of materials or illumination conditions [Ma et al. 2007][Ramamoorthi 2006].	Cosine lobe based relighting from gradient illumination photographs	NA	2009
Hiroyuki Kubo:Mai Hariu:Shuhei Wemler:Shigeo Morishima	Simulating sub-surface scattering is one of the most effective ways to realistically synthesize translucent materials such as marble, milk and human skin. In previous work, the method developed by Jensen et al. [2002] improved significantly on the speed of the simulation, yet still cannot produce real-time rendering. Thus, we have developed a simple local illumination model which mimics the presence of a subsurface scattering effect. Furthermore, this approach is easy implementable on the GPU and doesn't require any complicated pre-processing as is often the case in this area of research [Mertens et al. 2003].	Curvature-dependent local illumination approximation for translucent materials	NA:NA:NA:NA	2009
Greg Nichols:Chris Wyman	Area light sources are common in the real world, and thus important in realistic images. However, interactive rendering with area light sources is challenging, as each surface in a scene can receive light from every point in the area light. This problem is similar in nature to the rendering of single-bounce indirect illumination, and can be addressed with similar techniques.	Direct illumination from dynamic area lights	NA:NA	2009
Sajid Farooq:J. Paul Siebert	Gaussian Projection is an algorithm based on the Gaussian Pyramid, that can render point-sampled-geometry - obtained from stereo data in the form of range images - without polygonization, and at full native resolution. Gaussian Projection makes use of the GPU to perform efficient and fast multi-resolution rendering of point-based data, with automatic hole-filling (scattered point interpolation), and without any preprocessing other than Image Pyramid generation.	Gaussian projection: a novel PBR algorithm for real-time rendering	NA:NA	2009
Cyril Crassin:Fabrice Neyret:Sylvain Lefebvre:Miguel Sainz:Elmar Eisemann	Voxel representations are commonly used for scientific data visualization, but also for many special effects involving complex or fuzzy data (e.g., clouds, smoke, foam). Since voxel rendering permits better and easier filtering than triangle-based representations it is also an efficient high-quality choice for complex meshes (with several triangles per pixel) and detailed geometric data (e.g., boats in Pirates of the Caribbean).	Beyond triangles: gigavoxels effects in video games	NA:NA:NA:NA:NA	2009
Borom Tunwattanapong:Paul Debevec	We present a technique for relighting an image such that different areas of the image are illuminated with different combinations of lighting directions. The key idea is to capture illumination data using a lighting apparatus system such as Hawkins et al. [2004], calculate radial basis function interpolation of light constraints specified by users and render the calculated illumination result in realtime using GPU. The application can simulate the result of unnatural lighting conditions, for example, the image of a whole face lit from per pixel view dependence reflection angles or from gazing angles (see Fig. 1, a). The application can also render a high-resolution result at 1920 x 1080 in three to four minutes.	Interactive lighting manipulation application on GPU	NA:NA	2009
Kenshi Takayama:Takeo Igarashi	In our previous work of lapped solid textures [Takayama et al. 2008], layered (or 'type 1-b') texture exemplars were used to create solid textured models such as strata and cakes. However, no methods have been proposed so far to synthesize this kind of texture automatically. This poster proposes an extension of Kopf et al.'s method [2007] to synthesize such layered solid textures from single 2D exemplars.	Layered solid texture synthesis from a single 2D exemplar	NA:NA	2009
Seungju Bang:Kyoungju Park	In oriental paintings, artists have developed a unique style that exploits the effects of the dispersion of the ink, composed of soot and glue, onto absorbent paper. These artists produce these effects purposely by manipulating the ink concentration, stroke speed and brush angle. We describe these artistic styles of oriental painting and show how we render a single image in the oriental painting style. Our work is to take a single image as input and produce an oriental brushwork-like image automatically as a result. This nonphotorealistic rendering do not have richness of physical painting system such as 3-D brushes but reproduce artistic style of paintings to an image with a speed. In recent years, oriental brushwork rendering have been applied for 3-D objects e.g. Non-photorealistic rendering in Chinese painting of animals by Yeh and Ouhyoung 2002 and stereo images e.g. Humanistic oriental art created using automated computer pocessing and non-photorealistic rendering by Cheok et al. 2007 that require geometric details and depth information, but our method proposes a system that reproduce the styles and effects of oriental paintings from single 2-D image by using a set of image processing techniques. Figure 1 shows an input image and the reproduced oriental paintings. We focus on reproducing the stroke drawing and artistic shades that are essential in conventional oriental paintings. Some artistic styles of painting exploit the variety of lines and deep shades at once, giving a charming feel to the oriental brushwork. One general technique is "gu-ru law"Figure 2(a), in which artists draw tufted thin lines for boundaries and geometric details, and brush the interior with abstract but contrasting shades of ink. Another artistic style shades the whole objects instead of drawing its boundary with lines Figure 2(b), and paints near boundary regions with highly contrasting intensities. Our overall framework consists of two modules: stroke drawing and artistic shading Figure 3. These modules, essential to oriental painting, are generated using a set of techniques. Abstraction: Given an input image, we simplify it for geometric abstraction and color quantization using bilateral filtering. The abstracted image is then used as the input for our stroke drawing and artistic shading modules. Stroke drawing: We extract and draw representative lines for boundaries and geometric details. Line extraction includes selecting feature points, discarding small features, and clustering and linking similar points. Next, the spline curves are fitted to the linked points after the linked points are once again subdivided into several clusters according to their size. Line fitting produces smooth curves of appropriate length and curvature, similar to human-drawn strokes. Then, the curves' thickness is defined as proportional to the curvatures. Artistic shading: While oriental ink interacts with absorbent paper, ink disperses on the paper as water flows, the concentration of ink leaves bright-and-dark shades, and the residual ink disperses along the direction of the paper's fibers. We reproduce these effects by following steps. First, filtering over time is used to reproduce the ink dispersion effects when water spreads and stops, by applying blurring and sharpening filters consecutively. Next, we reproduce the bright and dark shades that arise from irregular ink concentration by stretching lightness contrast nonlinearly based on intensity values from blurred images. Finally, we reproduce the local patterns due to dispersion along the paper's fibers by using texture masks that are similar to textures of absorbent paper fibers. Composition: We compose the final results by combining stroke drawing and artistic shading, and by adding textures of absorbent papers for realistic effects. We produce the final oriental painting rendering as a composite of the results of stroke drawing and artistic shading from a single 2-D image. Figure 3 shows that strokes drawing; (a) is a map of edge by canny's edge filter, (b) is for clustering, linking sampled features and discarding some groups which have few features, (c) presents a thickness in each group, artistic shading; (d) Filtering overtime from input image, (e) contrast stretching, and (f) applying the mask of paper-fiber.	Oriental stylization with strokes and shades	NA:NA	2009
Yoon-Seok Choi:In-Kwon Lee:Bon-Ki Koo	This study describes a fully automated system for generating caricature images by using a painterly rendering method. This system transforms photos into caricature images automatically. A few similar approaches have been proposed by other researchers including [Gooch et al. 2004] and [Liang et al. 2002]. These methods, however, did not produce satisfying results, as the caricatures produced did not resemble handiwork. By simulating the brush strokes used by painters [Park et al. 2006], we reproduced the brush painting technique and incorporated it into our caricature system.	Painterly caricature maker	NA:NA:NA	2009
Tae-Joon Kim:Bochang Moon:Duksu Kim:Sung-Eui Yoon	Bounding volume hierarchies (BVHs) are widely used to accelerate the performance of various geometric and graphics applications. These applications include ray tracing, collision detection, visibility queries, dynamic simulation, and motion planning. These applications typically precompute BVHs of input models and traverse the BVHs at runtime in order to perform intersection or culling tests.	RACBVHs: random-accessible compressed bounding volume hierarchies	NA:NA:NA:NA	2009
Masashi Baba:Naoki Asada	Nowadays metallic paints are used in many situations. Although a lot of industrial products are painted because of the significant appearance, the reflection of the metallic paint is very complex and it is difficult to generate a photo-realistic image of a particular metallic paint. Recently, Rump et al. [Rump et al. 2008] proposed a method to acquire the reflectance and to generate photo-realistic images of metallic paints. They used BTF to capture and represent flakes in metallic paints, however, it is hard to capture and to store. In this paper, we propose a simple model to express the metallic paints including the sparkling effect of the flakes in metallic paints.	Reflection model of metallic paints for reflectance acquisition	NA:NA	2009
Jan-Phillip Tiesel:Christoph W. Borst	We describe an efficient single-pass rendering approach for composable 3D volumetric lenses. Composing rendering effects by intersecting multiple 3D lenses is a logical and intuitive extension of the Magic Lens metaphor and volumetric lenses. However, 3D lens composition was once considered intractable and recent multi-lens approaches require a number of passes that can grow exponentially with lens count. They generally involve substantial per-frame data structure generation or advanced techniques such as depth peeling. In contrast, we summarize a simple and effective technique that renders intersecting lenses of various shapes and effects in a single pass, does not require the maintenance of costly data structures, and can easily be incorporated into existing real-time rendering systems. It also supports more flexibility in the way complex lens effects are combined by using shade tree concepts to build composite shader programs.	Single-pass rendering of composable volumetric lens effects	NA:NA	2009
Kuntee Viriyothai:Paul Debevec	We present a technique for sampling the light probe image using variance minimization. The technique modifies median cut algorithm for light probe sampling [Debevec 2005] so that the variance of each region is minimized. The algorithm is fast, efficient, and easy to implement.	Variance minimization light probe sampling	NA:NA	2009
Christian Lipski:Christian Linz:Kai Berger:Marcus Magnor	We present an image-based rendering system to viewpoint-navigate through space and time of complex real-world, dynamic scenes. Our approach accepts unsynchronized, uncalibrated multi-video footage as input. Inexpensive, consumer-grade camcorders suffice to acquire arbitrary scenes, e.g., in the outdoors, without elaborate recording setup procedures. Instead of scene depth estimation, layer segmentation, or 3D reconstruction, our approach is based on dense image correspondences, treating view interpolation uniformly in space and time: spatial viewpoint navigation, slow motion, and freeze-and-rotate effects can all be created in the same fashion. Acquisition simplification, generalization to difficult scenes, and space-time symmetric interpolation amount to a widely applicable Virtual Video Camera system.	Virtual video camera: image-based viewpoint navigation through space and time	NA:NA:NA:NA	2009
Phoebe Coleman:Bill Elder	A 3D medical animation that visually describes the role of the Cystic Fibrosis Transmembrane conductance Regulator, which is directed by the code dictated by the Cystic Fibrosis gene in chromosome number seven.	The journey of the Cystic Fibrosis gene	NA:NA	2009
Takashi Nariya:Young Ah Seong:Tomoko Hashida:Takeshi Naemura	Global warming is one of many urgent problems we face and a positive individual attitude to the environment is necessary. The goal of our project is to raise awareness to the carbon dioxide (CO2) surrounding people which is a main factor causing global warming.	Spatio-temporal sensing and visualizing of CO2	NA:NA:NA:NA	2009
Alejandro Aguilar-Sierra	Modern technology allows to improve teaching methologies by allowing interactivity and to involve more senses in the learning process at an affordable price. In particular, visual learning has proved to be a very important way to understand otherwise elusive scientific principles. Our Visualization Laboratory for Earth Sciences is aiming to be a visual learning environment for Earth Science graduate students and at the same time to be a training platform for Computer Science undergraduates specializing in Graphics Application Programming. The importance of visual learning, specifically for Earth Sciences, has been examined previously [McGrath and Brown 2005], [Reynolds 2005].	Visualization laboratory for Earth Sciences: a multidisciplinary visual learning environment	NA	2009
Lisa Blum:Wolfgang Broll:Stefan Müller	Fascinated by a stunning variety of corals and fishes or mysterious wrecks more and more people are attracted by snorkeling and diving adventures. Virtual Reality scenarios like the virtual oceanarium [Froehlich 2000] try to satisfy this interest by allowing for discovery of underwater worlds in a riskfree and comfortable way, but a realistic feeling of diving is never achieved by virtual submarine worlds.	Augmented reality under water	NA:NA:NA	2009
Bruno Fernandes:Joaquin Fernández	Augmented Reality (AR) techniques have been applied to many application areas; however, there is still research that needs to be conducted on the best way to interact with AR content. Since hands are our main means of interaction with objects in real life, it would be natural for AR interfaces to allow free hand interaction with virtual objects. We present a system that tracks the 2D position of the user's hands on a tabletop surface, allowing the user to move, rotate and resize the virtual objects over this surface. Our implementation is based on a computer vision tracking system that processes the video stream of a single usb camera.	Bare hand interaction in tabletop augmented reality	NA:NA	2009
Hiroshi Mori:Kazuhito Shiratori:Tomoyuki Fujieda:Jun'ichi Hoshino	We propose the wellness entertainment system Versatile Training Field (VTF). In this system, we use the flexible foot interface as the input device. The system enables the user to move and jump freely in VR space by exaggerated movement corresponding to walking or jumping on the mini trampoline of the flexible foot interface. Improvements in exercise motivation and support for continuous exercise are achieved in our system, since it is possible to enjoy strolling through a virtual space, which is usually difficult to experience, by exercising on the mini trampoline without injury to the user's joints.	Flexible foot interface for versatile training field	NA:NA:NA:NA	2009
Takafumi Aoki:Hironori Mitake:Shoichi Hasegawa:Makoto Sato	We propose a new method for Symmetrical Haptic Interaction System with Virtual Creatures (VCs) in Mixed Reality. It's achieved by small and light haptic interface and Reactive VCs with touch sensations. People can touch VCs directly by fingers and watch their reaction. And VCs also can touch us directly.	Haptic ring: touching virtual creatures in mixed reality environments	NA:NA:NA:NA	2009
Daniel Makoto Tokunaga:Ricardo Nakamura:Romero Tori	The digital games industry is always looking for innovation in user experience. A new trend in this field is the use of Augmented Reality (AR) techniques for intuitive, novel game interfaces [Bernardes Jr et al. 2008]. Among the several technologies related to AR, video avatars are one of the most attractive for games, because they allow the insertion of the game player's image in the game. Furthermore, the availability of commodity stereo cameras makes it feasible to employ 3D video avatars in consumer games. However, when a non-photorealistic rendering style is required for design reasons (often the case for games), the use of a conventional video avatar, even with 3D information, results in visual inconsistencies. This work presents a new approach to enable nonphotorealistic rendering of a real-time 3D video avatar. The project builds upon a 3D video avatar system designed for teleconferencing over Internet 2 for educational purposes, extending it with this new rendering method. The expected result is a reduction in the visual and cognitive mismatch between player image and synthetic environment, allowing for a more immersive experience.	Non-photorealistic 3D video-avatar	NA:NA:NA	2009
Sheila Tejada	The mixed-reality game Robot RockStars combines a massively-interactive videogame controlled by multiple WiiGuitar players with a herd of Pleo and AIBO wireless robots as back-up singers, which can both react and affect gameplay. Attendees can affect gameplay by interacting with the robots or with mobile wireless devices. The main innovation lies in this project with the novelty of creating open-source, mixed-reality, plug-n-play tools that we build and the challenge for applications, such as the mixed-reality game Robot RockStars, to be constructed with them, allowing technology to go beyond what is currently possible for massively interactive collaboration between people and agents.	Robot rockstars: a mixed-reality game	NA	2009
Ryo Kishi:Yasuaki Kakehi:Takeshi Naemura	In this paper, we present a novel persistence of vision display named SteganoScan. SteganoScan is a stick shaped display device with a single line of LEDs that emits light in full color and shows 2D images without any screen.	SteganoScan: persistence of vision display with pixel-level visible light communication projector	NA:NA:NA	2009
