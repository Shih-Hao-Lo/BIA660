James L. Mohler	NA	Session details: Educators program	NA	2018
Gary R. Bertoline:Cary Laxer	Computer graphics is a powerful medium used to communicate information and knowledge. It is a discipline whose time has come. Until recently it was a mysterious specialty involving expensive display hardware, considerable computing resources, and specialized software. In the last few years, computer graphics has found its way into the mainstream of society, from entertainment, to engineering design, to the web, and virtually every industry. Much of this has been the result of spectacular improvements in the price and performance of computer graphics hardware and software. Interactive computer graphics is finding its way into nearly every discipline, industry, home, hospital, theatre, football stadium, automobile, appliance, and engineering office. Computer graphics is or will have an impact on nearly everything we do. The discipline of computer graphics is like a wide-open frontier where no matter which direction you move you will find more opportunities and undiscovered applications. Computer graphics is the pictorial synthesis of real or imagined objects from their computer-based models. A related field is image processing which is the reconstruction of models of 2D and 3D objects from their pictures (Foley, et al, 1996). Traditionally, computer graphics is contained within the well-recognized disciplines of computer science, electrical and computer engineering, and art and design. The discipline of computer graphics has traditionally focused on developing new software algorithms (computer science) and hardware innovations (electrical and computer engineering) with an attempt to force art and design principles into the mix. Those types of developments will continue but the discipline is beginning to mature to a point where it is more than just about hardware and software developments and innovations. More attention must be given to the effective and novel use of the hardware and software developments in the context of graphics communication. This is a much more holistic approach to computer graphics as a discipline. This holistic approach is more focused on the end user and how these tools can be used in much more profound and revolutionary ways. Obviously there have been many major and profound applications of computer graphics in its short history. However, the recent past of computer graphics is but a prelude of what is to come. Computer graphics can and will touch every human in a number of ways from entertainment to assisting in finding the cure of the most dreaded diseases. Computer graphics can and will have a profound effect on every type of business, industry, government, education, and the home. But it will take a very special type of education to prepare this next generation of computer graphics specialists. Computer graphics is a powerful medium but only if combined with the principles of information design. The principles of information design are universal, like mathematics, and are not bound to unique features of a particular language or culture. The universal nature of the graphic language makes it a powerful tool in today's society where collecting, analyzing, and communicating all the available information is so important. Information becomes knowledge and knowledge can be communicated more efficiently through computer graphics. Knowledge can become power and is the catalyst for stunning new developments in virtually every field. The pure computer graphics discipline of the future will not be in computer science, art and design, technology, or electrical and computer engineering. The computer graphics discipline of the future will have its legacy in all these disciplines but will look to merge the software and hardware technology with the human communication process, which will result in novel ways of solving problems and disseminating information. As shown in Figure 1, computer graphics is the overlap between art, science and technology, and psychology. The ideal student of this emerging discipline is bright, articulate, visual, analytic, and motivated by a passion for computer graphics. This student uses both sides of their brain but is keenly focused on the visual mode to solve problems. They are the modern-day Da Vincis, capable of visualizing what is nonexistent and finding solutions to complex problems. New opportunities are unfolding that require special talents and abilities for people with high visualization abilities who can use computer graphics tools to visualize scientific concepts and for the analysis and manipulation of complex three-dimensional information. As these new opportunities continue to unfold, the skill in manipulating and creating imagery may become more important than skill with words and numbers. Different kinds of tools may require different talents and favor a different type of discipline.	A knowledge base for the computer graphics discipline	NA:NA	2018
Jason Della Rocca:Robin Hunike:Warren Spector:Eric Zimmerman	Electronic gaming has changed. What was a curiosity twenty years ago is now one of the most popular forms of entertainment for all age groups. Games are now an accepted, pervasive part of US and world culture. The new ubiquity of games demands that we understand them as commercial products, aesthetic objects, learning contexts and cultural phenomena. This, in turn, creates a demand for people to fill a variety of roles associated with the creation, consumption and analysis of games. For gaming to have a healthy future, industry and academia must find or create new contributors with new goals, ideas and experiences. We believe that educators have the best chance of creating and fostering these new goals and ideas while industry brings to the table a vital experiential understanding of the realities of game development. Academia and industry must work together, then, if progress is to be made. Schools have begun to recognize the cultural and academic relevance of games. In addition to their increasing presence in the fabric of student life, games are rapidly evolving -- diversifying and broadening their role as a communication medium. As universities design academic programs for the critical analysis of games and begin researching related interactive entertainment technologies, they see the value of including industry veterans in their plans. The International Game Developers Association's Education Committee, comprised of both developers and academics, has been working on a curriculum framework and set of guidelines to aid in the development of game-oriented curriculum. During this session, the Education Committee's Curriculum Subcommittee will present its work to date, and solicit input and criticism on the framework. The goal is to generate significant feedback from the academic community to further refine the framework. Here are some of the guiding principles used to develop the curriculum framework: • Gaming is an interdisciplinary field. This curriculum framework presents a wide range of topics encompassed by the acts of game creation, analysis and criticism. We strongly advocate cross-disciplinary study in any game-related course or curriculum. • Analysis, practice and context are equally important. Game development students must be exposed to analytical, practical and contextual materials. • The framework was designed to be versatile. Acknowledging the realities of educational institutions, especially when it comes to the introduction of new disciplines to the curriculum, we felt we had to create a game studies outline that was as versatile as possible. • The framework is not vocational but is career-oriented. The curriculum framework is focused not on churning out developers, per se, but on turning out well-rounded students of gaming, some of whom will no doubt choose to become developers. The IGDA's curriculum framework and related endeavors can be found online at the IGDA web site: http://www.igda.org/Committees/education.htm	Game development, design and analysis curriculum	NA:NA:NA:NA	2018
Donna Cox	Originally, "university" was given its name because it was believed that a student would go to the university to find his or her place within the universe. Today's liberal arts education has limited this universal process and forces an early decision: the "major." Students follow a well-defined curriculum that often precludes interdisciplinary study and cross-campus interaction. While most universities provide structure for in-depth, discipline-specific education; few offer curricula that take the student over many academic routes before making a decision on the major. Technical requirements can be the most constraining due to the vast foundational knowledge that must be mastered in the sciences and engineering. Educational reward and economic systems force segregation of the disciplines. Faculty are often rewarded to be discipline specific, in both research and teaching. Students are not able to take classes across campus because the "major's" students fill the courses leaving no room for non-majors. Universities are struggling due to low budgets and cutbacks. And it is very difficult to manage students who want to create new degrees that span campus boundaries. In contrast to this segregation of disciplines, the computer graphics industry often demands generalist knowledge. Often a programmer is required to design or a designer is required to have advanced technical skills. It is apparent in the area of computer graphics, visual literacy is one of the most critical skills that span both art and science [West 1991]. A National Research Council committee studying these issues found a strain on the talent pool in 3-D computer graphics and that universities are failing to provide adequate cross-disciplinary training: "The development of workers with a mixture of technical and artistic capabilities represents a particular challenge because of its interdisciplinary nature. Whereas computer science and electrical engineering departments will train technical workers to address questions about networking and distributed simulation, the creation of visually literate workers demands cooperation between engineering and art departments, which are separated by large cultural and institutional gaps" [Zyda et al. 1997]. These cultural divisions have been talked about since C. P. Snow's essay on "Two Cultures." However there are many cultures and cultures within cultures. Within the educational system, there are many illogical divisions within and among departments. Certainly the divide between science and art is the greatest in education. How does this type of education prepare the student to find himself or herself in the universe? Given the constraints of contemporary educational systems, how can we educators inspire the Renaissance person, one who has truly interdisciplinary knowledge and skills? Issues to consider: • How do we focus students' attention without constraining the breadth of knowledge? • As educators, we are asked to prepare students for the "market"...do we really know the market? • Should artists be taught to develop software as part of their curriculum? • What are examples of organizing projects that encourage cross-disciplinary thinking? • How does one find willing collaborative partners on large campuses? • How can technology help in this quest to match collaborators and facilitate interdisciplinary activity? • How can we change or by-pass organizational structures that inhibit the Renaissance person? • Do collaborative teams in the classroom facilitate or hinder this kind of thinking? • Given the vast knowledge that an interdisciplinary study must cover, should college terms be extended?	Inspiring the renaissance person	NA	2018
Dena Elisabeth Eber:Bonnie Mitchell:Heather Elliott	Despite the artistic maturity computer graphics has gained throughout the 1990s and into the new millennium, what we call the art student "wow" factor in computer graphics---the phenomenon of student obsession over new technology instead of artistic substance---is still a point of contention. It remains a challenge to teach computer graphics to art students while maintaining a balance between thoughtful art and sophisticated technology. In this session we will, with the help of the participants, reveal pedagogical solutions and uncover various approaches to teaching art students to use digital media in rich and expressive ways. When the curve of new computer graphics technology was steeper and interfaces were not as intuitive as they now are, much of student learning was centered on the hardware and software and students struggled to comprehend digital media's place in the art world. In essence, we were in what Marshall McLuhan referred to as the first phase of a new technology---a stage in which, in the context of teaching digital media in the arts, the students were trying to understand computer graphics technology in terms of what they already knew. A digital image became a painting or a photograph, and 3D modeling and animation were understood in terms of film studies. Further, many students did not understand the scope of digital art and how it encompassed sculptural digital installations, how the art did not have to result in physical objects, or how the pieces could exist as interactive programs or websites. Students also worried more about the technology---what it could do and how to use it---rather than understanding the unique ways in which they could create aesthetically pleasing and profound works of art. Today, computer graphics-based art is now closer to being established in the art world and embraces a more intuitive and sensible mode in which to create. However, the "wow" factor is still a problem and the changing pace of technology still keeps students and teachers alike tuned into the tool. Students are still trying to understand what the technology can do and often forget why or what they are making. Art instructors have a genuine struggle with balancing the time to teach students how to do something digitally with teaching them how to express themselves visually. Further, some software programs are truly complex and the concepts behind them require the student to understand, at least intuitively, the laws of physics, lighting, and how things change over time. Perhaps in some ways we are still in the first phase of digital technology in the arts. From uncomplicated to elaborate software, how can instructors approach lessons without being bogged down with too much technology? Should instructors teach software or should they teach concepts, thus emphasizing the importance of technology as a means to an end rather than the end? What are some examples of lessons, assignments, or overall approaches that give proper weight to art and technology? How can teachers facilitate unique and expressive works of art? Even if instructors are successful teaching individual expression with computer graphics technology, what domain within this vast field is most important? What are some areas on which teachers could focus their energies? Are some computer graphics disciplines harder to teach than others? Is it more difficult to express ideas and compose formally beautiful works in some forms of digital graphics? Why or why not? The best lessons and the best pedagogical approaches still will not hinder the dedicated special effects gurus from obsessing about what the software can do instead of the resulting work. How can instructors help these students care more about the art works? Finally, and maybe most importantly, how can teachers keep up with the technology? Perhaps it is the teacher's concern with keeping current that influences the way they teach and what the students perceive as important. What are some solutions for instructors to keep up with changing computer graphics technology without compromising the core of the art they want to teach? We ask the participants to come prepared to talk about some of these ideas and more. Once we target issues to resolve, we will ask the forum participants to put forward their gems: lessons, approaches, philosophical bends, and ideas that have worked or they believe will work. Attendees will leave the session with a notebook full of ideas and solutions to teaching students how to be expressive and create aesthetically pleasing works of computer graphics based art.	Teaching gems for art and design	NA:NA:NA	2018
Bruce Wands	This forum will give attendees a chance to present their own views on creativity and curriculum, as well as hear those of educators from a diverse group of colleges. Computer graphics education has grown tremendously in the last five years, particularly on the department level. Many issues have arisen related to the place of computer graphics education within a specific department's curriculum. They include the type of courses offered, challenges arising from the impact of this added educational component, and the desire to maintain traditional art education elements, such as theory and critique. Creativity has historically been addressed in theory and critique classes. It is also now being taught along with software in computer graphics classes. Outcomes from this forum will allow educators to gain insight into their approach to nurturing creativity as it relates to computer graphics education. Other topics to be discussed include interdisciplinary approaches to curriculum, creating content for courses, and the relationship of computer graphics to traditional art education. The role of creativity in computer science classes, particularly programming, will also be discussed.	The role of creativity in computer graphics education	NA	2018
Mike Bailey:Steve Cunningham:Lew Hitchner	Moderators and attendees of this forum will present, discuss, and assess examples of "best practices" for teaching computer graphics to computer science and engineering students. Types of "teaching gems" to be presented include: classroom lectures and demonstrations, lab exercises, homework projects, self-instruction techniques such as tutorials, demo programs, and Web applets, use of analog models, examples from industry and the rest of the "real world", and field trips (real and virtual). The forum moderators will contribute presentations by electronic submission from virtual attendees prior to the conference and by attendees on site during the forum session. Three outcomes are anticipated from this forum: 1. presentation of several teaching gems, 2. group analysis and discussion of how best to apply each gem in a course and it's learning effectiveness, and 3. posting of the results for public access on the SIGGRAPH Education Committee web site, http://www.siggraph.org/education.	Teaching gems for computer science and engineering	NA:NA:NA	2018
Darlene Wolfe:Jeff Scheetz:Roger Cotton:Timothy Comolli:Chris Stapleton	Professional animators and digital effects artists and companies often work with high school teachers to explore relationships that can be developed and maintained between high schools and industry. The goals of this panel are to make more educators aware of the on-line and in person mentoring, sharing, and peer training that can occur in graphics, animation and related fields for 1) improving curricula and integrated technologies, 2) creating or furthering personal student interest or as a career vehicle and foundation, and 3) encouraging teachers to become involved in, and comfortable with, the new media. When Josh Spector wrote "Studios drawn to CGI features" for the Hollywood Reporter [January 2, 2002], he noted the studios™ big interest in animation and computer graphics based on Shrek, Monsters, Inc. and the realization that there were audiences for the non-Disney animated feature. Youth of all ages, as well as adults, not only enjoy CGI, but can also create it themselves with the current technology on home computers. Today's high school students, with the current technology available in home machines, are able to create products comparable to that of professionals, using computer technology that was very expensive just a few years ago. Although many students do not have access to the Internet or recent computer technology, those that do are producing programs for the Internet, their schools, and even, for sales. To gain and maintain the skill, one must practice. To give every child an even chance, the technology must be made available through education. Ideally, students would be given or lent computers with appropriate software for their classes, skills and academic or hobby interests. In order to do that, administrators and teachers today must first recognize the convergence of electronic media, information technology, communication, and entertainment. Then plans must be made or programs adapted to further the educational needs and foundations of the student (at any age). Therefore, it is imperative that teachers and administrators integrate 21st century technology into the curricula. However, we are sometimes afraid or hesitant to make changes we do not thoroughly understand, much less that are new to us. Many of today's educators are nearing retirement and there is a growing shortage of teachers. There is usually a shortage of funds in public education, too. Schools, therefore, often fall behind in the technology learning curve. New and exciting curriculum may be welcomed with open arms and with great desire but a lack of resources, combined with fear and trepidation by the very people who must learn and teach it, may negate its implementation.	K-12 and industry partnering	NA:NA:NA:NA:NA	2018
Roberta Tarbell:LiQin Tan	"Animating Art History for Teaching" is a creative learning methodology that incorporates new perspectives for introductory art history courses through digital technology created by computer animation and art history professors and their students.	Animating art history for teaching	NA:NA	2018
Anne Morgan Spalter	Reuse is vital in the education world because the time and money necessary to create high quality educational software is prohibitive. Estimates for the cost of creating a single well designed, highly graphical and interactive online course in the commercial domain range from several hundred thousand dollars to a million or more. Thus the idea of reusable software components that can be easily shared is tremendously appealing. In fact, "component" has become a buzzword in the educational software community, with millions of dollars from the National Science Foundation and other sponsors funding a wide variety of "component-based" projects. But few, if any, of these projects, have approached the grand vision of creating repositories of easy to reuse components for developers and educators. This paper investigates some of the factors that stand in the way of achieving this goal. We begin by defining the word component and looking at several projects using components, with a focus on our Exploratories project at Brown University. We then discuss challenges in: Searching and Metadata, Quality Assurance, Programming in the University Environment, Platform and System Specificity, Social Issues, Intellectual Property Issues, and Critical Mass. We look at relevant software engineering issues and describe why we believe educational applications have unique factors that should be considered when using components.	Problems with using components in educational software	NA	2018
Tiffany Holmes	This paper presents new curriculum for an introductory course in art and technology in which students compare the software industry with fast food to investigate patterns of consumption in our culture.	What do computers eat?: teaching beginners to think critically about technology and art	NA	2018
Kevin L. Novins	This paper describes the design and implementation of a student-centered course in advanced undergraduate computer graphics. Instead of providing students with all necessary background before exposing them to recent research, students start with the latest SIGGRAPH proceedings and discover what topics they need to learn to understand them. The power of the group of students is exploited by having each student write a tutorial on one of the topics. Students then trade these written tutorials before being tested on the SIGGRAPH papers themselves. The course aims to nurture lifelong learning skills as well as an understanding of state of the art methods in computer graphics.	SIGGRAPH as textbook: learning skills for undergraduates	NA	2018
Hannes Kaufmann:Dieter Schmalstieg	Construct3D is a three-dimensional geometric construction tool specifically designed for mathematics and geometry education. It is based on the mobile collaborative augmented reality system "Studierstube." We describe our efforts in developing a system for the improvement of spatial abilities and maximization of transfer of learning. In order to support various teacher-student interaction scenarios we implemented flexible methods for context and user dependent rendering of parts of the construction. Together with hybrid hardware setups they allow the use of Construct3D in today's classrooms and provide a test bed for future evaluations. Means of application and integration in mathematics and geometry education at the high school, as well as the university, level are being discussed. Anecdotal evidence supports our claim that Construct3D is easy to learn, encourages experimentation with geometric constructions, and improves spatial skills.	Mathematics and geometry education with collaborative augmented reality	NA:NA	2018
Dennis J. Bouvier	A number of published papers recommend teaching scene graphs in the introductory computer graphics course [Bouvier 2002; Cunningham 1999; Hitchner and Sowizral 1999; Wolfe 1999]. However, little has been published concerning how to effectively use scene graphs in the introductory computer graphics course. This paper summarizes possible scene graphs exercises and teaching experience of the author.	Assignment: scene graphs in computer graphics courses	NA	2018
Sampson D. Asare:Petros M. Mashwama:Steve Cunningham	Computer graphics is becoming a tool for communication, change, and development throughout the world, and in order for this tool to be effective, there must be an educational base for its use. In southern Africa the development of computer graphics is underway, but is has not made the impact it could have. We believe that one of the main reasons for this is that there has not been as much educational development as is needed in the region. This note will describe some of the challenges that are faced by educational institutions in southern Africa as they try to develop the computer graphics education in the region, and will discuss some of the ways these institutions are working to meet these challenges.	Building computer graphics education in developing African countries	NA:NA:NA	2018
Julie Callahan:Charles C. H. Jui	Flash provides the ASPIRE team a means to create interactive virtual learning environments, where teachers and students can explore places and things previously limited by the classroom or simply by time constraints. Flash can become an immersive world for online science education. Producing curriculum support material in Flash provides many benefits for teachers and students. Teachers are provided with an affordable tool, which aligns to local and national curriculum standards. This material provides a rich classroom experience at little cost to the educator, while also satisfying the curriculum goals. Students can participate in an engaging learning experience, where science becomes an experience beyond a textbook lesson or a classroom lab. Situations are presented that would be either difficult to reproduce due to cost or scale. For example, students can investigate the causes of tides in the ASPIRE Flash activity "Gravity and Tides." Flash, the software available for authoring these activities, can produce a mathematically correct, physically based model that students can observe and investigate. These lessons provide a high-quality experience; meanwhile, the cost and speed of production of these activities continue to decrease.	Macromedia flash in physics education aspire's interactive online labs and lessons	NA:NA	2018
Fiona Bailey:Magnus Moar	Children invent imaginary worlds and enact scenarios within them on a daily basis as part of their imaginative play. Given the opportunity and the tools, what kind of worlds would children create for themselves within a virtual space, and what kind of learning can emerge within these playful, child-centered spaces? In VERTEX, young children inhabit an imaginary virtual world that they have designed and created using 3D modeling tools and net-based virtual worlds software. Crossing traditional subject disciplines and involving local and remote collaboration, the project demonstrates children's design and communication abilities above and beyond the expectations of the curriculum.	The Vertex project: exploring the creative use of shared 3D virtual worlds in the primary (K-12) classroom	NA:NA	2018
Dave Pape:Josephine Anstey	Immersive, interactive virtual reality is a tool with hypothetically limitless uses. However, so far it has been put to serious use primarily in technical application areas such as computational science, automotive engineering, and chemical exploration. Groups working in these fields often have large budgets and can afford expensive, advanced displays. VR should also be of value to schools and museums, but most of them have much smaller budgets than major research labs, or are not able to support high-end graphics workstations. A simple, affordable, projection based display system can make VR far more accessible. In schools, displays could be put into individual classrooms and not just a central computer lab. In the museum world, small institutions would be capable of showing cutting edge digital work that previously has been restricted to a few large museums. This workshop describes the construction of a single screen, passive stereo, VR display based on commodity, or otherwise low-cost, components. There are many options available for the major elements of such a system and the basic system can be modified or adapted to many different styles of use. Figure 1 shows a photo of such a system in use at the University at Buffalo.	Building an affordable projective, immersive display	NA:NA	2018
Gregory P. Garvey	This workshop, first given at SIGGRAPH 2001 [Garvey 2002] is now offered in two-parts. Part I introduces the process of transferring life drawings into 3D models using MAYA followed by a life drawing session. In Part II, workshop attendees working in the CAL import digitized life drawings into MAYA for setup as image planes to guide the modeling process. The goal of the workshop is to explore and develop skills of observational figure drawing and integrate them with the process of 3D modeling.	Life drawing and 3D figure modeling with MAYA	NA	2018
Mitch Williams	3D curriculum, once focused on modeling, rendering and animation, now includes web 3D and interactivity. But what options exist for new courses in web 3D? How in-depth are some web 3D tools and their learning curves? Should the curriculum expand to make 3D artists into programmers? What considerations should we give to the end-users' experience in viewing an online web 3D portfolio? This session examines how these issues are being addressed in teaching "Interactive Web 3D Media" at UC Berkeley, UC Irvine and UCLA Digital Arts and Entertainment Studies Extension. The session also provides hands-on experience with various tools and technologies.	Integrating web 3D into 3D animation curricula	NA	2018
Susana Maria Halpine	Integrating multimedia applications in the classroom can be overwhelming. Grants may address the cost of computer hardware, but where can instructors find the time to explore available software? Many visualization programs are free or low cost, but students will not grasp the importance of what they are viewing without proper conceptual introduction. Furthermore, many K-12 instructors are now expected to teach topics, including basic chemistry concepts, in which they may lack proper training. The STArt! teaching Science Through Art program was developed to help teachers prepare for these educational challenges. Using an "Artist in Residence" format, workshops are developed in collaboration with participating teachers. Specifically, STArt! focuses on basic concepts addressed in the new California K-12 Science Content Standards. The program introduces molecular visualization software using narrative discussions, educational animation, and hands-on workshops using art materials and everyday objects. By exploring different learning modes, it makes basic science concepts more understandable to a broader audience. Furthermore, by collaborating with instructors within their classrooms, the program provides a creative resource for teachers in meeting the academic standards.	Hi tech - lo tech: K-12 science visualization	NA	2018
Adam Watkins	As 3D student skills progress, sooner or later they begin work on accurately proportioned human models. While caricature design is often very forgiving in its realization, human designs need to be very close to "right on." This presents a tremendous challenge for 3D animation students, many of whom have had limited anatomy training. The traditional solution to source material has been to provide photographic references. The standard front and side shots provide a good starting point for students to work from but provide no information on appropriate polygon topology or details such as the curvature of the head between the eye and the temple. Yet, it is impractical to get a live model to provide 3D reference by sitting next to the student as he or she models. The difficulty of finding a good 3D reference for students to work from becomes the challenge. At the University of the Incarnate Word, we have begun using a traditional method of face casting. Traditionally, this technique is used to create plaster masks or molds upon which prosthetics can be constructed. The mold is reusable and can be constructed from plaster or even lighter cements.	Teaching human facial modeling through plaster face casting	NA	2018
Scott Senften	NA	Session details: Emerging technologies	NA	2018
John M. Fujii	In computer graphics, history frames the possible, imagination paints the impossible, and passion fills in the rest.	Tomorrow's yesterday: mapping the E-Tech continuum	NA	2018
Laroussi Bouguila:Makoto Sato:Shoichi Hasegawa:Hashimoto Naoki:Naoki Matsumoto:Atsushi Toyama:Jelel Ezzine:Dalel Maghrebi	The project presents a new locomotion interface for virtual environment with large display system. Users will be able to direct and control the traveling in the VE by in-place stepping and turning actions. Using a turntable technology, Visual feedback is continuously provided though the use of screen of limited size.	A new step-in-place locomotion interface for virtual environment with large display system	NA:NA:NA:NA:NA:NA:NA:NA	2018
Horst Hörtner:Christopher Lindinger:Robert Praxmarer:Andreas Riedler	The ARS BOX is a projection-based (cave-like), PC-based VR system. It significantly reduces the time and money needed to develop and present Immersive Virtual Environment (IVE) applications while simultaneously expanding the options available compared to similar systems. A handheld PC serves as its interaction interface, making possible numerous innovative applications.	ARS BOX with palmist: advanced VR-system based on commodity hardware	NA:NA:NA:NA	2018
Michael Haller:Daniel Dobler:Philipp Stampfl	Augmented reality (AR) is not only a new type of computer entertainment, but it can also be used for serious applications. Because the user is directly involved in the virtual world, enhanced reality can be more engaging than traditional computer work. Especially for a better sound impression, AR could be a solution for many problems. The sound component is still missing in current AR applications, which combine live video and computer graphics to produce real-time visual effects.	Augmenting the reality with 3D sound sources	NA:NA:NA	2018
Hiroo Iwata:Hiroaki Yano:Hiromi Igawa	Audio Haptics is a rendering technique for auditory and haptic sensations. Sound and force are generated by using a physical model of virtual objects. We developed a software for real-time calculation of a physical model of virtual objects. A speaker is set at the grip of the haptic interface for spatial localization of the sound.	Audio haptics	NA:NA:NA	2018
Henry Newton-Dunn:Hiroaki Nakano:James Gibson	Block Jam is a musical interface controlled by the arrangement of 24 tangible blocks. By positioning the blocks (Figure 1), musical phrases and sequences are created, allowing multiple users to play and collaborate.	Block jam	NA:NA:NA	2018
Dave Warner:Matt Carbone	Cyberarium knowledge fountain is a opportunity: • To demonstrate socially responsible applications of communication technology • To provide a social exchange at which multiple communities can explore unconventional applications of advanced technological concepts • To identify key areas where information technologies can be effectively applied to improve quality of life • To encourage young minds to explore science and think about how to make the world a better place	Cyberarium knowledge fountain: center for really neat research	NA:NA	2018
Zack Butler:Robert Fitch:Keith Kotay:Daniela Rus	A robot designed for a single purpose can perform a specific task very well, but it may perform poorly on a different task, or in a different environment. This is acceptable if the environment is structured; however, if the task is in an unknown environment, then a robot with the ability to change shape to suit the environment and the required functionality will be more likely to succeed. We wish to create more versatile robots by using self-reconfiguration: hundreds of small modules will autonomously organize and reorganize as geometric structures to best fit the terrain on which the robot has to move, the shape of the object the robot has to manipulate, or the sensing needs for the given task. For example, a robot could synthesize a snake shape to travel through a narrow tunnel, and then morph into a six-legged insect to navigate on rough terrain upon exit.	Distributed systems of self-reconfiguring robots	NA:NA:NA:NA	2018
Patrick Baudisch:Nathan Good	Focus plus context screens are wall-size, low-resolution displays with an embedded, high-resolution display region. Focus plus context screens allow users to view details of a document up close, while simultaneously seeing peripheral parts of the document in lower resolution. Application areas range from geographic information systems to interactive simulations and games.	Focus plus context screens: visual context and immersion on the desktop	NA:NA	2018
Andrew Joel:Michael Brown:Almos Elekes	The Immersive and Interactive Rear-Projected Stereo DLP Reality Center aims to improve the quality of immersive visualization and increase the efficacy of seamless real-time interaction with complex stereoscopic data.	Immersive and interactive rear-projected stereo DLP reality center	NA:NA:NA	2018
Cindy M. Grimm:William D. Smart	Lewis is a (short) human-sized mobile robot that wanders through crowded rooms, taking pictures of people, much like a photographer at a wedding reception does. The goal is take high-quality, well-composed photographs of people non-intrusively, and to offer these pictures as keep-sakes of the conference.	Lewis the robotic photographer	NA:NA	2018
Hiroo Iwata:Hiroaki Yano:Motohiro Tsuzuki:Fumitaka Nakaizumi:Takayuki Yoshioka:Yutaka Miyakita	The NONA-Vision is a high-resolution, wide-angle video capture and projection system. The display is composed of nine rear-projection screens. Images for the nine screens are captured by a specialized camera-head, in which the optical centers of nine video cameras are located at the identical position.	NONA-vision	NA:NA:NA:NA:NA:NA	2018
Kiyoshi Kiyokawa:Hiroyuki Ohno:Yoshinori Kurata	There are several approaches to realize a 3D display that can be viewed by multiple co-located users, and each has its own pros and cons. A volumetric display utilizes afterimage effects, and it usually can only show transparent images within its volume. Some projection-based systems support independent viewpoints for more than two users [1], however, virtual objects can be shown only within a viewing frustum in front of the screen. On the other hand, head mount display (HMD) based Augmented Reality (AR) can show virtual objects at arbitrary locations [2]. Besides, an HMD can potentially show correct occlusion phenomena between virtual and real scenes. While a video see-through HMD severely degrades the quality of the real scene and adds a certain system delay, an optical see-through HMD keeps the intrinsic quality of the real scene. However, a virtual scene had to be a semi-transparent ghost due to a half-silvered optical combiner so far.	Occlusive optical see-through displays in a collaborative setup	NA:NA:NA	2018
Michael Meehan:Mary Whitton:Sharif Razzaque:Paul Zimmons:Brent Insko:Greg Combe:Ben Lok:Thorsten Scheuermann:Samir Naik:Jason Jerald:Mark Harris:Angus Antley:Frederick P. Brooks	A common metric of VE quality is presence --- the degree to which the user feels like they are in the virtual scene as opposed to the real world. Presence is important for many VE applications [Hodges et al. 1994]. Since presence is a subjective condition, it is most commonly measured by self-reporting, either during the VE experience or immediately afterwards by questionnaires. There is vigorous debate in the literature as to how to best measure presence [Meehan 2001].	Physiological reaction and presence in stressful virtual environments	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Cynthia Breazeal:Andrew Brooks:Matt Hancher:Josh Strickon:Cory Kidd:John McBean:Dan Stiehl	We have created an articulated robotic creature and situated it within an interactive terrarium to explore the aesthetic, expressive, and interactive qualities that give robots an organic and engaging presence to people.	Public anemone: an organic robot creature	NA:NA:NA:NA:NA:NA:NA	2018
Hiroto Matsuoka:Akira Onozawa:Hidenori Sato:Hisao Nojima	The vision of this work is to regenerate real objects that had existed at some moment in the past and/or at some remote location as if they have been transferred to the present across space and time. The objects could be museum pieces or items in stores, for example. For this work, we have developed a quick and fully automated system that can capture a three-dimensional image of real objects. This success brought us close to our goal of "regeneration of real objects in the real world."	Regeneration of real objects in the real world	NA:NA:NA:NA	2018
Hideyuki Ando:Takeshi Miki:Masahiko Inami:Taro Maeda	"Smart Finger" is a novel type of tactile display for Augmented Reality (AR) that is wearable, like a press-on fingernail. This device allows the user to feel various textures while tracing his or her fingers along smooth objects. This wearable AR interface can supplement bump mapping information to real objects.	SmartFinger: nail-mounted tactile display	NA:NA:NA:NA	2018
Joseph A. Paradiso:Che King Leo:Nicholas Yu:Marc Downie	We have developed a very simple retrofit to a large display surface that enables knocks or taps to be located and characterized (e.g., determining type of hit --- metallic tap, knuckle tap, or bash --- and intensity) in real time. We do this by analyzing the waveforms captured by 4 piezoelectric transducers (one mounted in each corner of the surface) and a dynamic microphone (mounted anywhere on the glass) in a digital signal processor. Differential timing yields the position, frequency content infers the kind of hit, and peak amplitude reflects the intensity. This technique was first explored in collaboration between Paradiso and Ishii [Ishii et. al. 1999] to make an interactive ping-pong table. Moving to glass display surfaces introduced significant problems, however --- knuckle taps are low-frequency impulses that vary considerably hit-to-hit, and the bending waves propagating through the glass are highly dispersive. A heuristically-guided cross-correlation algorithm [Paradiso et al. 2002] was developed to counteract these effects and provide spatial measurements that can resolve knuckle impacts to within σ = 2-4 cm (depending on the material thickness) across a 2-meter sheet of glass. As the requisite hardware is minimal, and everything is mounted on the inside sheet of glass, this is a very simple retrofit to, for example, store window displays, ushering in an entirely new concept of interactive window browsing, where passers-by can interact with information on the store's products by simply knocking. We have explored this concept in retail, where one of our trackers was installed on the main display window of an American Greetings store near Rockefeller Center in Manhattan for this year's Christmas-Valentine's Day season (right figure), and in museums (e.g., left figure, which shows the system running at the Ars Electronica Center in Linz, Austria).	The interactive window	NA:NA:NA:NA	2018
Oliver Bimber:Bernd Fröhlich:Dieter Schmalstieg:L. Miguel Encarnação	The Virtual Showcase is a new projection-based and application-specific Augmented Reality display that offers an innovative way of accessing, presenting, and interacting with scientific and cultural content. Conceptually, the Virtual Showcase is compatible with the conventional showcases used, for instance, by museums. However, it allows the display of computer generated 3D graphics and animations together with real artifacts within the same space. From the technological point of view, the Virtual Showcase provides perspective correct stereoscopic viewing for multiple users, high resolution, low parallax (reflected projection plane inside the showcase), and support for mutual occlusion between real and virtual objects.	The virtual showcase: a projection-based multi-user augmented reality display	NA:NA:NA:NA	2018
Kenji Tanaka:Junya Hayashi:Yutaka Kunita:Masahiko Inami:Taro Maeda:Susumu Tachi	TWISTER, (Telexistence Wide-angle Immersive STERe-oscope) is an immersive full-color autostereoscopic display, designed for a face-to-face telecommunication system called "mutual telexistence", where people in distant locations can communicate as if they were in the same virtual three dimensional space.	TWISTER: a media booth	NA:NA:NA:NA:NA:NA	2018
Damion Shelton:George Stetten:Wilson Chang	From the discovery of X-rays over a century ago, clinicians have been presented with a wide assortment of imaging modalities yielding maps of localized structure and function within the patient. Some imaging modalities are tomographic, meaning that the data are localized into voxels, rather than projected along lines of sight as with conventional X-ray images. Tomographic modalities include magnetic resonance (MR), computerized tomography (CT), ultrasound, and others. Tomographic images, with their spatially distinct voxels, are essential to our present work.	Ultrasound visualization with the sonic flashlight	NA:NA:NA	2018
Daijiro Koga:Takahiro Itagaki	Chanbara is the name of our game, which reproduces the original ancient art of "samurai sword fighting". For this work, we created a new force feedback device called "GEK12". The player holds the sword and wears an HMD. In Chanbara, the player walks around a virtual arena to battle against CG characters.	Virtual Chanbara	NA:NA	2018
Christopher Shaw	NA	Session details: Panels	NA	2018
Kurt Akeley:David Kirk:Larry Seiler:Philipp Slusallek:Brad Grantham	Ray-tracing produces images of stunning quality but is difficult to make interactive. Rasterization is fast but making realistic images with it requires splicing many different algorithms together. Both GPU and CPU hardware grow faster each year. Increased GPU performance facilitates new techniques for interactive realism, including high polygon counts, multipass rendering, and texture-intensive techniques such as bumpmapping and shadows. On the other hand, increased CPU performance and dedicated ray-tracing hardware push the potential framerate of ray-tracing ever higher.	When will ray-tracing replace rasterization?	NA:NA:NA:NA:NA	2018
Norman I. Badler:Nadia Magenat-Thalmann:Laurie McCulloch:Evan Marc Hirsch:Phil LoPiccolo	Computer graphics technology has progressed to the point where it is possible to create digital humans that are virtually indistinguishable from the real items. The potential benefits are immense, but there are implications to consider as well, in a range of applications that includes film, video, the Web, and gaming. This panel of experts from diverse disciplines of computer graphics will discuss how far we have come in the use of digital humans, where they are heading, and what they will mean to us.	Digital humans: what roles will they play?	NA:NA:NA:NA:NA	2018
Natalie Jeremijenko:Thecla Schiphorst:Michael Mateas:Wolfgang Strauss:Will Wright:Andruid Kerne	Interface ecology is an emerging metadisciplinary approach, in which the creation of rich interactive experiences spans n disciplines --- such as computer graphics, mathematics, gaming, visual art, performance, and cultural theory. Interfaces extend beyond interactive artifacts, activities, and social spaces, forming intricate ecosystems. Interfaces are the catalytic border zones where systems of representation meet, mix, and recombine. Through this recombination, interface ecosystems generate fundamental innovations of form, experience, knowledge, and technology. This panel brings together a diverse range of practitioners who work from concept to experience not in terms of a particular discipline, métier, or medium but with a practice that interconnects multiple systems, forming a whole.	Extending interface practice: an ecosystem approach	NA:NA:NA:NA:NA:NA	2018
David S. Ebert:Bill Buxton:Patricia Davies:Elliot K. Fishman:Andrew Glassner	Computer graphics research and hardware has matured as a field to the point that high-quality computer graphics is becoming ubiquitous. Computer graphics shortly will be where word processing is today: everyone uses it, but there are very few people doing basic research in word processing. All of the challenges lie in the applications and use of this technology to enable advances in many fields. This panel will combine experts in computer graphics and associated technology with experts from a few applications areas to discuss the possibilities and future ways that computer graphics can advance discovery in many fields.	The future of computer graphics: an enabling technology?	NA:NA:NA:NA:NA	2018
Michael Gleicher:Michael Cohen:Nancy Pollard:Jessica Hodgins:Michiel van de Panne	"Three-minute madness." Papers from the new ACM SIGGRAPH Symposium on Computer Animation are summarized in three minutes or less, followed by a discussion of new directions in computer animation research.	Symposium on computer animation in fast forward	NA:NA:NA:NA:NA	2018
Vincent Scheib:Theo Engell-Nielsen:Saku Lehtinen:Eric Haines:Phil Taylor	For 20 years, an underground movement has produced short real-time animations running on home computers. This group, the "demoscene," primarily consists of students who pursue their technical and artistic interests beyond the classroom, to create inspiring works of real-time art. These productions encompass a broad range of computer graphics techniques such as procedural geometry, real-time ray-tracing, and real-time shading. Game developers have been utilizing this talent pool yet it has little visibility in the SIGGRAPH community. This panel explores the demoscene, technical tricks used in demos, and how scene educational and creative aspects can contribute to the SIGGRAPH community.	The demo scene	NA:NA:NA:NA:NA	2018
Scott Elson:Eamonn Butler:Scott Clark:Carlos Saldanha	This panel seeks to demystify, debunk and goad the dialogue of 3D character animation. This panel is motivated by the sea change that is currently effecting our industry. In the last year and a half what was once a battle to get 3D into Hollywood has completely crossed over --- Hollywood can't get enough. 11 years ago the quote from top Disney management was "there will never be a digital character in a Disney feature film." Now traditional animators and artists are giving up their fear for pragmatism and embracing 3D in droves. What does that mean for those already working in 3D? What does it portend? What can we learn from the traditional animator?	Animation's turning tide	NA:NA:NA:NA	2018
Mark Ollila:Staffan Björk:Kevin Bradshaw:Steven Feiner:Kari Pulli	With the number of mobile devices exceeding PCs, research is required in many areas with respect to graphics and interaction. There are problems with interaction, streaming, graphics algorithms, bandwidth with current and future devices. This panel examines the state of the art from both an industrial and research point of view, and provides directions for future work in this area.	Unsolved problems in mobile computer graphics and interaction	NA:NA:NA:NA:NA	2018
Loren Carpenter:Brian Fisher:Richard A. May:Norbert Streitz:David J. Kasik	The world of display devices is expanding rapidly, both literally and figuratively. New commercial and research devices come in larger sizes (measured in meters, not inches) and different physical forms (e.g. rectangular surfaces, cylindrical segments, truncated spheres). Such expansion means that graphics and interactive techniques are becoming far more amenable to group activities and can display more and more data at once.	Graphics in the large: is bigger better?	NA:NA:NA:NA:NA	2018
Margaret S. Geroch:Evan Hirsch:Joan Staveley:Tom Tolles:Barb Helfer:Suba Varadarajan	The application of motion capture in the movie industry has continued to increase in the last couple of years, ranging from background action to major characters. Using motion capture in a production pipeline requires both motion capture and animation experience. An animator needs an understanding of biomechanics and how the body moves both for planning marker placement and to be able to apply motion capture to a character in such a way that it fits the character and the story development. For motion capture to be usable, it requires planning well in advance, knowing what needs to be captured and how it's going to be applied.	How does motion capture affect animation?	NA:NA:NA:NA:NA:NA	2018
Noah Wardrip-Fruin:Andrew Stern:Peter Molyneux:Michael Mateas:Bernard Yee	While pure theory can be interesting, this panel starts where the rubber meets the road for interactive stories --- real systems, and the practice-oriented insights of creators. Expect to hear concrete examples and solutions to hard problems, ranging from the technical to the artistic. Of course, there's also a lot of controversy about what interactive stories are, and how to best make them, which this panel doesn't plan to ignore. It includes people working on major systems that represent three different approaches, and puts them together with a moderator that knows the hot buttons (as well as the points of agreement). The three approaches can be called: narrative game, interactive drama, and massively multiplayer storytelling.	Interactive stories: real systems, three solutions	NA:NA:NA:NA:NA	2018
Bob Nicoll:Glenn Entis:Patrick Gilmore:J. C. Herz:Alex Pham:Will Wright:Ken Perlin	Driven by trends in silicon and software, computer gaming is the medium that will define the recreational and cultural experience of the twenty-first century in the way that motion pictures and their offspring television, defined the recreation of the twentieth. Or so we assert! This panel will debate the truth of the statement that gaming is the dominant medium of the future with believers, skeptics, and outside referees.	Games: the dominant medium of the future	NA:NA:NA:NA:NA:NA:NA	2018
Marc Barr	NA	Session details: sigKIDS	NA	2018
Daria Tsoupikova	Being interested in Interactive Animation, Illustration, Multimedia, and the creative use of programming languages, I explored these areas to produce an educational game for preschool children. The following four chapters present the concept and development of an experimental interactive game titled "Winter Dreams".	Interactive animation as an educational tool in "Winter Dreams"	NA	2018
Karen Monahan	Can games be educational as well as fun? Educators and children's media developers have long been trying to answer this question. Much of the software produced for kids and marketed as educational is comprised of the type of games that Seymour Papert has so adeptly defined as "edutainment".1 Many of these games are drill-type memorization exercises where the game activity has little or no relation to the "educational" component. Mainstream educational software might be fun, even engaging, but very little of it challenges the user intellectually or encourages open-ended discovery.	Gaming as an educational tool: internet scavenger hunt	NA	2018
Hilary J. Wright	FORM is an educational software prototype designed to encourage children's creative play with fundamental geometric shapes. The open-ended program serves as a creative tool to help form a deep and lasting understanding of the beauty, elegance and underlying unity of math, science, design and nature.	FORM	NA	2018
Satoko Moroi:Shinji Sasada:Ryoji Shibata	"The Floating Words" is a new device for playing with words. This installation provides the viewer with a new feeling: If you speak to the microphone, your voice drips into a water pool as water drops, and will begin to float. You can stir or ladle the "letters" with a stick or ladle. The installation of version_1 was exhibited at SIGGRAPH2001 Art Gallery, and it was succeeded very much. This time, we add new expression to new version: "The Floating Words for KIDS", and the expression is device to learn alphabet playing with the "The Floating Words". The expression is: If you ladle some letters with a ladle, they will turn to the characters showing word that has the alphabet in an initial. This project is a proposal of using this installation for learning alphabet for kids. These feeling: it seem like a magic are results from tricks of combination of voice recognition system, 3D magnetic sensor, simple computer graphics and real water and so on.	The floating words for kids: interactive installation for learning alphabet	NA:NA:NA	2018
James Dai:Michael Wu:Jonathan Cohen:Troy Wu:Maria Klawe	Whereas traditional research on collaborative educational systems has primarily focused on how to define better modes of digital interaction, this approach is found lacking when applied to developing collaborative systems for elementary school aged children. It is creatively and collaboratively restrictive to filter the enthusiastic interactions of these excited 12-year-old children through progressively more complicated GUIs. Current E-GEMS research examines design factors of educational systems that recognize and facilitate the social context of the classroom and hopes to encourage, rather than restrict, peer-to-peer social discussion and interaction. By correlating observed interactions in the digital domain with those in the social domain, we hope to shed light on design factors of collaborative systems that can be an integral and exciting part of a child's mathematical education. The vessel of our current research is the two-player collaborative mathematical exercise PrimeClimb, developed at E-GEMS. This paper describes the study conducted with PrimeClimb and documents the methodology of data capture and analysis used in the study -- methods borrowed from ethnography, education research and sociology.	Toys to teach: mathematics as a collaborative climbing exercise	NA:NA:NA:NA:NA	2018
Yuichiro Kamata	JOLLEE-MAIL PLAYGROUND is a simulated environment where children can experience the Internet communication in a more involving fashion.	Jollee-Mail Playground	NA	2018
Robert Dunn	The Virtual Dig is an interactive adventure for young people and provides an educational experience in archaeology and historical architecture. The web site was produced for the Israel Museum, Jerusalem, in collaboration with museum archaeologists and can be seen online at the addresses: "http://www.imj.org.il/arc-tel" and "http://www.arcv.org".	The virtual dig	NA	2018
DaShawn L. Hall:Kenneth Sakatani	Virtual Reality or VR is a computer-designed environment that allows the user to interact within the digital environment as he or she would in a real world situation. VR has been successfully used in game development, medical technology and flight simulation. Virtual Studio is a partial immersive, interactive virtual learning environment that is based on the metaphor of the artist studio. Its primarily use will be to teach elementary students about the techniques, styles and history of artist and art. Each part of Virtual Studio is named after major artists, who used or pioneered a particular style or technique. There are three core sections of Virtual Studio: 1) Architectural Design, 2) Sculptural Design, 3) Digital Painting and Color Theory.	Virtual studio: virtual reality in art	NA:NA	2018
Lori Scarlatos	Over the years, educators and government officials have searched for ways to improve learning in our schools, particularly in the areas of math and science. Many have come to recognize that collaborative activities, learning through play, and teacher guidance can help children to get over their initial fears and even begin to enjoy these subjects. Yet, at the same time, shrinking school budgets are making it harder to support these approaches to learning. Tangible Interfaces for Collaborative Learning Environments (TICLE) was conceived in response to this need [Scarlatos 2002].	An application of tangible interfaces in collaborative learning environments	NA	2018
Jonathan T. Blocksom	GollyGee Blocks™ is a 3D modeling program for children. Children use it to stack, transform, color and texture 3D objects in a 3D scene which can be viewed from any angle. GollyGee Blocks™ was designed as an educational, open-ended creativity tool for 3D graphics.	GollyGee Blocks™: a 3D modeler for children	NA	2018
Jacqueline Nuwame	Can oral storytelling live in a digital medium? Anansi's World of Folklore is a celebration of the art of storytelling, created as a broadband site for showcasing and collecting folktales. It was created to discover a way to give traditional oral storytelling a meaningful place on the Internet.	Anansi's world of folklore	NA	2018
Christopher Stapleton	The Immersive Jukebox offers users a choice of musical experiences exploring various influences of African American Blues music: traditional African music, spirituals, work songs, and others. Inspired by the curriculum of the International House of Blues Foundation (IHOBF)* Blues SchoolHouse program, it introduces students and teachers to the music, art and history of the blues and its cultural origins.	The ToyScouts' immersive jukebox: University of Central Florida	NA	2018
Douglas R. Roble	NA	Session details: Sketches and applications	NA	2018
Doug Cooper	DreamWorks Feature animated film "Spirit" takes new steps in the hybridization of 2D and 3D production techniques for Character Animation, even to the extent of using both approaches within the same shot.	2D/3D hybrid character animation on "Spirit"	NA	2018
Kotro Petri:Mannerkoski Olli:Lesonen Hannu:Lustila Risto	Digital media convergence is creating a vast new parallel universe, the digital dimension, where the range of available services is growing exponentially. Different forms of media and communication are integrating into consumer devices. This means new challenges for information user interfaces, especially from the point of view of user experience. The sketch introduced here is a functional prototype, a proof-of-concept, of a new kind of graphical user interface for interactive television. It integrates different services for television, Internet, radio, personal communication and games under one user interface.	3D browser for interactive television	NA:NA:NA:NA	2018
Huirong Han:Juli Yamashita:Issei Fujishiro	Is a 2-dimensional (2D) force feedback device capable of presenting 3-dimensional (3D) shapes? The answer is a qualified "yes." "Force shading", a haptic counterpart of bump mapping in computer graphics, presents a non-flat shape on a nominally flat surface by varying the force vector direction in haptic rendering [Morgenbesser and Srinivasan 1996][Robles-De-La-Torre and Hayward 2001]. To our knowledge, such phenomena have been qualitatively measured only by 3D devices, and a quantitative comparison to 2D devices has not been made. We compare thresholds of human shape perception of the plane experimentally, using 2D and 3D force feedback devices.	3D haptic shape perception using a 2D device	NA:NA:NA	2018
Kevin Thomason:Robert V. Cavaleri	This sketch presents how the 3D Layout Department at Blue Sky Studios dealt with the production pipeline issues of transforming their first feature film Ice Age from sequences of hand-drawn storyboards into blocked out, ready-to-animate scenes in 3D for the Animation Department. To achieve this, custom tools were developed to automate sequence set-up and distribution of shots, in order to minimize the artists' concern with data management and maximize the creative time available to accommodate revisions in the story.	3D layout and propagation of environmental phenomena for Ice Age	NA:NA	2018
Arunachalam Somasundaram:Rick Parent	A consumer-grade camera is used to view the motion of individuals moving in an area, such as that observed in the lobby of a building. By assuming individuals are engaged in fairly normal walking behavior, we can extract sufficient information to synthetically reconstruct the scene using simple image processing techniques. The reconstructed scene can then be viewed from any angle and used to track interesting individuals. Features of the person such as shirt and pants color can be extracted from the video and applied to the synthetic model thus allowing the real individual to be recognized from viewing the reconstruction. The technique proposed has the advantage of using a single camera and would find applications in gait analysis and security. This is a work in progress and we present some initial results of a single moving figure.	3D reconstruction of walking behaviour using a single camera	NA:NA	2018
Patrick Min:Joyce Chen:Thomas Funkhouser	This sketch describes our experiences with creating query interfaces for an online search engine for 3D models. With the advent of affordable powerful 3D graphics hardware, and the improvement of model acquisition methods, an increasing number of 3D models is available on the web, creating a need for a 3D model search engine [Paquet and Rioux 2000; Suzuki 2001]. An important problem that arises for such an application is how to create an effective query interface. To investigate this issue, we created several different query interfaces, and tested them both in controlled experiments as well as in a publicly available 3D model search engine.	A 2D sketch interface for a 3D model search engine	NA:NA:NA	2018
Masaki Oshita:Akifumi Makinouchi	In this sketch, we present a middleware for computer games that has the ability to realize dynamic motion control of characters. The specific novelty of the middleware is to produce dynamically changing motions in response to physical interaction between the character and environments such as collision impulses and external forces as shown in Figure 1.	A dynamic motion control middleware for computer games	NA:NA	2018
Alexandre François:Elaine Kang:Umberto Malesci	This sketch presents the design and construction of a handheld virtual mirror device. The perception of the world reflected through a mirror depends on the viewer's position with respect to the mirror and the 3-D geometry of the world. In order to simulate a real mirror on a computer screen, images of the observed world, consistent with the viewer's position, must be synthesized and displayed in real-time. Our system is build around a flat LCD screen manipulated by the user, a single camera fixed on the screen, and a tracking device. The continuous input video stream and tracker data is used to synthesize, in real-time, a continuous video stream displayed on the LCD screen. The synthesized video stream is a close approximation of what the user would see on the screen surface if it were a real mirror.	A handheld virtual mirror	NA:NA:NA	2018
Barrera Salvador:Hiroki Takahashi:Masayuki Nakajima	Presently, most virtual reality systems use upper body parts to interact with objects in the virtual environment. This situation is caused by technological limitations of current interface devices. Starting from this viewpoint we developed a new interface for detecting ankle motions relative to the knee. We believe that hands-free navigation, unlike the majority of navigation techniques based on hand motions, has the greatest potential for maximizing the interactivity of virtual environments since navigation modes are more direct motion of the feet.	A new interface for the virtual world foot motion sensing input device	NA:NA:NA	2018
Juan Buhler:Dan Wexler	"Bokeh" is a Japanese word used to describe the quality of the out-of-focus areas as rendered on film by a physical lens. In areas that are out of focus, the circle of confusion (i.e., the distribution of light absorbed onto film from a single point of light in the scene) is bigger than in areas that are focused. Different lenses distribute light within the circle of confusion differently, depending on a number of factors, like the shape and number of diaphragm blades and the optical design of the lens itself. For example, lenses that are designed to correct for optical aberrations tend to render a circle of confusion that is more intense on the edges and slightly less so in the center. Other lenses, like the so called "mirror" lenses, which include reflective elements besides refractive ones, typically produce donut-shaped circles of confusion since some of the light paths are cut from the center by the reflective element. Moreover, in a real lens the circle of confusion sometimes becomes elliptical on the areas outside of the center of the image, with the shorter axis of the ellipsis oriented radially from it. Standard rendering techniques model depth of field using point sampling techniques which converge towards a bokeh model with uniform density across the circle of confusion. This represents an idealized lens that does not exist in reality. Real lenses have non-uniform bokeh distributions. For example, a smooth, or "creamy" lens will have a Gaussian distribution across the circle of confusion. Real lenses may have different distributions at different points on the film plane. Our technique allows the animator to specify an arbitrary probability density function to represent the distribution of intensity within the circle of confusion. The probability curve is used by the renderer to jitter the location of the sample point on the lens. For example, if the density function is a Gaussian, more samples will be taken towards the center of the lens than at the edges. Conversely, for a density function that simulates a mirror lens, fewer samples are taken near the center of the lens because the mirror blocks light as it moves through the lens. By generating enough sample points, the model converges on the true bokeh density function. We will also show the effect of specifying two shapes for the light distribution, one for close and the other for far focused areas, and the deformation of the circle of confusion so it becomes elliptical on the edges of the image. Future work includes the specification of the diaphragm shape, and the possible implementation of this technique as a particle renderer, which would provide efficient sampling at a much lower cost. Additional work to add support for stratified sampling is also possible.	A phenomenological model for bokeh rendering	NA:NA	2018
Maria A. Alberti:Dario Maggiorini:Paola Trapani	In the design of multimedia communication artifacts few, if any, tools support the early stage of a creative process: the heuristic project. In this work we give a proof of concept of an application addressed to a specific kind of heuristic project: given the logical sequence of episodes of a narrative, the fabula, the goal is to obtain different plots expressed in multi-modal language. The case study is provided by the task of transposing a written synopsis to the multi-modal language of a movie. We adopted the semiotic theory of Greimas to analyze the narrative and reveal its deep structure. The application enables users to interact with this structure in order to simulate and anticipate the effects of meaning resulting from their manipulation.	A semiotic approach to narrative manipulation	NA:NA:NA	2018
Alan Price:Dan Bailey	This sketch presents how the real-time 3D interactive simulation, "The Virtual Tour of the Cone Sisters' Apartments", from the SIGGRAPH 02 Art Gallery exhibition was designed, authored and produced by the Imaging Research Center (IRC) at UMBC for two installations at the Baltimore Museum of Art (BMA).	A virtual reconstruction of the Cone sisters' apartments	NA:NA	2018
Pieter Peers:Philip Dutré	Image-based relighting represents a class of techniques that apply new lighting conditions to a scene, given a set of basis images. In this sketch we present a relighting technique that, for a single viewpoint, accurately captures the reflectance field of objects, without restrictions on their geometrical complexity or material properties. Once the reflectance field is captured, the objects can be relit under arbitrary lighting conditions. To achieve such accurate results, our method combines the strengths of both the Light Stage [Debevec et al. 2000] and environment matting [Zongker et al. 1999; Chuang et al. 2000] into a single framework.	Accurate image based re-lighting through optimization	NA:NA	2018
Askold Strat:Manuel M. Oliveira	We describe the implementation of a portable 3D-camera prototype based on a consumer grade digital camera and an inexpensive laser raster generator. Such hand-held device can be used to capture smooth shapes by acquiring one or more images. Its output can be either a 3D wireframe or a textured model.	An inexpensive 3D camera	NA:NA	2018
Taly Sharon	We describe here two installations of direct manipulation systems in the art and education domain. In these installations we use a pressure sensitive computer-projected canvas for user manipulation. As the user presses the canvas, an art piece is created, or image layers are revealed.	Art and education using direct manipulation of a sensor array	NA	2018
Daniel Dobler:Michael Haller:Philipp Stampfl	This sketch describes the Mixed Reality application ASR (Augmented Sound Reality) which uses the overlay of virtual images on the real world to support the placement of three dimensional sound sources. Our system allows to place sound sources in a virtual or real room with the advantage of feeling, seeing and hearing them. This implies a more intuitive and better feeling of space and 3D sound.	ASR: augmented sound reality	NA:NA:NA	2018
Xiaoyang Mao:Yoshinori Nagasaka:Atsumi Imamiya	Line Integral Convolution (LIC)[Cabral and Leedom 1993]] is a texture based vector field visualization technique. Why using LIC for pencil drawing generation? Let us look at the two images shown in Figure 1. Figure 1(a) is a digitized sample of a real pencil drawing. Look over it, we can perceive the traces of parallel pencil strokes and a gray scale tone built with the strokes. If we look at any local area of the image, however, we can find that the direction of strokes and the intensity of pixels vary randomly. The variance of intensity results from the interaction of lead material and drawing paper. The LIC image shown in Figure 1(b), however, presents the very similar features. Since an LIC image is obtained by low-pass filtering a white noise along the streamlines of a vector field, we can see traces along streamlines. On the other hand, the intensities of pixels within any local area vary randomly as the input image is a white noise. Such similarity suggests us that we can imitate the tone of pencil drawings with an LIC image.	Automatic generation of pencil drawing using LIC	NA:NA:NA	2018
Liselott Brunnberg:Mark Ollila	This sketch presents a prototype developed as part of the Backseat gaming project. The aim of the project is to explore how to make use of mobile properties for developing compelling and fun game experiences. The prototype is developed for use in a highly mobile situation, that of a car passenger and is realized by the use of mobile devices and the users physical location during speed to merge the virtual content and surrounding road context into an augmented reality game. In this research, in addition to location, we also introduce variables such as speed, direction, timing, changing surrounding, fast movement of manipulative objects and multiple entry and exits.	Backseat gaming: augmented reality with speed	NA:NA	2018
Diane Gromala	BioMorphic Typography is a new conception of writing and a morphing typeface driven by biofeedback. It enables users to become aware of their autonomic physiological functions while they type, in real-time. In doing so, BioMorphic Typography seeks to challenge longstanding Western notions about the relationship among the senses, representation, and technology.	BioMorphic typography	NA	2018
Xiaohuan Corina Wang:Cary Phillips	This sketch describes a new method for deforming the skin of a digital character around its skeleton by computing statistical fit to an input training exercise. In this input, the skeleton and the skin move together, by arbitrary external means, through a range of motion representative of what the character is expected to achieve in practice. Using least-squares fitting techniques, we compute the coefficients, or "weights," of our new deformation equation. The result is that the equation, which is compact and efficient to evaluate, generalizes the motion represented in the input. Once the training process is complete, even characters with high levels of geometric detail can move at interactive frames rates.	Body building through weight training: using fitting techniques for skin animation	NA:NA	2018
Tobias Skog:Sara Ljungblad:Lars Erik Holmquist	The field of display technology is rapidly developing, and LCD-and plasma-displays are already invading our surroundings. Alternative technologies such as "electronic ink", electro-luminescent materials, and even color-changing textiles [Holmquist and Melin 2001] will further increase the number of possibilities to integrate computer graphics in our everyday lives. We believe that computer graphics for everyday life will have requirements that are very different from those of a Web page or a movie special effect. To explore this, we have developed a type of applications that anticipates a future use of computer graphics, so-called Informative Art.	Bringing computer graphics to everyday environments with informative art	NA:NA:NA	2018
Michael J. Lyons:Gert J. Van Tonder:Ian Shortreed:Nobuji Tetsutani	Zen gardens exhibit sophisticated visual designs achieved with minimal compositions and engender a calm, contemplative atmosphere. Here we use perceptual models to study Zen garden design with the aim of discovering guidelines for the creation of calm visual spaces.	Calming visual spaces: learning from Kyoto Zen gardens	NA:NA:NA:NA	2018
Jay Riddle	This technical sketch surveys the wide variety of projection techniques that are employed in today's video and computer games. We will discuss the evolution of the "game view" and the reasons behind the choice of perspective, some of which are practical, while others are legacies. We will also explore the limits inherent in choosing a particular technique.	Cameras and point-of-view in the gamespace	NA	2018
Doug Cooper	The opening shot in "Spirit" runs a full 3 minutes without scene cuts, and introduces the audience to the breathtaking scenery of the mythic Old West. This elaborate scene was accomplished with every possible technique by mixing traditional drawing and painting art forms with 3D environments, effects, and digital characters. This Sketch will attempt to summarize some of the key challenges in producing this scene, which took over two years to complete.	Challenges of the homeland pan in "Spirit"	NA	2018
Youngha Chang:Suguru Saito:Masayuki Nakajima	Each painter renders a painting in her own style. This style can be distinguished by looking at elements such as motif, color, shape deformation and texture. Previously, [Hertzmann et al. 2001] suggested a method for applying the texture of an image to a photograph. In this paper, we will focus on the element of "color".	Color transformation based on the basic color categories of a painting	NA:NA:NA	2018
Daisuke Goto:Junichi Hoshino	In this sketch, we propose a new technique for computer generated clay animation. Unlike the traditional approaches based on physical simulations, we focus on generating various animation effects produced by the clay animator.	Computer generated clay animation	NA:NA	2018
Koh Kakusho:Yutaka Minekura:Michihiko Minoh:Shinobu Mizuta:Tomoko Nakatsu:Kohei Shiota	This sketch describes three dimensional (3D) computer graphics (CG) produced to illustrate the development of a human embryo for education in embryology, which is one of the basic subjects in professional medical education (Fig. 1). Although similar CG have already been produced for TV programs, they are insufficient in precision for professional medical education.	Computer graphics to illustrate the development of a human embryo for professional medical education	NA:NA:NA:NA:NA:NA	2018
Kyle Odermatt:Chris Springfield	Since the first color short, Walt Disney animated films have been known for their beautiful and breathtaking environments that seamlessly integrate with the hand drawn characters that populate them. Over the years, technological innovations have streamlined the process of creating these environments while still maintaining the extremely high artistic standards that have made our films famous. Other advances, like the multi-plane camera, have helped to revolutionize how the camera moves through these environments. In general, such technological innovations are inspired by an artistic need which must be achieved with out being prohibitively expensive. On Walt Disney's "Treasure Planet", new innovations to the DeepCanvas renderer were inspired by such artistic needs and desires.	Creating 3D painterly environments for Disney's "Treasure Planet"	NA:NA	2018
Brian Goldberg	Our team was tasked with the project of creating a full-screen, completely believable, CG stunt double for a well-known motion picture star. Successful project completion required adapting and applying many years worth of computer graphics research in a new and novel way. Most importantly, development had to be done within usual production constraints---ultimately guaranteed results, known pipeline, quick turnaround on tests, and a final product that would rival any work in this genre ever attempted.	Creation of a photo-real CG human	NA	2018
Fumio Matsumoto:Akira Wakita	CT is a project to reconstruct an existing urban space as a 3D information city on the web. A visitor can browse the city with "building wall browsers" and communicate with other visitors.	CT (city tomography)	NA:NA	2018
Johannes Hirche:Alexander Ehlert	Displacement Mapping [Cook 1984] is commonly used in commercial rendering software for adding surface detail. In contrast to Bump Mapping not only the normal is perturbed, the surface is modified as well. A given point P on a surface is displaced to a new point P' in the direction of the surface normal of the point P, N scaled with the value stored in the Displacement Map:	Curvature driven sampling of displacement maps	NA:NA	2018
Bonny Lhotka	In working with custom designs for digital imaging on textiles, source images and garment patterns are scanned and the images mapped to fit the pattern design in Adobe Photoshop. For these pieces the images were scanned on the Eclipse 48" x 96" flatbed scanner.	Custom designs for digital imaging on textiles	NA	2018
Gerhard Kurka	Densely occluded regions containing many stacked objects along the line of view generally show a high local depth-complexity (DC). With respect to occlusion culling such regions require dense occluder-sets, in order to provide gap free occlusion. Using pure size or distance based selection heuristics fail to consider such regions. This sketch presents a novel dynamic occluder selection approach, which selects occluders that cover regions of high DC. Depth-Complexity Based Occluder Selection [Kurka 2001] is based on the evaluation of a low resolution (128x128 pixel) raw scene depiction (RSD), which is rendered once per frame by a simple but fast image-based rendering (IBR) technique.	Depth-complexity based occluder selection	NA	2018
Guangzheng Fei:Nadia Magnenat-Thalmann:Kangying Cai:Enhua Wu	Instead of generating an initial in-core model in first pass and performing a second pass adaptive processing over the original model according to the detail analysis of the in-core model, in this paper we propose a single-pass algorithm to realize detail calibration by introducing a scheme of interlaced sampling in order to obtain higher efficiency.	Detail calibration for out-of-core model simplification through interlaced sampling	NA:NA:NA:NA	2018
Patrick Dalton:Rob Rosenblum:Shyh-Chyuan Huang:Lawrence Lee:Hank Driskill	Our goal was to create realistic digital pyrotechnic effects for the film "Reign of Fire". The specific effects we needed were fire, smoke, fog, flying embers and dust kicked up from dragons flying overhead. We also needed the effects to interact with the environment as well as the dragons. To make the challenge more difficult, we realized that it wasn't good enough to create real world physical interactions; the film needed imagery that was larger than life and exagerated. We needed to be able to art direct the simulations.	Digital pyro for Reign of Fire	NA:NA:NA:NA:NA	2018
Masayuki Takemura:Yuichi Ohta	Eye-contact plays a very important role in the face-to-face human communication. In the shared augmented reality space, however, the eye-contact between two people is blocked by the head-mounted displays. An HMD is a necessary device for merging the virtual and real worlds to realize the augmented/mixed reality. [Ohta and Tamura 1999] As a side effect, it covers the user eyes. In a shared AR/MR space, multiple users want to share the real world as well as the virtual objects. However, the eye-contact information in the real world is dropped because of the HMDs.	Diminishing head-mounted display for shared augmented reality	NA:NA	2018
Koji Mikami:Toru Tokuhara:Mitsuru Kaneko	In this sketch, we will demonstrate a video storyboard tool targeted for 3D computer animation. Our tool provides limited but specialized functions of standard 3D computer graphics software, focusing on ease of scene construction, camera control and the ability to preview in realtime. This allows for quicker and easier creation of video storyboards over existing approaches.	Diorama engine: a 3D video storyboard editor for 3D computer animation	NA:NA:NA	2018
Mashhuda Glencross:James Marsh:Jon Cook:Sylvain Daubrenet:Steve Pettifer:Roger Hubbold	DIVIPRO is a prototype tool for simulating the assembly and disassembly of mechanical engineering components. It is aimed particularly at situations where the responsibility for decision making is shared between geographically dispersed design teams, and provides a collaborative environment in which different team members concurrently visualize and manipulate models. It is currently being evaluated using models from the aerospace and medical industries.	DIVIPRO: Distributed Interactive VIrtual PROtotoyping	NA:NA:NA:NA:NA:NA	2018
Leonie Schäfer:Elaine M. Raybourn:Amanda Oldroyd	This sketch describes DocuDrama, a tool that offers a generation of interactive narratives that are based on activities in a collaborative virtual environment. DocuDrama [Schäfer et al. 2001] is built as part of TOWER [2002], a Theatre of Work Enabling Relationships, which allows project members to be aware of project relevant activities as well as to establish and maintain the social relationships that intensify team coherence.	DocuDrama conversations	NA:NA:NA	2018
Ernest J Petti:Thomas V Thompson, II:Adolph Lusinsky:Hank Driskill	In recent years there have been several movies starring creatures with scaled surfaces. Among these are Jurassic Park, Dragonheart, and Lake Placid. The surfaces of these creatures have generally been constructed by layering painted textures atop displacement maps. This gives the model texture, but the scales stretch and shrink under the movement of the creature, giving a rubbery look that is not realistic.	Dragon scales: the evolution of scale tool for Reign of Fire	NA:NA:NA:NA	2018
Ivan Poupyrev:Shigeaki Maruyama	There is a sense of satisfaction using a pen or pencil to write or draw. Feeling the imperfections on the paper as the pen tip moves over it and observing marks emerge create the inherently physical and intimate feeling of drawing. Our extensive discussions with artists and designers suggest that this physicality and intimacy with drawing not only brings enjoyment, but also perhaps assist in the artist's creative process.	Drawing with feeling: designing tactile display for pen	NA:NA	2018
Carlos Gonzalez-Ochoa:David Eberle:Rob Dressel	The challenge of animating believable dragons was presented to Disney Feature Animation and The Secret Lab (TSL) for the film "Reign of Fire". The film called for 100ft creatures with wing spans of 300ft that could undergo enormous speeds and accelerations. The artistic direction required each dragon to have wings that transition between a variety of physical behaviors and interact with the environment. To solve this challenge the Muscle and Skin system used in Disney's Dinosaur [Eskuri 2000] was extended with a variety of controls to do physical simulation. In this sketch we discuss some of the issues encompassing the creation of this simulator and give an overview of the successful and unsuccessful paths taken during its development.	Dynamic simulation of wing motion on "Reign of Fire"	NA:NA:NA	2018
Jimmy Chim:Hyunsuk Kim	This sketch presents a solution for creating expressive computer facial animation using off-the-shelf software. We will describe how to apply dynamic skin deformation on a CG character using Maya Cloth and how to create artist-friendly animation controls. Historically, facial animation with sophisticated skin deformation based on physical characteristics of skin could only be done using proprietary software in big studios, which are not accessible to small studios, independent animators and students. The solution that we have developed is easy to understand and does not involve any programming knowledge.	Dynamic skin deformation and animation controls using maya cloth for facial animation	NA:NA	2018
Adam Burr:Ross Scroble	This sketch explains how the Animation Team at Blue Sky Studios executed a challenging sequence for "Ice Age" and describes in detail a particularly useful animation software tool called followThrough.	Dynamics and dodos: rigging and animation methods for Ice Age	NA:NA	2018
Gianfranco Doretto:Stefano Soatto	This technical sketch presents a simple and efficient algorithm for editing realistic sequences of images of dynamic scenes that exhibit some form of temporal regularity. Such scenes include flowing water, steam, smoke, flames, foliage of trees in wind, crowds, dense traffic flow etc. We call this kind of scenes dynamic textures.	Editable dynamic textures	NA:NA	2018
Bill Hill	Embodied Interaction, explores the components of interactive media, with a primary focus on the role that interaction design has on virtual experiences. My electronic artwork is concerned with the transformation of the human species, specifically its biological components and its behavioral characteristics. This transformation or evolution is an environmental reaction to the manifestations of science and technology. This presentation examines the need to address the physical body and how the action of users needs to be interconnected with the interface and content of a interactive piece. From the development of opaque sculptural input devices to the use of transparent technologies my interactive installations seeks to examine the process of conditioning users; their predetermined interaction; and the physicality of computing.	Embodied interaction	NA	2018
Alan Kapler	High-resolution voxel rendering is no longer the pipe dream it once was. In the age of multi-gigabyte RAM, voxels are finding an ever-increasing role in creating cutting-edge visual effects. Digital Domain has developed a unique, animator-friendly way of working with voxels that is revolutionizing the way volumetric effects are done.	Evolution of a VFX voxel tool	NA	2018
Hiroshi Mori:Jun'ichi Hoshino	Generating realistic human motions from sparse pose descriptions are important for computer animation and virtual human applications. In this sketch, we propose an example-based motion interpolation technique using independent component analysis (ICA). The proposed method is also useful for human motion conversion, motion blending, and converting symbolic descriptions to continuous motion.	Example-based interpolation of human motion	NA:NA	2018
Ian Mackinnon	This sketch concerns the development of a renderer which takes advantage of the two dimensional nature of the frames it produces. The project is chiefly inspired by the way in which artists have used 2D abstraction in the past, from scribbled cartoons to cubist paintings. It would seem that exploiting the plane of the canvas can help us to make more interesting compositions.	Exploiting screen space	NA	2018
James W. Davis:Vignesh S. Kannappan	Given a single motion-capture sequence of a person performing a dynamic activity at a particular intensity (or effort), our goal is to automatically warp that movement into a natural-looking exaggerated version of that action. Consider warping a movement of a person lifting a lightweight box to make the movement appear as if the box were actually very heavy. We describe an efficient data-driven approach applicable to animation re-use that learns the underlying regularity in an action to select the most "expressive" features to exaggerate. Other "style-based" approaches are presented in [Gleicher 1998; Brand and Hertzmann 2000; Vasilescu 2001].	Expressive features for movement exaggeration	NA:NA	2018
Tazama U. St. Julien:Chris D. Shaw	The Firefighter Training Simulation is a virtual environment being developed at Georgia Tech in collaboration with the Atlanta Fire Department. The VE user is a commanding officer trainee who instructs teams of virtual firefighters to perform different actions to help put out virtual fires. This simulation was developed using the Simple Virtual Environment (SVE) Library, an extensible framework for building VE applications [Kessler 2000].	Firefighter training virtual environment	NA:NA	2018
Samantha Krukowski	A painting is a landscape of possibilities (fig.1) - a form field, a material experiment, a background or foreground, a place of play and imagination. Photographs of people looking at paintings reveal them looking into them, finding things in them that were never consciously put down, never put on the canvas. The way people look at paintings requires a painter to remember the world outside of the work that it, itself, points to. Not only is the materiality of the painted field interesting--its image, vibrant hues and liquid surfaces. What is increasingly interesting, given the possibilities of expanded visualization systems, is how a painting might move into and out of itself, towards and away from its material existence, based on what may or may not be found in its home medium.	Folded: negotiating the space between real and virtual worlds	NA	2018
Timothy Chen:Sidney Fels:Thecla Schiphorst	We have created a new interactive experience piece called Flow-Field. Participants touch and caress a multi-point touchpad, the MTC Express, in a CAVE (CAVE Automatic Virtual Environment), directly controlling a flowing particle field. Collisions in the particle field emit musical sounds providing a new type of musical interface that uses a dynamic flow process for its underlying musical structure. The particle flow field circles around the participant in a cylindrical path. Obstructions formed by whole hand input disturb the flow field like a hand in water. The interaction has very low latency and a fast frame rate, providing a visceral, dynamic experience. In FlowField, participants explore interaction through caress, suggesting reconnection with a sense of play, and experiencing a world through touch.	FlowField: investigating the semantics of caress	NA:NA:NA	2018
Markus Kurtz:Greg Duda	This sketch explains the technical and artistic techniques used by Digital Domain to produce the Ford of Bruinen sequence in New Line Cinema's Lord of the Rings.	Foamy creatures: Digital Domain wrangles whitewater for "Lord of the Rings"	NA:NA	2018
Saty Raghavachary	This sketch describes a way of generating realistic cracks and fragments to visually simulate brittle fracture on polygonal surfaces.	Fracture generation on polygonal meshes using Voronoi polygons	NA	2018
L. Streit:W. Heidrich	Feathers, like hair and fur, dramatically alter the appearance of a surface. However, unlike hair and fur, feathers have a wide range of colours and patterns and a well-defined branching structure. The variety of individual feather structures and patterns contribute to the surface appearance. Thus, it is important to model the complete range of feather types found in a feather coat. Since a feather coat can consist of thousands of structurally unique feathers, it is desirable to automatically generate most of the feathers, while maintaining intuitive control over the coat design and the creation of a wide variety of feather types. Previous work on feathers [Dai et al. 1995] does not address the intuitive generation of such a coat.	Generating feather coats using Bezier curves	NA:NA	2018
Tamotsu Machida	Display technology has been advancing every year. However, the topologically speaking, almost all of these use the same "Planar" surface for their displays. The topology of our display, to be introduced here, is completely different, in that we use "Spherical" surface. GEO-COSMOS, the world's first full color spherical display, is truly remarkable. It is also the main exhibit of the National Museum of Emerging Science and Innovation, opened in Tokyo on July 10, 2001. As shown in Figure 1 and 2, the images/movies on this display are intended for the viewers from all directions.	GEO-COSMOS: world's first spherical display	NA	2018
Koen Meinds:Bart Barenbrug:Frans Peters	Texture mapping is a standard feature of 3D graphics systems. To avoid aliasing artifacts, proper filtering is mandatory. We have developed a novel algorithm for texture mapping and filtering that is suited for hardware implementation. To eliminate texture aliasing artifacts, our algorithm uses higher order FIR filters that are know from digital signal processing. Compared to current state-of-the-art "anisotropic level 8 filtering", that can be found in commercially available advanced graphics accelerators, our method produces higher image quality at equal costs.	Hardware-accelerated texture and edge antialiasing using FIR filters	NA:NA:NA	2018
Michael Kazhdan:Thomas Funkhouser	With the advent of the world wide web, the number of available 3D models has increased substantially and the challenge has changed from "How do we generate 3D models?" to "How do we find them?" In this sketch we describe a new 3D model matching and indexing algorithm that uses spherical harmonics to compute discriminating similarity measures without requiring repair of model degeneracies or alignment of orientations. It provides 46-245% better performance than related shape matching methods during precision-recall experiments, and it is fast enough to return query results from a repository of 20,000 models in under half a second.	Harmonic 3D shape matching	NA:NA	2018
Cynthia Beth Rubin:Daniel F. Keefe	Hiding Spaces is an immersive VR Cave artwork which pushes past the limitations of physical media by exploring the new ambiguities that can delight the viewer in the virtual world. By using innovative tools developed especially for creative work within the Cave environment, in combination with more established digital methods and artistic practice, the authors collaborated to produce a work which transgresses the usual borders of 2D and 3D, including those that are common even in VR environments.	Hiding spaces: a CAVE of elusive immateriality	NA:NA	2018
Bill Spitzak	Most rendering software today destroys their accurate lighting and shading calculations by doing an inaccurate linear conversion to a screen image. This sketch presents a technique that quickly converts floating point data to a screen image while preserving the correct brightness levels and original detail.	High-speed conversion of floating point images to 8-bit	NA	2018
Dan Maynes-Aminzade:Beng-Kiang Tan:Ken Goulding:Catherine Vaucelle	This sketch presents Hover, a device that enhances remote telecommunication by providing a sense of the activity and presence of remote users. The motion of a remote persona is manifested as the playful movements of a ball floating in midair. Hover is both a communication medium and an aesthetic object.	Hover: conveying remote presence	NA:NA:NA:NA	2018
David Esneault:Mitch Kopelman:Jodi Whitsel	At Blue Sky Studios, two overriding principles have guided the development of the renderer and production lighting tools: 1) the lighting model should be as physically accurate as possible, and 2) be straightforward and easy to use so that the computers take care of the technical work leaving the artists free to concentrate on the creative aspects of lighting a scene. Blue Sky's proprietary renderer, CGIStudio™, has one of the most robust lighting models in the industry. The renderer's realistic approach to how light actually behaves in the real world lets the artist add details like soft shadows, reflections, and radiosity at the flip of a switch. Artists are able to achieve complex and subtle lighting with relatively simple lighting rigs, allowing them to get the image as 'right' as possible in the original render.	How a CSG-based raytracer saves time: lighting and scripting for Ice Age	NA:NA:NA	2018
Ari Rapkin	This sketch presents how cloth simulation is used at Industrial Light + Magic to create clothing for digital characters in Star Wars: Episode 2 and other films. Many costumes have to match complex physical costumes worn by live actors in other shots. Digital costumes have not only to look the same as their physical counterparts, but also move identically in response to their wearers' movements and environmental influences..	How to dress like a Jedi: techniques for digital clothing	NA	2018
Simon Gibson:Jon Cook:Toby Howard:Roger Hubbold	The ICARUS system is a suite of software packages, developed at the University of Manchester, that allows geometric models to be quickly and easily reconstructed from image and video sequences captured with uncalibrated digital cameras. The system combines automatic and semi-automatic camera calibration algorithms with an easy-to-use interactive model-building phase (Figure 1). Surface textures are automatically extracted from images and mapped onto the reconstructed models.	ICARUS: interactive reconstruction from uncalibrated image sequences	NA:NA:NA:NA	2018
Yonatan Wexler:Andrew W. Fitzgibbon:Andrew Zisserman	Environment matting is a powerful technique for modelling the complex light-transport properties of real-world optically active elements: transparent, refractive and reflective objects. Zongker et al [1999] and Chuang et al [2000] show how environment mattes can be computed for real objects under carefully controlled laboratory conditions. However, for many objects of interest, such calibration is difficult to arrange. For example, we might wish to determine the distortion caused by filming through an ancient window where the glass has flowed; we may have access only to archive footage; or we might simply want a more convenient means of acquiring the matte.	Image-based environment matting	NA:NA:NA	2018
Da Young Ju:Jin-Ho Yoo:Kyoung Chin Seo:Gregory Sharp:Sang Wook Lee	Visual impressions from two-dimensional artistic paintings greatly vary under different illumination conditions, but this effect has been largely overlooked in most poster productions and electronic display. The light-dependent impressions are more pronounced in oil paintings and they arise mainly from the non-diffuse specular reflectances. We present an efficient method of representing the variability of lighting conditions on artistic paintings utilizing both simple empirical reflectance models and an image-based lighting method. The Lambertian and Phong models account for a significant portion of image variations depending on illumination directions, and residual intensity and color variations that cannot be explained by the reflection models are processed in a manner that is similar to the image-based lighting methods. Our technique allows brush strokes and paint materials to be clearly visible with relatively low data dimensionality.	Image-based illumination for electronic display of artistic paintings	NA:NA:NA:NA:NA	2018
Mark W. Scott	The inspiration for the project described in this sketch was certainly the M. C. Escher lithograph <u>ASCENDING AND DESCENDING</u>. An image of this drawing can be seen at <u>http://www.worldofescher.com/gallery</u>. It is based on the continuous staircase illusion of L. S. and Roger Penrose and depicts a three-dimensional scene that appears to have properties that contradict what is possible in an actual Cartesian representation of spatial objects. The goal of this project was to add animation and a navigable viewpoint to this illusion by using the three-dimensional environment of OpenGL.	Implementing the continuous staircase illusion in OpenGL	NA	2018
Abhinav Dayal:Benjamin Watson:David Luebke	Realtime rendering requires accurate display of a dynamic scene with minimal delay. Frameless rendering [Bishop et al. 1994] offers unique flexibility in this regard: because it samples time per pixel, it can respond to change with very little delay, and at any location in the image. However, sampling is random, resulting in blurring in changing image regions. We present an approach for improving frameless rendering by making sampling sensitive to change in the image, as suggested in [Bishop et al. 1994]. By measuring this change in visual terms, we are able to direct sampling to those regions of change. The resulting algorithm produces sharper imagery, while introducing minimal overhead into the standard frameless algorithm.	Improving frameless rendering by focusing on change	NA:NA:NA	2018
Dorothy Simpson Krause	On September 11 I was in the Middle East on my continuing quest to understand why, in the name of religion, one would kill another who did not share their beliefs. The evening before I had crossed the border to Israel from Jordan, where I had photographed the crumbling sandstone ruins of Petra.	Integrating lenticular into digital printmaking	NA	2018
Thomas Howard:Bryan Morse	This sketch presents a suite of interactive image-editing tools based on properties of and manipulation of image level sets. This suite includes level-set smoothing, level-set constrained sharpening, and level-set "nudging" (image distortion).	Interactive level-set tools for photo editing	NA:NA	2018
Patric Ljung:Mark Dieckmann:Anders Ynnerman	Visual analysis of time varying scientific data can be divided into four different categories with an increasing degree of user interaction. 1) Production of static images representing scientific data at selected times. 2) Production of video sequences in which graphical representation, time line and viewpoints are predefined. 3) Interactive streaming of logged data sets, allowing the user to alter graphical representation, filtering, time lines and viewpoints. 4) Real time interaction with the simulation or experiment that produces the data, allowing the user to alter parameters, graphical representation, filtering, time lines and viewpoints.	Interactive visualization of large scale time varying data sets	NA:NA:NA	2018
J. Cliff Woolley:David Luebke:Ben Watson	Interruptible rendering is a novel approach to the fidelity-versus-performance tradeoff ubiquitous in real-time rendering. Interruptible rendering unifies spatial error, caused by rendering coarse approximations for speed, and temporal error, caused by the delay imposed by rendering, into a single image-space error metric. The heart of this approach is a progressive rendering framework that renders a coarse image into the back buffer and continuously refines it, while tracking the temporal error. When the temporal error exceeds the spatial error caused by coarse rendering, further refinement is pointless and the image is displayed. We discuss the requirements for a rendering algorithm to be suitable for interruptible use, and describe one such algorithm based on hierarchical splatting. Interruptible rendering provides a low-latency, self-tuning approach to interactive rendering. Interestingly, it also leads to a "one-and-a-half buffered" approach that renders sometimes to the back buffer and sometimes to the front buffer.	Interruptible rendering	NA:NA:NA	2018
Robert Falco:Hank Driskill	Standard texturing of a geometric surface works by mapping a 2D image into the parametric space of the surface. This technique works great whenever the texture image is created (painted, etc) for a specific surface with predetermined contours. Serious problems arise when the same texture is applied to the same surface but after some deformation has been applied. The result is the all too familiar rubbery texture look. This stretchy appearance can be particularly noticeable on surfaces that have identifiable traits such as scars, holes, text, repeating patterns, or in our case reptilian scales.	Inverse texture warping	NA:NA	2018
Steve DiPaola	Imagine an n-dimensional space describing every conceivable humanoid face, where each dimension represents a different facial characteristic. Within this continuous space, it would be possible to traverse a path from any face to any other face, morphing through locally similar faces along that path.	Investigating face space	NA	2018
Cindy M. Grimm:William D. Smart	Lewis is a (short) human-sized mobile robot that wanders through crowded rooms taking pictures of people, much like a photographer at a wedding reception does. The goal is to take high-quality, well-composed photographs of people non-intrusively, and to offer these pictures as keep-sakes of the conference.	Lewis the robotic photographer	NA:NA	2018
Marc Downie	This sketch outlines some of the background to a collaborative artwork created in September 2001 by Marc Downie, Shelley Eshkar and Paul Kaiser. This digital portrait of dance legend Merce Cunningham uses as a point of departure a motion-captured recording of 'Loops', his solo dance for hands and fingers; it uses new real-time non-photorealistic rendering techniques; and it exploits an advanced behavior architecture to structure the performance of the piece. In the resulting animation motion-captured joints become nodes in a network that sets them into fluctuating relationships with one another, at times suggesting the hands underlying them, but more often depicting complex cat's-cradle variations.	Loops: a digital portrait	NA	2018
Piotr Karwas	The need to recreate reality is a driving force behind art. Mimesis-imitation of the real world as a main function of art was observed and analyzed as early as the fourth century B. C. by Socrates, Plato, and Aristotle. Although artists searched for inner inspiration, the psychological need to record reality was always present, appearing during the renaissance, in baroque painting, the art of film, and probably reaching it's most sophisticated form in computer graphics and animation. Seventy years ago, Polish critic Karol Irzykowski, in his book "The 10th Muse", predicted that one day animated film would evolve into the most important film genre of the future. He also contemplated the possibility of animation reaching a state of realism in which it could show "ordinary things and people". He would be surprised to learn how true his words were to become. Digital media has given us an opportunity to go even further towards that goal of "registering reality". In this sketch, I present these concepts in relation to Digital Domain's work on New Line Cinema's release of J. R. R. Tolkien's classic tale "Lord of the Rings: Fellowship of the Ring".	Lord of the Rings: animation that was not there	NA	2018
Damian Isla:Bruce Blumberg	It has been suggested that investing animated characters with low-level cognitive models can allow a rich set of low-level behavior to be produced automatically. For example, a character that has the cognitive capability of Object Persistence can intelligently direct its gaze over a scene and even respond emotionally to certain world events. This level of cognitive modeling allows for complete behavioral control by a human controller or a script, if that degree of control is necessary for the application.	"Low level" intelligence for "low level" character animation	NA:NA	2018
Tatsuo Yotsukura:Mitsunori Takahashi:Shigeo Morishima:Kazunori Nakamura:Hirokazu Kudoh	In recent years, tremendous advances have been achieved in the 3D computer graphics used in the entertainment industry, and in the semiconductor technologies used to fabricate graphics chips and CPUs. However, although good reproduction of facial expressions is possible through 3D CG, the creation of realistic expressions and mouth motion is not a simple task.	Magical face: integrated tool for muscle based facial animation	NA:NA:NA:NA:NA	2018
Raphaël Grasset:Jean-Dominique Gascuel	Using augmented reality (AR) in collaborative situations is appealing: it combines the use of natural metaphors of communication (gesture, voice, expression), with the power of virtual ones (simulation, animation, persistent data). But few 3D AR collaborative systems are devoted to keep human's ability (like grasping, writing). The motivation of this research is to mix together virtual reality techniques ([Schmalstieg et al. 2000]) and computer human interaction techniques ([Fjeld et al. 2002]), so to have the best of both worlds.	MARE: multiuser augmented reality environment on table setup	NA:NA	2018
Philo Tan Chua:Rebecca Crivella:Bo Daly:Ning Hu:Russ Schaaf:David Ventura:Todd Camill:Jessica Hodgins:Randy Pausch	We present a wireless virtual reality system and MasterMotion, the full-body training application built with it. We combine realtime full-body optical motion capture with wireless audio/video broadcast, belt-worn electronics, and a lightweight head-mounted display (HMD), to provide a wide-area, untethered virtual environment system that allows exploration of new application areas.	MasterMotion: full body wireless virtual reality for Tai Chi	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
S. Gumhold	Several methods have been proposed to help the user to position lighting sources for a given view of a 3d-scene. In the first kind of methods the user defines a desired illumination through highlight and shadow locations for which the lighting system optimizes the light positions. The more automatic approaches are based on perceptual image metrics. [Marks et al. 1997] define an image metric that measures how different two images are perceived. A collection of maximally different images is presented to the user for selection. [Shacked and Lischinski 2001] define a perceptual based image quality metric composed of six contributing terms, for which the user has to specify weights before the system searches for a locally optimal light source placement.	Maximum entropy light source placement	NA	2018
Charlotte Belland	This animation sketch presents a study of human-driven character animation with motion capture as presented as part of the 2002 SIGGRAPH Course Motion Capture: Pipeline, Applications, and Use.	MOCAP game reserve: a study of puppetry and motion capture	NA	2018
Chen Shen:Kris K. Hauser:Christine M. Gatchalian:James F. O'Brien	This technical sketch describes how a standard analysis technique known as modal decomposition can be used for real-time modeling of viscoelastic deformation. While most prior work on interactive deformation has relied on geometrically simple models and advantageously selected material parameters to achieve interactive speeds, the approach described here has two qualities that we belive should be required of a real-time deformation method: the simulation cost is decoupled from both the model's geometric complexity and from stiffness of the material's parameters. Additionally, the simulation may be advanced at arbitrarily large time-steps without introducing objectionable errors such as artificial damping.	Modal analysis for real-time viscoelastic deformation	NA:NA:NA:NA	2018
Bryan E. Feldman:James F. O'Brien	This technical sketch presents a method for modeling the appearance of snow drifts formed by the accumulation of wind-blown snow near buildings and other obstacles. Our method combines previous work on snow accumulation [Fearing] with techniques for incompressible fluid flows [Fedkiw et al.]. By computing the three-dimensional flow of air in the volume around the obstacles our method is able to model how the snow is convected, deposited, and lifted by the wind. The results demonstrate realistic snow accumulation patterns with deep windward and leeward drifts, furrows, and low accumulation in wind shadowed areas. (See figure.)	Modeling the accumulation of wind-driven snow	NA:NA	2018
Hamish Carr:Thomas Theußl:Torsten Möller	Theußl et al. [Theußl et al. 2001] showed that volumetric data sampled on a body-centred cubic (BCC) lattice is nearly 30% more efficient than data sampled on a cubic lattice, and produced volume renderings using splatting. We extend this work to generate isosurfaces based on the BCC lattice, and also on the hexagonal-close packed (HCP) grid. This sketch presents a modified version of marching octahedra that simplifies the BCC mesh to an octahedral mesh to reduce the number of triangles generated for the isosurface.	Modified marching octahedra for optimal regular meshes	NA:NA:NA	2018
Stefan Gustavson	Modern real time motion capture systems are complex, large scale installations. They focus on accuracy, reliability, ease of use for technical staff and convenience for performers, but unfortunately they are also prohibitively expensive for many applications where motion capture might otherwise prove useful. Low budget work and experiments, VR applications and university research could all benefit from a low cost alternative for motion capture. Such applications could also tolerate a somewhat lower quality, and possibly even some slight inconvenience for users and performers.	Motion capture done dirt cheap	NA	2018
Eric B. Lum:Aleksander Stompel:Kwan-Liu Ma	Motion provides strong visual cues for the perception of shape and depth, as demonstrated by cognitive scientists and visual artists. We present a novel visualization technique that uses particle systems to add supplemental motion cues. Based on a set of rules following perceptual and physical principles, particles flowing over the surface of an object not only bring out, but also attract attention to essential shape information of the object that might not be readily visible with conventional rendering. Replacing still images with animations in this fashion, we show with both surface and volumetric models that the resulting visualizations effectively enhance the perception of three-dimensional shape and structure.	Motion-based shape illustration	NA:NA:NA	2018
Marc Cardle:Stephen Brooks:Loic Barthe:Mo Hassan:Peter Robinson	Sounds are generally associated with motion events in the real world, and as a result there is an intimate connection between the two. Hence, producing effective animations requires the correct association of sound and motion, which remains an essential, yet difficult, task. But unlike prior, application-specific systems such as [Lytle90; Singer97], we address this problem with a general framework for synchronizing motion curves to perceptual cues extracted from the music. The user is able to modify existing motions rather than needing to incorporate unadapted musical motions into animations. An additional fundamental feature of our system is the use of music analysis techniques on complementary MIDI and analog audio representations of the same soundtrack.	Music-driven motion editing	NA:NA:NA:NA:NA	2018
Konstantin Economou:Mark Ollila:Martin Etherton Friberg:Anders Ynnerman	As cultural heritages go, or rather stand firm, the Norrköping "industrial landscape" (industrilandskapet in Swedish) has been the object of cultural regeneration over the last two decades. Once, one of Sweden's largest city center industrial sites, major supplier of textile and paper it has now become the nexus of cultural activity. Museums, a concert hall and, the university, have now made the scenery different. In turning the landscape over we might think that old industrial life has succumbed to the changes in industrialisation making Norrköping move from manufacture to knowledge industry. In museums, this era of "old" is depicted in nostalgic ways - as "lost" cultural heritage rather than as living imagery. And, where did manufacturing go? Is it the case that industry has changed or has it just moved.	Now and then, here and there: industrilandskapet	NA:NA:NA:NA	2018
Timothy Nohe	This sketch presents an artwork entitled Occidio, which has been produced to interpret the alarming phenomena of global warming in the form of a computer-controlled sound and DVD installation. Occidio interprets NASA scientific visualizations through the interplay of video projections, event-triggered synthetic sound generated by optical theremins, and metaphoric sculptural forms. Visitors to Occidio shall "see," and, "hear" interpretations of global warming.	Occidio	NA	2018
Saty Raghavachary:Fernando Benitez	This sketch describes how to construct a painterly 'wall' of fire, one which exhibits realistic motion while managing to maintain an artistic look.	Painterly fire	NA:NA	2018
Kevin G Suffern	This sketch presents fractal art work created by ray tracing the specular highlights of point lights on the inside surface of a hollow sphere. The sphere has a mirror surface on the inside that contributes no colour to the images, but there is spread in the local specular reflection. The resulting images consist entirely of specular highlights and their reflections. I call this painting with light because, although recursive ray tracing is used, no objects are visible.	Painting with light	NA	2018
Kyoko Murakami:Reiji Tsuruno	There has been any works specifically on computer generated 3D pastel rendering. Some rendering or imaging softwares can generate pastel drawing-like images recently, but most of them cannot modeling and rendering objects as 3D, and do not have enough power of expression in comparison with real-pastel drawings.	Pastel-like rendering considering the properties of pigments and support medium	NA:NA	2018
Hunter Murphy:Andrew T. Duchowski	A common assumption exploited in perceptual Virtual Reality studies is that eye movements made while immersed in VR generally do not deviate more than 30° (visual angle) from the head-centric view direction (e.g., see Barnes [1979]). In this sketch we report eye tracking evidence which generally supports this observation in the context of peripheral Level Of Detail management during a visual search task in VR. We present results from experiments based on the work of Watson et al. [1997] and discuss an extension to the peripheral degradation paradigm to include a dynamic eye-slaved high-resolution inset.	Perceptual gaze extent & level of detail in VR: looking outside the box	NA:NA	2018
Patrick Ledda:Alan Chalmers:Greg Ward	A major goal of realistic image synthesis is to generate images that are both physically and perceptually indistinguishable from reality. One of the practical obstacles in reaching this goal is that the natural world exhibits a wide range of colors and intensities. The range of the luminances in the real world can vary from 10-4cd/m2 (for starlight) to 105cd/m2 (for a daylight scene). Reproducing these luminances on a cathode-ray tube (CRT) display is currently not possible as the achievable intensities are about 100 cd/m2 and the practical ratio between maximum and minimum pixel intensity is approximately 100:1. At the University of Bristol, we have constructed a High Dynamic Range (HDR) viewer that is capable of achieving a 10,000:1 contrast ratio. This sketch investigates, by means of psychophysical experiments, the benefits such a HDR device has to offer realistic computer graphics.	Perceptual tone mapping operators for high dynamic range scenes	NA:NA:NA	2018
Markus Manninen	This animation sketch presents a production case study of how computer graphics work was designed to support and allow for director Jonathan Glazer's live action shooting style on "Odyssey", his latest internationally acclaimed commercial for Levi's. The film features a man and a woman in an ecstatic state of freedom to move. They achieve this by first escaping their drab interior surroundings, relentlessly running through a succession of walls. Once outside, they run vertically up two enormous trees and upon reaching the very tip take a huge leap and launch themselves up towards the stars.	Performance driven computer graphics making Odyssey	NA	2018
Isao Mihara:Takahiro Harashima:Shunichi Numazaki:Miwako Doi	A new video camera system, Pop.eye, is able to capture a realtime 3D video based on color image and reflection image. The current prototype system (Figure 1) can capture 160x120 sized images at a rate of 25 frames per second. The captured image of a stuffed toy is viewed as the "Pop-out" image frame shown in Figure 2-left. The "Pop-out" shape is constructed from the captured reflection image. Furthermore, this system is able to output an enhanced "Pop-out" movie that extracts only the target object from background by using depth-key (Figure 2-right).	Pop.eye: a pop-out video camera system for personal use	NA:NA:NA:NA	2018
Adrian Secord:Wolfgang Heidrich	Non-photorealistic rendering often requires placing drawing primitives onto a 2D canvas in such a way that the resulting tone approximates that of a greyscale reference image. Several iterative methods have been used where each stroke is tentatively placed on the canvas and the resulting tone is evaluated with respect to the reference image. [Salisbury et al. 1994; Salisbury et al. 1997; Praun et al. 2001] If the stroke over-darkens the output image it is rejected, otherwise it is accepted. While this back-and-forth iteration between the output and the reference image is capable of producing high-quality results, it is extremely costly in terms of computation and memory references.	Probabilistically placing primitives	NA:NA	2018
Kathleen Gretchen Greene	A plague of frogs, a river of stones, a dragon's back of scales... When a large group should exhibit variation in some characteristic, how can we describe and experiment with that variation and the look and feel of the group as a whole?	Probability paint: controlling group characteristics with PDFs	NA	2018
Jonah Hall	Dreamworks Pictures film "The Time Machine" presented a number of interesting problems related to creating time-lapse photography using computer animation. I was assigned with the task of making realistic vines creep and grow along the surface of the green house as the main character began to move forward in time. In order to keep such a complicated task manageable, I needed to organize the order in which things were processed to keep things workable as changes were made. The pipeline needed to be designed in such a way so that the maximum amount of processing is taken care of at the earliest stage.	Pushing the limits of L-systems for time-lapse vine growth in "The Time Machine"	NA	2018
Jerome Grosjean:Sabine Coquillart	Virtual environments (VEs) like the projection-based Responsive Workbench have greatly enhanced the interactive visualization and manipulation of 3D objects. In these configurations, the classical desktop interaction techniques have to be reconsidered. In particular, a simple operation like text typing, although needed for basic operations like saving one's work under a specific file name or entering a precise numerical value inside an application, is still problematic.	Quikwriting on the responsive workbench	NA:NA	2018
Oliver Bimber:L. Miguel Encarnação	Paleontology is filled with mysteries about organisms such as plants and animals that lived thousands, millions, and billions of years before the first modern humans walked the earth. To solve these mysteries, paleontologists rely on the excavation, analysis, and interpretation of fossils. Fossils are the remains or traces of ancient life forms that are usually preserved in stones and rocks. Examples include bones, teeth, shells, leaf imprints, nests, and footprints. Such fossil discoveries reveal what life on our planet was like long ago. Fossils also disclose the evolution of organisms over time and how they are related to one another. While fossils reveal what ancient living things looked like, they keep us guessing about their color, sounds, and most of all their behavior. Each year, paleontologists continue to piece together the stories of the past.	RAPTOR: towards augmented paleontology	NA:NA	2018
Sarah Witt	The Sony PlayStation2, with its powerful rendering and vector processing capabilities, built-in MPEG decoder, and large capacity hard disc drive possesses fundamental assets that make it an ideal platform for processing video. Research done at Sony BPRL, where we have extensive experience of video effects processing, has shown that this inexpensive games console can, in fact, create in real time the type of video effects which normally require investment in dedicated hardware (or endless patience as a PC renders them slowly).	Real time video effects on a PlayStation2	NA	2018
Simon Prince:Adrian David Check:Farzam Farbiz:Todd Williamson:Nik Johnson:Mark Billinghurst:Hirokazu Kato	We demonstrate a real-time 3-D augmented reality video-conferencing system. The observer sees the real world from his viewpoint, but modified so that the image of a remote collaborator is rendered into the scene. For each frame, we estimate the transformation between the camera and a fiducial marker using techniques developed in Kato and Billinghurst [1999]. We use a shape-from-silhouette algorithm to generate the appropriate view of the collaborator in real time. This is based on simultaneous measurements from fifteen calibrated cameras that surround the collaborator. The novel view is then superimposed upon the real world image and appropriate directional audio is added. The result gives the strong impression that the virtual collaborator is a real part of the scene.	Real-time 3D interaction for augmented and virtual reality	NA:NA:NA:NA:NA:NA:NA	2018
Jason L. Mitchell:Chris Brennan:Drew Card	In Non-Photorealistic Rendering (NPR), outlines at object silhouettes, shadow edges and texture boundaries are important visual cues which have previously been difficult to generate in real-time. We present an image-space technique which uses pixel shading hardware to generate these three classes of outlines in real time. In all three cases, we render alternate representations of the desired scene into texture maps which are subsequently processed by pixel shaders to find discontinuities corresponding to outlines in the scene. The outlines are then composited with the shaded scene.	Real-time image-space outlining for non-photorealistic rendering	NA:NA:NA	2018
Ruigang Yang:Greg Welch:Gary Bishop:Herman Towles	We present a novel use of commodity graphics hardware that effectively combines a plane-sweeping algorithm [Collins 1996] and view synthesis in a single step for real-time, on-line 3D view synthesis. Unlike typical stereo algorithms that use image-based metrics to estimate depths, we focus on using image-based metrics to directly estimate images. Using real-time imagery from a few calibrated cameras, our method can generate new images from nearby viewpoints, without any prior geometric information or requiring any user interaction, in real time and on line.	Real-time view synthesis using commodity graphics hardware	NA:NA:NA:NA	2018
Tomáš Staudek:Petr Machala	Exact aesthetics is a challenging field of the computer-aided visual creativity, reconstructing the methods of design and criticism on an algorithmic basis and integrating a computer into processes of an artistic creation and aesthetic evaluation. The discipline involves principles of mathematics, geometry, theory of communication, perceptual psychology, computer graphics, or generative arts into classifying and assessing the aesthetic phenomena. The sketch introduces recent applications in this domain.	Recent exact aesthetics applications	NA:NA	2018
Andrzej Zarzycki	There is a certain mystery surrounding the unbuilt projects or unrealized ideas of famous architects. Often there is an expectation of deeper meaning and hidden genius present in unfulfilled buildings. Some critics go as far as to claim that the best and most interesting projects remain unrealized because of the progressiveness of the ideas associated with those buildings.	Reconstructing or inventing the past: a computer simulation of the unbuilt church by Alvar Aalto	NA	2018
Hiroto Matsuoka:Akira Onozawa:Hidenori Sato:Hisao Nojima	The vision of this work is make it possible to regenerate real objects that had existed at some moment in the past and/or at some remote location as if they have been transported to the present across space and time. The objects could be museum pieces or items in stores, for example. In this work, we have developed a quick and fully automated system that can capture a three-dimensional image of real objects. This success has brought us close to realizing our vision.	Regeneration of real objects in the real world	NA:NA:NA:NA	2018
Suguru Saito:Akane Kani:Youngha Chang:Masayuki Nakajima	In simple style drawings, like Comics and traditional cel animation, curved strokes are relatively important. Of course, the shape of the curve is the most important. However, subtle changes of curve width cannot be ignored. We propose a powerful method allowing subtle width changes to be applied to general 2D curve data. The algorithm is based on curvature information of the input curve, and keeps carefully the impression of the original curvature. The resulting image expresses a pen-and-ink drawing style.	Rich curve drawing	NA:NA:NA:NA	2018
Jörg Peters:Xiaobin Wu	Given a composite spline surface, we show how to efficiently construct two matching triangulations that sandwich the surface. Such a two-sided enclosure, (s-, s+), supports collision detection, re-approximation for format conversion, meshing with tolerance, one-sided smoothing and silhouette detection as illustrated in Figure 2.	Sandwiching surfaces	NA:NA	2018
Johnny Gibson	For The Time Machine the CG effects team faced the daunting task of producing imagery that represented long exposure and time-lapse photography of various types of terrain. The scope of the effort and the shot design largely preempted the use of simulated or exclusively explicit techniques for terrain construction. The challenge we faced was to procedurally model surface detail features into terrain that would visually represent the erosion of volumes of earth and rock over time. These features had to be consistent over time and through the entire volume of earth and rock through which the terrain would erode. These restrictions gave rise to two shader techniques used both in the procedural construction of geometry and in the displacement phase of terrain shading: bouldering and gulleying.	Shader analytical approximations for terrain animation in "The Time Machine"	NA	2018
Aaron Hertzmann:Nuria Oliver:Brian Curless:Steven M. Seitz	This sketch presents "Shape Analogies," a method for learning line styles from examples. With this approach, an artist or end-user simply draws in the desired style; the system analyzes the drawings and generates new imagery in the same style. For example, to design an outline style for a nervous character, one may draw a jittery stroke; to design an outline style for a robot, one may draw a very rigid style with many sharp angles.	Shape analogies	NA:NA:NA:NA	2018
Marco Tarini:Hendrik Lensch:Michael Goesele:Hans-Peter Seidel	Objects with mirroring surfaces are left out of the scope of most recent 3D scanning methods. We developed a new acquisition approach, shape-from-distortion, that focuses on that category of objects, requires only a still camera and a monitor, and generates high quality range scans (plus a normal field). Our contributions are a novel acquisition technique based on environment matting [Chuang et al. 2000] and corresponding geometry reconstruction method that recovers a very precise geometry model for mirroring objects.	Shape from distortion: 3D range scanning of mirroring objects	NA:NA:NA:NA	2018
Eric Guaglione:Doug Sweetland	In character animation, it is often said that a strong pose is supported by its' silhouette. Since the shape of a 3D character represented in a graphical, 2D screen-space is ultimately our goal, shouldn't our tools better support posing the silhouette? Yet commercial software packages have focused on implementations to pose the character from the inside out, rather than looking at the shape inward.	Shape-based character animation	NA:NA	2018
John Haddon	The animated short "a flatpack project" was intended to emulate the aesthetics of traditional art techniques in a digital medium. This necessitated the creation of custom rendering and image processing code to reproduce the appearance of line drawings in both pencil and ink, along with other effects such as the bleeding of ink in water and the application of pastel to paper. This sketch describes some of the techniques used in achieving these ends.	Sketchy rendering	NA	2018
Valerio Pascucci	In recent years subdivision methods have been successfully applied to the multi-resolution representation and compression of surface meshes. Unfortunately their use in the volumetric case has remained impractical because of the use of tensor-product generalizations that induce an excessive growth of the mesh size before sufficient number is preformed. This technical sketch presents a new subdivision technique that refines volumetric (and higher-dimensional) meshes at the same rate of surface meshes. The scheme builds adaptive refinements of a mesh without using special decompositions of the cells connecting different levels of resolution. Lower dimensional "sharp" features are also handled directly in a natural way. The averaging rules allow to reproduce the same smoothness of the two best known previous tensor-product refinement methods [Bajaj et al. 2001;MacCracken and Joy 1996].	Slow growing volumetric subdivision	NA	2018
Maria Giannakouros	Night scenes filmed on indoor stage sets, and in day or evening light, all have one thing in common-the absence of a starry sky. Even shots filmed at night, under perfect conditions, fail to capture stars. Since the focus is on local subjects which need adequate lighting to be captured, the sky is too under exposed to capture stars.	Star fields in 2D	NA	2018
Umesh Shukla	Painterly animation has till now been a part of research groups and enthusiast. This film attempts to bring it closer to the masses using commercially available software.	"Still I Rise" painterly animation off the shelf	NA	2018
Ramon Montoya-Vozmediano:Mark Hammel	A project currently in production at Walt Disney Feature Animation requires realistic hair with a high degree of artistic control. In this sketch we present a set of techniques that achieve these goals along the production pipeline.	Stylized flowing hair controlled with NURBS surfaces	NA:NA	2018
Sandra Villarreal	Synchronous Pronouncement is a generative interactive installation that will explore the aesthetics of an interactive medium as an extension of our body. The irregularity and unpredictability of our world is a result of unique patterns left behind by movements. The installation will be a representation of interacting processes that operate in nature. Autonomous behaviors generated by code will be triggered by the presence of users for the creation of a unique real-time audiovisual experience.	Synchronous pronouncement	NA	2018
Ali Mazalek:Glorianna Davenport	Over the centuries, stories have moved from the physical environment (around campfires and on the stage), to the printed page, then to movie, television, and computer screens. Today, using wireless and tag sensing technologies, story creators are able to bring digital stories back into our physical environment. The Tangible Viewpoints project explores how physical objects and augmented surfaces can be used as tangible embodiments of different character perspectives in a multiple point-of-view interactive narrative. These graspable surrogates provide a more direct mode of navigation to the story world, bringing us closer to bridging the gap between the separate realms of bits and atoms within the field of digital storytelling.	Tangible viewpoints: a physical interface for exploring character-driven narratives	NA:NA	2018
Dan Maynes-Aminzade:Randy Pausch:Steve Seitz	At SIGGRAPH in 1991, Loren and Rachel Carpenter unveiled an interactive entertainment system that allowed members of a large audience to control an onscreen game using red and green reflective paddles. In the spirit of this approach, we present a new set of techniques that enable members of an audience to participate, either cooperatively or competitively, in shared entertainment experiences. Our techniques allow audiences with hundreds of people to control onscreen activity by (1) leaning left and right in their seats, (2) batting a beach ball while its shadow is used as a pointing device, and (3) pointing laser pointers at the screen. All of these techniques can be implemented with inexpensive, off the shelf hardware.	Techniques for interactive audience participation	NA:NA:NA	2018
David Mould:Eugene Fiume	A great deal of attention has been paid to the problem of texture synthesis. Procedural techniques have become commonplace. Yet while a proliferation of models, and the success of nonparametric synthesis-from-example methods, has made an extraordinary variety of textures realizable, the creation of novel textures remains a challenge.	Textures from nonlinear dynamical cascades	NA:NA	2018
Jose L. Hernandez-Rebollar:Nicholas Kyriakopoulos:Robert W. Lindeman	We present The AcceleGlove, a novel whole-hand input device to manipulate three different virtual objects: a virtual hand, icons on a virtual desktop and a virtual keyboard using the 26 postures of the American Sign Language (ASL) alphabet.	The AcceleGlove: a whole-hand input device for virtual reality	NA:NA:NA	2018
Volker Paelke:Joerg Stoecklein:Lennart Groetzbach:Christian Geiger:Christian Reimann:Waldemar Rosenbach	The AR-ENIGMA combines a PDA (personal digital assistant) with a camera, a high-speed wireless network connection and AR (augmented reality) technology to enable museum visitors to interact with an Enigma encryption machine.	The AR-ENIGMA: a PDA based interactive illustration	NA:NA:NA:NA:NA:NA	2018
John Jay Miller:Weidong Wang:Gavin Jenkins	Inadequately or poorly designed environments and tools of daily living impose barriers to people with a disability. This issue needs to be addressed in order for people with disabilities to lead full and purposeful lives. To accomplish this goal it is imperative that designers of environments and artifacts have an in-depth knowledge of human functioning in the performance of tasks and problem-solving strategies to develop environments and products that best accommodate performance of these tasks. They require an understanding and useful characterizations of the abilities of people with disabilities and relevant mechanisms to incorporate this into a modern design process.	The development of a functional visualization system for the creation of digital human models	NA:NA:NA	2018
Vincent Masselus:Philip Dutré:Frederik Anrys	The Free-form Light Stage captures the reflectance field of an object using a freely movable, hand-held light source. By photographing the object under different illumination conditions, we are able to render the object under any lighting condition, using a linear combination of basis images. Our technique builds on recent techniques, such as the Light Stage [Debevec et al. 2000], where light sources are placed at fixed and known positions (e.g. using a gantry). We remove this limitation, and put no restrictions on light source placement.	The free-form light stage	NA:NA:NA	2018
A. Fleming Seay:Diane Gromala:Larry Hodges:Chris Shaw	During the Emerging Technologies exhibition at Siggraph 2001, over 400 attendees experienced The Meditation Chamber. This immersive, bio-interactive environment was designed to use visual, audio, and tactile cues to create, guide, and maintain a user's guided relaxation and meditation experience. During this sketch, the project's producers will discuss the design and implementation of this unique installation. We will also show footage from the experience and discuss the subjective relaxation measures and the GSR, heart rate and respiration data generated by 411 Siggraph attendees.	The meditation chamber: a debriefing	NA:NA:NA:NA	2018
Hideyuki Ando:Takeshi Miki:Masahiko Inami:Taro Maeda	The "SmartFinger" is a new type of tactile display, which is worn on the nail side of the finger. It does not inhibit our tactile sensation, since the ball of the finger is naked and we can feel the environment directly. It is important to insert nothing between a finger and an object.	The nail-mounted tactile display for the behavior modeling	NA:NA:NA:NA	2018
David K. McAllister:Anselmo A. Lastra:Benjamin P. Cloward:Wolfgang Heidrich	Combining texture mapping with bi-directional reflectance distribution functions (BRDFs) yields a representation of surface appearance with both spatial and angular detail. We call a texture map with a unique BRDF at each pixel a spatial bi-directional reflectance distribution function, or SBRDF. The SBRDF is a six-dimensional function representing the reflectance from each incident direction to each exitant direction at each surface point. Because of the high dimensionality of the SBRDF, previous appearance capture and representation work has focused on either spatial or angular detail, has relied on a small set of basis BRDFs, or has only treated spatial detail statistically [Dana 1999; Lensch 2001].	The spatial bi-directional reflectance distribution function	NA:NA:NA:NA	2018
Tsunemi Takahashi:Heihachi Ueki:Atsushi Kunimatsu:Hiroko Fujii	This sketch describes the modeling of the interaction between fluid and rigid bodies, how to simulate scenes in which fluid pressure acts on a rigid body, and conversely, in which rigid body motion drives a force back to fluid. We construct the interface between fluid simulation using the Cubic Interpolated Propagation (CIP) method and rigid body simulation using the impulse-based method. For fast simulation, we apply the CIP method to uniform structured meshes. For treating the interaction between rigid bodies and fluid efficiently, we use Volume of Solid (VOS) for rigid bodies, and for the collision among rigid bodies, we use Polygon-Polygon collision detection. For fast response to rigid body's collision, we use smaller time step for rigid body than for fluid.	The simulation of fluid-rigid body interaction	NA:NA:NA:NA	2018
Kajsa Ellegård:Johan Torne:Henric Joanson:Anders Ynnerman:Matthew Cooper:Mark Ollila	In natural sciences the practice of using computers to visualize and analyze data was adopted early. In social and cultural studies, however, computer technology has not been as commonly used for visualization purposes. This project to visualize Time Geographical datasets, the Time Geography Project, is therefore somewhat groundbreaking.	The time geography project: using computer graphics to visualize problems in social science	NA:NA:NA:NA:NA:NA	2018
Blair MacIntyre:Jay David Bolter:Jeannie Vaughan:Brendan Hannigan:Emmanuel Moreno:Markus Haas:Maribeth Gandy	"Three Angry Men" is a novel augmented reality experience that explores the use of Augmented Reality (AR) as a dramatic medium. The user participates in an AR version of the famous twentieth-century play, "Twelve Angry Men," [Rose 1983] which for practical reasons we have abbreviated into a scene involving 3 characters (thus, "Three Angry Men"). The participant finds herself immersed in a physical jury-room, where virtual characters (jurors in the drama, rendered as video-based characters overlaid at appropriate 3D locations around the physical table using a see-through head-worn display) debate the guilt of a young man on trial for murder (see Figure 1).	Three angry men: dramatizing point-of-view using augmented reality	NA:NA:NA:NA:NA:NA:NA	2018
Christopher Jaynes:Joan Mazur:Cindy Lio	The Metaverse is a synthesized world that combines computer-generated elements with the real-world to either augment of supplant a users view reality. To fully participate in this meta-world, users must access it through a unique interface that is visually immersive and non-restrictive, interactive and collaborative. Our ongoing research program in immersive environments emphasizes techniques that make widespread deployment and use of the Metaverse feasible. To that end, we are addressing technical challenges related to self-configuring, self-monitoring displays. Other researchers have made significant progress in overcoming the obstacles related to building tilted displays. Although these technical advances have made low-cost immersive displays feasible for a larger community of users, there is little understanding of the remaining HCI obstacles that prevent widespread deployment of such systems. Our early work, as part of a five-year evaluation project, focuses on defining the methods that will be required to understand usability and utility of the technology. Additionally, we are developing of tools that will assist researchers in characterizing the behavior of users within nonrestrictive immersive environments.	Towards visualizing HCI for immersive environments: the meta-situational tracker	NA:NA:NA	2018
Lori L. Scarlatos:Saira Qureshi:Shalva S. Landy	Children naturally learn about their world by manipulating objects within it. Playing with blocks and puzzles helps to develop their understanding of spatial relationships and other mathematical concepts. Using physical objects also allows them to work and learn in groups. Yet sometimes they need outside intervention from an adult or knowledgeable guide to help them learn more and stay engaged longer. Unfortunately, instructors often have too many students to give each one adequate attention. Our work focuses on developing computer-based "guides on the side" that can "watch" as children play with physical puzzles, and offer help or suggestions as needed. Our approach is to use the physical puzzle pieces as parts of a tangible interface. With our system, children are free to explore and collaborate without a computer, yet they can benefit from the computer's instruction as they need it. We have successfully implemented and tested a 2D Tangram puzzle using this approach [Scarlatos 2002].	Tracking 3D puzzle pieces for collaborative learning environments	NA:NA:NA	2018
Kenji Tanaka:Junya Hayashi:Yutaka Kunita:Masahiko Inami:Taro Maeda:Susumu Tachi	TWISTER (Telexistence Wide-angle Immersive STERe-oscope) is an immersive full-color autostereoscopic display, designed for a face-to-face telecommunication system called "mutual telexistence", where people in distant locations can communicate as if they were in the same virtual three dimensional space.	TWISTER: a media booth	NA:NA:NA:NA:NA:NA	2018
Brad Parker	Digital Domain's character animation team was asked to create a complex destruction sequence for Time Machine's main villain... the Uber Morlock. This was to be no ordinary demise. Our villain had to age rapidly. His skin was to wrinkle, discolor, and disintegrate. His hair was to recede, his clothing to oxidize and fall apart. His muscles were to atrophy, eventually releasing his bones one by one. Finally his bones were to break apart and turn to dust.	Uber destruction in "The Time Machine"	NA	2018
John Isidoro:Jason L. Mitchell	Recent advances in real-time fur rendering have enabled the development of more realistic furry characters. In this sketch, we outline a number of advances to the shell and fin based fur rendering technique by Lengyel et al [2001] using the pixel and vertex shader capabilities of modern 3D hardware.	User customizable real-time fur	NA:NA	2018
Bent Dalgaard Larsen:Jakob Andreas Bærentzen:Niels Jørgen Christensen	Distributed VR promises to change the way we work and collaborate [Singhal and Zyda 1999]. In this sketch we will extend the accessibility of the virtual world originally developed in [Larsen and Eriksen 1998] by introducing the use of the modern cellular phone as a platform for primitive interfaces to VR applications. We believe that our use of a cellular phone has led to the first completely pocketable platform for VR user interfaces.	Using cellular phones to interact with virtual environments	NA:NA:NA	2018
Michael Flaxman	Traditional architectural software tools are well suited to individual building design, but don't scale up well to large landscapes and cities. In city and regional planning applications, it is important to visualize city blocks and neighborhoods, a task which requires integration of data from geographic information systems and 3D surface models from CAD. However, the development of semantically and visually rich real-time representations of cities and large landscapes has proven difficult. Two major problems are the acquisition of appropriate data, and the volume of data once acquired. A typical city or suburban landscape may have thousands of streets, tens of thousands of buildings, and hundreds of thousands of trees and bushes. Three dimensional data specifically describing all of these objects is not usually available, and visual simulations must be based on abstract classifications originally developed for entirely different purposes, such as tax assessor's databases or generalized vegetation maps.	Using the virtual terrain project to plan real cities: alternative futures for Hangzhou, China	NA	2018
Neill Campbell:Colin Dalton:David Gibson:Barry Thomas	Recently, there have been attempts at creating 'video textures', that is, synthesising new video clips based on existing ones. Schodl et al. showed new video clips by carefully choosing sub-loops of an original video sequence that could be replayed.	Video textures using the auto-regressive process	NA:NA:NA:NA	2018
Bernadette Kiss:Gábor Szijártó:Barnabás Takács	We describe our ongoing research on creating a Virtual Human Interface that employs photo-realistic virtual people and animated characters to provide digital media users with information, learning services and entertainment in a highly personalized manner. Our system was designed to be able to create emotional engagement between the virtual character and the user, thus increasing the efficiency of learning and/or absorbing any information broadcasted through this device. We developed innovative technologies for (i) photo-real facial modeling & animation, (ii) context dependent motion libraries with on-line retargeting, (iii) artificial emotions to modulate the characters' behavior and (iv) artificial vision to make the virtual human "aware" of its surroundings. The second key aspect of our solution is a simple to use high level content authoring process, comprising of video-based MPEG4 facial tracking and an innovative interface called the "Disc Controller", which allows users to create new actors, make them move and even direct them to achieve a final rendered output within minutes.	Virtual human interface: towards building an intelligent animated agent	NA:NA:NA	2018
Patrick Hartling:Allen Bierbaum:Carolina Cruz-Neira	Developers of virtual environments often face a difficult problem: users must have some way to interact with the virtual world. The application developers must determine how to map available inputs (buttons, gestures, etc.) to actions within the virtual environment (VE). As a result, user interfaces may be limited by the input hardware available with a given virtual reality (VR) system.	Virtual reality interfaces using Tweek	NA:NA:NA	2018
Akira Kubota:Kiyoharu Aizawa	In this sketch, we present a novel approach to image-based rendering (IBR) techniques for generating a virtual view image from different positions with arbitrary focus using two differently focused images captured from a fixed position. In the conventional approach [Potmesil and Chakravarty 1981], focus blur have been produced by applying a realistic camera model to given 3D objects. We propose here a much simpler and more effective method only using space-invariant filters to render both parallax and focusing effects on objects in a scene, according to the virtual camera's position and focus depth respectively. The proposed method does not need any segmentation and 3D modeling.	Virtual view generation by linear processing of two differently focused images	NA:NA	2018
Dietmar Offenhuber	This web3d project explores how the concept of non-linear space -- that is space structured by relative units -- can be used in VR and architecture. It offers a dynamic view on Los Angeles' structure, radically different from usual architectural representations.	Wegzeit: the geometry of relative distance	NA	2018
Jackie White	NA	Session details: Special session - industrial light & magic	NA	2018
Dawn Yamada:Geoff Campbell:Sebastian Marino:Zoran Kacic-Alesic:James Tooley	Computer graphics play a starring role in the production of Star Wars Episode II: Attack of the Clones. This session focuses on the creation of the digital cast of the latest prequel to the Star Wars saga. Industrial Light & Magic developed a variety of systems to make the computer-generated characters in this film stand up to the actors with whom they share the screen, both in visual quality and physical realism. These systems also made it possible for digital doubles to stand in for actors in scenes either too difficult or too dangerous to shoot practically. In an effort to match the fidelity of motion of the computer graphics characters to that of their live-action counterparts, physically based simulation was used extensively throughout the production of this film. Multi-layered clothing, skin with underlying musculoskeletal structures, and the motion of rigid bodies each played a key role in imparting a new level of physical realism into the performance of computer graphics elements. The challenge of employing this level of proceduralism is also providing methods for directing the resulting performances. In this session, we will present an overview of the pipeline and systems used to produce Episode II, with the focus of the discussion being on the specialization required to evolve technologies, deeply rooted in academic research, into effective filmmaking tools. The panel will include individuals who played key roles in the development of key digital characters for the latest prequel to Star Wars.	Yoda and beyond: creating the digital cast of Star Wars episode II	NA:NA:NA:NA:NA	2018
Jason Della Rocca:Raph Koster:Lorne Lanning:Scott Miller:Warren Spector:Will Wright	Prominent members of the International Game Developers Association will investigate and discuss the direction of the game industry and the impact interactive entertainment will have on our future. This panel of game industry revolutionaries will explore how game design, character development, online connectivity, business models, and social and cultural implications all weave together with advances in technology to drive the industry forward.	The fate of play: game industry revolutionaries speak out	NA:NA:NA:NA:NA:NA	2018
Mary Reardon	Sony Pictures Imageworks takes you for a spin through the virtual world of Spider-Man. Scott Stokdyk and his team reveal how the effects, buildings, and characters were created in the computer and integrated with the live action. CG Supervisor Ken Hahn discusses the complexities involved in creating the buildings of New York City. CG Supervisor Peter Nofz explains the challenges of setting up characters for animation, and Character look Lead Greg Anderson shows us the process of look development for character lighting.	Spider-Man: behind the mask	NA	2018
Jill Smolin:Scott Clark:Jennifer Emberly:Stephen A. Fossati:Doug Sweetland:Barry Weiss	Cartoons turned a corner when Chuck Jones came to town. He transformed Walt Disney's vision to one of wit, humor and mischief. We watched as Wile E. Coyote repeatedly attempted to trap the Roadrunner, only to fall victim to his own falling anvils; Pepe Le Pew's aromatic expressions taught us everything we need to know about unsuccessful romance; Marvin Martian's Gladiator skirt, tennis shoes and romanesque helmet gave us an alternate view of aliens out to destroy the earth. And of course, Bugs Bunny's gregarious self-confidence enabled him to outwit, outsmart, and outsing any adversary. For most of the 20th Century, Chuck Jones has shaped the way we see a particular side of the world, and our art and our souls are all the better for it.	What's up doc?: a fond remembrance of Chuck Jones	NA:NA:NA:NA:NA:NA	2018
Dan Collins	NA	Session details: Studio	NA	2018
Bill Brody:Glenn G. Chappell:Chris Hartman	BLUIsculpt™ is an interactive virtual reality application that permits a user to freely sketch voxels inside a ten foot cube for output as physical objects. It connects the imagination to physical form. Our interface presents the user with real-time feedback in the form of surfaces. We save a file representing the surface in rapid protyping format. Generating a solid-by-rapid prototyping completes the cycle of perception and imagination that starts in the physical world, proceeds through vision, thought, imagination that starts in the physical world, proceeds through vision, thought, imagination and the dance of drawing to finally arrive at tangible sculpture. Our enterprise is based on the premise that symbolic representation, of which drawing is an exemplar, derives from perception, imagination and thought. We hold that these mental activities recruit and employ the same brain machinery that is fundamental to voluntary physical movement. Such is the common wisdom of generations of artists, and is well-supported by a mass of more recent studies in cognitive and evolutionary neuroscience.	BLUIsculpt™	NA:NA:NA	2018
DaShawn L. Hall	From the Oscar winning film, Gladiator, to studios, such as Pacific Data Images (PDI), Cinema 4D XL has been used for background design, modeling, and pre-visualization artwork. With its user-friendly interface, powerful animation, advanced modeling tools, and high-speed rendering technology, major film and animation studios, as well as educators have utilized it. Cinema 4D XL's advanced tools, such as a VRML plug-in and various modeling systems, can be used to integrate art with other subjects in the classroom. Because of its user-friendly architecture, Cinema 4D XL is well suited for introductory courses in 3D animation and modeling. With one of the fastest radiosity rendering engines, advanced students and studio professionals will also value the application. Students at all levels will have the capability of building professional digital portfolios.	Cinema 4D XL: advanced 3D software for educators and studio professionals	NA	2018
O. Makai Smith:James Stewart	Drawing Circle borrows the traditional structure and activities of a life drawing class to explore that structure's potential for digital media. It proposes using the familiar structure of a well-lit model or still life surrounded by easels, within the Studio at SIGGRAPH. We intend to explore a merger of this time-tested convention with 3D modeling in an attempt to provide a setting for the further development of participant's skills, and to promote the Studio as a place for active learning and group investigation.	Drawing circle	NA:NA	2018
Lily Díaz-Kommonen	This presentation describes a concept proposal for a system that will create an archive of the activities of the Studio at SIGGRAPH 2002. The initial parameters that constrain the design process are that the materials of such repository must be displayed and archived during the conference itself, and they must be formatted in such a manner that facilitates their retrieval and replay at a later date. Additionally, in order to reflect the diversity of content present, the archive must be created in a collaborative manner, and with the help of other Studio participants. The amplitude of the desired coverage is yet another factor to be considered. All of these factors must be considered in the light of the quantity of data that can be potentially generated by such endeavor.	Collaborative frameworks: a proposal for an archive in the studio	NA	2018
Simon Allardice	NA	Session details: Web graphics	NA	2018
Michela Ledwidge	This is a presentation of the web3d pipeline and production process used to deliver the multi-lingual interactive animated short film Horses for Courses --- winner of the Web3d Roundup art prize at SIGGRAPH 2001. The film features English, French and Spanish soundtracks, different endings, mouse triggered hotspots, and manual camera controls. Photography, low-polygon modelling, and optimised texture maps, were key to publishing the film as low-bandwidth streaming media in July 2001. The pipeline developed for this project has since been used to achieve higher resolution output for offline applications.	A case study in web3d film-making: Horses for Courses	NA	2018
Márcio O. Costa:Jônatas Manzolli:Dan Sharoni	As an interactive, computerized, network oriented musical composition tool, Rabisco allows users to create stream of MIDI data in real-time by drawing simple sketches using a simple 2-D graphical tablet (pad). Utilizing a client-server architecture, Rabisco allows virtual joint compositions where several musicians interact over the network or the internet, each using a Rabisco client. Current applications includes Distance Learning, Interactive Music Composition, and Interaction with real world devices such as robots.	A distributed interactive composition tool	NA:NA:NA	2018
Bing-Yu Chen:Tomoyuki Nishita	Solid texturing [Peachey 1985; Perlin 1985] has become a well-known computer graphics technology since it was first presented more than fifteen years ago. However, solid texturing still remains problems today, because it consumes too much time and has a very high memory requirement. Although some methods have recently been proposed to solve these problems, almost all of them need the support of hardware accelerators. Hence, these methods could not be applied to all kinds of machines, especially the low-cost ones available over the Internet. Therefore, we present a new method for procedural solid texturing in this paper. Our approach could almost render an object with procedural solid texturing in real-time using only a software solution. The basic idea of this approach is similar to the cache mechanism used for main memory control. Furthermore, to demonstrate that our approach is widely applicable we choose pure Java for it's implementation, since this could not receive any benefit from the hardware and could be executed on the Internet directly.	Adaptive solid texturing for web graphics	NA:NA	2018
Alain Chesnais:Tim Beck:Rudy Ziegler	One of the most labor-intensive tasks in the development of web sites is the creation of derivative images for display of a visual element at multiple sizes or in multiple styles. For instance, a typical online catalog will have at least three different visual representations for each product sold: one thumbnail sized image for visual browsing, one mid sized image for viewing a product description and one large sized image for viewing product detail. The approach outlined in this presentation is to enable the web server to actively generate any derivative image from a high-resolution base image. Derivative images are then cached on the server to speed delivery of subsequent requests for the same derived content.	Architecting a distributed dynamic image server for the web	NA:NA:NA	2018
Alfredo Andia	The "Internet Studio Network" is an initiative designed to create academic relationships among architectural schools to work on semester-studio projects collaborating via the Internet. Past participants included a maximum of 300 architectural students from Miami, Argentina, Chile, Ecuador and Venezuela, who collaborated in semester-long design studios via the Internet and videoconference technology during the Fall, 2001.	Architectural studios online: the "internet studio network"	NA	2018
Guillaume Clary	Banja is the first communal adventure game in 3D flash on the web.	Banja: flash programming system & game design	NA	2018
Lynda Weinman	Color has long played an essential role in successful visual design, but the web has some distinct constraints and considerations that warrant further study by the web graphics and SIGGRAPH community. The objective of this lecture is to establish what special constraints related to color exist, share design principles and techniques to address those constraints, and critique existing examples of web design on the success or lack thereof of color communication.	Color aesthetics for web graphics creation	NA	2018
Anders Henrysson:Mark Ollila	Hybrid representations - that of using image and procedural methods to synthesize images. Procedural methods allow us to describe media (2D images, 3D objects etc.) with very little information and render it photorealistically. Since the procedure is run on the client (for instance a PC or a mobile phone with limited network), it makes sense to adapt the procedure to the properties of the client.	Combining procedural, polygonal, and bitmap representations using XML	NA:NA	2018
Eric Morin	While setting out to create a multimedia authoring tool, the Anark development team set out to use both common design paradigms and evolving technologies. Considering current Web graphics and general multimedia trends, there is a thrust for incorporating multiple types of media into a single visual presentation. Technologies currently exist to bring the concept of pure 3D to Web graphics, but much of the difficulty in creating 3D content comes in the form of multiple and confusing workflows. What started from this thought is an investigation into the technology, use and exposure of 3D technology not only as a core component, but also as a technique to achieve new design metaphors and practices.	Content creation with Anark studio	NA	2018
J. Paul Nykamp	Over the past few years, 3D on the web has been emerging as a viable format. The growth in both computing power and bandwidth has made this technically possible. 3D has many advantages over more traditional frame animations, such as QTVR object movies. These include a greater ability to display objects from any angle, and a greater ability to animate objects and have them react to user interaction. One of the challenges of creating compelling 3D, has been to make it "photorealistic". Recently a new type of 3D has emerged, which is "photographic" 3D. Photography is now being used to create both textures and geometry for 3D objects, and not just in the lab. Diginiche is doing this efficiently and commercially now, using Viewpoint technology.	Creating and implementing photographic 3D on the web	NA	2018
Teresa Lang	AXEL is a complete authoring software to create 3d interactive animation for the Internet. Developed by MindAvenue, AXEL makes it easy for designers to create interactive 3D content without scripting.	Creating interactive 3D web content using AXEL	NA	2018
Akira Wakita:Fumio Matsumoto	CT is a project to reconstruct an existing urban space as a 3D information city on the web. A visitor can browse the city with "building wall browsers" and communicate with other visitors.	CT (city tomography)	NA:NA	2018
Peter Coppin:Karl Fischer:Natalie Koch:Dana Martinelli:W. Ronald McCloskey:Michael Wagner	Telepresence is experiencing a place without physically being there. Telepresence interfaces receive information from robots or sensors in distant, hard to reach places. Scientists use telepresence to explore places that are inaccessible to human beings, such as Mars. However, the technology used on such missions is so complex that the missions themselves are as inaccessible to the public as the extreme environments being studied. Subsequently, design and engineering barriers have kept this vast resource off-limits to America's classrooms despite the Internet's widespread proliferation. Existing public telepresence interfaces either do not scale well to worldwide dissemination or do not fully engage school students.	EventScope: discovering Mars with internet-based virtual environments	NA:NA:NA:NA:NA:NA	2018
Robert Reinhardt	To date, the most common forms of real-time messaging between individuals on the Internet have been text-based, including e-mail and instant messaging. Studies have suggested that Internet users overall spend more hours online with e-mail and instant messaging (IM) than with all other web browsing.1 Macromedia's new technology opens up exciting possibilities for rich media applications that allow Internet users to hear and see each other instantly, using standard microphones and video devices. Although the reach of broadband connectivity is expanding into more businesses and homes, most Internet users have yet to utilize the full potential of two-way, high-speed communication available with broadband connections.	Flash MX live: real-time video and audio delivery in multi-user environments	NA	2018
Fusako Nishikubo:Manabu Tanaka:Shinta Ookino:Hiroshi Ogihara:Kazuhito Ezawa	This web content adopts the web3D technology designed for children, especially for younger children.	Gearation: the web3D content for children	NA:NA:NA:NA:NA	2018
Martin Isenburg	The most popular way of distributing 3D content on the Web is in form of a textual representation of the scene such as VRML and its variants. The advantage of such a description is that it is very author friendly in the sense of being meaningful to the human reader. A scene represented in a textual format can be viewed, understood, and modified with any text editor (see Figure 1). Most importantly, anyone can do this, even without knowledge about the specific software package that generated the 3D content.	Geometry compression for ASCII scenes	NA	2018
Annette Weintraub	This presentation describes an approach to interface and experience design in which fact and fiction are mixed in multiple narrative modes. The Mirror That Changes is a Flash-based Web project that integrates fact and fiction, introspection and information in several simultaneous narratives of animated text, voiceover, ambient sound and moving image. Designed to explore issues of water and sustainability from the multiple perspectives of personal use and global resource, The Mirror merges extensive research on water use with a sensual evocation of water so that information and symbolism mesh within a minimalist envelope.	Integrating multiple narratives: the mirror that changes	NA	2018
Ed Sims:Dan Silverglate	Interactive, life-like characters can enhance motivation, communication, and knowledge retention in computer-based learning [Lester et al. 2000]. However, until now, the development cost and computational requirements for highquality animation have limited its widespread use.	Interactive 3D characters for web-based learning and accessibility	NA:NA	2018
Christian Babski:Stéphane Carion:Patrick Keller:Christophe Guignard	_knowscape builds new networked communities of knowledge. It explores original forms of online [user's] representation and builds new kinds of virtual world that carry on information.	_knowscape, a 3D multi-user experimental web browser	NA:NA:NA:NA	2018
Viswanath Parameswaran	Flash has simplified deployment of content on multiple platforms and browsers. Now with the ability to handle Unicode, it takes web applications into the next level. We will be looking at ways for creating multilingual content and a sample multilingual application using FlashMX.	Multilingual flash applications	NA	2018
Hiroya Tanaka:Masatoshi Arikawa:Ryosuke Shibasaki	Pseudo-3D photo collage is a new technique for creating extensive pseudo-3D scenes on the Web. This technique enables users to create, publish, navigate and share pseudo-3D scenes by easy operations. Our basic idea comes from an artistic representation "photo collage" on 2D canvases, that is, a general method of scanning and arranging original photos. Photo collage is originally 2D and static graphic representation, while our proposed representation is pseudo-3D and interactive one. Our developed system for pseudo-3D photo collage is called STAMP (Spatio-Temporal Associations with Multiple Photographs). STAMP includes basic two components as tools: STAMP-Maker and STAMP-Navigator, for creating and navigating pseudo-3D scenes respectively.	Pseudo-3D photo collage	NA:NA:NA	2018
Hidenori Watanave	"Rhythm Engine" (REg) is a spatial communication tool with "Music" and "visual effects" beyond the space time. "REg" proposes an ideal way of new un-simultaneous communication to the current web-world where mainly "exchanging words" on "real time" is getting more focused.	Rhythm engine	NA	2018
Dean Jackson	Scalable Vector Graphics (SVG) is a language for representing two-dimensional graphics. It was developed by the World Wide Web Consortium (W3C) to be the open standard format for both static and animated vector graphics on Web appliances, from desktop machines to mobile devices. The SVG 1.0 specification, whose authors Include representatives from Adobe, Microsoft, Sun, Kodak, Corel, Macromedia, IBM and Apple, became a W3C Recommendation in September 2001. SVG Is rapidly becoming the open standard of choice for graphics on the Web, and the many SVG implementations already in existence ensure the SVG documents can be viewed on a wide range of platforms.	Scalable vector graphics (SVG): the world wide web consortium's recommendation for high quality web graphics	NA	2018
A. Basu:L Cheng:A. Mistri:D. Wolford	In this report we describe and demonstrate our technology for creating and browsing super high resolution (SHR) & 3D digital content for a variety of applications including museum artifacts and galleries, archeology, anthropology, art design and heritage conservation. SHR 3D images and associated wireframes require large bandwidth to be transmitted in full detail. To address limitations resulting from limited bandwidth, two operations are performed: (i) bandwidth is optimally monitored using a statistical model and (ii) the quality of the 3D objects transmitted are adjusted to best fit the measured bandwidth. Regions of interest (ROIs) specified by users are stored in multiple levels of detail hierarchy. Our approach extends past systems [Martinez et al. 2000] for 2D image browsing, and supports quality of service (QoS) [Vogel et al. 1995] based retrieval. Experimental results demonstrate the feasibility of the proposed approach.	Scalable visualization of super high resolution 3D images for museum archiving	NA:NA:NA:NA	2018
Philipp Hoschka	The Synchronized Multimedia Integration Language (SMIL; pronounced "smile") enables authors to bring interactive audiovisual content to the Web. With SMIL, producing audio-visual content is easy; it does not require learning a programming language and can be done using a simple text editor.	SMIL: an introduction	NA	2018
Kazuyuki Okada:Masa Inakage	"Spoiral" is a multi-user online mystery game, which is implemented with web3D technology. (Fig1) Five players experience an ad-lib drama together, as if they are playing a role in a detective novel, in which the story proceeds in real time with real people.	Spoiral: an online ad-lib mystery	NA:NA	2018
Yasuhiro Santo:Catherine Hu:Mamata Rao	The idea behind the Bridge as a collaborative groupware arose initially with the need to find appropriate tools and environments to facilitate international design collaborations. In particular, this tool needs to support on-line collaborative activities in design learning where language is a barrier and conventional groupware fails to facilitate effective communication. Moreover, designers are inherently dependent on visual elements in all levels and phases of design development, something which current text-based groupware again fails to support.	The bridge: an environment for collaborative design learning	NA:NA:NA	2018
Sandro M. Corsaro	Flash is the future of television animation. It cuts costs, saves time, and empowers the artist. There is a gap in the education and understanding of how to effectively utilize this program for broadcast animation purposes. This lecture will serve to educate this gap from both an artistic and production point of view. Creating broadcast animation has long been limited to having lots of money and time. The average half hour cartoon runs between 300,000 to 1.5 million dollars to produce. The traditional animation process takes over twelve weeks to produce one episode with most of the grunt work being done overseas. Flash is an artistically empowering program that will change the face of animation within the next year. Flash is slowly creeping into broadcast. Some traditionalists have hesitated to accept Flash animation as viable method of production is because the taint from the dot com era. The reason Flash has not been accepted as a viable mainstream production method is because for the most part, the animation that has been produced on the web lacked the quality of television and film. Currently, there are very few animators who have walked on both sides of the fence. But any of them will tell you Flash Animation will become a necessity in the television industry.	The evolution of animation: bedrock revisited	NA	2018
Lily Díaz-Kommonen	Sometimes web sites get built to promote a company or a product. It is also possible that a website is constructed as part of the documentation strategy in a project. In my presentation I want to talk about how a website can be used to foster collaboration among the partners in a research project.	The Raisio archaeology archive: using design to build collaborations	NA	2018
Branden Hall:Samuel Wan	The behavioral and cognitive principles of collaboration are well understood, i.e. how people negotiate common meaning in order to work together. During the implementation of collaborative systems, however, the significance of these principles in humancomputer interaction are often shadowed by the low-level challenges of building networked applications. The REALITY CLUSTER project explores the question "What if building networked applications was easy?" by utilizing new technologies recently introduced by Macromedia and the Flash 6 plugin. The REALITY CLUSTER allows multiple users to manipulate a graphical representation of both real-time and stored information in a common repository. The user interface for REALITY CLUSTER borrows principles found in information visualization literature to show relationships between multiple nodes of information while providing users with both focus and context in navigating the nodes. Each node may consist of either recordings or real-time channels for video, audio, text, and static graphics. Hopefully, the REALITY CLUSTER prototype will open web developers to new perspectives in designing web applications. We believe that these technologies from Macromedia, combined with strong grounding in HCI principles and software engineering, will fulfill the promise of a truly disintermediated network communication.	The reality cluster: realtime multimedia communication with persistence	NA:NA	2018
Alan D. Hudson:Justin Couch:Stephen N. Matsuba	This presentation outlines the development process of the Xj3D browser. Xj3D is an open source API for developing X3D and VRML 97 applications. It is also the sample implementation and test bed for the next generation VRML specification known as Extensible 3D (X3D). Indeed, Xj3D was initiated by the Web3D Consortium to provide input to the X3D authors and the 3D graphics community with input concerning problems and ambiguities with the specification.	The Xj3D browser: community-based 3D software development	NA:NA:NA	2018
Yuichiro Haraguchi:Sakura Toyabe:Masaru Murata	TTT is a Web community tool that promotes new encounters mediated by friends.	TTT: a web community tool mediated by friends	NA:NA:NA	2018
T. J. Jankun-Kelly:Kwan-Liu Ma	The exploration of complex data sets requires interfaces to present and navigate through the visualization of the data. In recent work [Jankun-Kelly and Ma 2001], we produced a visualization exploration spreadsheet to address this issue. The developed application, however, was implemented for off-line use only. For data sets on remote sites, this approach is not appropriate. Thus, a web-based version of the visualization exploration spreadsheet is needed. This abstract discusses the process of transforming the interface from an off-line to an on-line design.	VisSheet redux: redesigning a visualization exploration spreadsheet for the web	NA:NA	2018
Aaron Hertzmann	NA	Session details: Texture synthesis by example	NA	2018
Vivek Kwatra:Arno Schödl:Irfan Essa:Greg Turk:Aaron Bobick	In this paper we introduce a new algorithm for image and video texture synthesis. In our approach, patch regions from a sample image or video are transformed and copied to the output and then stitched together along optimal seams to generate a new (and typically larger) output. In contrast to other techniques, the size of the patch is not chosen a-priori, but instead a graph cut technique is used to determine the optimal patch region for any given offset between the input and output texture. Unlike dynamic programming, our graph cut technique for seam optimization is applicable in any dimension. We specifically explore it in 2D and 3D to perform video texture synthesis in addition to regular image synthesis. We present approximative offset search techniques that work well in conjunction with the presented patch size optimization. We show results for synthesizing regular, random, and natural images and videos. We also demonstrate how this method can be used to interactively merge different images to generate new scenes.	Graphcut textures: image and video synthesis using graph cuts	NA:NA:NA:NA:NA	2018
Michael F. Cohen:Jonathan Shade:Stefan Hiller:Oliver Deussen	We present a simple stochastic system for non-periodically tiling the plane with a small set of Wang Tiles. The tiles may be filled with texture, patterns, or geometry that when assembled create a continuous representation. The primary advantage of using Wang Tiles is that once the tiles are filled, large expanses of non-periodic texture (or patterns or geometry) can be created as needed very efficiently at runtime.Wang Tiles are squares in which each edge is assigned a color. A valid tiling requires all shared edges between tiles to have matching colors. We present a new stochastic algorithm to non-periodically tile the plane with a small set of Wang Tiles at runtime.Furthermore, we present new methods to fill the tiles with 2D texture, 2D Poisson distributions, or 3D geometry to efficiently create at runtime as much non-periodic texture (or distributions, or geometry) as needed. We leverage previous texture synthesis work and adapt it to fill Wang Tiles. We demonstrate how to fill individual tiles with Poisson distributions that maintain their statistical properties when combined. These are used to generate a large arrangement of plants or other objects on a terrain. We show how such environments can be rendered efficiently by pre-lighting the individual Wang Tiles containing the geometry.We also extend the definition of Wang Tiles to include a coding of the tile corners to allow discrete objects to overlap more than one edge. The larger set of tiles provides increased degrees of freedom.	Wang Tiles for image and texture generation	NA:NA:NA:NA	2018
Jingdan Zhang:Kun Zhou:Luiz Velho:Baining Guo:Heung-Yeung Shum	We present an approach for decorating surfaces with progressively-variant textures. Unlike a homogeneous texture, a progressively-variant texture can model local texture variations, including the scale, orientation, color, and shape variations of texture elements. We describe techniques for modeling progressively-variant textures in 2D as well as for synthesizing them over surfaces. For 2D texture modeling, our feature-based warping technique allows the user to control the shape variations of texture elements, making it possible to capture complex texture variations such as those seen in animal coat patterns. In addition, our feature-based blending technique can create a smooth transition between two given homogeneous textures, with progressive changes of both shapes and colors of texture elements. For synthesizing textures over surfaces, the biggest challenge is that the synthesized texture elements tend to break apart as they progressively vary. To address this issue, we propose an algorithm based on texton masks, which mark most prominent texture elements in the 2D texture sample. By leveraging the power of texton masks, our algorithm can maintain the integrity of the synthesized texture elements on the target surface.	Synthesis of progressively-variant textures on arbitrary surfaces	NA:NA:NA:NA:NA	2018
Iddo Drori:Daniel Cohen-Or:Hezy Yeshurun	We present a new method for completing missing parts caused by the removal of foreground or background elements from an image. Our goal is to synthesize a complete, visually plausible and coherent image. The visible parts of the image serve as a training set to infer the unknown parts. Our method iteratively approximates the unknown regions and composites adaptive image fragments into the image. Values of an inverse matte are used to compute a confidence map and a level set that direct an incremental traversal within the unknown area from high to low confidence. In each step, guided by a fast smooth approximation, an image fragment is selected from the most similar and frequent examples. As the selected fragments are composited, their likelihood increases along with the mean confidence of the image, until reaching a complete image. We demonstrate our method by completion of photographs and paintings.	Fragment-based image completion	NA:NA:NA	2018
Stephen Marschner	NA	Session details: Images, video, and texture	NA	2018
Patrick Pérez:Michel Gangnet:Andrew Blake	Using generic interpolation machinery based on solving Poisson equations, a variety of novel tools are introduced for seamless editing of image regions. The first set of tools permits the seamless importation of both opaque and transparent source image regions into a destination region. The second set is based on similar mathematical ideas and allows the user to modify the appearance of the image seamlessly, within a selected region. These changes can be arranged to affect the texture, the illumination, and the color of objects lying in the region, or to make tileable a rectangular selection.	Poisson image editing	NA:NA:NA	2018
Sing Bing Kang:Matthew Uyttendaele:Simon Winder:Richard Szeliski	Typical video footage captured using an off-the-shelf camcorder suffers from limited dynamic range. This paper describes our approach to generate high dynamic range (HDR) video from an image sequence of a dynamic scene captured while rapidly varying the exposure of each frame. Our approach consists of three parts: automatic exposure control during capture, HDR stitching across neighboring frames, and tonemapping for viewing. HDR stitching requires accurately registering neighboring frames and choosing appropriate pixels for computing the radiance map. We show examples for a variety of dynamic scenes. We also show how we can compensate for scene and camera movement when creating an HDR still from a series of bracketed still photographs.	High dynamic range video	NA:NA:NA:NA	2018
Vladislav Kraevoy:Alla Sheffer:Craig Gotsman	Texture mapping enhances the visual realism of 3D models by adding fine details. To achieve the best results, it is often necessary to force a correspondence between some of the details of the texture and the features of the model.The most common method for mapping texture onto 3D meshes is to use a planar parameterization of the mesh. This, however, does not reflect any special correspondence between the mesh geometry and the texture. The Matchmaker algorithm presented here forces user-defined feature correspondence for planar parameterization of meshes. This is achieved by adding positional constraints to the planar parameterization. Matchmaker allows users to introduce scores of constraints while maintaining a valid one-to-one mapping between the embedding and the 3D surface. Matchmaker's constraint mechanism can be used for other applications requiring parameterization besides texture mapping, such as morphing and remeshing.Matchmaker begins with an unconstrained planar embedding of the 3D mesh generated by conventional methods. It moves the constrained vertices to the required positions by matching a triangulation of these positions to a triangulation of the planar mesh formed by paths between constrained vertices. The matching triangulations are used to generate a new parameterization that satisfies the constraints while minimizing the deviation from the original 3D geometry.	Matchmaker: constructing constrained texture maps	NA:NA:NA	2018
Lifeng Wang:Xi Wang:Xin Tong:Stephen Lin:Shimin Hu:Baining Guo:Heung-Yeung Shum	Significant visual effects arise from surface mesostructure, such as fine-scale shadowing, occlusion and silhouettes. To efficiently render its detailed appearance, we introduce a technique called view-dependent displacement mapping (VDM) that models surface displacements along the viewing direction. Unlike traditional displacement mapping, VDM allows for efficient rendering of self-shadows, occlusions and silhouettes without increasing the complexity of the underlying surface mesh. VDM is based on per-pixel processing, and with hardware acceleration it can render mesostructure with rich visual appearance in real time.	View-dependent displacement mapping	NA:NA:NA:NA:NA:NA:NA	2018
Michael Garland	NA	Session details: Parameterization	NA	2018
Emil Praun:Hugues Hoppe	The traditional approach for parametrizing a surface involves cutting it into charts and mapping these piecewise onto a planar domain. We introduce a robust technique for directly parametrizing a genus-zero surface onto a spherical domain. A key ingredient for making such a parametrization practical is the minimization of a stretch-based measure, to reduce scale-distortion and thereby prevent undersampling. Our second contribution is a scheme for sampling the spherical domain using uniformly subdivided polyhedral domains, namely the tetrahedron, octahedron, and cube. We show that these particular semi-regular samplings can be conveniently represented as completely regular 2D grids, i.e. geometry images. Moreover, these images have simple boundary extension rules that aid many processing operations. Applications include geometry remeshing, level-of-detail, morphing, compression, and smooth surface subdivision.	Spherical parametrization and remeshing	NA:NA	2018
Andrei Khodakovsky:Nathan Litke:Peter Schröder	Good parameterizations are of central importance in many digital geometry processing tasks. Typically the behavior of such processing algorithms is related to the smoothness of the parameterization and how much distortion it contains. Since a parameterization maps a bounded region of the plane to the surface, a parameterization for a surface which is not homeomorphic to a disc must be made up of multiple pieces. We present a novel parameterization algorithm for arbitrary topology surface meshes which computes a globally smooth parameterization with low distortion. We optimize the patch layout subject to criteria such as shape quality and metric distortion, which are used to steer a mesh simplification approach for base complex construction. Global smoothness is achieved through simultaneous relaxation over all patches, with suitable transition functions between patches incorporated into the relaxation procedure. We demonstrate the quality of our parameterizations through numerical evaluation of distortion measures and the excellent rate distortion performance of semi-regular remeshes produced with these parameterizations. The numerical algorithms required to compute the parameterizations are robust and run on the order of minutes even for large meshes.	Globally smooth parameterizations with low distortion	NA:NA:NA	2018
Craig Gotsman:Xianfeng Gu:Alla Sheffer	Parameterization of 3D mesh data is important for many graphics applications, in particular for texture mapping, remeshing and morphing. Closed manifold genus-0 meshes are topologically equivalent to a sphere, hence this is the natural parameter domain for them. Parameterizing a triangle mesh onto the sphere means assigning a 3D position on the unit sphere to each of the mesh vertices, such that the spherical triangles induced by the mesh connectivity are not too distorted and do not overlap. Satisfying the non-overlapping requirement is the most difficult and critical component of this process. We describe a generalization of the method of barycentric coordinates for planar parameterization which solves the spherical parameterization problem, prove its correctness by establishing a connection to spectral graph theory and show how to compute these parameterizations.	Fundamentals of spherical parameterization for 3D meshes	NA:NA:NA	2018
Bruno Lévy	Shape optimization and surface fairing for polygon meshes have been active research areas for the last few years. Existing approaches either require the border of the surface to be fixed, or are only applicable to closed surfaces. In this paper, we propose a new approach, that computes natural boundaries. This makes it possible not only to smooth an existing geometry, but also to extrapolate its shape beyond the existing border. Our approach is based on a global parameterization of the surface and on a minimization of the squared curvatures, discretized on the edges of the surface. The so-constructed surface is an approximation of a minimal energy surface (MES). Using a global parameterization makes it possible to completely decouple the outer fairness (surface smoothness) from the inner fairness (mesh quality). In addition, the parameter space provides the user with a new means of controlling the shape of the surface. When used as a geometry filter, our approach computes a smoothed mesh that is discrete conformal to the original one. This allows smoothing textured meshes without introducing distortions.	Dual domain extrapolation	NA	2018
Henrik Wann Jensen	NA	Session details: Precomputed radiance transfer	NA	2018
Peter-Pike Sloan:Xinguo Liu:Heung-Yeung Shum:John Snyder	Radiance transfer represents how generic source lighting is shadowed and scattered by an object to produce view-dependent appearance. We generalize by rendering transfer at two scales. A macro-scale is coarsely sampled over an object's surface, providing global effects like shadows cast from an arm onto a body. A meso-scale is finely sampled over a small patch to provide local texture. Low-order (25D) spherical harmonics represent low-frequency lighting dependence for both scales. To render, a coefficient vector representing distant source lighting is first transformed at the macro-scale by a matrix at each vertex of a coarse mesh. The resulting vectors represent a spatially-varying hemisphere of lighting incident to the meso-scale. A 4D function, called a radiance transfer texture (RTT), then specifies the surface's meso-scale response to each lighting basis component, as a function of a spatial index and a view direction. Finally, a 25D dot product of the macro-scale result vector with the vector looked up from the RTT performs the correct shading integral. We use an id map to place RTT samples from a small patch over the entire object; only two scalars are specified at high spatial resolution. Results show that bi-scale decomposition makes preprocessing practical and efficiently renders self-shadowing and interreflection effects from dynamic, low-frequency light sources at both scales.	Bi-scale radiance transfer	NA:NA:NA:NA	2018
Ren Ng:Ravi Ramamoorthi:Pat Hanrahan	We present a method, based on pre-computed light transport, for real-time rendering of objects under all-frequency, time-varying illumination represented as a high-resolution environment map. Current techniques are limited to small area lights, with sharp shadows, or large low-frequency lights, with very soft shadows. Our main contribution is to approximate the environment map in a wavelet basis, keeping only the largest terms (this is known as a non-linear approximation). We obtain further compression by encoding the light transport matrix sparsely but accurately in the same basis. Rendering is performed by multiplying a sparse light vector by a sparse transport matrix, which is very fast. For accurate rendering, using non-linear wavelets is an order of magnitude faster than using linear spherical harmonics, the current best technique.	All-frequency shadows using non-linear wavelet lighting approximation	NA:NA:NA	2018
Peter-Pike Sloan:Jesse Hall:John Hart:John Snyder	We compress storage and accelerate performance of precomputed radiance transfer (PRT), which captures the way an object shadows, scatters, and reflects light. PRT records over many surface points a transfer matrix. At run-time, this matrix transforms a vector of spherical harmonic coefficients representing distant, low-frequency source lighting into exiting radiance. Per-point transfer matrices form a high-dimensional surface signal that we compress using clustered principal component analysis (CPCA), which partitions many samples into fewer clusters each approximating the signal as an affine subspace. CPCA thus reduces the high-dimensional transfer signal to a low-dimensional set of per-point weights on a per-cluster set of representative matrices. Rather than computing a weighted sum of representatives and applying this result to the lighting, we apply the representatives to the lighting per-cluster (on the CPU) and weight these results per-point (on the GPU). Since the output of the matrix is lower-dimensional than the matrix itself, this reduces computation. We also increase the accuracy of encoded radiance functions with a new least-squares optimal projection of spherical harmonics onto the hemisphere. We describe an implementation on graphics hardware that performs real-time rendering of glossy objects with dynamic self-shadowing and interreflection without fixing the view or light as in previous work. Our approach also allows significantly increased lighting frequency when rendering diffuse objects and includes subsurface scattering.	Clustered principal components for precomputed radiance transfer	NA:NA:NA:NA	2018
Michiel van de Panne	NA	Session details: Character animation	NA	2018
Tae-hoon Kim:Sang Il Park:Sung Yong Shin	Real-time animation of human-like characters is an active research area in computer graphics. The conventional approaches have, however, hardly dealt with the rhythmic patterns of motions, which are essential in handling rhythmic motions such as dancing and locomotive motions. In this paper, we present a novel scheme for synthesizing a new motion from unlabelled example motions while preserving their rhythmic pattern. Our scheme first captures the motion beats from the example motions to extract the basic movements and their transitions. Based on those data, our scheme then constructs a movement transition graph that represents the example motions. Given an input sound signal, our scheme finally synthesizes a novel motion in an on-line manner while traversing the motion transition graph, which is synchronized with the input sound signal and also satisfies kinematic constraints given explicitly and implicitly. Through experiments, we have demonstrated that our scheme can effectively produce a variety of rhythmic motions.	Rhythmic-motion synthesis based on motion-beat analysis	NA:NA:NA	2018
Okan Arikan:David A. Forsyth:James F. O'Brien	This paper describes a framework that allows a user to synthesize human motion while retaining control of its qualitative properties. The user paints a timeline with annotations --- like walk, run or jump --- from a vocabulary which is freely chosen by the user. The system then assembles frames from a motion database so that the final motion performs the specified actions at specified times. The motion can also be forced to pass through particular configurations at particular times, and to go to a particular position and orientation. Annotations can be painted positively (for example, must run), negatively (for example, may not run backwards) or as a don't-care. The system uses a novel search method, based around dynamic programming at several scales, to obtain a solution efficiently so that authoring is interactive. Our results demonstrate that the method can generate smooth, natural-looking motion.The annotation vocabulary can be chosen to fit the application, and allows specification of composite motions (run and jump simultaneously, for example). The process requires a collection of motion data that has been annotated with the chosen vocabulary. This paper also describes an effective tool, based around repeated use of support vector machines, that allows a user to annotate a large collection of motions quickly and easily so that they may be used with the synthesis algorithm.	Motion synthesis from annotations	NA:NA:NA	2018
Mira Dontcheva:Gary Yngve:Zoran Popović	We introduce an acting-based animation system for creating and editing character animation at interactive speeds. Our system requires minimal training, typically under an hour, and is well suited for rapidly prototyping and creating expressive motion. A real-time motion-capture framework records the user's motions for simultaneous analysis and playback on a large screen. The animator's real-world, expressive motions are mapped into the character's virtual world. Visual feedback maintains a tight coupling between the animator and character. Complex motion is created by layering multiple passes of acting. We also introduce a novel motion-editing technique, which derives implicit relationships between the animator and character. The animator mimics some aspect of the character motion, and the system infers the association between features of the animator's motion and those of the character. The animator modifies the mimic by acting again, and the system maps the changes onto the character. We demonstrate our system with several examples and present the results from informal user studies with expert and novice animators.	Layered acting for character animation	NA:NA:NA	2018
Anthony C. Fang:Nancy S. Pollard	Optimization is a promising way to generate new animations from a minimal amount of input data. Physically based optimization techniques, however, are difficult to scale to complex animated characters, in part because evaluating and differentiating physical quantities becomes prohibitively slow. Traditional approaches often require optimizing or constraining parameters involving joint torques; obtaining first derivatives for these parameters is generally an O(D2) process, where D is the number of degrees of freedom of the character. In this paper, we describe a set of objective functions and constraints that lead to linear time analytical first derivatives. The surprising finding is that this set includes constraints on physical validity, such as ground contact constraints. Considering only constraints and objective functions that lead to linear time first derivatives results in fast per-iteration computation times and an optimization problem that appears to scale well to more complex characters. We show that qualities such as squash-and-stretch that are expected from physically based optimization result from our approach. Our animation system is particularly useful for synthesizing highly dynamic motions, and we show examples of swinging and leaping motions for characters having from 7 to 22 degrees of freedom.	Efficient synthesis of physically valid human motion	NA:NA	2018
Jack Tumblin	NA	Session details: Visualization and printing	NA	2018
Roger D. Hersch:Fabien Collaud:Patrick Emmel	By combining a metallic ink and standard inks, one may create printed images having a dynamic appearance: an image viewed under specular reflection may be considerably different from the same image viewed under non-specular reflection. Patterns which are either dark or hidden become highlighted under specular reflection, yielding interesting visual effects. To create such images, one needs to be able to reproduce at non-specular reflection angles the same colors, by standard inks alone or in combination with a metallic ink. Accurate color prediction models need to be established which model the underlying physical phenomena in a consistent manner. To meet this challenge, we propose two models, one for predicting the reflection spectra of standard inks on coated paper and one for predicting the reflection spectra of a combination of standard inks and a metallic ink. They are enhancements of the classical Clapper-Yule model which models optical dot gain of halftone prints by taking into account lateral scattering within the paper bulk and multiple internal reflections. The models we propose also take into account physical dot gain and ink spreading for standard inks as well as the low reflectance of metallic inks at non-specular reflection angles and the poor adherence of standard inks printed on top of a metallic ink (trapping effect). These models open the way towards color separation of images to be reproduced by combining a metallic ink and standard inks. Several designs printed on an offset press demonstrate their applicability and their benefits for high-end design and security applications.	Reproducing color images with embedded metallic patterns	NA:NA:NA	2018
Bingfeng Zhou:Xifeng Fang	In this paper, we describe the use of threshold modulation to remove the visual artifacts contained in the variable-coefficient error-diffusion algorithm. To obtain a suitable parameter set for the threshold modulation, a cost function used for the search of optimal parameters is designed. An optimal diffusion parameter set, as well as the corresponding threshold modulation strength values, is thus obtained. Experiments over this new set of parameters show that, compared with the original variable-coefficient error-diffusion algorithm, threshold modulation can remove visual anomalies more effectively. The result of the new algorithm is an artifact-free halftoning in the full range of intensities. Fourier analysis of the experimental results further support this conclusion.	Improving mid-tone quality of variable-coefficient error diffusion using threshold modulation	NA:NA	2018
Yiying Tong:Santiago Lombeyda:Anil N. Hirani:Mathieu Desbrun	While 2D and 3D vector fields are ubiquitous in computational sciences, their use in graphics is often limited to regular grids, where computations are easily handled through finite-difference methods. In this paper, we propose a set of simple and accurate tools for the analysis of 3D discrete vector fields on arbitrary tetrahedral grids. We introduce a variational, multiscale decomposition of vector fields into three intuitive components: a divergence-free part, a curl-free part, and a harmonic part. We show how our discrete approach matches its well-known smooth analog, called the Helmotz-Hodge decomposition, and that the resulting computational tools have very intuitive geometric interpretation. We demonstrate the versatility of these tools in a series of applications, ranging from data visualization to fluid and deformable object simulation.	Discrete multiscale vector field decomposition	NA:NA:NA:NA	2018
Tamara Munzner:François Guimbretière:Serdar Tasiran:Li Zhang:Yunhong Zhou	Structural comparison of large trees is a difficult task that is only partially supported by current visualization techniques, which are mainly designed for browsing. We present TreeJuxtaposer, a system designed to support the comparison task for large trees of several hundred thousand nodes. We introduce the idea of "guaranteed visibility", where highlighted areas are treated as landmarks that must remain visually apparent at all times. We propose a new methodology for detailed structural comparison between two trees and provide a new nearly-linear algorithm for computing the best corresponding node from one tree to another. In addition, we present a new rectilinear Focus+Context technique for navigation that is well suited to the dynamic linking of side-by-side views while guaranteeing landmark visibility and constant frame rates. These three contributions result in a system delivering a fluid exploration experience that scales both in the size of the dataset and the number of pixels in the display. We have based the design decisions for our system on the needs of a target audience of biologists who must understand the structural details of many phylogenetic, or evolutionary, trees. Our tool is also useful in many other application domains where tree comparison is needed, ranging from network management to call graph optimization to genealogy.	TreeJuxtaposer: scalable tree comparison using Focus+Context with guaranteed visibility	NA:NA:NA:NA:NA	2018
Emil Praun	NA	Session details: Surfaces	NA	2018
Yutaka Ohtake:Alexander Belyaev:Marc Alexa:Greg Turk:Hans-Peter Seidel	We present a new shape representation, the multi-level partition of unity implicit surface, that allows us to construct surface models from very large sets of points. There are three key ingredients to our approach: 1) piecewise quadratic functions that capture the local shape of the surface, 2) weighting functions (the partitions of unity) that blend together these local shape functions, and 3) an octree subdivision method that adapts to variations in the complexity of the local shape.Our approach gives us considerable flexibility in the choice of local shape functions, and in particular we can accurately represent sharp features such as edges and corners by selecting appropriate shape functions. An error-controlled subdivision leads to an adaptive approximation whose time and memory consumption depends on the required accuracy. Due to the separation of local approximation and local blending, the representation is not global and can be created and evaluated rapidly. Because our surfaces are described using implicit functions, operations such as shape blending, offsets, deformations and CSG are simple to perform.	Multi-level partition of unity implicits	NA:NA:NA:NA:NA	2018
Haeyoung Lee:Mathieu Desbrun:Peter Schröder	We present a progressive encoding technique specifically designed for complex isosurfaces. It achieves better rate distortion performance than all standard mesh coders, and even improves on all previous single rate isosurface coders. Our novel algorithm handles isosurfaces with or without sharp features, and deals gracefully with high topologic and geometric complexity. The inside/outside function of the volume data is progressively transmitted through the use of an adaptive octree, while a local frame based encoding is used for the fine level placement of surface samples. Local patterns in topology and local smoothness in geometry are exploited by context-based arithmetic encoding, allowing us to achieve an average of 6.10 bits per vertex (b/v) at very low distortion. Of this rate only 0.65 b/v are dedicated to connectivity data: this improves by 24% over the best previous single rate isosurface encoder.	Progressive encoding of complex isosurfaces	NA:NA:NA	2018
Thomas W. Sederberg:Jianmin Zheng:Almaz Bakenov:Ahmad Nasri	This paper presents a generalization of non-uniform B-spline surfaces called T-splines. T-spline control grids permit T-junctions, so lines of control points need not traverse the entire control grid. T-splines support many valuable operations within a consistent framework, such as local refinement, and the merging of several B-spline surfaces that have different knot vectors into a single gap-free model. The paper focuses on T-splines of degree three, which are C2 (in the absence of multiple knots). T-NURCCs (Non-Uniform Rational Catmull-Clark Surfaces with T-junctions) are a superset of both T-splines and Catmull-Clark surfaces. Thus, a modeling program for T-NURCCs can handle any NURBS or Catmull-Clark model as special cases. T-NURCCs enable true local refinement of a Catmull-Clark-type control grid: individual control points can be inserted only where they are needed to provide additional control, or to create a smoother tessellation, and such insertions do not alter the limit surface. T-NURCCs use stationary refinement rules and are C2 except at extraordinary points and features.	T-splines and T-NURCCs	NA:NA:NA:NA	2018
Pierre Alliez:David Cohen-Steiner:Olivier Devillers:Bruno Lévy:Mathieu Desbrun	In this paper, we propose a novel polygonal remeshing technique that exploits a key aspect of surfaces: the intrinsic anisotropy of natural or man-made geometry. In particular, we use curvature directions to drive the remeshing process, mimicking the lines that artists themselves would use when creating 3D models from scratch. After extracting and smoothing the curvature tensor field of an input genus-0 surface patch, lines of minimum and maximum curvatures are used to determine appropriate edges for the remeshed version in anisotropic regions, while spherical regions are simply point sampled since there is no natural direction of symmetry locally. As a result our technique generates polygon meshes mainly composed of quads in anisotropic regions, and of triangles in spherical regions. Our approach provides the flexibility to produce meshes ranging from isotropic to anisotropic, from coarse to dense, and from uniform to curvature adapted.	Anisotropic polygonal remeshing	NA:NA:NA:NA:NA	2018
Kavita Bala	NA	Session details: Shadows	NA	2018
Yung-Yu Chuang:Dan B Goldman:Brian Curless:David H. Salesin:Richard Szeliski	In this paper, we describe a method for extracting shadows from one natural scene and inserting them into another. We develop physically-based shadow matting and compositing equations and use these to pull a shadow matte from a source scene in which the shadow is cast onto an arbitrary planar background. We then acquire the photometric and geometric properties of the target scene by sweeping oriented linear shadows (cast by a straight object) across it. From these shadow scans, we can construct a shadow displacement map without requiring camera or light source calibration. This map can then be used to deform the original shadow matte. We demonstrate our approach for both indoor scenes with controlled lighting and for outdoor scenes using natural lighting.	Shadow matting and compositing	NA:NA:NA:NA:NA	2018
Naga K. Govindaraju:Brandon Lloyd:Sung-Eui Yoon:Avneesh Sud:Dinesh Manocha	We present a new algorithm for interactive generation of hard-edged, umbral shadows in complex environments with a moving light source. Our algorithm uses a hybrid approach that combines the image quality of object-precision methods with the efficiencies of image-precision techniques. We present an algorithm for computing a compact potentially visible set (PVS) using levels-of-detail (LODs) and visibility culling. We use the PVSs computed from both the eye and the light in a novel cross-culling algorithm that identifies a reduced set of potential shadow-casters and shadow-receivers. Finally, we use a combination of shadow-polygons and shadow maps to generate shadows. We also present techniques for LOD-selection to minimize possible artifacts arising from the use of LODs. Our algorithm can generate sharp shadow edges and reduces the aliasing in pure shadow map approaches. We have implemented the algorithm on a three-PC system with NVIDIA GeForce 4 cards. We achieve 7--25 frames per second in three complex environments composed of millions of triangles.	Interactive shadow generation in complex environments	NA:NA:NA:NA:NA	2018
Ulf Assarsson:Tomas Akenine-Möller	Most previous soft shadow algorithms have either suffered from aliasing, been too slow, or could only use a limited set of shadow casters and/or receivers. Therefore, we present a strengthened soft shadow volume algorithm that deals with these problems. Our critical improvements include robust penumbra wedge construction, geometry-based visibility computation, and also simplified computation through a four-dimensional texture lookup. This enables us to implement the algorithm using programmable graphics hardware, and it results in images that most often are indistinguishable from images created as the average of 1024 hard shadow images. Furthermore, our algorithm can use both arbitrary shadow casters and receivers. Also, one version of our algorithm completely avoids sampling artifacts which is rare for soft shadow algorithms. As a bonus, the four-dimensional texture lookup allows for small textured light sources, and, even video textures can be used as light sources. Our algorithm has been implemented in pure software, and also using the GeForce FX emulator with pixel shaders. Our software implementation renders soft shadows at 0.5--5 frames per second for the images in this paper. With actual hardware, we expect that our algorithm will render soft shadows in real time. An important performance measure is bandwidth usage. For the same image quality, an algorithm using the accumulated hard shadow images uses almost two orders of magnitude more bandwidth than our algorithm.	A geometry-based soft shadow volume algorithm using graphics hardware	NA:NA	2018
Pradeep Sen:Mike Cammarano:Pat Hanrahan	The most popular techniques for interactive rendering of hard shadows are shadow maps and shadow volumes. Shadow maps work well in regions that are completely in light or in shadow but result in objectionable artifacts near shadow boundaries. In contrast, shadow volumes generate precise shadow boundaries but require high fill rates. In this paper, we propose the method of silhouette maps, in which a shadow depth map is augmented by storing the location of points on the geometric silhouette. This allows the shader to construct a piecewise linear approximation to the true shadow silhouette, improving the visual quality over the piecewise constant approximation of conventional shadow maps. We demonstrate an implementation of our approach running on programmable graphics hardware in real-time.	Shadow silhouette maps	NA:NA:NA	2018
Holly Rushmeier	NA	Session details: Perception and manipulation	NA	2018
Carol O'Sullivan:John Dingliana:Thanh Giang:Mary K. Kaiser	For many systems that produce physically based animations, plausibility rather than accuracy is acceptable. We consider the problem of evaluating the visual quality of animations in which physical parameters have been distorted or degraded, either unavoidably due to real-time frame-rate requirements, or intentionally for aesthetic reasons. To date, no generic means of evaluating or predicting the fidelity, either physical or visual, of the dynamic events occurring in an animation exists. As a first step towards providing such a metric, we present a set of psychophysical experiments that established some thresholds for human sensitivity to dynamic anomalies, including angular, momentum and spatio-temporal distortions applied to simple animations depicting the elastic collision of two rigid objects. In addition to finding significant acceptance thresholds for these distortions under varying conditions, we identified some interesting biases that indicate non-symmetric responses to these distortions (e.g., expansion of the angle between post-collision trajectories was preferred to contraction and increases in velocity were preferred to decreases). Based on these results, we derived a set of probability functions that can be used to evaluate the visual fidelity of a physically based simulation. To illustrate how our results could be used, two simple case studies of simulation levels of detail and constrained dynamics are presented.	Evaluating the visual fidelity of physically based animations	NA:NA:NA:NA	2018
Paul S. A. Reitsma:Nancy S. Pollard	Motion capture data and techniques for blending, editing, and sequencing that data can produce rich, realistic character animation; however, the output of these motion processing techniques sometimes appears unnatural. For example, the motion may violate physical laws or reflect unreasonable forces from the character or the environment. While problems such as these can be fixed, doing so is not yet feasible in real time environments. We are interested in developing ways to estimate perceived error in animated human motion so that the output quality of motion processing techniques can be better controlled to meet user goals.This paper presents results of a study of user sensitivity to errors in animated human motion. Errors were systematically added to human jumping motion, and the ability of subjects to detect these errors was measured. We found that users were able to detect motion with errors, and noted some interesting trends: errors in horizontal velocity were easier to detect than errors in vertical velocity, and added accelerations were easier to detect than added decelerations. On the basis of our results, we propose a perceptually based metric for measuring errors in ballistic human motion.	Perceptual metrics for character animation: sensitivity to errors in ballistic motion	NA:NA	2018
Miguel A. Otaduy:Ming C. Lin	We introduce a novel "sensation preserving" simplification algorithm for faster collision queries between two polyhedral objects in haptic rendering. Given a polyhedral model, we construct a multiresolution hierarchy using " filtered edge collapse", subject to constraints imposed by collision detection. The resulting hierarchy is then used to compute fast contact response for haptic display. The computation model is inspired by human tactual perception of contact information. We have successfully applied and demonstrated the algorithm on a time-critical collision query framework for haptically displaying complex object-object interaction. Compared to existing exact contact query algorithms, we observe noticeable performance improvement in update rates with little degradation in the haptic perception of contacts.	Sensation preserving simplification for haptic rendering	NA:NA	2018
Irfan Essa	NA	Session details: Human bodies	NA	2018
Kolja Kähler:Jörg Haber:Hans-Peter Seidel	Facial reconstruction for postmortem identification of humans from their skeletal remains is a challenging and fascinating part of forensic art. The former look of a face can be approximated by predicting and modeling the layers of tissue on the skull. This work is as of today carried out solely by physical sculpting with clay, where experienced artists invest up to hundreds of hours to craft a reconstructed face model. Remarkably, one of the most popular tissue reconstruction methods bears many resemblances with surface fitting techniques used in computer graphics, thus suggesting the possibility of a transfer of the manual approach to the computer. In this paper, we present a facial reconstruction approach that fits an anatomy-based virtual head model, incorporating skin and muscles, to a scanned skull using statistical data on skull / tissue relationships. The approach has many advantages over the traditional process: a reconstruction can be completed in about an hour from acquired skull data; also, variations such as a slender or a more obese build of the modeled individual are easily created. Last not least, by matching not only skin geometry but also virtual muscle layers, an animatable head model is generated that can be used to form facial expressions beyond the neutral face typically used in physical reconstructions.	Reanimating the dead: reconstruction of expressive faces from skull data	NA:NA:NA	2018
Alex Mohr:Michael Gleicher	Good character animation requires convincing skin deformations including subtleties and details like muscle bulges. Such effects are typically created in commercial animation packages which provide very general and powerful tools. While these systems are convenient and flexible for artists, the generality often leads to characters that are slow to compute or that require a substantial amount of memory and thus cannot be used in interactive systems. Instead, interactive systems restrict artists to a specific character deformation model which is fast and memory efficient but is notoriously difficult to author and can suffer from many deformation artifacts. This paper presents an automated framework that allows character artists to use the full complement of tools in high-end systems to create characters for interactive systems. Our method starts with an arbitrarily rigged character in an animation system. A set of examples is exported, consisting of skeleton configurations paired with the deformed geometry as static meshes. Using these examples, we fit the parameters of a deformation model that best approximates the original data yet remains fast to compute and compact in memory.	Building efficient, accurate character skins from examples	NA:NA	2018
Joel Carranza:Christian Theobalt:Marcus A. Magnor:Hans-Peter Seidel	In free-viewpoint video, the viewer can interactively choose his viewpoint in 3-D space to observe the action of a dynamic real-world scene from arbitrary perspectives. The human body and its motion plays a central role in most visual media and its structure can be exploited for robust motion estimation and efficient visualization. This paper describes a system that uses multi-view synchronized video footage of an actor's performance to estimate motion parameters and to interactively re-render the actor's appearance from any viewpoint.The actor's silhouettes are extracted from synchronized video frames via background segmentation and then used to determine a sequence of poses for a 3D human body model. By employing multi-view texturing during rendering, time-dependent changes in the body surface are reproduced in high detail. The motion capture subsystem runs offline, is non-intrusive, yields robust motion parameter estimates, and can cope with a broad range of motion. The rendering subsystem runs at real-time frame rates using ubiquous graphics hardware, yielding a highly naturalistic impression of the actor. The actor can be placed in virtual environments to create composite dynamic scenes. Free-viewpoint video allows the creation of camera fly-throughs or viewing the action interactively from arbitrary perspectives.	Free-viewpoint video of human actors	NA:NA:NA:NA	2018
Peter Sand:Leonard McMillan:Jovan Popović	We describe a method for the acquisition of deformable human geometry from silhouettes. Our technique uses a commercial tracking system to determine the motion of the skeleton, then estimates geometry for each bone using constraints provided by the silhouettes from one or more cameras. These silhouettes do not give a complete characterization of the geometry for a particular point in time, but when the subject moves, many observations of the same local geometries allow the construction of a complete model. Our reconstruction algorithm provides a simple mechanism for solving the problems of view aggregation, occlusion handling, hole filling, noise removal, and deformation modeling. The resulting model is parameterized to synthesize geometry for new poses of the skeleton. We demonstrate this capability by rendering the geometry for motion sequences that were not included in the original datasets.	Continuous capture of skin deformation	NA:NA:NA	2018
Brett Allen:Brian Curless:Zoran Popović	We develop a novel method for fitting high-resolution template meshes to detailed human body range scans with sparse 3D markers. We formulate an optimization problem in which the degrees of freedom are an affine transformation at each template vertex. The objective function is a weighted combination of three measures: proximity of transformed vertices to the range data, similarity between neighboring transformations, and proximity of sparse markers at corresponding locations on the template and target surface. We solve for the transformations with a non-linear optimizer, run at two resolutions to speed convergence. We demonstrate reconstruction and consistent parameterization of 250 human body models. With this parameterized set, we explore a variety of applications for human body modeling, including: morphing, texture transfer, statistical analysis of shape, model fitting from sparse markers, feature analysis to modify multiple correlated parameters (such as the weight and height of an individual), and transfer of surface detail and animation controls from a template to fitted models.	The space of human body shapes: reconstruction and parameterization from range scans	NA:NA:NA	2018
Brian Curless	NA	Session details: Light fields and visibility	NA	2018
Tommer Leyvand:Olga Sorkine:Daniel Cohen-Or	From-region visibility culling is considered harder than from-point visibility culling, since it is inherently four-dimensional. We present a conservative occlusion culling method based on factorizing the 4D visibility problem into horizontal and vertical components. The visibility of the two components is solved asymmetrically: the horizontal component is based on a parameterization of the ray space, and the visibility of the vertical component is solved by incrementally merging umbrae. The technique is designed so that the horizontal and vertical operations can be efficiently realized together by modern graphics hardware. Similar to image-based from-point methods, we use an occlusion map to encode visibility; however, the image-space occlusion map is in the ray space rather than in the primal space. Our results show that the culling time and the size of the computed potentially visible set depend on the size of the viewcell. For moderate viewcells, conservative occlusion culling of large urban scenes takes less than a second, and the size of the potentially visible set is only about two times larger than the size of the exact visible set.	Ray space factorization for from-region visibility	NA:NA:NA	2018
Sameer Agarwal:Ravi Ramamoorthi:Serge Belongie:Henrik Wann Jensen	We introduce structured importance sampling, a new technique for efficiently rendering scenes illuminated by distant natural illumination given in an environment map. Our method handles occlusion, high-frequency lighting, and is significantly faster than alternative methods based on Monte Carlo sampling. We achieve this speedup as a result of several ideas. First, we present a new metric for stratifying and sampling an environment map taking into account both the illumination intensity as well as the expected variance due to occlusion within the scene. We then present a novel hierarchical stratification algorithm that uses our metric to automatically stratify the environment map into regular strata. This approach enables a number of rendering optimizations, such as pre-integrating the illumination within each stratum to eliminate noise at the cost of adding bias, and sorting the strata to reduce the number of sample rays. We have rendered several scenes illuminated by natural lighting, and our results indicate that structured importance sampling is better than the best previous Monte Carlo techniques, requiring one to two orders of magnitude fewer samples for the same image quality.	Structured importance sampling of environment maps	NA:NA:NA:NA	2018
Vincent Masselus:Pieter Peers:Philip Dutré:Yves D. Willems	We present an image-based technique to relight real objects illuminated by a 4D incident light field, representing the illumination of an environment. By exploiting the richness in angular and spatial variation of the light field, objects can be relit with a high degree of realism.We record photographs of an object, illuminated from various positions and directions, using a projector mounted on a gantry as a moving light source. The resulting basis images are used to create a subset of the full reflectance field of the object. Using this reflectance field, we can create an image of the object, relit with any incident light field and observed from a flxed camera position.To maintain acceptable recording times and reduce the amount of data, we propose an efficient data acquisition method.Since the object can be relit with a 4D incident light field, illumination effects encoded in the light field, such as shafts of shadow or spot light effects, can be realized.	Relighting with 4D incident light fields	NA:NA:NA:NA	2018
Michael Goesele:Xavier Granier:Wolfgang Heidrich:Hans-Peter Seidel	Realistic image synthesis requires both complex and realistic models of real-world light sources and efficient rendering algorithms to deal with them. In this paper, we describe a processing pipeline for dealing with complex light sources from acquisition to global illumination rendering. We carefully design optical filters to guarantee high precision measurements of real-world light sources. We discuss two practically feasible setups that allow us to measure light sources with different characteristics. Finally, we introduce an efficient importance sampling algorithm for our representation that can be used, for example, in conjunction with Photon Maps.	Accurate light source acquisition and rendering	NA:NA:NA:NA	2018
Marc Alexa	NA	Session details: Points	NA	2018
Kavita Bala:Bruce Walter:Donald P. Greenberg	This paper presents a new interactive rendering and display technique for complex scenes with expensive shading, such as global illumination. Our approach combines sparsely sampled shading (points) and analytically computed discontinuities (edges) to interactively generate high-quality images. The edge-and-point image is a new compact representation that combines edges and points such that fast, table-driven interpolation of pixel shading from nearby point samples is possible, while respecting discontinuities.The edge-and-point renderer is extensible, permitting the use of arbitrary shaders to collect shading samples. Shading discontinuities, such as silhouettes and shadow edges, are found at interactive rates. Our software implementation supports interactive navigation and object manipulation in scenes that include expensive lighting effects (such as global illumination) and geometrically complex objects. For interactive rendering we show that high-quality images of these scenes can be rendered at 8--14 frames per second on a desktop PC: a speedup of 20--60 over a ray tracer computing a single sample per pixel.	Combining edges and points for interactive high-quality rendering	NA:NA:NA	2018
Mark Pauly:Richard Keiser:Leif P. Kobbelt:Markus Gross	We present a versatile and complete free-form shape modeling framework for point-sampled geometry. By combining unstructured point clouds with the implicit surface definition of the moving least squares approximation, we obtain a hybrid geometry representation that allows us to exploit the advantages of implicit and parametric surface models. Based on this representation we introduce a shape modeling system that enables the designer to perform large constrained deformations as well as boolean operations on arbitrarily shaped objects. Due to minimum consistency requirements, point-sampled surfaces can easily be re-structured on the fly to support extreme geometric deformations during interactive editing. In addition, we show that strict topology control is possible and sharp features can be generated and preserved on point-sampled objects. We demonstrate the effectiveness of our system on a large set of input models, including noisy range scans, irregular point clouds, and sparsely as well as densely sampled models.	Shape modeling with point-sampled geometry	NA:NA:NA:NA	2018
Bart Adams:Philip Dutré	In this paper we present an algorithm to perform interactive boolean operations on free-form solids bounded by surfels. We introduce a fast inside-outside test to check whether surfels lie within the bounds of another surfel-bounded solid. This enables us to add, subtract and intersect complex solids at interactive rates. Our algorithm is fast both in displaying and constructing the new geometry resulting from the boolean operation.We present a resampling operator to solve problems resulting from sharp edges in the resulting solid. The operator resamples the surfels intersecting with the surface of the other solid. This enables us to represent the sharp edges with great detail.We believe our algorithm to be an ideal tool for interactive editing of free-form solids.	Interactive boolean operations on surfel-bounded solids	NA:NA	2018
Carsten Dachsbacher:Christian Vogelgsang:Marc Stamminger	In this paper we present sequential point trees, a data structure that allows adaptive rendering of point clouds completely on the graphics processor. Sequential point trees are based on a hierarchical point representation, but the hierarchical rendering traversal is replaced by sequential processing on the graphics processor, while the CPU is available for other tasks. Smooth transition to triangle rendering for optimized performance is integrated. We describe optimizations for backface culling and texture adaptive point selection. Finally, we discuss implementation issues and show results.	Sequential point trees	NA:NA:NA	2018
Carol O'Sullivan	NA	Session details: Modeling and simplification	NA	2018
Ignacio Llamas:Byungmoon Kim:Joshua Gargus:Jarek Rossignac:Chris D. Shaw	A free-form deformation that warps a surface or solid may be specified in terms of one or several point-displacement constraints that must be interpolated by the deformation. The Twister approach introduced here, adds the capability to impose an orientation change, adding three rotational constraints, at each displaced point. Furthermore, it solves for a space warp that simultaneously interpolates two sets of such displacement and orientation constraints. With a 6 DoF magnetic tracker in each hand, the user may grab two points on or near the surface of an object and simultaneously drag them to new locations while rotating the trackers to tilt, bend, or twist the shape near the displaced points. Using a new formalism based on a weighted average of screw displacements, Twister computes in realtime a smooth deformation, whose effect decays with distance from the grabbed points, simultaneously interpolating the 12 constraints. It is continuously applied to the shape, providing realtime graphic feedback. The two-hand interface and the resulting deformation are intuitive and hence offer an effective direct manipulation tool for creating or modifying 3D shapes.	Twister: a space-warp operator for the two-handed editing of 3D shapes	NA:NA:NA:NA:NA	2018
Peter Wonka:Michael Wimmer:François Sillion:William Ribarsky	This paper presents a new method for the automatic modeling of architecture. Building designs are derived using split grammars, a new type of parametric set grammar based on the concept of shape. The paper also introduces an attribute matching system and a separate control grammar, which offer the flexibility required to model buildings using a large variety of different styles and design ideas. Through the adaptive nature of the design grammar used, the created building designs can either be generic or adhere closely to a specified goal, depending on the amount of data available.	Instant architecture	NA:NA:NA:NA	2018
Andrew Wilson:Dinesh Manocha	We present an incremental algorithm to compute image-based simplifications of a large environment. We use an optimization-based approach to generate samples based on scene visibility, and from each viewpoint create textured depth meshes (TDMs) using sampled range panoramas of the environment. The optimization function minimizes artifacts such as skins and cracks in the reconstruction. We also present an encoding scheme for multiple TDMs that exploits spatial coherence among different viewpoints. The resulting simplifications, incremental textured depth meshes (ITDMs), reduce preprocessing, storage, rendering costs and visible artifacts. Our algorithm has been applied to large, complex synthetic environments comprising millions of primitives. It is able to render them at 20 -- 40 frames a second on a PC with little loss in visual fidelity.	Simplifying complex environments using incremental textured depth meshes	NA:NA	2018
Xavier Décoret:Frédo Durand:François X. Sillion:Julie Dorsey	We introduce billboard clouds -- a new approach for extreme simplification in the context of real-time rendering. 3D models are simplified onto a set of planes with texture and transparency maps. We present an optimization approach to build a billboard cloud given a geometric error threshold. After computing an appropriate density function in plane space, a greedy approach is used to select suitable representative planes. A good surface approximation is ensured by favoring planes that are "nearly tangent" to the model. This method does not require connectivity information, but instead avoids cracks by projecting primitives onto multiple planes when needed. For extreme simplification, our approach combines the strengths of mesh decimation and image-based impostors. We demonstrate our technique on a large class of models, including smooth manifolds and composite objects.	Billboard clouds for extreme model simplification	NA:NA:NA:NA	2018
Joe Marks	NA	Session details: Reprise of UIST 2003 and I3D 2003	NA	2018
Takeo Igarashi:John F. Hughes	NA	Clothing manipulation	NA:NA	2018
Michael Tsang:George W. Fitzmzurice:Gordon Kurtenbach:Azam Khan:Bill Buxton	We review the Boom Chameleon, a novel input/output device consisting of a flat-panel display mounted on a tracked mechanical armature. The display acts as a physical window into 3D virtual environments, through which a one-to-one mapping between real and virtual space is preserved. The Boom Chameleon is further augmented with a touch-screen and a microphone/speaker combination. We created a 3D annotation application that exploits this unique configuration in order to simultaneously capture viewpoint, voice and gesture information. Results of an informal user study show that the Boom Chameleon annotation facilities have the potential to be an effective, and intuitive system for reviewing 3D designs.	Boom chameleon: simultaneous capture of 3D viewpoint, voice and gesture annotations on a spatially-aware display	NA:NA:NA:NA:NA	2018
NA	The Actuated Workbench is a device that uses magnetic forces to move objects on a table in two dimensions. It is intended for use with existing tabletop tangible interfaces, providing an additional feedback loop for computer output, and helping to resolve inconsistencies that otherwise arise from the computer's inability to move objects on the table.	The actuated workbench: computer-controlled actuation in tabletop tangible interfaces	NA	2018
Christopher Niederauer:Mike Houston:Maneesh Agrawala:Greg Humphreys	NA	Non-invasive interactive visualization of dynamic architectural environments	NA:NA:NA:NA	2018
Benjamin Lok:Samir Naik:Mary Whitton:Frederick P. Brooks	NA	Incorporating dynamic real objects into immersive virtual environments	NA:NA:NA:NA	2018
Michael Gleicher:Hyun Joon Shin:Lucas Kovar:Andrew Jepsen	NA	Snap-together motion: assembling run-time animations	NA:NA:NA:NA	2018
Doug James	NA	Session details: Fluids and smoke	NA	2018
Nick Rasmussen:Duc Quang Nguyen:Willi Geiger:Ronald Fedkiw	In this paper, we present an efficient method for simulating highly detailed large scale participating media such as the nuclear explosions shown in figure 1. We capture this phenomena by simulating the motion of particles in a fluid dynamics generated velocity field. A novel aspect of this paper is the creation of highly detailed three-dimensional turbulent velocity fields at interactive rates using a low to moderate amount of memory. The key idea is the combination of two-dimensional high resolution physically based flow fields with a moderate sized three-dimensional Kolmogorov velocity field tiled periodically in space.	Smoke simulation for large scale phenomena	NA:NA:NA:NA	2018
Bryan E. Feldman:James F. O'Brien:Okan Arikan	This paper describes a method for animating suspended particle explosions. Rather than modeling the numerically troublesome, and largely invisible blast wave, the method uses a relatively stable incompressible fluid model to account for the motion of air and hot gases. The fluid's divergence field is adjusted directly to account for detonations and the generation and expansion of gaseous combustion products. Particles immersed in the fluid track the motion of particulate fuel and soot as they are advected by the fluid. Combustion is modeled using a simple but effective process governed by the particle and fluid systems. The method has enough flexibility to also approximate sprays of burning liquids. This paper includes several demonstrative examples showing air bursts, explosions near obstacles, confined explosions, and burning sprays. Because the method is based on components that allow large time integration steps, it only requires a few seconds of computation per frame for the examples shown.	Animating suspended particle explosions	NA:NA:NA	2018
Adrien Treuille:Antoine McNamara:Zoran Popović:Jos Stam	We describe a method for controlling smoke simulations through user-specified keyframes. To achieve the desired behavior, a continuous quasi-Newton optimization solves for appropriate "wind" forces to be applied to the underlying velocity field throughout the simulation. The cornerstone of our approach is a method to efficiently compute exact derivatives through the steps of a fluid simulation. We formulate an objective function corresponding to how well a simulation matches the user's keyframes, and use the derivatives to solve for force parameters that minimize this function. For animations with several keyframes, we present a novel multiple-shooting approach. By splitting large problems into smaller overlapping subproblems, we greatly speed up the optimization process while avoiding certain local minima.	Keyframe control of smoke simulations	NA:NA:NA:NA	2018
Jos Stam	In this paper we introduce a method to simulate fluid flows on smooth surfaces of arbitrary topology: an effect never seen before. We achieve this by combining a two-dimensional stable fluid solver with an atlas of parametrizations of a Catmull-Clark surface. The contributions of this paper are: (i) an extension of the Stable Fluids solver to arbitrary curvilinear coordinates, (ii) an elegant method to handle cross-patch boundary conditions and (iii) a set of new external forces custom tailored for surface flows. Our techniques can also be generalized to handle other types of processes on surfaces modeled by partial differential equations, such as reaction-diffusion. Some of our simulations allow a user to interactively place densities and apply forces to the surface, then watch their effects in real-time. We have also computed higher resolution animations of surface flows off-line.	Flows on surfaces of arbitrary topology	NA	2018
Yoshinori Dobashi:Tsuyoshi Yamamoto:Tomoyuki Nishita	In computer graphics, most research focuses on creating images. However, there has been much recent work on the automatic generation of sound linked to objects in motion and the relative positions of receivers and sound sources. This paper proposes a new method for creating one type of sound called aerodynamic sound. Examples of aerodynamic sound include sound generated by swinging swords or by wind blowing. A major source of aerodynamic sound is vortices generated in fluids such as air. First, we propose a method for creating sound textures for aerodynamic sound by making use of computational fluid dynamics. Next, we propose a method using the sound textures for real-time rendering of aerodynamic sound according to the motion of objects or wind velocity.	Real-time rendering of aerodynamic sound using sound textures based on computational fluid dynamics	NA:NA:NA	2018
Julie Dorsey	NA	Session details: Scattering and reflectance measurement	NA	2018
Jefferson Y. Han:Ken Perlin	We describe a new technique for measuring the bidirectional texture function (BTF) of a surface that requires no mechanical movement, can measure surfaces in situ under arbitrary lighting conditions, and can be made small, portable and inexpensive. The enabling innovation is the use of a tapered kaleidoscope, which allows a camera to view the same surface sample simultaneously from many directions. Similarly, the surface can be simultaneously illuminated from many directions, using only a single structured light source. We describe the techniques of construction and measurement, and we show experimental results.	Measuring bidirectional texture reflectance with a kaleidoscope	NA:NA	2018
Andrew Gardner:Chris Tchou:Tim Hawkins:Paul Debevec	This paper presents a technique for estimating the spatially-varying reflectance properties of a surface based on its appearance during a single pass of a linear light source. By using a linear light rather than a point light source as the illuminant, we are able to reliably observe and estimate the diffuse color, specular color, and specular roughness of each point of the surface. The reflectometry apparatus we use is simple and inexpensive to build, requiring a single direction of motion for the light source and a fixed camera viewpoint. Our model fitting technique first renders a reflectance table of how diffuse and specular reflectance lobes would appear under moving linear light source illumination. Then, for each pixel we compare its series of intensity values to the tabulated reflectance lobes to determine which reflectance model parameters most closely produce the observed reflectance values. Using two passes of the linear light source at different angles, we can also estimate per-pixel surface normals as well as the reflectance parameters. Additionally our system records a per-pixel height map for the object and estimates its per-pixel translucency. We produce real-time renderings of the captured objects using a custom hardware shading algorithm. We apply the technique to a test object exhibiting a variety of materials as well as to an illuminated manuscript with gold lettering. To demonstrate the technique's accuracy, we compare renderings of the captured models to real photographs of the original objects.	Linear light source reflectometry	NA:NA:NA:NA	2018
Wojciech Matusik:Hanspeter Pfister:Matt Brand:Leonard McMillan	We present a generative model for isotropic bidirectional reflectance distribution functions (BRDFs) based on acquired reflectance data. Instead of using analytical reflectance models, we represent each BRDF as a dense set of measurements. This allows us to interpolate and extrapolate in the space of acquired BRDFs to create new BRDFs. We treat each acquired BRDF as a single high-dimensional vector taken from a space of all possible BRDFs. We apply both linear (subspace) and non-linear (manifold) dimensionality reduction tools in an effort to discover a lower-dimensional representation that characterizes our measurements. We let users define perceptually meaningful parametrization directions to navigate in the reduced-dimension BRDF space. On the low-dimensional manifold, movement along these directions produces novel but valid BRDFs.	A data-driven reflectance model	NA:NA:NA:NA	2018
Norimichi Tsumura:Nobutoshi Ojima:Kayoko Sato:Mitsuhiro Shiraishi:Hideto Shimizu:Hirohide Nabeshima:Syuuichi Akazaki:Kimihiko Hori:Yoichi Miyake	This paper proposes an E-cosmetic function for digital images based on physics and physiologically-based image processing. A practical skin color and texture analysis/synthesis technique is introduced for this E-cosmetic function. Shading on the face is removed by a simple color vector analysis in the optical density domain as an inverse lighting technique. The image without shading is analyzed by a previously introduced technique that extracts hemoglobin and melanin components by independent component analysis. Experimental results using UV-B irradiation and the application of methyl nicotinate on the arms support the physiological validity of the analysis and the effectiveness of the proposed shading removal. We synthesized the way facial images changed due to tanning or alcohol consumption, and compared the synthesized images with images of actual changes in skin color. The comparison shows an excellent match between the synthesized and actual images of changes due to tanning and alcohol consumption. We also proposed a technique to synthesize the change of texture in pigment due to aging or the application of cosmetics. The pyramid-based texture analysis/synthesis technique was used for the spatial processing of texture. Using the proposed technique, we could realistically change the skin color and texture of a 50 year-old woman to that of a 20 year-old woman.	Image-based skin color and texture analysis/synthesis by extracting hemoglobin and melanin information in the skin	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Stephen R. Marschner:Henrik Wann Jensen:Mike Cammarano:Steve Worley:Pat Hanrahan	Light scattering from hair is normally simulated in computer graphics using Kajiya and Kay's classic phenomenological model. We have made new measurements of scattering from individual hair fibers that exhibit visually significant effects not predicted by Kajiya and Kay's model. Our measurements go beyond previous hair measurements by examining out-of-plane scattering, and together with this previous work they show a multiple specular highlight and variation in scattering with rotation about the fiber axis. We explain the sources of these effects using a model of a hair fiber as a transparent elliptical cylinder with an absorbing interior and a surface covered with tilted scales. Based on an analytical scattering function for a circular cylinder, we propose a practical shading model for hair that qualitatively matches the scattering behavior shown in the measurements. In a comparison between a photograph and rendered images, we demonstrate the new model's ability to match the appearance of real hair.	Light scattering from human hair fibers	NA:NA:NA:NA:NA	2018
Bengt-Olaf Schneider	NA	Session details: Hardware and displays	NA	2018
Timo Aila:Ville Miettinen:Petri Nordlund	In causal processes decisions do not depend on future data. Many well-known problems, such as occlusion culling, order-independent transparency and edge antialiasing cannot be properly solved using the traditional causal rendering architectures, because future data may change the interpretation of current events.We propose adding a delay stream between the vertex and pixel processing units. While a triangle resides in the delay stream, subsequent triangles generate occlusion information. As a result, the triangle may be culled by primitives that were submitted after it. We show two-to fourfold efficiency improvements in pixel processing and video memory bandwidth usage in common benchmark scenes. We also demonstrate how the memory requirements of order-independent transparency can be substantially reduced by using delay streams. Finally, we describe how discontinuity edges can be detected in hardware. Previously used heuristics for collapsing samples in adaptive supersampling are thus replaced by connectivity information.	Delay streams for graphics hardware	NA:NA:NA	2018
Tomas Akenine-Möller:Jacob Ström	The mobile phone is one of the most widespread devices with rendering capabilities. Those capabilities have been very limited because the resources on such devices are extremely scarce; small amounts of memory, little bandwidth, little chip area dedicated for special purposes, and limited power consumption. The small display resolutions present a further challenge; the angle subtended by a pixel is relatively large, and therefore reasonably high quality rendering is needed to generate high fidelity images.To increase the mobile rendering capabilities, we propose a new hardware architecture for rasterizing textured triangles. Our architecture focuses on saving memory bandwidth, since an external memory access typically is one of the most energy-consuming operations, and because mobile phones need to use as little power as possible. Therefore, our system includes three new key innovations: I) an inexpensive multisampling scheme that gives relatively high quality at the same cost of previous inexpensive schemes, II) a texture minification system, including texture compression, which gives quality relatively close to trilinear mipmapping at the cost of 1.33 32-bit memory accesses on average, III) a scanline-based culling scheme that avoids a significant amount of z-buffer reads, and that only requires one context. Software simulations show that these three innovations together significantly reduce the memory bandwidth, and thus also the power consumption.	Graphics for the masses: a hardware rasterization architecture for mobile phones	NA:NA	2018
Ramesh Raskar:Jeroen van Baar:Paul Beardsley:Thomas Willwacher:Srinivas Rao:Clifton Forlines	Projectors are currently undergoing a transformation as they evolve from static output devices to portable, environment-aware, communicating systems. An enhanced projector can determine and respond to the geometry of the display surface, and can be used in an ad-hoc cluster to create a self-configuring display. Information display is such a prevailing part of everyday life that new and more flexible ways to present data are likely to have significant impact. This paper examines geometrical issues for enhanced projectors, relating to customized projection for different shapes of display surface, object augmentation, and co-operation between multiple units.We introduce a new technique for adaptive projection on nonplanar surfaces using conformal texture mapping. We describe object augmentation with a hand-held projector, including interaction techniques. We describe the concept of a display created by an ad-hoc cluster of heterogeneous enhanced projectors, with a new global alignment scheme, and new parametric image transfer methods for quadric surfaces, to make a seamless projection. The work is illustrated by several prototypes and applications.	iLamps: geometrically aware and self-configuring projectors	NA:NA:NA:NA:NA:NA	2018
Markus Gross:Stephan Würmlin:Martin Naef:Edouard Lamboray:Christian Spagno:Andreas Kunz:Esther Koller-Meier:Tomas Svoboda:Luc Van Gool:Silke Lang:Kai Strehlke:Andrew Vande Moere:Oliver Staadt	We present blue-c, a new immersive projection and 3D video acquisition environment for virtual design and collaboration. It combines simultaneous acquisition of multiple live video streams with advanced 3D projection technology in a CAVE™-like environment, creating the impression of total immersion. The blue-c portal currently consists of three rectangular projection screens that are built from glass panels containing liquid crystal layers. These screens can be switched from a whitish opaque state (for projection) to a transparent state (for acquisition), which allows the video cameras to "look through" the walls. Our projection technology is based on active stereo using two LCD projectors per screen. The projectors are synchronously shuttered along with the screens, the stereo glasses, active illumination devices, and the acquisition hardware. From multiple video streams, we compute a 3D video representation of the user in real time. The resulting video inlays are integrated into a networked virtual environment. Our design is highly scalable, enabling blue-c to connect to portals with less sophisticated hardware.	blue-c: a spatially immersive display and 3D video portal for telepresence	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Frédo Durand	NA	Session details: Design and depiction	NA	2018
Maneesh Agrawala:Doantam Phan:Julie Heiser:John Haymaker:Jeff Klingner:Pat Hanrahan:Barbara Tversky	We present design principles for creating effective assembly instructions and a system that is based on these principles. The principles are drawn from cognitive psychology research which investigated people's conceptual models of assembly and effective methods to visually communicate assembly information. Our system is inspired by earlier work in robotics on assembly planning and in visualization on automated presentation design. Although other systems have considered presentation and planning independently, we believe it is necessary to address the two problems simultaneously in order to create effective assembly instructions. We describe the algorithmic techniques used to produce assembly instructions given object geometry, orientation, and optional grouping and ordering constraints on the object's parts. Our results demonstrate that it is possible to produce aesthetically pleasing and easy to follow instructions for many everyday objects.	Designing effective step-by-step assembly instructions	NA:NA:NA:NA:NA:NA:NA	2018
Charles Jacobs:Wilmot Li:Evan Schrier:David Bargeron:David Salesin	Grid-based page designs are ubiquitous in commercially printed publications, such as newspapers and magazines. Yet, to date, no one has invented a good way to easily and automatically adapt such designs to arbitrarily-sized electronic displays. The difficulty of generalizing grid-based designs explains the generally inferior nature of on-screen layouts when compared to their printed counterparts, and is arguably one of the greatest remaining impediments to creating on-line reading experiences that rival those of ink on paper. In this work, we present a new approach to adaptive grid-based document layout, which attempts to bridge this gap. In our approach, an adaptive layout style is encoded as a set of grid-based templates that know how to adapt to a range of page sizes and other viewing conditions. These templates include various types of layout elements (such as text, figures, etc.) and define, through constraint-based relationships, just how these elements are to be laid out together as a function of both the properties of the content itself, such as a figure's size and aspect ratio, and the properties of the viewing conditions under which the content is being displayed. We describe an XML-based representation for our templates and content, which maintains a clean separation between the two. We also describe the various parts of our research prototype system: a layout engine for formatting the page; a paginator for determining a globally optimal allocation of content amongst the pages, as well as an optimal pairing of templates with content; and a graphical user interface for interactively creating adaptive templates. We also provide numerous examples demonstrating the capabilities of this prototype, including this paper, itself, which has been laid out with our system.	Adaptive grid-based document layout	NA:NA:NA:NA:NA	2018
Doug DeCarlo:Adam Finkelstein:Szymon Rusinkiewicz:Anthony Santella	In this paper, we describe a non-photorealistic rendering system that conveys shape using lines. We go beyond contours and creases by developing a new type of line to draw: the suggestive contour. Suggestive contours are lines drawn on clearly visible parts of the surface, where a true contour would first appear with a minimal change in viewpoint. We provide two methods for calculating suggestive contours, including an algorithm that finds the zero crossings of the radial curvature. We show that suggestive contours can be drawn consistently with true contours, because they anticipate and extend them. We present a variety of results, arguing that these images convey shape more effectively than contour alone.	Suggestive contours for conveying shape	NA:NA:NA:NA	2018
Robert D. Kalnins:Philip L. Davidson:Lee Markosian:Adam Finkelstein	We describe a way to render stylized silhouettes of animated 3D models with temporal coherence. Coherence is one of the central challenges for non-photorealistic rendering. It is especially difficult for silhouettes, because they may not have obvious correspondences between frames. We demonstrate various coherence effects for stylized silhouettes with a robust working system. Our method runs in real-time for models of moderate complexity, making it suitable for both interactive applications and offline animation.	Coherent stylized silhouettes	NA:NA:NA:NA	2018
James F. O'Brien	NA	Session details: Dynamics	NA	2018
David Baraff:Andrew Witkin:Michael Kass	Deficient cloth-to-cloth collision response is the most serious shortcoming of most cloth simulation systems. Past approaches to cloth-cloth collision have used history to decide whether nearby cloth regions have interpenetrated. The biggest pitfall of history-based methods is that an error anywhere along the way can give rise to persistent tangles. This is a particularly serious issue for production character animation, because characters' bodies routinely self-intersect, for instance in the bend of an elbow or knee, or where the arm or hand rests against the body. Cloth that becomes pinched in these regions is often forced into jagged self-intersections that defeat history-based methods, leaving a tangled mess when the body parts separate. This paper describes a history-free cloth collision response algorithm based on global intersection analysis of cloth meshes at each simulation step. The algorithm resolves tangles that arise during pinching as soon as the surrounding geometry permits, and also resolves tangled initial conditions. The ability to untangle cloth after pinching is not sufficient, because standard cloth-solid collision algorithms handle pinches so poorly that they often give rise to visible flutters and other simulation artifacts during the pinch. As a companion to the global intersection analysis method, we present a cloth-solid collision algorithm called collision flypapering, that eliminates these artifacts. The two algorithms presented have been used together extensively and successfully in a production animation environment.	Untangling cloth	NA:NA:NA	2018
Eran Guendelman:Robert Bridson:Ronald Fedkiw	We consider the simulation of nonconvex rigid bodies focusing on interactions such as collision, contact, friction (kinetic, static, rolling and spinning) and stacking. We advocate representing the geometry with both a triangulated surface and a signed distance function defined on a grid, and this dual representation is shown to have many advantages. We propose a novel approach to time integration merging it with the collision and contact processing algorithms in a fashion that obviates the need for ad hoc threshold velocities. We show that this approach matches the theoretical solution for blocks sliding and stopping on inclined planes with friction. We also present a new shock propagation algorithm that allows for efficient use of the propagation (as opposed to the simultaneous) method for treating contact. These new techniques are demonstrated on a variety of problems ranging from simple test cases to stacking problems with as many as 1000 nonconvex rigid bodies with friction as shown in Figure 1.	Nonconvex rigid bodies with stacking	NA:NA:NA	2018
Doug L. James:Kayvon Fatahalian	We present an approach for precomputing data-driven models of interactive physically based deformable scenes. The method permits real-time hardware synthesis of nonlinear deformation dynamics, including self-contact and global illumination effects, and supports real-time user interaction. We use data-driven tabulation of the system's deterministic state space dynamics, and model reduction to build efficient low-rank parameterizations of the deformed shapes. To support runtime interaction, we also tabulate impulse response functions for a palette of external excitations. Although our approach simulates particular systems under very particular interaction conditions, it has several advantages. First, parameterizing all possible scene deformations enables us to precompute novel reduced coparameterizations of global scene illumination for low-frequency lighting conditions. Second, because the deformation dynamics are precomputed and parameterized as a whole, collisions are resolved within the scene during precomputation so that runtime self-collision handling is implicit. Optionally, the data-driven models can be synthesized on programmable graphics hardware, leaving only the low-dimensional state space dynamics and appearance data models to be computed by the main CPU.	Precomputing interactive dynamic deformable scenes	NA:NA	2018
Jia-chi Wu:Zoran Popović	In this paper we describe a physics-based method for synthesis of bird flight animations. Our method computes a realistic set of wingbeats that enables a bird to follow the specified trajectory. We model the bird as an articulated skeleton with elastically deformable feathers. The bird motion is created by applying joint torques and aerodynamic forces over time in a forward dynamics simulation. We solve for each wingbeat motion separately by optimizing for wingbeat parameters that create the most natural motion. The final animation is constructed by concatenating a series of optimal wingbeats. This detailed bird flight model enables us to produce flight motions of different birds performing a variety of maneuvers including taking off, cruising, rapidly descending, turning, and landing.	Realistic modeling of bird flight animations	NA:NA	2018
Michael McCool	NA	Session details: Computation on GPUs	NA	2018
William R. Mark:R. Steven Glanville:Kurt Akeley:Mark J. Kilgard	The latest real-time graphics architectures include programmable floating-point vertex and fragment processors, with support for data-dependent control flow in the vertex processor. We present a programming language and a supporting system that are designed for programming these stream processors. The language follows the philosophy of C, in that it is a hardware-oriented, general-purpose language, rather than an application-specific shading language. The language includes a variety of facilities designed to support the key architectural features of programmable graphics processors, and is designed to support multiple generations of graphics architectures with different levels of functionality. The system supports both of the major 3D graphics APIs: OpenGL and Direct3D. This paper identifies many of the choices that we faced as we designed the system, and explains why we made the decisions that we did.	Cg: a system for programming graphics hardware in a C-like language	NA:NA:NA:NA	2018
Jens Krüger:Rüdiger Westermann	In this work, the emphasis is on the development of strategies to realize techniques of numerical computing on the graphics chip. In particular, the focus is on the acceleration of techniques for solving sets of algebraic equations as they occur in numerical simulation. We introduce a framework for the implementation of linear algebra operators on programmable graphics processors (GPUs), thus providing the building blocks for the design of more complex numerical algorithms. In particular, we propose a stream model for arithmetic operations on vectors and matrices that exploits the intrinsic parallelism and efficient communication on modern GPUs. Besides performance gains due to improved numerical computations, graphics algorithms benefit from this model in that the transfer of computation results to the graphics processor for display is avoided. We demonstrate the effectiveness of our approach by implementing direct solvers for sparse matrices, and by applying these solvers to multi-dimensional finite difference equations, i.e. the 2D wave equation and the incompressible Navier-Stokes equations.	Linear algebra operators for GPU implementation of numerical algorithms	NA:NA	2018
Jeff Bolz:Ian Farmer:Eitan Grinspun:Peter Schröder	Many computer graphics applications require high-intensity numerical simulation. We show that such computations can be performed efficiently on the GPU, which we regard as a full function streaming processor with high floating-point performance. We implemented two basic, broadly useful, computational kernels: a sparse matrix conjugate gradient solver and a regular-grid multigrid solver. Real time applications ranging from mesh smoothing and parameterization to fluid solvers and solid mechanics can greatly benefit from these, evidence our example applications of geometric flow and fluid simulation running on NVIDIA's GeForce FX.	Sparse matrix solvers on the GPU: conjugate gradients and multigrid	NA:NA:NA:NA	2018
Karl E. Hillesland:Sergey Molinov:Radek Grzeszczuk	Graphics hardware is undergoing a change from fixed-function pipelines to more programmable organizations that resemble general purpose stream processors. In this paper, we show that certain general algorithms, not normally associated with computer graphics, can be mapped to such designs. Specifically, we cast nonlinear optimization as a data streaming process that is well matched to modern graphics processors. Our framework is particularly well suited for solving image-based modeling problems since it can be used to represent a large and diverse class of these problems using a common formulation. We successfully apply this approach to two distinct image-based modeling problems: light field mapping approximation and fitting the Lafortune model to spatial bidirectional reflectance distribution functions. Comparing the performance of the graphics hardware implementation to a CPU implementation, we show more than 5-fold improvement.	Nonlinear optimization framework for image-based modeling on programmable graphics hardware	NA:NA:NA	2018
Martin Isenburg:Stefan Gumhold	Polygonal models acquired with emerging 3D scanning technology or from large scale CAD applications easily reach sizes of several gigabytes and do not fit in the address space of common 32-bit desktop PCs. In this paper we propose an out-of-core mesh compression technique that converts such gigantic meshes into a streamable, highly compressed representation. During decompression only a small portion of the mesh needs to be kept in memory at any time. As full connectivity information is available along the decompression boundaries, this provides seamless mesh access for incremental in-core processing on gigantic meshes. Decompression speeds are CPU-limited and exceed one million vertices and two million triangles per second on a 1.8 GHz Athlon processor.A novel external memory data structure provides our compression engine with transparent access to arbitrary large meshes. This out-of-core mesh was designed to accommodate the access pattern of our region-growing based compressor, which - in return - performs mesh queries as seldom and as local as possible by remembering previous queries as long as needed and by adapting its traversal slightly. The achieved compression rates are state-of-the-art.	Out-of-core compression for gigantic polygon meshes	NA:NA	2018
Thouis R. Jones:Frédo Durand:Mathieu Desbrun	With the increasing use of geometry scanners to create 3D models, there is a rising need for fast and robust mesh smoothing to remove inevitable noise in the measurements. While most previous work has favored diffusion-based iterative techniques for feature-preserving smoothing, we propose a radically different approach, based on robust statistics and local first-order predictors of the surface. The robustness of our local estimates allows us to derive a non-iterative feature-preserving filtering technique applicable to arbitrary "triangle soups". We demonstrate its simplicity of implementation and its efficiency, which make it an excellent solution for smoothing large, noisy, and non-manifold meshes.	Non-iterative, feature-preserving mesh smoothing	NA:NA:NA	2018
Shachar Fleishman:Iddo Drori:Daniel Cohen-Or	We present an anisotropic mesh denoising algorithm that is effective, simple and fast. This is accomplished by filtering vertices of the mesh in the normal direction using local neighborhoods. Motivated by the impressive results of bilateral filtering for image denoising, we adopt it to denoise 3D meshes; addressing the specific issues required in the transition from two-dimensions to manifolds in three dimensions. We show that the proposed method successfully removes noise from meshes while preserving features. Furthermore, the presented algorithm excels in its simplicity both in concept and implementation.	Bilateral mesh denoising	NA:NA:NA	2018
Frédo Durand	NA	Session details: Graphics is fun	NA	2018
Stephane Guy:Cyril Soler	We present an algorithm for rendering faceted colored gemstones in real time, using graphics hardware. Beyond the technical challenge of handling the complex behavior of light in such objects, a real time high quality rendering of gemstones has direct applications in the field of jewelry prototyping, which has now become a standard practice for replacing tedious (and less interactive) wax carving methods. Our solution is based on a number of controlled approximations of the physical phenomena involved when light enters a stone, which permit an implementation based on the most recent -- yet commonly available -- hardware features such as fragment programs, cube-mapping.	Graphics gems revisited: fast and physically-based rendering of gemstones	NA:NA	2018
Roger David Hersch:Sylvain Chosson	We propose a new powerful way of synthesizing moiré images that enables the creation of dynamically moving messages incorporating text, symbols, and color elements. Moiré images appear when superposing a base layer made of replicated base bands and a revealing layer made of a line grating comprising thin transparent lines. Each replicated base band contains the same image, e.g. text or color motifs. Since the base bands and the revealing line grating have similar periods, the revealed moiré image is the image located within each base band, enlarged along one dimension. By considering the formation of the moiré image as a line sampling process, we derive the linear transformation between the base layer and the moiré image. We obtain the geometric layout of the resulting moiré image, i.e. its orientation, size and displacement direction when moving the revealing layer on top of the base layer. Interesting moiré images can be synthesized by applying geometric transformations to both the base and the revealing layers. We propose a mathematical model describing the geometric transformation that a moiré image undergoes, when its base layer and its revealing layer are subject to different freely chosen non-linear geometric transformations. By knowing in advance the layout of a moiré image as a function of the layouts of the base layer and of the revealing layer, we are able to create moiré components running up and down at different speeds and orientations upon translation of the revealing layer. We also derive layer transformations which yield periodic moiré images despite the fact that both the base and the revealing layers are curved. By offering a new means of artistic expression, band moiré images can be used to create new designs and to synthesize visually appealing applications.	Band moiré images	NA:NA	2018
Nicolas Tsingos:Emmanuel Gallo:George Drettakis	We propose a real-time 3D audio rendering pipeline for complex virtual scenes containing hundreds of moving sound sources. The approach, based on auditory culling and spatial level-of-detail, can handle more than ten times the number of sources commonly available on consumer 3D audio hardware, with minimal decrease in audio quality. The method performs well for both indoor and outdoor environments. It leverages the limited capabilities of audio hardware for many applications, including interactive architectural acoustics simulations and automatic 3D voice management for video games.Our approach dynamically eliminates inaudible sources and groups the remaining audible sources into a budget number of clusters. Each cluster is represented by one impostor sound source, positioned using perceptual criteria. Spatial audio processing is then performed only on the impostor sound sources rather than on every original source thus greatly reducing the computational cost.A pilot validation study shows that degradation in audio quality, as well as localization impairment, are limited and do not seem to vary significantly with the cluster budget. We conclude that our real-time perceptual audio rendering pipeline can generate spatialized audio for complex auditory environments without introducing disturbing changes in the resulting perceived soundfield.	Perceptual audio rendering of complex virtual environments	NA:NA:NA	2018
Jun Mitani:Hiromasa Suzuki	We propose a new method for producing unfolded papercraft patterns of rounded toy animal figures from triangulated meshes by means of strip-based approximation. Although in principle a triangulated model can be unfolded simply by retaining as much as possible of its connectivity while checking for intersecting triangles in the unfolded plane, creating a pattern with tens of thousands of triangles is unrealistic. Our approach is to approximate the mesh model by a set of continuous triangle strips with no internal vertices. Initially, we subdivide our mesh into parts corresponding to the features of the model. We segment each part into zonal regions, grouping triangles which are similar topological distances from the part boundary. We generate triangle strips by simplifying the mesh while retaining the borders of the zonal regions and additional cut-lines. The pattern is then created simply by unfolding the set of strips. The distinguishing feature of our method is that we approximate a mesh model by a set of continuous strips, not by other ruled surfaces such as parts of cones or cylinders. Thus, the approximated unfolded pattern can be generated using only mesh operations and a simple unfolding algorithm. Furthermore, a set of strips can be crafted just by bending the paper (without breaking edges) and can represent smooth features of the original mesh models.	Making papercraft toys from meshes using strip-based approximate unfolding	NA:NA	2018
Marc Alexa	NA	Session details: Curves & surfaces	NA	2018
Nina Amenta:Yong Joo Kil	The MLS surface [Levin 2003], used for modeling and rendering with point clouds, was originally defined algorithmically as the output of a particular meshless construction. We give a new explicit definition in terms of the critical points of an energy function on lines determined by a vector field. This definition reveals connections to research in computer vision and computational topology.Variants of the MLS surface can be created by varying the vector field and the energy function. As an example, we define a similar surface determined by a cloud of surfels (points equipped with normals), rather than points.We also observe that some procedures described in the literature to take points in space onto the MLS surface fail to do so, and we describe a simple iterative procedure which does.	Defining point-set surfaces	NA:NA	2018
Lexing Ying:Denis Zorin	We present a smooth surface construction based on the manifold approach of Grimm and Hughes. We demonstrate how this approach can relatively easily produce a number of desirable properties which are hard to achieve simultaneously with polynomial patches, subdivision or variational surfaces. Our surfaces are C∞-continuous with explicit nonsingular C∞ parameterizations, high-order flexible at control vertices, depend linearly on control points, have fixed-size local support for basis functions, and have good visual quality.	A simple manifold-based construction of surfaces of arbitrary smoothness	NA:NA	2018
Thomas W. Sederberg:David L. Cardon:G. Thomas Finnigan:Nicholas S. North:Jianmin Zheng:Tom Lyche	A typical NURBS surface model has a large percentage of superfluous control points that significantly interfere with the design process. This paper presents an algorithm for eliminating such superfluous control points, producing a T-spline. The algorithm can remove substantially more control points than competing methods such as B-spline wavelet decomposition. The paper also presents a new T-spline local refinement algorithm and answers two fundamental open questions on T-spline theory.	T-spline simplification and local refinement	NA:NA:NA:NA:NA:NA	2018
Michael Hofer:Helmut Pottmann	Variational interpolation in curved geometries has many applications, so there has always been demand for geometrically meaningful and efficiently computable splines in manifolds. We extend the definition of the familiar cubic spline curves and splines in tension, and we show how to compute these on parametric surfaces, level sets, triangle meshes, and point samples of surfaces. This list is more comprehensive than it looks, because it includes variational motion design for animation, and allows the treatment of obstacles via barrier surfaces. All these instances of the general concept are handled by the same geometric optimization algorithm, which minimizes an energy of curves on surfaces of arbitrary dimension and codimension.	Energy-minimizing splines in manifolds	NA:NA	2018
Aaron Hertzmann	NA	Session details: Interacting with images	NA	2018
Aseem Agarwala:Mira Dontcheva:Maneesh Agrawala:Steven Drucker:Alex Colburn:Brian Curless:David Salesin:Michael Cohen	We describe an interactive, computer-assisted framework for combining parts of a set of photographs into a single composite picture, a process we call "digital photomontage." Our framework makes use of two techniques primarily: graph-cut optimization, to choose good seams within the constituent images so that they can be combined as seamlessly as possible; and gradient-domain fusion, a process based on Poisson equations, to further reduce any remaining visible artifacts in the composite. Also central to the framework is a suite of interactive tools that allow the user to specify a variety of high-level image objectives, either globally across the image, or locally through a painting-style interface. Image objectives are applied independently at each pixel location and generally involve a function of the pixel values (such as "maximum contrast") drawn from that same location in the set of source images. Typically, a user applies a series of image objectives iteratively in order to create a finished composite. The power of this framework lies in its generality; we show how it can be used for a wide variety of applications, including "selective composites" (for instance, group photos in which everyone looks their best), relighting, extended depth of field, panoramic stitching, clean-plate production, stroboscopic visualization of movement, and time-lapse mosaics.	Interactive digital photomontage	NA:NA:NA:NA:NA:NA:NA:NA	2018
Yin Li:Jian Sun:Chi-Keung Tang:Heung-Yeung Shum	In this paper, we present Lazy Snapping, an interactive image cutout tool. Lazy Snapping separates coarse and fine scale processing, making object specification and detailed adjustment easy. Moreover, Lazy Snapping provides instant visual feedback, snapping the cutout contour to the true object boundary efficiently despite the presence of ambiguous or low contrast edges. Instant feedback is made possible by a novel image segmentation algorithm which combines graph cut with pre-computed over-segmentation. A set of intuitive user interface (UI) tools is designed and implemented to provide flexible control and editing for the users. Usability studies indicate that Lazy Snapping provides a better user experience and produces better segmentation results than the state-of-the-art interactive image cutout tool, Magnetic Lasso in Adobe Photoshop.	Lazy snapping	NA:NA:NA:NA	2018
Carsten Rother:Vladimir Kolmogorov:Andrew Blake	The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for "border matting" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools.	"GrabCut": interactive foreground extraction using iterated graph cuts	NA:NA:NA	2018
Jian Sun:Jiaya Jia:Chi-Keung Tang:Heung-Yeung Shum	In this paper, we formulate the problem of natural image matting as one of solving Poisson equations with the matte gradient field. Our approach, which we call Poisson matting, has the following advantages. First, the matte is directly reconstructed from a continuous matte gradient field by solving Poisson equations using boundary information from a user-supplied trimap. Second, by interactively manipulating the matte gradient field using a number of filtering tools, the user can further improve Poisson matting results locally until he or she is satisfied. The modified local result is seamlessly integrated into the final result. Experiments on many complex natural images demonstrate that Poisson matting can generate good matting results that are not possible using existing matting techniques.	Poisson matting	NA:NA:NA:NA	2018
Julie Dorsey	NA	Session details: 3D texture	NA	2018
Shigeru Owada:Frank Nielsen:Makoto Okabe:Takeo Igarashi	This paper presents an interactive system for designing and browsing volumetric illustrations. Volumetric illustrations are 3D models with internal textures that the user can browse by cutting the models at desired locations. To assign internal textures to a surface mesh, the designer cuts the mesh and provides simple guiding information to specify the correspondence between the cross-section and a reference 2D image. The guiding information is stored with the geometry and used during the synthesis of cross-sectional textures. The key idea is to synthesize a plausible cross-sectional image using a 2D texture-synthesis technique, instead of sampling from a complete 3D RGB volumetric representation directly. This simplifies the design interface and reduces the amount of data, making it possible for non-experts to rapidly design and use volumetric illustrations. We believe that our system can enrich human communications in various domains, such as medicine, biology, and geology.	Volumetric illustration: designing 3D models with internal textures	NA:NA:NA:NA	2018
Robert Jagnow:Julie Dorsey:Holly Rushmeier	We describe the use of traditional stereological methods to synthesize 3D solid textures from 2D images of existing materials. We first illustrate our approach for aggregate materials of spherical particles, and then extend the technique to apply to particles of arbitrary shapes. We demonstrate the effectiveness of the approach with side-by-side comparisons of a real material and a synthetic model with its appearance parameters derived from its physical counterpart. Unlike ad hoc methods for texture synthesis, stereology provides a disciplined, systematic basis for predicting material structure with well-defined assumptions.	Stereological techniques for solid textures	NA:NA:NA	2018
M. Alex O. Vasilescu:Demetri Terzopoulos	This paper introduces a tensor framework for image-based rendering. In particular, we develop an algorithm called TensorTextures that learns a parsimonious model of the bidirectional texture function (BTF) from observational data. Given an ensemble of images of a textured surface, our nonlinear, generative model explicitly represents the multifactor interaction implicit in the detailed appearance of the surface under varying photometric angles, including local (per-texel) reflectance, complex mesostructural self-occlusion, interreflection and self-shadowing, and other BTF-relevant phenomena. Mathematically, TensorTextures is based on multilinear algebra, the algebra of higher-order tensors, hence its name. It is computed through a decomposition known as the N-mode SVD, an extension to tensors of the conventional matrix singular value decomposition (SVD). We demonstrate the application of TensorTextures to the image-based rendering of natural and synthetic textured surfaces under continuously varying viewpoint and illumination conditions.	TensorTextures: multilinear image-based rendering	NA:NA	2018
Yanyun Chen:Xin Tong:Jiaping Wang:Stephen Lin:Baining Guo:Heung-Yeung Shum	We propose a texture function for realistic modeling and efficient rendering of materials that exhibit surface mesostructures, translucency and volumetric texture variations. The appearance of such complex materials for dynamic lighting and viewing directions is expensive to calculate and requires an impractical amount of storage to precompute. To handle this problem, our method models an object as a shell layer, formed by texture synthesis of a volumetric material sample, and a homogeneous inner core. To facilitate computation of surface radiance from the shell layer, we introduce the shell texture function (STF) which describes voxel irradiance fields based on precomputed fine-level light interactions such as shadowing by surface mesostructures and scattering of photons inside the object. Together with a diffusion approximation of homogeneous inner core radiance, the STF leads to fast and detailed raytraced renderings of complex materials.	Shell texture functions	NA:NA:NA:NA:NA:NA	2018
Dani Lischinski	NA	Session details: Photo & video texture	NA	2018
Hui Fang:John C. Hart	We combine existing techniques for shape-from-shading and texture synthesis to create a new tool for texturing objects in photographs. Our approach clusters pixels with similar recovered normals into patches on which texture is synthesized. Distorting the texture based on the recovered normals creates the illusion that the texture adheres to the undulations of the photographed surface. Inconsistencies in the recovered surface are disguised by the graphcut blending of the individually textured patches. Further applications include the generation of detail on manually-shaded painting, extracting and synthesizing a displacement map from a texture swatch, and the embossed transfer of normals from one image to another, which would be difficult to create with current image processing packages.	Textureshop: texture synthesis as a photograph editing tool	NA:NA	2018
Kiran S. Bhat:Steven M. Seitz:Jessica K. Hodgins:Pradeep K. Khosla	This paper presents a novel algorithm for synthesizing and editing video of natural phenomena that exhibit continuous flow patterns. The algorithm analyzes the motion of textured particles in the input video along user-specified flow lines, and synthesizes seamless video of arbitrary length by enforcing temporal continuity along a second set of user-specified flow lines. The algorithm is simple to implement and use. We used this technique to edit video of water-falls, rivers, flames, and smoke.	Flow-based video synthesis and editing	NA:NA:NA:NA	2018
Qing Wu:Yizhou Yu	One significant problem in patch-based texture synthesis is the presence of broken features at the boundary of adjacent patches. The reason is that optimization schemes for patch merging may fail when neighborhood search cannot find satisfactory candidates in the sample texture because of an inaccurate similarity measure. In this paper, we consider both curvilinear features and their deformation. We develop a novel algorithm to perform feature matching and alignment by measuring structural similarity. Our technique extracts a feature map from the sample texture, and produces both a new feature map and texture map. Texture synthesis guided by feature maps can significantly reduce the number of feature discontinuities and related artifacts, and gives rise to satisfactory results.	Feature matching and deformation for texture synthesis	NA:NA	2018
Yanxi Liu:Wen-Chieh Lin:James Hays	A near-regular texture deviates geometrically and photometrically from a regular congruent tiling. Although near-regular textures are ubiquitous in the man-made and natural world, they present computational challenges for state of the art texture analysis and synthesis algorithms. Using regular tiling as our anchor point, and with user-assisted lattice extraction, we can explicitly model the deformation of a near-regular texture with respect to geometry, lighting and color. We treat a deformation field both as a function that acts on a texture and as a texture that is acted upon, and develop a multi-modal framework where each deformation field is subject to analysis, synthesis and manipulation. Using this formalization, we are able to construct simple parametric models to faithfully synthesize the appearance of a near-regular texture and purposefully control its regularity.	Near-regular texture analysis and manipulation	NA:NA:NA	2018
Jovan Popović	NA	Session details: Dynamics & modeling	NA	2018
Mark Carlson:Peter J. Mucha:Greg Turk	We present the Rigid Fluid method, a technique for animating the interplay between rigid bodies and viscous incompressible fluid with free surfaces. We use distributed Lagrange multipliers to ensure two-way coupling that generates realistic motion for both the solid objects and the fluid as they interact with one another. We call our method the rigid fluid method because the simulator treats the rigid objects as if they were made of fluid. The rigidity of such an object is maintained by identifying the region of the velocity field that is inside the object and constraining those velocities to be rigid body motion. The rigid fluid method is straightforward to implement, incurs very little computational overhead, and can be added as a bridge between current fluid simulators and rigid body solvers. Many solid objects of different densities (e.g., wood or lead) can be combined in the same animation.	Rigid fluid: animating the interplay between rigid bodies and fluid	NA:NA:NA	2018
Neil Molino:Zhaosheng Bao:Ron Fedkiw	We propose a virtual node algorithm that allows material to separate along arbitrary (possibly branched) piecewise linear paths through a mesh. The material within an element is fragmented by creating several replicas of the element and assigning a portion of real material to each replica. This results in elements that contain both real material and empty regions. The missing material is contained in another copy (or copies) of this element. Our new virtual node algorithm automatically determines the number of replicas and the assignment of material to each. Moreover, it provides the degrees of freedom required to simulate the partially or fully fragmented material in a fashion consistent with the embedded geometry. This approach enables efficient simulation of complex geometry with a simple mesh, i.e. the geometry need not align itself with element boundaries. It also alleviates many shortcomings of traditional Lagrangian simulation techniques for meshes with changing topology. For example, slivers do not require small CFL time step restrictions since they are embedded in well shaped larger elements. To enable robust simulation of embedded geometry, we propose new algorithms for handling rigid body and self collisions. In addition, we present several mechanisms for influencing and controlling fracture with grain boundaries, prescoring, etc. We illustrate our method for both volumetric and thin-shell simulations.	A virtual node algorithm for changing mesh topology during simulation	NA:NA:NA	2018
Doug L. James:Dinesh K. Pai	We introduce the Bounded Deformation Tree, or BD-Tree, which can perform collision detection with reduced deformable models at costs comparable to collision detection with rigid objects. Reduced deformable models represent complex deformations as linear superpositions of arbitrary displacement fields, and are used in a variety of applications of interactive computer graphics. The BD-Tree is a bounding sphere hierarchy for output-sensitive collision detection with such models. Its bounding spheres can be updated after deformation in any order, and at a cost independent of the geometric complexity of the model; in fact the cost can be as low as one multiplication and addition per tested sphere, and at most linear in the number of reduced deformation coordinates. We show that the BD-Tree is also extremely simple to implement, and performs well in practice for a variety of real-time and complex off-line deformable simulation examples.	BD-tree: output-sensitive collision detection for reduced deformable models	NA:NA	2018
Robert W. Sumner:Jovan Popović	Deformation transfer applies the deformation exhibited by a source triangle mesh onto a different target triangle mesh. Our approach is general and does not require the source and target to share the same number of vertices or triangles, or to have identical connectivity. The user builds a correspondence map between the triangles of the source and those of the target by specifying a small set of vertex markers. Deformation transfer computes the set of transformations induced by the deformation of the source mesh, maps the transformations through the correspondence from the source to the target, and solves an optimization problem to consistently apply the transformations to the target shape. The resulting system of linear equations can be factored once, after which transferring a new deformation to the target mesh requires only a backsubstitution step. Global properties such as foot placement can be achieved by constraining vertex positions. We demonstrate our method by retargeting full body key poses, applying scanned facial deformations onto a digital character, and remapping rigid and non-rigid animation sequences from one mesh onto another.	Deformation transfer for triangle meshes	NA:NA	2018
Maneesh Agrawala	NA	Session details: Identifying & sketching the future	NA	2018
Ramesh Raskar:Paul Beardsley:Jeroen van Baar:Yao Wang:Paul Dietz:Johnny Lee:Darren Leigh:Thomas Willwacher	This paper describes how to instrument the physical world so that objects become self-describing, communicating their identity, geometry, and other information such as history or user annotation. The enabling technology is a wireless tag which acts as a radio frequency identity and geometry (RFIG) transponder. We show how addition of a photo-sensor to a wireless tag significantly extends its functionality to allow geometric operations - such as finding the 3D position of a tag, or detecting change in the shape of a tagged object. Tag data is presented to the user by direct projection using a handheld locale-aware mobile projector. We introduce a novel technique that we call interactive projection to allow a user to interact with projected information e.g. to navigate or update the projected information.The ideas are demonstrated using objects with active radio frequency (RF) tags. But the work was motivated by the advent of unpowered passive-RFID, a technology that promises to have significant impact in real-world applications. We discuss how our current prototypes could evolve to passive-RFID in the future.	RFIG lamps: interacting with a self-describing world via photosensing wireless tags and projectors	NA:NA:NA:NA:NA:NA:NA:NA	2018
J. P. Lewis:Ruth Rosenholtz:Nickson Fong:Ulrich Neumann	Although existing GUIs have a sense of space, they provide no sense of place. Numerous studies report that users misplace files and have trouble wayfinding in virtual worlds despite the fact that people have remarkable visual and spatial abilities. This issue is considered in the human-computer interface field and has been addressed with alternate display/navigation schemes. Our paper presents a fundamentally graphics based approach to this 'lost in hyperspace' problem. Specifically, we propose that spatial display of files is not sufficient to engage our visual skills; scenery (distinctive visual appearance) is needed as well. While scenery (in the form of custom icon assignments) is already possible in current operating systems, few if any users take the time to manually assign icons to all their files. As such, our proposal is to generate visually distinctive icons ("VisualIDs") automatically, while allowing the user to replace the icon if desired. The paper discusses psychological and conceptual issues relating to icons, visual memory, and the necessary relation of scenery to data. A particular icon generation algorithm is described; subjects using these icons in simulated file search and recall tasks show significantly improved performance with little effort. Although the incorporation of scenery in a graphical user interface will introduce many new (and interesting) design problems that cannot be addressed in this paper, we show that automatically created scenery is both beneficial and feasible.	VisualIDs: automatic distinctive icons for desktop interfaces	NA:NA:NA:NA	2018
Matthew Thorne:David Burke:Michiel van de Panne	In this paper we present a novel system for sketching the motion of a character. The process begins by sketching a character to be animated. An animated motion is then created for the character by drawing a continuous sequence of lines, arcs, and loops. These are parsed and mapped to a parameterized set of output motions that further reflect the location and timing of the input sketch. The current system supports a repertoire of 18 different types of motions in 2D and a subset of these in 3D. The system is unique in its use of a cursive motion specification, its ability to allow for fast experimentation, and its ease of use for non-experts.	Motion doodles: an interface for sketching character motion	NA:NA:NA	2018
Joseph J. LaViola, Jr.:Robert C. Zeleznik	We present mathematical sketching, a novel, pen-based, modeless gestural interaction paradigm for mathematics problem solving. Mathematical sketching derives from the familiar pencil-and-paper process of drawing supporting diagrams to facilitate the formulation of mathematical expressions; however, with a mathematical sketch, users can also leverage their physical intuition by watching their hand-drawn diagrams animate in response to continuous or discrete parameter changes in their written formulas. Diagram animation is driven by implicit associations that are inferred, either automatically or with gestural guidance, from mathematical expressions, diagram labels, and drawing elements. The modeless nature of mathematical sketching enables users to switch freely between modifying diagrams or expressions and viewing animations. Mathematical sketching can also support computational tools for graphing, manipulating and solving equations; initial feedback from a small user group of our mathematical sketching prototype application, MathPad2, suggests that it has the potential to be a powerful tool for mathematical problem solving and visualization.	MathPad2: a system for the creation and exploration of mathematical sketches	NA:NA	2018
Doug L. James	NA	Session details: Smoke, water & goop	NA	2018
Raanan Fattal:Dani Lischinski	In this paper we present a new method for efficiently controlling animated smoke. Given a sequence of target smoke states, our method generates a smoke simulation in which the smoke is driven towards each of these targets in turn, while exhibiting natural-looking interesting smoke-like behavior. This control is made possible by two new terms that we add to the standard flow equations: (i) a driving force term that causes the fluid to carry the smoke towards a particular target, and (ii) a smoke gathering term that prevents the smoke from diffusing too much. These terms are explicitly defined by the instantaneous state of the system at each simulation timestep. Thus, no expensive optimization is required, allowing complex smoke animations to be generated with very little additional cost compared to ordinary flow simulations.	Target-driven smoke animation	NA:NA	2018
Antoine McNamara:Adrien Treuille:Zoran Popović:Jos Stam	We describe a novel method for controlling physics-based fluid simulations through gradient-based nonlinear optimization. Using a technique known as the adjoint method, derivatives can be computed efficiently, even for large 3D simulations with millions of control parameters. In addition, we introduce the first method for the full control of free-surface liquids. We show how to compute adjoint derivatives through each step of the simulation, including the fast marching algorithm, and describe a new set of control parameters specifically designed for liquids.	Fluid control using the adjoint method	NA:NA:NA:NA	2018
Frank Losasso:Frédéric Gibou:Ron Fedkiw	We present a method for simulating water and smoke on an unrestricted octree data structure exploiting mesh refinement techniques to capture the small scale visual detail. We propose a new technique for discretizing the Poisson equation on this octree grid. The resulting linear system is symmetric positive definite enabling the use of fast solution methods such as preconditioned conjugate gradients, whereas the standard approximation to the Poisson equation on an octree grid results in a non-symmetric linear system which is more computationally challenging to invert. The semi-Lagrangian characteristic tracing technique is used to advect the velocity, smoke density, and even the level set making implementation on an octree straightforward. In the case of smoke, we have multiple refinement criteria including object boundaries, optical depth, and vorticity concentration. In the case of water, we refine near the interface as determined by the zero isocontour of the level set function.	Simulating water and smoke with an octree data structure	NA:NA:NA	2018
Tolga G. Goktekin:Adam W. Bargteil:James F. O'Brien	This paper describes a technique for animating the behavior of viscoelastic fluids, such as mucus, liquid soap, pudding, toothpaste, or clay, that exhibit a combination of both fluid and solid characteristics. The technique builds upon prior Eulerian methods for animating incompressible fluids with free surfaces by including additional elastic terms in the basic Navier-Stokes equations. The elastic terms are computed by integrating and advecting strain-rate throughout the fluid. Transition from elastic resistance to viscous flow is controlled by von Mises's yield condition, and subsequent behavior is then governed by a quasi-linear plasticity model.	A method for animating viscoelastic fluids	NA:NA:NA	2018
Kavita Bala	NA	Session details: Lighting & sampling	NA	2018
Eric Tabellion:Arnauld Lamorlette	Lighting models used in the production of computer generated feature animation have to be flexible, easy to control, and efficient to compute. Global illumination techniques do not lend themselves easily to flexibility, ease of use, or speed, and have remained out of reach thus far for the vast majority of images generated in this context. This paper describes the implementation and integration of indirect illumination within a feature animation production renderer. For efficiency reasons, we choose to partially solve the rendering equation. We explain how this compromise allows us to speed-up final gathering calculations and reduce noise. We describe an efficient ray tracing strategy and its integration with a micro-polygon based scan line renderer supporting displacement mapping and programmable shaders. We combine a modified irradiance gradient caching technique with an approximate lighting model that enhances caching coherence and provides good scalability to render complex scenes into high-resolution images suitable for film. We describe the tools that are made available to the artists to control indirect lighting in final renders. We show that our approach provides an efficient solution, easy to art direct, that allows animators to enhance considerably the quality of images generated for a large category of production work.	An approximate global illumination system for computer generated films	NA:NA	2018
Ren Ng:Ravi Ramamoorthi:Pat Hanrahan	This paper focuses on efficient rendering based on pre-computed light transport, with realistic materials and shadows under all-frequency direct lighting such an environment maps. The basic difficulty is representation and computation in the 6D space of light direction, view direction, and surface position. While image-based and synthetic methods for real-time rendering have been proposed, they do not scale to high sampling rates with variation of both lighting and viewpoint. Current approaches are therefore limited to lower dimensionality (only lighting or viewpoint variation, not both) or lower sampling rates (low frequency lighting and materials). We propose a new mathematical and computational analysis of pre-computed light transport. We use factored forms, separately pre-computing and representing visibility and material properties. Rendering then requires computing triple product integrals at each vertex, involving the lighting, visibility and BRDF. Our main contribution is a general analysis of these triple product integrals, which are likely to have broad applicability in computer graphics and numerical analysis. We first determine the computational complexity in a number of bases like point samples, spherical harmonics and wavelets. We then give efficient linear and sublinear-time algorithms for Haar wavelets, incorporating non-linear wavelet approximation of lighting and BRDFs. Practically, we demonstrate rendering of images under new lighting and viewing conditions in a few seconds, significantly faster than previous techniques.	Triple product wavelet integrals for all-frequency relighting	NA:NA:NA	2018
Victor Ostromoukhov:Charles Donohue:Pierre-Marc Jodoin	This paper presents a novel method for efficiently generating a good sampling pattern given an importance density over a 2D domain. A Penrose tiling is hierarchically subdivided creating a sufficiently large number of sample points. These points are numbered using the Fibonacci number system, and these numbers are used to threshold the samples against the local value of the importance density. Pre-computed correction vectors, obtained using relaxation, are used to improve the spectral characteristics of the sampling pattern. The technique is deterministic and very fast; the sampling time grows linearly with the required number of samples. We illustrate our technique with importance-based environment mapping, but the technique is versatile enough to be used in a large variety of computer graphics applications, such as light transport calculations, digital halftoning, geometry processing, and various rendering techniques.	Fast hierarchical importance sampling with blue noise properties	NA:NA:NA	2018
Jason Lawrence:Szymon Rusinkiewicz:Ravi Ramamoorthi	High-quality Monte Carlo image synthesis requires the ability to importance sample realistic BRDF models. However, analytic sampling algorithms exist only for the Phong model and its derivatives such as Lafortune and Blinn-Phong. This paper demonstrates an importance sampling technique for a wide range of BRDFs, including complex analytic models such as Cook-Torrance and measured materials, which are being increasingly used for realistic image synthesis. Our approach is based on a compact factored representation of the BRDF that is optimized for sampling. We show that our algorithm consistently offers better efficiency than alternatives that involve fitting and sampling a Lafortune or Blinn-Phong lobe, and is more compact than sampling strategies based on tabulating the full BRDF. We are able to efficiently create images involving multiple measured and analytic BRDFs, under both complex direct lighting and global illumination.	Efficient BRDF importance sampling using a factored representation	NA:NA:NA	2018
Nancy Pollard	NA	Session details: Data driven character animation	NA	2018
Matthew Stone:Doug DeCarlo:Insuk Oh:Christian Rodriguez:Adrian Stere:Alyssa Lees:Chris Bregler	We describe a method for using a database of recorded speech and captured motion to create an animated conversational character. People's utterances are composed of short, clearly-delimited phrases; in each phrase, gesture and speech go together meaningfully and synchronize at a common point of maximum emphasis. We develop tools for collecting and managing performance data that exploit this structure. The tools help create scripts for performers, help annotate and segment performance data, and structure specific messages for characters to use within application contexts. Our animations then reproduce this structure. They recombine motion samples with new speech samples to recreate coherent phrases, and blend segments of speech and motion together phrase-by-phrase into extended utterances. By framing problems for utterance generation and synthesis so that they can draw closely on a talented performance, our techniques support the rapid construction of animated characters with rich and appropriate expression.	Speaking with hands: creating animated conversational characters from recordings of human performance	NA:NA:NA:NA:NA:NA:NA	2018
Alla Safonova:Jessica K. Hodgins:Nancy S. Pollard	Optimization is an appealing way to compute the motion of an animated character because it allows the user to specify the desired motion in a sparse, intuitive way. The difficulty of solving this problem for complex characters such as humans is due in part to the high dimensionality of the search space. The dimensionality is an artifact of the problem representation because most dynamic human behaviors are intrinsically low dimensional with, for example, legs and arms operating in a coordinated way. We describe a method that exploits this observation to create an optimization problem that is easier to solve. Our method utilizes an existing motion capture database to find a low-dimensional space that captures the properties of the desired behavior. We show that when the optimization problem is solved within this low-dimensional subspace, a sparse sketch can be used as an initial guess and full physics constraints can be enabled. We demonstrate the power of our approach with examples of forward, vertical, and turning jumps; with running and walking; and with several acrobatic flips.	Synthesizing physically realistic human motion in low-dimensional, behavior-specific spaces	NA:NA:NA	2018
Keith Grochow:Steven L. Martin:Aaron Hertzmann:Zoran Popović	This paper presents an inverse kinematics system based on a learned model of human poses. Given a set of constraints, our system can produce the most likely pose satisfying those constraints, in real-time. Training the model on different input data leads to different styles of IK. The model is represented as a probability distribution over the space of all possible poses. This means that our IK system can generate any pose, but prefers poses that are most similar to the space of poses in the training data. We represent the probability with a novel model called a Scaled Gaussian Process Latent Variable Model. The parameters of the model are all learned automatically; no manual tuning is required for the learning component of the system. We additionally describe a novel procedure for interpolating between styles.Our style-based IK can replace conventional IK, wherever it is used in computer animation and computer vision. We demonstrate our system in the context of a number of applications: interactive character posing, trajectory keyframing, real-time motion capture with missing markers, and posing from a 2D image.	Style-based inverse kinematics	NA:NA:NA:NA	2018
Katsu Yamane:James J. Kuffner:Jessica K. Hodgins	Even such simple tasks as placing a box on a shelf are difficult to animate, because the animator must carefully position the character to satisfy geometric and balance constraints while creating motion to perform the task with a natural-looking style. In this paper, we explore an approach for animating characters manipulating objects that combines the power of path planning with the domain knowledge inherent in data-driven, constraint-based inverse kinematics. A path planner is used to find a motion for the object such that the corresponding poses of the character satisfy geometric, kinematic, and posture constraints. The inverse kinematics computation of the character's pose resolves redundancy by biasing the solution toward natural-looking poses extracted from a database of captured motions. Having this database greatly helps to increase the quality of the output motion. The computed path is converted to a motion trajectory using a model of the velocity profile. We demonstrate the effectiveness of the algorithm by generating animations across a wide range of scenarios that cover variations in the geometric, kinematic, and dynamic models of the character, the manipulated object, and obstacles in the scene.	Synthesizing animations of human manipulation tasks	NA:NA:NA	2018
Leonard McMillan	NA	Session details: Shape & motion	NA	2018
Christian Theobalt:Irene Albrecht:Jörg Haber:Marcus Magnor:Hans-Peter Seidel	Athletes and coaches in most professional sports make use of high-tech equipment to analyze and, subsequently, improve the athlete's performance. High-speed video cameras are employed, for instance, to record the swing of a golf club or a tennis racket, the movement of the feet while running, and the body motion in apparatus gymnastics. High-tech and high-speed equipment, however, usually implies high-cost as well. In this paper, we present a passive optical approach to capture high-speed motion using multi-exposure images obtained with low-cost commodity still cameras and a stroboscope. The recorded motion remains completely undisturbed by the motion capture process. We apply our approach to capture the motion of hand and ball for a variety of baseball pitches and present algorithms to automatically track the position, velocity, rotation axis, and spin of the ball along its trajectory. To demonstrate the validity of our setup and algorithms, we analyze the consistency of our measurements with a physically based model that predicts the trajectory of a spinning baseball. Our approach can be applied to capture a wide variety of other high-speed objects and activities such as golfing, bowling, or tennis for visualization as well as analysis purposes.	Pitching a baseball: tracking high-speed motion with multi-exposure images	NA:NA:NA:NA:NA	2018
Li Zhang:Noah Snavely:Brian Curless:Steven M. Seitz	We present an end-to-end system that goes from video sequences to high resolution, editable, dynamically controllable face models. The capture system employs synchronized video cameras and structured light projectors to record videos of a moving face from multiple viewpoints. A novel spacetime stereo algorithm is introduced to compute depth maps accurately and overcome over-fitting deficiencies in prior work. A new template fitting and tracking procedure fills in missing data and yields point correspondence across the entire sequence without using markers. We demonstrate a data-driven, interactive method for inverse kinematics that draws on the large set of fitted templates and allows for posing new expressions by dragging surface points directly. Finally, we describe new tools that model the dynamics in the input sequence to enable new animations, created via key-framing or texture-synthesis techniques.	Spacetime faces: high resolution capture for modeling and animation	NA:NA:NA:NA	2018
Lucas Kovar:Michael Gleicher	Large motion data sets often contain many variants of the same kind of motion, but without appropriate tools it is difficult to fully exploit this fact. This paper provides automated methods for identifying logically similar motions in a data set and using them to build a continuous and intuitively parameterized space of motions. To find logically similar motions that are numerically dissimilar, our search method employs a novel distance metric to find "close" motions and then uses them as intermediaries to find more distant motions. Search queries are answered at interactive speeds through a precomputation that compactly represents all possibly similar motion segments. Once a set of related motions has been extracted, we automatically register them and apply blending techniques to create a continuous space of motions. Given a function that defines relevant motion parameters, we present a method for extracting motions from this space that accurately possess new parameters requested by the user. Our algorithm extends previous work by explicitly constraining blend weights to reasonable values and having a run-time cost that is nearly independent of the number of example motions. We present experimental results on a test data set of 37,000 frames, or about ten minutes of motion sampled at 60 Hz.	Automated extraction and parameterization of motions in large data sets	NA:NA	2018
Jason Harrison:Ronald A. Rensink:Michiel van de Panne	In this paper we examine to what extent the lengths of the links in an animated articulated-figure can be changed without the viewer being aware of the change. This is investigated in terms of a framework that emphasizes the role of attention in visual perception. We conducted a set of five experiments to establish bounds for the sensitivity to changes in length as a function of several parameters and the amount of attention available. We found that while length changes of 3% can be perceived when the relevant links are given full attention, changes of over 20% can go unnoticed when attention is not focused in this way. These results provide general guidelines for algorithms that produce or process character motion data and also bring to light some of the potential gains that stand to be achieved with attention-based algorithms.	Obscuring length changes during animated motion	NA:NA:NA	2018
Irfan Essa	NA	Session details: Video-based rendering	NA	2018
Jue Wang:Yingqing Xu:Heung-Yeung Shum:Michael F. Cohen	We describe a system for transforming an input video into a highly abstracted, spatio-temporally coherent cartoon animation with a range of styles. To achieve this, we treat video as a space-time volume of image data. We have developed an anisotropic kernel mean shift technique to segment the video data into contiguous volumes. These provide a simple cartoon style in themselves, but more importantly provide the capability to semi-automatically rotoscope semantically meaningful regions.In our system, the user simply outlines objects on keyframes. A mean shift guided interpolation algorithm is then employed to create three dimensional semantic regions by interpolation between the keyframes, while maintaining smooth trajectories along the time dimension. These regions provide the basis for creating smooth two dimensional edge sheets and stroke sheets embedded within the spatio-temporal video volume. The regions, edge sheets, and stroke sheets are rendered by slicing them at particular times. A variety of styles of rendering are shown. The temporal coherence provided by the smoothed semantic regions and sheets results in a temporally consistent non-photorealistic appearance.	Video tooning	NA:NA:NA:NA	2018
Aseem Agarwala:Aaron Hertzmann:David H. Salesin:Steven M. Seitz	We describe a new approach to rotoscoping --- the process of tracking contours in a video sequence --- that combines computer vision with user interaction. In order to track contours in video, the user specifies curves in two or more frames; these curves are used as keyframes by a computer-vision-based tracking algorithm. The user may interactively refine the curves and then restart the tracking algorithm. Combining computer vision with user interaction allows our system to track any sequence with significantly less effort than interpolation-based systems --- and with better reliability than "pure" computer vision systems. Our tracking algorithm is cast as a spacetime optimization problem that solves for time-varying curve shapes based on an input video sequence and user-specified constraints. We demonstrate our system with several rotoscoped examples. Additionally, we show how these rotoscoped contours can be used to help create cartoon animation by attaching user-drawn strokes to the tracked contours.	Keyframe-based tracking for rotoscoping and animation	NA:NA:NA:NA	2018
Peter Sand:Seth Teller	This paper describes a method for bringing two videos (recorded at different times) into spatiotemporal alignment, then comparing and combining corresponding pixels for applications such as background subtraction, compositing, and increasing dynamic range. We align a pair of videos by searching for frames that best match according to a robust image registration process. This process uses locally weighted regression to interpolate and extrapolate high-likelihood image correspondences, allowing new correspondences to be discovered and refined. Image regions that cannot be matched are detected and ignored, providing robustness to changes in scene content and lighting, which allows a variety of new applications.	Video matching	NA:NA	2018
C. Lawrence Zitnick:Sing Bing Kang:Matthew Uyttendaele:Simon Winder:Richard Szeliski	The ability to interactively control viewpoint while watching a video is an exciting application of image-based rendering. The goal of our work is to render dynamic scenes with interactive viewpoint control using a relatively small number of video cameras. In this paper, we show how high-quality video-based rendering of dynamic scenes can be accomplished using multiple synchronized video streams combined with novel image-based modeling and rendering algorithms. Once these video streams have been processed, we can synthesize any intermediate view between cameras at any time, with the potential for space-time manipulation.In our approach, we first use a novel color segmentation-based stereo algorithm to generate high-quality photoconsistent correspondences across all camera views. Mattes for areas near depth discontinuities are then automatically extracted to reduce artifacts during view synthesis. Finally, a novel temporal two-layer compressed representation that handles matting is developed for rendering at interactive rates.	High-quality video view interpolation using a layered representation	NA:NA:NA:NA:NA	2018
Nina Amenta	NA	Session details: Shape analysis	NA	2018
Yutaka Ohtake:Alexander Belyaev:Hans-Peter Seidel	We propose a simple and effective method for detecting view-and scale-independent ridge-valley lines defined via first- and second-order curvature derivatives on shapes approximated by dense triangle meshes. A high-quality estimation of high-order surface derivatives is achieved by combining multi-level implicit surface fitting and finite difference approximations. We demonstrate that the ridges and valleys are geometrically and perceptually salient surface features, and, therefore, can be potentially used for shape recognition, coding, and quality evaluation purposes.	Ridge-valley lines on meshes via implicit surface fitting	NA:NA:NA	2018
Xinlai Ni:Michael Garland:John C. Hart	Morse theory reveals the topological structure of a shape based on the critical points of a real function over the shape. A poor choice of this real function can lead to a complex configuration of an unnecessarily high number of critical points. This paper solves a relaxed form of Laplace's equation to find a "fair" Morse function with a user-controlled number and configuration of critical points. When the number is minimal, the resulting Morse complex cuts the shape into a disk. Specifying additional critical points at surface features yields a base domain that better represents the geometry and shares the same topology as the original mesh, and can also cluster a mesh into approximately developable patches. We make Morse theory on meshes more robust with teflon saddles and flat edge collapses, and devise a new "intermediate value propagation" multigrid solver for finding fair Morse functions that runs in provably linear time.	Fair morse functions for extracting the topological structure of a surface mesh	NA:NA:NA	2018
Michael Kazhdan:Thomas Funkhouser:Szymon Rusinkiewicz	With recent improvements in methods for the acquisition and rendering of 3D models, the need for retrieval of models has gained prominence in the graphics and vision communities. A variety of methods have been proposed that enable the efficient querying of model repositories for a desired 3D shape. Many of these methods use a 3D model as a query and attempt to retrieve models from the database that have a similar shape.In this paper we consider the implications of anisotropy on the shape matching paradigm. In particular, we propose a novel method for matching 3D models that factors the shape matching equation as the disjoint outer product of anisotropy and geometric comparisons. We provide a general method for computing the factored similarity metric and show how this approach can be applied to improve the matching performance of many existing shape matching methods.	Shape matching and anisotropy	NA:NA:NA	2018
Mark Pauly	NA	Session details: Interactive modeling	NA	2018
Mario Botsch:Leif Kobbelt	We present a freeform modeling framework for unstructured triangle meshes which is based on constraint shape optimization. The goal is to simplify the user interaction even for quite complex freeform or multiresolution modifications. The user first sets various boundary constraints to define a custom tailored (abstract) basis function which is adjusted to a given design task. The actual modification is then controlled by moving one single 9-dof manipulator object. The technique can handle arbitrary support regions and piecewise boundary conditions with smoothness ranging continuously from C0 to C2. To more naturally adapt the modification to the shape of the support region, the deformed surface can be tuned to bend with anisotropic stiffness. We are able to achieve real-time response in an interactive design session even for complex meshes by precomputing a set of scalar-valued basis functions that correspond to the degrees of freedom of the manipulator by which the user controls the modification.	An intuitive framework for real-time freeform modeling	NA:NA	2018
Jianbo Peng:Daniel Kristjansson:Denis Zorin	Volume textures aligned with a surface can be used to add topologically complex geometric detail to objects in an efficient way, while retaining an underlying simple surface structure.Adding a volume texture to a surface requires more than a conventional two-dimensional parameterization: a part of the space surrounding the surface has to be parameterized. Another problem with using volume textures for adding geometric detail is the difficulty in rendering implicitly represented surfaces, especially when they are changed interactively.In this paper we present algorithms for constructing and rendering volume-textured surfaces. We demonstrate a number of interactive operations that these algorithms enable.	Interactive modeling of topologically complex geometric detail	NA:NA:NA	2018
Yizhou Yu:Kun Zhou:Dong Xu:Xiaohan Shi:Hujun Bao:Baining Guo:Heung-Yeung Shum	In this paper, we introduce a novel approach to mesh editing with the Poisson equation as the theoretical foundation. The most distinctive feature of this approach is that it modifies the original mesh geometry implicitly through gradient field manipulation. Our approach can produce desirable and pleasing results for both global and local editing operations, such as deformation, object merging, and smoothing. With the help from a few novel interactive tools, these operations can be performed conveniently with a small amount of user interaction. Our technique has three key components, a basic mesh solver based on the Poisson equation, a gradient field manipulation scheme using local transforms, and a generalized boundary condition representation based on local frames. Experimental results indicate that our framework can outperform previous related mesh editing techniques.	Mesh editing with poisson-based gradient field manipulation	NA:NA:NA:NA:NA:NA:NA	2018
Thomas Funkhouser:Michael Kazhdan:Philip Shilane:Patrick Min:William Kiefer:Ayellet Tal:Szymon Rusinkiewicz:David Dobkin	In this paper, we investigate a data-driven synthesis approach to constructing 3D geometric surface models. We provide methods with which a user can search a large database of 3D meshes to find parts of interest, cut the desired parts out of the meshes with intelligent scissoring, and composite them together in different ways to form new objects. The main benefit of this approach is that it is both easy to learn and able to produce highly detailed geometric models -- the conceptual design for new models comes from the user, while the geometric details come from examples in the database. The focus of the paper is on the main research issues motivated by the proposed approach: (1) interactive segmentation of 3D surfaces, (2) shape-based search to find 3D models with parts matching a query, and (3) composition of parts to form new models. We provide new research contributions on all three topics and incorporate them into a prototype modeling system. Experience with our prototype system indicates that it allows untrained users to create interesting and detailed 3D models.	Modeling by example	NA:NA:NA:NA:NA:NA:NA:NA	2018
Rick Szeliski	NA	Session details: Flash & color	NA	2018
Georg Petschnigg:Richard Szeliski:Maneesh Agrawala:Michael Cohen:Hugues Hoppe:Kentaro Toyama	Digital photography has made it possible to quickly and easily take a pair of images of low-light environments: one with flash to capture detail and one without flash to capture ambient illumination. We present a variety of applications that analyze and combine the strengths of such flash/no-flash image pairs. Our applications include denoising and detail transfer (to merge the ambient qualities of the no-flash image with the high-frequency flash detail), white-balancing (to change the color tone of the ambient image), continuous flash (to interactively adjust flash intensity), and red-eye removal (to repair artifacts in the flash image). We demonstrate how these applications can synthesize new images that are of higher quality than either of the originals.	Digital photography with flash and no-flash image pairs	NA:NA:NA:NA:NA:NA	2018
Elmar Eisemann:Frédo Durand	We enhance photographs shot in dark environments by combining a picture taken with the available light and one taken with the flash. We preserve the ambiance of the original lighting and insert the sharpness from the flash image. We use the bilateral filter to decompose the images into detail and large scale. We reconstruct the image using the large scale of the available lighting and the detail of the flash. We detect and correct flash shadows. This combines the advantages of available illumination and flash photography.	Flash photography enhancement via intrinsic relighting	NA:NA	2018
Ramesh Raskar:Kar-Han Tan:Rogerio Feris:Jingyi Yu:Matthew Turk	We present a non-photorealistic rendering approach to capture and convey shape features of real-world scenes. We use a camera with multiple flashes that are strategically positioned to cast shadows along depth discontinuities in the scene. The projective-geometric relationship of the camera-flash setup is then exploited to detect depth discontinuities and distinguish them from intensity edges due to material discontinuities.We introduce depiction methods that utilize the detected edge features to generate stylized static and animated images. We can highlight the detected features, suppress unnecessary details or combine features from multiple images. The resulting images more clearly convey the 3D structure of the imaged scenes.We take a very different approach to capturing geometric features of a scene than traditional approaches that require reconstructing a 3D model. This results in a method that is both surprisingly simple and computationally efficient. The entire hardware/software setup can conceivably be packaged into a self-contained device no larger than existing digital cameras.	Non-photorealistic camera: depth edge detection and stylized rendering using multi-flash imaging	NA:NA:NA:NA:NA	2018
Anat Levin:Dani Lischinski:Yair Weiss	Colorization is a computer-assisted process of adding color to a monochrome image or movie. The process typically involves segmenting images into regions and tracking these regions across image sequences. Neither of these tasks can be performed reliably in practice; consequently, colorization requires considerable user intervention and remains a tedious, time-consuming, and expensive task.In this paper we present a simple colorization method that requires neither precise image segmentation, nor accurate region tracking. Our method is based on a simple premise; neighboring pixels in space-time that have similar intensities should have similar colors. We formalize this premise using a quadratic cost function and obtain an optimization problem that can be solved efficiently using standard techniques. In our approach an artist only needs to annotate the image with a few color scribbles, and the indicated colors are automatically propagated in both space and time to produce a fully colorized image or sequence. We demonstrate that high quality colorizations of stills and movie clips may be obtained from a relatively modest amount of user input.	Colorization using optimization	NA:NA:NA	2018
Marcus Gross	NA	Session details: Capture from images	NA	2018
David Koller:Michael Turitzin:Marc Levoy:Marco Tarini:Giuseppe Croccia:Paolo Cignoni:Roberto Scopigno	Valuable 3D graphical models, such as high-resolution digital scans of cultural heritage objects, may require protection to prevent piracy or misuse, while still allowing for interactive display and manipulation by a widespread audience. We have investigated techniques for protecting 3D graphics content, and we have developed a remote rendering system suitable for sharing archives of 3D models while protecting the 3D geometry from unauthorized extraction. The system consists of a 3D viewer client that includes low-resolution versions of the 3D models, and a rendering server that renders and returns images of high-resolution models according to client requests. The server implements a number of defenses to guard against 3D reconstruction attacks, such as monitoring and limiting request streams, and slightly perturbing and distorting the rendered images. We consider several possible types of reconstruction attacks on such a rendering server, and we examine how these attacks can be defended against without excessively compromising the interactive experience for non-malicious users.	Protected interactive 3D graphics via remote rendering	NA:NA:NA:NA:NA:NA:NA	2018
Ko Nishino:Shree K. Nayar	The combination of the cornea of an eye and a camera viewing the eye form a catadioptric (mirror + lens) imaging system with a very wide field of view. We present a detailed analysis of the characteristics of this corneal imaging system. Anatomical studies have shown that the shape of a normal cornea (without major defects) can be approximated with an ellipsoid of fixed eccentricity and size. Using this shape model, we can determine the geometric parameters of the corneal imaging system from the image. Then, an environment map of the scene with a large field of view can be computed from the image. The environment map represents the illumination of the scene with respect to the eye. This use of an eye as a natural light probe is advantageous in many relighting scenarios. For instance, it enables us to insert virtual objects into an image such that they appear consistent with the illumination of the scene. The eye is a particularly useful probe when relighting faces. It allows us to reconstruct the geometry of a face by simply waving a light source in front of the face. Finally, in the case of an already captured image, eyes could be the only direct means for obtaining illumination information. We show how illumination computed from eyes can be used to replace a face in an image with another one. We believe that the eye not only serves as a useful tool for relighting but also makes relighting possible in situations where current approaches are hard to use.	Eyes for relighting	NA:NA	2018
Sylvain Paris:Hector M. Briceño:François X. Sillion	Hair is a major feature of digital characters. Unfortunately, it has a complex geometry which challenges standard modeling tools. Some dedicated techniques exist, but creating a realistic hairstyle still takes hours. Complementary to user-driven methods, we here propose an image-based approach to capture the geometry of hair.The novelty of this work is that we draw information from the scattering properties of the hair that are normally considered a hindrance. To do so, we analyze image sequences from a fixed camera with a moving light source. We first introduce a novel method to compute the image orientation of the hairs from their anisotropic behavior. This method is proven to subsume and extend existing work while improving accuracy. This image orientation is then raised into a 3D orientation by analyzing the light reflected by the hair fibers. This part relies on minimal assumptions that have been proven correct in previous work.Finally, we show how to use several such image sequences to reconstruct the complete hair geometry of a real person. Results are shown to illustrate the fidelity of the captured geometry to the original hair. This technique paves the way for a new approach to digital hair generation.	Capture of hair geometry from multiple images	NA:NA:NA	2018
Alex Reche-Martinez:Ignacio Martin:George Drettakis	Reconstructing and rendering trees is a challenging problem due to the geometric complexity involved, and the inherent difficulties of capture. In this paper we propose a volumetric approach to capture and render trees with relatively sparse foliage. Photographs of such trees typically have single pixels containing the blended projection of numerous leaves/branches and background. We show how we estimate opacity values on a recursive grid, based on alphamattes extracted from a small number of calibrated photographs of a tree. This data structure is then used to render billboards attached to the centers of the grid cells. Each billboard is assigned a set of view-dependent textures corresponding to each input view. These textures are generated by approximating coverage masks based on opacity and depth from the camera. Rendering is performed using a view-dependent texturing algorithm. The resulting volumetric tree structure has low polygon count, permitting interactive rendering of realistic 3D trees. We illustrate the implementation of our system on several different real trees, and show that we can insert the resulting model in virtual scenes.	Volumetric reconstruction and interactive rendering of trees from photographs	NA:NA:NA	2018
Eric Saund:David Fleet:Daniel Larner:James Mahoney	This extended abstract reprises our UIST '03 paper on "Perceptually-Supported Image Editing of Text and Graphics." We introduce a novel image editing program, called ScanScribe, that emphasizes easy selection and manipulation of material found in informal, casual documents such as sketches, handwritten notes, whiteboard images, screen snapshots, and scanned documents.	Perceptually-supported image editing of text and graphics	NA:NA:NA:NA	2018
Xiang Cao:Ravin Balakrishnan	A passive wand tracked in 3D using computer vision techniques is explored as a new input mechanism for interacting with large displays. We demonstrate a variety of interaction techniques and visual widgets that exploit the affordances of the wand, resulting in an effective interface for large scale display interaction. The lack of any buttons or other electronics on the wand presents a challenge that we address by developing a set of gestures and postures to track and enable command input.	VisionWand: interaction techniques for large displays using a passive wand tracked in 3D	NA:NA	2018
James Fogarty:Scott E. Hudson	Recent work is beginning to reveal the potential of numerical optimization as an approach to generating interfaces and displays. Optimization-based approaches can often allow a mix of independent goals and constraints to be blended in ways that are difficult to describe algorithmically. While optimization-based techniques appear to offer several potential advantages, further research in this area is hampered by the lack of appropriate tools. Optimization toolkits do exist, but they typically require substantial specialized knowledge because they have been designed for traditional optimization problems.GADGET is an experimental toolkit to support optimization as an approach to interface and display generation. GADGET provides three core abstractions, initializers, iterations, and evaluations. An initializer creates an initial solution to be optimized, based on an existing algorithm or randomly. Iterations are responsible for transforming one potential solution into another, typically using methods that are at least partially random. Finally, evaluations are used for judging the different notions of goodness in a solution. Together with a evaluation standardization framework, support for generic properties integrated with an efficient lazy evaluation framework, and a library of reusable iterations and evaluations, the abstractions provided by GADGET simplify the development of optimization-based approaches to interface and display generation.	GADGET: a toolkit for optimization-based approaches to interface and display generation	NA:NA	2018
Martin Hachet:Pascal Guitton:Patrick Reuter:Florence Tyndiuk	We present the CAT (Control Action Table), a 6 degrees of freedom freestanding input device designed for interaction with Virtual Environments displayed on huge screens. Both isotonic and isometric sensing modes allow the user to easily and efficiently perform 3D interaction techniques. A 2D tablet fixed on the tabletop allows them to perform accurate 2D interaction techniques.	The CAT for efficient 2D and 3D interaction as an alternative to mouse adaptations	NA:NA:NA:NA	2018
Hajime Nagahara:Yasushi Yagi:Masahiko Yachida	Many applications have used a head-mounted display (HMD), such as in virtual and mixed realities and telepresence. However, the field of view (FOV) of commercial HMD systems is too narrow for feeling immersion. In this paper, we propose a super-wide field of view head-mounted display consisting of an ellipsoidal mirror and a hyperboloidal curved mirror. The horizontal FOV of the proposed HMD is 180 degrees and includes the peripheral vision of humans. It increases the reality and immersion for users.	Super wide viewer using catadioptrical optics	NA:NA:NA	2018
Jack Tumblin	NA	Session details: HDR and perception	NA	2018
Rafal Mantiuk:Grzegorz Krawczyk:Karol Myszkowski:Hans-Peter Seidel	Due to rapid technological progress in high dynamic range (HDR) video capture and display, the efficient storage and transmission of such data is crucial for the completeness of any HDR imaging pipeline. We propose a new approach for inter-frame encoding of HDR video, which is embedded in the well-established MPEG-4 video compression standard. The key component of our technique is luminance quantization that is optimized for the contrast threshold perception in the human visual system. The quantization scheme requires only 10--11 bits to encode 12 orders of magnitude of visible luminance range and does not lead to perceivable contouring artifacts. Besides video encoding, the proposed quantization provides perceptually-optimized luminance sampling for fast implementation of any global tone mapping operator using a lookup table. To improve the quality of synthetic video sequences, we introduce a coding scheme for discrete cosine transform (DCT) blocks with high contrast. We demonstrate the capabilities of HDR video in a player, which enables decoding, tone mapping, and applying post-processing effects in real-time. The tone mapping algorithm as well as its parameters can be changed interactively while the video is playing. We can simulate post-processing effects such as glare, night vision, and motion blur, which appear very realistic due to the usage of HDR data.	Perception-motivated high dynamic range video encoding	NA:NA:NA:NA	2018
William A. Stokes:James A. Ferwerda:Bruce Walter:Donald P. Greenberg	In this paper we introduce a new perceptual metric for efficient, high quality, global illumination rendering. The metric is based on a rendering-by-components framework in which the direct, and indirect diffuse, glossy, and specular light transport paths are separately computed and then composited to produce an image. The metric predicts the perceptual importances of the computationally expensive indirect illumination components with respect to image quality. To develop the metric we conducted a series of psychophysical experiments in which we measured and modeled the perceptual importances of the components. An important property of this new metric is that it predicts component importances from inexpensive estimates of the reflectance properties of a scene, and therefore adds negligible overhead to the rendering process. This perceptual metric should enable the development of an important new class of efficient global-illumination rendering systems that can intelligently allocate limited computational resources, to provide high quality images at interactive rates.	Perceptual illumination components: a new approach to efficient, high quality global illumination rendering	NA:NA:NA:NA	2018
Benjamin Watson:Neff Walker:Larry F. Hodges	Level of detail (LOD) is widely used to control visual feedback in interactive applications. LOD control is typically based on perception at threshold -- the conditions in which a stimulus first becomes perceivable. Yet most LOD manipulations are quite perceivable and occur well above threshold. Moreover, research shows that supra-threshold perception differs drastically from perception at threshold. In that case, should supra-threshold LOD control also differ from LOD control at threshold?In two experiments, we examine supra-threshold LOD control in the visual periphery and find that indeed, it should differ drastically from LOD control at threshold. Specifically, we find that LOD must support a task-dependent level of reliable perceptibility. Above that level, perceptibility of LOD control manipulations should be minimized, and detail contrast is a better predictor of perceptibility than detail size. Below that level, perceptibility must be maximized, and LOD should be improved as eccentricity rises or contrast drops. This directly contradicts prevailing threshold-based LOD control schemes, and strongly suggests a reexamination of LOD control for foveal display.	Supra-threshold control of peripheral LOD	NA:NA:NA	2018
Helge Seetzen:Wolfgang Heidrich:Wolfgang Stuerzlinger:Greg Ward:Lorne Whitehead:Matthew Trentacoste:Abhijeet Ghosh:Andrejs Vorozcovs	The dynamic range of many real-world environments exceeds the capabilities of current display technology by several orders of magnitude. In this paper we discuss the design of two different display systems that are capable of displaying images with a dynamic range much more similar to that encountered in the real world. The first display system is based on a combination of an LCD panel and a DLP projector, and can be built from off-the-shelf components. While this design is feasible in a lab setting, the second display system, which relies on a custom-built LED panel instead of the projector, is more suitable for usual office workspaces and commercial applications. We describe the design of both systems as well as the software issues that arise. We also discuss the advantages and disadvantages of the two designs and potential applications for both systems.	High dynamic range display systems	NA:NA:NA:NA:NA:NA:NA:NA	2018
Peter-Pike Sloan	NA	Session details: Large meshes and GPU programming	NA	2018
Frank Losasso:Hugues Hoppe	Rendering throughput has reached a level that enables a novel approach to level-of-detail (LOD) control in terrain rendering. We introduce the geometry clipmap, which caches the terrain in a set of nested regular grids centered about the viewer. The grids are stored as vertex buffers in fast video memory, and are incrementally refilled as the viewpoint moves. This simple framework provides visual continuity, uniform frame rate, complexity throttling, and graceful degradation. Moreover it allows two new exciting real-time functionalities: decompression and synthesis. Our main dataset is a 40GB height map of the United States. A compressed image pyramid reduces the size by a remarkable factor of 100, so that it fits entirely in memory. This compressed data also contributes normal maps for shading. As the viewer approaches the surface, we synthesize grid levels finer than the stored terrain using fractal noise displacement. Decompression, synthesis, and normal-map computations are incremental, thereby allowing interactive flight at 60 frames/sec.	Geometry clipmaps: terrain rendering using nested regular grids	NA:NA	2018
Ian Buck:Tim Foley:Daniel Horn:Jeremy Sugerman:Kayvon Fatahalian:Mike Houston:Pat Hanrahan	In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.	Brook for GPUs: stream computing on graphics hardware	NA:NA:NA:NA:NA:NA:NA	2018
Michael McCool:Stefanus Du Toit:Tiberiu Popa:Bryan Chan:Kevin Moule	An algebra consists of a set of objects and a set of operators that act on those objects. We treat shader programs as first-class objects and define two operators: connection and combination. Connection is functional composition: the outputs of one shader are fed into the inputs of another. Combination concatenates the input channels, output channels, and computations of two shaders. Similar operators can be used to manipulate streams and apply computational kernels expressed as shaders to streams. Connecting a shader program to a stream applies that program to all elements of the stream; combining streams concatenates the record definitions of those streams.In conjunction with an optimizing compiler, these operators can manipulate shader programs in many useful ways, including specialization, without modifying the original source code. We demonstrate these operators in Sh, a metaprogramming shading language embedded in C++.	Shader algebra	NA:NA:NA:NA:NA	2018
Paolo Cignoni:Fabio Ganovelli:Enrico Gobbetti:Fabio Marton:Federico Ponchio:Roberto Scopigno	We describe an efficient technique for out-of-core construction and accurate view-dependent visualization of very large surface models. The method uses a regular conformal hierarchy of tetrahedra to spatially partition the model. Each tetrahedral cell contains a precomputed simplified version of the original model, represented using cache coherent indexed strips for fast rendering. The representation is constructed during a fine-to-coarse simplification of the surface contained in diamonds (sets of tetrahedral cells sharing their longest edge). The construction preprocess operates out-of-core and parallelizes nicely. Appropriate boundary constraints are introduced in the simplification to ensure that all conforming selective subdivisions of the tetrahedron hierarchy lead to correctly matching surface patches. For each frame at runtime, the hierarchy is traversed coarse-to-fine to select diamonds of the appropriate resolution given the view parameters. The resulting system can interatively render high quality views of out-of-core models of hundreds of millions of triangles at over 40Hz (or 70M triangles/s) on current commodity graphics platforms.	Adaptive tetrapuzzles: efficient out-of-core construction and visualization of gigantic multiresolution polygonal models	NA:NA:NA:NA:NA:NA	2018
Hanspeter Pfister	NA	Session details: Lightfield acquisition & display	NA	2018
Kurt Akeley:Simon J. Watt:Ahna Reza Girshick:Martin S. Banks	Typical stereo displays provide incorrect focus cues because the light comes from a single surface. We describe a prototype stereo display comprising two independent fixed-viewpoint volumetric displays. Like autostereoscopic volumetric displays, fixed-viewpoint volumetric displays generate near-correct focus cues without tracking eye position, because light comes from sources at the correct focal distances. (In our prototype, from three image planes at different physical distances.) Unlike autostereoscopic volumetric displays, however, fixed-viewpoint volumetric displays retain the qualities of modern projective graphics: view-dependent lighting effects such as occlusion, specularity, and reflection are correctly depicted; modern graphics processor and 2-D display technology can be utilized; and realistic fields of view and depths of field can be implemented. While not a practical solution for general-purpose viewing, our prototype display is a proof of concept and a platform for ongoing vision research. The design, implementation, and verification of this stereo display are described, including a novel technique of filtering along visual lines using 1-D texture mapping.	A stereo display prototype with multiple focal distances	NA:NA:NA:NA	2018
Wojciech Matusik:Hanspeter Pfister	Three-dimensional TV is expected to be the next revolution in the history of television. We implemented a 3D TV prototype system with real-time acquisition, transmission, and 3D display of dynamic scenes. We developed a distributed, scalable architecture to manage the high computation and bandwidth demands. Our system consists of an array of cameras, clusters of network-connected PCs, and a multi-projector 3D display. Multiple video streams are individually encoded and sent over a broadband network to the display. The 3D display shows high-resolution (1024 × 768) stereoscopic color images for multiple viewpoints without special glasses. We implemented systems with rear-projection and front-projection lenticular screens. In this paper, we provide a detailed overview of our 3D TV system, including an examination of design choices and tradeoffs. We present the calibration and image alignment procedures that are necessary to achieve good image quality. We present qualitative results and some early user feedback. We believe this is the first real-time end-to-end 3D TV system with enough views and resolution to provide a truly immersive 3D experience.	3D TV: a scalable system for real-time acquisition, transmission, and autostereoscopic display of dynamic scenes	NA:NA	2018
Marc Levoy:Billy Chen:Vaibhav Vaish:Mark Horowitz:Ian McDowall:Mark Bolas	Confocal microscopy is a family of imaging techniques that employ focused patterned illumination and synchronized imaging to create cross-sectional views of 3D biological specimens. In this paper, we adapt confocal imaging to large-scale scenes by replacing the optical apertures used in microscopy with arrays of real or virtual video projectors and cameras. Our prototype implementation uses a video projector, a camera, and an array of mirrors. Using this implementation, we explore confocal imaging of partially occluded environments, such as foliage, and weakly scattering environments, such as murky water. We demonstrate the ability to selectively image any plane in a partially occluded environment, and to see further through murky water than is otherwise possible. By thresholding the confocal images, we extract mattes that can be used to selectively illuminate any plane in the scene.	Synthetic aperture confocal imaging	NA:NA:NA:NA:NA:NA	2018
Michael Goesele:Hendrik P. A. Lensch:Jochen Lang:Christian Fuchs:Hans-Peter Seidel	Translucent objects are characterized by diffuse light scattering beneath the object's surface. Light enters and leaves an object at possibly distinct surface locations. This paper presents the first method to acquire this transport behavior for arbitrary inhomogeneous objects. Individual surface points are illuminated in our DISCO measurement facility and the object's impulse response is recorded with a high-dynamic range video camera. The acquired data is resampled into a hierarchical model of the object's light scattering properties. Missing values are consistently interpolated resulting in measurement-based, complete and accurate representations of real translucent objects which can be rendered with various algorithms.	DISCO: acquisition of translucent objects	NA:NA:NA:NA:NA	2018
Michael Garland	NA	Session details: Mesh parameterization	NA	2018
Nathan A. Carr:John C. Hart	Surface painting is a technique that allows a user to paint a texture directly onto a surface, usually with a texture atlas: a 1:1 mapping between the surface and its texture image. Many good automatic texture atlas generation methods exist that evenly distribute texture samples across a surface based on its area and/or curvature, and some are even sensitive to the frequency spectrum of the input texture. However, during the surface painting process, the texture can change non-uniformly and unpredictably and even the atlases are static and can thus fail to reproduce sections of finely painted detail such as surface illustration.We present a new texture atlas algorithm that distributes initial texture samples evenly according to surface area and texture frequency, and, more importantly, maintains this distribution as the texture signal changes during the surface painting process. The running time is further accelerated with a novel GPU implementation of the surface painting process. The redistribution of samples is transparent to the user, resulting in a surface painting system of seemingly unlimited resolution. The atlas construction is local, making it fast enough to run interactively on models containing over 100K faces.	Painting detail	NA:NA	2018
Marco Tarini:Kai Hormann:Paolo Cignoni:Claudio Montani	Standard texture mapping of real-world meshes suffers from the presence of seams that need to be introduced in order to avoid excessive distortions and to make the topology of the mesh compatible to the one of the texture domain. In contrast, cube maps provide a mechanism that could be used for seamless texture mapping with low distortion, but only if the object roughly resembles a cube. We extend this concept to arbitrary meshes by using as texture domain the surface of a polycube whose shape is similar to that of the given mesh. Our approach leads to a seamless texture mapping method that is simple enough to be implemented in currently available graphics hardware.	PolyCube-Maps	NA:NA:NA:NA	2018
Vladislav Kraevoy:Alla Sheffer	Many geometry processing applications, such as morphing, shape blending, transfer of texture or material properties, and fitting template meshes to scan data, require a bijective mapping between two or more models. This mapping, or cross-parameterization, typically needs to preserve the shape and features of the parameterized models, mapping legs to legs, ears to ears, and so on. Most of the applications also require the models to be represented by compatible meshes, i.e. meshes with identical connectivity, based on the cross-parameterization. In this paper we introduce novel methods for shape preserving cross-parameterization and compatible remeshing. Our cross-parameterization method computes a low-distortion bijective mapping between models that satisfies user prescribed constraints. Using this mapping, the remeshing algorithm preserves the user-defined feature vertex correspondence and the shape correlation between the models. The remeshing algorithm generates output meshes with significantly fewer elements compared to previous techniques, while accurately approximating the input geometry. As demonstrated by the examples, the compatible meshes we construct are ideally suitable for morphing and other geometry processing applications.	Cross-parameterization and compatible remeshing of 3D models	NA:NA	2018
John Schreiner:Arul Asirvatham:Emil Praun:Hugues Hoppe	We consider the problem of creating a map between two arbitrary triangle meshes. Whereas previous approaches compose parametrizations over a simpler intermediate domain, we directly create and optimize a continuous map between the meshes. Map distortion is measured with a new symmetric metric, and is minimized during interleaved coarse-to-fine refinement of both meshes. By explicitly favoring low inter-surface distortion, we obtain maps that naturally align corresponding shape elements. Typically, the user need only specify a handful of feature correspondences for initial registration, and even these constraints can be removed during optimization. Our method robustly satisfies hard constraints if desired. Inter-surface mapping is shown using geometric and attribute morphs. Our general framework can also be applied to parametrize surfaces onto simplicial domains, such as coarse meshes (for semi-regular remeshing), and octahedron and toroidal domains (for geometry image remeshing). In these settings, we obtain better parametrizations than with previous specialized techniques, thanks to our fine-grain optimization.	Inter-surface mapping	NA:NA:NA:NA	2018
Emil Praun	NA	Session details: Fixing models	NA	2018
Andrei Sharf:Marc Alexa:Daniel Cohen-Or	Sampling complex, real-world geometry with range scanning devices almost always yields imperfect surface samplings. These "holes" in the surface are commonly filled with a smooth patch that conforms with the boundary. We introduce a context-based method: the characteristics of the given surface are analyzed, and the hole is iteratively filled by copying patches from valid regions of the given surface. In particular, the method needs to determine best matching patches, and then, fit imported patches by aligning them with the surrounding surface. The completion process works top down, where details refine intermediate coarser approximations. To align an imported patch with the existing surface, we apply a rigid transformation followed by an iterative closest point procedure with non-rigid transformations. The surface is essentially treated as a point set, and local implicit approximations aid in measuring the similarity between two point set patches. We demonstrate the method at several point-sampled surfaces, where the holes either result from imperfect sampling during range scanning or manual removal.	Context-based surface completion	NA:NA:NA	2018
Tao Ju	We present a robust method for repairing arbitrary polygon models. The method is guaranteed to produce a closed surface that partitions the space into disjoint internal and external volumes. Given any model represented as a polygon soup, we construct an inside/outside volume using an octree grid, and reconstruct the surface by contouring. Our novel algorithm can efficiently process large models containing millions of polygons and is capable of reproducing sharp features in the original geometry.	Robust repair of polygonal models	NA	2018
Chen Shen:James F. O'Brien:Jonathan R. Shewchuk	This paper describes a method for building interpolating or approximating implicit surfaces from polygonal data. The user can choose to generate a surface that exactly interpolates the polygons, or a surface that approximates the input by smoothing away features smaller than some user-specified size. The implicit functions are represented using a moving least-squares formulation with constraints integrated over the polygons. The paper also presents an improved method for enforcing normal constraints and an iterative procedure for ensuring that the implicit surface tightly encloses the input vertices.	Interpolating and approximating implicit surfaces from polygon soup	NA:NA:NA	2018
James L. Mohler	NA	Keynote and Awards presentations	NA	2018
Bruce Carse	NA	Keynote Speech: George Lucas: A keynote Q&A with the father of digital cinema	NA	2018
Ronen Barzel	NA	Session details: Skin & faces	NA	2018
Doug L. James:Christopher D. Twigg	We extend approaches for skinning characters to the general setting of skinning deformable mesh animations. We provide an automatic algorithm for generating progressive skinning approximations, that is particularly efficient for pseudo-articulated motions. Our contributions include the use of nonparametric mean shift clustering of high-dimensional mesh rotation sequences to automatically identify statistically relevant bones, and robust least squares methods to determine bone transformations, bone-vertex influence sets, and vertex weight values. We use a low-rank data reduction model defined in the undeformed mesh configuration to provide progressive convergence with a fixed number of bones. We show that the resulting skinned animations enable efficient hardware rendering, rest pose editing, and deformable collision detection. Finally, we present numerous examples where skins were automatically generated using a single set of parameter values.	Skinning mesh animations	NA:NA	2018
Dragomir Anguelov:Praveen Srinivasan:Daphne Koller:Sebastian Thrun:Jim Rodgers:James Davis	We introduce the SCAPE method (Shape Completion and Animation for PEople)---a data-driven method for building a human shape model that spans variation in both subject shape and pose. The method is based on a representation that incorporates both articulated and non-rigid deformations. We learn a pose deformation model that derives the non-rigid surface deformation as a function of the pose of the articulated skeleton. We also learn a separate model of variation based on body shape. Our two models can be combined to produce 3D surface models with realistic muscle deformation for different people in different poses, when neither appear in the training set. We show how the model can be used for shape completion --- generating a complete surface mesh given a limited set of markers specifying the target shape. We present applications of shape completion to partial view completion and motion capture animation. In particular, our method is capable of constructing a high-quality animated surface model of a moving person, with realistic muscle deformation, using just a single static scan and a marker motion capture sequence of the person.	SCAPE: shape completion and animation of people	NA:NA:NA:NA:NA:NA	2018
Eftychios Sifakis:Igor Neverov:Ronald Fedkiw	We built an anatomically accurate model of facial musculature, passive tissue and underlying skeletal structure using volumetric data acquired from a living male subject. The tissues are endowed with a highly nonlinear constitutive model including controllable anisotropic muscle activations based on fiber directions. Detailed models of this sort can be difficult to animate requiring complex coordinated stimulation of the underlying musculature. We propose a solution to this problem automatically determining muscle activations that track a sparse set of surface landmarks, e.g. acquired from motion capture marker data. Since the resulting animation is obtained via a three dimensional nonlinear finite element method, we obtain visually plausible and anatomically correct deformations with spatial and temporal coherence that provides robustness against outliers in the motion capture data. Moreover, the obtained muscle activations can be used in a robust simulation framework including contact and collision of the face with external objects.	Automatic determination of facial muscle activations from sparse motion capture marker data	NA:NA:NA	2018
Daniel Vlasic:Matthew Brand:Hanspeter Pfister:Jovan Popović	Face Transfer is a method for mapping videorecorded performances of one individual to facial animations of another. It extracts visemes (speech-related mouth articulations), expressions, and three-dimensional (3D) pose from monocular video or film footage. These parameters are then used to generate and drive a detailed 3D textured face mesh for a target identity, which can be seamlessly rendered back into target footage. The underlying face model automatically adjusts for how the target performs facial expressions and visemes. The performance data can be easily edited to change the visemes, expressions, pose, or even the identity of the target---the attributes are separably controllable. This supports a wide variety of video rewrite and puppetry applications.Face Transfer is based on a multilinear model of 3D face meshes that separably parameterizes the space of geometric variations due to different attributes (e.g., identity, expression, and viseme). Separability means that each of these attributes can be independently varied. A multilinear model can be estimated from a Cartesian product of examples (identities × expressions × visemes) with techniques from statistical analysis, but only after careful preprocessing of the geometric data set to secure one-to-one correspondence, to minimize cross-coupling artifacts, and to fill in any missing examples. Face Transfer offers new solutions to these problems and links the estimated model with a face-tracking algorithm to extract pose, expression, and viseme parameters.	Face transfer with multilinear models	NA:NA:NA:NA	2018
Hanspeter Pfister	NA	Session details: Hardware rendering	NA	2018
Sven Woop:Jörg Schmittler:Philipp Slusallek	Recursive ray tracing is a simple yet powerful and general approach for accurately computing global light transport and rendering high quality images. While recent algorithmic improvements and optimized parallel software implementations have increased ray tracing performance to realtime levels, no compact and programmable hardware solution has been available yet.This paper describes the architecture and a prototype implementation of a single chip, fully programmable Ray Processing Unit (RPU). It combines the flexibility of general purpose CPUs with the efficiency of current GPUs for data parallel computations. This design allows for realtime ray tracing of dynamic scenes with programmable material, geometry, and illumination shaders.Although, running at only 66 MHz the prototype FPGA implementation already renders images at up to 20 frames per second, which in many cases beats the performance of highly optimized software running on multi-GHz desktop CPUs. The performance and efficiency of the proposed architecture is analyzed using a variety of benchmark scenes.	RPU: a programmable ray processing unit for realtime ray tracing	NA:NA:NA	2018
Fabio Pellacini	Programmable shading is a fundamental technique for specifying appearance in 3d environments. While shading architectures provides fast execution of shaders, shader evaluation is today a major cost in the rendering process. In the same manner in which geometric simplification lets us deal with large models, it would be beneficial to have an automatic technique that trades off shader quality for speed.This paper presents such a technique by introducing a framework for the automatic simplification of complex procedural shaders, where a sequence of increasingly simplified shaders is generated starting from an original shader together with ranges for all of its input parameters. Our approach works by applying simplification rules to the code of a shader to generate a series of candidates, whose differences from the original one are measured and used to select the candidate with the smallest error. This procedure is repeated until the last shader is a constant. While this automatic procedure generates high quality simplified shaders, the artist might want to emphasize particular aspects of a shader during simplification. Our framework supports this desire by allowing the user to specify additional rules to be considered during simplification. The term user-configurable simplification comes from this feature of our system.We implemented our algorithm to support the simplification of fragment shaders running on graphics hardware. Our results show that automatic simplification of complex procedural shaders is possible with high quality.	User-configurable automatic shader simplification	NA	2018
Nathaniel Duca:Krzysztof Niski:Jonathan Bilodeau:Matthew Bolitho:Yuan Chen:Jonathan Cohen	We present a new, unified approach to debugging graphics software. We propose a representation of all graphics state over the course of program execution as a relational database, and produce a query-based framework for extracting, manipulating, and visualizing data from all stages of the graphics pipeline. Using an SQL-based query language, the programmer can establish functional relationships among all the data, linking OpenGL state to primitives to vertices to fragments to pixels. Based on the Chromium library, our approach requires no modification to or recompilation of the program to be debugged, and forms a superset of many existing techniques for debugging graphics software.	A relational debugging engine for the graphics pipeline	NA:NA:NA:NA:NA:NA	2018
Fabio Pellacini:Kiril Vidimče:Aaron Lefohn:Alex Mohr:Mark Leone:John Warren	In computer cinematography, the process of lighting design involves placing and configuring lights to define the visual appearance of environments and to enhance story elements. This process is labor intensive and time consuming, primarily because lighting artists receive poor feedback from existing tools: interactive previews have very poor quality, while final-quality images often take hours to render.This paper presents an interactive cinematic lighting system used in the production of computer-animated feature films containing environments of very high complexity, in which surface and light appearances are described using procedural RenderMan shaders. Our system provides lighting artists with high-quality previews at interactive framerates with only small approximations compared to the final rendered images. This is accomplished by combining numerical estimation of surface response, image-space caching, deferred shading, and the computational power of modern graphics hardware.Our system has been successfully used in the production of two feature-length animated films, dramatically accelerating lighting tasks. In our experience interactivity fundamentally changes an artist's workflow, improving both productivity and artistic expressiveness.	Lpics: a hybrid hardware-accelerated relighting engine for computer cinematography	NA:NA:NA:NA:NA:NA	2018
Ioana Boier-Martin	NA	Session details: Mesh manipulation	NA	2018
Matthias Müller:Bruno Heidelberger:Matthias Teschner:Markus Gross	We present a new approach for simulating deformable objects. The underlying model is geometrically motivated. It handles pointbased objects and does not need connectivity information. The approach does not require any pre-processing, is simple to compute, and provides unconditionally stable dynamic simulations.The main idea of our deformable model is to replace energies by geometric constraints and forces by distances of current positions to goal positions. These goal positions are determined via a generalized shape matching of an undeformed rest state with the current deformed state of the point cloud. Since points are always drawn towards well-defined locations, the overshooting problem of explicit integration schemes is eliminated. The versatility of the approach in terms of object representations that can be handled, the efficiency in terms of memory and computational complexity, and the unconditional stability of the dynamic simulation make the approach particularly interesting for games.	Meshless deformations based on shape matching	NA:NA:NA:NA	2018
Yaron Lipman:Olga Sorkine:David Levin:Daniel Cohen-Or	We introduce a rigid motion invariant mesh representation based on discrete forms defined on the mesh. The reconstruction of mesh geometry from this representation requires solving two sparse linear systems that arise from the discrete forms: the first system defines the relationship between local frames on the mesh, and the second encodes the position of the vertices via the local frames. The reconstructed geometry is unique up to a rigid transformation of the mesh. We define surface editing operations by placing user-defined constraints on the local frames and the vertex positions. These constraints are incorporated in the two linear reconstruction systems, and their solution produces a deformed surface geometry that preserves the local differential properties in the least-squares sense. Linear combination of shapes expressed with our representation enables linear shape interpolation that correctly handles rotations. We demonstrate the effectiveness of the new representation with various detail-preserving editing operators and shape morphing.	Linear rotation-invariant coordinates for meshes	NA:NA:NA:NA	2018
Robert W. Sumner:Matthias Zwicker:Craig Gotsman:Jovan Popović	The ability to position a small subset of mesh vertices and produce a meaningful overall deformation of the entire mesh is a fundamental task in mesh editing and animation. However, the class of meaningful deformations varies from mesh to mesh and depends on mesh kinematics, which prescribes valid mesh configurations, and a selection mechanism for choosing among them. Drawing an analogy to the traditional use of skeleton-based inverse kinematics for posing skeletons. we define mesh-based inverse kinematics as the problem of finding meaningful mesh deformations that meet specified vertex constraints.Our solution relies on example meshes to indicate the class of meaningful deformations. Each example is represented with a feature vector of deformation gradients that capture the affine transformations which individual triangles undergo relative to a reference pose. To pose a mesh, our algorithm efficiently searches among all meshes with specified vertex positions to find the one that is closest to some pose in a nonlinear span of the example feature vectors. Since the search is not restricted to the span of example shapes, this produces compelling deformations even when the constraints require poses that are different from those observed in the examples. Furthermore, because the span is formed by a nonlinear blend of the example feature vectors, the blending component of our system may also be used independently to pose meshes by specifying blending weights or to compute multi-way morph sequences.	Mesh-based inverse kinematics	NA:NA:NA:NA	2018
Kun Zhou:Jin Huang:John Snyder:Xinguo Liu:Hujun Bao:Baining Guo:Heung-Yeung Shum	We present a novel technique for large deformations on 3D meshes using the volumetric graph Laplacian. We first construct a graph representing the volume inside the input mesh. The graph need not form a solid meshing of the input mesh's interior; its edges simply connect nearby points in the volume. This graph's Laplacian encodes volumetric details as the difference between each point in the graph and the average of its neighbors. Preserving these volumetric details during deformation imposes a volumetric constraint that prevents unnatural changes in volume. We also include in the graph points a short distance outside the mesh to avoid local self-intersections. Volumetric detail preservation is represented by a quadric energy function. Minimizing it preserves details in a least-squares sense, distributing error uniformly over the whole deformed mesh. It can also be combined with conventional constraints involving surface positions, details or smoothness, and efficiently minimized by solving a sparse linear system.We apply this technique in a 2D curve-based deformation system allowing novice users to create pleasing deformations with little effort. A novel application of this system is to apply nonrigid and exaggerated deformations of 2D cartoon characters to 3D meshes. We demonstrate our system's potential with several examples.	Large mesh deformation using the volumetric graph Laplacian	NA:NA:NA:NA:NA:NA:NA	2018
François X. Sillion	NA	Session details: Illustration and image based modeling	NA	2018
Nelson S.-H. Chu:Chiew-Lan Tai	This paper presents a physically-based method for simulating ink dispersion in absorbent paper for art creation purposes. We devise a novel fluid flow model based on the lattice Boltzmann equation suitable for simulating percolation in disordered media, like paper, in real time. Our model combines the simulations of spontaneous shape evolution and porous media flow under a unified framework. We also couple our physics simulation with simple implicit modeling and image-based methods to render high quality output. We demonstrate the effectiveness of our techniques in a digital paint system and achieve various realistic effects of ink dispersion, including complex flow patterns observed in real artwork, and other special effects.	MoXi: real-time ink dispersion in absorbent paper	NA:NA	2018
Michael Burns:Janek Klawe:Szymon Rusinkiewicz:Adam Finkelstein:Doug DeCarlo	Renderings of volumetric data have become an important data analysis tool for applications ranging from medicine to scientific simulation. We propose a volumetric drawing system that directly extracts sparse linear features, such as silhouettes and suggestive contours, using a temporally coherent seed-and-traverse framework. In contrast to previous methods based on isosurfaces or nonrefractive transparency, producing these drawings requires examining an asymptotically smaller subset of the data, leading to efficiency on large data sets. In addition, the resulting imagery is often more comprehensible than standard rendering styles, since it focuses attention on important features in the data. We test our algorithms on datasets up to 5123, demonstrating interactive extraction and rendering of line drawings in a variety of drawing styles.	Line drawings from volume data	NA:NA:NA:NA:NA	2018
Ce Liu:Antonio Torralba:William T. Freeman:Frédo Durand:Edward H. Adelson	We present motion magnification, a technique that acts like a microscope for visual motion. It can amplify subtle motions in a video sequence, allowing for visualization of deformations that would otherwise be invisible. To achieve motion magnification, we need to accurately measure visual motions, and group the pixels to be modified. After an initial image registration step, we measure motion by a robust analysis of feature point trajectories, and segment pixels based on similarity of position, color, and motion. A novel measure of motion similarity groups even very small motions according to correlation over time, which often relates to physical cause. An outlier mask marks observations not explained by our layered motion model, and those pixels are simply reproduced on the output from the original registered observations.The motion of any selected layer may be magnified by a user-specified amount; texture synthesis fills-in unseen "holes" revealed by the amplified motions. The resulting motion-magnified images can reveal or emphasize small motions in the original sequence, as we demonstrate with deformations in load-bearing structures, subtle motions or balancing corrections of people, and "rigid" structures bending under hand pressure.	Motion magnification	NA:NA:NA:NA:NA	2018
Hongcheng Wang:Qing Wu:Lin Shi:Yizhou Yu:Narendra Ahuja	Tensor approximation is necessary to obtain compact multilinear models for multi-dimensional visual datasets. Traditionally, each multi-dimensional data item is represented as a vector. Such a scheme flattens the data and partially destroys the internal structures established throughout the multiple dimensions. In this paper, we retain the original dimensionality of the data items to more effectively exploit existing spatial redundancy and allow more efficient computation. Since the size of visual datasets can easily exceed the memory capacity of a single machine, we also present an out-of-core algorithm for higher-order tensor approximation. The basic idea is to partition a tensor into smaller blocks and perform tensor-related operations blockwise. We have successfully applied our techniques to three graphics-related data-driven models, including 6D bidirectional texture functions, 7D dynamic BTFs and 4D volume simulation sequences. Experimental results indicate that our techniques can not only process out-of-core data, but also achieve higher compression ratios and quality than previous methods.	Out-of-core tensor approximation of multi-dimensional matrices of visual data	NA:NA:NA:NA:NA	2018
Alla Sheffer	NA	Session details: Meshes I	NA	2018
Diego Nehab:Szymon Rusinkiewicz:James Davis:Ravi Ramamoorthi	Range scanning, manual 3D editing, and other modeling approaches can provide information about the geometry of surfaces in the form of either 3D positions (e.g., triangle meshes or range images) or orientations (normal maps or bump maps). We present an algorithm that combines these two kinds of estimates to produce a new surface that approximates both. Our formulation is linear, allowing it to operate efficiently on complex meshes commonly used in graphics. It also treats high-and low-frequency components separately, allowing it to optimally combine outputs from data sources such as stereo triangulation and photometric stereo, which have different error-vs.-frequency characteristics. We demonstrate the ability of our technique to both recover high-frequency details and avoid low-frequency bias, producing surfaces that are more widely applicable than position or orientation data alone.	Efficiently combining positions and normals for precise 3D geometry	NA:NA:NA:NA	2018
Shachar Fleishman:Daniel Cohen-Or:Cláudio T. Silva	We introduce a robust moving least-squares technique for reconstructing a piecewise smooth surface from a potentially noisy point cloud. We use techniques from robust statistics to guide the creation of the neighborhoods used by the moving least squares (MLS) computation. This leads to a conceptually simple approach that provides a unified framework for not only dealing with noise, but also for enabling the modeling of surfaces with sharp features.Our technique is based on a new robust statistics method for outlier detection: the forward-search paradigm. Using this powerful technique, we locally classify regions of a point-set to multiple outlier-free smooth regions. This classification allows us to project points on a locally smooth region rather than a surface that is smooth everywhere, thus defining a piecewise smooth surface and increasing the numerical stability of the projection operator. Furthermore, by treating the points across the discontinuities as outliers, we are able to define sharp features. One of the nice features of our approach is that it automatically disregards outliers during the surface-fitting phase.	Robust moving least-squares fitting with sharp features	NA:NA:NA	2018
Vitaly Surazhsky:Tatiana Surazhsky:Danil Kirsanov:Steven J. Gortler:Hugues Hoppe	The computation of geodesic paths and distances on triangle meshes is a common operation in many computer graphics applications. We present several practical algorithms for computing such geodesics from a source point to one or all other points efficiently. First, we describe an implementation of the exact "single source, all destination" algorithm presented by Mitchell, Mount, and Papadimitriou (MMP). We show that the algorithm runs much faster in practice than suggested by worst case analysis. Next, we extend the algorithm with a merging operation to obtain computationally efficient and accurate approximations with bounded error. Finally, to compute the shortest path between two given points, we use a lower-bound property of our approximate geodesic algorithm to efficiently prune the frontier of the MMP algorithm. thereby obtaining an exact solution even more quickly.	Fast exact and approximate geodesics on meshes	NA:NA:NA:NA:NA	2018
Tao Ju:Scott Schaefer:Joe Warren	Constructing a function that interpolates a set of values defined at vertices of a mesh is a fundamental operation in computer graphics. Such an interpolant has many uses in applications such as shading, parameterization and deformation. For closed polygons, mean value coordinates have been proven to be an excellent method for constructing such an interpolant. In this paper, we generalize mean value coordinates from closed 2D polygons to closed triangular meshes. Given such a mesh P, we show that these coordinates are continuous everywhere and smooth on the interior of P. The coordinates are linear on the triangles of P and can reproduce linear functions on the interior of P. To illustrate their usefulness, we conclude by considering several interesting applications including constructing volumetric textures and surface deformation.	Mean value coordinates for closed triangular meshes	NA:NA:NA	2018
Wojciech Matusik	NA	Session details: Video & image matting	NA	2018
Morgan McGuire:Wojciech Matusik:Hanspeter Pfister:John F. Hughes:Frédo Durand	Video matting is the process of pulling a high-quality alpha matte and foreground from a video sequence. Current techniques require either a known background (e.g., a blue screen) or extensive user interaction (e.g., to specify known foreground and background elements). The matting problem is generally under-constrained, since not enough information has been collected at capture time. We propose a novel, fully autonomous method for pulling a matte using multiple synchronized video streams that share a point of view but differ in their plane of focus. The solution is obtained by directly minimizing the error in filter-based image formation equations, which are over-constrained by our rich data stream. Our system solves the fully dynamic video matting problem without user assistance: both the foreground and background may be high frequency and have dynamic content, the foreground may resemble the background, and the scene is lit by natural (as opposed to polarized or collimated) illumination.	Defocus video matting	NA:NA:NA:NA:NA	2018
Derek Hoiem:Alexei A. Efros:Martial Hebert	This paper presents a fully automatic method for creating a 3D model from a single photograph. The model is made up of several texture-mapped planar billboards and has the complexity of a typical children's pop-up book illustration. Our main insight is that instead of attempting to recover precise geometry, we statistically model geometric classes defined by their orientations in the scene. Our algorithm labels regions of the input image into coarse categories: "ground", "sky", and "vertical". These labels are then used to "cut and fold" the image into a pop-up model using a set of simple assumptions. Because of the inherent ambiguity of the problem and the statistical nature of the approach, the algorithm is not expected to work on every image. However. it performs surprisingly well for a wide range of scenes taken from a typical person's photo album.	Automatic photo pop-up	NA:NA:NA	2018
Jue Wang:Pravin Bhat:R. Alex Colburn:Maneesh Agrawala:Michael F. Cohen	We present an interactive system for efficiently extracting foreground objects from a video. We extend previous min-cut based image segmentation techniques to the domain of video with four new contributions. We provide a novel painting-based user interface that allows users to easily indicate the foreground object across space and time. We introduce a hierarchical mean-shift preprocess in order to minimize the number of nodes that min-cut must operate on. Within the min-cut we also define new local cost functions to augment the global costs defined in earlier work. Finally, we extend 2D alpha matting methods designed for images to work with 3D video volumes. We demonstrate that our matting approach preserves smoothness across both space and time. Our interactive video cutout system allows users to quickly extract foreground objects from video sequences for use in a variety of applications including compositing onto new backgrounds and NPR cartoon style rendering.	Interactive video cutout	NA:NA:NA:NA:NA	2018
Yin Li:Jian Sun:Heung-Yeung Shum	In this paper, we present a system for cutting a moving object out from a video clip. The cutout object sequence can be pasted onto another video or a background image. To achieve this, we first apply a new 3D graph cut based segmentation approach on the spatial-temporal video volume. Our algorithm partitions watershed presegmentation regions into foreground and background while preserving temporal coherence. Then, the initial segmentation result is refined locally. Given two frames in the video sequence, we specify two respective windows of interest which are then tracked using a bi-directional feature tracking algorithm. For each frame in between these two given frames, the segmentation in each tracked window is refined using a 2D graph cut that utilizes a local color model. Moreover, we provide brush tools for the user to control the object boundary precisely wherever needed. Based on the accurate binary segmentation result, we apply coherent matting to extract the alpha mattes and foreground colors of the object.	Video object cut and paste	NA:NA:NA	2018
Hugues Hoppe	NA	Session details: Meshes II	NA	2018
Gabriel Peyré:Stéphane Mallat	This paper describes the construction of second generation bandelet bases and their application to 3D geometry compression. This new coding scheme is orthogonal and the corresponding basis functions are regular. In our method, surfaces are decomposed in a bandelet basis with a fast bandeletization algorithm that removes the geometric redundancy of orthogonal wavelet coefficients. The resulting transform coding scheme has an error decay that is asymptotically optimal for geometrically regular surfaces. We then use these bandelet bases to perform geometry image and normal map compression. Numerical tests show that for complex surfaces bandelets bring an improvement of 1.5dB to 2dB over state of the art compression schemes.	Surface compression with geometric bandelets	NA:NA	2018
Jingliang Peng:C.-C. Jay Kuo	A new progressive lossless 3D triangular mesh encoder is proposed in this work, which can encode any 3D triangular mesh with an arbitrary topological structure. Given a mesh, the quantized 3D vertices are first partitioned into an octree (OT) structure, which is then traversed from the root and gradually to the leaves. During the traversal, each 3D cell in the tree front is subdivided into eight childcells. For each cell subdivision, both local geometry and connectivity changes are encoded, where the connectivity coding is guided by the geometry coding. Furthermore, prioritized cell subdivision is performed in the tree front to provide better rate-distortion (RD) performance. Experiments show that the proposed mesh coder outperforms the kd-tree algorithm in both geometry and connectivity coding efficiency. For the geometry coding part, the range of improvement is typically around 10%~20%, but may go up to 50%~60% for meshes with highly regular geometry data and/or tight clustering of vertices.	Geometry-guided progressive lossless 3D mesh coding with octree (OT) decomposition	NA:NA	2018
Pierre Alliez:David Cohen-Steiner:Mariette Yvinec:Mathieu Desbrun	In this paper, a novel Delaunay-based variational approach to isotropic tetrahedral meshing is presented. To achieve both robustness and efficiency, we minimize a simple mesh-dependent energy through global updates of both vertex positions and connectivity. As this energy is known to be the ∠1 distance between an isotropic quadratic function and its linear interpolation on the mesh, our minimization procedure generates well-shaped tetrahedra. Mesh design is controlled through a gradation smoothness parameter and selection of the desired number of vertices. We provide the foundations of our approach by explaining both the underlying variational principle and its geometric interpretation. We demonstrate the quality of the resulting meshes through a series of examples.	Variational tetrahedral meshing	NA:NA:NA:NA	2018
Serban D. Porumbescu:Brian Budge:Louis Feng:Kenneth I. Joy	A shell map is a bijective mapping between shell space and texture space that can be used to generate small-scale features on surfaces using a variety of modeling techniques. The method is based upon the generation of an offset surface and the construction of a tetrahedral mesh that fills the space between the base surface and its offset. By identifying a corresponding tetrahedral mesh in texture space, the shell map can be implemented through a straightforward barycentric-coordinate map between corresponding tetrahedra. The generality of shell maps allows texture space to contain geometric objects, procedural volume textures, scalar fields, or other shell-mapped objects.	Shell maps	NA:NA:NA:NA	2018
Maneesh Agrawala	NA	Session details: Perception	NA	2018
Amy A. Gooch:Sven C. Olsen:Jack Tumblin:Bruce Gooch	Visually important image features often disappear when color images are converted to grayscale. The algorithm introduced here reduces such losses by attempting to preserve the salient features of the color image. The Color2Gray algorithm is a 3-step process: 1) convert RGB inputs to a perceptually uniform CIE L*a*b* color space, 2) use chrominance and luminance differences to create grayscale target differences between nearby image pixels, and 3) solve an optimization problem designed to selectively modulate the grayscale representation as a function of the chroma variation of the source image. The Color2Gray results offer viewers salient information missing from previous grayscale image creation methods.	Color2Gray: salience-preserving color removal	NA:NA:NA:NA	2018
Patrick Ledda:Alan Chalmers:Tom Troscianko:Helge Seetzen	Tone mapping operators are designed to reproduce visibility and the overall impression of brightness, contrast and color of the real world onto limited dynamic range displays and printers. Although many tone mapping operators have been published in recent years, no thorough psychophysical experiments have yet been undertaken to compare such operators against the real scenes they are purporting to depict. In this paper, we present the results of a series of psychophysical experiments to validate six frequently used tone mapping operators against linearly mapped High Dynamic Range (HDR) scenes displayed on a novel HDR device. Individual operators address the tone mapping issue using a variety of approaches and the goals of these techniques are often quite different from one another. Therefore, the purpose of this investigation was not simply to determine which is the "best" algorithm, but more generally to propose an experimental methodology to validate such operators and to determine the participants' impressions of the images produced compared to what is visible on a high contrast ratio display.	Evaluation of tone mapping operators using a High Dynamic Range display	NA:NA:NA:NA	2018
Michael F. Deering	A photon accurate model of individual cones in the human eye perceiving images on digital display devices is presented. Playback of streams of pixel video data is modeled as individual photon emission events from within the physical substructure of each display pixel. The thus generated electromagnetic wavefronts are refracted through a four surface model of the human cornea and lens, and diffracted at the pupil. The position, size, shape, and orientation of each of the five million photoreceptor cones in the retina are individually modeled by a new synthetic retina model. Photon absorption events map the collapsing wavefront to photon detection events in a particular cone, resulting in images of the photon counts in the retinal cone array. The custom rendering systems used to generate sequences of these images takes a number of optical and physical properties of the image formation into account, including wavelength dependent absorption in the tissues of the eye, and the motion blur caused by slight movement of the eye during a frame of viewing. The creation of this new model is part of a larger framework for understanding how changes to computer graphics rendering algorithms and changes in image display devices are related to artifacts visible to human viewers.	A photon accurate model of the human eye	NA	2018
Chang Ha Lee:Amitabh Varshney:David W. Jacobs	Research over the last decade has built a solid mathematical foundation for representation and analysis of 3D meshes in graphics and geometric modeling. Much of this work however does not explicitly incorporate models of low-level human visual attention. In this paper we introduce the idea of mesh saliency as a measure of regional importance for graphics meshes. Our notion of saliency is inspired by low-level human visual system cues. We define mesh saliency in a scale-dependent manner using a center-surround operator on Gaussian-weighted mean curvatures. We observe that such a definition of mesh saliency is able to capture what most would classify as visually interesting regions on a mesh. The human-perception-inspired importance measure computed by our mesh saliency operator results in more visually pleasing results in processing and viewing of 3D meshes. compared to using a purely geometric measure of shape. such as curvature. We discuss how mesh saliency can be incorporated in graphics applications such as mesh simplification and viewpoint selection and present examples that show visually appealing results from using mesh saliency.	Mesh saliency	NA:NA:NA	2018
Nancy Pollard	NA	Session details: Motion capture data: interaction and selection	NA	2018
Jackie Assa:Yaron Caspi:Daniel Cohen-Or	Illustrating motion in still imagery for the purpose of summary, abstraction and motion description is important for a diverse spectrum of fields, ranging from arts to sciences. In this paper, we introduce a method that produces an action synopsis for presenting motion in still images. The method carefully selects key poses based on an analysis of a skeletal animation sequence, to facilitate expressing complex motions in a single image or a small number of concise views. Our approach is to embed the high-dimensional motion curve in a low-dimensional Euclidean space, where the main characteristics of the skeletal action are kept. The lower complexity of the embedded motion curve allows a simple iterative method which analyzes the curve and locates significant points, associated with the key poses of the original motion. We present methods for illustrating the selected poses in an image as a means to convey the action. We applied our methods to a variety of motions of human actions given either as 3D animation sequences or as video clips, and generated images that depict their synopsis.	Action synopsis: pose selection and illustration	NA:NA:NA	2018
Meinard Müller:Tido Röder:Michael Clausen	The reuse of human motion capture data to create new, realistic motions by applying morphing and blending techniques has become an important issue in computer animation. This requires the identification and extraction of logically related motions scattered within some data set. Such content-based retrieval of motion capture data, which is the topic of this paper, constitutes a difficult and time-consuming problem due to significant spatio-temporal variations between logically related motions. In our approach, we introduce various kinds of qualitative features describing geometric relations between specified body points of a pose and show how these features induce a time segmentation of motion capture data streams. By incorporating spatio-temporal invariance into the geometric features and adaptive segments, we are able to adopt efficient indexing methods allowing for flexible and efficient content-based retrieval and browsing in huge motion capture databases. Furthermore, we obtain an efficient preprocessing method substantially accelerating the cost-intensive classical dynamic time warping techniques for the time alignment of logically similar motion data streams. We present experimental results on a test data set of more than one million frames, corresponding to 180 minutes of motion. The linearity of our indexing algorithms guarantees the scalability of our results to much larger data sets.	Efficient content-based retrieval of motion capture data	NA:NA:NA	2018
Jinxiang Chai:Jessica K. Hodgins	This paper introduces an approach to performance animation that employs video cameras and a small set of retro-reflective markers to create a low-cost, easy-to-use system that might someday be practical for home use. The low-dimensional control signals from the user's performance are supplemented by a database of pre-recorded human motion. At run time, the system automatically learns a series of local models from a set of motion capture examples that are a close match to the marker locations captured by the cameras. These local models are then used to reconstruct the motion of the user as a full-body animation. We demonstrate the power of this approach with real-time control of six different behaviors using two video cameras and a small set of retro-reflective markers. We compare the resulting animation to animation from commercial motion capture equipment with a full set of markers.	Performance animation from low-dimensional control signals	NA:NA	2018
Victor Brian Zordan:Anna Majkowska:Bill Chiu:Matthew Fast	Human motion capture embeds rich detail and style which is difficult to generate with competing animation synthesis technologies. However, such recorded data requires principled means for creating responses in unpredicted situations, for example reactions immediately following impact. This paper introduces a novel technique for incorporating unexpected impacts into a motion capture-driven animation system through the combination of a physical simulation which responds to contact forces and a specialized search routine which determines the best plausible re-entry into motion library playback following the impact. Using an actuated dynamic model, our system generates a physics-based response while connecting motion capture segments. Our method allows characters to respond to unexpected changes in the environment based on the specific dynamic effects of a given contact while also taking advantage of the realistic movement made available through motion capture. We show the results of our system under various conditions and with varying responses using martial arts motion capture as a testbed.	Dynamic response for motion capture animation	NA:NA:NA:NA	2018
Greg Turk	NA	Session details: Plants	NA	2018
Adam Runions:Martin Fuhrer:Brendan Lane:Pavol Federl:Anne-Gaëlle Rolland-Lagan:Przemyslaw Prusinkiewicz	We introduce a class of biologically-motivated algorithms for generating leaf venation patterns. These algorithms simulate the interplay between three processes: (1) development of veins towards hormone (auxin) sources embedded in the leaf blade; (2) modification of the hormone source distribution by the proximity of veins; and (3) modification of both the vein pattern and source distribution by leaf growth. These processes are formulated in terms of iterative geometric operations on sets of points that represent vein nodes and auxin sources. In addition, a vein connection graph is maintained to determine vein widths. The effective implementation of the algorithms relies on the use of space subdivision (Voronoi diagrams) and time coherence between iteration steps. Depending on the specification details and parameters used, the algorithms can simulate many types of venation patterns, both open (tree-like) and closed (with loops). Applications of the presented algorithms include texture and detailed structure generation for image synthesis purposes, and modeling of morphogenetic processes in support of biological research.	Modeling and visualization of leaf venation patterns	NA:NA:NA:NA:NA:NA	2018
Lifeng Wang:Wenle Wang:Julie Dorsey:Xu Yang:Baining Guo:Heung-Yeung Shum	This paper presents a framework for the real-time rendering of plant leaves with global illumination effects. Realistic rendering of leaves requires a sophisticated appearance model and accurate lighting computation. For leaf appearance we introduce a parametric model that describes leaves in terms of spatially-variant BRDFs and BTDFs. These BRDFs and BTDFs, incorporating analysis of subsurface scattering inside leaf tissues and rough surface scattering on leaf surfaces, can be measured from real leaves. More importantly, this description is compact and can be loaded into graphics hardware for fast run-time shading calculations, which are essential for achieving high frame rates. For lighting computation, we present an algorithm that extends the Precomputed Radiance Transfer (PRT) approach to all-frequency lighting for leaves. In particular, we handle the combined illumination effects due to low-frequency environment light and high-frequency sunlight. This is done by decomposing the local incident radiance of sunlight into direct and indirect components. The direct component, which contains most of the high frequencies, is not pre-computed with spherical harmonics as in PRT; instead it is evaluated on-the-fly using pre-computed light-visibility convolution data. We demonstrate our framework by the rendering of a variety of leaves and assemblies thereof.	Real-time rendering of plant leaves	NA:NA:NA:NA:NA:NA	2018
Takashi Ijiri:Shigeru Owada:Makoto Okabe:Takeo Igarashi	We present a system for modeling flowers in three dimensions quickly and easily while preserving correct botanical structures. We use floral diagrams and inflorescences, which were developed by botanists to concisely describe structural information of flowers. Floral diagrams represent the layout of floral components on a single flower, while inflorescences are arrangements of multiple flowers. Based on these notions, we created a simple user interface that is specially tailored to flower editing, while retaining a maximum variety of generable models. We also provide sketching interfaces to define the geometries of floral components. Separation of structural editing and editing of geometry makes the authoring process more flexible and efficient. We found that even novice users could easily design various flower models using our technique. Our system is an example of application-customized sketching, illustrating the potential power of a sketching interface that is carefully designed for a specific application.	Floral diagrams and inflorescences: interactive flower modeling using botanical structural constraints	NA:NA:NA:NA	2018
Stephen R. Marschner:Stephen H. Westin:Adam Arbree:Jonathan T. Moon	Wood coated with transparent finish has a beautiful and distinctive appearance that is familiar to everyone. Woods with unusual grain patterns. such as tiger, burl, and birdseye figures, have a strikingly unusual directional reflectance that is prized for decorative applications. With new, high resolution measurements of spatially varying BRDFs. we show that this distinctive appearance is due to light scattering that does not conform to the usual notion of anisotropic surface reflection. The behavior can be explained by scattering from the matrix of wood fibers below the surface, resulting in a subsurface highlight that occurs on a cone with an out-of-plane axis. We propose a new shading model component to handle reflection from subsurface fibers, which is combined with the standard diffuse and specular components to make a complete shading model. Rendered results from fits of our model to the measurement data demonstrate that this new model captures the distinctive appearance of wood.	Measuring and modeling the appearance of finished wood	NA:NA:NA:NA	2018
Szymon Rusinkiewicz	NA	Session details: Capturing reality I	NA	2018
Ren Ng	This paper contributes to the theory of photograph formation from light fields. The main result is a theorem that, in the Fourier domain, a photograph formed by a full lens aperture is a 2D slice in the 4D light field. Photographs focused at different depths correspond to slices at different trajectories in the 4D space. The paper demonstrates the utility of this theorem in two different ways. First, the theorem is used to analyze the performance of digital refocusing, where one computes photographs focused at different depths from a single light field. The analysis shows in closed form that the sharpness of refocused photographs increases linearly with directional resolution. Second, the theorem yields a Fourier-domain algorithm for digital refocusing, where we extract the appropriate 2D slice of the light field's Fourier transform, and perform an inverse 2D Fourier transform. This method is faster than previous approaches.	Fourier slice photography	NA	2018
Pradeep Sen:Billy Chen:Gaurav Garg:Stephen R. Marschner:Mark Horowitz:Marc Levoy:Hendrik P. A. Lensch	We present a novel photographic technique called dual photography, which exploits Helmholtz reciprocity to interchange the lights and cameras in a scene. With a video projector providing structured illumination, reciprocity permits us to generate pictures from the viewpoint of the projector, even though no camera was present at that location. The technique is completely image-based, requiring no knowledge of scene geometry or surface properties, and by its nature automatically includes all transport paths, including shadows, inter-reflections and caustics. In its simplest form, the technique can be used to take photographs without a camera; we demonstrate this by capturing a photograph using a projector and a photo-resistor. If the photo-resistor is replaced by a camera, we can produce a 4D dataset that allows for relighting with 2D incident illumination. Using an array of cameras we can produce a 6D slice of the 8D reflectance field that allows for relighting with arbitrary light fields. Since an array of cameras can operate in parallel without interference, whereas an array of light sources cannot, dual photography is fundamentally a more efficient way to capture such a 6D dataset than a system based on multiple projectors and one camera. As an example, we show how dual photography can be used to capture and relight scenes.	Dual photography	NA:NA:NA:NA:NA:NA:NA	2018
Andreas Wenger:Andrew Gardner:Chris Tchou:Jonas Unger:Tim Hawkins:Paul Debevec	We present a technique for capturing an actor's live-action performance in such a way that the lighting and reflectance of the actor can be designed and modified in postproduction. Our approach is to illuminate the subject with a sequence of time-multiplexed basis lighting conditions, and to record these conditions with a high-speed video camera so that many conditions are recorded in the span of the desired output frame interval. We investigate several lighting bases for representing the sphere of incident illumination using a set of discrete LED light sources, and we estimate and compensate for subject motion using optical flow and image warping based on a set of tracking frames inserted into the lighting basis. To composite the illuminated performance into a new background, we include a time-multiplexed matte within the basis. We also show that the acquired data enables time-varying surface normals, albedo, and ambient occlusion to be estimated, which can be used to transform the actor's reflectance to produce both subtle and stylistic effects.	Performance relighting and reflectance transformation with time-multiplexed illumination	NA:NA:NA:NA:NA:NA	2018
Bennett Wilburn:Neel Joshi:Vaibhav Vaish:Eino-Ville Talvala:Emilio Antunez:Adam Barth:Andrew Adams:Mark Horowitz:Marc Levoy	The advent of inexpensive digital image sensors and the ability to create photographs that combine information from a number of sensed images are changing the way we think about photography. In this paper, we describe a unique array of 100 custom video cameras that we have built, and we summarize our experiences using this array in a range of imaging applications. Our goal was to explore the capabilities of a system that would be inexpensive to produce in the future. With this in mind, we used simple cameras, lenses, and mountings, and we assumed that processing large numbers of images would eventually be easy and cheap. The applications we have explored include approximating a conventional single center of projection video camera with high performance along one or more axes, such as resolution, dynamic range, frame rate, and/or large aperture, and using multiple cameras to approximate a video camera with a large synthetic aperture. This permits us to capture a video light field, to which we can apply spatiotemporal view interpolation algorithms in order to digitally simulate time dilation and camera motion. It also permits us to create video sequences using custom non-uniform synthetic apertures.	High performance imaging using large camera arrays	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Heung-Yeung Shum	NA	Session details: Texture synthesis	NA	2018
Sylvain Lefebvre:Hugues Hoppe	We present a texture synthesis scheme based on neighborhood matching, with contributions in two areas: parallelism and control. Our scheme defines an infinite, deterministic, aperiodic texture, from which windows can be computed in real-time on a GPU. We attain high-quality synthesis using a new analysis structure called the Gaussian stack, together with a coordinate upsampling step and a subpass correction approach. Texture variation is achieved by multiresolution jittering of exemplar coordinates. Combined with the local support of parallel synthesis, the jitter enables intuitive user controls including multiscale randomness, spatial modulation over both exemplar and output, feature drag-and-drop, and periodicity constraints. We also introduce synthesis magnification, a fast method for amplifying coarse synthesis results to higher resolution.	Parallel controllable texture synthesis	NA:NA	2018
Wojciech Matusik:Matthias Zwicker:Frédo Durand	We present a system for designing novel textures in the space of textures induced by an input database. We capture the structure of the induced space by a simplicial complex where vertices of the simplices represent input textures. A user can generate new textures by interpolating within individual simplices. We propose a morphable interpolation for textures, which also defines a metric used to build the simplicial complex. To guarantee sharpness in interpolated textures, we enforce histograms of high-frequency content using a novel method for histogram interpolation. We allow users to continuously navigate in the simplicial complex and design new textures using a simple and efficient user interface. We demonstrate the usefulness of our system by integrating it with a 3D texture painting application, where the user interactively designs desired textures.	Texture design using a simplicial complex of morphable textures	NA:NA:NA	2018
Vivek Kwatra:Irfan Essa:Aaron Bobick:Nipun Kwatra	We present a novel technique for texture synthesis using optimization. We define a Markov Random Field (MRF)-based similarity metric for measuring the quality of synthesized texture with respect to a given input sample. This allows us to formulate the synthesis problem as minimization of an energy function, which is optimized using an Expectation Maximization (EM)-like algorithm. In contrast to most example-based techniques that do region-growing, ours is a joint optimization approach that progressively refines the entire texture. Additionally, our approach is ideally suited to allow for controllable synthesis of textures. Specifically, we demonstrate controllability by animating image textures using flow fields. We allow for general two-dimensional flow fields that may dynamically change over time. Applications of this technique include dynamic texturing of fluid animations and texture-based flow visualization.	Texture optimization for example-based synthesis	NA:NA:NA:NA	2018
Robert L. Cook:Tony DeRose	Noise functions are an essential building block for writing procedural shaders in 3D computer graphics. The original noise function introduced by Ken Perlin is still the most popular because it is simple and fast, and many spectacular images have been made with it. Nevertheless, it is prone to problems with aliasing and detail loss. In this paper we analyze these problems and show that they are particularly severe when 3D noise is used to texture a 2D surface. We use the theory of wavelets to create a new class of simple and fast noise functions that avoid these problems.	Wavelet noise	NA:NA	2018
Steve Marschner	NA	Session details: Capturing reality II	NA	2018
Tim Hawkins:Per Einarsson:Paul Debevec	We present a technique for capturing time-varying volumetric data of participating media. A laser sheet is swept repeatedly through the volume, and the scattered light is imaged using a high-speed camera. Each sweep of the laser provides a near-simultaneous volume of density values. We demonstrate rendered animations under changing viewpoint and illumination, making use of measured values for the scattering phase function and albedo.	Acquisition of time-varying participating media	NA:NA:NA	2018
Yichen Wei:Eyal Ofek:Long Quan:Heung-Yeung Shum	In this paper, we propose a novel image-based approach to model hair geometry from images taken at multiple viewpoints. Unlike previous hair modeling techniques that require intensive user interactions or rely on special capturing setup under controlled illumination conditions, we use a handheld camera to capture hair images under uncontrolled illumination conditions. Our multi-view approach is natural and flexible for capturing. It also provides inherent strong and accurate geometric constraints to recover hair models.In our approach, the hair fibers are synthesized from local image orientations. Each synthesized fiber segment is validated and optimally triangulated from all visible views. The hair volume and the visibility of synthesized fibers can also be reliably estimated from multiple views. Flexibility of acquisition, little user interaction, and high quality results of recovered complex hair models are the key advantages of our method.	Modeling hair from multiple views	NA:NA:NA:NA	2018
Aseem Agarwala:Ke Colin Zheng:Chris Pal:Maneesh Agrawala:Michael Cohen:Brian Curless:David Salesin:Richard Szeliski	This paper describes a mostly automatic method for taking the output of a single panning video camera and creating a panoramic video texture (PVT): a video that has been stitched into a single, wide field of view and that appears to play continuously and indefinitely. The key problem in creating a PVT is that although only a portion of the scene has been imaged at any given time, the output must simultaneously portray motion throughout the scene. Like previous work in video textures, our method employs min-cut optimization to select fragments of video that can be stitched together both spatially and temporally. However, it differs from earlier work in that the optimization must take place over a much larger set of data. Thus, to create PVTs, we introduce a dynamic programming step, followed by a novel hierarchical min-cut optimization algorithm. We also use gradient-domain compositing to further smooth boundaries between video fragments. We demonstrate our results with an interactive viewer in which users can interactively pan and zoom on high-resolution PVTs.	Panoramic video textures	NA:NA:NA:NA:NA:NA:NA:NA	2018
Amit Agrawal:Ramesh Raskar:Shree K. Nayar:Yuanzhen Li	Flash images are known to suffer from several problems: saturation of nearby objects, poor illumination of distant objects, reflections of objects strongly lit by the flash and strong highlights due to the reflection of flash itself by glossy surfaces. We propose to use a flash and no-flash (ambient) image pair to produce better flash images. We present a novel gradient projection scheme based on a gradient coherence model that allows removal of reflections and highlights from flash images. We also present a brightness-ratio based algorithm that allows us to compensate for the falloff in the flash image brightness due to depth. In several practical scenarios, the quality of flash/no-flash images may be limited in terms of dynamic range. In such cases, we advocate using several images taken under different flash intensities and exposures. We analyze the flash intensity-exposure space and propose a method for adaptively sampling this space so as to minimize the number of captured images for any given scene. We present several experimental results that demonstrate the ability of our algorithms to produce improved flash images.	Removing photography artifacts using gradient projection and flash-exposure sampling	NA:NA:NA:NA	2018
Chris Bregler	NA	Session details: Image processing	NA	2018
Yuanzhen Li:Lavanya Sharan:Edward H. Adelson	High dynamic range (HDR) imaging is an area of increasing importance, but most display devices still have limited dynamic range (LDR). Various techniques have been proposed for compressing the dynamic range while retaining important visual information. Multi-scale image processing techniques, which are widely used for many image processing tasks, have a reputation of causing halo artifacts when used for range compression. However, we demonstrate that they can work when properly implemented. We use a symmetrical analysis-synthesis filter bank, and apply local gain control to the subbands. We also show that the technique can be adapted for the related problem of "companding", in which an HDR image is converted to an LDR image, and later expanded back to high dynamic range.	Compressing and companding high dynamic range images with subband architectures	NA:NA:NA	2018
Eric P. Bennett:Leonard McMillan	We enhance underexposed, low dynamic range videos by adaptively and independently varying the exposure at each photoreceptor in a post-process. This virtual exposure is a dynamic function of both the spatial neighborhood and temporal history at each pixel. Temporal integration enables us to expand the image's dynamic range while simultaneously reducing noise. Our non-linear exposure variation and denoising filters smoothly transition from temporal to spatial for moving scene elements. Our virtual exposure framework also supports temporally coherent per frame tone mapping. Our system outputs restored video sequences with significantly reduced noise, increased exposure time of dark pixels, intact motion, and improved details.	Video enhancement using per-pixel virtual exposures	NA:NA	2018
Yung-Yu Chuang:Dan B Goldman:Ke Colin Zheng:Brian Curless:David H. Salesin:Richard Szeliski	In this paper, we explore the problem of enhancing still pictures with subtly animated motions. We limit our domain to scenes containing passive elements that respond to natural forces in some fashion. We use a semi-automatic approach, in which a human user segments the scene into a series of layers to be individually animated. Then, a "stochastic motion texture" is automatically synthesized using a spectral method, i.e., the inverse Fourier transform of a filtered noise spectrum. The motion texture is a time-varying 2D displacement map, which is applied to each layer. The resulting warped layers are then recomposited to form the animated frames. The result is a looping video texture created from a single still image, which has the advantages of being more controllable and of generally higher image quality and resolution than a video texture created from a video source. We demonstrate the technique on a variety of photographs and paintings.	Animating pictures with stochastic motion textures	NA:NA:NA:NA:NA:NA	2018
Jian Sun:Lu Yuan:Jiaya Jia:Heung-Yeung Shum	In this paper, we introduce a novel approach to image completion, which we call structure propagation. In our system, the user manually specifies important missing structure information by extending a few curves or line segments from the known to the unknown regions. Our approach synthesizes image patches along these user-specified curves in the unknown region using patches selected around the curves in the known region. Structure propagation is formulated as a global optimization problem by enforcing structure and consistency constraints. If only a single curve is specified, structure propagation is solved using Dynamic Programming. When multiple intersecting curves are specified, we adopt the Belief Propagation algorithm to find the optimal patches. After completing structure propagation, we fill in the remaining unknown regions using patch-based texture synthesis. We show that our approach works well on a number of examples that are challenging to state-of-the-art techniques.	Image completion with structure propagation	NA:NA:NA:NA	2018
Marc Stamminger	NA	Session details: Large models & large displays	NA	2018
Louis Borgeat:Guy Godin:François Blais:Philippe Massicotte:Christian Lahanier	This paper presents a new technique for fast, view-dependent, real-time visualization of large multiresolution geometric models with color or texture information. This method uses geomorphing to smoothly interpolate between geometric patches composing a hierarchical level-of-detail structure, and to maintain seamless continuity between neighboring patches of the model. It combines the advantages of view-dependent rendering with numerous additional features: the high performance rendering associated with static preoptimized geometry, the capability to display at both low and high resolution with minimal artefacts, and a low CPU usage since all the geomorphing is done on the GPU. Furthermore, the hierarchical subdivision of the model into a tree structure can be accomplished according to any spatial or topological criteria. This property is particularly useful in dealing with models with high resolution textures derived from digital photographs. Results are presented for both highly tesselated models (372 million triangles), and for models which also contain large quantities of texture (200 million triangles + 20 GB of compressed texture). The method also incorporates asynchronous out-of-core model management. Performances obtained on commodity hardware are in the range of 50 million geomorphed triangles/second for a benchmark model such as Stanford's St. Matthew dataset.	GoLD: interactive display of huge colored and textured models	NA:NA:NA:NA:NA	2018
Enrico Gobbetti:Fabio Marton	We present an efficient approach for end-to-end out-of-core construction and interactive inspection of very large arbitrary surface models. The method tightly integrates visibility culling and out-of-core data management with a level-of-detail framework. At preprocessing time, we generate a coarse volume hierarchy by binary space partitioning the input triangle soup. Leaf nodes partition the original data into chunks of a fixed maximum number of triangles, while inner nodes are discretized into a fixed number of cubical voxels. Each voxel contains a compact direction dependent approximation of the appearance of the associated volumetric subpart of the model when viewed from a distance. The approximation is constructed by a visibility aware algorithm that fits parametric shaders to samples obtained by casting rays against the full resolution dataset. At rendering time, the volumetric structure, maintained off-core, is refined and rendered in front-to-back order, exploiting vertex programs for GPU evaluation of view-dependent voxel representations, hardware occlusion queries for culling occluded subtrees, and asynchronous I/O for detecting and avoiding data access latencies. Since the granularity of the multiresolution structure is coarse, data management, traversal and occlusion culling cost is amortized over many graphics primitives. The efficiency and generality of the approach is demonstrated with the interactive rendering of extremely complex heterogeneous surface models on current commodity graphics platforms.	Far voxels: a multiresolution framework for interactive rendering of huge complex 3D models on commodity graphics platforms	NA:NA	2018
Sung-Eui Yoon:Peter Lindstrom:Valerio Pascucci:Dinesh Manocha	We present a novel method for computing cache-oblivious layouts of large meshes that improve the performance of interactive visualization and geometric processing algorithms. Given that the mesh is accessed in a reasonably coherent manner, we assume no particular data access patterns or cache parameters of the memory hierarchy involved in the computation. Furthermore, our formulation extends directly to computing layouts of multi-resolution and bounding volume hierarchies of large meshes.We develop a simple and practical cache-oblivious metric for estimating cache misses. Computing a coherent mesh layout is reduced to a combinatorial optimization problem. We designed and implemented an out-of-core multilevel minimization algorithm and tested its performance on unstructured meshes composed of tens to hundreds of millions of triangles. Our layouts can significantly reduce the number of cache misses. We have observed 2--20 times speedups in view-dependent rendering, collision detection, and isocontour extraction without any modification of the algorithms or runtime applications.	Cache-oblivious mesh layouts	NA:NA:NA:NA	2018
Daniel J. Sandin:Todd Margolis:Jinghua Ge:Javier Girado:Tom Peterka:Thomas A. DeFanti	Virtual reality (VR) has long been hampered by the gear needed to make the experience possible; specifically, stereo glasses and tracking devices. Autostereoscopic display devices are gaining popularity by freeing the user from stereo glasses, however few qualify as VR displays. The Electronic Visualization Laboratory (EVL) at the University of Illinois at Chicago (UIC) has designed and produced a large scale, high resolution head-tracked barrier-strip autostereoscopic display system that produces a VR immersive experience without requiring the user to wear any encumbrances. The resulting system, called Varrier, is a passive parallax barrier 35-panel tiled display that produces a wide field of view, head-tracked VR experience. This paper presents background material related to parallax barrier autostereoscopy, provides system configuration and construction details, examines Varrier interleaving algorithms used to produce the stereo images, introduces calibration and testing, and discusses the camera-based tracking subsystem.	The VarrierTM autostereoscopic virtual reality display	NA:NA:NA:NA:NA:NA	2018
John Anderson	NA	Session details: Fluid simulation	NA	2018
Bryan E. Feldman:James F. O'Brien:Bryan M. Klingner	This paper presents a method for animating gases on unstructured tetrahedral meshes to efficiently model the interaction of fluids with irregularly shaped obstacles. Because our discretization scheme parallels that of the standard staggered grid mesh. we are able to combine tetrahedral cells with regular hexahedral cells in a single mesh. This hybrid mesh offers both accuracy near obstacles and efficiency in open regions.	Animating gases with hybrid meshes	NA:NA:NA	2018
Andrew Selle:Nick Rasmussen:Ronald Fedkiw	Vorticity confinement reintroduces the small scale detail lost when using efficient semi-Lagrangian schemes for simulating smoke and fire. However, it only amplifies the existing vorticity, and thus can be insufficient for highly turbulent effects such as explosions or rough water. We introduce a new hybrid technique that makes synergistic use of Lagrangian vortex particle methods and Eulerian grid based methods to overcome the weaknesses of both. Our approach uses vorticity confinement itself to couple these two methods together. We demonstrate that this approach can generate highly turbulent effects unachievable by standard grid based methods, and show applications to smoke, water and explosion simulations.	A vortex particle method for smoke, water and explosions	NA:NA:NA	2018
Jeong-Mo Hong:Chang-Hun Kim	At interfaces between different fluids, properties such as density, viscosity, and molecular cohesion are discontinuous. To animate small-scale details of incompressible viscous multi-phase fluids realistically, we focus on the discontinuities in the state variables that express these properties. Surface tension of both free and bubble surfaces is modeled using the jump condition in the pressure field; and discontinuities in the velocity gradient field. driven by viscosity differences, are also considered. To obtain derivatives of the pressure and velocity fields with sub-grid accuracy, they are extrapolated across interfaces using continuous variables based on physical properties. The numerical methods that we present are easy to implement and do not impact the performance of existing solvers. Small-scale fluid motions, such as capillary instability, breakup of liquid sheets, and bubbly water can all be successfully animated.	Discontinuous fluids	NA:NA	2018
Huamin Wang:Peter J. Mucha:Greg Turk	We present a physically-based method to enforce contact angles at the intersection of fluid free surfaces and solid objects, allowing us to simulate a variety of small-scale fluid phenomena including water drops on surfaces. The heart of this technique is a virtual surface method, which modifies the level set distance field representing the fluid surface in order to maintain an appropriate contact angle. The surface tension that is calculated on the contact line between the solid surface and liquid surface can then capture all interfacial tensions, including liquid-solid, liquid-air and solid-air tensions. We use a simple dynamic contact angle model to select contact angles according to the solid material property, water history, and the fluid front's motion. Our algorithm robustly and accurately treats various drop shape deformations, and handles both flat and curved solid surfaces. Our results show that our algorithm is capable of realistically simulating several small-scale liquid phenomena such as beading and flattened drops, stretched and separating drops, suspended drops on curved surfaces, and capillary action.	Water drops on surfaces	NA:NA:NA	2018
Steven Feiner	NA	Session details: Reprise of UIST and I3D: UIST (user interface software and technology)	NA	2018
Georg Apitz:François Guimbretière	We introduce CrossY, a simple drawing application developed as a benchmark to demonstrate the feasibility of goal-crossing as the basis for a graphical user interface. While crossing was previously identified as a potential substitute for the classic point-and-click interaction, this work is the first to report on the practical aspects of implementing an interface solely based on goal-crossing.	CrossY: a crossing-based drawing application	NA:NA	2018
Tovi Grossman:Daniel Wigdor:Ravin Balakrishnan	Volumetric displays provide interesting opportunities and challenges for 3D interaction and visualization, particularly when used in a highly interactive manner. We explore this area through the design and implementation of techniques for interactive direct manipulation of objects with a 3D volumetric display. Motion tracking of the user's fingers provides for direct gestural interaction with the virtual objects, through manipulations on and around the display's hemispheric enclosure. Our techniques leverage the unique features of volumetric displays, including a 360° viewing volume that enables manipulation from any viewpoint around the display, as well as natural and accurate perception of true depth information in the displayed 3D scene. We demonstrate our techniques within a prototype 3D geometric model building application.	Multi-finger gestural interaction with 3D volumetric displays	NA:NA:NA	2018
Blair MacIntyre:Maribeth Gandy:Steven Dow:Jay David Bolter	In this paper [MacIntyre et al 2004]. we describe The Designer's Augmented Reality Toolkit (DART). DART is built on top of Macromedia Director, a widely used multimedia development environment. We summarize the most significant problems faced by designers working with AR in the real world, and discuss how DART addresses them. Most of DART is implemented in an interpreted scripting language, and can be modified by designers to suit their needs. Our work focuses on supporting early design activities, especially a rapid transition from storyboards to working experience, so that the experiential part of a design can be tested early and often. DART allows designers to specify complex relationships between the physical and virtual worlds, and supports 3D animatic actors (informal, sketch-based content) in addition to more polished content. Designers can capture and replay synchronized video and sensor data, allowing them to work off-site and to test specific parts of their experience more effectively.	DART: a toolkit for rapid design exploration of augmented reality experiences	NA:NA:NA:NA	2018
David Luebke	NA	Session details: I3D (symposium on interactive 3D graphics)	NA	2018
Simon Dobbyn:John Hamill:Keith O'Conor:Carol O'Sullivan	The simulation of large crowds of humans is important in many fields of computer graphics, including real-time applications such as games, as they can breathe life into otherwise static scenes and enhance believability. Although many new games are released each year, it is very unusual to find large-scale crowds populating the environments depicted. Such applications need to deal with having limited resources available at each frame. With many hundreds or thousands of potential virtual humans in a crowd, traditional techniques rapidly become overwhelmed and are not able to sustain an interactive frame-rate. Therefore, simpler approaches to the rendering, animation and behaviour control of the crowds are needed. Additionally, these new approaches must provide for variety, as environments inhabited by carbon-copy clones can be disconcerting and unrealistic.	Geopostors: a real-time geometry/impostor crowd rendering system	NA:NA:NA:NA	2018
Youngihn Kho:Michael Garland	Techniques for interactive deformation of unstructured polygon meshes are of fundamental importance to a host of applications. Most traditional approaches to this problem have emphasized precise control over the deformation being made. However, they are often cumbersome and unintuitive for non-expert users.In this paper, we present an interactive system for deforming unstructured polygon meshes that is very easy to use. The user interacts with the system by sketching curves in the image plane. A single stroke can define a free-form skeleton and the region of the model to be deformed. By sketching the desired deformation of this reference curve, the user can implicitly and intuitively control the deformation of an entire region of the surface. At the same time, the reference curve also provides a basis for controlling additional parameters, such as twist and scaling. We demonstrate that our system can be used to interactively edit a variety of unstructured mesh models with very little effort. Furthermore, we can also use our formulation of the deformation to achieve a natural interpolation between character poses, thus producing simple key framed animations.	Sketching mesh deformations	NA:NA	2018
Fábio Policarpo:Manuel M. Oliveira:João L. D. Comba	We present a technique for mapping relief textures onto arbitrary polygonal models in real time, producing correct self-occlusions, interpenetrations, shadows and per-pixel lighting. The technique uses a pixel-driven formulation based on an efficient ray-height-field intersection implemented on the GPU. It has very low memory requirements, supports extreme close-up views of the surfaces and can be applicable to surfaces undergoing deformation.	Real-time relief mapping on arbitrary polygonal surfaces	NA:NA:NA	2018
Jovan Popovic	NA	Session details: Dynamics of solids	NA	2018
Stephane Redon:Nico Galoppo:Ming C. Lin	Forward dynamics is central to physically-based simulation and control of articulated bodies. We present an adaptive algorithm for computing forward dynamics of articulated bodies: using novel motion error metrics, our algorithm can automatically simplify the dynamics of a multi-body system, based on the desired number of degrees of freedom and the location of external forces and active joint forces. We demonstrate this method in plausible animation of articulated bodies, including a large-scale simulation of 200 animated humanoids and multi-body dynamics systems with many degrees of freedom. The graceful simplification allows us to achieve up to two orders of magnitude performance improvement in several complex benchmarks.	Adaptive dynamics of articulated bodies	NA:NA:NA	2018
Danny M. Kaufman:Timothy Edmunds:Dinesh K. Pai	We describe an efficient algorithm for the simulation of large sets of non-convex rigid bodies. The algorithm finds a simultaneous solution for a multi-body system that is linear in the total number of contacts detected in each iteration. We employ a novel contact model that uses mass, location, and velocity information from all contacts, at the moment of maximum compression, to constrain rigid body velocities. We also develop a new friction model in the configuration space of rigid bodies. These models are used to compute the feasible velocity and the frictional response of each body. Implementation is simple and leads to a fast rigid body simulator that computes steps on the order of seconds for simulations involving over one thousand non-convex objects in high contact configurations.	Fast frictional dynamics for rigid bodies	NA:NA:NA	2018
Mark Pauly:Richard Keiser:Bart Adams:Philip Dutré:Markus Gross:Leonidas J. Guibas	We present a new meshless animation framework for elastic and plastic materials that fracture. Central to our method is a highly dynamic surface and volume sampling method that supports arbitrary crack initiation, propagation, and termination, while avoiding many of the stability problems of traditional mesh-based techniques. We explicitly model advancing crack fronts and associated fracture surfaces embedded in the simulation volume. When cutting through the material, crack fronts directly affect the coupling between simulation nodes, requiring a dynamic adaptation of the nodal shape functions. We show how local visibility tests and dynamic caching lead to an efficient implementation of these effects based on point collocation. Complex fracture patterns of interacting and branching cracks are handled using a small set of topological operations for splitting, merging, and terminating crack fronts. This allows continuous propagation of cracks with highly detailed fracture surfaces, independent of the spatial resolution of the simulation nodes, and provides effective mechanisms for controlling fracture paths. We demonstrate our method for a wide range of materials, from stiff elastic to highly plastic objects that exhibit brittle and/or ductile fracture.	Meshless animation of fracturing solids	NA:NA:NA:NA:NA:NA	2018
Mathieu Desbrun	NA	Session details: Deformable models	NA	2018
Yongning Zhu:Robert Bridson	We present a physics-based simulation method for animating sand. To allow for efficiently scaling up to large volumes of sand, we abstract away the individual grains and think of the sand as a continuum. In particular we show that an existing water simulator can be turned into a sand simulator with only a few small additions to account for inter-grain and boundary friction.We also propose an alternative method for simulating fluids. Our core representation is a cloud of particles, which allows for accurate and flexible surface tracking and advection, but we use an auxiliary grid to efficiently enforce boundary conditions and incompressibility. We further address the issue of reconstructing a surface from particle data to render each frame.	Animating sand as a fluid	NA:NA	2018
Eran Guendelman:Andrew Selle:Frank Losasso:Ronald Fedkiw	We present a novel method for solid/fluid coupling that can treat infinitesimally thin solids modeled by a lower dimensional triangulated surface. Since classical solid/fluid coupling algorithms rasterize the solid body onto the fluid grid, an entirely new approach is required to treat thin objects that do not contain an interior region. Robust ray casting is used to augment a number of interpolation, finite difference and rendering techniques so that fluid does not leak through the triangulated surface. Moreover, we propose a technique for properly enforcing incompressibility so that fluid does not incorrectly compress (and appear to lose mass) near the triangulated surface. This allows for the robust interaction of cloth and shells with thin sheets of water. The proposed method works for both rigid body shells and for deformable manifolds such as cloth, and we present a two way coupling technique that allows the fluid's pressure to affect the solid. Examples illustrate that our method performs well, especially in the difficult case of water and cloth where it produces visually rich interactions between the particle level set method for treating the water/air interface and our newly proposed method for treating the solid/fluid interface. We have implemented the method on both uniform and adaptive octree grids.	Coupling water and smoke to thin deformable and rigid shells	NA:NA:NA:NA	2018
Jernej Barbič:Doug L. James	In this paper, we present an approach for fast subspace integration of reduced-coordinate nonlinear deformable models that is suitable for interactive applications in computer graphics and haptics. Our approach exploits dimensional model reduction to build reduced-coordinate deformable models for objects with complex geometry. We exploit the fact that model reduction on large deformation models with linear materials (as commonly used in graphics) result in internal force models that are simply cubic polynomials in reduced coordinates. Coefficients of these polynomials can be precomputed, for efficient runtime evaluation. This allows simulation of nonlinear dynamics using fast implicit Newmark subspace integrators, with subspace integration costs independent of geometric complexity. We present two useful approaches for generating low-dimensional subspace bases: modal derivatives and an interactive sketching technique. Mass-scaled principal component analysis (mass-PCA) is suggested for dimensionality reduction. Finally, several examples are given from computer animation to illustrate high performance, including force-feedback haptic rendering of a complicated object undergoing large deformations.	Real-Time subspace integration for St. Venant-Kirchhoff deformable models	NA:NA	2018
Naga K. Govindaraju:David Knott:Nitin Jain:Ilknur Kabul:Rasmus Tamstorf:Russell Gayle:Ming C. Lin:Dinesh Manocha	We present a novel algorithm for accurately detecting all contacts, including self-collisions, between deformable models. We precompute a chromatic decomposition of a mesh into non-adjacent primitives using graph coloring algorithms. The chromatic decomposition enables us to check for collisions between non-adjacent primitives using a linear-time culling algorithm. As a result, we achieve higher culling efficiency and significantly reduce the number of false positives. We use our algorithm to check for collisions among complex deformable models consisting of tens of thousands of triangles for cloth modeling and medical simulation. Our algorithm accurately computes all contacts at interactive rates. We observed up to an order of magnitude speedup over prior methods.	Interactive collision detection between deformable models using chromatic decomposition	NA:NA:NA:NA:NA:NA:NA:NA	2018
Henry Fuchs	NA	Session details: Geometry on GPUs	NA	2018
Charles Loop:Jim Blinn	We present a method for resolution independent rendering of paths and bounded regions, defined by quadratic and cubic spline curves, that leverages the parallelism of programmable graphics hardware to achieve high performance. A simple implicit equation for a parametric curve is found in a space that can be thought of as an analog to texture space. The image of a curve's Bézier control points are found in this space and assigned to the control points as texture coordinates. When the triangle(s) corresponding to the Bézier curve control hull are rendered, a pixel shader program evaluates the implicit equation for a pixel's interpolated texture coordinates to determine an inside/outside test for the curve. We extend our technique to handle anti-aliasing of boundaries. We also construct a vector image from mosaics of triangulated Bézier control points and show how to deform such images to create resolution independent texture on three dimensional objects.	Resolution independent curve rendering using programmable graphics hardware	NA:NA	2018
Le-Jeng Shiue:Ian Jones:Jörg Peters	By organizing the control mesh of subdivision in texture memory so that irregularities occur strictly inside independently refinable fragment meshes, all major features of subdivision algorithms can be realized in the framework of highly parallel stream processing. Our implementation of Catmull-Clark subdivision as a GPU kernel in programmable graphics hardware can model features like semi-smooth creases and global boundaries; and a simplified version achieves near-realtime depth-five re-evaluation of moderate-sized subdivision meshes. The approach is easily adapted to other refinement patterns, such as Loop, Doo-Sabin or √3 and it allows for postprocessing with additional shaders.	A realtime GPU subdivision kernel	NA:NA:NA	2018
Michael Guthe:Aákos Balázs:Reinhard Klein	As there is no hardware support neither for rendering trimmed NURBS -- the standard surface representation in CAD -- nor for T-Spline surfaces the usability of existing rendering APIs like OpenGL, where a run-time tessellation is performed on the CPU, is limited to simple scenes. Due to the irregular mesh data structures required for trimming no algorithms exists that exploit the GPU for tessellation. Therefore, recent approaches perform a pretessellation and use level-of-detail techniques. In contrast to a simple API these methods require tedious preparation of the models before rendering and hinder interactive editing. Furthermore, due to the tremendous amount of triangle data smooth zoom-ins from long shot to close-up are not possible, In this paper we show how the trimming region can be defined by a trim-texture that is dynamically adapted to the required resolution and allows for an efficient trimming of surfaces on the GPU. Combining this new method with GPU-based tessellation of cubic rational surfaces allows a new rendering algorithm for arbitrary trimmed NURBS and T-Spline surfaces with prescribed error in screen space on the GPU. The performance exceeds current CPU-based techniques by a factor of up to 1000 and makes real-time visualization of real-world trimmed NURBS and T-Spline models possible on consumer-level graphics cards.	GPU-based trimming and tessellation of NURBS and T-Spline surfaces	NA:NA:NA	2018
John Hable:Jarek Rossignac	By combining depth peeling with a linear formulation of a Boolean expression called Blist, the Blister algorithm renders an arbitrary CSG model of n primitives in at most k steps, where k is the number of depth-layers in the arrangement of the primitives. Each step starts by rendering each primitive to produce candidate surfels on the next depth-layer. Then, it renders the primitives again, one at a time, to classify the candidate surfels against the primitive and to evaluate the Boolean expression directly on the GPU. Since Blist does not expand the CSG expression into a disjunctive (sum-of-products) form, Blister has O(kn) time complexity. We explain the Blist formulation while providing algorithms for CSG-to-Blist conversion and Blist-based parallel surfel classification. We report real-time performance for nontrivial CSG models. On hardware with an 8-bit stencil buffer, we can render all possible CSG expressions with 3909 primitives.	Blister: GPU-based rendering of Boolean combinations of free-form triangulated shapes	NA:NA	2018
George Drettakis	NA	Session details: Transparency & translucency	NA	2018
Craig Donner:Henrik Wann Jensen	This paper introduces a shading model for light diffusion in multi-layered translucent materials. Previous work on diffusion in translucent materials has assumed smooth semi-infinite homogeneous materials and solved for the scattering of light using a dipole diffusion approximation. This approximation breaks down in the case of thin translucent slabs and multi-layered materials. We present a new efficient technique based on multiple dipoles to account for diffusion in thin slabs. We enhance this multipole theory to account for mismatching indices of refraction at the top and bottom of of translucent slabs, and to model the effects of rough surfaces. To model multiple layers, we extend this single slab theory by convolving the diffusion profiles of the individual slabs. We account for multiple scattering between slabs by using a variant of Kubelka-Munk theory in frequency space. Our results demonstrate diffusion of light in thin slabs and multi-layered materials such as paint, paper, and human skin.	Light diffusion in multi-layered translucent materials	NA:NA	2018
Bo Sun:Ravi Ramamoorthi:Srinivasa G. Narasimhan:Shree K. Nayar	We consider real-time rendering of scenes in participating media, capturing the effects of light scattering in fog, mist and haze. While a number of sophisticated approaches based on Monte Carlo and finite element simulation have been developed, those methods do not work at interactive rates. The most common real-time methods are essentially simple variants of the OpenGL fog model. While easy to use and specify, that model excludes many important qualitative effects like glows around light sources, the impact of volumetric scattering on the appearance of surfaces such as the diffusing of glossy highlights, and the appearance under complex lighting such as environment maps. In this paper, we present an alternative physically based approach that captures these effects while maintaining real time performance and the ease-of-use of the OpenGL fog model. Our method is based on an explicit analytic integration of the single scattering light transport equations for an isotropic point light source in a homogeneous participating medium. We can implement the model in modern programmable graphics hardware using a few small numerical lookup tables stored as texture maps. Our model can also be easily adapted to generate the appearances of materials with arbitrary BRDFs, environment map lighting, and precomputed radiance transfer methods, in the presence of participating media. Hence, our techniques can be widely used in real-time rendering.	A practical analytic single scattering model for real time rendering	NA:NA:NA:NA	2018
Chris Wyman	Many interactive applications strive for realistic renderings, but framerate constraints usually limit realism to effects that run efficiently in graphics hardware. One effect largely ignored in such applications is refraction. We introduce a simple, image-space approach to refractions that easily runs on modern graphics cards. Our method requires two passes on a GPU, and allows refraction of a distant environment through two interfaces, compared to current interactive techniques that are restricted to a single interface. Like all image-based algorithms, aliasing can occur in certain circumstances, but the plausible refractions generated with our approach should suffice for many applications.	An approximate image-space approach for interactive refraction	NA	2018
Xin Tong:Jiaping Wang:Stephen Lin:Baining Guo:Heung-Yeung Shum	Many translucent materials consist of evenly-distributed heterogeneous elements which produce a complex appearance under different lighting and viewing directions. For these quasi-homogeneous materials, existing techniques do not address how to acquire their material representations from physical samples in a way that allows arbitrary geometry models to be rendered with these materials. We propose a model for such materials that can be readily acquired from physical samples. This material model can be applied to geometric models of arbitrary shapes, and the resulting objects can be efficiently rendered without expensive subsurface light transport simulation. In developing a material model with these attributes, we capitalize on a key observation about the subsurface scattering characteristics of quasi-homogeneous materials at different scales. Locally, the non-uniformity of these materials leads to inhomogeneous subsurface scattering. For subsurface scattering on a global scale, we show that a lengthy photon path through an even distribution of heterogeneous elements statistically resembles scattering in a homogeneous medium. This observation allows us to represent and measure the global light transport within quasi-homogeneous materials as well as the transfer of light into and out of a material volume through surface mesostructures. We demonstrate our technique with results for several challenging materials that exhibit sophisticated appearance features such as transmission of back illumination through surface mesostructures.	Modeling and rendering of quasi-homogeneous materials	NA:NA:NA:NA:NA	2018
Jehee Lee	NA	Session details: Styles of human motion	NA	2018
Tomohiko Mukai:Shigeru Kuriyama	A common motion interpolation technique for realistic human animation is to blend similar motion samples with weighting functions whose parameters are embedded in an abstract space. Existing methods, however, are insensitive to statistical properties, such as correlations between motions. In addition, they lack the capability to quantitatively evaluate the reliability of synthesized motions. This paper proposes a method that treats motion interpolations as statistical predictions of missing data in an arbitrarily definable parametric space. A practical technique of geostatistics, called universal kriging, is then introduced for statistically estimating the correlations between the dissimilarity of motions and the distance in the parametric space. Our method statistically optimizes interpolation kernels for given parameters at each frame, using a pose distance metric to efficiently analyze the correlation. Motions are accurately predicted for the spatial constraints represented in the parametric space, and they therefore have few undesirable artifacts, if any. This property alleviates the problem of spatial inconsistencies, such as foot-sliding, that are associated with many existing methods. Moreover, numerical estimates for the reliability of predictions enable motions to be adaptively sampled. Since the interpolation kernels are computed with a linear system in real-time, motions can be interactively edited using various spatial controls.	Geostatistical motion interpolation	NA:NA	2018
C. Karen Liu:Aaron Hertzmann:Zoran Popović	This paper presents a novel physics-based representation of realistic character motion. The dynamical model incorporates several factors of locomotion derived from the biomechanical literature, including relative preferences for using some muscles more than others. elastic mechanisms at joints due to the mechanical properties of tendons, ligaments, and muscles, and variable stiffness at joints depending on the task. When used in a spacetime optimization framework, the parameters of this model define a wide range of styles of natural human movement.Due to the complexity of biological motion, these style parameters are too difficult to design by hand. To address this, we introduce Nonlinear Inverse Optimization, a novel algorithm for estimating optimization parameters from motion capture data. Our method can extract the physical parameters from a single short motion sequence. Once captured, this representation of style is extremely flexible: motions can be generated in the same style but performing different tasks, and styles may be edited to change the physical properties of the body.	Learning physics-based motion style with nonlinear inverse optimization	NA:NA:NA	2018
Eugene Hsu:Kari Pulli:Jovan Popović	Style translation is the process of transforming an input motion into a new style while preserving its original content. This problem is motivated by the needs of interactive applications, which require rapid processing of captured performances. Our solution learns to translate by analyzing differences between performances of the same content in input and output styles. It relies on a novel correspondence algorithm to align motions, and a linear time-invariant model to represent stylistic differences. Once the model is estimated with system identification, our system is capable of translating streaming input with simple linear operations at each frame.	Style translation for human motion	NA:NA:NA	2018
Liu Ren:Alton Patrick:Alexei A. Efros:Jessica K. Hodgins:James M. Rehg	In this paper, we investigate whether it is possible to develop a measure that quantifies the naturalness of human motion (as defined by a large database). Such a measure might prove useful in verifying that a motion editing operation had not destroyed the naturalness of a motion capture clip or that a synthetic motion transition was within the space of those seen in natural human motion. We explore the performance of mixture of Gaussians (MoG), hidden Markov models (HMM), and switching linear dynamic systems (SLDS) on this problem. We use each of these statistical models alone and as part of an ensemble of smaller statistical models. We also implement a Naive Bayes (NB) model for a baseline comparison. We test these techniques on motion capture data held out from a database, keyframed motions, edited motions, motions with noise added, and synthetic motion transitions. We present the results as receiver operating characteristic (ROC) curves and compare the results to the judgments made by subjects in a user study.	A data-driven approach to quantifying natural human motion	NA:NA:NA:NA:NA	2018
Julie Dorsey	NA	Session details: Appearance & illumination	NA	2018
Bruce Walter:Sebastian Fernandez:Adam Arbree:Kavita Bala:Michael Donikian:Donald P. Greenberg	Lightcuts is a scalable framework for computing realistic illumination. It handles arbitrary geometry, non-diffuse materials, and illumination from a wide variety of sources including point lights, area lights, HDR environment maps, sun/sky models, and indirect illumination. At its core is a new algorithm for accurately approximating illumination from many point lights with a strongly sublinear cost. We show how a group of lights can be cheaply approximated while bounding the maximum approximation error. A binary light tree and perceptual metric are then used to adaptively partition the lights into groups to control the error vs. cost tradeoff.We also introduce reconstruction cuts that exploit spatial coherence to accelerate the generation of anti-aliased images with complex illumination. Results are demonstrated for five complex scenes and show that lightcuts can accurately approximate hundreds of thousands of point lights using only a few hundred shadow rays. Reconstruction cuts can reduce the number of shadow rays to tens.	Lightcuts: a scalable approach to illumination	NA:NA:NA:NA:NA:NA	2018
Okan Arikan:David A. Forsyth:James F. O'Brien	In this paper we present an approximate method for accelerated computation of the final gathering step in a global illumination algorithm. Our method operates by decomposing the radiance field close to surfaces into separate far- and near-field components that can be approximated individually. By computing surface shading using these approximations, instead of directly querying the global illumination solution, we have been able to obtain rendering time speed ups on the order of 10x compared to previous acceleration methods. Our approximation schemes rely mainly on the assumptions that radiance due to distant objects will exhibit low spatial and angular variation, and that the visibility between a surface and nearby surfaces can be reasonably predicted by simple location and orientation-based heuristics. Motivated by these assumptions, our far-field scheme uses scattered-data interpolation with spherical harmonics to represent spatial and angular variation, and our near-field scheme employs an aggressively simple visibility heuristic. For our test scenes, the errors introduced when our assumptions fail do not result in visually objectionable artifacts or easily noticeable deviation from a ground-truth solution. We also discuss how our near-field approximation can be used with standard local illumination algorithms to produce significantly improved images at only negligible additional cost.	Fast and detailed approximate global illumination by irradiance decomposition	NA:NA:NA	2018
Frédo Durand:Nicolas Holzschuch:Cyril Soler:Eric Chan:François X. Sillion	We present a signal-processing framework for light transport. We study the frequency content of radiance and how it is altered by phenomena such as shading, occlusion, and transport. This extends previous work that considered either spatial or angular dimensions, and it offers a comprehensive treatment of both space and angle.We show that occlusion, a multiplication in the primal, amounts in the Fourier domain to a convolution by the spectrum of the blocker. Propagation corresponds to a shear in the space-angle frequency domain, while reflection on curved objects performs a different shear along the angular frequency axis. As shown by previous work, reflection is a convolution in the primal and therefore a multiplication in the Fourier domain. Our work shows how the spatial components of lighting are affected by this angular convolution.Our framework predicts the characteristics of interactions such as caustics and the disappearance of the shadows of small features. Predictions on the frequency content can then be used to control sampling rates for rendering. Other potential applications include precomputed radiance transfer and inverse rendering.	A frequency analysis of light transport	NA:NA:NA:NA:NA	2018
Yanyun Chen:Lin Xia:Tien-Tsin Wong:Xin Tong:Hujun Bao:Baining Guo:Heung-Yeung Shum	Weathering modeling introduces blemishes such as dirt, rust, cracks and scratches to virtual scenery. In this paper we present a visual stimulation technique that works well for a wide variety of weathering phenomena. Our technique, called γ-ton tracing, is based on a type of aging-inducing particles called γ-tons. Modeling a weathering effect with γ-ton tracing involves tracing a large number of γ-tons through the scene in a way similar to photon tracing and then generating the weathering effect using the recorded γ-ton transport information. With this technique, we can produce weathering effects that are customized to the scene geometry and tailored to the weathering sources. Several effects that are challenging for existing techniques can be readily captured by γ-ton tracing. These include global transport effects. or "stainbleeding". γ-ton tracing also enables visual simulations of complex multi-weathering effects. Lastly γ-ton tracing can generate weathering effects that not only involve texture changes but also large-scale geometry changes. We demonstrate our technique with a variety of examples.	Visual simulation of weathering by γ-ton tracing	NA:NA:NA:NA:NA:NA:NA	2018
David Ebert	NA	Session details: Shape & texture	NA	2018
Takeo Igarashi:Tomer Moscovich:John F. Hughes	We present an interactive system that lets a user move and deform a two-dimensional shape without manually establishing a skeleton or freeform deformation (FFD) domain beforehand. The shape is represented by a triangle mesh and the user moves several vertices of the mesh as constrained handles. The system then computes the positions of the remaining free vertices by minimizing the distortion of each triangle. While physically based simulation or iterative refinement can also be used for this purpose, they tend to be slow. We present a two-step closed-form algorithm that achieves real-time interaction. The first step finds an appropriate rotation for each triangle and the second step adjusts its scale. The key idea is to use quadratic error metrics so that each minimization problem becomes a system of linear equations. After solving the simultaneous equations at the beginning of interaction, we can quickly find the positions of free vertices during interactive manipulation. Our approach successfully conveys a sense of rigidity of the shape, which is difficult in space-warp approaches. With a multiple-point input device, even beginners can easily move, rotate, and deform shapes at will.	As-rigid-as-possible shape manipulation	NA:NA:NA	2018
Andrew Nealen:Olga Sorkine:Marc Alexa:Daniel Cohen-Or	In this paper we present a method for the intuitive editing of surface meshes by means of view-dependent sketching. In most existing shape deformation work, editing is carried out by selecting and moving a handle, usually a set of vertices. Our system lets the user easily determine the handle, either by silhouette selection and cropping, or by sketching directly onto the surface. Subsequently, an edit is carried out by sketching a new, view-dependent handle position or by indirectly influencing differential properties along the sketch. Combined, these editing and handle metaphors greatly simplify otherwise complex shape modeling tasks.	A sketch-based interface for detail-preserving mesh editing	NA:NA:NA:NA	2018
Kun Zhou:Xi Wang:Yiying Tong:Mathieu Desbrun:Baining Guo:Heung-Yeung Shum	We propose a technique, called TextureMontage, to seamlessly map a patchwork of texture images onto an arbitrary 3D model. A texture atlas can be created through the specification of a set of correspondences between the model and any number of texture images. First, our technique automatically partitions the mesh and the images, driven solely by the choice of feature correspondences. Most charts will then be parameterized over their corresponding image planes through the minimization of a distortion metric based on both geometric distortion and texture mismatch across patch boundaries and images. Lastly, a surface texture inpainting technique is used to fill in the remaining charts of the surface with no corresponding texture patches. The resulting texture mapping satisfies the (sparse or dense) user-specified constraints while minimizing the distortion of the texture images and ensuring a smooth transition across the boundaries of different mesh patches. Seamless Texturing of Arbitrary Surfaces From Multiple Images	TextureMontage	NA:NA:NA:NA:NA:NA	2018
Nelson Max	NA	Session details: Ray tracing	NA	2018
Samuli Laine:Timo Aila:Ulf Assarsson:Jaakko Lehtinen:Tomas Akenine-Möller	We present a new, fast algorithm for rendering physically-based soft shadows in ray tracing-based renderers. Our method replaces the hundreds of shadow rays commonly used in stochastic ray tracers with a single shadow ray and a local reconstruction of the visibility function. Compared to tracing the shadow rays. our algorithm produces exactly the same image while executing one to two orders of magnitude faster in the test scenes used. Our first contribution is a two-stage method for quickly determining the silhouette edges that overlap an area light source, as seen from the point to be shaded. Secondly, we show that these partial silhouettes of occluders, along with a single shadow ray, are sufficient for reconstructing the visibility function between the point and the light source.	Soft shadow volumes for ray tracing	NA:NA:NA:NA:NA	2018
Petrik Clarberg:Wojciech Jarosz:Tomas Akenine-Möller:Henrik Wann Jensen	We present a new technique for importance sampling products of complex functions using wavelets. First, we generalize previous work on wavelet products to higher dimensional spaces and show how this product can be sampled on-the-fly without the need of evaluating the full product. This makes it possible to sample products of high-dimensional functions even if the product of the two functions in itself is too memory consuming. Then, we present a novel hierarchical sample warping algorithm that generates high-quality point distributions, which match the wavelet representation exactly. One application of the new sampling technique is rendering of objects with measured BRDFs illuminated by complex distant lighting --- our results demonstrate how the new sampling technique is more than an order of magnitude more efficient than the best previous techniques.	Wavelet importance sampling: efficiently evaluating products of complex functions	NA:NA:NA:NA	2018
Alexander Reshetov:Alexei Soupikov:Jim Hurley	We propose new approaches to ray tracing that greatly reduce the required number of operations while strictly preserving the geometrical correctness of the solution. A hierarchical "beam" structure serves as a proxy for a collection of rays. It is tested against a kd-tree representing the overall scene in order to discard from consideration the sub-set of the kd-tree (and hence the scene) that is guaranteed not to intersect with any possible ray inside the beam. This allows for all the rays inside the beam to start traversing the tree from some node deep inside thus eliminating unnecessary operations. The original beam can be further sub-divided, and we can either continue looking for new optimal entry points for the sub-beams, or we can decompose the beam into individual rays. This is a hierarchical process that can be adapted to the geometrical complexity of a particular view direction allowing for efficient geometric anti-aliasing. By amortizing the cost of partially traversing the tree for all the rays in a beam, up to an order of magnitude performance improvement can be achieved enabling interactivity for complex scenes on ordinary desktop machines.	Multi-level ray tracing algorithm	NA:NA:NA	2018
David Cline:Justin Talbot:Parris Egbert	We present Energy Redistribution (ER) sampling as an unbiased method to solve correlated integral problems. ER sampling is a hybrid algorithm that uses Metropolis sampling-like mutation strategies in a standard Monte Carlo integration setting, rather than resorting to an intermediate probability distribution step. In the context of global illumination, we present Energy Redistribution Path Tracing (ERPT). Beginning with an inital set of light samples taken from a path tracer, ERPT uses path mutations to redistribute the energy of the samples over the image plane to reduce variance. The result is a global illumination algorithm that is conceptually simpler than Metropolis Light Transport (MLT) while retaining its most powerful feature, path mutation. We compare images generated with the new technique to standard path tracing and MLT.	Energy redistribution path tracing	NA:NA:NA	2018
Wolfgang Heidrich	NA	Session details: Precomputed light transport	NA	2018
Kun Zhou:Yaohua Hu:Stephen Lin:Baining Guo:Heung-Yeung Shum	We present a soft shadow technique for dynamic scenes with moving objects under the combined illumination of moving local light sources and dynamic environment maps. The main idea of our technique is to precompute for each scene entity a shadow field that describes the shadowing effects of the entity at points around it. The shadow field for a light source, called a source radiance field (SRF), records radiance from an illuminant as cube maps at sampled points in its surrounding space. For an occluder, an object occlusion field (OOF) conversely represents in a similar manner the occlusion of radiance by an object. A fundamental difference between shadow fields and previous shadow computation concepts is that shadow fields can be precomputed independent of scene configuration. This is critical for dynamic scenes because, at any given instant, the shadow information at any receiver point can be rapidly computed as a simple combination of SRFs and OOFs according to the current scene configuration. Applications that particularly benefit from this technique include large dynamic scenes in which many instances of an entity can share a single shadow field. Our technique enables low-frequency shadowing effects in dynamic scenes in real-time and all-frequency shadows at interactive rates.	Precomputed shadow fields for dynamic scenes	NA:NA:NA:NA:NA	2018
Rui Wang:John Tran:David Luebke	We present a technique, based on precomputed light transport, for interactive rendering of translucent objects under all-frequency environment maps. We consider the complete BSSRDF model proposed by Jensen et al. [2001]. which includes both single and diffuse multiple scattering components. The challenge is how to efficiently precompute all-frequency light transport functions due to subsurface scattering. We apply the two-pass hierarchical technique by Jensen et al. [2002] in the space of non-linearly approximated transport vectors, which allows us to efficiently evaluate transport vectors due to diffuse multiple scattering. We then include an approximated single scattering term in the precomputation, which previous interactive systems have ignored. For an isotropic phase function, this approximation produces a diffuse transport vector per vertex, and is combined with the multiple scattering component. For a general phase function, we introduce a technique from BRDF rendering to factor the phase function using a separable decomposition to allow for view-dependent rendering. We show that our rendering results qualitatively match the appearance of translucent objects, achieving a high level of realism at interactive rates.	All-frequency interactive relighting of translucent objects with single and multiple scattering	NA:NA:NA	2018
Anders Wang Kristensen:Tomas Akenine-Möller:Henrik Wann Jensen	This paper introduces a new method for real-time relighting of scenes illuminated by local light sources. We extend previous work on precomputed radiance transfer for distant lighting to local lighting by introducing the concept of unstructured light clouds. The unstructured light cloud enables a compact representation of local lights in the model and real-time rendering of complex models with full global illumination due to local light sources. We use simplification of lights, and clustered PCA to obtain a compressed representation. When storing only the indirect component of the illumination, we are able to get high quality with only 8-16 lighting coefficients per vertex. Our results demonstrate real-time rendering of scenes with moving lights, dynamic cameras, glossy materials and global illumination.	Precomputed local radiance transfer for real-time lighting design	NA:NA:NA	2018
Kavita Bala	NA	Session details: Sampling and ray tracing	NA	2018
Ingo Wald:Thiago Ize:Andrew Kensler:Aaron Knoll:Steven G. Parker	We present a new approach to interactive ray tracing of moderate-sized animated scenes based on traversing frustum-bounded packets of coherent rays through uniform grids. By incrementally computing the overlap of the frustum with a slice of grid cells, we accelerate grid traversal by more than a factor of 10, and achieve ray tracing performance competitive with the fastest known packet-based kd-tree ray tracers. The ability to efficiently rebuild the grid on every frame enables this performance even for fully dynamic scenes that typically challenge interactive ray tracing systems.	Ray tracing animated scenes using coherent grid traversal	NA:NA:NA:NA:NA	2018
Peter Wonka:Michael Wimmer:Kaichi Zhou:Stefan Maierhofer:Gerd Hesina:Alexander Reshetov	This paper addresses the problem of computing the triangles visible from a region in space. The proposed aggressive visibility solution is based on stochastic ray shooting and can take any triangular model as input. We do not rely on connectivity information, volumetric occluders, or the availability of large occluders, and can therefore process any given input scene. The proposed algorithm is practically memoryless, thereby alleviating the large memory consumption problems prevalent in several previous algorithms. The strategy of our algorithm is to use ray mutations in ray space to cast rays that are likely to sample new triangles. Our algorithm improves the sampling efficiency of previous work by over two orders of magnitude.	Guided visibility sampling	NA:NA:NA:NA:NA:NA	2018
Daniel Dunbar:Greg Humphreys	Sampling distributions with blue noise characteristics are widely used in computer graphics. Although Poisson-disk distributions are known to have excellent blue noise characteristics, they are generally regarded as too computationally expensive to generate in real time. We present a new method for sampling by dart-throwing in O(N log N) time and introduce a novel and efficient variation for generating Poisson-disk distributions in O(N) time and space.	A spatial data structure for fast Poisson-disk sample generation	NA:NA	2018
Johannes Kopf:Daniel Cohen-Or:Oliver Deussen:Dani Lischinski	Well distributed point sets play an important role in a variety of computer graphics contexts, such as anti-aliasing, global illumination, halftoning, non-photorealistic rendering, point-based modeling and rendering, and geometry processing. In this paper, we introduce a novel technique for rapidly generating large point sets possessing a blue noise Fourier spectrum and high visual quality. Our technique generates non-periodic point sets, distributed over arbitrarily large areas. The local density of a point set may be prescribed by an arbitrary target density function, without any preset bound on the maximum density. Our technique is deterministic and tile-based; thus, any local portion of a potentially infinite point set may be consistently regenerated as needed. The memory footprint of the technique is constant, and the cost to generate any local portion of the point set is proportional to the integral over the target density in that area. These properties make our technique highly suitable for a variety of real-time interactive applications, some of which are demonstrated in the paper.Our technique utilizes a set of carefully constructed progressive and recursive blue noise Wang tiles. The use of Wang tiles enables the generation of infinite non-periodic tilings. The progressive point sets inside each tile are able to produce spatially varying point densities. Recursion allows our technique to adaptively subdivide tiles only where high density is required, and makes it possible to zoom into point sets by an arbitrary amount, while maintaining a constant apparent density.	Recursive Wang tiles for real-time blue noise	NA:NA:NA:NA	2018
Yizhou Yu	NA	Session details: Image processing	NA	2018
Ben Weiss	Median filtering is a cornerstone of modern image processing and is used extensively in smoothing and de-noising applications. The fastest commercial implementations (e.g. in Adobe® Photoshop® CS2) exhibit O(r) runtime in the radius of the filter, which limits their usefulness in realtime or resolution-independent contexts. We introduce a CPU-based, vectorizable O(log r) algorithm for median filtering, to our knowledge the most efficient yet developed. Our algorithm extends to images of any bit-depth, and can also be adapted to perform bilateral filtering. On 8-bit data our median filter outperforms Photoshop's implementation by up to a factor of fifty.	Fast median and bilateral filtering	NA	2018
Aude Oliva:Antonio Torralba:Philippe G. Schyns	We present hybrid images, a technique that produces static images with two interpretations, which change as a function of viewing distance. Hybrid images are based on the multiscale processing of images by the human visual system and are motivated by masking studies in visual perception. These images can be used to create compelling displays in which the image appears to change as the viewing distance changes. We show that by taking into account perceptual grouping mechanisms it is possible to build compelling hybrid images with stable percepts at each distance. We show examples in which hybrid images are used to create textures that become visible only when seen up-close, to generate facial expressions whose interpretation changes with viewing distance, and to visualize changes over time within a single picture.	Hybrid images	NA:NA:NA	2018
Scott Schaefer:Travis McPhail:Joe Warren	We provide an image deformation method based on Moving Least Squares using various classes of linear functions including affine, similarity and rigid transformations. These deformations are realistic and give the user the impression of manipulating real-world objects. We also allow the user to specify the deformations using either sets of points or line segments, the later useful for controlling curves and profiles present in the image. For each of these techniques, we provide simple closed-form solutions that yield fast deformations, which can be performed in real-time.	Image deformation using moving least squares	NA:NA:NA	2018
Sylvain Lefebvre:Hugues Hoppe	The traditional approach in texture synthesis is to compare color neighborhoods with those of an exemplar. We show that quality is greatly improved if pointwise colors are replaced by appearance vectors that incorporate nonlocal information such as feature and radiance-transfer data. We perform dimensionality reduction on these vectors prior to synthesis, to create a new appearance-space exemplar. Unlike a texton space, our appearance space is low-dimensional and Euclidean. Synthesis in this information-rich space lets us reduce runtime neighborhood vectors from 5x5 grids to just 4 locations. Building on this unifying framework, we introduce novel techniques for coherent anisometric synthesis, surface texture synthesis directly in an ordinary atlas, and texture advection. Remarkably, we achieve all these functionalities in real-time, or 3 to 4 orders of magnitude faster than prior work.	Appearance-space texture synthesis	NA:NA	2018
Ioana Boier-Martin	NA	Session details: Shape matching and symmetry	NA	2018
Joshua Podolak:Philip Shilane:Aleksey Golovinskiy:Szymon Rusinkiewicz:Thomas Funkhouser	Symmetry is an important cue for many applications, including object alignment, recognition, and segmentation. In this paper, we describe a planar reflective symmetry transform (PRST) that captures a continuous measure of the reflectional symmetry of a shape with respect to all possible planes. This transform combines and extends previous work that has focused on global symmetries with respect to the center of mass in 3D meshes and local symmetries with respect to points in 2D images. We provide an efficient Monte Carlo sampling algorithm for computing the transform for surfaces and show that it is stable under common transformations. We also provide an iterative refinement algorithm to find local maxima of the transform precisely. We use the transform to define two new geometric properties, center of symmetry and principal symmetry axes, and show that they are useful for aligning objects in a canonical coordinate system. Finally, we demonstrate that the symmetry transform is useful for several applications in computer graphics, including shape matching, segmentation of meshes into parts, and automatic viewpoint selection.	A planar-reflective symmetry transform for 3D shapes	NA:NA:NA:NA:NA	2018
Niloy J. Mitra:Leonidas J. Guibas:Mark Pauly	"Symmetry is a complexity-reducing concept [...]; seek it every-where." - Alan J. PerlisMany natural and man-made objects exhibit significant symmetries or contain repeated substructures. This paper presents a new algorithm that processes geometric models and efficiently discovers and extracts a compact representation of their Euclidean symmetries. These symmetries can be partial, approximate, or both. The method is based on matching simple local shape signatures in pairs and using these matches to accumulate evidence for symmetries in an appropriate transformation space. A clustering stage extracts potential significant symmetries of the object, followed by a verification step. Based on a statistical sampling analysis, we provide theoretical guarantees on the success rate of our algorithm. The extracted symmetry graph representation captures important high-level information about the structure of a geometric model which in turn enables a large set of further processing operations, including shape compression, segmentation, consistent editing, symmetrization, indexing for retrieval, etc.	Partial and approximate symmetry detection for 3D geometry	NA:NA:NA	2018
Qi-Xing Huang:Simon Flöry:Natasha Gelfand:Michael Hofer:Helmut Pottmann	We present a system for automatic reassembly of broken 3D solids. Given as input 3D digital models of the broken fragments, we analyze the geometry of the fracture surfaces to find a globally consistent reconstruction of the original object. Our reconstruction pipeline consists of a graph-cuts based segmentation algorithm for identifying potential fracture surfaces, feature-based robust global registration for pairwise matching of fragments, and simultaneous constrained local registration of multiple fragments. We develop several new techniques in the area of geometry processing, including the novel integral invariants for computing multi-scale surface characteristics, registration based on forward search techniques and surface consistency, and a non-penetrating iterated closest point algorithm. We illustrate the performance of our algorithms on a number of real-world examples.	Reassembling fractured objects by geometric matching	NA:NA:NA:NA:NA	2018
Sylvain Lefebvre:Hugues Hoppe	We explore using hashing to pack sparse data into a compact table while retaining efficient random access. Specifically, we design a perfect multidimensional hash function -- one that is precomputed on static data to have no hash collisions. Because our hash function makes a single reference to a small offset table, queries always involve exactly two memory accesses and are thus ideally suited for parallel SIMD evaluation on graphics hardware. Whereas prior hashing work strives for pseudorandom mappings, we instead design the hash function to preserve spatial coherence and thereby improve runtime locality of reference. We demonstrate numerous graphics applications including vector images, texture sprites, alpha channel compression, 3D-parameterized textures, 3D painting, simulation, and collision detection.	Perfect spatial hashing	NA:NA	2018
David Ebert	NA	Session details: Shape modeling and textures	NA	2018
Olga A. Karpenko:John F. Hughes	We introduce SmoothSketch---a system for inferring plausible 3D free-form shapes from visible-contour sketches. In our system, a user's sketch need not be a simple closed curve as in Igarashi's Teddy [1999], but may have cusps and T-junctions, i.e., endpoints of hidden parts of the contour. We follow a process suggested by Williams [1994] for inferring a smooth solid shape from its visible contours: completion of hidden contours, topological shape reconstruction, and smoothly embedding the shape via relaxation. Our main contribution is a practical method to go from a contour drawing to a fairly smooth surface with that drawing as its visible contour. In doing so, we make several technical contributions: • extending Williams' and Mumford's work [Mumford 1994] on figural completion of hidden contours containing T-junctions to contours containing cusps as well, • characterizing a class of visible-contour drawings for which inflation can be proved possible, • finding a topological embedding of the combinatorial surface that Williams creates from the figural completion, and • creating a fairly smooth solid shape by smoothing the topological embedding using a mass-spring system.We handle many kinds of drawings (including objects with holes), and the generated shapes are plausible interpretations of the sketches. The method can be incorporated into any sketch-based free-form modeling interface like Teddy.	SmoothSketch: 3D free-form shapes from complex sketches	NA:NA	2018
Long Quan:Ping Tan:Gang Zeng:Lu Yuan:Jingdong Wang:Sing Bing Kang	In this paper, we propose a semi-automatic technique for modeling plants directly from images. Our image-based approach has the distinct advantage that the resulting model inherits the realistic shape and complexity of a real plant. We designed our modeling system to be interactive, automating the process of shape recovery while relying on the user to provide simple hints on segmentation. Segmentation is performed in both image and 3D spaces, allowing the user to easily visualize its effect immediately. Using the segmented image and 3D data, the geometry of each leaf is then automatically recovered from the multiple views by fitting a deformable leaf model. Our system also allows the user to easily reconstruct branches in a similar manner. We show realistic reconstructions of a variety of plants, and demonstrate examples of plant editing.	Image-based plant modeling	NA:NA:NA:NA:NA:NA	2018
Ryan Schmidt:Cindy Grimm:Brian Wyvill	A method is described for texturing surfaces using decals, images placed on the surface using local parameterizations. Decal parameterizations are generated with a novel O(N log N) discrete approximation to the exponential map which requires only a single additional step in Dijkstra's graph-distance algorithm. Decals are dynamically composited in an interface that addresses many limitations of previous work. Tools for image processing, deformation/feature-matching, and vector graphics are implemented using direct surface interaction. Exponential map decals can contain holes and can also be combined with conformal parameterization to reduce distortion. The exponential map approximation can be computed on any point set, including meshes and sampled implicit surfaces, and is relatively stable under resampling. The decals stick to the surface as it is interactively deformed, allowing the texture to be preserved even if the surface changes topology. These properties make exponential map decals a suitable approach for texturing animated implicit surfaces.	Interactive decal compositing with discrete exponential maps	NA:NA:NA	2018
Pascal Müller:Peter Wonka:Simon Haegler:Andreas Ulmer:Luc Van Gool	CGA shape, a novel shape grammar for the procedural modeling of CG architecture, produces building shells with high visual quality and geometric detail. It produces extensive architectural models for computer games and movies, at low cost. Context sensitive shape rules allow the user to specify interactions between the entities of the hierarchical shape descriptions. Selected examples demonstrate solutions to previously unsolved modeling problems, especially to consistent mass modeling with volumetric shapes of arbitrary orientation. CGA shape is shown to efficiently generate massive urban models with unprecedented level of detail, with the virtual rebuilding of the archaeological site of Pompeii as a case in point.	Procedural modeling of buildings	NA:NA:NA:NA:NA	2018
Alexei Efros	NA	Session details: Image manipulation	NA	2018
Daniel Cohen-Or:Olga Sorkine:Ran Gal:Tommer Leyvand:Ying-Qing Xu	Harmonic colors are sets of colors that are aesthetically pleasing in terms of human visual perception. In this paper, we present a method that enhances the harmony among the colors of a given photograph or of a general image, while remaining faithful, as much as possible, to the original colors. Given a color image, our method finds the best harmonic scheme for the image colors. It then allows a graceful shifting of hue values so as to fit the harmonic scheme while considering spatial coherence among colors of neighboring pixels using an optimization technique. The results demonstrate that our method is capable of automatically enhancing the color "look-and-feel" of an ordinary image. In particular, we show the results of harmonizing the background image to accommodate the colors of a foreground image, or the foreground with respect to the background, in a cut-and-paste setting. Our color harmonization technique proves to be useful in adjusting the colors of an image composed of several parts taken from different sources.	Color harmonization	NA:NA:NA:NA:NA	2018
Jiaya Jia:Jian Sun:Chi-Keung Tang:Heung-Yeung Shum	In this paper, we present a user-friendly system for seamless image composition, which we call drag-and-drop pasting. We observe that for Poisson image editing [Perez et al. 2003] to work well, the user must carefully draw a boundary on the source image to indicate the region of interest, such that salient structures in source and target images do not conflict with each other along the boundary. To make Poisson image editing more practical and easy to use, we propose a new objective function to compute an optimized boundary condition. A shortest closed-path algorithm is designed to search for the location of the boundary. Moreover, to faithfully preserve the object's fractional boundary, we construct a blended guidance field to incorporate the object's alpha matte. To use our system, the user needs only to simply outline a region of interest in the source image, and then drag and drop it onto the target image. Experimental results demonstrate the effectiveness of our "drag-and-drop pasting" system.	Drag-and-drop pasting	NA:NA:NA:NA	2018
Soonmin Bae:Sylvain Paris:Frédo Durand	We introduce a new approach to tone management for photographs. Whereas traditional tone-mapping operators target a neutral and faithful rendition of the input image, we explore pictorial looks by controlling visual qualities such as the tonal balance and the amount of detail. Our method is based on a two-scale non-linear decomposition of an image. We modify the different layers based on their histograms and introduce a technique that controls the spatial variation of detail. We introduce a Poisson correction that prevents potential gradient reversal and preserves detail. In addition to directly controlling the parameters, the user can transfer the look of a model photograph to the picture being edited.	Two-scale tone management for photographic look	NA:NA:NA	2018
Dani Lischinski:Zeev Farbman:Matt Uyttendaele:Richard Szeliski	This paper presents a new interactive tool for making local adjustments of tonal values and other visual parameters in an image. Rather than carefully selecting regions or hand-painting layer masks, the user quickly indicates regions of interest by drawing a few simple brush strokes and then uses sliders to adjust the brightness, contrast, and other parameters in these regions. The effects of the user's sparse set of constraints are interpolated to the entire image using an edge-preserving energy minimization method designed to prevent the propagation of tonal adjustments to regions of significantly different luminance. The resulting system is suitable for adjusting ordinary and high dynamic range images, and provides the user with much more creative control than existing tone mapping algorithms. Our tool is also able to produce a tone mapping automatically, which may serve as a basis for further local adjustments, if so desired. The constraint propagation approach developed in this paper is a general one, and may also be used to interactively control a variety of other adjustments commonly performed in the digital darkroom.	Interactive local adjustment of tonal values	NA:NA:NA:NA	2018
Erum Arif Khan:Erik Reinhard:Roland W. Fleming:Heinrich H. Bülthoff	Photo editing software allows digital images to be blurred, warped or re-colored at the touch of a button. However, it is not currently possible to change the material appearance of an object except by painstakingly painting over the appropriate pixels. Here we present a method for automatically replacing one material with another, completely different material, starting with only a single high dynamic range image as input. Our approach exploits the fact that human vision is surprisingly tolerant of certain (sometimes enormous) physical inaccuracies, while being sensitive to others. By adjusting our simulations to be careful about those aspects to which the human visual system is sensitive, we are for the first time able to demonstrate significant material changes on the basis of a single photograph as input.	Image-based material editing	NA:NA:NA:NA	2018
Leif P. Kobbelt	NA	Session details: Surfaces	NA	2018
Charles Loop:Jim Blinn	We consider the problem of real-time GPU rendering of algebraic surfaces defined by Bézier tetrahedra. These surfaces are rendered directly in terms of their polynomial representations, as opposed to a collection of approximating triangles, thereby eliminating tessellation artifacts and reducing memory usage. A key step in such algorithms is the computation of univariate polynomial coefficients at each pixel; real roots of this polynomial correspond to possibly visible points on the surface. Our approach leverages the strengths of GPU computation and is highly efficient. Furthermore, we compute these coefficients in Bernstein form to maximize the stability of root finding, and to provide shader instances with an early exit test based on the sign of these coefficients. Solving for roots is done using analytic techniques that map well to a SIMD architecture, but limits us to fourth order algebraic surfaces. The general framework could be extended to higher order with numerical root finding.	Real-time GPU rendering of piecewise algebraic surfaces	NA:NA	2018
Anders Adamson:Marc Alexa	A piecewise smooth surface, possibly with boundaries, sharp edges, corners, or other features is defined by a set of samples. The basic idea is to model surface patches, curve segments and points explicitly, and then to glue them together based on explicit connectivity information. The geometry is defined as the set of stationary points of a projection operator, which is generalized to allow modeling curves with samples, and extended to account for the connectivity information. Additional tangent constraints can be used to model shapes with continuous tangents across edges and corners.	Point-sampled cell complexes	NA:NA	2018
Yang Liu:Helmut Pottmann:Johannes Wallner:Yong-Liang Yang:Wenping Wang	In architectural freeform design, the relation between shape and fabrication poses new challenges and requires more sophistication from the underlying geometry. The new concept of conical meshes satisfies central requirements for this application: They are quadrilateral meshes with planar faces, and therefore particularly suitable for the design of freeform glass structures. Moreover, they possess a natural offsetting operation and provide a support structure orthogonal to the mesh. Being a discrete analogue of the network of principal curvature lines, they represent fundamental shape characteristics. We show how to optimize a quad mesh such that its faces become planar, or the mesh becomes even conical. Combining this perturbation with subdivision yields a powerful new modeling tool for all types of quad meshes with planar faces, making subdivision attractive for architecture design and providing an elegant way of modeling developable surfaces.	Geometric modeling with conical meshes and developable surfaces	NA:NA:NA:NA:NA	2018
Kun Zhou:Xin Huang:Xi Wang:Yiying Tong:Mathieu Desbrun:Baining Guo:Heung-Yeung Shum	We introduce mesh quilting, a geometric texture synthesis algorithm in which a 3D texture sample given in the form of a triangle mesh is seamlessly applied inside a thin shell around an arbitrary surface through local stitching and deformation. We show that such geometric textures allow interactive and versatile editing and animation, producing compelling visual effects that are difficult to achieve with traditional texturing methods. Unlike pixel-based image quilting, mesh quilting is based on stitching together 3D geometry elements. Our quilting algorithm finds corresponding geometry elements in adjacent texture patches, aligns elements through local deformation, and merges elements to seamlessly connect texture patches. For mesh quilting on curved surfaces, a critical issue is to reduce distortion of geometry elements inside the 3D space of the thin shell. To address this problem we introduce a low-distortion parameterization of the shell space so that geometry elements can be synthesized even on very curved objects without the visual distortion present in previous approaches. We demonstrate how mesh quilting can be used to generate convincing decorations for a wide range of geometric textures.	Mesh quilting for geometric texture synthesis	NA:NA:NA:NA:NA:NA:NA	2018
Greg Ward	NA	Session details: HDR and systems	NA	2018
Jacob Munkberg:Petrik Clarberg:Jon Hasselgren:Tomas Akenine-Möller	In this paper, we break new ground by presenting algorithms for fixed-rate compression of high dynamic range textures at low bit rates. First, the S3TC low dynamic range texture compression scheme is extended in order to enable compression of HDR data. Second, we introduce a novel robust algorithm that offers superior image quality. Our algorithm can be efficiently implemented in hardware, and supports textures with a dynamic range of over 109:1. At a fixed rate of 8 bits per pixel, we obtain results virtually indistinguishable from uncompressed HDR textures at 48 bits per pixel. Our research can have a big impact on graphics hardware and real-time rendering, since HDR texturing suddenly becomes affordable.	High dynamic range texture compression for graphics hardware	NA:NA:NA:NA	2018
Kimmo Roimela:Tomi Aarnio:Joonas Itäranta	We present a novel compression scheme for high dynamic range textures, targeted for hardware implementation. Our method encodes images at a constant 8 bits per pixel, for a compression ratio of 6:1. We demonstrate that our method achieves good visual fidelity, surpassing DXTC texture compression of RGBE data which is the most practical method on existing graphics hardware. The decoding logic for our method is simple enough to be implemented as part of the texture fetch unit in graphics hardware.	High dynamic range texture compression	NA:NA:NA	2018
Rafał Mantiuk:Alexander Efremov:Karol Myszkowski:Hans-Peter Seidel	To embrace the imminent transition from traditional low-contrast video (LDR) content to superior high dynamic range (HDR) content, we propose a novel backward compatible HDR video compression (HDR MPEG) method. We introduce a compact reconstruction function that is used to decompose an HDR video stream into a residual stream and a standard LDR stream, which can be played on existing MPEG decoders, such as DVD players. The reconstruction function is finely tuned to the content of each HDR frame to achieve strong decorrelation between the LDR and residual streams, which minimizes the amount of redundant information. The size of the residual stream is further reduced by removing invisible details prior to compression using our HDR-enabled filter, which models luminance adaptation, contrast sensitivity, and visual masking based on the HDR content. Designed especially for DVD movie distribution, our HDR MPEG compression method features low storage requirements for HDR content resulting in a 30% size increase to an LDR video sequence. The proposed compression method does not impose restrictions or modify the appearance of the LDR or HDR video. This is important for backward compatibility of the LDR stream with current DVD appearance, and also enables independent fine tuning, tone mapping, and color grading of both streams.	Backward compatible high dynamic range MPEG video compression	NA:NA:NA:NA	2018
David Blythe	We present a system architecture for the 4th generation of PC-class programmable graphics processing units (GPUs). The new pipeline features significant additions and changes to the prior generation pipeline including a new programmable stage capable of generating additional primitives and streaming primitive data to memory, an expanded, common feature set for all of the programmable stages, generalizations to vertex and image memory resources, and new storage formats. We also describe structural modifications to the API, runtime, and shading language to complement the new pipeline. We motivate the design with descriptions of frequently encountered obstacles in current systems. Throughout the paper we present rationale behind prominent design choices and alternatives that were ultimately rejected, drawing on insights collected during a multi-year collaboration with application developers and hardware designers.	The Direct3D 10 system	NA	2018
Holly Rushmeier	NA	Session details: Appearance representation	NA	2018
Jason Lawrence:Aner Ben-Artzi:Christopher DeCoro:Wojciech Matusik:Hanspeter Pfister:Ravi Ramamoorthi:Szymon Rusinkiewicz	Recent progress in the measurement of surface reflectance has created a demand for non-parametric appearance representations that are accurate, compact, and easy to use for rendering. Another crucial goal, which has so far received little attention, is editability: for practical use, we must be able to change both the directional and spatial behavior of surface reflectance (e.g., making one material shinier, another more anisotropic, and changing the spatial "texture maps" indicating where each material appears). We introduce an Inverse Shade Tree framework that provides a general approach to estimating the "leaves" of a user-specified shade tree from high-dimensional measured datasets of appearance. These leaves are sampled 1- and 2-dimensional functions that capture both the directional behavior of individual materials and their spatial mixing patterns. In order to compute these shade trees automatically, we map the problem to matrix factorization and introduce a flexible new algorithm that allows for constraints such as non-negativity, sparsity, and energy conservation. Although we cannot infer every type of shade tree, we demonstrate the ability to reduce multi-gigabyte measured datasets of the Spatially-Varying Bidirectional Reflectance Distribution Function (SVBRDF) into a compact representation that may be edited in real time.	Inverse shade trees for non-parametric material representation and editing	NA:NA:NA:NA:NA:NA:NA	2018
Pieter Peers:Karl vom Berge:Wojciech Matusik:Ravi Ramamoorthi:Jason Lawrence:Szymon Rusinkiewicz:Philip Dutré	Many translucent materials exhibit heterogeneous subsurface scattering, which arises from complex internal structures. The acquisition and representation of these scattering functions is a complex problem that has been only partially addressed in previous techniques. Unlike homogeneous materials, the spatial component of heterogeneous subsurface scattering can vary arbitrarily over surface locations. Storing the spatial component without compression leads to impractically large datasets. In this paper, we address the problem of acquiring and compactly representing the spatial component of heterogeneous subsurface scattering functions. We propose a material model based on matrix factorization that can be mapped onto arbitrary geometry, and, due to its compact form, can be incorporated into most visualization systems with little overhead. We present results of several real-world datasets that are acquired using a projector and a digital camera.	A compact factored representation of heterogeneous subsurface scattering	NA:NA:NA:NA:NA:NA:NA	2018
Jiaping Wang:Xin Tong:Stephen Lin:Minghao Pan:Chao Wang:Hujun Bao:Baining Guo:Heung-Yeung Shum	We present a visual simulation technique called appearance manifolds for modeling the time-variant surface appearance of a material from data captured at a single instant in time. In modeling time-variant appearance, our method takes advantage of the key observation that concurrent variations in appearance over a surface represent different degrees of weathering. By reorganizing these various appearances in a manner that reveals their relative order with respect to weathering degree, our method infers spatial and temporal appearance properties of the material's weathering process that can be used to convincingly generate its weathered appearance at different points in time. Results with natural non-linear reflectance variations are demonstrated in applications such as visual simulation of weathering on 3D models, increasing and decreasing the weathering of real objects, and material transfer with weathering effects.	Appearance manifolds for modeling time-variant appearance of materials	NA:NA:NA:NA:NA:NA:NA:NA	2018
Jinwei Gu:Chien-I Tu:Ravi Ramamoorthi:Peter Belhumeur:Wojciech Matusik:Shree Nayar	For computer graphics rendering, we generally assume that the appearance of surfaces remains static over time. Yet, there are a number of natural processes that cause surface appearance to vary dramatically, such as burning of wood, wetting and drying of rock and fabric, decay of fruit skins, and corrosion and rusting of steel and copper. In this paper, we take a significant step towards measuring, modeling, and rendering time-varying surface appearance. We describe the acquisition of the first time-varying database of 26 samples, encompassing a variety of natural processes including burning, drying, decay, and corrosion. Our main technical contribution is a Space-Time Appearance Factorization (STAF). This model factors space and time-varying effects. We derive an overall temporal appearance variation characteristic curve of the specific process, as well as space-dependent textures, rates, and offsets. This overall temporal curve controls different spatial locations evolve at the different rates, causing spatial patterns on the surface over time. We show that the model accurately represents a variety of phenomena. Moreover, it enables a number of novel rendering applications, such as transfer of the time-varying effect to a new static surface, control to accelerate time evolution in certain areas, extrapolation beyond the acquired sequence, and texture synthesis of time-varying appearance.	Time-varying surface appearance: acquisition, modeling and rendering	NA:NA:NA:NA:NA:NA	2018
Paul Debevec	NA	Session details: Matting & deblurring	NA	2018
Jian Sun:Yin Li:Sing Bing Kang:Heung-Yeung Shum	In this paper, we propose a novel approach to extract mattes using a pair of flash/no-flash images. Our approach, which we call flash matting, was inspired by the simple observation that the most noticeable difference between the flash and no-flash images is the foreground object if the background scene is sufficiently distant. We apply a new matting algorithm called joint Bayesian flash matting to robustly recover the matte from flash/no-flash images, even for scenes in which the foreground and the background are similar or the background is complex. Experimental results involving a variety of complex indoors and outdoors scenes show that it is easy to extract high-quality mattes using an off-the-shelf, flash-equipped camera. We also describe extensions to flash matting for handling more general scenes.	Flash matting	NA:NA:NA:NA	2018
Neel Joshi:Wojciech Matusik:Shai Avidan	We present an algorithm and a system for high-quality natural video matting using a camera array. The system uses high frequencies present in natural scenes to compute mattes by creating a synthetic aperture image that is focused on the foreground object, which reduces the variance of pixels reprojected from the foreground while increasing the variance of pixels reprojected from the background. We modify the standard matting equation to work directly with variance measurements and show how these statistics can be used to construct a trimap that is later upgraded to an alpha matte. The entire process is completely automatic, including an automatic method for focusing the synthetic aperture image on the foreground object and an automatic method to compute the trimap and the alpha matte. The proposed algorithm is very efficient and has a per-pixel running time that is linear in the number of cameras. Our current system runs at several frames per second, and we believe that it is the first system capable of computing high-quality alpha mattes at near real-time rates without the use of active illumination or special backgrounds.	Natural video matting using camera arrays	NA:NA:NA	2018
Rob Fergus:Barun Singh:Aaron Hertzmann:Sam T. Roweis:William T. Freeman	Camera shake during exposure leads to objectionable image blur and ruins many photographs. Conventional blind deconvolution methods typically assume frequency-domain constraints on images, or overly simplified parametric forms for the motion path during camera shake. Real camera motions can follow convoluted paths, and a spatial domain prior can better maintain visually salient image characteristics. We introduce a method to remove the effects of camera shake from seriously blurred images. The method assumes a uniform camera blur over the image and negligible in-plane camera rotation. In order to estimate the blur from the camera shake, the user must specify an image region without saturation effects. We show results for a variety of digital photographs taken from personal photo collections.	Removing camera shake from a single photograph	NA:NA:NA:NA:NA	2018
Ramesh Raskar:Amit Agrawal:Jack Tumblin	In a conventional single-exposure photograph, moving objects or moving cameras cause motion blur. The exposure time defines a temporal box filter that smears the moving object across the image by convolution. This box filter destroys important high-frequency spatial details so that deblurring via deconvolution becomes an ill-posed problem.Rather than leaving the shutter open for the entire exposure duration, we "flutter" the camera's shutter open and closed during the chosen exposure time with a binary pseudo-random sequence. The flutter changes the box filter to a broad-band filter that preserves high-frequency spatial details in the blurred image and the corresponding deconvolution becomes a well-posed problem. We demonstrate that manually-specified point spread functions are sufficient for several challenging cases of motion-blur removal including extremely large motions, textured backgrounds and partial occluders.	Coded exposure photography: motion deblurring using fluttered shutter	NA:NA:NA	2018
Ming Lin	NA	Session details: Fluids	NA	2018
Geoffrey Irving:Eran Guendelman:Frank Losasso:Ronald Fedkiw	We present a new method for the efficient simulation of large bodies of water, especially effective when three-dimensional surface effects are important. Similar to a traditional two-dimensional height field approach, most of the water volume is represented by tall cells which are assumed to have linear pressure profiles. In order to avoid the limitations typically associated with a height field approach, we simulate the entire top surface of the water volume with a state of the art, fully three-dimensional Navier-Stokes free surface solver. Our philosophy is to use the best available method near the interface (in the three-dimensional region) and to coarsen the mesh away from the interface for efficiency. We coarsen with tall, thin cells (as opposed to octrees or AMR), because they maintain good resolution horizontally allowing for accurate representation of bottom topography.	Efficient simulation of large bodies of water by coupling two and three dimensional techniques	NA:NA:NA:NA	2018
Frank Losasso:Tamar Shinar:Andrew Selle:Ronald Fedkiw	The particle level set method has proven successful for the simulation of two separate regions (such as water and air, or fuel and products). In this paper, we propose a novel approach to extend this method to the simulation of as many regions as desired. The various regions can be liquids (or gases) of any type with differing viscosities, densities, viscoelastic properties, etc. We also propose techniques for simulating interactions between materials, whether it be simple surface tension forces or more complex chemical reactions with one material converting to another or two materials combining to form a third. We use a separate particle level set method for each region, and propose a novel projection algorithm that decodes the resulting vector of level set values providing a "dictionary" that translates between them and the standard single-valued level set representation. An additional difficulty occurs since discretization stencils (for interpolation, tracing semi-Lagrangian rays, etc.) cross region boundaries naively combining non-smooth or even discontinuous data. This has recently been addressed via ghost values, e.g. for fire or bubbles. We instead propose a new paradigm that allows one to incorporate physical jump conditions in data "on the fly," which is significantly more efficient for multiple regions especially at triple points or near boundaries with solids.	Multiple interacting liquids	NA:NA:NA:NA	2018
Bryan M. Klingner:Bryan E. Feldman:Nuttapong Chentanez:James F. O'Brien	This paper presents a method for animating fluid using unstructured tetrahedral meshes that change at each time step. We show that meshes that conform well to changing boundaries and that focus computation in the visually important parts of the domain can be generated quickly and reliably using existing techniques. We also describe a new approach to two-way coupling of fluid and rigid bodies that, while general, benefits from remeshing. Overall, the method provides a flexible environment for creating complex scenes involving fluid animation.	Fluid animation with dynamic meshes	NA:NA:NA:NA	2018
Adrien Treuille:Andrew Lewis:Zoran Popović	We present a new model reduction approach to fluid simulation, enabling large, real-time, detailed flows with continuous user interaction. Our reduced model can also handle moving obstacles immersed in the flow. We create separate models for the velocity field and for each moving boundary, and show that the coupling forces may be reduced as well. Our results indicate that surprisingly few basis functions are needed to resolve small but visually important features such as spinning vortices.	Model reduction for real-time fluids	NA:NA:NA	2018
Wojciech Matusik	NA	Session details: Image collections	NA	2018
Noah Snavely:Steven M. Seitz:Richard Szeliski	We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.	Photo tourism: exploring photo collections in 3D	NA:NA:NA	2018
Carsten Rother:Lucas Bordeaux:Youssef Hamadi:Andrew Blake	The paper defines an automatic procedure for constructing a visually appealing collage from a collection of input images. The aim is that the resulting collage should be representative of the collection, summarising its main themes. It is also assembled largely seamlessly, using graph-cut, Poisson blending of alpha-masks, to hide the joins between input images. This paper makes several new contributions. Firstly, we show how energy terms can be included that: encourage the selection of a representative set of images; that are sensitive to particular object classes; that encourage a spatially efficient and seamless layout. Secondly the resulting optimization poses a search problem that, on the face of it, is computationally in-feasible. Rather than attempt an expensive, integrated optimization procedure, we have developed a sequence of optimization steps, from static ranking of images, through region of interest optimization, optimal packing by constraint satisfaction, and lastly graph-cut alpha-expansion. To illustrate the power of AutoCollage, we have used it to create collages of many home photo sets; we also conducted a user study in which AutoCollage outperformed competitive methods.	AutoCollage	NA:NA:NA:NA	2018
Aseem Agarwala:Maneesh Agrawala:Michael Cohen:David Salesin:Richard Szeliski	We present a system for producing multi-viewpoint panoramas of long, roughly planar scenes, such as the facades of buildings along a city street, from a relatively sparse set of photographs captured with a handheld still camera that is moved along the scene. Our work is a significant departure from previous methods for creating multi-viewpoint panoramas, which composite thin vertical strips from a video sequence captured by a translating video camera, in that the resulting panoramas are composed of relatively large regions of ordinary perspective. In our system, the only user input required beyond capturing the photographs themselves is to identify the dominant plane of the photographed scene; our system then computes a panorama automatically using Markov Random Field optimization. Users may exert additional control over the appearance of the result by drawing rough strokes that indicate various high-level goals. We demonstrate the results of our system on several scenes, including urban streets, a river bank, and a grocery store aisle.	Photographing long scenes with multi-viewpoint panoramas	NA:NA:NA:NA:NA	2018
Dan B Goldman:Brian Curless:David Salesin:Steven M. Seitz	We present a method for visualizing short video clips in a single static image, using the visual language of storyboards. These schematic storyboards are composed from multiple input frames and annotated using outlines, arrows, and text describing the motion in the scene. The principal advantage of this storyboard representation over standard representations of video -- generally either a static thumbnail image or a playback of the video clip in its entirety -- is that it requires only a moment to observe and comprehend but at the same time retains much of the detail of the source video. Our system renders a schematic storyboard layout based on a small amount of user interaction. We also demonstrate an interaction technique to scrub through time using the natural spatial dimensions of the storyboard. Potential applications include video editing, surveillance summarization, assembly instructions, composition of graphic novels, and illustration of camera technique for film studies.	Schematic storyboarding for video visualization and editing	NA:NA:NA:NA	2018
Doug L. James	NA	Session details: Motion capture	NA	2018
Paul G. Kry:Dinesh K. Pai	Modifying motion capture to satisfy the constraints of new animation is difficult when contact is involved, and a critical problem for animation of hands. The compliance with which a character makes contact also reveals important aspects of the movement's purpose. We present a new technique called interaction capture, for capturing these contact phenomena. We capture contact forces at the same time as motion, at a high rate, and use both to estimate a nominal reference trajectory and joint compliance. Unlike traditional methods, our method estimates joint compliance without the need for motorized perturbation devices. New interactions can then be synthesized by physically based simulation. We describe a novel position-based linear complementarity problem formulation that includes friction, breaking contact, and the compliant coupling between contacts at different fingers. The technique is validated using data from previous work and our own perturbation-based estimates.	Interaction capture and synthesis	NA:NA	2018
Sang Il Park:Jessica K. Hodgins	During dynamic activities, the surface of the human body moves in many subtle but visually significant ways: bending, bulging, jiggling, and stretching. We present a technique for capturing and animating those motions using a commercial motion capture system and approximately 350 markers. Although the number of markers is significantly larger than that used in conventional motion capture, it is only a sparse representation of the true shape of the body. We supplement this sparse sample with a detailed, actor-specific surface model. The motion of the skin can then be computed by segmenting the markers into the motion of a set of rigid parts and a residual deformation (approximated first as a quadratic transformation and then with radial basis functions). We demonstrate the power of this approach by capturing flexing muscles, high frequency motions, and abrupt decelerations on several actors. We compare these results both to conventional motion capture and skinning and to synchronized video of the actors.	Capturing and animating skin deformation in human motion	NA:NA	2018
Okan Arikan	We present a lossy compression algorithm for large databases of motion capture data. We approximate short clips of motion using Bezier curves and clustered principal component analysis. This approximation has a smoothing effect on the motion. Contacts with the environment (such as foot strikes) have important detail that needs to be maintained. We compress these environmental contacts using a separate, JPEG like compression algorithm and ensure these contacts are maintained during decompression.Our method can compress 6 hours 34 minutes of human motion capture from 1080 MB data into 35.5 MB with little visible degradation. Compression and decompression is fast: our research implementation can decompress at about 1.2 milliseconds/frame, 7 times faster than real-time (for 120 frames per second animation). Our method also yields smaller compressed representation for the same error or produces smaller error for the same compressed size.	Compression of motion capture databases	NA	2018
Kang Hoon Lee:Myung Geol Choi:Jehee Lee	Real time animation of human figures in virtual environments is an important problem in the context of computer games and virtual environments. Recently, the use of large collections of captured motion data has increased realism in character animation. However, assuming that the virtual environment is large and complex, the effort of capturing motion data in a physical environment and adapting them to an extended virtual environment is the bottleneck for achieving interactive character animation and control. We present a new technique for allowing our animated characters to navigate through a large virtual environment, which is constructed using a set of building blocks. The building blocks, called motion patches, can be arbitrarily assembled to create novel environments. Each patch is annotated with motion data, which informs what actions are available for animated characters within the block. The versatility and flexibility of our approach are demonstrated through examples in which multiple characters are animated and controlled at interactive rates in large, complex virtual environments.	Motion patches: building blocks for virtual environments annotated with motion data	NA:NA:NA	2018
Szymon Rusinkiewicz	NA	Session details: Image capture	NA	2018
Li Zhang:Shree Nayar	In order to produce bright images, projectors have large apertures and hence narrow depths of field. In this paper, we present methods for robust scene capture and enhanced image display based on projection defocus analysis. We model a projector's defocus using a linear system. This model is used to develop a novel temporal defocus analysis method to recover depth at each camera pixel by estimating the parameters of its projection defocus kemel in frequency domain. Compared to most depth recovery methods, our approach is more accurate near depth discontinuities. Furthermore, by using a coaxial projector-camera system, we ensure that depth is computed at all camera pixels, without any missing parts. We show that the recovered scene geometry can be used for refocus synthesis and for depth-based image composition. Using the same projector defocus model and estimation technique, we also propose a defocus compensation method that filters a projection image in a spatially-varying, depth-dependent manner to minimize its defocus blur after it is projected onto the scene. This method effectively increases the depth of field of a projector without modifying its optics. Finally, we present an algorithm that exploits projector defocus to reduce the strong pixelation artifacts produced by digital projectors, while preserving the quality of the projected image. We have experimentally verified each of our methods using real scenes.	Projection defocus analysis for scene capture and image display	NA:NA	2018
Sujit Kuthirummal:Shree K. Nayar	In this paper, we present a class of imaging systems, called radial imaging systems, that capture a scene from a large number of view-points within a single image, using a camera and a curved mirror. These systems can recover scene properties such as geometry, reflectance, and texture. We derive analytic expressions that describe the properties of a complete family of radial imaging systems, including their loci of viewpoints, fields of view, and resolution characteristics. We have built radial imaging systems that, from a single image, recover the frontal 3D structure of an object, generate the complete texture map of a convex object, and estimate the parameters of an analytic BRDF model for an isotropic material. In addition, one of our systems can recover the complete geometry of a convex object by capturing only two images. These results show that radial imaging systems are simple, effective, and convenient devices for a wide range of applications in computer graphics and computer vision.	Multiview radial catadioptric imaging for scene capture	NA:NA	2018
Marc Levoy:Ren Ng:Andrew Adams:Matthew Footer:Mark Horowitz	By inserting a microlens array into the optical train of a conventional microscope, one can capture light fields of biological specimens in a single photograph. Although diffraction places a limit on the product of spatial and angular resolution in these light fields, we can nevertheless produce useful perspective views and focal stacks from them. Since microscopes are inherently orthographic devices, perspective views represent a new way to look at microscopic specimens. The ability to create focal stacks from a single photograph allows moving or light-sensitive specimens to be recorded. Applying 3D deconvolution to these focal stacks, we can produce a set of cross sections, which can be visualized using volume rendering. In this paper, we demonstrate a prototype light field microscope (LFM), analyze its optical performance, and show perspective views, focal stacks, and reconstructed volumes for a variety of biological specimens. We also show that synthetic focusing followed by 3D deconvolution is equivalent to applying limited-angle tomography directly to the 4D light field.	Light field microscopy	NA:NA:NA:NA:NA	2018
Shree K. Nayar:Gurunandan Krishnan:Michael D. Grossberg:Ramesh Raskar	We present fast methods for separating the direct and global illumination components of a scene measured by a camera and illuminated by a light source. In theory, the separation can be done with just two images taken with a high frequency binary illumination pattern and its complement. In practice, a larger number of images are used to overcome the optical and resolution limitations of the camera and the source. The approach does not require the material properties of objects and media in the scene to be known. However, we require that the illumination frequency is high enough to adequately sample the global components received by scene points. We present separation results for scenes that include complex interreflections, subsurface scattering and volumetric scattering. Several variants of the separation approach are also described. When a sinusoidal illumination pattern is used with different phase shifts, the separation can be done using just three images. When the computed images are of lower resolution than the source and the camera, smoothness constraints are used to perform the separation using a single image. Finally, in the case of a static scene that is lit by a simple point source, such as the sun, a moving occluder and a video camera can be used to do the separation. We also show several simple examples of how novel images of a scene can be computed from the separation results.	Fast separation of direct and global components of a scene using high frequency illumination	NA:NA:NA:NA	2018
Fabio Pellacini	NA	Session details: Precomputed transfer	NA	2018
Aner Ben-Artzi:Ryan Overbeck:Ravi Ramamoorthi	Current systems for editing BRDFs typically allow users to adjust analytic parameters while visualizing the results in a simplified setting (e.g. unshadowed point light). This paper describes a real-time rendering system that enables interactive edits of BRDFs, as rendered in their final placement on objects in a static scene, lit by direct, complex illumination. All-frequency effects (ranging from near-mirror reflections and hard shadows to diffuse shading and soft shadows) are rendered using a precomputation-based approach. Inspired by real-time relighting methods, we create a linear system that fixes lighting and view to allow real-time BRDF manipulation. In order to linearize the image's response to BRDF parameters, we develop an intermediate curve-based representation, which also reduces the rendering and precomputation operations to 1D while maintaining accuracy for a very general class of BRDFs. Our system can be used to edit complex analytic BRDFs (including anisotropic models), as well as measured reflectance data. We improve on the standard precomputed radiance transfer (PRT) rendering computation by introducing an incremental rendering algorithm that takes advantage of frame-to-frame coherence. We show that it is possible to render reference-quality images while only updating 10% of the data at each frame, sustaining frame-rates of 25-30fps.	Real-time BRDF editing in complex lighting	NA:NA:NA	2018
Weifeng Sun:Amar Mukherjee	We consider real-time rendering of dynamic glossy objects with realistic shadows under distant all-frequency environment lighting. Previous PRT approaches pre-compute light transport for a fixed scene and cannot account for cast shadows on high-glossy objects occluded by dynamic neighbors. In this paper, we extend double/triple product integral to generalized multi-function product integral. We represent shading integral at each vertex as the product integral of multiple functions, involving the lighting, BRDF, local visibility and dynamic occlusions. Our main contribution is a new mathematical representation and analysis of multi-function product integral in the wavelet domain. We show that multi-function product integral in the primal corresponds to the summation of the product of basis coefficients and integral coefficients. We propose a novel generalized Haar integral coefficient theorem to evaluate arbitrary Haar integral coefficients. We present an efficient sub-linear algorithm to render dynamic glossy objects under time-variant all-frequency lighting and arbitrary view conditions in a few seconds on a commodity CPU, orders of magnitude faster than previous techniques. To further accelerate shadow computation, we propose a Just-in-time Radiance Transfer (JRT) technique. JRT is a new generalization to PRT for dynamic scenes. It is compact and flexible, and supports glossy materials. By pre-computing radiance transfer vectors at runtime, we demonstrate rendering dynamic view-dependent all-frequency shadows in real-time.	Generalized wavelet product integral for rendering dynamic glossy objects	NA:NA	2018
Yu-Ting Tsai:Zen-Chung Shih	This paper introduces a new data representation and compression technique for precomputed radiance transfer (PRT). The light transfer functions and light sources are modeled with spherical radial basis functions (SRBFs). A SRBF is a rotation-invariant function that depends on the geodesic distance between two points on the unit sphere. Rotating functions in SRBF representation is as straightforward as rotating the centers of SRBFs. Moreover, high-frequency signals are handled by adjusting the bandwidth parameters of SRBFs. To exploit inter-vertex coherence, the light transfer functions are further classified iteratively into disjoint clusters, and tensor approximation is applied within each cluster. Compared with previous methods, the proposed approach enables real-time rendering with comparable quality under high-frequency lighting environments. The data storage is also more compact than previous all-frequency PRT algorithms.	All-frequency precomputed radiance transfer using spherical radial basis functions and clustered tensor approximation	NA:NA	2018
Zhong Ren:Rui Wang:John Snyder:Kun Zhou:Xinguo Liu:Bo Sun:Peter-Pike Sloan:Hujun Bao:Qunsheng Peng:Baining Guo	Previous methods for soft shadows numerically integrate over many light directions at each receiver point, testing blocker visibility in each direction. We introduce a method for real-time soft shadows in dynamic scenes illuminated by large, low-frequency light sources where such integration is impractical. Our method operates on vectors representing low-frequency visibility of blockers in the spherical harmonic basis. Blocking geometry is modeled as a set of spheres; relatively few spheres capture the low-frequency blocking effect of complicated geometry. At each receiver point, we compute the product of visibility vectors for these blocker spheres as seen from the point. Instead of computing an expensive SH product per blocker as in previous work, we perform inexpensive vector sums to accumulate the log of blocker visibility. SH exponentiation then yields the product visibility vector over all blockers. We show how the SH exponentiation required can be approximated accurately and efficiently for low-order SH, accelerating previous CPU-based methods by a factor of 10 or more, depending on blocker complexity, and allowing real-time GPU implementation.	Real-time soft shadows in dynamic scenes using spherical harmonic exponentiation	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Doug L. James:Jernej Barbič:Dinesh K. Pai	Simulating sounds produced by realistic vibrating objects is challenging because sound radiation involves complex diffraction and interreflection effects that are very perceptible and important. These wave phenomena are well understood, but have been largely ignored in computer graphics due to the high cost and complexity of computing them at audio rates.We describe a new algorithm for real-time synthesis of realistic sound radiation from rigid objects. We start by precomputing the linear vibration modes of an object, and then relate each mode to its sound pressure field, or acoustic transfer function, using standard methods from numerical acoustics. Each transfer function is then approximated to a specified accuracy using low-order multi-pole sources placed near the object. We provide a low-memory, multilevel, randomized algorithm for optimized source placement that is suitable for complex geometries. At runtime, we can simulate new interaction sounds by quickly summing contributions from each mode's equivalent multipole sources. We can efficiently simulate global effects such as interreflection and changes in sound due to listener location. The simulation costs can be dynamically traded-off for sound quality. We present several examples of sound generation from physically based animations.	Precomputed acoustic transfer: output-sensitive, accurate sound generation for geometrically complex vibration sources	NA:NA:NA	2018
Philip Dutré	NA	Session details: Appearance modeling	NA	2018
Kshitiz Garg:Shree K. Nayar	Photorealistic rendering of rain streaks with lighting and viewpoint effects is a challenging problem. Raindrops undergo rapid shape distortions as they fall, a phenomenon referred to as oscillations. Due to these oscillations, the reflection of light by, and the refraction of light through, a falling raindrop produce complex brightness patterns within a single motion-blurred rain streak captured by a camera or observed by a human. The brightness pattern of a rain streak typically includes speckles, multiple smeared highlights and curved brightness contours. In this work, we propose a new model for rain streak appearance that captures the complex interactions between the lighting direction, the viewing direction and the oscillating shape of the drop. Our model builds upon a raindrop oscillation model that has been developed in atmospheric sciences. We have measured rain streak appearances under a wide range of lighting and viewing conditions and empirically determined the oscillation parameters that are dominant in raindrops. Using these parameters, we have rendered thousands of rain streaks to create a database that captures the variations in streak appearance with respect to lighting and viewing directions. We have developed an efficient image-based rendering algorithm that uses our streak database to add rain to a single image or a captured video with moving objects and sources. The rendering algorithm is very simple to use as it only requires a coarse depth map of the scene and the locations and properties of the light sources. We have rendered rain in a wide range of scenarios and the results show that our physically-based rain streak model greatly enhances the visual realism of rendered rain.	Photorealistic rendering of rain streaks	NA:NA	2018
Srinivasa G. Narasimhan:Mohit Gupta:Craig Donner:Ravi Ramamoorthi:Shree K. Nayar:Henrik Wann Jensen	The visual world around us displays a rich set of volumetric effects due to participating media. The appearance of these media is governed by several physical properties such as particle densities, shapes and sizes, which must be input (directly or indirectly) to a rendering algorithm to generate realistic images. While there has been significant progress in developing rendering techniques (for instance, volumetric Monte Carlo methods and analytic approximations), there are very few methods that measure or estimate these properties for media that are of relevance to computer graphics. In this paper, we present a simple device and technique for robustly estimating the properties of a broad class of participating media that can be either (a) diluted in water such as juices, beverages, paints and cleaning supplies, or (b) dissolved in water such as powders and sugar/salt crystals, or (c) suspended in water such as impurities. The key idea is to dilute the concentrations of the media so that single scattering effects dominate and multiple scattering becomes negligible, leading to a simple and robust estimation algorithm. Furthermore, unlike previous approaches that require complicated or separate measurement setups for different types or properties of media, our method and setup can be used to measure media with a complete range of absorption and scattering properties from a single HDR photograph. Once the parameters of the diluted medium are estimated, a volumetric Monte Carlo technique may be used to create renderings of any medium concentration and with multiple scattering. We have measured the scattering parameters of forty commonly found materials, that can be immediately used by the computer graphics community. We can also create realistic images of combinations or mixtures of the original measured materials, thus giving the user a wide flexibility in making realistic images of participating media.	Acquiring scattering properties of participating media by dilution	NA:NA:NA:NA:NA:NA	2018
Tim Weyrich:Wojciech Matusik:Hanspeter Pfister:Bernd Bickel:Craig Donner:Chien Tu:Janet McAndless:Jinho Lee:Addy Ngan:Henrik Wann Jensen:Markus Gross	We have measured 3D face geometry, skin reflectance, and subsurface scattering using custom-built devices for 149 subjects of varying age, gender, and race. We developed a novel skin reflectance model whose parameters can be estimated from measurements. The model decomposes the large amount of measured skin data into a spatially-varying analytic BRDF, a diffuse albedo map, and diffuse subsurface scattering. Our model is intuitive, physically plausible, and -- since we do not use the original measured data -- easy to edit as well. High-quality renderings come close to reproducing real photographs. The analysis of the model parameters for our sample population reveals variations according to subject age, gender, skin type, and external factors (e.g., sweat, cold, or makeup). Using our statistics, a user can edit the overall appearance of a face (e.g., changing skin type and age) or change small-scale features using texture synthesis (e.g., adding moles and freckles). We are making the collected statistics publicly available to the research community for applications in face synthesis and analysis.	Analysis of human faces using a measurement-based skin reflectance model	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Aleksey Golovinskiy:Wojciech Matusik:Hanspeter Pfister:Szymon Rusinkiewicz:Thomas Funkhouser	Detailed surface geometry contributes greatly to the visual realism of 3D face models. However, acquiring high-resolution face geometry is often tedious and expensive. Consequently, most face models used in games, virtual reality, or computer vision look unrealistically smooth. In this paper, we introduce a new statistical technique for the analysis and synthesis of small three-dimensional facial features, such as wrinkles and pores. We acquire high-resolution face geometry for people across a wide range of ages, genders, and races. For each scan, we separate the skin surface details from a smooth base mesh using displaced subdivision surfaces. Then, we analyze the resulting displacement maps using the texture analysis/synthesis framework of Heeger and Bergen, adapted to capture statistics that vary spatially across a face. Finally, we use the extracted statistics to synthesize plausible detail on face meshes of arbitrary subjects. We demonstrate the effectiveness of this method in several applications, including analysis of facial texture in subjects with different ages and genders, interpolation between high-resolution face scans, adding detail to low-resolution face scans, and adjusting the apparent age of faces. In all cases, we are able to re-produce fine geometric details consistent with those observed in high resolution scans.	A statistical model for synthesis of detailed facial geometry	NA:NA:NA:NA:NA	2018
Joe Warren	NA	Session details: Meshes	NA	2018
Adi Levin	We present a modification to subdivision surfaces, which guarantees second-order smoothness everywhere in the surface, including extraordinary points. The idea is to blend the limit surface with a low degree polynomial defined over the characteristic map, in the vicinity of each extraordinary point. We demonstrate our method on Catmull-Clark surfaces, but a similar modification can be applied to other schemes as well. The proposed modification to Catmull-Clark is simple to implement and can be applied to quad meshes of arbitrary topological type, even when extraordinary vertices share edges.	Modified subdivision surfaces with continuous curvature	NA	2018
Ke Wang:Weiwei:Yiying Tong:Mathieu Desbrun:Peter Schröder	Vertex- and face-based subdivision schemes are now routinely used in geometric modeling and computational science, and their primal/dual relationships are well studied. In this paper, we interpret these schemes as defining bases for discrete differential 0- resp. 2-forms, and complete the picture by introducing edge-based subdivision schemes to construct the missing bases for discrete differential 1-forms. Such subdivision schemes map scalar coefficients on edges from the coarse to the refined mesh and are intrinsic to the surface. Our construction is based on treating vertex-, edge-, and face-based subdivision schemes as a joint triple and enforcing that subdivision commutes with the topological exterior derivative. We demonstrate our construction for the case of arbitrary topology triangle meshes. Using Loop's scheme for 0-forms and generalized half-box splines for 2-forms results in a unique generalized spline scheme for 1-forms, easily incorporated into standard subdivision surface codes. We also provide corresponding boundary stencils. Once a metric is supplied, the scalar 1-form coefficients define a smooth tangent vector field on the underlying subdivision surface. Design of tangent vector fields is made particularly easy with this machinery as we demonstrate.	Edge subdivision schemes and the construction of smooth vector fields	NA:NA:NA:NA:NA	2018
Martin Isenburg:Yuanxin Liu:Jonathan Shewchuk:Jack Snoeyink	We show how to greatly accelerate algorithms that compute Delaunay triangulations of huge, well-distributed point sets in 2D and 3D by exploiting the natural spatial coherence in a stream of points. We achieve large performance gains by introducing spatial finalization into point streams: we partition space into regions, and augment a stream of input points with finalization tags that indicate when a point is the last in its region. By extending an incremental algorithm for Delaunay triangulation to use finalization tags and produce streaming mesh output, we compute a billion-triangle terrain representation for the Neuse River system from 11.2 GB of LIDAR data in 48 minutes using only 70 MB of memory on a laptop with two hard drives. This is a factor of twelve faster than the previous fastest out-of-core Delaunay triangulation software.	Streaming computation of Delaunay triangulations	NA:NA:NA:NA	2018
Shen Dong:Peer-Timo Bremer:Michael Garland:Valerio Pascucci:John C. Hart	Resampling raw surface meshes is one of the most fundamental operations used by nearly all digital geometry processing systems. The vast majority of this work has focused on triangular remeshing, yet quadrilateral meshes are preferred for many surface PDE problems, especially fluid dynamics, and are best suited for defining Catmull-Clark subdivision surfaces. We describe a fundamentally new approach to the quadrangulation of manifold polygon meshes using Laplacian eigenfunctions, the natural harmonics of the surface. These surface functions distribute their extrema evenly across a mesh, which connect via gradient flow into a quadrangular base mesh. An iterative relaxation algorithm simultaneously refines this initial complex to produce a globally smooth parameterization of the surface. From this, we can construct a well-shaped quadrilateral mesh with very few extraordinary vertices. The quality of this mesh relies on the initial choice of eigenfunction, for which we describe algorithms and hueristics to efficiently and effectively select the harmonic most appropriate for the intended application.	Spectral surface quadrangulation	NA:NA:NA:NA:NA	2018
Ravi Ramamoorthi	NA	Session details: Light transport	NA	2018
Jonathan T. Moon:Stephen R. Marschner	Simulating multiple scattering correctly is important for accurate rendering of hair. However, a volume of hair is a difficult scene to simulate because scattering from an individual fiber is very structured and forward directed, and because the radiance distributions that arise from many such scattering events remain quite directional. For these reasons, previous methods cannot compute accurate images substantially faster than Monte Carlo path tracing.This paper proposes a new physically accurate method for rendering hair that is based on previous volumetric photon mapping methods. The first pass generates a photon map by tracing particles through the hair geometry, depositing them along paths rather than at scattering events. The second pass ray traces the hair, computing direct illumination and looking up indirect radiance in the photon map. Photons are stored and looked up in 5D position-direction space to allow for the very directional radiance distributions that occur in hair. Together with a new radiance caching method for fibers, our method simulates difficult scattering problems in hair efficiently and with low noise.The new algorithm is validated against path tracing and also compared with a photograph of light scattering in real hair.	Simulating multiple scattering in hair using a photon mapping approach	NA:NA	2018
Mark Meyer:John Anderson	Global illumination provides important visual cues to an animation, however its computational expense limits its use in practice. In this paper, we present an easy to implement technique for accelerating the computation of indirect illumination for an animated sequence using stochastic ray tracing. We begin by computing a quick but noisy solution using a small number of sample rays at each sample location. The variation of these noisy solutions over time is then used to create a smooth basis. Finally, the noisy solutions are projected onto the smooth basis to produce the final solution. The resulting animation has greatly reduced spatial and temporal noise, and a computational cost roughly equivalent to the noisy, low sample computation.	Statistical acceleration for animated global illumination	NA:NA	2018
Bruce Walter:Adam Arbree:Kavita Bala:Donald P. Greenberg	Multidimensional lightcuts is a new scalable method for efficiently rendering rich visual effects such as motion blur, participating media, depth of field, and spatial anti-aliasing in complex scenes. It introduces a flexible, general rendering framework that unifies the handling of such effects by discretizing the integrals into large sets of gather and light points and adaptively approximating the sum of all possible gather-light pair interactions.We create an implicit hierarchy, the product graph, over the gather-light pairs to rapidly and accurately approximate the contribution from hundreds of millions of pairs per pixel while only evaluating a tiny fraction (e.g., 200--1,000). We build upon the techniques of the prior Lightcuts method for complex illumination at a point, however, by considering the complete pixel integrals, we achieve much greater efficiency and scalability.Our example results demonstrate efficient handling of volume scattering, camera focus, and motion of lights, cameras, and geometry. For example, enabling high quality motion blur with 256x temporal sampling requires only a 6.7x increase in shading cost in a scene with complex moving geometry, materials, and illumination.	Multidimensional lightcuts	NA:NA:NA:NA	2018
Miloš Hašan:Fabio Pellacini:Kavita Bala	This paper presents an interactive GPU-based system for cinematic relighting with multiple-bounce indirect illumination from a fixed view-point. We use a deep frame-buffer containing a set of view samples, whose indirect illumination is recomputed from the direct illumination on a large set of gather samples, distributed around the scene. This direct-to-indirect transfer is a linear transform which is particularly large, given the size of the view and gather sets. This makes it hard to precompute, store and multiply with. We address this problem by representing the transform as a set of sparse matrices encoded in wavelet space. A hierarchical construction is used to impose a wavelet basis on the unstructured gather cloud, and an image-based approach is used to map the sparse matrix computations to the GPU. We precompute the transfer matrices using a hierarchical algorithm and a variation of photon mapping in less than three hours on one processor. We achieve high-quality indirect illumination at 10-20 frames per second for complex scenes with over 2 million polygons, with diffuse and glossy materials, and arbitrary direct lighting models (expressed using shaders). We compute per-pixel indirect illumination without the need of irradiance caching or other subsampling techniques.	Direct-to-indirect transfer for cinematic relighting	NA:NA:NA	2018
Marc Alexa	NA	Session details: Shape deformation	NA	2018
Scott Kircher:Michael Garland	Deforming surfaces, such as cloth, can be generated through physical simulation, morphing, and even video capture. Such data is currently very difficult to alter after the generation process is complete, and data generated for one purpose generally cannot be adapted to other uses. Such adaptation would be extremely useful, however. Being able to take cloth captured from a flapping flag and attach it to a character to make a cape, or enhance the wrinkles on a simulated garment, would greatly enhance the usability and re-usability of deforming surface data. In addition, it is often necessary to cleanup or "tweak" simulation results. Doing this by editing each frame individually is a very time consuming and tedious process. Extensive research has investigated how to edit and re-use skeletal motion capture data, but very little has addressed completely non-rigid deforming surfaces. We have developed a novel method that now makes it easy to edit such arbitrary deforming surfaces. Our system enables global signal processing, direct manipulation, multiresolution embossing, and constraint editing on arbitrarily deforming surfaces, such as simulated cloth, motion-captured cloth, morphs, and other animations. The foundation of our method is a novel time-varying multiresolution transform, which adapts to the changing geometry of the surface in a temporally coherent manner.	Editing arbitrarily deforming surface animations	NA:NA	2018
Lin Shi:Yizhou Yu:Nathan Bell:Wei-Wen Feng	In this paper, we present a multigrid technique for efficiently deforming large surface and volume meshes. We show that a previous least-squares formulation for distortion minimization reduces to a Laplacian system on a general graph structure for which we derive an analytic expression. We then describe an efficient multigrid algorithm for solving the relevant equations. Here we develop novel prolongation and restriction operators used in the multigrid cycles. Combined with a simple but effective graph coarsening strategy, our algorithm can outperform other multigrid solvers and the factorization stage of direct solvers in both time and memory costs for large meshes. It is demonstrated that our solver can trade off accuracy for speed to achieve greater interactivity, which is attractive for manipulating large meshes. Our multigrid solver is particularly well suited for a mesh editing environment which does not permit extensive precomputation. Experimental evidence of these advantages is provided on a number of meshes with a wide range of size. With our mesh deformation solver, we also successfully demonstrate that visually appealing mesh animations can be generated from both motion capture data and a single base mesh even when they are inconsistent.	A fast multigrid algorithm for mesh deformation	NA:NA:NA:NA	2018
Wolfram von Funck:Holger Theisel:Hans-Peter Seidel	We present an approach to define shape deformations by constructing and interactively modifying C1 continuous time-dependent divergence-free vector fields. The deformation is obtained by a path line integration of the mesh vertices. This way, the deformation is volume-preserving, free of (local and global) self-intersections, feature preserving, smoothness preserving, and local. Different modeling metaphors support the approach which is able to modify the vector field on-the-fly according to the user input. The approach works at interactive frame rates for moderate mesh sizes, and the numerical integration preserves the volume with a high accuracy.	Vector field based shape deformations	NA:NA:NA	2018
Jin Huang:Xiaohan Shi:Xinguo Liu:Kun Zhou:Li-Yi Wei:Shang-Hua Teng:Hujun Bao:Baining Guo:Heung-Yeung Shum	In this paper we present a general framework for performing constrained mesh deformation tasks with gradient domain techniques. We present a gradient domain technique that works well with a wide variety of linear and nonlinear constraints. The constraints we introduce include the nonlinear volume constraint for volume preservation, the nonlinear skeleton constraint for maintaining the rigidity of limb segments of articulated figures, and the projection constraint for easy manipulation of the mesh without having to frequently switch between multiple viewpoints. To handle nonlinear constraints, we cast mesh deformation as a nonlinear energy minimization problem and solve the problem using an iterative algorithm. The main challenges in solving this nonlinear problem are the slow convergence and numerical instability of the iterative solver. To address these issues, we develop a subspace technique that builds a coarse control mesh around the original mesh and projects the deformation energy and constraints onto the control mesh vertices using the mean value interpolation. The energy minimization is then carried out in the subspace formed by the control mesh vertices. Running in this subspace, our energy minimization solver is both fast and stable and it provides interactive responses. We demonstrate our deformation constraints and subspace deformation technique with a variety of constrained deformation examples.	Subspace gradient domain mesh deformation	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
John Snyder	NA	Session details: Numerical and geometric algorithms and crowds	NA	2018
Richard Szeliski	This paper develops locally adapted hierarchical basis functions for effectively preconditioning large optimization problems that arise in computer graphics applications such as tone mapping, gradient-domain blending, colorization, and scattered data interpolation. By looking at the local structure of the coefficient matrix and performing a recursive set of variable eliminations, combined with a simplification of the resulting coarse level problems, we obtain bases better suited for problems with inhomogeneous (spatially varying) data, smoothness, and boundary constraints. Our approach removes the need to heuristically adjust the optimal number of preconditioning levels, significantly outperforms previously proposed approaches, and also maps cleanly onto data-parallel architectures such as modern GPUs.	Locally adapted hierarchical basis preconditioning	NA	2018
Avneesh Sud:Naga Govindaraju:Russell Gayle:Ilknur Kabul:Dinesh Manocha	We present novel algorithms to perform collision and distance queries among multiple deformable models in dynamic environments. These include inter-object queries between different objects as well as intra-object queries. We describe a unified approach to compute these queries based on N-body distance computation and use properties of the 2nd order discrete Voronoi diagram to perform N-body culling. Our algorithms involve no preprocessing and also work well on models with changing topologies. We can perform all proximity queries among complex deformable models consisting of thousands of triangles in a fraction of a second on a high-end PC. Moreover, our Voronoi-based culling algorithm can improve the performance of separation distance and penetration queries by an order of magnitude.	Fast proximity computation among deformable models using discrete Voronoi diagrams	NA:NA:NA:NA:NA	2018
Pascal Volino:Nadia Magnenat-Thalmann	Robust handling of collisions on non-oriented deformable surfaces requires advanced methods for recovering intersecting surfaces. We present a novel method that resolves intersections between two intersecting surface regions by inducing relative displacements which minimize the length of the intersection contour between them. This method, which does not rely on intersection regions, has a broader application field than existing methods, and its implementation is also much simpler, allowing integration into most existing collision response schemes. We demonstrate the efficiency of this method through examples in the context of cloth simulation.	Resolving surface collisions through intersection contour minimization	NA:NA	2018
Adrien Treuille:Seth Cooper:Zoran Popović	We present a real-time crowd model based on continuum dynamics. In our model, a dynamic potential field simultaneously integrates global navigation with moving obstacles such as other people, efficiently solving for the motion of large crowds without the need for explicit collision avoidance. Simulations created with our system run at interactive rates, demonstrate smooth flow under a variety of conditions, and naturally exhibit emergent phenomena that have been observed in real crowds.	Continuum crowds	NA:NA:NA	2018
Dinesh K. Pai	NA	Session details: Animation	NA	2018
Jue Wang:Steven M. Drucker:Maneesh Agrawala:Michael F. Cohen	We present the "Cartoon Animation Filter", a simple filter that takes an arbitrary input motion signal and modulates it in such a way that the output motion is more "alive" or "animated". The filter adds a smoothed, inverted, and (sometimes) time shifted version of the second derivative (the acceleration) of the signal back into the original signal. Almost all parameters of the filter are automated. The user only needs to set the desired strength of the filter. The beauty of the animation filter lies in its simplicity and generality. We apply the filter to motions ranging from hand drawn trajectories, to simple animations within PowerPoint presentations, to motion captured DOF curves, to video segmentation results. Experimental results show that the filtered motion exhibits anticipation, follow-through, exaggeration and squash-and-stretch effects which are not present in the original input motion data.	The cartoon animation filter	NA:NA:NA:NA	2018
Kevin G. Der:Robert W. Sumner:Jovan Popović	Articulated shapes are aptly described by reduced deformable models that express required shape deformations using a compact set of control parameters. Although sufficient to describe most shape deformations, these control parameters can be ill-suited for animation tasks, particularly when reduced deformable models are inferred automatically from example shapes. Our algorithm provides intuitive and direct control of reduced deformable models similar to a conventional inverse-kinematics algorithm for jointed rigid structures. We present a fully automated pipeline that transforms a set of unarticulated example shapes into a controllable, articulated model. With only a few manipulations, an animator can automatically and interactively pose detailed shapes at rates independent of their geometric complexity.	Inverse kinematics for reduced deformable models	NA:NA:NA	2018
Florence Bertails:Basile Audoly:Marie-Paule Cani:Bernard Querleux:Frédéric Leroy:Jean-Luc Lévêque	Simulating human hair is recognized as one of the most difficult tasks in computer animation. In this paper, we show that the Kirchhoff equations for dynamic, inextensible elastic rods can be used for accurately predicting hair motion. These equations fully account for the nonlinear behavior of hair strands with respect to bending and twisting. We introduce a novel deformable model for solving them: each strand is represented by a Super-Helix, i.e., a piecewise helical rod which is animated using the principles of Lagrangian mechanics. This results in a realistic and stable simulation, allowing large time steps. Our second contribution is an in-depth validation of the Super-Helix model, carried out through a series of experiments based on the comparison of real and simulated hair motions. We show that our model efficiently handles a wide range of hair types with a high level of realism.	Super-helices for predicting the dynamics of natural hair	NA:NA:NA:NA:NA:NA	2018
Sung-Hee Lee:Demetri Terzopoulos	Unlike the human face, the neck has been largely overlooked in the computer graphics literature, this despite its complex anatomical structure and the important role that it plays in supporting the head in balance while generating the controlled head movements that are essential to so many aspects of human behavior. This paper makes two major contributions. First, we introduce a biomechanical model of the human head-neck system. Emulating the relevant anatomy, our model is characterized by appropriate kinematic redundancy (7 cervical vertebrae coupled by 3-DOF joints) and muscle actuator redundancy (72 neck muscles arranged in 3 muscle layers). This anatomically consistent biomechanical model confronts us with a challenging motor control problem, even for the relatively simple task of balancing the mass of the head in gravity atop the cervical spine. Hence, our second contribution is a novel neuromuscular control model for human head animation that emulates the relevant biological motor control mechanisms. Incorporating low-level reflex and high-level voluntary sub-controllers, our hierarchical controller provides input motor signals to the numerous muscle actuators. In addition to head pose and movement, it controls the tone of mutually opposed neck muscles to regulate the stiffness of the head-neck multibody system. Employing machine learning techniques, the neural networks within our neuromuscular controller are trained offline to efficiently generate the online pose and tone control signals necessary to synthesize a variety of autonomous movements for the behavioral animation of the human head and face.	Heads up!: biomechanical modeling and neuromuscular control of the neck	NA:NA	2018
Aaron Hertzmann	NA	Session details: Non-photorealistic rendering	NA	2018
Szymon Rusinkiewicz:Michael Burns:Doug DeCarlo	In fields ranging from technical illustration to mapmaking, artists have developed distinctive visual styles designed to convey both detail and overall shape as clearly as possible. We investigate a non-photorealistic shading model, inspired by techniques for carto-graphic terrain relief, based on dynamically adjusting the effective light position for different areas of the surface. It reveals detail regardless of surface orientation and, by operating at multiple scales, is designed to convey detail at all frequencies simultaneously.	Exaggerated shading for depicting shape and detail	NA:NA:NA	2018
Thomas Luft:Carsten Colditz:Oliver Deussen	We present a simple and efficient method to enhance the perceptual quality of images that contain depth information. Similar to an unsharp mask, the difference between the original depth buffer content and a low-pass filtered copy is utilized to determine information about spatially important areas in a scene. Based on this information we locally enhance the contrast, color, and other parameters of the image. Our technique aims at improving the perception of complex scenes by introducing additional depth cues. The idea is motivated by artwork and findings in the field of neurology, and can be applied to images of any kind, ranging from complex landscape data and technical artifacts, to volume rendering, photograph, and video with depth information.	Image enhancement by unsharp masking the depth buffer	NA:NA:NA	2018
Yingge Qu:Tien-Tsin Wong:Pheng-Ann Heng	This paper proposes a novel colorization technique that propagates color over regions exhibiting pattern-continuity as well as intensity-continuity. The proposed method works effectively on colorizing black-and-white manga which contains intensive amount of strokes, hatching, halftoning and screening. Such fine details and discontinuities in intensity introduce many difficulties to intensity-based colorization methods. Once the user scribbles on the drawing, a local, statistical based pattern feature obtained with Gabor wavelet filters is applied to measure the pattern-continuity. The boundary is then propagated by the level set method that monitors the pattern-continuity. Regions with open boundaries or multiple disjointed regions with similar patterns can be sensibly segmented by a single scribble. With the segmented regions, various colorization techniques can be applied to replace colors, colorize with stroke preservation, or even convert pattern to shading. Several results are shown to demonstrate the effectiveness and convenience of the proposed method.	Manga colorization	NA:NA:NA	2018
Lu Yuan:Jian Sun:Long Quan:Heung-Yeung Shum	Taking satisfactory photos under dim lighting conditions using a hand-held camera is challenging. If the camera is set to a long exposure time, the image is blurred due to camera shake. On the other hand, the image is dark and noisy if it is taken with a short exposure time but with a high camera gain. By combining information extracted from both blurred and noisy images, however, we show in this paper how to produce a high quality image that cannot be obtained by simply denoising the noisy image, or deblurring the blurred image alone. Our approach is image deblurring with the help of the noisy image. First, both images are used to estimate an accurate blur kernel, which otherwise is difficult to obtain from a single blurred image. Second, and again using both images, a residual deconvolution is proposed to significantly reduce ringing artifacts inherent to image deconvolution. Third, the remaining ringing artifacts in smooth image regions are further suppressed by a gain-controlled deconvolution process. We demonstrate the effectiveness of our approach using a number of indoor and outdoor images taken by off-the-shelf hand-held cameras in poor lighting environments.	Image deblurring with blurred/noisy image pairs	NA:NA:NA:NA	2018
Richard Szeliski	NA	Session details: Image analysis & enhancement	NA	2018
Johannes Kopf:Chi-Wing Fu:Daniel Cohen-Or:Oliver Deussen:Dani Lischinski:Tien-Tsin Wong	We present a novel method for synthesizing solid textures from 2D texture exemplars. First, we extend 2D texture optimization techniques to synthesize 3D texture solids. Next, the non-parametric texture optimization approach is integrated with histogram matching, which forces the global statistics of the synthesized solid to match those of the exemplar. This improves the convergence of the synthesis process and enables using smaller neighborhoods. In addition to producing compelling texture mapped surfaces, our method also effectively models the material in the interior of solid objects. We also demonstrate that our method is well-suited for synthesizing textures with a large number of channels per texel.	Solid texture synthesis from 2D exemplars	NA:NA:NA:NA:NA:NA	2018
Jean-François Lalonde:Derek Hoiem:Alexei A. Efros:Carsten Rother:John Winn:Antonio Criminisi	We present a system for inserting new objects into existing photographs by querying a vast image-based object library, pre-computed using a publicly available Internet object database. The central goal is to shield the user from all of the arduous tasks typically involved in image compositing. The user is only asked to do two simple things: 1) pick a 3D location in the scene to place a new object; 2) select an object to insert using a hierarchical menu. We pose the problem of object insertion as a data-driven, 3D-based, context-sensitive object retrieval task. Instead of trying to manipulate the object to change its orientation, color distribution, etc. to fit the new image, we simply retrieve an object of a specified class that has all the required properties (camera pose, lighting, resolution, etc) from our large object library. We present new automatic algorithms for improving object segmentation and blending, estimating true 3D object size and orientation, and estimating scene lighting conditions. We also present an intuitive user interface that makes object insertion fast and simple even for the artistically challenged.	Photo clip art	NA:NA:NA:NA:NA:NA	2018
James Hays:Alexei A. Efros	What can you do with a million images? In this paper we present a new image completion algorithm powered by a huge database of photographs gathered from the Web. The algorithm patches up holes in images by finding similar image regions in the database that are not only seamless but also semantically valid. Our chief insight is that while the space of images is effectively infinite, the space of semantically differentiable scenes is actually not that large. For many image completion tasks we are able to find similar scenes which contain image fragments that will convincingly complete the image. Our algorithm is entirely data-driven, requiring no annotations or labelling by the user. Unlike existing image completion methods, our algorithm can generate a diverse set of results for each input image and we allow users to select among them. We demonstrate the superiority of our algorithm over existing image completion approaches.	Scene completion using millions of photographs	NA:NA	2018
Michiel van de Panne	NA	Session details: Character animation I	NA	2018
Seth Cooper:Aaron Hertzmann:Zoran Popović	This paper describes an approach to building real-time highly-controllable characters. A kinematic character controller is built on-the-fly during a capture session, and updated after each new motion clip is acquired. Active learning is used to identify which motion sequence the user should perform next, in order to improve the quality and responsiveness of the controller. Because motion clips are selected adaptively, we avoid the difficulty of manually determining which ones to capture, and can build complex controllers from scratch while significantly reducing the number of necessary motion samples.	Active learning for real-time motion controllers	NA:NA:NA	2018
James McCann:Nancy Pollard	In game environments, animated character motion must rapidly adapt to changes in player input - for example, if a directional signal from the player's gamepad is not incorporated into the character's trajectory immediately, the character may blithely run off a ledge. Traditional schemes for data-driven character animation lack the split-second reactivity required for this direct control; while they can be made to work, motion artifacts will result. We describe an on-line character animation controller that assembles a motion stream from short motion fragments, choosing each fragment based on current player input and the previous fragment. By adding a simple model of player behavior we are able to improve an existing reinforcement learning method for precalculating good fragment choices. We demonstrate the efficacy of our model by comparing the animation selected by our new controller to that selected by existing methods and to the optimal selection, given knowledge of the entire path. This comparison is performed over real-world data collected from a game prototype. Finally, we provide results indicating that occasional low-quality transitions between motion segments are crucial to high-quality on-line motion generation; this is an important result for others crafting animation systems for directly-controlled characters, as it argues against the common practice of transition thresholding.	Responsive characters from motion fragments	NA:NA	2018
Adrien Treuille:Yongjoon Lee:Zoran Popović	We present a new approach to realtime character animation with interactive control. Given a corpus of motion capture data and a desired task, we automatically compute near-optimal controllers using a low-dimensional basis representation. We show that these controllers produce motion that fluidly responds to several dimensions of user control and environmental constraints in realtime. Our results indicate that very few basis functions are required to create high-fidelity character controllers which permit complex user navigation and obstacle-avoidance tasks.	Near-optimal character animation with continuous control	NA:NA:NA	2018
Jinxiang Chai:Jessica K. Hodgins	In this paper, we present a technique for generating animation from a variety of user-defined constraints. We pose constraint-based motion synthesis as a maximum a posterior (MAP) problem and develop an optimization framework that generates natural motion satisfying user constraints. The system automatically learns a statistical dynamic model from motion capture data and then enforces it as a motion prior. This motion prior, together with user-defined constraints, comprises a trajectory optimization problem. Solving this problem in the low-dimensional space yields optimal natural motion that achieves the goals specified by the user. We demonstrate the effectiveness of this approach by generating whole-body and facial motion from a variety of spatial-temporal constraints.	Constraint-based motion optimization using a statistical dynamic model	NA:NA	2018
Aseem Agarwala	NA	Session details: Image slicing & stretching	NA	2018
Jue Wang:Maneesh Agrawala:Michael F. Cohen	We present Soft Scissors, an interactive tool for extracting alpha mattes of foreground objects in realtime. We recently proposed a novel offline matting algorithm capable of extracting high-quality mattes for complex foreground objects such as furry animals [Wang and Cohen 2007]. In this paper we both improve the quality of our offline algorithm and give it the ability to incrementally update the matte in an online interactive setting. Our realtime system efficiently estimates foreground color thereby allowing both the matte and the final composite to be revealed instantly as the user roughly paints along the edge of the foreground object. In addition, our system can dynamically adjust the width and boundary conditions of the scissoring paint brush to approximately capture the boundary of the foreground object that lies ahead on the scissor's path. These advantages in both speed and accuracy create the first interactive tool for high quality image matting and compositing.	Soft scissors: an interactive tool for realtime high quality matting	NA:NA:NA	2018
Shai Avidan:Ariel Shamir	Effective resizing of images should not only use geometric constraints, but consider the image content as well. We present a simple image operator called seam carving that supports content-aware image resizing for both reduction and expansion. A seam is an optimal 8-connected path of pixels on a single image from top to bottom, or left to right, where optimality is defined by an image energy function. By repeatedly carving out or inserting seams in one direction we can change the aspect ratio of an image. By applying these operators in both directions we can retarget the image to a new size. The selection and order of seams protect the content of the image, as defined by the energy function. Seam carving can also be used for image content enhancement and object removal. We support various visual saliency measures for defining the energy of an image, and can also include user input to guide the process. By storing the order of seams in an image we create multi-size images, that are able to continuously change in real time to fit a given size.	Seam carving for content-aware image resizing	NA:NA	2018
Jian Sun:Lin Liang:Fang Wen:Heung-Yeung Shum	Recently, gradient meshes have been introduced as a powerful vector graphics representation to draw multicolored mesh objects with smooth transitions. Using tools from Abode Illustrator and Corel CorelDraw, a user can manually create gradient meshes even for photo-realistic vector arts, which can be further edited, stylized and animated. In this paper, we present an easy-to-use interactive tool, called optimized gradient mesh, to semi-automatically and quickly create gradient meshes from a raster image. We obtain the optimized gradient mesh by formulating an energy minimization problem. The user can also interactively specify a few vector lines to guide the mesh generation. The resulting optimized gradient mesh is an editable and scalable mesh that otherwise would have taken many hours for a user to manually create.	Image vectorization using optimized gradient meshes	NA:NA:NA:NA	2018
Hui Fang:John C. Hart	Shape deformation is a common practice in digital image editing, but can unrealistically stretch or compress texture detail. We propose an image editing system that decouples feature position from pixel color generation, by resynthesizing texture from the source image to preserve its detail and orientation around a new feature curve location. We introduce a new distortion to patch-based texture synthesis that aligns texture features with image features. A dense correspondence field between source and target images generated by the control curves then guides texture synthesis.	Detail preserving shape deformation in image editing	NA:NA	2018
Carol O'Sullivan	NA	Session details: Squish, bounce and collide	NA	2018
Geoffrey Irving:Craig Schroeder:Ronald Fedkiw	We propose a numerical method for modeling highly deformable nonlinear incompressible solids that conserves the volume locally near each node in a finite element mesh. Our method works with arbitrary constitutive models, is applicable to both passive and active materials (e.g. muscles), and works with simple tetrahedra without the need for multiple quadrature points or stabilization techniques. Although simple linear tetrahedra typically suffer from locking when modeling incompressible materials, our method enforces incompressibility per node (in a one-ring), and we demonstrate that it is free from locking. We correct errors in volume without introducing oscillations by treating position and velocity in separate implicit solves. Finally, we propose a novel method for treating both object contact and self-contact as linear constraints during the incompressible solve, alleviating issues in enforcing multiple possibly conflicting constraints.	Volume conserving finite element simulations of deformable models	NA:NA:NA	2018
Christopher D. Twigg:Doug L. James	Animation techniques for controlling passive simulation are commonly based on an optimization paradigm: the user provides goals a priori, and sophisticated numerical methods minimize a cost function that represents these goals. Unfortunately, for multibody systems with discontinuous contact events these optimization problems can be highly nontrivial to solve, and many-hour offline optimizations, unintuitive parameters, and convergence failures can frustrate end-users and limit usage. On the other hand, users are quite adaptable, and systems which provide interactive feedback via an intuitive interface can leverage the user's own abilities to quickly produce interesting animations. However, the online computation necessary for interactivity limits scene complexity in practice. We introduce Many-Worlds Browsing, a method which circumvents these limits by exploiting the speed of multibody simulators to compute numerous example simulations in parallel (offline and online), and allow the user to browse and modify them interactively. We demonstrate intuitive interfaces through which the user can select among the examples and interactively adjust those parts of the scene that do not match his requirements. We show that using a combination of our techniques, unusual and interesting results can be generated for moderately sized scenes with under an hour of user time. Scalability is demonstrated by sampling much larger scenes using modest offline computations.	Many-worlds browsing for control of multibody dynamics	NA:NA	2018
Xinyu Zhang:Stephane Redon:Minkyoung Lee:Young J. Kim	We present a fast continuous collision detection (CCD) algorithm for articulated models using Taylor models and temporal culling. Our algorithm is a generalization of conservative advancement (CA) from convex models [Mirtich 1996] to articulated models with non-convex links. Given the initial and final configurations of a moving articulated model, our algorithm creates a continuous motion with constant translational and rotational velocities for each link, and checks for interferences between the articulated model under continuous motion and other models in the environment and for self-collisions. If collisions occur, our algorithm reports the first time of contact (TOC) as well as collision witness features. We have implemented our CCD algorithm and applied it to several challenging scenarios including locomotion generation, articulated-body dynamics and character motion planning. Our algorithm can perform CCDs including self-collision detection for articulated models consisting of many links and tens of thousands of triangles in 1.22 ms on average running on a 3.6 GHz Pentium 4 PC. This is an improvement on the performance of prior algorithms of more than an order of magnitude.	Continuous collision detection for articulated models using Taylor models and temporal culling	NA:NA:NA:NA	2018
Adam W. Bargteil:Chris Wojtan:Jessica K. Hodgins:Greg Turk	We present an extension to Lagrangian finite element methods to allow for large plastic deformations of solid materials. These behaviors are seen in such everyday materials as shampoo, dough, and clay as well as in fantastic gooey and blobby creatures in special effects scenes. To account for plastic deformation, we explicitly update the linear basis functions defined over the finite elements during each simulation step. When these updates cause the basis functions to become ill-conditioned, we remesh the simulation domain to produce a new high-quality finite-element mesh, taking care to preserve the original boundary. We also introduce an enhanced plasticity model that preserves volume and includes creep and work hardening/softening. We demonstrate our approach with simulations of synthetic objects that squish, dent, and flow. To validate our methods, we compare simulation results to videos of real materials.	A finite element method for animating large viscoplastic flow	NA:NA:NA:NA	2018
Doug DeCarlo	NA	Session details: Shape depiction and stylization	NA	2018
Hideki Todo:Ken-ichi Anjyo:William Baxter:Takeo Igarashi	Recent progress in non-photorealistic rendering (NPR) has led to many stylized shading techniques that efficiently convey visual information about the objects depicted. Another crucial goal of NPR is to give artists simple and direct ways to express the abstract ideas born of their imaginations. In particular, the ability to add intentional, but often unrealistic, shading effects is indispensable for many applications. We propose a set of simple stylized shading algorithms that allow the user to freely add localized light and shade to a model in a manner that is consistent and seamlessly integrated with conventional lighting techniques. The algorithms provide an intuitive, direct manipulation method based on a paint-brush metaphor, to control and edit the light and shade locally as desired. Our prototype system demonstrates how our method can enhance both the quality and range of applicability of conventional stylized shading for offline animation and interactive applications.	Locally controllable stylized shading	NA:NA:NA:NA	2018
Yunjin Lee:Lee Markosian:Seungyong Lee:John F. Hughes	We describe a GPU-based algorithm for rendering a 3D model as a line drawing, based on the insight that a line drawing can be understood as an abstraction of a shaded image. We thus render lines along tone boundaries or thin dark areas in the shaded image. We extend this notion to the dual: we render highlight lines along thin bright areas and tone boundaries. We combine the lines with toon shading to capture broad regions of tone. The resulting line drawings effectively convey both shape and material cues. The lines produced by the method can include silhouettes. creases, and ridges, along with a generalization of suggestive contours that responds to lighting as well as viewing changes. The method supports automatic level of abstraction, where the size of depicted shape features adjusts appropriately as the camera zooms in or out. Animated models can be rendered in real time because costly mesh curvature calculations are not needed.	Line drawings via abstracted shading	NA:NA:NA:NA	2018
Tilke Judd:Frédo Durand:Edward Adelson	Three-dimensional shape can be drawn using a variety of feature lines, but none of the current definitions alone seem to capture all visually-relevant lines. We introduce a new definition of feature lines based on two perceptual observations. First, human perception is sensitive to the variation of shading, and since shape perception is little affected by lighting and reflectance modification, we should focus on normal variation. Second, view-dependent lines better convey smooth surfaces. From this we define view-dependent curvature as the variation of the surface normal with respect to a viewing screen plane, and apparent ridges as the loci of points that maximize a view-dependent curvature. We present a formal definition of apparent ridges and an algorithm to render line drawings of 3D meshes. We show that our apparent ridges encompass or enhance aspects of several other feature lines.	Apparent ridges for line drawing	NA:NA:NA	2018
Simon Breslav:Karol Szerszen:Lee Markosian:Pascal Barla:Joëlle Thollot	We describe a new way to render 3D scenes in a variety of non-photorealistic styles, based on patterns whose structure and motion are defined in 2D. In doing so, we sacrifice the ability of patterns that wrap onto 3D surfaces to convey shape through their structure and motion. In return, we gain several advantages, chiefly that 2D patterns are more visually abstract - a quality often sought by artists, which explains their widespread use in hand-drawn images. Extending such styles to 3D graphics presents a challenge: how should a 2D pattern move? Our solution is to transform it each frame by a 2D similarity transform that closely follows the underlying 3D shape. The resulting motion is often surprisingly effective, and has a striking cartoon quality that matches the visual style.	Dynamic 2D patterns for shading 3D scenes	NA:NA:NA:NA:NA	2018
Mark Pauly	NA	Session details: Point sets	NA	2018
Benedict J. Brown:Szymon Rusinkiewicz	A key challenge in reconstructing high-quality 3D scans is registering data from different viewpoints. Existing global (multiview) alignment algorithms are restricted to rigid-body transformations, and cannot adequately handle non-rigid warps frequently present in real-world datasets. Moreover, algorithms that can compensate for such warps between pairs of scans do not easily generalize to the multiview case. We present an algorithm for obtaining a globally optimal alignment of multiple overlapping datasets in the presence of low-frequency non-rigid deformations, such as those caused by device nonlinearities or calibration error. The process first obtains sparse correspondences between views using a locally weighted, stability-guaranteeing variant of iterative closest points (ICP). Global positions for feature points are found using a relaxation method, and the scans are warped to their final positions using thin-plate splines. Our framework efficiently handles large datasets---thousands of scans comprising hundreds of millions of samples---for both rigid and non-rigid alignment, with the non-rigid case requiring little overhead beyond rigid-body alignment. We demonstrate that, relative to rigid-body registration, it improves the quality of alignment and better preserves detail in 3D datasets from a variety of scanners exhibiting non-rigid distortion.	Global non-rigid alignment of 3-D scans	NA:NA	2018
Yaron Lipman:Daniel Cohen-Or:David Levin:Hillel Tal-Ezer	We introduce a Locally Optimal Projection operator (LOP) for surface approximation from point-set data. The operator is parameterization free, in the sense that it does not rely on estimating a local normal, fitting a local plane, or using any other local parametric representation. Therefore, it can deal with noisy data which clutters the orientation of the points. The method performs well in cases of ambiguous orientation, e.g., if two folds of a surface lie near each other, and other cases of complex geometry in which methods based upon local plane fitting may fail. Although defined by a global minimization problem, the method is effectively local, and it provides a second order approximation to smooth surfaces. Hence allowing good surface approximation without using any explicit or implicit approximation space. Furthermore, we show that LOP is highly robust to noise and outliers and demonstrate its effectiveness by applying it to raw scanned data of complex shapes.	Parameterization-free projection for geometry reconstruction	NA:NA:NA:NA	2018
Gaël Guennebaud:Markus Gross	In this paper we present a new Point Set Surface (PSS) definition based on moving least squares (MLS) fitting of algebraic spheres. Our surface representation can be expressed by either a projection procedure or in implicit form. The central advantages of our approach compared to existing planar MLS include significantly improved stability of the projection under low sampling rates and in the presence of high curvature. The method can approximate or interpolate the input point set and naturally handles planar point clouds. In addition, our approach provides a reliable estimate of the mean curvature of the surface at no additional cost and allows for the robust handling of sharp features and boundaries. It processes a simple point set as input, but can also take significant advantage of surface normals to improve robustness, quality and performance. We also present an novel normal estimation procedure which exploits the properties of the spherical fit for both direction estimation and orientation propagation. Very efficient computational procedures enable us to compute the algebraic sphere fitting with up to 40 million points per second on latest generation GPUs.	Algebraic point set surfaces	NA:NA	2018
Sagi Katz:Ayellet Tal:Ronen Basri	This paper proposes a simple and fast operator, the "Hidden" Point Removal operator, which determines the visible points in a point cloud, as viewed from a given viewpoint. Visibility is determined without reconstructing a surface or estimating normals. It is shown that extracting the points that reside on the convex hull of a transformed point cloud, amounts to determining the visible points. This operator is general - it can be applied to point clouds at various dimensions, on both sparse and dense point clouds, and on viewpoints internal as well as external to the cloud. It is demonstrated that the operator is useful in visualizing point clouds, in view-dependent reconstruction and in shadow casting.	Direct visibility of point sets	NA:NA:NA	2018
Steve Marschner	NA	Session details: Lighting	NA	2018
Jonathan Ragan-Kelley:Charlie Kilpatrick:Brian W. Smith:Doug Epps:Paul Green:Christophe Hery:Frédo Durand	We present an automated approach for high-quality preview of feature-film rendering during lighting design. Similar to previous work, we use a deep-framebuffer shaded on the GPU to achieve interactive performance. Our first contribution is to generate the deep-framebuffer and corresponding shaders automatically through data-flow analysis and compilation of the original scene. Cache compression reduces automatically-generated deep-framebuffers to reasonable size for complex production scenes and shaders. We also propose a new structure, the indirect framebuffer, that decouples shading samples from final pixels and allows a deep-framebuffer to handle antialiasing, motion blur and transparency efficiently. Progressive refinement enables fast feedback at coarser resolution. We demonstrate our approach in real-world production.	The lightspeed automatic interactive lighting preview system	NA:NA:NA:NA:NA:NA:NA	2018
Miloš Hašan:Fabio Pellacini:Kavita Bala	Rendering complex scenes with indirect illumination, high dynamic range environment lighting, and many direct light sources remains a challenging problem. Prior work has shown that all these effects can be approximated by many point lights. This paper presents a scalable solution to the many-light problem suitable for a GPU implementation. We view the problem as a large matrix of sample-light interactions; the ideal final image is the sum of the matrix columns. We propose an algorithm for approximating this sum by sampling entire rows and columns of the matrix on the GPU using shadow mapping. The key observation is that the inherent structure of the transfer matrix can be revealed by sampling just a small number of rows and columns. Our prototype implementation can compute the light transfer within a few seconds for scenes with indirect and environment illumination, area lights, complex geometry and arbitrary shaders. We believe this approach can be very useful for rapid previewing in applications like cinematic and architectural lighting design.	Matrix row-column sampling for the many-light problem	NA:NA:NA	2018
Xin Sun:Kun Zhou:Yanyun Chen:Stephen Lin:Jiaoying Shi:Baining Guo	We present a technique for interactive relighting in which source radiance, viewing direction, and BRDFs can all be changed on the fly. In handling dynamic BRDFs, our method efficiently accounts for the effects of BRDF modification on the reflectance and incident radiance at a surface point. For reflectance, we develop a BRDF tensor representation that can be factorized into adjustable terms for lighting, viewing, and BRDF parameters. For incident radiance, there exists a non-linear relationship between indirect lighting and BRDFs in a scene, which makes linear light transport frameworks such as PRT unsuitable. To overcome this problem, we introduce precomputed transfer tensors (PTTs) which decompose indirect lighting into precomputable components that are each a function of BRDFs in the scene, and can be rapidly combined at run time to correctly determine incident radiance. We additionally describe a method for efficient handling of high-frequency specular reflections by separating them from the BRDF tensor representation and processing them using precomputed visibility information. With relighting based on PTTs, interactive performance with indirect lighting is demonstrated in applications to BRDF animation and material tuning.	Interactive relighting with dynamic BRDFs	NA:NA:NA:NA:NA:NA	2018
Charles Han:Bo Sun:Ravi Ramamoorthi:Eitan Grinspun	Filtering is critical for representing detail, such as color textures or normal maps, across a variety of scales. While MIP-mapping texture maps is commonplace, accurate normal map filtering remains a challenging problem because of nonlinearities in shading---we cannot simply average nearby surface normals. In this paper, we show analytically that normal map filtering can be formalized as a spherical convolution of the normal distribution function (NDF) and the BRDF, for a large class of common BRDFs such as Lambertian, microfacet and factored measurements. This theoretical result explains many previous filtering techniques as special cases, and leads to a generalization to a broader class of measured and analytic BRDFs. Our practical algorithms leverage a significant body of work that has studied lighting-BRDF convolution. We show how spherical harmonics can be used to filter the NDF for Lambertian and low-frequency specular BRDFs, while spherical von Mises-Fisher distributions can be used for high-frequency materials.	Frequency domain normal map filtering	NA:NA:NA:NA	2018
Takeo Igarashi	NA	Session details: Illustration & sculpture	NA	2018
Jie Xu:Craig S. Kaplan	We present a set of graphical and combinatorial algorithms for designing mazes based on images. The designer traces regions of interest in an image and annotates the regions with style parameters. They can optionally specify a solution path, which provides a rough guide for laying out the maze's actual solution. The system uses novel extensions to well-known maze construction algorithms to build mazes that approximate the tone of the source image, express the desired style in each region, and conform to the user's solution path.	Image-guided maze construction	NA:NA	2018
Paul Asente:Mike Schuster:Teri Pettit	There are many types of illustrations that are easier to create in planar-map-based illustration systems than in the more common stacking-based systems. One weakness shared by all existing planar-map-based systems is that the editability of the drawing is severely hampered once coloring has begun. The paths that define the areas to be filled become divided wherever they intersect, making it difficult or impossible to edit them as a whole. Live Paint is a new metaphor that allows planar-map-based coloring while maintaining all the original paths unchanged. When a user makes a change, the regions and edges defined by the new paths take on fill and stroke attributes from the previous regions and edges. This results in greater editing flexibility and ease of use. Live Paint uses a set of heuristics to match each region and edge in a changed illustration with a region or edge in the previous version, a task that is more difficult than it at first appears. It then transfers fill and stroke attributes accordingly.	Dynamic planar map illustration	NA:NA:NA	2018
Wilmot Li:Lincoln Ritter:Maneesh Agrawala:Brian Curless:David Salesin	We present a system for authoring and viewing interactive cutaway illustrations of complex 3D models using conventions of traditional scientific and technical illustration. Our approach is based on the two key ideas that 1) cuts should respect the geometry of the parts being cut, and 2) cutaway illustrations should support interactive exploration. In our approach, an author instruments a 3D model with auxiliary parameters, which we call "rigging," that define how cutaways of that structure are formed. We provide an authoring interface that automates most of the rigging process. We also provide a viewing interface that allows viewers to explore rigged models using high-level interactions. In particular, the viewer can just select a set of target structures, and the system will automatically generate a cutaway illustration that exposes those parts. We have tested our system on a variety of CAD and anatomical models, and our results demonstrate that our approach can be used to create and view effective interactive cutaway illustrations for a variety of complex objects with little user effort.	Interactive cutaway illustrations of complex 3D models	NA:NA:NA:NA:NA	2018
Tim Weyrich:Jia Deng:Connelly Barnes:Szymon Rusinkiewicz:Adam Finkelstein	We present a system for semi-automatic creation of bas-relief sculpture. As an artistic medium, relief spans the continuum between 2D drawing or painting and full 3D sculpture. Bas-relief (or low relief) presents the unique challenge of squeezing shapes into a nearly-flat surface while maintaining as much as possible the perception of the full 3D scene. Our solution to this problem adapts methods from the tone-mapping literature, which addresses the similar problem of squeezing a high dynamic range image into the (low) dynamic range available on typical display devices. However, the bas-relief medium imposes its own unique set of requirements, such as maintaining small, fixed-size depth discontinuities. Given a 3D model, camera, and a few parameters describing the relative attenuation of different frequencies in the shape, our system creates a relief that gives the illusion of the 3D shape from a given vantage point while conforming to a greatly compressed height.	Digital bas-relief from 3D scenes	NA:NA:NA:NA:NA	2018
James Davis	NA	Session details: Performance capture	NA	2018
Bernd Bickel:Mario Botsch:Roland Angst:Wojciech Matusik:Miguel Otaduy:Hanspeter Pfister:Markus Gross	We present a novel multi-scale representation and acquisition method for the animation of high-resolution facial geometry and wrinkles. We first acquire a static scan of the face including reflectance data at the highest possible quality. We then augment a traditional marker-based facial motion-capture system by two synchronized video cameras to track expression wrinkles. The resulting model consists of high-resolution geometry, motion-capture data, and expression wrinkles in 2D parametric form. This combination represents the facial shape and its salient features at multiple scales. During motion synthesis the motion-capture data deforms the high-resolution geometry using a linear shell-based mesh-deformation method. The wrinkle geometry is added to the facial base mesh using nonlinear energy optimization. We present the results of our approach for performance replay as well as for wrinkle editing.	Multi-scale capture of facial geometry and motion	NA:NA:NA:NA:NA:NA:NA	2018
Ryan White:Keenan Crane:D. A. Forsyth	We capture the shape of moving cloth using a custom set of color markers printed on the surface of the cloth. The output is a sequence of triangle meshes with static connectivity and with detail at the scale of individual markers in both smooth and folded regions. We compute markers' coordinates in space using correspondence across multiple synchronized video cameras. Correspondence is determined from color information in small neighborhoods and refined using a novel strain pruning process. Final correspondence does not require neighborhood information. We use a novel data driven hole-filling technique to fill occluded regions. Our results include several challenging examples: a wrinkled shirt sleeve, a dancing pair of pants, and a rag tossed onto a cup. Finally, we demonstrate that cloth capture is reusable by animating a pair of pants using human motion capture data.	Capturing and animating occluded cloth	NA:NA:NA	2018
Daniel Vlasic:Rolf Adelsberger:Giovanni Vannucci:John Barnwell:Markus Gross:Wojciech Matusik:Jovan Popović	Commercial motion-capture systems produce excellent in-studio reconstructions, but offer no comparable solution for acquisition in everyday environments. We present a system for acquiring motions almost anywhere. This wearable system gathers ultrasonic time-of-flight and inertial measurements with a set of inexpensive miniature sensors worn on the garment. After recording, the information is combined using an Extended Kalman Filter to reconstruct joint configurations of a body. Experimental results show that even motions that are traditionally difficult to acquire are recorded with ease within their natural settings. Although our prototype does not reliably recover the global transformation, we show that the resulting motions are visually similar to the original ones, and that the combined acoustic and intertial system reduces the drift commonly observed in purely inertial systems. Our final results suggest that this system could become a versatile input device for a variety of augmented-reality applications.	Practical motion capture in everyday surroundings	NA:NA:NA:NA:NA:NA:NA	2018
Ramesh Raskar:Hideaki Nii:Bert deDecker:Yuki Hashimoto:Jay Summet:Dylan Moore:Yong Zhao:Jonathan Westhues:Paul Dietz:John Barnwell:Shree Nayar:Masahiko Inami:Philippe Bekaert:Michael Noland:Vlad Branzoi:Erich Bruns	In this paper, we present a high speed optical motion capture method that can measure three dimensional motion, orientation, and incident illumination at tagged points in a scene. We use tracking tags that work in natural lighting conditions and can be imperceptibly embedded in attire or other objects. Our system supports an unlimited number of tags in a scene, with each tag uniquely identified to eliminate marker reacquisition issues. Our tags also provide incident illumination data which can be used to match scene lighting when inserting synthetic elements. The technique is therefore ideal for on-set motion capture or real-time broadcasting of virtual sets. Unlike previous methods that employ high speed cameras or scanning lasers, we capture the scene appearance using the simplest possible optical devices - a light-emitting diode (LED) with a passive binary mask used as the transmitter and a photosensor used as the receiver. We strategically place a set of optical transmitters to spatio-temporally encode the volume of interest. Photosensors attached to scene points demultiplex the coded optical signals from multiple transmitters, allowing us to compute not only receiver location and orientation but also their incident illumination and the reflectance of the surfaces to which the photosensors are attached. We use our untethered tag system, called Prakash, to demonstrate methods of adding special effects to captured videos that cannot be accomplished using pure vision techniques that rely on camera images.	Prakash: lighting aware motion capture using photosensing markers and multiplexed illuminators	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Sing Bing Kang	NA	Session details: Light field & high dynamic range imaging	NA	2018
Eino-Ville Talvala:Andrew Adams:Mark Horowitz:Marc Levoy	The ability of a camera to record a high dynamic range image, whether by taking one snapshot or a sequence, is limited by the presence of veiling glare - the tendency of bright objects in the scene to reduce the contrast everywhere within the field of view. Veiling glare is a global illumination effect that arises from multiple scattering of light inside the camera's body and lens optics. By measuring separately the direct and indirect components of the intra-camera light transport, one can increase the maximum dynamic range a particular camera is capable of recording. In this paper, we quantify the presence of veiling glare and related optical artifacts for several types of digital cameras, and we describe two methods for removing them: deconvolution by a measured glare spread function, and a novel direct-indirect separation of the lens transport using a structured occlusion mask. In the second method, we selectively block the light that contributes to veiling glare, thereby attaining significantly higher signal-to-noise ratios than with deconvolution. Finally, we demonstrate our separation method for several combinations of cameras and realistic scenes.	Veiling glare in high dynamic range imaging	NA:NA:NA:NA	2018
Ahmet Oǧuz Akyüz:Roland Fleming:Bernhard E. Riecke:Erik Reinhard:Heinrich H. Bülthoff	The development of high dynamic range (HDR) imagery has brought us to the verge of arguably the largest change in image display technologies since the transition from black-and-white to color television. Novel capture and display hardware will soon enable consumers to enjoy the HDR experience in their own homes. The question remains, however, of what to do with existing images and movies, which are intrinsically low dynamic range (LDR). Can this enormous volume of legacy content also be displayed effectively on HDR displays? We have carried out a series of rigorous psychophysical investigations to determine how LDR images are best displayed on a state-of-the-art HDR monitor, and to identify which stages of the HDR imaging pipeline are perceptually most critical. Our main findings are: (1) As expected, HDR displays outperform LDR ones. (2) Surprisingly, HDR images that are tone-mapped for display on standard monitors are often no better than the best single LDR exposure from a bracketed sequence. (3) Most importantly of all, LDR data does not necessarily require sophisticated treatment to produce a compelling HDR experience. Simply boosting the range of an LDR image linearly to fit the HDR display can equal or even surpass the appearance of a true HDR image. Thus the potentially tricky process of inverse tone mapping can be largely circumvented.	Do HDR displays support LDR content?: a psychophysical evaluation	NA:NA:NA:NA:NA	2018
Allan G. Rempel:Matthew Trentacoste:Helge Seetzen:H. David Young:Wolfgang Heidrich:Lorne Whitehead:Greg Ward	New generations of display devices promise to provide significantly improved dynamic range over conventional display technology. In the long run, evolving camera technology and file formats will provide high fidelity content for these display devices. In the near term, however, the vast majority of images and video will only be available in low dynamic range formats. In this paper we describe a method for boosting the dynamic range of legacy video and photographs for viewing on high dynamic range displays. Our emphasis is on real-time processing of video streams, such as web streams or the signal from a DVD player. We place particular emphasis on robustness of the method, and its ability to deal with a wide range of content without user adjusted parameters or visible artifacts. The method can be implemented on both graphics hardware and on signal processors that are directly integrated in the HDR displays.	Ldr2Hdr: on-the-fly reverse tone mapping of legacy video and photographs	NA:NA:NA:NA:NA:NA:NA	2018
Andrew Jones:Ian McDowall:Hideshi Yamada:Mark Bolas:Paul Debevec	We describe a set of rendering techniques for an autostereoscopic light field display able to present interactive 3D graphics to multiple simultaneous viewers 360 degrees around the display. The display consists of a high-speed video projector, a spinning mirror covered by a holographic diffuser, and FPGA circuitry to decode specially rendered DVI video signals. The display uses a standard programmable graphics card to render over 5,000 images per second of interactive 3D graphics, projecting 360-degree views with 1.25 degree separation up to 20 updates per second. We describe the system's projection geometry and its calibration process, and we present a multiple-center-of-projection rendering technique for creating perspective-correct images from arbitrary viewpoints around the display. Our projection technique allows correct vertical perspective and parallax to be rendered for any height and distance when these parameters are known, and we demonstrate this effect with interactive raster graphics using a tracking system to measure the viewer's height and distance. We further apply our projection technique to the display of photographed light fields with accurate horizontal and vertical parallax. We conclude with a discussion of the display's visual accommodation performance and discuss techniques for displaying color imagery.	Rendering for an interactive 360° light field display	NA:NA:NA:NA:NA	2018
Tom Funkhouser	NA	Session details: Sketching 3D shapes	NA	2018
Andrew Nealen:Takeo Igarashi:Olga Sorkine:Marc Alexa	This paper presents a system for designing freeform surfaces with a collection of 3D curves. The user first creates a rough 3D model by using a sketching interface. Unlike previous sketching systems, the user-drawn strokes stay on the model surface and serve as handles for controlling the geometry. The user can add, remove, and deform these control curves easily, as if working with a 2D line drawing. The curves can have arbitrary topology; they need not be connected to each other. For a given set of curves, the system automatically constructs a smooth surface embedding by applying functional optimization. Our system provides real-time algorithms for both control curve deformation and the subsequent surface optimization. We show that one can create sophisticated models using this system, which have not yet been seen in previous sketching or functional optimization systems.	FiberMesh: designing freeform surfaces with 3D curves	NA:NA:NA:NA	2018
Tao Ju:Qian-Yi Zhou:Shi-Min Hu	We present a method for modifying the topology of a 3D model with user control. The heart of our method is a guided topology editing algorithm. Given a source model and a user-provided target shape, the algorithm modifies the source so that the resulting model is topologically consistent with the target. Our algorithm permits removing or adding various topological features (e.g., handles, cavities and islands) in a common framework and ensures that each topological change is made by minimal modification to the source model. To create the target shape, we have also designed a convenient 2D sketching interface for drawing 3D line skeletons. As demonstrated in a suite of examples, the use of sketching allows more accurate removal of topological artifacts than previous methods, and enables creative designs with specific topological goals.	Editing the topology of 3D models by sketching	NA:NA:NA	2018
Andrei Sharf:Thomas Lewiner:Gil Shklarski:Sivan Toledo:Daniel Cohen-Or	The reconstruction of a complete watertight model from scan data is still a difficult process. In particular, since scanned data is often incomplete, the reconstruction of the expected shape is an ill-posed problem. Techniques that reconstruct poorly-sampled areas without any user intervention fail in many cases to faithfully reconstruct the topology of the model. The method that we introduce in this paper is topology-aware: it uses minimal user input to make correct decisions at regions where the topology of the model cannot be automatically induced with a reasonable degree of confidence. We first construct a continuous function over a three-dimensional domain. This function is constructed by minimizing a penalty function combining the data points, user constraints, and a regularization term. The optimization problem is formulated in a mesh-independent manner, and mapped onto a specific mesh using the finite-element method. The zero level-set of this function is a first approximation of the reconstructed surface. At complex under-sampled regions, the constraints might be insufficient. Hence, we analyze the local topological stability of the zero level-set to detect weak regions of the surface. These regions are suggested to the user for adding local inside/outside constraints by merely scribbling over a 2D tablet. Each new user constraint modifies the minimization problem, which is solved incrementally. The process is repeated, converging to a topology-stable reconstruction. Reconstructions of models acquired by a structured-light scanner with a small number of scribbles demonstrate the effectiveness of the method.	Interactive topology-aware surface reconstruction	NA:NA:NA:NA:NA	2018
Tai-Pang Wu:Chi-Keung Tang:Michael S. Brown:Heung-Yeung Shum	We present a simple interactive approach to specify 3D shape in a single view using "shape palettes". The interaction is as follows: draw a simple 2D primitive in the 2D view and then specify its 3D orientation by drawing a corresponding primitive on a shape palette. The shape palette is presented as an image of some familiar shape whose local 3D orientation is readily understood and can be easily marked over. The 3D orientation from the shape palette is transferred to the 2D primitive based on the markup. As we will demonstrate, only sparse markup is needed to generate expressive and detailed 3D surfaces. This markup approach can be used to model freehand 3D surfaces drawn in a single view, or combined with image-snapping tools to quickly extract surfaces from images and photographs.	ShapePalettes: interactive normal transfer via sketching	NA:NA:NA:NA	2018
Yuki Mori:Takeo Igarashi	We introduce Plushie, an interactive system that allows nonprofessional users to design their own original plush toys. To design a plush toy, one needs to construct an appropriate two-dimensional (2D) pattern. However, it is difficult for non-professional users to appropriately design a 2D pattern. Some recent systems automatically generate a 2D pattern for a given three-dimensional (3D) model, but constructing a 3D model is itself a challenge. Furthermore, an arbitrary 3D model cannot necessarily be realized as a real plush toy, and the final sewn result can be very different from the original 3D model. We avoid this mismatch by constructing appropriate 2D patterns and applying simple physical simulation to it on the fly during 3D modeling. In this way, the model on the screen is always a good approximation of the final sewn result, which makes the design process much more efficient. We use a sketching interface for 3D modeling and also provide various editing operations tailored for plush toy design. Internally, the system constructs a 2D cloth pattern in such a way that the simulation result matches the user's input stroke. Our goal is to show that relatively simple algorithms can provide fast, satisfactory results to the user whereas the pursuit of optimal layout and simulation accuracy lies outside this paper's scope. We successfully demonstrated that non-professional users could design plush toys or balloon easily using Plushie.	Plushie: an interactive design system for plush toys	NA:NA	2018
Irfan Essa	NA	Session details: Physical simulation	NA	2018
Robert Bridson:Jim Houriham:Marcus Nordenstam	Procedural methods for animating turbulent fluid are often preferred over simulation, both for speed and for the degree of animator control. We offer an extremely simple approach to efficiently generating turbulent velocity fields based on Perlin noise, with a formula that is exactly incompressible (necessary for the characteristic look of everyday fluids), exactly respects solid boundaries (not allowing fluid to flow through arbitrarily-specified surfaces), and whose amplitude can be modulated in space as desired. In addition, we demonstrate how to combine this with procedural primitives for flow around moving rigid objects, vortices, etc.	Curl-noise for procedural fluid flow	NA:NA:NA	2018
Jeong-Mo Hong:Tamar Shinar:Ronald Fedkiw	We model flames and fire using the Navier-Stokes equations combined with the level set method and jump conditions to model the reaction front. Previous works modeled the flame using a combination of propagation in the normal direction and a curvature term which leads to a level set equation that is parabolic in nature and thus overly dissipative and smooth. Asymptotic theory shows that one can obtain more interesting velocities and fully hyperbolic (as opposed to parabolic) equations for the level set evolution. In particular, researchers in the field of detonation shock dynamics (DSD) have derived a set of equations which exhibit characteristic cellular patterns. We show how to make use of the DSD framework in the context of computer graphics simulations of flames and fire to obtain interesting features such as flame wrinkling and cellular patterns.	Wrinkled flames and cellular patterns	NA:NA:NA	2018
Bart Adams:Mark Pauly:Richard Keiser:Leonidas J. Guibas	We present novel adaptive sampling algorithms for particle-based fluid simulation. We introduce a sampling condition based on geometric local feature size that allows focusing computational resources in geometrically complex regions, while reducing the number of particles deep inside the fluid or near thick flat surfaces. Further performance gains are achieved by varying the sampling density according to visual importance. In addition, we propose a novel fluid surface definition based on approximate particle-to-surface distances that are carried along with the particles and updated appropriately. The resulting surface reconstruction method has several advantages over existing methods, including stability under particle resampling and suitability for representing smooth flat surfaces. We demonstrate how our adaptive sampling and distance-based surface reconstruction algorithms lead to significant improvements in time and memory as compared to single resolution particle simulations, without significantly affecting the fluid flow behavior.	Adaptively sampled particle fluids	NA:NA:NA:NA	2018
Rony Goldenthal:David Harmon:Raanan Fattal:Michel Bercovier:Eitan Grinspun	Many textiles do not noticeably stretch under their own weight. Unfortunately, for better performance many cloth solvers disregard this fact. We propose a method to obtain very low strain along the warp and weft direction using Constrained Lagrangian Mechanics and a novel fast projection method. The resulting algorithm acts as a velocity filter that easily integrates into existing simulation code.	Efficient simulation of inextensible cloth	NA:NA:NA:NA:NA	2018
Miklós Bergou:Saurabh Mathur:Max Wardetzky:Eitan Grinspun	We combine the often opposing forces of artistic freedom and mathematical determinism to enrich a given animation or simulation of a surface with physically based detail. We present a process called tracking, which takes as input a rough animation or simulation and enhances it with physically simulated detail. Building on the foundation of constrained Lagrangian mechanics, we propose weak-form constraints for tracking the input motion. This method allows the artist to choose where to add details such as characteristic wrinkles and folds of various thin shell materials and dynamical effects of physical forces. We demonstrate multiple applications ranging from enhancing an artist's animated character to guiding a simulated inanimate object.	TRACKS: toward directable thin shells	NA:NA:NA:NA	2018
Hendrik Lensch	NA	Session details: Appearance capture & editing	NA	2018
Raanan Fattal:Maneesh Agrawala:Szymon Rusinkiewicz	We present a new image-based technique for enhancing the shape and surface details of an object. The input to our system is a small set of photographs taken from a fixed viewpoint, but under varying lighting conditions. For each image we compute a multiscale decomposition based on the bilateral filter and then reconstruct an enhanced image that combines detail information at each scale across all the input images. Our approach does not require any information about light source positions, or camera calibration, and can produce good results with 3 to 5 input images. In addition our system provides a few high-level parameters for controlling the amount of enhancement and does not require pixel-level user input. We show that the bilateral filter is a good choice for our multiscale algorithm because it avoids the halo artifacts commonly associated with the traditional Laplacian image pyramid. We also develop a new scheme for computing our multiscale bilateral decomposition that is simple to implement, fast O(N2 log N) and accurate.	Multiscale shape and detail enhancement from multi-light image collections	NA:NA:NA	2018
Pieter Peers:Naoki Tamura:Wojciech Matusik:Paul Debevec	We propose a novel post-production facial performance relighting system for human actors. Our system uses just a dataset of view-dependent facial appearances with a neutral expression, captured for a static subject using a Light Stage apparatus. For the actual performance, however, a potentially different actor is captured under known, but static, illumination. During post-production, the reflectance field of the reference dataset actor is transferred onto the dynamic performance, enabling image-based relighting of the entire sequence. Our approach makes post-production relighting more practical and could easily be incorporated in a traditional production pipeline since it does not require additional hardware during principal photography. Additionally, we show that our system is suitable for real-time post-production illumination editing.	Post-production facial performance relighting using reflectance transfer	NA:NA:NA:NA	2018
Jan Kautz:Solomon Boulos:Frédo Durand	While measured Bidirectional Texture Functions (BTF) enable impressive realism in material appearance, they offer little control, which limits their use for content creation. In this work, we interactively manipulate BTFs and create new BTFs from flat textures. We present an out-of-core approach to manage the size of BTFs and introduce new editing operations that modify the appearance of a material. These tools achieve their full potential when selectively applied to subsets of the BTF through the use of new selection operators. We further analyze the use of our editing operators for the modification of important visual characteristics such as highlights, roughness, and fuzziness. Results compare favorably to the direct alteration of micro-geometry and reflectances of synthetic reference data.	Interactive editing and modeling of bidirectional texture functions	NA:NA:NA	2018
Fabio Pellacini:Jason Lawrence	We investigate a new approach to editing spatially- and temporally-varying measured materials that adopts a stroke-based workflow. In our system, a user specifies a small number of editing constraints with a 3-D painting interface which are smoothly propagated to the entire dataset through an optimization that enforces similar edits are applied to areas with similar appearance. The sparse nature of this appearance-driven optimization permits the use of efficient solvers, allowing the designer to interactively refine the constraints. We have found this approach supports specifying a wide range of complex edits that would not be easy with existing techniques which present the user with a fixed segmentation of the data. Furthermore, it is independent of the underlying reflectance model and we show edits to both analytic and non-parametric representations in examples from several material databases.	AppWand: editing measured materials using appearance-driven optimization	NA:NA	2018
Pierre Alliez	NA	Session details: Geometry processing I	NA	2018
Jonathan Palacios:Eugene Zhang	Designing rotational symmetries on surfaces is a necessary task for a wide variety of graphics applications, such as surface parameterization and remeshing, painterly rendering and pen-and-ink sketching, and texture synthesis. In these applications, the topology of a rotational symmetry field such as singularities and separatrices can have a direct impact on the quality of the results. In this paper, we present a design system that provides control over the topology of rotational symmetry fields on surfaces. As the foundation of our system, we provide comprehensive analysis for rotational symmetry fields on surfaces and present efficient algorithms to identify singularities and separatrices. We also describe design operations that allow a rotational symmetry field to be created and modified in an intuitive fashion by using the idea of basis fields and relaxation. In particular, we provide control over the topology of a rotational symmetry field by allowing the user to remove singularities from the field or to move them to more desirable locations. At the core of our analysis and design implementations is the observations that N-way rotational symmetries can be described by symmetric N-th order tensors, which allows an efficient vector-based representation that not only supports coherent definitions of arithmetic operations on rotational symmetries but also enables many analysis and design operations for vector fields to be adapted to rotational symmetry fields. To demonstrate the effectiveness of our approach, we apply our design system to pen-and-ink sketching and geometry remeshing.	Rotational symmetry field design on surfaces	NA:NA	2018
Matthew Fisher:Peter Schröder:Mathieu Desbrun:Hugues Hoppe	Tangent vector fields are an essential ingredient in controlling surface appearance for applications ranging from anisotropic shading to texture synthesis and non-photorealistic rendering. To achieve a desired effect one is typically interested in smoothly varying fields that satisfy a sparse set of user-provided constraints. Using tools from Discrete Exterior Calculus, we present a simple and efficient algorithm for designing such fields over arbitrary triangle meshes. By representing the field as scalars over mesh edges (i.e., discrete 1-forms), we obtain an intrinsic, coordinate-free formulation in which field smoothness is enforced through discrete Laplace operators. Unlike previous methods, such a formulation leads to a linear system whose sparsity permits efficient pre-factorization. Constraints are incorporated through weighted least squares and can be updated rapidly enough to enable interactive design, as we demonstrate in the context of anisotropic texture synthesis.	Design of tangent vector fields	NA:NA:NA:NA	2018
François Labelle:Jonathan Richard Shewchuk	The isosurface stuffing algorithm fills an isosurface with a uniformly sized tetrahedral mesh whose dihedral angles are bounded between 10.7° and 164.8°, or (with a change in parameters) between 8.9° and 158.8°. The algorithm is whip fast, numerically robust, and easy to implement because, like Marching Cubes, it generates tetrahedra from a small set of precomputed stencils. A variant of the algorithm creates a mesh with internal grading: on the boundary, where high resolution is generally desired, the elements are fine and uniformly sized, and in the interior they may be coarser and vary in size. This combination of features makes isosurface stuffing a powerful tool for dynamic fluid simulation, large-deformation mechanics, and applications that require interactive remeshing or use objects defined by smooth implicit surfaces. It is the first algorithm that rigorously guarantees the suitability of tetrahedra for finite element methods in domains whose shapes are substantially more challenging than boxes. Our angle bounds are guaranteed by a computer-assisted proof. If the isosurface is a smooth 2-manifold with bounded curvature, and the tetrahedra are sufficiently small, then the boundary of the mesh is guaranteed to be a geometrically and topologically accurate approximation of the isosurface.	Isosurface stuffing: fast tetrahedral meshes with good dihedral angles	NA:NA	2018
Valerio Pascucci:Giorgio Scorzelli:Peer-Timo Bremer:Ajith Mascarenhas	Reeb graphs are a fundamental data structure for understanding and representing the topology of shapes. They are used in computer graphics, solid modeling, and visualization for applications ranging from the computation of similarities and finding defects in complex models to the automatic selection of visualization parameters. We introduce an on-line algorithm that reads a stream of elements (vertices, triangles, tetrahedra, etc.) and continuously maintains the Reeb graph of all elements already reed. The algorithm is robust in handling non-manifold meshes and general in its applicability to input models of any dimension. Optionally, we construct a skeleton-like embedding of the Reeb graph, and/or remove topological noise to reduce the output size. For interactive multi-resolution navigation we also build a hierarchical data structure which allows real-time extraction of approximated Reeb graphs containing all topological features above a given error threshold. Our extensive experiments show both high performance and practical linear scalability for meshes ranging from thousands to hundreds of millions of triangles. We apply our algorithm to the largest, most general, triangulated surfaces available to us, including 3D, 4D and 5D simplicial meshes. To demonstrate one important application we use Reeb graphs to find and highlight topological defects in meshes, including some widely believed to be "clean."	Robust on-line computation of Reeb graphs: simplicity and speed	NA:NA:NA:NA	2018
Kavita Bala	NA	Session details: Light transport	NA	2018
Ivo Ihrke:Gernot Ziegler:Art Tevs:Christian Theobalt:Marcus Magnor:Hans-Peter Seidel	We present a new method for real-time rendering of sophisticated lighting effects in and around refractive objects. It enables us to realistically display refractive objects with complex material properties, such as arbitrarily varying refractive index, inhomogeneous attenuation, as well as spatially-varying anisotropic scattering and reflectance properties. User-controlled changes of lighting positions only require a few seconds of update time. Our method is based on a set of ordinary differential equations derived from the eikonal equation, the main postulate of geometric optics. This set of equations allows for fast casting of bent light rays with the complexity of a particle tracer. Based on this concept, we also propose an efficient light propagation technique using adaptive wavefront tracing. Efficient GPU implementations for our algorithmic concepts enable us to render a combination of visual effects that were previously not reproducible in real-time.	Eikonal rendering: efficient light transport in refractive objects	NA:NA:NA:NA:NA:NA	2018
Jeppe Revall Frisvad:Niels Jørgen Christensen:Henrik Wann Jensen	This paper introduces a theoretical model for computing the scattering properties of participating media and translucent materials. The model takes as input a description of the components of a medium and computes all the parameters necessary to render it. These parameters are the extinction and scattering coefficients, the phase function, and the index of refraction, Our theory is based on a robust generalization of the Lorenz-Mie theory. Previous models using Lorenz-Mie theory have been limited to non-absorbing media with spherical particles such as paints and clouds. Our generalized theory is capable of handling both absorbing host media and non-spherical particles, which significantly extends the classes of media and materials that can be modeled. We use the theory to computer optical properties for different types of ice and ocean water, and we derive a novel appearance model for milk parameterized by the fat and protein contents. Our results show that we are able to match measured scattering properties in cases where the classical Lorez-Mie theory breaks down, and we can compute properties for media that cannot be measured using existing techniques in computer graphics.	Computing the scattering properties of participating media using Lorenz-Mie theory	NA:NA:NA	2018
Carsten Dachsbacher:Marc Stamminger:George Drettakis:Frédo Durand	We reformulate the rendering equation to alleviate the need for explicit visibility computation, thus enabling interactive global illumination on graphics hardware. This is achieved by treating visibility implicitly and propagating an additional quantity, called antiradiance, to compensate for light transmitted extraneously. Our new algorithm shifts visibility computation to simple local iterations by maintaining additional directional antiradiance information with samples in the scene. It is easy to parallelize on a GPU. By correctly treating discretization and filtering, we can compute indirect illumination in scenes with dynamic objects much faster than traditional methods. Our results show interactive update of indirect illumination with moving characters and lights.	Implicit visibility and antiradiance for interactive global illumination	NA:NA:NA:NA	2018
Dhruv Mahajan:Ira Kemelmacher Shlizerman:Ravi Ramamoorthi:Peter Belhumeur	Blockwise or Clustered Principal Component Analysis (CPCA) is commonly used to achieve real-time rendering of shadows and glossy reflections with precomputed radiance transfer (PRT). The vertices or pixels are partitioned into smaller coherent regions, and light transport in each region is approximated by a locally low-dimensional subspace using PCA. Many earlier techniques such as surface light field and reflectance field compression use a similar paradigm. However, there has been no clear theoretical understanding of how light transport dimensionality increases with local patch size, nor of the optimal block size or number of clusters. In this paper, we develop a theory of locally low dimensional light transport, by using Szego's eigenvalue theorem to analytically derive the eigenvalues of the covariance matrix for canonical cases. We show mathematically that for symmetric patches of area A, the number of basis functions for glossy reflections increases linearly with A, while for simple cast shadows, it often increases as √A. These results are confirmed numerically on a number of test scenes. Next, we carry out an analysis of the cost of rendering, trading off local dimensionality and the number of patches, deriving an optimal block size. Based on this analysis, we provide useful practical insights for setting parameters in CPCA and also derive a new adaptive subdivision algorithm. Moreover, we show that rendering time scales sub-linearly with the resolution of the image, allowing for interactive all-frequency relighting of 1024 x 1024 images.	A theory of locally low dimensional light transport	NA:NA:NA:NA	2018
Nina Amenta	NA	Session details: Geometry processing II	NA	2018
Niloy J. Mitra:Leonidas J. Guibas:Mark Pauly	We present a symmetrization algorithm for geometric objects. Our algorithm enhances approximate symmetries of a model while minimally altering its shape. Symmetrizing deformations are formulated as an optimization process that couples the spatial domain with a transformation configuration space, where symmetries can be expressed more naturally and compactly as parametrized point-pair mappings. We derive closed-form solution for the optimal symmetry transformations, given a set of corresponding sample pairs. The resulting optimal displacement vectors are used to drive a constrained deformation model that pulls the shape towards symmetry. We show how our algorithm successfully symmetrizes both the geometry and the discretization of complex 2D and 3D shapes and discuss various applications of such symmetrizing deformations.	Symmetrization	NA:NA:NA	2018
Martin Kilian:Niloy J. Mitra:Helmut Pottmann	We present a novel framework to treat shapes in the setting of Riemannian geometry. Shapes -- triangular meshes or more generally straight line graphs in Euclidean space -- are treated as points in a shape space. We introduce useful Riemannian metrics in this space to aid the user in design and modeling tasks, especially to explore the space of (approximately) isometric deformations of a given shape. Much of the work relies on an efficient algorithm to compute geodesics in shape spaces; to this end, we present a multi-resolution framework to solve the interpolation problem -- which amounts to solving a boundary value problem -- as well as the extrapolation problem -- an initial value problem -- in shape space. Based on these two operations, several classical concepts like parallel transport and the exponential map can be used in shape space to solve various geometric modeling and geometry processing tasks. Applications include shape morphing, shape deformation, deformation transfer, and intuitive shape exploration.	Geometric modeling in shape space	NA:NA:NA	2018
Helmut Pottmann:Yang Liu:Johannes Wallner:Alexander Bobenko:Wenping Wang	The geometric challenges in the architectural design of freeform shapes come mainly from the physical realization of beams and nodes. We approach them via the concept of parallel meshes, and present methods of computation and optimization. We discuss planar faces, beams of controlled height, node geometry, and multilayer constructions. Beams of constant height are achieved with the new type of edge offset meshes. Mesh parallelism is also the main ingredient in a novel discrete theory of curvatures. These methods are applied to the construction of quadrilateral, pentagonal and hexagonal meshes, discrete minimal surfaces, discrete constant mean curvature surfaces, and their geometric transforms. We show how to design geometrically optimal shapes, and how to find a meaningful meshing and beam layout for existing shapes.	Geometry of multi-layer freeform structures for architecture	NA:NA:NA:NA:NA	2018
Patrick Mullen:Alexander McKenzie:Yiying Tong:Mathieu Desbrun	We present a purely Eulerian framework for geometry processing of surfaces and foliations. Contrary to current Eulerian methods used in graphics, we use conservative methods and a variational interpretation, offering a unified framework for routine surface operations such as smoothing, offsetting, and animation. Computations are performed on a fixed volumetric grid without recourse to Lagrangian techniques such as triangle meshes, particles, or path tracing. At the core of our approach is the use of the Coarea Formula to express area integrals over isosurfaces as volume integrals. This enables the simultaneous processing of multiple isosurfaces, while a single interface can be treated as the special case of a dense foliation. We show that our method is a powerful alternative to conventional geometric representations in delicate cases such as the handling of high-genus surfaces, weighted offsetting, foliation smoothing of medical datasets, and incompressible fluid animation.	A variational approach to Eulerian geometry processing	NA:NA:NA:NA	2018
Marc Levoy	NA	Session details: Computational cameras	NA	2018
Francesc Moreno-Noguer:Peter N. Belhumeur:Shree K. Nayar	We present a system for refocusing images and videos of dynamic scenes using a novel, single-view depth estimation method. Our method for obtaining depth is based on the defocus of a sparse set of dots projected onto the scene. In contrast to other active illumination techniques, the projected pattern of dots can be removed from each captured image and its brightness easily controlled in order to avoid under- or over-exposure. The depths corresponding to the projected dots and a color segmentation of the image are used to compute an approximate depth map of the scene with clean region boundaries. The depth map is used to refocus the acquired image after the dots are removed, simulating realistic depth of field effects. Experiments on a wide variety of scenes, including close-ups and live action, demonstrate the effectiveness of our method.	Active refocusing of images and videos	NA:NA:NA	2018
Paul Green:Wenyang Sun:Wojciech Matusik:Frédo Durand	The emergent field of computational photography is proving that, by coupling generalized imaging optics with software processing, the quality and flexibility of imaging systems can be increased. In this paper, we capture and manipulate multiple images of a scene taken with different aperture settings (f-numbers). We design and implement a prototype optical system and associated algorithms to capture four images of the scene in a single exposure, each taken with a different aperture setting. Our system can be used with commercially available DSLR cameras and photographic lenses without modification to either. We leverage the fact that defocus blur is a function of scene depth and f/# to estimate a depth map. We demonstrate several applications of our multi-aperture camera, such as post-exposure editing of the depth of field, including extrapolation beyond the physical limits of the lens, synthetic refocusing, and depth-guided deconvolution.	Multi-aperture photography	NA:NA:NA:NA	2018
Ashok Veeraraghavan:Ramesh Raskar:Amit Agrawal:Ankit Mohan:Jack Tumblin	We describe a theoretical framework for reversibly modulating 4D light fields using an attenuating mask in the optical path of a lens based camera. Based on this framework, we present a novel design to reconstruct the 4D light field from a 2D camera image without any additional refractive elements as required by previous light field cameras. The patterned mask attenuates light rays inside the camera instead of bending them, and the attenuation recoverably encodes the rays on the 2D sensor. Our mask-equipped camera focuses just as a traditional camera to capture conventional 2D photos at full sensor resolution, but the raw pixel values also hold a modulated 4D light field. The light field can be recovered by rearranging the tiles of the 2D Fourier transform of sensor values into 4D planes, and computing the inverse Fourier transform. In addition, one can also recover the full resolution image information for the in-focus parts of the scene. We also show how a broadband mask placed at the lens enables us to compute refocused images at full sensor resolution for layered Lambertian scenes. This partial encoding of 4D ray-space data enables editing of image contents by depth, yet does not require computational recovery of the complete 4D light field.	Dappled photography: mask enhanced cameras for heterodyned light fields and coded aperture refocusing	NA:NA:NA:NA:NA	2018
Anat Levin:Rob Fergus:Frédo Durand:William T. Freeman	A conventional camera captures blurred versions of scene information away from the plane of focus. Camera systems have been proposed that allow for recording all-focus images, or for extracting depth, but to record both simultaneously has required more extensive hardware and reduced spatial resolution. We propose a simple modification to a conventional camera that allows for the simultaneous recovery of both (a) high resolution image information and (b) depth information adequate for semi-automatic extraction of a layered depth representation of the image. Our modification is to insert a patterned occluder within the aperture of the camera lens, creating a coded aperture. We introduce a criterion for depth discriminability which we use to design the preferred aperture pattern. Using a statistical model of images, we can recover both depth information and an all-focus image from single photographs taken with the modified camera. A layered depth map is then extracted, requiring user-drawn strokes to clarify layer assignments in some cases. The resulting sharp image and layered depth map can be combined for various photographic applications, including automatic scene segmentation, post-exposure refocusing, or re-rendering of the scene from an alternate viewpoint.	Image and depth from a conventional camera with a coded aperture	NA:NA:NA:NA	2018
Doug James	NA	Session details: Articulation	NA	2018
Pushkar Joshi:Mark Meyer:Tony DeRose:Brian Green:Tom Sanocki	In this paper we consider the problem of creating and controlling volume deformations used to articulate characters for use in high-end applications such as computer generated feature films. We introduce a method we call harmonic coordinates that significantly improves upon existing volume deformation techniques. Our deformations are controlled using a topologically flexible structure, called a cage, that consists of a closed three dimensional mesh. The cage can optionally be augmented with additional interior vertices, edges, and faces to more precisely control the interior behavior of the deformation. We show that harmonic coordinates are generalized barycentric coordinates that can be extended to any dimension. Moreover, they are the first system of generalized barycentric coordinates that are non-negative even in strongly concave situations, and their magnitude falls off with distance as measured within the cage.	Harmonic coordinates for character articulation	NA:NA:NA:NA:NA	2018
Ilya Baran:Jovan Popović	Animating an articulated 3D character currently requires manual rigging to specify its internal skeletal structure and to define how the input motion deforms its surface. We present a method for animating characters automatically. Given a static character mesh and a generic skeleton, our method adapts the skeleton to the character and attaches it to the surface, allowing skeletal motion data to animate the character. Because a single skeleton can be used with a wide range of characters, our method, in conjunction with a library of motions for a few skeletons, enables a user-friendly animation system for novices and children. Our prototype implementation, called Pinocchio, typically takes under a minute to rig a character on a modern midrange PC.	Automatic rigging and animation of 3D characters	NA:NA	2018
Robert Y. Wang:Kari Pulli:Jovan Popović	Enveloping, or the mapping of skeletal controls to the deformations of a surface, is key to driving realistic animated characters. Despite its widespread use, enveloping still relies on slow or inaccurate deformation methods. We propose a method that is both fast, accurate and example-based. Our technique introduces a rotational regression model that captures common skinning deformations such as muscle bulging, twisting, and challenging areas such as the shoulders. Our improved treatment of rotational quantities is made practical by model reduction that ensures real-time solution of least-squares problems, independent of the mesh size. Our method is significantly more accurate than linear blend skinning and almost as fast, suggesting its use as a replacement for linear blend skinning when examples are available.	Real-time enveloping with rotational regression	NA:NA:NA	2018
Mark Meyer:John Anderson	Many applications in Computer Graphics contain computationally expensive calculations. These calculations are often performed at many points to produce a full solution, even though the subspace of reasonable solutions may be of a relatively low dimension. The calculation of facial articulation and rendering of scenes with global illumination are two example applications that require these sort of computations. In this paper, we present Key Point Subspace Acceleration and Soft Caching, a technique for accelerating these types of computations. Key Point Subspace Acceleration (KPSA) is a statistical acceleration scheme that uses examples to compute a statistical subspace and a set of characteristic key points. The full calculation is then computed only at these key points and these points are used to provide a subspace based estimate of the entire calculation. The soft caching process is an extension to the KPSA technique where the key points are also used to provide a confidence estimate for the KPSA result. In cases with high anticipated error the calculation will then "fail through" to a full evaluation of all points (a cache miss), while frames with low error can use the accelerated statistical evaluation (a cache hit).	Key Point Subspace Acceleration and soft caching	NA:NA	2018
Erik Reinhard	NA	Session details: Perception & color	NA	2018
Roger D. Hersch:Philipp Donzé:Sylvain Chosson	The present contribution aims at creating color images printed with fluorescent inks that are only visible under UV light. The considered fluorescent inks absorb light in the UV wavelength range and reemit part of it in the visible wavelength range. In contrast to normal color printing which relies on the spectral absorption of light by the inks, at low concentration fluorescent inks behave additively, i.e. their light emission spectra sum up. We first analyze to which extent different fluorescent inks can be superposed. Due to the quenching effect, at high concentrations of the fluorescent molecules, the fluorescent effect diminishes. With an ink-jet printer capable of printing pixels at reduced dot sizes, we reduce the concentration of the individual fluorescent inks and are able to create from the blue, red and greenish-yellow inks the new colorants white and magenta. In order to avoid quenching effects, we propose a color halftoning method relying on diagonally oriented pre-computed screen dots, which are printed side by side. For gamut mapping and color separation, we create a 3D representation of the fluorescent ink gamut in CIELAB space by predicting halftone fluorescent emission spectra according to the spectral Neugebauer model. Thanks to gamut mapping and juxtaposed halftoning, we create color images, which are invisible under daylight and have, under UV light, a high resemblance with the original images.	Color images visible under UV light	NA:NA:NA	2018
Ganesh Ramanarayanan:James Ferwerda:Bruce Walter:Kavita Bala	Efficient, realistic rendering of complex scenes is one of the grand challenges in computer graphics. Perceptually based rendering addresses this challenge by taking advantage of the limits of human vision. However, existing methods, based on predicting visible image differences, are too conservative because some kinds of image differences do not matter to human observers. In this paper, we introduce the concept of visual equivalence, a new standard for image fidelity in graphics. Images are visually equivalent if they convey the same impressions of scene appearance, even if they are visibly different. To understand this phenomenon, we conduct a series of experiments that explore how object geometry, material, and illumination interact to provide information about appearance, and we characterize how two kinds of transformations on illumination maps (blurring and warping) affect these appearance attributes. We then derive visual equivalence predictors (VEPs): metrics for predicting when images rendered with transformed illumination maps will be visually equivalent to images rendered with reference maps. We also run a confirmatory study to validate the effectiveness of these VEPs for general scenes. Finally, we show how VEPs can be used to improve the efficiency of two rendering algorithms: Light-cuts and precomputed radiance transfer. This work represents some promising first steps towards developing perceptual metrics based on higher order aspects of visual coding.	Visual equivalence: towards a new standard for image fidelity	NA:NA:NA:NA	2018
Peter Vangorp:Jurgen Laurijssen:Philip Dutré	Visual observation is our principal source of information in determining the nature of objects, including shape, material or roughness. The physiological and cognitive processes that resolve visual input into an estimate of the material of an object are influenced by the illumination and the shape of the object. This affects our ability to select materials by observing them on a point-lit sphere, as is common in current 3D modeling applications. In this paper we present an exploratory psychophysical experiment to study various influences on material discrimination in a realistic setting. The resulting data set is analyzed using a wide range of statistical techniques. Analysis of variance is used to estimate the magnitude of the influence of geometry, and fitted psychometric functions produce significantly diverse material discrimination thresholds across different shapes and materials. Suggested improvements to traditional material pickers include direct visualization on the target object, environment illumination, and the use of discrimination thresholds as a step size for parameter adjustments.	The influence of shape on the perception of material reflectance	NA:NA:NA	2018
Fabio Pellacini	NA	Session details: Sampling	NA	2018
Victor Ostromoukhov	We present a new general-purpose method for fast hierarchical importance sampling with blue-noise properties. Our approach is based on self-similar tiling of the plane or the surface of a sphere with rectifiable polyominoes. Sampling points are associated with polyominoes, one point per polyomino. Each polyomino is recursively subdivided until the desired local density of samples is reached. A numerical code generated during the subdivision process is used for thresholding to accept or reject the sample. The exact position of the sampling point within the polyomino is determined according to a structural index, which indicates the polyomino's local neighborhood. The variety of structural indices and associated sampling point positions are computed during the offline optimization process, and tabulated. Consequently, the sampling itself is extremely fast. The method allows both deterministic and pseudo-non-deterministic sampling. It can be successfully applied in a large variety of graphical applications, where fast sampling with good spectral and visual properties is required. The prime application is rendering.	Sampling with polyominoes	NA	2018
Robert L. Cook:John Halstead:Maxwell Planck:David Ryu	Many renderers perform poorly on scenes that contain a lot of detailed geometry. The load on the renderer can be alleviated by simplification techniques, which create less expensive representations of geometry that is small on the screen. Current simplification techniques for high-quality surface-based rendering tend to work best with element detail (i.e., detail due to the complexity of individual elements) but not as well with aggregate detail (i.e., detail due to the large number of elements). To address this latter type of detail, we introduce a stochastic technique related to some approaches used for point-based renderers. Scenes are rendered by randomly selecting a subset of the geometric elements and altering those elements statistically to preserve the overall appearance of the scene. The amount of simplification can depend on a number of factors, including screen size, motion blur, and depth of field.	Stochastic simplification of aggregate detail	NA:NA:NA:NA	2018
Denis Zorin	NA	Session details: Shape deformation	NA	2018
Robert W. Sumner:Johannes Schmid:Mark Pauly	We present an algorithm that generates natural and intuitive deformations via direct manipulation for a wide range of shape representations and editing scenarios. Our method builds a space deformation represented by a collection of affine transformations organized in a graph structure. One transformation is associated with each graph node and applies a deformation to the nearby space. Positional constraints are specified on the points of an embedded object. As the user manipulates the constraints, a nonlinear minimization problem is solved to find optimal values for the affine transformations. Feature preservation is encoded directly in the objective function by measuring the deviation of each transformation from a true rotation. This algorithm addresses the problem of "embedded deformation" since it deforms space through direct manipulation of objects embedded within it, while preserving the embedded objects' features. We demonstrate our method by editing meshes, polygon soups, mesh animations, and animated particle systems.	Embedded deformation for shape manipulation	NA:NA:NA	2018
Xiaohan Shi:Kun Zhou:Yiying Tong:Mathieu Desbrun:Hujun Bao:Baining Guo	We present mesh puppetry, a variational framework for detail-preserving mesh manipulation through a set of high-level, intuitive, and interactive design tools. Our approach builds upon traditional rigging by optimizing skeleton position and vertex weights in an integrated manner. New poses and animations are created by specifying a few desired constraints on vertex positions, balance of the character, length and rigidity preservation, joint limits, and/or self-collision avoidance. Our algorithm then adjusts the skeleton and solves for the deformed mesh simultaneously through a novel cascading optimization procedure, allowing realtime manipulation of meshes with 50K+ vertices for fast design of pleasing and realistic poses. We demonstrate the potential of our framework through an interactive deformation platform and various applications such as deformation transfer and motion retargeting.	Mesh puppetry: cascading optimization of mesh deformation with inverse kinematics	NA:NA:NA:NA:NA:NA	2018
Alec R. Rivers:Doug L. James	We introduce a simple technique that enables robust approximation of volumetric, large-deformation dynamics for real-time or large-scale offline simulations. We propose Lattice Shape Matching, an extension of deformable shape matching to regular lattices with embedded geometry; lattice vertices are smoothed by convolution of rigid shape matching operators on local lattice regions, with the effective mechanical stiffness specified by the amount of smoothing via region width. Since the naïve method can be very slow for stiff models - per-vertex costs scale cubically with region width - we provide a fast summation algorithm, Fast Lattice Shape Matching (FastLSM), that exploits the inherent summation redundancy of shape matching and can provide large-region matching at constant per-vertex cost. With this approach, large lattices can be simulated in linear time. We present several examples and benchmarks of an efficient CPU implementation, including many dozens of soft bodies simulated at real-time rates on a typical desktop machine.	FastLSM: fast lattice shape matching for robust real-time deformation	NA:NA	2018
Oscar Kin-Chung Au:Hongbo Fu:Chiew-Lan Tai:Daniel Cohen-Or	Handle-based mesh deformation is essentially a nonlinear problem. To allow scalability, the original deformation problem can be approximately represented by a compact set of control variables. We show the direct relation between the locations of handles on the mesh and the local rigidity under deformation, and introduce the notion of handle-aware rigidity. Then, we present a reduced model whose control variables are intelligently distributed across the surface, respecting the rigidity information and the geometry. Specifically, for each handle, the control variables are the transformations of the isolines of a harmonic scalar field representing the deformation propagation from that handle. The isolines constitute a virtual skeletal structure similar to the bones in skinning deformation, thus correctly capturing the low-frequency shape deformation. To interpolate the transformations from the isolines to the original mesh, we design a method which is local, linear and geometry-dependent. This novel interpolation scheme and the transformation-based reduced domain allow each iteration of the nonlinear solver to be fully computed over the reduced domain. This makes the per-iteration cost dependent on only the number of isolines and enables compelling deformation of highly detailed shapes at interactive rates. In addition, we show how the handle-driven isolines provide an efficient means for deformation transfer without full shape correspondence.	Handle-aware isolines for scalable shape editing	NA:NA:NA:NA	2018
Weiwei Xu:Kun Zhou:Yizhou Yu:Qifeng Tan:Qunsheng Peng:Baining Guo	Many graphics applications, including computer games and 3D animated films, make heavy use of deforming mesh sequences. In this paper, we generalize gradient domain editing to deforming mesh sequences. Our framework is keyframe based. Given sparse and irregularly distributed constraints at unevenly spaced keyframes, our solution first adjusts the meshes at the keyframes to satisfy these constraints, and then smoothly propagate the constraints and deformations at keyframes to the whole sequence to generate new deforming mesh sequence. To achieve convenient keyframe editing, we have developed an efficient alternating least-squares method. It harnesses the power of subspace deformation and two-pass linear methods to achieve high-quality deformations. We have also developed an effective algorithm to define boundary conditions for all frames using handle trajectory editing. Our deforming mesh editing framework has been successfully applied to a number of editing scenarios with increasing complexity, including footprint editing, path editing, temporal filtering, handle-based deformation mixing, and spacetime morphing.	Gradient domain editing of deforming mesh sequences	NA:NA:NA:NA:NA:NA	2018
Hanspeter Pfister	NA	Session details: Image-based modeling	NA	2018
Pascal Müller:Gang Zeng:Peter Wonka:Luc Van Gool	This paper describes algorithms to automatically derive 3D models of high visual quality from single facade images of arbitrary resolutions. We combine the procedural modeling pipeline of shape grammars with image analysis to derive a meaningful hierarchical facade subdivision. Our system gives rise to three exciting applications: urban reconstruction based on low resolution oblique aerial imagery, reconstruction of facades based on higher resolution ground-based imagery, and the automatic derivation of shape grammar rules from facade images to build a rule base for procedural modeling technology.	Image-based procedural modeling of facades	NA:NA:NA:NA	2018
Anton van den Hengel:Anthony Dick:Thorsten Thormählen:Ben Ward:Philip H. S. Torr	VideoTrace is a system for interactively generating realistic 3D models of objects from video---models that might be inserted into a video game, a simulation environment, or another video sequence. The user interacts with VideoTrace by tracing the shape of the object to be modelled over one or more frames of the video. By interpreting the sketch drawn by the user in light of 3D information obtained from computer vision techniques, a small number of simple 2D interactions can be used to generate a realistic 3D model. Each of the sketching operations in VideoTrace provides an intuitive and powerful means of modelling shape from video, and executes quickly enough to be used interactively. Immediate feedback allows the user to model rapidly those parts of the scene which are of interest and to the level of detail required. The combination of automated and manual reconstruction allows VideoTrace to model parts of the scene not visible, and to succeed in cases where purely automated approaches would fail.	VideoTrace: rapid interactive scene modelling from video	NA:NA:NA:NA:NA	2018
Ping Tan:Gang Zeng:Jingdong Wang:Sing Bing Kang:Long Quan	In this paper, we propose an approach for generating 3D models of natural-looking trees from images that has the additional benefit of requiring little user intervention. While our approach is primarily image-based, we do not model each leaf directly from images due to the large leaf count, small image footprint, and widespread occlusions. Instead, we populate the tree with leaf replicas from segmented source images to reconstruct the overall tree shape. In addition, we use the shape patterns of visible branches to predict those of obscured branches. We demonstrate our approach on a variety of trees.	Image-based tree modeling	NA:NA:NA:NA:NA	2018
Boris Neubert:Thomas Franken:Oliver Deussen	We present a method for producing 3D tree models from input photographs with only limited user intervention. An approximate voxel-based tree volume is estimated using image information. The density values of the voxels are used to produce initial positions for a set of particles. Performing a 3D flow simulation, the particles are traced downwards to the tree basis and are combined to form twigs and branches. If possible, the trunk and the first-order branches are determined in the input photographs and are used as attractors for particle simulation. The geometry of the tree skeleton is produced using botanical rules for branch thicknesses and branching angles. Finally, leaves are added. Different initial seeds for particle simulation lead to a variety, yet similar-looking branching structures for a single set of photographs.	Approximate image-based tree-modeling using particle flows	NA:NA:NA	2018
Greg Humphreys	NA	Session details: Graphics architecture	NA	2018
Pedro V. Sander:Diego Nehab:Joshua Barczak	We present novel algorithms that optimize the order in which triangles are rendered, to improve post-transform vertex cache efficiency as well as for view-independent overdraw reduction. The resulting triangle orders perform on par with previous methods, but are orders magnitude faster to compute. The improvements in processing speed allow us to perform the optimization right after a model is loaded, when more information on the host hardware is available. This allows our vertex cache optimization to often outperform other methods. In fact, our algorithms can even be executed interactively, allowing for re-optimization in case of changes to geometry or topology, which happen often in CAD/CAM applications. We believe that most real-time rendering applications will immediately benefit from these new results.	Fast triangle reordering for vertex locality and reduced overdraw	NA:NA:NA	2018
Tim Weyrich:Simon Heinzle:Timo Aila:Daniel B. Fasnacht:Stephan Oetiker:Mario Botsch:Cyril Flaig:Simon Mall:Kaspar Rohrer:Norbert Felber:Hubert Kaeslin:Markus Gross	We present a novel architecture for hardware-accelerated rendering of point primitives. Our pipeline implements a refined version of EWA splatting, a high quality method for antialiased rendering of point sampled representations. A central feature of our design is the seamless integration of the architecture into conventional, OpenGL-like graphics pipelines so as to complement triangle-based rendering. The specific properties of the EWA algorithm required a variety of novel design concepts including a ternary depth test and using an on-chip pipelined heap data structure for making the memory accesses of splat primitives more coherent. In addition, we developed a computationally stable evaluation scheme for perspectively corrected splats. We implemented our architecture both on reconfigurable FPGA boards and as an ASIC prototype, and we integrated it into an OpenGL-like software implementation. Our evaluation comprises a detailed performance analysis using scenes of varying complexity.	A hardware architecture for surface splatting	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Kun Zhou:Xin Huang:Weiwei Xu:Baining Guo:Heung-Yeung Shum	We present an algorithm for interactive deformation of subdivision surfaces, including displaced subdivision surfaces and subdivision surfaces with geometric textures. Our system lets the user directly manipulate the surface using freely-selected surface points as handles. During deformation the control mesh vertices are automatically adjusted such that the deforming surface satisfies the handle position constraints while preserving the original surface shape and details. To best preserve surface details, we develop a gradient domain technique that incorporates the handle position constraints and detail preserving objectives into the deformation energy. For displaced subdivision surfaces and surfaces with geometric textures, the deformation energy is highly nonlinear and cannot be handled with existing iterative solvers. To address this issue, we introduce a shell deformation solver, which replaces each numerically unstable iteration step with two stable mesh deformation operations. Our deformation algorithm only uses local operations and is thus suitable for GPU implementation. The result is a real-time deformation system running orders of magnitude faster than the state-of-the-art multigrid mesh deformation solver. We demonstrate our technique with a variety of examples, including examples of creating visually pleasing character animations in real-time by driving a subdivision surface with motion capture data.	Direct manipulation of subdivision surfaces on GPUs	NA:NA:NA:NA:NA	2018
Jon Hasselgren:Thomas Akenine-Möller	Culling techniques have always been a central part of computer graphics, but graphics hardware still lack efficient and flexible support for culling. To improve the situation, we introduce the programmable culling unit, which is as flexible as the fragment program unit and capable of quickly culling entire blocks of fragments. Furthermore, it is very easy for the developer to use the PCU as culling programs can be automatically derived from fragment programs containing a discard instruction. Our PCU can be integrated into an existing fragment program unit with a modest hardware overhead of only about 10%. Using the PCU, we have observed shader speedups between 1.4 and 2.1 for relevant scenes.	PCU: the programmable culling unit	NA:NA	2018
Ramesh Raskar	NA	Session details: Big images	NA	2018
Johannes Kopf:Matt Uyttendaele:Oliver Deussen:Michael F. Cohen	We present a system to capture and view "Gigapixel images": very high resolution, high dynamic range, and wide angle imagery consisting of several billion pixels each. A specialized camera mount, in combination with an automated pipeline for alignment, exposure compensation, and stitching, provide the means to acquire Gigapixel images with a standard camera and lens. More importantly, our novel viewer enables exploration of such images at interactive rates over a network, while dynamically and smoothly interpolating the projection between perspective and curved projections, and simultaneously modifying the tone-mapping to ensure an optimal view of the portion of the scene being viewed.	Capturing and viewing gigapixel images	NA:NA:NA:NA	2018
Aseem Agarwala	We describe a hierarchical approach to improving the efficiency of gradient-domain compositing, a technique that constructs seamless composites by combining the gradients of images into a vector field that is then integrated to form a composite. While gradient-domain compositing is powerful and widely used, it suffers from poor scalability. Computing an n pixel composite requires solving a linear system with n variables; solving such a large system quickly overwhelms the main memory of a standard computer when performed for multi-megapixel composites, which are common in practice. In this paper we show how to perform gradient-domain compositing approximately by solving an O(p) linear system, where p is the total length of the seams between image regions in the composite; for typical cases, p is O(√n). We achieve this reduction by transforming the problem into a space where much of the solution is smooth, and then utilize the pattern of this smoothness to adaptively subdivide the problem domain using quadtrees. We demonstrate the merits of our approach by performing panoramic stitching and image region copy-and-paste in significantly reduced time and memory while achieving visually identical results.	Efficient gradient-domain compositing using quadtrees	NA	2018
Raanan Fattal	In this paper we propose a new method for upsampling images which is capable of generating sharp edges with reduced input-resolution grid-related artifacts. The method is based on a statistical edge dependency relating certain edge features of two different resolutions, which is generically exhibited by real-world images. While other solutions assume some form of smoothness, we rely on this distinctive edge dependency as our prior knowledge in order to increase image resolution. In addition to this relation we require that intensities are conserved; the output image must be identical to the input image when downsampled to the original resolution. Altogether the method consists of solving a constrained optimization problem, attempting to impose the correct edge relation and conserve local intensities with respect to the low-resolution input image. Results demonstrate the visual importance of having such edge features properly matched, and the method's capability to produce images in which sharp edges are successfully reconstructed.	Image upsampling via imposed edge statistics	NA	2018
Johannes Kopf:Michael F. Cohen:Dani Lischinski:Matt Uyttendaele	Image analysis and enhancement tasks such as tone mapping, colorization, stereo depth, and photomontage, often require computing a solution (e.g., for exposure, chromaticity, disparity, labels) over the pixel grid. Computational and memory costs often require that a smaller solution be run over a downsampled image. Although general purpose upsampling methods can be used to interpolate the low resolution solution to the full resolution, these methods generally assume a smoothness prior for the interpolation. We demonstrate that in cases, such as those above, the available high resolution input image may be leveraged as a prior in the context of a joint bilateral upsampling procedure to produce a better high resolution solution. We show results for each of the applications above and compare them to traditional upsampling methods.	Joint bilateral upsampling	NA:NA:NA:NA	2018
Markus Gross	NA	Session details: Fluids	NA	2018
Paul W. Cleary:Soon Hyoung Pyo:Mahesh Prakash:Bon Ki Koo	We present a discrete particle based method capable of creating very realistic animations of bubbles in fluids. It allows for the generation (nucleation) of bubbles from gas dissolved in the fluid, the motion of the discrete bubbles including bubble collisions and drag interactions with the liquid which could be undergoing complex free surface motion, the formation and motion of coupled foams and the final dissipation of bubbles. This allows comprehensive simulations of dynamic bubble behavior. The underlying fluid simulation is based on the mesh-free Smoothed Particle Hydrodynamics method. Each particle representing the liquid contains an amount of dissolved gas. Gas is transferred from the continuum fluid model to the discrete bubble model at nucleation sites on the surface of solid bodies. The rate of gas transport to the nucleation sites controls the rate of bubble generation, producing very natural time variations in bubble numbers. Rising bubbles also grow by gathering more gas from the surrounding liquid as they move. This model contains significant bubble scale physics and allows, in principle, the capturing of many important processes that cannot be directly modeled by traditional methods. The method is used here to realistically animate the pouring of a glass of beer, starting with a stream of fresh beer entering the glass, the formation of a dense cloud of bubbles, which rise to create a good head as the beer reaches the top of the glass.	Bubbling and frothing liquids	NA:NA:NA:NA	2018
Byungmoon Kim:Yingjie Liu:Ignacio Llamas:Xiangmin Jiao:Jarek Rossignac	Liquid and gas interactions often produce bubbles that stay for a long time without bursting on the surface, making a dry foam structure. Such long lasting bubbles simulated by the level set method can suffer from a small but steady volume error that accumulates to a visible amount of volume change. We propose to address this problem by using the volume control method. We track the volume change of each connected region, and apply a carefully computed divergence that compensates undesired volume changes. To compute the divergence, we construct a mathematical model of the volume change, choose control strategies that regulate the modeled volume error, and establish methods to compute the control gains that provide robust and fast reduction of the volume error, and (if desired) the control of how the volume changes over time.	Simulation of bubbles in foam with the volume control method	NA:NA:NA:NA:NA	2018
Cem Yuksel:Donald H. House:John Keyser	We present a new method for the real-time simulation of fluid surface waves and their interactions with floating objects. The method is based on the new concept of wave particles, which offers a simple, fast, and unconditionally stable approach to wave simulation. We show how graphics hardware can be used to convert wave particles to a height field surface, which is warped horizontally to account for local wave-induced flow. The method is appropriate for most fluid simulation situations that do not involve significant global flow. It is demonstrated to work well in constrained areas, including wave reflections off of boundaries, and in unconstrained areas, such as an ocean surface. Interactions with floating objects are easily integrated by including wave forces on the objects and wave generation due to object motion. Theoretical foundations and implementation details are provided, and experiments demonstrate that we achieve plausible realism. Timing studies show that the method is scalable to allow simulation of wave interaction with several hundreds of objects at real-time rates.	Wave particles	NA:NA:NA	2018
Christopher Batty:Florence Bertails:Robert Bridson	Physical simulation has emerged as a compelling animation technique, yet current approaches to coupling simulations of fluids and solids with irregular boundary geometry are inefficient or cannot handle some relevant scenarios robustly. We propose a new variational approach which allows robust and accurate solution on relatively coarse Cartesian grids, allowing possibly orders of magnitude faster simulation. By rephrasing the classical pressure projection step as a kinetic energy minimization, broadly similar to modern approaches to rigid body contact, we permit a robust coupling between fluid and arbitrary solid simulations that always gives a well-posed symmetric positive semi-definite linear system. We provide several examples of efficient fluid-solid interaction and rigid body coupling with sub-grid cell flow. In addition, we extend the framework with a new boundary condition for free-surface flow, allowing fluid to separate naturally from solids.	A fast variational framework for accurate solid-fluid coupling	NA:NA:NA	2018
Maneesh Agrawala	NA	Session details: Video processing	NA	2018
Kalyan Sunkavalli:Wojciech Matusik:Hanspeter Pfister:Szymon Rusinkiewicz	We describe a method for converting time-lapse photography captured with outdoor cameras into Factored Time-Lapse Video (FTLV): a video in which time appears to move faster (i.e., lapsing) and where data at each pixel has been factored into shadow, illumination, and reflectance components. The factorization allows a user to easily relight the scene, recover a portion of the scene geometry (normals), and to perform advanced image editing operations. Our method is easy to implement, robust, and provides a compact representation with good reconstruction characteristics. We show results using several publicly available time-lapse sequences.	Factored time-lapse video	NA:NA:NA:NA	2018
Eric P. Bennett:Leonard McMillan	We present methods for generating novel time-lapse videos that address the inherent sampling issues that arise with traditional photographic techniques. Starting with video-rate footage as input, our post-process downsamples the source material into a time-lapse video and provides user controls for retaining, removing, and resampling events. We employ two techniques for selecting and combining source frames to form the output. First, we present a non-uniform sampling method, based on dynamic programming, which optimizes the sampling of the input video to match the user's desired duration and visual objectives. We present multiple error metrics for this optimization, each resulting in different sampling characteristics. To complement the non-uniform sampling, we present the virtual shutter, a non-linear filtering technique that synthetically extends the exposure time of time-lapse frames.	Computational time-lapse video	NA:NA	2018
Jiawen Chen:Sylvain Paris:Frédo Durand	We present a new data structure---the bilateral grid, that enables fast edge-aware image processing. By working in the bilateral grid, algorithms such as bilateral filtering, edge-aware painting, and local histogram equalization become simple manipulations that are both local and independent. We parallelize our algorithms on modern GPUs to achieve real-time frame rates on high-definition video. We demonstrate our method on a variety of applications such as image editing, transfer of photographic look, and contrast enhancement of medical images.	Real-time edge-aware image processing with the bilateral grid	NA:NA:NA	2018
Adrien Bousseau:Fabrice Neyret:Joëlle Thollot:David Salesin	In this paper, we present a method for creating watercolor-like animation, starting from video as input. The method involves two main steps: applying textures that simulate a watercolor appearance; and creating a simplified, abstracted version of the video to which the texturing operations are applied. Both of these steps are subject to highly visible temporal artifacts, so the primary technical contributions of the paper are extensions of previous methods for texturing and abstraction to provide temporal coherence when applied to video sequences. To maintain coherence for textures, we employ texture advection along lines of optical flow. We furthermore extend previous approaches by incorporating advection in both forward and reverse directions through the video, which allows for minimal texture distortion, particularly in areas of disocclusion that are otherwise highly problematic. To maintain coherence for abstraction, we employ mathematical morphology extended to the temporal domain, using filters whose temporal extents are locally controlled by the degree of distortions in the optical flow. Together, these techniques provide the first practical and robust approach for producing watercolor animations from video, which we demonstrate with a number of examples.	Video watercolorization using bidirectional texture advection	NA:NA:NA:NA	2018
Okan Arikan	NA	Session details: Character animation II	NA	2018
KangKang Yin:Kevin Loken:Michiel van de Panne	Physics-based simulation and control of biped locomotion is difficult because bipeds are unstable, underactuated, high-dimensional dynamical systems. We develop a simple control strategy that can be used to generate a large variety of gaits and styles in real-time, including walking in all directions (forwards, backwards, sideways, turning), running, skipping, and hopping. Controllers can be authored using a small number of parameters, or their construction can be informed by motion capture data. The controllers are applied to 2D and 3D physically-simulated character models. Their robustness is demonstrated with respect to pushes in all directions, unexpected steps and slopes, and unexpected variations in kinematic and dynamic parameters. Direct transitions between controllers are demonstrated as well as parameterized control of changes in direction and speed. Feedback-error learning is applied to learn predictive torque models, which allows for the low-gain control that typifies many natural motions as well as producing smoother simulated motion.	SIMBICON: simple biped locomotion control	NA:NA:NA	2018
Alla Safonova:Jessica K. Hodgins	Many compelling applications would become feasible if novice users had the ability to synthesize high quality human motion based only on a simple sketch and a few easily specified constraints. We approach this problem by representing the desired motion as an interpolation of two time-scaled paths through a motion graph. The graph is constructed to support interpolation and pruned for efficient search. We use an anytime version of A* search to find a globally optimal solution in this graph that satisfies the user's specification. Our approach retains the natural transitions of motion graphs and the ability to synthesize physically realistic variations provided by interpolation. We demonstrate the power of this approach by synthesizing optimal or near optimal motions that include a variety of behaviors in a single motion.	Construction and optimal search of interpolated motion graphs	NA:NA	2018
Kwang Won Sok:Manmyung Kim:Jehee Lee	Physically based simulation of human motions is an important issue in the context of computer animation, robotics and biomechanics. We present a new technique for allowing our physically-simulated planar biped characters to imitate human behaviors. Our contribution is twofold. We developed an optimization method that transforms any (either motion-captured or kinematically synthesized) biped motion into a physically-feasible, balance-maintaining simulated motion. Our optimization method allows us to collect a rich set of training data that contains stylistic, personality-rich human behaviors. Our controller learning algorithm facilitates the creation and composition of robust dynamic controllers that are learned from training data. We demonstrate a planar articulated character that is dynamically simulated in real time, equipped with an integrated repertoire of motor skills, and controlled interactively to perform desired motions.	Simulating biped behaviors from human motion data	NA:NA:NA	2018
Aseem Agarwala	NA	Session details: Image collections and video	NA	2018
Huamin Wang:Yonatan Wexler:Eyal Ofek:Hugues Hoppe	We reduce transmission bandwidth and memory space for images by factoring their repeated content. A transform map and a condensed epitome are created such that all image blocks can be reconstructed from transformed epitome patches. The transforms may include affine deformation and color scaling to account for perspective and tonal variations across the image. The factored representation allows efficient random-access through a simple indirection, and can therefore be used for real-time texture mapping without expansion in memory. Our scheme is orthogonal to traditional image compression, in the sense that the epitome is amenable to further compression such as DXT. Moreover it allows a new mode of progressivity, whereby generic features appear before unique detail. Factoring is also effective across a collection of images, particularly in the context of image-based rendering. Eliminating redundant content lets us include textures that are several times as large in the same memory space.	Factoring repeated content within and among images	NA:NA:NA:NA	2018
Noah Snavely:Rahul Garg:Steven M. Seitz:Richard Szeliski	When a scene is photographed many times by different people, the viewpoints often cluster along certain paths. These paths are largely specific to the scene being photographed, and follow interesting regions and viewpoints. We seek to discover a range of such paths and turn them into controls for image-based rendering. Our approach takes as input a large set of community or personal photos, reconstructs camera viewpoints, and automatically computes orbits, panoramas, canonical views, and optimal paths between views. The scene can then be interactively browsed in 3D using these controls or with six degree-of-freedom free-viewpoint control. As the user browses the scene, nearby views are continuously selected and transformed, using control-adaptive reprojection techniques.	Finding paths through the world's photos	NA:NA:NA:NA	2018
Michael Rubinstein:Ariel Shamir:Shai Avidan	Video, like images, should support content aware resizing. We present video retargeting using an improved seam carving operator. Instead of removing 1D seams from 2D images we remove 2D seam manifolds from 3D space-time volumes. To achieve this we replace the dynamic programming method of seam carving with graph cuts that are suitable for 3D volumes. In the new formulation, a seam is given by a minimal cut in the graph and we show how to construct a graph such that the resulting cut is a valid seam. That is, the cut is monotonic and connected. In addition, we present a novel energy criterion that improves the visual quality of the retargeted images and videos. The original seam carving operator is focused on removing seams with the least amount of energy, ignoring energy that is introduced into the images and video by applying the operator. To counter this, the new criterion is looking forward in time - removing seams that introduce the least amount of energy into the retargeted result. We show how to encode the improved criterion into graph cuts (for images and video) as well as dynamic programming (for images). We apply our technique to images and videos and present results of various applications.	Improved seam carving for video retargeting	NA:NA:NA	2018
Alex Rav-Acha:Pushmeet Kohli:Carsten Rother:Andrew Fitzgibbon	We introduce a new representation for video which facilitates a number of common editing tasks. The representation has some of the power of a full reconstruction of 3D surface models from video, but is designed to be easy to recover from a priori unseen and uncalibrated footage. By modelling the image-formation process as a 2D-to-2D transformation from an object's texture map to the image, modulated by an object-space occlusion mask, we can recover a representation which we term the "unwrap mosaic". Many editing operations can be performed on the unwrap mosaic, and then re-composited into the original sequence, for example resizing objects, repainting textures, copying/cutting/pasting objects, and attaching effects layers to deforming objects.	Unwrap mosaics: a new representation for video editing	NA:NA:NA:NA	2018
Marc Olano	NA	Session details: Parallelism	NA	2018
Larry Seiler:Doug Carmean:Eric Sprangle:Tom Forsyth:Michael Abrash:Pradeep Dubey:Stephen Junkins:Adam Lake:Jeremy Sugerman:Robert Cavin:Roger Espasa:Ed Grochowski:Toni Juan:Pat Hanrahan	This paper presents a many-core visual computing architecture code named Larrabee, a new software rendering pipeline, a manycore programming model, and performance analysis for several applications. Larrabee uses multiple in-order x86 CPU cores that are augmented by a wide vector processor unit, as well as some fixed function logic blocks. This provides dramatically higher performance per watt and per unit of area than out-of-order CPUs on highly parallel workloads. It also greatly increases the flexibility and programmability of the architecture as compared to standard GPUs. A coherent on-die 2nd level cache allows efficient inter-processor communication and high-bandwidth local data access by CPU cores. Task scheduling is performed entirely with software in Larrabee, rather than in fixed function logic. The customizable software graphics rendering pipeline for this architecture uses binning in order to reduce required memory bandwidth, minimize lock contention, and increase opportunities for parallelism relative to standard GPUs. The Larrabee native programming model supports a variety of highly parallel applications that use irregular data structures. Performance analysis on those applications demonstrates Larrabee's potential for a broad range of parallel computation.	Larrabee: a many-core x86 architecture for visual computing	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Qiming Hou:Kun Zhou:Baining Guo	We present BSGP, a new programming language for general purpose computation on the GPU. A BSGP program looks much the same as a sequential C program. Programmers only need to supply a bare minimum of extra information to describe parallel processing on GPUs. As a result, BSGP programs are easy to read, write, and maintain. Moreover, the ease of programming does not come at the cost of performance. A well-designed BSGP compiler converts BSGP programs to kernels and combines them using optimally allocated temporary streams. In our benchmark, BSGP programs achieve similar or better performance than well-optimized CUDA programs, while the source code complexity and programming time are significantly reduced. To test BSGP's code efficiency and ease of programming, we implemented a variety of GPU applications, including a highly sophisticated X3D parser that would be extremely difficult to develop with existing GPU programming languages.	BSGP: bulk-synchronous GPU programming	NA:NA:NA	2018
Li-Yi Wei	Sampling is important for a variety of graphics applications include rendering, imaging, and geometry processing. However, producing sample sets with desired efficiency and blue noise statistics has been a major challenge, as existing methods are either sequential with limited speed, or are parallel but only through pre-computed datasets and thus fall short in producing samples with blue noise statistics. We present a Poisson disk sampling algorithm that runs in parallel and produces all samples on the fly with desired blue noise properties. Our main idea is to subdivide the sample domain into grid cells and we draw samples concurrently from multiple cells that are sufficiently far apart so that their samples cannot conflict one another. We present a parallel implementation of our algorithm running on a GPU with constant cost per sample and constant number of computation passes for a target number of samples. Our algorithm also works in arbitrary dimension, and allows adaptive sampling from a user-specified importance field. Furthermore, our algorithm is simple and easy to implement, and runs faster than existing techniques.	Parallel Poisson disk sampling	NA	2018
Michael Kazhdan:Hugues Hoppe	We introduce a new tool to solve the large linear systems arising from gradient-domain image processing. Specifically, we develop a streaming multigrid solver, which needs just two sequential passes over out-of-core data. This fast solution is enabled by a combination of three techniques: (1) use of second-order finite elements (rather than traditional finite differences) to reach sufficient accuracy in a single V-cycle, (2) temporally blocked relaxation, and (3) multi-level streaming to pipeline the restriction and prolongation phases into single streaming passes. A key contribution is the extension of the B-spline finite-element method to be compatible with the forward-difference gradient representation commonly used with images. Our streaming solver is also efficient for in-memory images, due to its fast convergence and excellent cache behavior. Remarkably, it can outperform spatially adaptive solvers that exploit application-specific knowledge. We demonstrate seamless stitching and tone-mapping of gigapixel images in about an hour on a notebook PC.	Streaming multigrid for gradient-domain operations on large images	NA:NA	2018
Miguel Otaduy	NA	Session details: Noisy collisions	NA	2018
Sung-Hee Lee:Demetri Terzopoulos	Spline joints are a novel class of joints that can model general scleronomic constraints for multibody dynamics based on the minimal-coordinates formulation. The main idea is to introduce spline curves and surfaces in the modeling of joints: We model 1-DOF joints using splines on SE(3), and construct multi-DOF joints as the product of exponentials of splines in Euclidean space. We present efficient recursive algorithms to compute the derivatives of the spline joint, as well as geometric algorithms to determine optimal parameters in order to achieve the desired joint motion. Our spline joints can be used to create interesting new simulated mechanisms for computer animation and they can more accurately model complex biomechanical joints such as the knee and shoulder.	Spline joints for multibody dynamics	NA:NA	2018
David Harmon:Etienne Vouga:Rasmus Tamstorf:Eitan Grinspun	Robust treatment of complex collisions is a challenging problem in cloth simulation. Some state of the art methods resolve collisions iteratively, invoking a fail-safe when a bound on iteration count is exceeded. The best-known fail-safe rigidifies the contact region, causing simulation artifacts. We present a fail-safe that cancels impact but not sliding motion, considerably reducing artificial dissipation. We equip the proposed fail-safe with an approximation of Coulomb friction, allowing finer control of sliding dissipation.	Robust treatment of simultaneous collisions	NA:NA:NA:NA	2018
Nicolas Bonneel:George Drettakis:Nicolas Tsingos:Isabelle Viaud-Delmon:Doug James	Audio rendering of impact sounds, such as those caused by falling objects or explosion debris, adds realism to interactive 3D audiovisual applications, and can be convincingly achieved using modal sound synthesis. Unfortunately, mode-based computations can become prohibitively expensive when many objects, each with many modes, are impacted simultaneously. We introduce a fast sound synthesis approach, based on short-time Fourier Tranforms, that exploits the inherent sparsity of modal sounds in the frequency domain. For our test scenes, this "fast mode summation" can give speedups of 5--8 times compared to a time-domain solution, with slight degradation in quality. We discuss different reconstruction windows, affecting the quality of impact sound "attacks". Our Fourier-domain processing method allows us to introduce a scalable, real-time, audio processing pipeline for both recorded and modal sounds, with auditory masking and sound source clustering. To avoid abrupt computation peaks, such as during the simultaneous impacts of an explosion, we use crossmodal perception results on audiovisual synchrony to effect temporal scheduling. We also conducted a pilot perceptual user evaluation of our method. Our implementation results show that we can treat complex audiovisual scenes in real time with high quality.	Fast modal sounds with scalable frequency-domain synthesis	NA:NA:NA:NA:NA	2018
Christopher D. Twigg:Doug L. James	Physically based simulation of rigid body dynamics is commonly done by time-stepping systems forward in time. In this paper, we propose methods to allow time-stepping rigid body systems back-ward in time. Unfortunately, reverse-time integration of rigid bodies involving frictional contact is mathematically ill-posed, and can lack unique solutions. We instead propose time-reversed rigid body integrators that can sample possible solutions when unique ones do not exist. We also discuss challenges related to dissipation-related energy gain, sensitivity to initial conditions, stacking, constraints and articulation, rolling, sliding, skidding, bouncing, high angular velocities, rapid velocity growth from micro-collisions, and other problems encountered when going against the usual flow of time.	Backward steps in rigid body simulation	NA:NA	2018
Karen Liu	NA	Session details: Characters	NA	2018
Rachel McDonnell:Michéal Larkin:Simon Dobbyn:Steven Collins:Carol O'Sullivan	When simulating large crowds, it is inevitable that the models and motions of many virtual characters will be cloned. However, the perceptual impact of this trade-off has never been studied. In this paper, we consider the ways in which an impression of variety can be created and the perceptual consequences of certain design choices. In a series of experiments designed to test people's perception of variety in crowds, we found that clones of appearance are far easier to detect than motion clones. Furthermore, we established that cloned models can be masked by color variation, random orientation, and motion. Conversely, the perception of cloned motions remains unaffected by the model on which they are displayed. Other factors that influence the ability to detect clones were examined, such as proximity, model type and characteristic motion. Our results provide novel insights and useful thresholds that will assist in creating more realistic, heterogeneous crowds.	Clone attack! Perception of crowd variety	NA:NA:NA:NA:NA	2018
Chris Hecker:Bernd Raabe:Ryan W. Enslow:John DeWeese:Jordan Maynard:Kees van Prooijen	Character animation in video games---whether manually keyframed or motion captured---has traditionally relied on codifying skeletons early in a game's development, and creating animations rigidly tied to these fixed skeleton morphologies. This paper introduces a novel system for animating characters whose morphologies are unknown at the time the animation is created. Our authoring tool allows animators to describe motion using familiar posing and key-framing methods. The system records the data in a morphology-independent form, preserving both the animation's structural relationships and its stylistic information. At runtime, the generalized data are applied to specific characters to yield pose goals that are supplied to a robust and efficient inverse kinematics solver. This system allows us to animate characters with highly varying skeleton morphologies that did not exist when the animation was authored, and, indeed, may be radically different than anything the original animator envisioned.	Real-time motion retargeting to highly varied user-created morphologies	NA:NA:NA:NA:NA:NA	2018
Michael Kass:John Anderson	Oscillatory motion is ubiquitous in computer graphics, yet existing animation techniques are ill-suited to its authoring. We introduce a new type of spline for this purpose, known as a "Wiggly Spline." The spline generalizes traditional piecewise cubics when its resonance and damping are set to zero, but creates oscillatory animation when its resonance and damping are changed. The spline provides a combination of direct manipulation and physical realism. To create overlapped and propagating motion, we generate phase shifts of the Wiggly Spline, and use these to control appropriate degrees of freedom in a model. The phase shifts can be created directly by procedural techniques or through a paint-like interface. A further option is to derive the phase shifts statistically by analyzing a time-series of a simulation. In this case, the Wiggly Spline makes it possible to canonicalize a simulation, generalize it by providing frequency and damping controls and control it through direct manipulation.	Animating oscillatory motion with overlap: wiggly splines	NA:NA	2018
Xiaohan Shi:Kun Zhou:Yiying Tong:Mathieu Desbrun:Hujun Bao:Baining Guo	In this paper we present an approach to enrich skeleton-driven animations with physically-based secondary deformation in real time. To achieve this goal, we propose a novel, surface-based deformable model that can interactively emulate the dynamics of both low-and high-frequency volumetric effects. Given a surface mesh and a few sample sequences of its physical behavior, a set of motion parameters of the material are learned during an off-line preprocessing step. The deformable model is then applicable to any given skeleton-driven animation of the surface mesh. Additionally, our dynamic skinning technique can be entirely implemented on GPUs and executed with great efficiency. Thus, with minimal changes to the conventional graphics pipeline, our approach can drastically enhance the visual experience of skeleton-driven animations by adding secondary deformation in real time.	Example-based dynamic skinning in real time	NA:NA:NA:NA:NA:NA	2018
Bruce Walter	NA	Session details: Hair and realistic rendering	NA	2018
Sylvain Paris:Will Chang:Oleg I. Kozhushnyan:Wojciech Jarosz:Wojciech Matusik:Matthias Zwicker:Frédo Durand	We accurately capture the shape and appearance of a person's hairstyle. We use triangulation and a sweep with planes of light for the geometry. Multiple projectors and cameras address the challenges raised by the reflectance and intricate geometry of hair. We introduce the use of structure tensors to infer the hidden geometry between the hair surface and the scalp. Our triangulation approach affords substantial accuracy improvement and we are able to measure elaborate hair geometry including complex curls and concavities. To reproduce the hair appearance, we capture a six-dimensional reflectance field. We introduce a new reflectance interpolation technique that leverages an analytical reflectance model to alleviate cross-fading artifacts caused by linear methods. Our results closely match the real hairstyles and can be used for animation.	Hair photobooth: geometric and photometric acquisition of real hairstyles	NA:NA:NA:NA:NA:NA:NA	2018
Jonathan T. Moon:Bruce Walter:Steve Marschner	Previous research has shown that a global multiple scattering simulation is needed to achieve physically realistic renderings of hair, particularly light-colored hair with low absorption. However, previous methods have either sacrificed accuracy or have been too computationally expensive for practical use. In this paper we describe a physically based, volumetric rendering method that computes multiple scattering solutions, including directional effects, much faster than previous accurate methods. Our two-pass method first traces light paths through a volumetric representation of the hair, contributing power to a 3D grid of spherical harmonic coefficients that store the directional distribution of scattered radiance everywhere in the hair volume. Then, in a ray tracing pass, multiple scattering is computed by integrating the stored radiance against the scattering functions of visible fibers using an efficient matrix multiplication. Single scattering is computed using conventional direct illumination methods. In our comparisons the new method produces quality similar to that of the best previous methods, but computes multiple scattering more than 10 times faster.	Efficient multiple scattering in hair using spherical harmonics	NA:NA:NA	2018
Arno Zinke:Cem Yuksel:Andreas Weber:John Keyser	When rendering light colored hair, multiple fiber scattering is essential for the right perception of the overall hair color. In this context, we present a novel technique to efficiently approximate multiple fiber scattering for a full head of human hair or a similar fiber based geometry. In contrast to previous ad-hoc approaches, our method relies on the physically accurate concept of the Bidirectional Scattering Distribution Functions and gives physically plausible results with no need for parameter tweaking. We show that complex scattering effects can be approximated very well by using aggressive simplifications based on this theoretical model. When compared to unbiased Monte-Carlo path tracing, our approximations preserve photo-realism in most settings but with rendering times at least two-orders of magnitude lower. Time and space complexity are much lower compared to photon mapping-based techniques and we can even achieve realistic results in real-time on a standard PC with consumer graphics hardware.	Dual scattering approximation for fast multiple scattering in hair	NA:NA:NA:NA	2018
Toshiya Hachisuka:Wojciech Jarosz:Richard Peter Weistroffer:Kevin Dale:Greg Humphreys:Matthias Zwicker:Henrik Wann Jensen	We present a new adaptive sampling strategy for ray tracing. Our technique is specifically designed to handle multidimensional sample domains, and it is well suited for efficiently generating images with effects such as soft shadows, motion blur, and depth of field. These effects are problematic for existing image based adaptive sampling techniques as they operate on pixels, which are possibly noisy results of a Monte Carlo ray tracing process. Our sampling technique operates on samples in the multidimensional space given by the rendering equation and as a consequence the value of each sample is noise-free. Our algorithm consists of two passes. In the first pass we adaptively generate samples in the multidimensional space, focusing on regions where the local contrast between samples is high. In the second pass we reconstruct the image by integrating the multidimensional function along all but the image dimensions. We perform a high quality anisotropic reconstruction by determining the extent of each sample in the multidimensional space using a structure tensor. We demonstrate our method on scenes with a 3 to 5 dimensional space, including soft shadows, motion blur, and depth of field. The results show that our method uses fewer samples than Mittchell's adaptive sampling technique while producing images with less noise.	Multidimensional adaptive sampling and reconstruction for ray tracing	NA:NA:NA:NA:NA:NA:NA	2018
Sumanta Pattanaik	NA	Session details: Real time rendering (mostly)	NA	2018
Thomas Annen:Zhao Dong:Tom Mertens:Philippe Bekaert:Hans-Peter Seidel:Jan Kautz	Shadow computation in dynamic scenes under complex illumination is a challenging problem. Methods based on precomputation provide accurate, real-time solutions, but are hard to extend to dynamic scenes. Specialized approaches for soft shadows can deal with dynamic objects but are not fast enough to handle more than one light source. In this paper, we present a technique for rendering dynamic objects under arbitrary environment illumination, which does not require any precomputation. The key ingredient is a fast, approximate technique for computing soft shadows, which achieves several hundred frames per second for a single light source. This allows for approximating environment illumination with a sparse collection of area light sources and yields real-time frame rates.	Real-time, all-frequency shadows in dynamic scenes	NA:NA:NA:NA:NA:NA	2018
Xin Sun:Kun Zhou:Eric Stollnitz:Jiaoying Shi:Baining Guo	We present a new technique for interactive relighting of dynamic refractive objects with complex material properties. We describe our technique in terms of a rendering pipeline in which each stage runs entirely on the GPU. The rendering pipeline converts surfaces to volumetric data, traces the curved paths of photons as they refract through the volume, and renders arbitrary views of the resulting radiance distribution. Our rendering pipeline is fast enough to permit interactive updates to lighting, materials, geometry, and viewing parameters without any precomputation. Applications of our technique include the visualization of caustics, absorption, and scattering while running physical simulations or while manipulating surfaces in real time.	Interactive relighting of dynamic refractive objects	NA:NA:NA:NA:NA	2018
Kun Zhou:Zhong Ren:Stephen Lin:Hujun Bao:Baining Guo:Heung-Yeung Shum	We present a real-time algorithm called compensated ray marching for rendering of smoke under dynamic low-frequency environment lighting. Our approach is based on a decomposition of the input smoke animation, represented as a sequence of volumetric density fields, into a set of radial basis functions (RBFs) and a sequence of residual fields. To expedite rendering, the source radiance distribution within the smoke is computed from only the low-frequency RBF approximation of the density fields, since the high-frequency residuals have little impact on global illumination under low-frequency environment lighting. Furthermore, in computing source radiances the contributions from single and multiple scattering are evaluated at only the RBF centers and then approximated at other points in the volume using an RBF-based interpolation. A slice-based integration of these source radiances along each view ray is then performed to render the final image. The high-frequency residual fields, which are a critical component in the local appearance of smoke, are compensated back into the radiance integral during this ray march to generate images of high detail. The runtime algorithm, which includes both light transfer simulation and ray marching, can be easily implemented on the GPU, and thus allows for real-time manipulation of viewpoint and lighting, as well as interactive editing of smoke attributes such as extinction cross section, scattering albedo, and phase function. Only moderate preprocessing time and storage is needed. This approach provides the first method for real-time smoke rendering that includes single and multiple scattering while generating results comparable in quality to offline algorithms like ray tracing.	Real-time smoke rendering using compensated ray marching	NA:NA:NA:NA:NA:NA	2018
Jaakko Lehtinen:Matthias Zwicker:Emmanuel Turquin:Janne Kontkanen:Frédo Durand:François X. Sillion:Timo Aila	We introduce a meshless hierarchical representation for solving light transport problems. Precomputed radiance transfer (PRT) and finite elements require a discrete representation of illumination over the scene. Non-hierarchical approaches such as per-vertex values are simple to implement, but lead to long precomputation. Hierarchical bases like wavelets lead to dramatic acceleration, but in their basic form they work well only on flat or smooth surfaces. We introduce a hierarchical function basis induced by scattered data approximation. It is decoupled from the geometric representation, allowing the hierarchical representation of illumination on complex objects. We present simple data structures and algorithms for constructing and evaluating the basis functions. Due to its hierarchical nature, our representation adapts to the complexity of the illumination, and can be queried at different scales. We demonstrate the power of the new basis in a novel precomputed direct-to-indirect light transport algorithm that greatly increases the complexity of scenes that can be handled by PRT approaches.	A meshless hierarchical representation for light transport	NA:NA:NA:NA:NA:NA:NA	2018
Jason Lawrence	NA	Session details: Faces & reflectance	NA	2018
Tommer Leyvand:Daniel Cohen-Or:Gideon Dror:Dani Lischinski	When human raters are presented with a collection of shapes and asked to rank them according to their aesthetic appeal, the results often indicate that there is a statistical consensus among the raters. Yet it might be difficult to define a succinct set of rules that capture the aesthetic preferences of the raters. In this work, we explore a data-driven approach to aesthetic enhancement of such shapes. Specifically, we focus on the challenging problem of enhancing the aesthetic appeal (or the attractiveness) of human faces in frontal photographs (portraits), while maintaining close similarity with the original. The key component in our approach is an automatic facial attractiveness engine trained on datasets of faces with accompanying facial attractiveness ratings collected from groups of human raters. Given a new face, we extract a set of distances between a variety of facial feature locations, which define a point in a high-dimensional "face space". We then search the face space for a nearby point with a higher predicted attractiveness rating. Once such a point is found, the corresponding facial distances are embedded in the plane and serve as a target to define a 2D warp field which maps the original facial features to their adjusted locations. The effectiveness of our technique was experimentally validated by independent rating experiments, which indicate that it is indeed capable of increasing the facial attractiveness of most portraits that we have experimented with.	Data-driven enhancement of facial attractiveness	NA:NA:NA:NA	2018
Dmitri Bitouk:Neeraj Kumar:Samreen Dhillon:Peter Belhumeur:Shree K. Nayar	In this paper, we present a complete system for automatic face replacement in images. Our system uses a large library of face images created automatically by downloading images from the internet, extracting faces using face detection software, and aligning each extracted face to a common coordinate system. This library is constructed off-line, once, and can be efficiently accessed during face replacement. Our replacement algorithm has three main stages. First, given an input image, we detect all faces that are present, align them to the coordinate system used by our face library, and select candidate face images from our face library that are similar to the input face in appearance and pose. Second, we adjust the pose, lighting, and color of the candidate face images to match the appearance of those in the input image, and seamlessly blend in the results. Third, we rank the blended candidate replacements by computing a match distance over the overlap region. Our approach requires no 3D model, is fully automatic, and generates highly plausible results across a wide range of skin tones, lighting conditions, and viewpoints. We show how our approach can be used for a variety of applications including face de-identification and the creation of appealing group photographs from a set of images. We conclude with a user study that validates the high quality of our replacement results, and a discussion on the current limitations of our system.	Face swapping: automatically replacing faces in photographs	NA:NA:NA:NA:NA	2018
Xiaobo An:Fabio Pellacini	We present an intuitive and efficient method for editing the appearance of complex spatially-varying datasets, such as images and measured materials. In our framework, users specify rough adjustments that are refined interactively by enforcing the policy that similar edits are applied to spatially-close regions of similar appearance. Rather than proposing a specific user interface, our method allows artists to quickly and imprecisely specify the initial edits with any method or workflow they feel most comfortable with. An energy optimization formulation is used to propagate the initial rough adjustments to the final refined ones by enforcing the editing policy over all pairs of points in the dataset. We show that this formulation is equivalent to solving a large linear system defined by a dense matrix. We derive an approximate algorithm to compute such a solution interactively by taking advantage of the inherent structure of the matrix. We demonstrate our approach by editing images, HDR radiance maps, and measured materials. Finally, we show that our framework generalizes prior methods while providing significant improvements in generality, robustness and efficiency.	AppProp: all-pairs appearance-space edit propagation	NA:NA	2018
Jiaping Wang:Shuang Zhao:Xin Tong:John Snyder:Baining Guo	We present a new technique for the visual modeling of spatiallyvarying anisotropic reflectance using data captured from a single view. Reflectance is represented using a microfacet-based BRDF which tabulates the facets' normal distribution (NDF) as a function of surface location. Data from a single view provides a 2D slice of the 4D BRDF at each surface point from which we fit a partial NDF. The fitted NDF is partial because the single view direction coupled with the set of light directions covers only a portion of the "half-angle" hemisphere. We complete the NDF at each point by applying a novel variant of texture synthesis using similar, overlapping partial NDFs from other points. Our similarity measure allows azimuthal rotation of partial NDFs, under the assumption that reflectance is spatially redundant but the local frame may be arbitrarily oriented. Our system includes a simple acquisition device that collects images over a 2D set of light directions by scanning a linear array of LEDs over a flat sample. Results demonstrate that our approach preserves spatial and directional BRDF details and generates a visually compelling match to measured materials.	Modeling anisotropic surface reflectance with example-based microfacet synthesis	NA:NA:NA:NA:NA	2018
Eitan Grinspun	NA	Session details: Shape analysis	NA	2018
Hongbo Fu:Daniel Cohen-Or:Gideon Dror:Alla Sheffer	Humans usually associate an upright orientation with objects, placing them in a way that they are most commonly seen in our surroundings. While it is an open challenge to recover the functionality of a shape from its geometry alone, this paper shows that it is often possible to infer its upright orientation by analyzing its geometry. Our key idea is to reduce the two-dimensional (spherical) orientation space to a small set of orientation candidates using functionality-related geometric properties of the object, and then determine the best orientation using an assessment function of several functional geometric attributes defined with respect to each candidate. Specifically we focus on obtaining the upright orientation for man-made objects that typically stand on some flat surface (ground, floor, table, etc.), which include the vast majority of objects in our everyday surroundings. For these types of models orientation candidates can be defined according to static equilibrium. For each candidate, we introduce a set of discriminative attributes linking shape to function. We learn an assessment function of these attributes from a training set using a combination of Random Forest classifier and Support Vector Machine classifier. Experiments demonstrate that our method generalizes well and achieves about 90% prediction accuracy for both a 10-fold cross-validation over the training set and a validation with an independent test set.	Upright orientation of man-made objects	NA:NA:NA:NA	2018
Mark Pauly:Niloy J. Mitra:Johannes Wallner:Helmut Pottmann:Leonidas J. Guibas	We introduce a computational framework for discovering regular or repeated geometric structures in 3D shapes. We describe and classify possible regular structures and present an effective algorithm for detecting such repeated geometric patterns in point- or meshbased models. Our method assumes no prior knowledge of the geometry or spatial location of the individual elements that define the pattern. Structure discovery is made possible by a careful analysis of pairwise similarity transformations that reveals prominent lattice structures in a suitable model of transformation space. We introduce an optimization method for detecting such uniform grids specifically designed to deal with outliers and missing elements. This yields a robust algorithm that successfully discovers complex regular structures amidst clutter, noise, and missing geometry. The accuracy of the extracted generating transformations is further improved using a novel simultaneous registration method in the spatial domain. We demonstrate the effectiveness of our algorithm on a variety of examples and show applications to compression, model repair, and geometry synthesis.	Discovering structural regularity in 3D geometry	NA:NA:NA:NA:NA	2018
Oscar Kin-Chung Au:Chiew-Lan Tai:Hung-Kuo Chu:Daniel Cohen-Or:Tong-Yee Lee	Extraction of curve-skeletons is a fundamental problem with many applications in computer graphics and visualization. In this paper, we present a simple and robust skeleton extraction method based on mesh contraction. The method works directly on the mesh domain, without pre-sampling the mesh model into a volumetric representation. The method first contracts the mesh geometry into zero-volume skeletal shape by applying implicit Laplacian smoothing with global positional constraints. The contraction does not alter the mesh connectivity and retains the key features of the original mesh. The contracted mesh is then converted into a 1D curve-skeleton through a connectivity surgery process to remove all the collapsed faces while preserving the shape of the contracted mesh and the original topology. The centeredness of the skeleton is refined by exploiting the induced skeleton-mesh mapping. In addition to producing a curve skeleton, the method generates other valuable information about the object's geometry, in particular, the skeleton-vertex correspondence and the local thickness, which are useful for various applications. We demonstrate its effectiveness in mesh segmentation and skinning animation.	Skeleton extraction by mesh contraction	NA:NA:NA:NA:NA	2018
Tamal K. Dey:Kuiyu Li:Jian Sun:David Cohen-Steiner	Many applications such as topology repair, model editing, surface parameterization, and feature recognition benefit from computing loops on surfaces that wrap around their 'handles' and 'tunnels'. Computing such loops while optimizing their geometric lengths is difficult. On the other hand, computing such loops without considering geometry is easy but may not be very useful. In this paper we strike a balance by computing topologically correct loops that are also geometrically relevant. Our algorithm is a novel application of the concepts from topological persistence introduced recently in computational topology. The usability of the computed loops is demonstrated with some examples in feature identification and topology simplification.	Computing geometry-aware handle and tunnel loops in 3D models	NA:NA:NA:NA	2018
Adam Bargteil	NA	Session details: Jiggly fluids	NA	2018
Avi Robinson-Mosher:Tamar Shinar:Jon Gretarsson:Jonathan Su:Ronald Fedkiw	We propose a novel solid/fluid coupling method that treats the coupled system in a fully implicit manner making it stable for arbitrary time steps, large density ratios, etc. In contrast to previous work in computer graphics, we derive our method using a simple back-of-the-envelope approach which lumps the solid and fluid momenta together, and which we show exactly conserves the momentum of the coupled system. Notably, our method uses the standard Cartesian fluid discretization and does not require (moving) conforming tetrahedral meshes or ALE frameworks. Furthermore, we use a standard Lagrangian framework for the solid, thus supporting arbitrary solid constitutive models, both implicit and explicit time integration, etc. The method is quite general, working for smoke, water, and multiphase fluids as well as both rigid and deformable solids, and both volumes and thin shells. Rigid shells and cloth are handled automatically without special treatment, and we support fully one-sided discretizations without leaking. Our equations are fully symmetric, allowing for the use of fast solvers, which is a natural result of properly conserving momentum. Finally, for simple explicit time integration of rigid bodies, we show that our equations reduce to form similar to previous work via a single block Gaussian elimination operation, but that this approach scales poorly, i.e. as though four spatial dimensions rather than three.	Two-way coupling of fluids to rigid and deformable solids and shells	NA:NA:NA:NA:NA	2018
Chris Wojtan:Greg Turk	We introduce a method for efficiently animating a wide range of deformable materials. We combine a high resolution surface mesh with a tetrahedral finite element simulator that makes use of frequent re-meshing. This combination allows for fast and detailed simulations of complex elastic and plastic behavior. We significantly expand the range of physical parameters that can be simulated with a single technique, and the results are free from common artifacts such as volume-loss, smoothing, popping, and the absence of thin features like strands and sheets. Our decision to couple a high resolution surface with low-resolution physics leads to efficient simulation and detailed surface features, and our approach to creating the tetrahedral mesh leads to an order-of-magnitude speedup over previous techniques in the time spent re-meshing. We compute masses, collisions, and surface tension forces on the scale of the fine mesh, which helps avoid visual artifacts due to the differing mesh resolutions. The result is a method that can simulate a large array of different material behaviors with high resolution features in a short amount of time.	Fast viscoelastic behavior with thin features	NA:NA	2018
Jeong-Mo Hong:Ho-Young Lee:Jong-Chul Yoon:Chang-Hun Kim	We propose a hybrid method for simulating multiphase fluids such as bubbly water. The appearance of subgrid visual details is improved by incorporating a new bubble model based on smoothed particle hydrodynamics (SPH) into an Eulerian grid-based simulation that handles background flows of large bodies of water and air. To overcome the difficulty in simulating small bubbles in the context of the multiphase flows on a coarse grid, we heuristically model the interphase properties of water and air by means of the interactions between bubble particles. As a result, we can animate lively motion of bubbly water with small scale details efficiently.	Bubbles alive	NA:NA:NA:NA	2018
Toon Lenaerts:Bart Adams:Philip Dutré	This paper presents the simulation of a fluid flowing through a porous deformable material. We introduce the physical principles governing porous flow, expressed by the Law of Darcy, into the Smoothed Particle Hydrodynamics (SPH) framework for simulating fluids and deformable objects. Contrary to previous SPH approaches, we simulate porous flow at a macroscopic scale, making abstraction of individual pores or cavities inside the material. Thus, the number of computational elements is kept low, while at the same time realistic simulations can be achieved. Our algorithm models the changing behavior of the wet material as well as the full two-way coupling between the fluid and the porous material. This enables various new effects, such as the simulation of sponge-like elastic bodies and water-absorbing sticky cloth.	Porous flow in particle-based fluid simulations	NA:NA:NA	2018
Theodore Kim:Nils Thürey:Doug James:Markus Gross	We present a novel wavelet method for the simulation of fluids at high spatial resolution. The algorithm enables large- and small-scale detail to be edited separately, allowing high-resolution detail to be added as a post-processing step. Instead of solving the Navier-Stokes equations over a highly refined mesh, we use the wavelet decomposition of a low-resolution simulation to determine the location and energy characteristics of missing high-frequency components. We then synthesize these missing components using a novel incompressible turbulence function, and provide a method to maintain the temporal coherence of the resulting structures. There is no linear system to solve, so the method parallelizes trivially and requires only a few auxiliary arrays. The method guarantees that the new frequencies will not interfere with existing frequencies, allowing animators to set up a low resolution simulation quickly and later add details without changing the overall fluid motion.	Wavelet turbulence for fluid simulation	NA:NA:NA:NA	2018
Yizhou Yu	NA	Session details: Texture	NA	2018
Charles Han:Eric Risser:Ravi Ramamoorthi:Eitan Grinspun	Example-based texture synthesis algorithms have gained widespread popularity for their ability to take a single input image and create a perceptually similar non-periodic texture. However, previous methods rely on single input exemplars that can capture only a limited band of spatial scales. For example, synthesizing a continent-like appearance at a variety of zoom levels would require an impractically high input resolution. In this paper, we develop a multiscale texture synthesis algorithm. We propose a novel example-based representation, which we call an exemplar graph, that simply requires a few low-resolution input exemplars at different scales. Moreover, by allowing loops in the graph, we can create infinite zooms and infinitely detailed textures that are impossible with current example-based methods. We also introduce a technique that ameliorates inconsistencies in the user's input, and show that the application of this method yields improved interscale coherence and higher visual quality. We demonstrate optimizations for both CPU and GPU implementations of our method, and use them to produce animations with zooming and panning at multiple scales, as well as static gigapixel-sized images with features spanning many spatial scales.	Multiscale texture synthesis	NA:NA:NA:NA	2018
Li-Yi Wei:Jianwei Han:Kun Zhou:Hujun Bao:Baining Guo:Heung-Yeung Shum	The quality and speed of most texture synthesis algorithms depend on a 2D input sample that is small and contains enough texture variations. However, little research exists on how to acquire such sample. For homogeneous patterns this can be achieved via manual cropping, but no adequate solution exists for inhomogeneous or globally varying textures, i.e. patterns that are local but not stationary, such as rusting over an iron statue with appearance conditioned on varying moisture levels. We present inverse texture synthesis to address this issue. Our inverse synthesis runs in the opposite direction with respect to traditional forward synthesis: given a large globally varying texture, our algorithm automatically produces a small texture compaction that best summarizes the original. This small compaction can be used to reconstruct the original texture or to re-synthesize new textures under user-supplied controls. More important, our technique allows real-time synthesis of globally varying textures on a GPU, where the texture memory is usually too small for large textures. We propose an optimization framework for inverse texture synthesis, ensuring that each input region is properly encoded in the output compaction. Our optimization process also automatically computes orientation fields for anisotropic textures containing both low- and high-frequency regions, a situation difficult to handle via existing techniques.	Inverse texture synthesis	NA:NA:NA:NA:NA:NA	2018
Kenshi Takayama:Makoto Okabe:Takashi Ijiri:Takeo Igarashi	We present a method for representing solid objects with spatially-varying oriented textures by repeatedly pasting solid texture exemplars. The underlying concept is to extend the 2D texture patch-pasting approach of lapped textures to 3D solids using a tetrahedral mesh and 3D texture patches. The system places texture patches according to the user-defined volumetric tensor fields over the mesh to represent oriented textures. We have also extended the original technique to handle nonhomogeneous textures for creating solid models whose textural patterns change gradually along the depth fields. We identify several texture types considering the amount of anisotropy and spatial variation and provide a tailored user interface for each. With our simple framework, large-scale realistic solid models can be created easily with little memory and computational cost. We demonstrate the effectiveness of our approach with several examples including trees, fruits, and vegetables.	Lapped solid textures: filling a model with anisotropic textures	NA:NA:NA:NA	2018
Alexander Goldberg:Matthias Zwicker:Frédo Durand	Programmable graphics hardware makes it possible to generate procedural noise textures on the fly for interactive rendering. However, filtering and antialiasing procedural noise involves a tradeoff between aliasing artifacts and loss of detail. In this paper we present a technique, targeted at interactive applications, that provides high-quality anisotropic filtering for noise textures. We generate noise tiles directly in the frequency domain by partitioning the frequency domain into oriented subbands. We then compute weighted sums of the subband textures to accurately approximate noise with a desired spectrum. This allows us to achieve high-quality anisotropic filtering. Our approach is based solely on 2D textures, avoiding the memory overhead of techniques based on 3D noise tiles. We devise a technique to compensate for texture distortions to generate uniform noise on arbitrary meshes. We develop a GPU-based implementation of our technique that achieves similar rendering performance as state-of-the-art algorithms for procedural noise. In addition, it provides anisotropic filtering and achieves superior image quality.	Anisotropic noise	NA:NA:NA	2018
Wojciech Matusik	NA	Session details: Computational photography & display	NA	2018
Chia-Kai Liang:Tai-Hsu Lin:Bing-Yi Wong:Chi Liu:Homer H. Chen	In this paper, we present a system including a novel component called programmable aperture and two associated post-processing algorithms for high-quality light field acquisition. The shape of the programmable aperture can be adjusted and used to capture light field at full sensor resolution through multiple exposures without any additional optics and without moving the camera. High acquisition efficiency is achieved by employing an optimal multiplexing scheme, and quality data is obtained by using the two post-processing algorithms designed for self calibration of photometric distortion and for multi-view depth estimation. View-dependent depth maps thus generated help boost the angular resolution of light field. Various post-exposure photographic effects are given to demonstrate the effectiveness of the system and the quality of the captured light field.	Programmable aperture photography: multiplexed light field acquisition	NA:NA:NA:NA:NA	2018
Ramesh Raskar:Amit Agrawal:Cyrus A. Wilson:Ashok Veeraraghavan	Glare arises due to multiple scattering of light inside the camera's body and lens optics and reduces image contrast. While previous approaches have analyzed glare in 2D image space, we show that glare is inherently a 4D ray-space phenomenon. By statistically analyzing the ray-space inside a camera, we can classify and remove glare artifacts. In ray-space, glare behaves as high frequency noise and can be reduced by outlier rejection. While such analysis can be performed by capturing the light field inside the camera, it results in the loss of spatial resolution. Unlike light field cameras, we do not need to reversibly encode the spatial structure of the ray-space, leading to simpler designs. We explore masks for uniform and non-uniform ray sampling and show a practical solution to analyze the 4D statistics without significantly compromising image resolution. Although diffuse scattering of the lens introduces 4D low-frequency glare, we can produce useful solutions in a variety of common scenarios. Our approach handles photography looking into the sun and photos taken without a hood, removes the effect of lens smudges and reduces loss of contrast due to camera body reflections. We show various applications in contrast enhancement and glare manipulation.	Glare aware photography: 4D ray sampling for reducing glare effects of camera lenses	NA:NA:NA:NA	2018
Oliver Cossairt:Shree Nayar:Ravi Ramamoorthi	We present a novel image-based method for compositing real and synthetic objects in the same scene with a high degree of visual realism. Ours is the first technique to allow global illumination and near-field lighting effects between both real and synthetic objects at interactive rates, without needing a geometric and material model of the real scene. We achieve this by using a light field interface between real and synthetic components---thus, indirect illumination can be simulated using only two 4D light fields, one captured from and one projected onto the real scene. Multiple bounces of interreflections are obtained simply by iterating this approach. The interactivity of our technique enables its use with time-varying scenes, including dynamic objects. This is in sharp contrast to the alternative approach of using 6D or 8D light transport functions of real objects, which are very expensive in terms of acquisition and storage and hence not suitable for real-time applications. In our method, 4D radiance fields are simultaneously captured and projected by using a lens array, video camera, and digital projector. The method supports full global illumination with restricted object placement, and accommodates moderately specular materials. We implement a complete system and show several example scene compositions that demonstrate global illumination effects between dynamic real and synthetic objects. Our implementation requires a single point light source and dark background.	Light field transfer: global illumination between real and synthetic objects	NA:NA:NA	2018
Martin Fuchs:Ramesh Raskar:Hans-Peter Seidel:Hendrik P. A. Lensch	Traditional flat screen displays present 2D images. 3D and 4D displays have been proposed making use of lenslet arrays to shape a fixed outgoing light field for horizontal or bidirectional parallax. In this article, we present different designs of multi-dimensional displays which passively react to the light of the environment behind. The prototypes physically implement a reflectance field and generate different light fields depending on the incident illumination, for example light falling through a window. We discretize the incident light field using an optical system, and modulate it with a 2D pattern, creating a flat display which is view and illumination-dependent. It is free from electronic components. For distant light and a fixed observer position, we demonstrate a passive optical configuration which directly renders a 4D reflectance field in the real-world illumination behind it. We further propose an optical setup that allows for projecting out different angular distributions depending on the incident light direction. Combining multiple of these devices we build a display that renders a 6D experience, where the incident 2D illumination influences the outgoing light field, both in the spatial and in the angular domain. Possible applications of this technology are time-dependent displays driven by sunlight, object virtualization and programmable light benders / ray blockers without moving parts.	Towards passive 6D reflectance field displays	NA:NA:NA:NA	2018
Karol Myszkowski	NA	Session details: Perception & hallucination	NA	2018
Mashhuda Glencross:Gregory J. Ward:Francho Melendez:Caroline Jay:Jun Liu:Roger Hubbold	Capturing detailed surface geometry currently requires specialized equipment such as laser range scanners, which despite their high accuracy, leave gaps in the surfaces that must be reconciled with photographic capture for relighting applications. Using only a standard digital camera and a single view, we present a method for recovering models of predominantly diffuse textured surfaces that can be plausibly relit and viewed from any angle under any illumination. Our multiscale shape-from-shading technique uses diffuse-lit/flash-lit image pairs to produce an albedo map and textured height field. Using two lighting conditions enables us to subtract one from the other to estimate albedo. In the absence of a flash-lit image of a surface for which we already have a similar exemplar pair, we approximate both albedo and diffuse shading images using histogram matching. Our depth estimation is based on local visibility. Unlike other depth-from-shading approaches, all operations are performed on the diffuse shading image in image space, and we impose no constant albedo restrictions. An experimental validation shows our method works for a broad range of textured surfaces, and viewers are frequently unable to identify our results as synthetic in a randomized presentation. Furthermore, in side-by-side comparisons, subjects found a rendering of our depth map equally plausible to one generated from a laser range scan. We see this method as a significant advance in acquiring surface detail for texturing using a standard digital camera, with applications in architecture, archaeological reconstruction, games and special effects.	A perceptually validated model for surface depth hallucination	NA:NA:NA:NA:NA:NA	2018
Ganesh Ramanarayanan:Kavita Bala:James A. Ferwerda	Aggregates of individual objects, such as forests, crowds, and piles of fruit, are a common source of complexity in computer graphics scenes. When viewing an aggregate, observers attend less to individual objects and focus more on overall properties such as numerosity, variety, and arrangement. Paradoxically, rendering and modeling costs increase with aggregate complexity, exactly when observers are attending less to individual objects. In this paper we take some first steps to characterize the limits of visual coding of aggregates to efficiently represent their appearance in scenes. We describe psychophysical experiments that explore the roles played by the geometric and material properties of individual objects in observers' abilities to discriminate different aggregate collections. Based on these experiments we derive metrics to predict when two aggregates have the same appearance, even when composed of different objects. In a follow-up experiment we confirm that these metrics can be used to predict the appearance of a range of realistic aggregates. Finally, as a proof-of-concept we show how these new aggregate perception metrics can be applied to simplify scenes by allowing substitution of geometrically simpler aggregates for more complex ones without changing appearance.	Perception of complex aggregates	NA:NA:NA	2018
Hamilton Y. Chong:Steven J. Gortler:Todd Zickler	Motivated by perceptual principles, we derive a new color space in which the associated metric approximates perceived distances and color displacements capture relationships that are robust to spectral changes in illumination. The resulting color space can be used with existing image processing algorithms with little or no change to the methods.	A perception-based color space for illumination-invariant image processing	NA:NA:NA	2018
Ming-Te Chi:Tong-Yee Lee:Yingge Qu:Tien-Tsin Wong	Illusory motion in a still image is a fascinating research topic in the study of human motion perception. Physiologists and psychologists have attempted to understand this phenomenon by constructing simple, color repeated asymmetric patterns (RAP) and have found several useful rules to enhance the strength of illusory motion. Based on their knowledge, we propose a computational method to generate self-animating images. First, we present an optimized RAP placement on streamlines to generate illusory motion for a given static vector field. Next, a general coloring scheme for RAP is proposed to render streamlines. Furthermore, to enhance the strength of illusion and respect the shape of the region, a smooth vector field with opposite directional flow is automatically generated given an input image. Examples generated by our method are shown as evidence of the illusory effect and the potential applications for entertainment and design purposes.	Self-animating images: illusory motion using repeated asymmetric patterns	NA:NA:NA:NA	2018
Mark Carlson	NA	Session details: Hair, rods & cloth	NA	2018
Miklós Bergou:Max Wardetzky:Stephen Robinson:Basile Audoly:Eitan Grinspun	We present a discrete treatment of adapted framed curves, parallel transport, and holonomy, thus establishing the language for a discrete geometric model of thin flexible rods with arbitrary cross section and undeformed configuration. Our approach differs from existing simulation techniques in the graphics and mechanics literature both in the kinematic description---we represent the material frame by its angular deviation from the natural Bishop frame---as well as in the dynamical treatment---we treat the centerline as dynamic and the material frame as quasistatic. Additionally, we describe a manifold projection method for coupling rods to rigid-bodies and simultaneously enforcing rod inextensibility. The use of quasistatics and constraints provides an efficient treatment for stiff twisting and stretching modes; at the same time, we retain the dynamic bending of the centerline and accurately reproduce the coupling between bending and twisting modes. We validate the discrete rod model via quantitative buckling, stability, and coupled-mode experiments, and via qualitative knot-tying comparisons.	Discrete elastic rods	NA:NA:NA:NA:NA	2018
Andrew Selle:Michael Lentine:Ronald Fedkiw	Our goal is to simulate the full hair geometry, consisting of approximately one hundred thousand hairs on a typical human head. This will require scalable methods that can simulate every hair as opposed to only a few guide hairs. Novel to this approach is that the individual hair/hair interactions can be modeled with physical parameters (friction, static attraction, etc.) at the scale of a single hair as opposed to clumped or continuum interactions. In this vein, we first propose a new altitude spring model for preventing collapse in the simulation of volumetric tetrahedra, and we show that it is also applicable both to bending in cloth and torsion in hair. We demonstrate that this new torsion model for hair behaves in a fashion similar to more sophisticated models with significantly reduced computational cost. For added efficiency, we introduce a semi-implicit discretization of standard springs that makes them truly linear in multiple spatial dimensions and thus unconditionally stable without requiring Newton-Raphson iteration. We also simulate complex hair/hair interactions including sticking and clumping behavior, collisions with objects (e.g. head and shoulders) and self-collisions. Notably, in line with our goal to simulate the full head of hair, we do not generate any new hairs at render time.	A mass spring model for hair simulation	NA:NA:NA	2018
Jonathan M. Kaldor:Doug L. James:Steve Marschner	Knitted fabric is widely used in clothing because of its unique and stretchy behavior, which is fundamentally different from the behavior of woven cloth. The properties of knits come from the nonlinear, three-dimensional kinematics of long, inter-looping yarns, and despite significant advances in cloth animation we still do not know how to simulate knitted fabric faithfully. Existing cloth simulators mainly adopt elastic-sheet mechanical models inspired by woven materials, focusing less on the model itself than on important simulation challenges such as efficiency, stability, and robustness. We define a new computational model for knits in terms of the motion of yarns, rather than the motion of a sheet. Each yarn is modeled as an inextensible, yet otherwise flexible, B-spline tube. To simulate complex knitted garments, we propose an implicit-explicit integrator, with yarn inextensibility constraints imposed using efficient projections. Friction among yarns is approximated using rigid-body velocity filters, and key yarn-yarn interactions are mediated by stiff penalty forces. Our results show that this simple model predicts the key mechanical properties of different knits, as demonstrated by qualitative comparisons to observed deformations of actual samples in the laboratory, and that the simulator can scale up to substantial animations with complex dynamic motion.	Simulating knitted cloth at the yarn level	NA:NA:NA	2018
Elliot English:Robert Bridson	We present a new discretization for the physics-based animation of developable surfaces. Constrained to not deform at all in-plane but free to bend out-of-plane, these are an excellent approximation for many materials, including most cloth, paper, and stiffer materials. Unfortunately the conforming (geometrically continuous) discretizations used in graphics break down in this limit. Our nonconforming approach solves this problem, allowing us to simulate surfaces with zero in-plane deformation as a hard constraint. However, it produces discontinuous meshes, so we further couple this with a "ghost" conforming mesh for collision processing and rendering. We also propose a new second order accurate constrained mechanics time integration method that greatly reduces the numerical damping present in the usual first order methods used in graphics, for virtually no extra cost and sometimes significant speed-up.	Animating developable surfaces using nonconforming elements	NA:NA	2018
Ramesh Raskar	NA	Session details: Tone & color	NA	2018
Zeev Farbman:Raanan Fattal:Dani Lischinski:Richard Szeliski	Many recent computational photography techniques decompose an image into a piecewise smooth base layer, containing large scale variations in intensity, and a residual detail layer capturing the smaller scale details in the image. In many of these applications, it is important to control the spatial scale of the extracted details, and it is often desirable to manipulate details at multiple scales, while avoiding visual artifacts. In this paper we introduce a new way to construct edge-preserving multi-scale image decompositions. We show that current basedetail decomposition techniques, based on the bilateral filter, are limited in their ability to extract detail at arbitrary scales. Instead, we advocate the use of an alternative edge-preserving smoothing operator, based on the weighted least squares optimization framework, which is particularly well suited for progressive coarsening of images and for multi-scale detail extraction. After describing this operator, we show how to use it to construct edge-preserving multi-scale decompositions, and compare it to the bilateral filter, as well as to other schemes. Finally, we demonstrate the effectiveness of our edge-preserving decompositions in the context of LDR and HDR tone mapping, detail enhancement, and other applications.	Edge-preserving decompositions for multi-scale tone and detail manipulation	NA:NA:NA:NA	2018
Rafał Mantiuk:Scott Daly:Louis Kerofsky	We propose a tone mapping operator that can minimize visible contrast distortions for a range of output devices, ranging from e-paper to HDR displays. The operator weights contrast distortions according to their visibility predicted by the model of the human visual system. The distortions are minimized given a display model that enforces constraints on the solution. We show that the problem can be solved very efficiently by employing higher order image statistics and quadratic programming. Our tone mapping technique can adjust image or video content for optimum contrast visibility taking into account ambient illumination and display characteristics. We discuss the differences between our method and previous approaches to the tone mapping problem.	Display adaptive tone mapping	NA:NA:NA	2018
Tunç Ozan Aydin:Rafał Mantiuk:Karol Myszkowski:Hans-Peter Seidel	The diversity of display technologies and introduction of high dynamic range imagery introduces the necessity of comparing images of radically different dynamic ranges. Current quality assessment metrics are not suitable for this task, as they assume that both reference and test images have the same dynamic range. Image fidelity measures employed by a majority of current metrics, based on the difference of pixel intensity or contrast values between test and reference images, result in meaningless predictions if this assumption does not hold. We present a novel image quality metric capable of operating on an image pair where both images have arbitrary dynamic ranges. Our metric utilizes a model of the human visual system, and its central idea is a new definition of visible distortion based on the detection and classification of visible changes in the image structure. Our metric is carefully calibrated and its performance is validated through perceptual experiments. We demonstrate possible applications of our metric to the evaluation of direct and inverse tone mapping operators as well as the analysis of the image appearance on displays with various characteristics.	Dynamic range independent image quality assessment	NA:NA:NA:NA	2018
Eugene Hsu:Tom Mertens:Sylvain Paris:Shai Avidan:Frédo Durand	White balance is a crucial step in the photographic pipeline. It ensures the proper rendition of images by eliminating color casts due to differing illuminants. Digital cameras and editing programs provide white balance tools that assume a single type of light per image, such as daylight. However, many photos are taken under mixed lighting. We propose a white balance technique for scenes with two light types that are specified by the user. This covers many typical situations involving indoor/outdoor or flash/ambient light mixtures. Since we work from a single image, the problem is highly underconstrained. Our method recovers a set of dominant material colors which allows us to estimate the local intensity mixture of the two light types. Using this mixture, we can neutralize the light colors and render visually pleasing images. Our method can also be used to achieve post-exposure relighting effects.	Light mixture estimation for spatially varying white balance	NA:NA:NA:NA:NA	2018
Hendrik Lensch	NA	Session details: Deblurring & dehazing	NA	2018
Anat Levin:Peter Sand:Taeg Sang Cho:Frédo Durand:William T. Freeman	Object motion during camera exposure often leads to noticeable blurring artifacts. Proper elimination of this blur is challenging because the blur kernel is unknown, varies over the image as a function of object velocity, and destroys high frequencies. In the case of motions along a 1D direction (e.g. horizontal) we show that these challenges can be addressed using a camera that moves during the exposure. Through the analysis of motion blur as space-time integration, we show that a parabolic integration (corresponding to constant sensor acceleration) leads to motion blur that is invariant to object velocity. Thus, a single deconvolution kernel can be used to remove blur and create sharp images of scenes with objects moving at different speeds, without requiring any segmentation and without knowledge of the object speeds. Apart from motion invariance, we prove that the derived parabolic motion preserves image frequency content nearly optimally. That is, while static objects are degraded relative to their image from a static camera, a reliable reconstruction of all moving objects within a given velocities range is made possible. We have built a prototype camera and present successful deblurring results over a wide variety of human motions.	Motion-invariant photography	NA:NA:NA:NA:NA	2018
Raanan Fattal	In this paper we present a new method for estimating the optical transmission in hazy scenes given a single input image. Based on this estimation, the scattered light is eliminated to increase scene visibility and recover haze-free scene contrasts. In this new approach we formulate a refined image formation model that accounts for surface shading in addition to the transmission function. This allows us to resolve ambiguities in the data by searching for a solution in which the resulting shading and transmission functions are locally statistically uncorrelated. A similar principle is used to estimate the color of the haze. Results demonstrate the new method abilities to remove the haze layer as well as provide a reliable transmission estimate which can be used for additional applications such as image refocusing and novel view synthesis.	Single image dehazing	NA	2018
Qi Shan:Jiaya Jia:Aseem Agarwala	We present a new algorithm for removing motion blur from a single image. Our method computes a deblurred image using a unified probabilistic model of both blur kernel estimation and unblurred image restoration. We present an analysis of the causes of common artifacts found in current deblurring methods, and then introduce several novel terms within this probabilistic model that are inspired by our analysis. These terms include a model of the spatial randomness of noise in the blurred image, as well a new local smoothness prior that reduces ringing artifacts by constraining contrast in the unblurred image wherever the blurred image exhibits low contrast. Finally, we describe an effficient optimization scheme that alternates between blur kernel estimation and unblurred image restoration until convergence. As a result of these steps, we are able to produce high quality deblurred results in low computation time. We are even able to produce results of comparable quality to techniques that require additional input images beyond a single blurry photograph, and to methods that require additional hardware.	High-quality motion deblurring from a single image	NA:NA:NA	2018
Lu Yuan:Jian Sun:Long Quan:Heung-Yeung Shum	Ringing is the most disturbing artifact in the image deconvolution. In this paper, we present a progressive inter-scale and intra-scale non-blind image deconvolution approach that significantly reduces ringing. Our approach is built on a novel edge-preserving deconvolution algorithm called bilateral Richardson-Lucy (BRL) which uses a large spatial support to handle large blur. We progressively recover the image from a coarse scale to a fine scale (inter-scale), and progressively restore image details within every scale (intra-scale). To perform the inter-scale deconvolution, we propose a joint bilateral Richardson-Lucy (JBRL) algorithm so that the recovered image in one scale can guide the deconvolution in the next scale. In each scale, we propose an iterative residual deconvolution to progressively recover image details. The experimental results show that our progressive deconvolution can produce images with very little ringing for large blur kernels.	Progressive inter-scale and intra-scale non-blind image deconvolution	NA:NA:NA:NA	2018
Bruno Levy	NA	Session details: Folding & unfolding surfaces	NA	2018
Martin Kilian:Simon Flöry:Zhonggui Chen:Niloy J. Mitra:Alla Sheffer:Helmut Pottmann	Fascinating and elegant shapes may be folded from a single planar sheet of material without stretching, tearing or cutting, if one incorporates curved folds into the design. We present an optimization-based computational framework for design and digital reconstruction of surfaces which can be produced by curved folding. Our work not only contributes to applications in architecture and industrial design, but it also provides a new way to study the complex and largely unexplored phenomena arising in curved folding.	Curved folding	NA:NA:NA:NA:NA:NA	2018
Helmut Pottmann:Alexander Schiftner:Pengbo Bo:Heinz Schmiedhofer:Wenping Wang:Niccolo Baldassini:Johannes Wallner	Motivated by applications in architecture and manufacturing, we discuss the problem of covering a freeform surface by single curved panels. This leads to the new concept of semi-discrete surface representation, which constitutes a link between smooth and discrete surfaces. The basic entity we are working with is the developable strip model. It is the semi-discrete equivalent of a quad mesh with planar faces, or a conjugate parametrization of a smooth surface. We present a B-spline based optimization framework for efficient computing with D-strip models. In particular we study conical and circular models, which semi-discretize the network of principal curvature lines, and which enjoy elegant geometric properties. Together with geodesic models and cylindrical models they offer a rich source of solutions for surface panelization problems.	Freeform surfaces from single curved panels	NA:NA:NA:NA:NA:NA:NA	2018
Boris Springborn:Peter Schröder:Ulrich Pinkall	We present a new algorithm for conformal mesh parameterization. It is based on a precise notion of discrete conformal equivalence for triangle meshes which mimics the notion of conformal equivalence for smooth surfaces. The problem of finding a flat mesh that is discretely conformally equivalent to a given mesh can be solved efficiently by minimizing a convex energy function, whose Hessian turns out to be the well known cot-Laplace operator. This method can also be used to map a surface mesh to a parameter domain which is flat except for isolated cone singularities, and we show how these can be placed automatically in order to reduce the distortion of the parameterization. We present the salient features of the theory and elaborate the algorithms with a number of examples.	Conformal equivalence of triangle meshes	NA:NA:NA	2018
Yaron Lipman:David Levin:Daniel Cohen-Or	We introduce Green Coordinates for closed polyhedral cages. The coordinates are motivated by Green's third integral identity and respect both the vertices position and faces orientation of the cage. We show that Green Coordinates lead to space deformations with a shape-preserving property. In particular, in 2D they induce conformal mappings, and extend naturally to quasi-conformal mappings in 3D. In both cases we derive closed-form expressions for the coordinates, yielding a simple and fast algorithm for cage-based space deformation. We compare the performance of Green Coordinates with those of Mean Value Coordinates and Harmonic Coordinates and show that the advantage of the shape-preserving property is not achieved at the expense of speed or simplicity. We also show that the new coordinates extend the mapping in a natural analytic manner to the exterior of the cage, allowing the employment of partial cages.	Green Coordinates	NA:NA:NA	2018
Thomas W. Sederberg:G. Thomas Finnigan:Xin Li:Hongwei Lin:Heather Ipson	This paper addresses the long-standing problem of the unavoidable gaps that arise when expressing the intersection of two NURBS surfaces using conventional trimmed-NURBS representation. The solution converts each trimmed NURBS into an untrimmed T-Spline, and then merges the untrimmed T-Splines into a single, watertight model. The solution enables watertight fillets of NURBS models, as well as arbitrary feature curves that do not have to follow iso-parameter curves. The resulting T-Spline representation can be exported without error as a collection of NURBS surfaces.	Watertight trimmed NURBS	NA:NA:NA:NA:NA	2018
Adrien Treuille	NA	Session details: Humans	NA	2018
Taesoo Kwon:Kang Hoon Lee:Jehee Lee:Shigeo Takahashi	Animating a crowd of characters is an important problem in computer graphics. The latest techniques enable highly realistic group motions to be produced in feature animation films and video games. However, interactive methods have not emerged yet for editing the existing group motion of multiple characters. We present an approach to editing group motion as a whole while maintaining its neighborhood formation and individual moving trajectories in the original animation as much as possible. The user can deform a group motion by pinning or dragging individuals. Multiple group motions can be stitched or merged to form a longer or larger group motion while avoiding collisions. These editing operations rely on a novel graph structure, in which vertices represent positions of individuals at specific frames and edges encode neighborhood formations and moving trajectories. We employ a shape-manipulation technique to minimize the distortion of relative arrangements among adjacent vertices while editing the graph structure. The usefulness and flexibility of our approach is demonstrated through examples in which the user creates and edits complex crowd animations interactively using a collection of group motion clips.	Group motion editing	NA:NA:NA:NA	2018
KangKang Yin:Stelian Coros:Philippe Beaudoin:Michiel van de Panne	Modeling the large space of possible human motions requires scalable techniques. Generalizing from example motions or example controllers is one way to provide the required scalability. We present techniques for generalizing a controller for physics-based walking to significantly different tasks, such as climbing a large step up, or pushing a heavy object. Continuation methods solve such problems using a progressive sequence of problems that trace a path from an existing solved problem to the final desired-but-unsolved problem. Each step in the continuation sequence makes progress towards the target problem while further adapting the solution. We describe and evaluate a number of choices in applying continuation methods to adapting walking gaits for tasks involving interaction with the environment. The methods have been successfully applied to automatically adapt a regular cyclic walk to climbing a 65cm step, stepping over a 55cm sill, pushing heavy furniture, walking up steep inclines, and walking on ice. The continuation path further provides parameterized solutions to these problems.	Continuation methods for adapting simulated skills	NA:NA:NA:NA	2018
Marco da Silva:Yeuhi Abe:Jovan Popović	Animating natural human motion in dynamic environments is difficult because of complex geometric and physical interactions. Simulation provides an automatic solution to parts of this problem, but it needs control systems to produce lifelike motions. This paper describes the systematic computation of controllers that can reproduce a range of locomotion styles in interactive simulations. Given a reference motion that describes the desired style, a derived control system can reproduce that style in simulation and in new environments. Because it produces high-quality motions that are both geometrically and physically consistent with simulated surroundings, interactive animation systems could begin to use this approach along with more established kinematic methods.	Interactive simulation of stylized human locomotion	NA:NA:NA	2018
Shinjiro Sueda:Andrew Kaufman:Dinesh K. Pai	We describe an automatic technique for generating the motion of tendons and muscles under the skin of a traditionally animated character. This is achieved by integrating the traditional animation pipeline with a novel biomechanical simulator capable of dynamic simulation with complex routing constraints on muscles and tendons. We also describe an algorithm for computing the activation levels of muscles required to track the input animation. We demonstrate the results with several animations of the human hand.	Musculotendon simulation for hand animation	NA:NA:NA	2018
Srinivasa Narasimhan	NA	Session details: Shape acquisition	NA	2018
Benedict J. Brown:Corey Toler-Franklin:Diego Nehab:Michael Burns:David Dobkin:Andreas Vlachopoulos:Christos Doumas:Szymon Rusinkiewicz:Tim Weyrich	Although mature technologies exist for acquiring images, geometry, and normals of small objects, they remain cumbersome and time-consuming for non-experts to employ on a large scale. In an archaeological setting, a practical acquisition system for routine use on every artifact and fragment would open new possibilities for archiving, analysis, and dissemination. We present an inexpensive system for acquiring all three types of information, and associated metadata, for small objects such as fragments of wall paintings. The acquisition system requires minimal supervision, so that a single, non-expert user can scan at least 10 fragments per hour. To achieve this performance, we introduce new algorithms to robustly and automatically align range scans, register 2-D scans to 3-D geometry, and compute normals from 2-D scans. As an illustrative application, we present a novel 3-D matching algorithm that efficiently searches for matching fragments using the scanned geometry.	A system for high-volume acquisition and matching of fresco fragments: reassembling Theran wall paintings	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Dror Aiger:Niloy J. Mitra:Daniel Cohen-Or	We introduce 4PCS, a fast and robust alignment scheme for 3D point sets that uses wide bases, which are known to be resilient to noise and outliers. The algorithm allows registering raw noisy data, possibly contaminated with outliers, without pre-filtering or denoising the data. Further, the method significantly reduces the number of trials required to establish a reliable registration between the underlying surfaces in the presence of noise, without any assumptions about starting alignment. Our method is based on a novel technique to extract all coplanar 4-points sets from a 3D point set that are approximately congruent, under rigid transformation, to a given set of coplanar 4-points. This extraction procedure runs in roughly O(n2 + k) time, where n is the number of candidate points and k is the number of reported 4-points sets. In practice, when noise level is low and there is sufficient overlap, using local descriptors the time complexity reduces to O(n + k). We also propose an extension to handle similarity and affine transforms. Our technique achieves an order of magnitude asymptotic acceleration compared to common randomized alignment techniques. We demonstrate the robustness of our algorithm on several sets of multiple range scans with varying degree of noise, outliers, and extent of overlap.	4-points congruent sets for robust pairwise surface registration	NA:NA:NA	2018
Thorsten Thormählen:Hans-Peter Seidel	A semi-automatic approach is presented that enables the generation of a high-quality 3D model of a static object from an image sequence that was taken by a moving, uncalibrated consumer camera. A bounding box is placed around the object, and orthographic projections onto the sides of the bounding box are automatically generated out of the image sequence. These ortho-images can be imported as background maps in the orthographic views (e.g., the top, side, and front view) of any modeling package. Modelers can now use these ortho-images to guide their modeling by tracing the shape of the object over the ortho-images. This greatly improves the accuracy and efficiency of the manual modeling process. An additional advantage over existing semi-automatic systems is that modelers can use the modeling package that they are trained in and can thereby increase their productivity by applying the advanced modeling features the package offers. The results presented show that accurate 3D models can even be generated for translucent or specular surfaces, and the approach is therefore still applicable in cases where today's fully automatic image-based approaches or laser scanners would fail.	3D-modeling by ortho-image generation from image sequences	NA:NA	2018
Matthias B. Hullin:Martin Fuchs:Ivo Ihrke:Hans-Peter Seidel:Hendrik P. A. Lensch	The quality of a 3D range scan should not depend on the surface properties of the object. Most active range scanning techniques, however, assume a diffuse reflector to allow for a robust detection of incident light patterns. In our approach we embed the object into a fluorescent liquid. By analyzing the light rays that become visible due to fluorescence rather than analyzing their reflections off the surface, we can detect the intersection points between the projected laser sheet and the object surface for a wide range of different materials. For transparent objects we can even directly depict a slice through the object in just one image by matching its refractive index to the one of the embedding liquid. This enables a direct sampling of the object geometry without the need for computational reconstruction. This way, a high-resolution 3D volume can be assembled simply by sweeping a laser plane through the object. We demonstrate the effectiveness of our light sheet range scanning approach on a set of objects manufactured from a variety of materials and material mixes, including dark, translucent and transparent objects.	Fluorescent immersion range scanning	NA:NA:NA:NA:NA	2018
Olga Sorkine	NA	Session details: NPR & deformation	NA	2018
Forrester Cole:Aleksey Golovinskiy:Alex Limpaecher:Heather Stoddart Barros:Adam Finkelstein:Thomas Funkhouser:Szymon Rusinkiewicz	This paper presents the results of a study in which artists made line drawings intended to convey specific 3D shapes. The study was designed so that drawings could be registered with rendered images of 3D models, supporting an analysis of how well the locations of the artists' lines correlate with other artists', with current computer graphics line definitions, and with the underlying differential properties of the 3D surface. Lines drawn by artists in this study largely overlapped one another (75% are within 1mm of another line), particularly along the occluding contours of the object. Most lines that do not overlap contours overlap large gradients of the image intensity, and correlate strongly with predictions made by recent line drawing algorithms in computer graphics. 14% were not well described by any of the local properties considered in this study. The result of our work is a publicly available data set of aligned drawings, an analysis of where lines appear in that data set based on local properties of 3D models, and algorithms to predict where artists will draw lines for new scenes.	Where do people draw lines?	NA:NA:NA:NA:NA:NA:NA	2018
Wai-Man Pang:Yingge Qu:Tien-Tsin Wong:Daniel Cohen-Or:Pheng-Ann Heng	This paper presents an optimization-based halftoning technique that preserves the structure and tone similarities between the original and the halftone images. By optimizing an objective function consisting of both the structure and the tone metrics, the generated halftone images preserve visually sensitive texture details as well as the local tone. It possesses the blue-noise property and does not introduce annoying patterns. Unlike the existing edge-enhancement halftoning, the proposed method does not suffer from the deficiencies of edge detector. Our method is tested on various types of images. In multiple experiments and the user study, our method consistently obtains the best scores among all tested methods.	Structure-aware halftoning	NA:NA:NA:NA:NA	2018
Tobias Ritschel:Kaleigh Smith:Matthias Ihrke:Thorsten Grosch:Karol Myszkowski:Hans-Peter Seidel	We present a new approach for enhancing local scene contrast by unsharp masking over arbitrary surfaces under any form of illumination. Our adaptation of a well-known 2D technique to 3D interactive scenarios is designed to aid viewers in tasks like understanding complex or detailed geometric models, medical visualization and navigation in virtual environments. Our holistic approach enhances the depiction of various visual cues, including gradients from surface shading, surface reflectance, shadows, and highlights, to ease estimation of viewpoint, lighting conditions, shapes of objects and their world-space organization. Motivated by recent perceptual findings on 3D aspects of the Cornsweet illusion, we create scene coherent enhancements by treating cues in terms of their 3D context; doing so has a stronger effect than approaches that operate in a 2D image context and also achieves temporal coherence. We validate our unsharp masking in 3D with psychophysical experiments showing that the enhanced images are perceived to have better contrast and are preferred over unenhanced originals. Our operator runs at real-time rates on a GPU and the effect is easily controlled interactively within the rendering pipeline.	3D unsharp masking for scene coherent enhancement	NA:NA:NA:NA:NA:NA	2018
Wei-Wen Feng:Byung-Uck Kim:Yizhou Yu	Achieving intuitive control of animated surface deformation while observing a specific style is an important but challenging task in computer graphics. Solutions to this task can find many applications in data-driven skin animation, computer puppetry, and computer games. In this paper, we present an intuitive and powerful animation interface to simultaneously control the deformation of a large number of local regions on a deformable surface with a minimal number of control points. Our method learns suitable deformation subspaces from training examples, and generate new deformations on the fly according to the movements of the control points. Our contributions include a novel deformation regression method based on kernel Canonical Correlation Analysis (CCA) and a Poisson-based translation solving technique for easy and fast deformation control based on examples. Our run-time algorithm can be implemented on GPUs and can achieve a few hundred frames per second even for large datasets with hundreds of training examples.	Real-time data driven deformation using kernel canonical correlation analysis	NA:NA:NA	2018
Matthias Zwicker	NA	Session details: Painting & sketching	NA	2018
Alexandrina Orzan:Adrien Bousseau:Holger Winnemöller:Pascal Barla:Joëlle Thollot:David Salesin	We describe a new vector-based primitive for creating smooth-shaded images, called the diffusion curve. A diffusion curve partitions the space through which it is drawn, defining different colors on either side. These colors may vary smoothly along the curve. In addition, the sharpness of the color transition from one side of the curve to the other can be controlled. Given a set of diffusion curves, the final image is constructed by solving a Poisson equation whose constraints are specified by the set of gradients across all diffusion curves. Like all vector-based primitives, diffusion curves conveniently support a variety of operations, including geometry-based editing, keyframe animation, and ready stylization. Moreover, their representation is compact and inherently resolution-independent. We describe a GPU-based implementation for rendering images defined by a set of diffusion curves in realtime. We then demonstrate an interactive drawing system for allowing artists to create artworks using diffusion curves, either by drawing the curves in a freehand style, or by tracing existing imagery. The system is simple and intuitive: we show results created by artists after just a few minutes of instruction. Furthermore, we describe a completely automatic conversion process for taking an image and turning it into a set of diffusion curves that closely approximate the original image content.	Diffusion curves: a vector representation for smooth-shaded images	NA:NA:NA:NA:NA:NA	2018
James McCann:Nancy S. Pollard	We present an image editing program which allows artists to paint in the gradient domain with real-time feedback on megapixel-sized images. Along with a pedestrian, though powerful, gradient-painting brush and gradient-clone tool, we introduce an edge brush designed for edge selection and replay. These brushes, coupled with special blending modes, allow users to accomplish global lighting and contrast adjustments using only local image manipulations --- e.g. strengthening a given edge or removing a shadow boundary. Such operations would be tedious in a conventional intensity-based paint program and hard for users to get right in the gradient domain without real-time feedback. The core of our paint program is a simple-to-implement GPU multigrid method which allows integration of megapixel-sized full-color gradient fields at over 20 frames per second on modest hardware. By way of evaluation, we present example images produced with our program and characterize the iteration time and convergence rate of our integration method.	Real-time gradient-domain painting	NA:NA	2018
Yoshinori Dobashi:Katsutoshi Kusumoto:Tomoyuki Nishita:Tsuyoshi Yamamoto	Clouds play an important role for creating realistic images of outdoor scenes. In order to generate realistic clouds, many methods have been developed for modeling and animating clouds. One of the most effective approaches for synthesizing realistic clouds is to simulate cloud formation processes based on the atmospheric fluid dynamics. Although this approach can create realistic clouds, the resulting shapes and motion depend on many simulation parameters and the initial status. Therefore, it is very difficult to adjust those parameters so that the clouds form the desired shapes. This paper addresses this problem and presents a method for controlling the simulation of cloud formation. In this paper, we focus on controlling cumuliform cloud formation. The user specifies the overall shape of the clouds. Then, our method automatically adjusts parameters during the simulation in order to generate clouds forming the specified shape. Our method can generate realistic clouds while their shapes closely match to the desired shape.	Feedback control of cumuliform cloud formation based on computational fluid dynamics	NA:NA:NA:NA	2018
Yotam Gingold:Denis Zorin	We present a system for free-form surface modeling that allows a user to modify a shape by changing its rendered, shaded image using stroke-based drawing tools. User input is translated into a set of tangent and positional constraints on the surface. A new shape, whose rendered image closely approximates user input, is computed using an efficient and stable surface optimization procedure. We demonstrate how several types of free-form surface edits which may be difficult to cast in terms of standard deformation approaches can be easily performed using our system.	Shading-based surface editing	NA:NA	2018
Jehee Lee	NA	Session details: Performance capture	NA	2018
Sang Il Park:Jessica K. Hodgins	In this paper, we present a data-driven technique for synthesizing skin deformation from skeletal motion. We first create a database of dynamic skin deformations by recording the motion of the surface of the skin with a very large set of motion capture markers. We then build a statistical model of the deformations by dividing them into two parts: static and dynamic. Static deformations are modeled as a function of pose. Dynamic deformations are caused by the actions of the muscles as they move the joints and the inertia of muscles and fat. We approximate these effects by fitting a set of dynamic equations to the pre-recorded data. We demonstrate the viability of this approach by generating skin deformations from the skeletal motion of an actor. We compare the generated animation both to synchronized video of the actor and to ground truth animation created directly from the large marker set.	Data-driven modeling of skin and muscle deformation	NA:NA	2018
Daniel Vlasic:Ilya Baran:Wojciech Matusik:Jovan Popović	Details in mesh animations are difficult to generate but they have great impact on visual quality. In this work, we demonstrate a practical software system for capturing such details from multi-view video recordings. Given a stream of synchronized video images that record a human performance from multiple viewpoints and an articulated template of the performer, our system captures the motion of both the skeleton and the shape. The output mesh animation is enhanced with the details observed in the image silhouettes. For example, a performance in casual loose-fitting clothes will generate mesh animations with flowing garment motions. We accomplish this with a fast pose tracking method followed by nonrigid deformation of the template to fit the silhouettes. The entire process takes less than sixteen seconds per frame and requires no markers or texture cues. Captured meshes are in full correspondence making them readily usable for editing operations including texturing, deformation transfer, and deformation model learning.	Articulated mesh animation from multi-view silhouettes	NA:NA:NA:NA	2018
Edilson de Aguiar:Carsten Stoll:Christian Theobalt:Naveed Ahmed:Hans-Peter Seidel:Sebastian Thrun	This paper proposes a new marker-less approach to capturing human performances from multi-view video. Our algorithm can jointly reconstruct spatio-temporally coherent geometry, motion and textural surface appearance of actors that perform complex and rapid moves. Furthermore, since our algorithm is purely meshbased and makes as few as possible prior assumptions about the type of subject being tracked, it can even capture performances of people wearing wide apparel, such as a dancer wearing a skirt. To serve this purpose our method efficiently and effectively combines the power of surface- and volume-based shape deformation techniques with a new mesh-based analysis-through-synthesis framework. This framework extracts motion constraints from video and makes the laser-scan of the tracked subject mimic the recorded performance. Also small-scale time-varying shape detail is recovered by applying model-guided multi-view stereo to refine the model surface. Our method delivers captured performance data at high level of detail, is highly versatile, and is applicable to many complex types of scenes that could not be handled by alternative marker-based or marker-free recording techniques.	Performance capture from sparse multi-view video	NA:NA:NA:NA:NA:NA	2018
Derek Bradley:Tiberiu Popa:Alla Sheffer:Wolfgang Heidrich:Tamy Boubekeur	A lot of research has recently focused on the problem of capturing the geometry and motion of garments. Such work usually relies on special markers printed on the fabric to establish temporally coherent correspondences between points on the garment's surface at different times. Unfortunately, this approach is tedious and prevents the capture of off-the-shelf clothing made from interesting fabrics. In this paper, we describe a marker-free approach to capturing garment motion that avoids these downsides. We establish temporally coherent parameterizations between incomplete geometries that we extract at each timestep with a multiview stereo algorithm. We then fill holes in the geometry using a template. This approach, for the first time, allows us to capture the geometry and motion of unpatterned, off-the-shelf garments made from a range of different fabrics.	Markerless garment capture	NA:NA:NA:NA:NA	2018
Claudio Silva	NA	Session details: Procedural modeling & design	NA	2018
Floraine Grabler:Maneesh Agrawala:Robert W. Sumner:Mark Pauly	Tourist maps are essential resources for visitors to an unfamiliar city because they visually highlight landmarks and other points of interest. Yet, hand-designed maps are static representations that cannot adapt to the needs and tastes of the individual tourist. In this paper we present an automated system for designing tourist maps that selects and highlights the information that is most important to tourists. Our system determines the salience of map elements using bottom-up vision-based image analysis and top-down web-based information extraction techniques. It then generates a map that emphasizes the most important elements, using a combination of multiperspective rendering to increase visibility of streets and landmarks, and cartographic generalization techniques such as simplification, deformation, and displacement to emphasize landmarks and de-emphasize less important buildings. We show a number of automatically generated tourist maps of San Francisco and compare them to existing automated and manual approaches.	Automatic generation of tourist maps	NA:NA:NA:NA	2018
Wilmot Li:Maneesh Agrawala:Brian Curless:David Salesin	We present a system for creating and viewing interactive exploded views of complex 3D models. In our approach, a 3D input model is organized into an explosion graph that encodes how parts explode with respect to each other. We present an automatic method for computing explosion graphs that takes into account part hierarchies in the input models and handles common classes of interlocking parts. Our system also includes an interface that allows users to interactively explore our exploded views using both direct controls and higher-level interaction modes.	Automated generation of interactive 3D exploded view diagrams	NA:NA:NA:NA	2018
Markus Lipp:Peter Wonka:Michael Wimmer	We introduce a real-time interactive visual editing paradigm for shape grammars, allowing the creation of rulebases from scratch without text file editing. In previous work, shape-grammar based procedural techniques were successfully applied to the creation of architectural models. However, those methods are text based, and may therefore be difficult to use for artists with little computer science background. Therefore the goal was to enable a visual work-flow combining the power of shape grammars with traditional modeling techniques. We extend previous shape grammar approaches by providing direct and persistent local control over the generated instances, avoiding the combinatorial explosion of grammar rules for modifications that should not affect all instances. The resulting visual editor is flexible: All elements of a complex state-of-the-art grammar can be created and modified visually.	Interactive visual editing of grammars for procedural architecture	NA:NA:NA	2018
