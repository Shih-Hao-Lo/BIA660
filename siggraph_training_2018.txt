I. Yosun Chang	Augmented Reality (AR) Interfaces for the Internet of Things (IoT) is an implementation of a provisional universal software platform for IoT devices, created by a solo independent developer. We showcase several use-cases of being able to point one's camera-bearing device or head mounted display (HMD) to IoT objects and the appropriate interfaces that arise to read and/or control these devices, as well as the infrastructure to enable accuracy in these interactions.	Augmented reality interfaces for the internet of things: extended abstract	NA	2018
Jisun Jang:Tomasz Bednarz	The HoloSensor project aims to enhance the visualisation and visual analytics data sourced from various sensors through use of Augmented Reality (AR) technology, allowing users to anchor information throughout various locations inside a building. The output of the project is an application, that connects networked sensors (Arduino with temperature, humidity, light sensors) communicating its data though a Python-based server. Users are able to interact with this data on holograms in real-time through the Microsoft HoloLens. This integration of the immersive AR technology with Internet of Things (IoT) shows its versatile usage in all three areas of human experience: health, home and entertainment.	HoloSensor for smart home, health, entertainment	NA:NA	2018
Fangwei Lee:Janet Lin:Elliot Segal	REALITEER Corp. created a cross-platform and kid-friendly digital mirror that can be used for education and body exercise utilizing AR/VR technologies. In a gamified manner, we take users through educational research-based exercises that will not only tackle the psychiatric and physical conditions but better overall well-being.	Kid-friendly digital mirror for education and exercise	NA:NA:NA	2018
Max Reimann:Amir Semmo:Jürgen Döllner:Sebastian Pasewaldt:Mandy Klingbeil	We present MaeSTrO, a mobile app for image stylization that empowers users to direct, edit and perform a neural style transfer with creative control. The app uses iterative style transfer, multi-style generative and adaptive networks to compute and apply flexible yet comprehensive style models of arbitrary images at run-time. Compared to other mobile applications, MaeSTrO introduces an interactive user interface that empowers users to orchestrate style transfers in a two-stage process for an individual visual expression: first, initial semantic segmentation of a style image can be complemented by on-screen painting to direct sub-styles in a spatially-aware manner. Second, semantic masks can be virtually drawn on top of a content image to adjust neural activations within local image regions, and thus direct the transfer of learned sub-styles. This way, the general feed-forward neural style transfer is evolved towards an interactive tool that is able to consider composition variables and mechanisms of general artwork production, such as color, size and location-based filtering. MaeSTrO additionally enables users to define new styles directly on a device and synthesize high-quality images based on prior segmentations via a service-based implementation of compute-intensive iterative style transfer techniques.	MaeSTrO: mobile style transfer orchestration using adaptive neural networks	NA:NA:NA:NA:NA	2018
Roberto Lopez Mendez	Up to now in mobile Virtual Reality (VR), we have been able to only control the camera orientation with our head. However, premium smartphones already incorporate the essential technology to track user position. Apple ARKit and Google ARCore designed for Augmented Reality (AR) applications are already enabled in millions of phones. Both libraries can be used to achieve 6DoF mobile VR. This contribution combines head orientation tracking provided by the VR headset with the position tracking capability provided by Google ARCore to achieve 6DoF tracking in mobile VR.	Mobile inside-out VR tracking, now available on your phone: extended abstract	NA	2018
Alyn Rockwood:Kun Gao	The SuperD 3D modeling app rapidly creates high quality, sleek and intricate shapes for 3D concept design. It employs the widely known SubD interface, which facilitates learning and provides intuitive shape controls; but without any of the troublesome extraordinary points or patch clusters.. The uniquely defined surfaces are smooth (often C2, approaching Class A) and are watertight for 3D printing. SuperD is VR/AR enabled.	SuperD: conceptual 3D modeling on mobiles	NA:NA	2018
Kevin J. Bruggeman:Skylar W. Wurster	This document explains the design, concept, and purpose behind The Hiatus System. This project aims to identify the possibility of using virtual reality to enhance the effectiveness of mindfulness based stress reduction (MBSR) practice on individuals with low cognitive memory. The Hiatus System was developed from the ongoing research at The Ohio State University on Virtual Healing Spaces. Healing Spaces, a phrase coined by Dr. Esther Sternberg, is a space that promotes a stress reductive state. This virtual experience is designed to attain and maintain user attentions towards the meditative practice. The hypothesis is that the virtual environment, combined with real time biofeedback of the breath, will create a system that can effectively teach the user how to meditate and reduce stress.	The Hiatus system: virtual healing spaces: low dose mindfulness based stress reduction virtual reality application	NA:NA	2018
Tobias Klein	3D printing allows unprecedented freedom in the design and manufacturing of even the most geometric complex forms---seemingly through a simple click of a button. In comparison, the making of glass is an analogue craftsmanship, coordinating an intricate interplay of individual tools and personal skills, giving shape to a material during the short time of its temperature-based plasticity. The two artworks discussed in this article, Augmented Fauna and Glass Mutations, were created during the artist's residence at the Pilchuck Glass School and articulate a synthesis between digital workflows and traditional craft processes to establish a digital craftsmanship.	Augmented fauna and glass mutations: a dialogue between material and technique in glassblowing and 3D printing: best paper award	NA	2018
Haru Hyunkyung Ji:Graham Wakefield	Inhabitat is a mixed-reality artwork in which participants become part of an imaginary ecology through three simultaneous perspectives of scale and agency; three distinct ways to see with other eyes. This imaginary world was exhibited at a children's science museum for five months, using an interactive projection-augmented sculpture, a large screen and speaker array, and a virtual reality head-mounted display. This paper documents the work's motivations and design contributions, along with accounts of visitors' playful engagements and reflections within the complex interconnectivity of an artificial nature.	Inhabitat: an imaginary ecosystem in a children's science museum	NA:NA	2018
Todd Berreth	There is a crisis in our communities about the tributes to a shared civic life represented in existing public artwork and monuments. Culture wars are being waged herein and appear increasingly unreconcilable. This paper discusses this moment and describes the range of strategies artists and designers have used to remediate these works. It presents a project description of an interactive artwork that suggests innovative approaches in this realm. The author introduces a conceptual model which served as inspiration for the piece that may be useful when discussing and designing such interventions.	Cop to conductor: negotiating and remapping meaning in existing public art	NA	2018
Nicole L'Huillier:Valentina Montero	Diastrophisms is a sound installation with a modular system that sends images through rhythmic patterns. It is built on a set of debris from the Alto Río building that was destroyed by the 27F earthquake in 2010 in Chile. Diastrophisms explores poetical, critical and political crossings between technology and matter in order to raise questions about the relationship between human beings and nature, to consider the construction of memory in a community by questioning the notion of monument, and to imagine new forms of communication in times of crisis.	Diastrophisms: visual and sound assembly in remembrance of an earthquake	NA:NA	2018
David Gochfeld:Corinne Brenner:Kris Layng:Sebastian Herscher:Connor DeFanti:Marta Olko:David Shinn:Stephanie Riggs:Clara Fernández-Vara:Ken Perlin	Holojam in Wonderland is a prototype of a new type of performance activity, "Immersive Mixed Reality Theater" (IMRT). With unique and novel properties possessed by neither cinema nor traditional theater, IMRT promises exciting new expressive possibilities for multi-user, participatory, immersive digital narratives. The authors describe the piece, the technology used to create it and some of the key aesthetic choices and takeaways.	Holojam in wonderland: immersive mixed reality theater	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Paul Charisse:Alex Counsell	This paper is an exploration of the processes used and ideas behind an animated full CGI feature film project that attempts to reach blockbuster production values, while retaining Art House sensibilities. It examines methods used to achieve these production values in an academic production environment and ways costs can be minimized while high quality levels are retained. It also examines the film's status as an Art House project, by comparing its narrative design and use of symbolism to existing works of Art House cinema.	Alienating the familiar with CGI: a recipe for making a full CGI art house animated feature	NA:NA	2018
Nicolas Henchoz:Allison Crank	To revive the Montreux Jazz Festival's archival live-concert footage, three immersive installations were designed using three different principles of augmentation, physicality and interaction. The primary aim was to engage the user in a new relationship with digitized heritage. Audience observations indicated a strong emotional connection to the content, the artist and the crowd, as well as the development of new social interactions. Experimentation showed close interaction between the three principles, while the three installations suggested methodologies for reviving audio-visual archives.	Digital heritage: bringing new life to the montreux jazz festival's audio-visual archives with immersive installations	NA:NA	2018
Courtney Starrett:Susan Reiser:Tom Pacio	Data materialization is a workflow developed to create 3D objects from data-informed designs. Building upon traditional metalwork and craft, and new technology's data visualization with generative art, this workflow expresses conceptually relevant data through 3D forms which are fabricated in traditional media. The process allows for the subtle application of data in visual art, allowing the aesthetic allure of the art object or installation to inspire intellectual intrigue. This paper describes the technical and creative process of Modern Dowry, a silver-plated 3D-print teapot on view at the Museum of the City of New York, June 2017--June 2018.	Data materialization: a hybrid process of crafting a teapot	NA:NA:NA	2018
Yuichiro Katsumoto	Humans use letters, which are two-dimensional static symbols, for communication. Writing these letters requires body movement as well as spending a certain amount of time; therefore, it can be demonstrated that a letter is a trajectory of movement and time. Based on this notion, the author conducted studies regarding multidimensional kinetic typography, primarily using robots to display a letter and visualize its time and movement simultaneously. This paper describes the project background and design of the three types of robotic displays that were developed and discusses possible expressions using robotic displays.	Robotype: studies of kinetic typography by robot display for expressing letters, time and movement	NA	2018
Brittany Myburgh	Examining the use of new media in works by Ruben Komangapik, Kent Monkman and the Wikiup Indigenous Knowledge Network reveals the diverse ways in which technologies are used to disrupt linear time and Western visions of history. New media works challenge those misleading stories that have been told about Canada's indigenous peoples and assert indigenous presence in both the digital and physical landscape. These artists employ QR codes, video and augmented reality to push artistic boundaries and create representations of the past and present.	Here and now: indigenous canadian perspectives and new media in works by ruben komangapik, kent monkman and adrian duke	NA	2018
Yiyun Kang	This paper investigates CASTING, Yiyun Kang's site-specific projection mapping installation at the Victoria and Albert Museum in London, U.K., and the acquisition of the piece by the V&A in the following year. It identifies how CASTING developed distinctive properties in the field of projected moving-image installation artworks and how these novel characteristics were reflected in the acquisition by the V&A.	Casting: site-specific projection mapping installation	NA	2018
Daniel Temkin	Coding, the translating of human intent into logical steps, reinforces a compulsive way of thinking, as described in Joseph Weitzenbaum's "Science and the Compulsive Programmer" (1976). Two projects by the author, Entropy (2010) and FatFinger (2017), challenge this by encouraging gestural approaches to code. In the Entropy programming language, data becomes slightly more approximate each time it is used, drifting from its original values, forcing programmers to be less precise. FatFinger, a Javascript dialect, allows the programmer to misspell code and interprets it as the closest runnable variation, strategically guessing at the programmer's intent.	Entropy and fatfinger: challenging the compulsiveness of code with programmatic anti-styles	NA	2018
Daniel C. Howe:Qianxun Chen:Zong Chen	Advertising Positions integrates 3D scanning, motion capture, novel image mapping algorithms and custom animation to create data portraits from the advertisements served by online trackers. Project volunteers use bespoke software to harvest the ads they receive over months of browsing. When enough ads have been collected, the volunteer is interviewed, 3D scanned and motion captured. Each ad is then mapped to a single polygon on the textured skin of their virtual avatar. Outcomes have been displayed as 2D/3D images, animations and interactive installations.	Advertising positions: data portraiture as aesthetic critique	NA:NA:NA	2018
Jason Edward Lewis:Skawennati	We started reading science fiction as teenagers. We fell in love with the fantastic worlds, the strange societies, the alien cultures and the amazing technologies. As we got older, though, we began to notice the lack of Native people in those futures. In fact, there were barely any nonwhite people at all.	The future is indigenous	NA:NA	2018
Matthew Allen	Should we see the early development of computer-aided design as an aesthetic movement? Just as eighteenth-century England had picturesque gardens and the world of social media today has spawned its own universe of visual conventions (to take two examples at random), was there such a thing as a "computational aesthetics" some 50 years ago? These are questions not about "computer art" per se, but about how a new visual culture might emerge alongside new practices and new concepts. They are particularly tricky questions to ask of early computational images because such images come from an era when people were eager to frame their work as scientific research, and aesthetics was often ruled out of bounds.	The computational imagination: notes on the exhibition designing the computational image, imagining computational design	NA	2018
Ernest Edmonds	In this note I describe my personal development of art systems over 50 years. In all of this work I have used computers and computational processes both to make the works and to advance my conception of art. This history is marked by a trace of publications in the journal Leonardo, itself being 50 years old. I will relate the story with specific reference to these publications. Each of the following sections relates to one Leonardo publication and includes quotations from that paper. In the earlier writing "he" or "him" is used sometimes to refer to a person who could well be female, as used to be the custom.	Art systems: 1968 to 2018	NA	2018
Daniel Cardoso Llach	"Archaeology of CAD" is an ongoing project that examines the origins of Computer-Aided Design by bringing to life some of its pioneering technologies, which were central to re-shape design practices in the image of computation during the second half of the twentieth century. On display at SIGGRAPH will be two interactive installations from this project: the reconstructions of Steven A. Coons's "Coons Patch" and of Ivan Sutherland's "Sketchpad." Drawing from primary archives and oral sources, these interactive installations playfully revisit these transformative technologies from the 1960s, and enable visitors to approximate the experience of designing with the first Computer-Aided Design systems. Developed with computational design students at Carnegie Mellon University using present-day hardware and software languages, these reconstructions are inquisitive artifacts of historical inquiry. By evoking the embodied experience of interacting with these technologies, they shed light on the new forms of human-machine work that emerged with the rise of interactive computing during the Cold-War years, and highlight the sensual and gestural dimensions of the "computer revolution." Along with the two reconstructions, a selection of rare handwritten notes and documents by Coons, and a selection of key contractual documents between the US Air Force and MIT, are displayed to offer glimpses of the institutional and intellectual context that motivated these foundational technologies of computational design.	Reconstructing "sketchpad" and the "coons patch": toward an archaeology of CAD	NA	2018
Skawennati	This sci-fi retelling of the Haudenosaunee (Iroquois) creation story reimagines Sky World as a futuristic utopia and Sky Woman as a brave astronaut and world-builder. When she learns that her planet is dying, Sky Woman volunteers to become the seed of the new world---an Earth yet covered in water. She Falls For Ages boldly mixes ancient storytelling with science fiction to connect the deep past with the far future.	She falls for ages	NA	2018
Nā 'Anae Mahiki	He Ao Hou is a point-and-click adventure game set in the far future, when Native Hawaiians have attained the next level of navigation---space travel. The gameplay is based on kānaka maoli (Native Hawaiian) stories and knowledge, and focuses in particular on the uses of the kukui nut, itself a symbol of knowledge.	He ao hou (a new world)	NA	2018
Amy Fredeen:Dima Veryovka	Never Alone (Kisima Inŋitchuŋa) is the product of an uncommon partnership of an Alaska Native community and game developers. Through all stages of development, members of both communities met extensively to ensure that all creative and business decisions were appropriately considered and supported the goals of all stakeholders. Throughout the game and in supporting material, players will hear directly from members of both communities who were instrumental in shaping the game.	Never alone: the art and the people of the story: E-line media	NA:NA	2018
Microsoft Garage:Andy Klein:Shawn Hunt	The Raven, the ultimate trickster, has become a cyborg. In this collaboration with Microsoft Vancouver, Shawn Hunt moves away from engaging with the handmade, exploring authenticity and our expectations of what it means to be indigenous through the removal of the hand-carved surface. The mask appropriates the traditional aspects of metamorphosis with the transformation from bird mask to human, yet in this adaptation the human mask has been altered, upgraded and merged with the machine. Incorporating aspects of technology, sound and space, each part of the work reflects Hunt's interest in how we understand and identify with the term indigenous.	Transformation mask	NA:NA:NA	2018
Danny Bazo:Marko Peljhan:Karl Yerkes	Somnium is a robotic and audiovisual installation that provides visitors with the ability to contemplate and experience exoplanetary discoveries, their macro- and microdimensions and the potential for life in our galaxy.	Somnium	NA:NA:NA	2018
Ruth West:Violet Johnson:I Chen Yeh:Zach Thomas:Eitan Mendelowitz:Lars Berg	INSTRUMENT | One Antarctic Night (IOAN) is a performative, multiparticipant reconfigurable artwork that engages open astronomical data in combination with data generated by robotic telescopes in Antarctica. IOAN places visitors inside a virtual star field of over 800,000 astronomical objects that form part of the Large Magellanic Cloud. This star field, created from observations in Antarctica and fused with additional data from multiple open astronomical repositories, is situated waist high within the virtual environment and stretches out beyond participants in all directions. Multiple participants can walk about the environment and collaboratively explore the star field by taking hold of the "fabric" of space, creating ripples and waves, and interacting with individual or sets of objects to create visual and auditory data remixes. The interaction places the astronomical data within a virtual reality visual and sonic remix engine that is a fundamental component of the artwork and is used to construct the virtual world. All graphics and spatialized ambisonic audio are procedurally generated from the data via real-time database queries. Our work incorporates machine learning approaches combined with granular and concatenative synthesis for generating the environment's unique soundscape. INSTRUMENT | One Antarctic Night evolved from our ongoing work in developing aesthetic data remixing and immersive data-driven experiences. Dataremix proposes the creation of the "datamade," a concept analogous to Duchamp's "readymade." IOAN is a meta-datamade in that it is a virtual instrument within which participants collaboratively create datamades through visual and auditory aesthetically driven remixes of astronomical data.	Instrument | one antarctic night	NA:NA:NA:NA:NA:NA	2018
Milton Sogabe:Fabio Oliveira Nunes:Carolina Peres:Soraya Braz:Rodrigo Dorta:Cleber Gazana:Mirian Steinberg:Melina Furquim:Daniel Malva:Fernando Luiz Fogliano	Considering the paradox between energy production and the contamination of the environment and reduction of biodiversity, cAt research group develops its work considering the discussion on sustainable sources of energy. The group's recent projects---Sopro (The Blow) and Toque (Touch)-have sought to aesthetically use the audience body's energy to interact and to animate the artworks. Simple devices are used to seek, in a kind of technological minimalist and interactive-art way, to raise public awareness of the issue of sustainability.	Sopro and toque (the blow and touch)	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Özge Samanci:Gabriel Caniglia	This interactive installation allows participants to control a digitally simulated ocean using only their brainwaves. Calm seas and storms alike are powered by the viewer's thoughts; the sheer act of concentration can conjure a squall or sunshine. Participants intentionally control their thinking while surrounded by the magnified consequence of their thoughts. Created in 2017, You Are the Ocean is about the theme of origins and one of the key concepts of Haudenosaunee and Anishnaabe cosmologies: "land is alive."	You are the ocean	NA:NA	2018
Alex Beim	Haven is a place of security and tranquility. Reminiscent of a mother's womb, it recalls our origins, where it all begins. The installation allows guests to leave their phones and all other technology at the door so they can be fully present without any of the prevailing modern distractions. They go in, spend some time, find themselves and maybe come out and start their day again. Fresh. A new beginning.	Haven	NA	2018
Shingo Kagami:Koichi Hashimoto	We demonstrate a 24-bit full-color projector that achieves over 2400-fps motion adaptability to a fast moving planar surface using single-chip DLP technology, which will be useful for projection mapping applications in highly dynamic scenes. The projector can be interfaced with a host PC via standard HDMI and USB without need of high computational burden.	A full-color single-chip-DLP projector with an embedded 2400-fps homography warping engine	NA:NA	2018
Azumi Maekawa:Ryuma Niiyama:Shunji Yamanaka	We present a biped robot which can move agiler than conventional robots. Our robot can generate bipedal walking motion automatically using the proposed method. By using a quadrotor for balance and movement it is possible to make an agiler movement, and generate a gait interactively and in real time according to the motion of the quadrotor using the optimized control policy of the legs. Our system takes the velocity of the quadrotor as an input and legs motions are produced so that the velocity of the foot in contact with the ground to zero, and bipedal walking motion is generated. The control policy is optimized using reinforcement learning with a physics engine.	Aerial-biped: a new physical expression by the biped robot using a quadrotor	NA:NA:NA	2018
Nitish Padmanaban:Robert Konrad:Gordon Wetzstein	Presbyopia, the loss of accommodation due to the stiffening of the crystalline lens, affects nearly 20% of the population worldwide. Traditional forms of presbyopia correction use fixed focal elements that inherently trade off field of view or stereo vision for a greater range of distances at which the wearer can see clearly. However, none of these offer the same natural refocusing enjoyed in youth. In this work, we built a new presbyopia correction, dubbed Autofocals, which externally mimics the natural accommodation response by combining data from eye trackers and a depth sensor, and then automatically drives focus-tunable lenses. In our testing, wearers generally reported that the Autofocals compare favorably with their own current corrective eyewear.	Autofocals: gaze-contingent eyeglasses for presbyopes	NA:NA:NA	2018
Yong-Ho Lee:Mincheol Kim:Hwang-Youn Kim:Dongmyoung Lee:Bum-Jae You	In the research, we propose a cost-effective 3-finger exoskeleton hand motion-capturing device and a physics engine-based hand interaction module for immersive experience in manipulation of virtual objects. The developed device provides 12 DOFs data of finger motion by a unique bevel-gear structure as well as the use of six 3D magnetic sensors. It shows a small error in relative distance between two fingertips less than 2 mm and allows the user to reproduce precise hand motion while processing the complex joint data in real-time. We synchronize hand motion with a physics engine-based interaction framework that includes a grasp interpreter and multi-modal feedback operation in virtual reality to minimize penetration of a hand into an object. The system enables feasibility of object manipulation as far as the needs go in various tasks in virtual environment.	CHICAP: low-cost hand motion capture device using 3D magnetic sensors for manipulation of virtual objects	NA:NA:NA:NA:NA	2018
Qian Zhou:Georg Hagemann:Sidney Fels:Dylan Fafard:Andrew Wagemakers:Chris Chamberlain:Ian Stavness	Fish Tank Virtual Reality (FTVR) creates a compelling 3D illusion for a single person by rendering to their perspective with head-tracking. However, typically, other participants cannot share in the experience since they see a weirdly distorted image when they look at the FTVR display making it difficult to work and play together. To overcome this problem, we have created CoGlobe: a large spherical FTVR display for multiple users. Using CoGlobe, Siggraph attendees will experience the latest advance of FTVR that supports multiple people co-located in a shared space working and playing together through two different multiplayer games and tasks. We have created a competitive two-person 3D Pong game (Figure 1b) for attendees to experience a highly interactive two-person game looking at the CoGlobe. Onlookers can also watch using a variation of mixed reality with a tracked mobile smartphone. Using a smartphone as a second screen registered to the same virtual world enables multiple people to interact together as well. We have also created a cooperative multi-person 3D drone game (Figure 1c) to illustrate cooperation in FTVR. Attendees will also see how effective co-located 3D FTVR is when cooperating on a complex 3D mental rotation (Figure 1d) and a path-tracing task (Figure 1a). CoGlobe overcomes the limited situation awareness of headset VR, while retaining the benefits of cooperative 3D interaction and thus is an exciting direction for the next wave of 3D displays for work and fun for Siggraph attendees to experience.	Coglobe: a co-located multi-person FTVR experience	NA:NA:NA:NA:NA:NA:NA	2018
Yu Matsuura:Naoya Koizumi	FairLift is an interaction system involving mid-air images, which are visible to the naked eye under and on a water surface. In this system, the water surface reflects the light from micro-mirror array plates, and a mid-air image appears. The system enables a user to interact with the mid-air image by controlling the image position of a light-source display from the water level measured with an ultrasonic sensor. The contributions of this system are enriching interaction with mid-air images and addressing the limitations of conventional water-display systems.	Fairlift: interaction with mid-air images on water surface	NA:NA	2018
MHD Yamen Saraiji:Tomoya Sasaki:Reo Matsumura:Kouta Minamizawa:Masahiko Inami	Effective communication is a key factor in social and professional contexts which involve sharing the skills and actions of more than one person. This research proposes a novel system to enable full body sharing over a remotely operated wearable system, allowing one person to dive into someone's else body. "Fusion" enables body surrogacy by sharing the same point of view of two-person: a surrogate and an operator, and it extends the limbs mobility and actions of the operator using two robotic arms mounted on the surrogate body. These arms can be used independently of the surrogate arms for collaborative scenarios or can be linked to surrogate's arms to be used in remote assisting and supporting scenarios. Using Fusion, we realize three levels of bodily driven communication: Direct, Enforced, and Induced. We demonstrate through this system the possibilities of truly embodying and transferring our body actions from one person to another, realizing true body communication.	Fusion: full body surrogacy for collaborative communication	NA:NA:NA:NA:NA	2018
Shunki Yamashita:Ryota Ishida:Arihide Takahashi:Hsueh-Han Wu:Hironori Mitake:Shoichi Hasegawa	Many people sometimes imagine if they can wield superhuman abilities like that appear in games and animation. Among these abilities, we focused particularly on representing the experience of arm stretching beyond the limits of the human body. We proposed a method for inducing a sense of arm stretching by designing the device attached to forearm and giving the user a visual cue by changing the body structure of the user's avatar in the virtual environment. Our device shifts the mass from the elbow to the wrist while stretching the skin of the forearm according to the animation in the virtual environment. The sensation of the elongation of the arm skin as well as the change in the weight of arm is thought to be the feeling when the arms are stretched out. As a result, we introduce these two mechanisms into our device, which allows the user to feel the sense of arm stretching.	Gum-gum shooting: inducing a sense of arm elongation via forearm skin-stretch and the change in the center of gravity	NA:NA:NA:NA:NA:NA	2018
Alon Grinshpoon:Shirin Sadri:Gabrielle J. Loeb:Carmine Elvezio:Samantha Siu:Steven K. Feiner	During a vascular intervention (a type of minimally invasive surgical procedure), physicians maneuver catheters and wires through a patient's blood vessels to reach a desired location in the body. Since the relevant anatomy is typically not directly visible in these procedures, virtual reality and augmented reality systems have been developed to assist in 3D navigation. Because both of a physician's hands may already be occupied, we developed an augmented reality system supporting hands-free interaction techniques that use voice and head tracking to enable the physician to interact with 3D virtual content on a head-worn display while leaving both hands available intraoperatively. We demonstrate how a virtual 3D anatomical model can be rotated and scaled using small head rotations through first-order (rate) control, and can be rigidly coupled to the head for combined translation and rotation through zero-order control. This enables easy manipulation of a model while it stays close to the center of the physician's field of view.	Hands-free augmented reality for vascular interventions	NA:NA:NA:NA:NA:NA	2018
Hwan Kim:HyeonBeom Yi:Richard Chulwoo Park:Woohun Lee	We developed a tactile actuator named HapCube that provides tangential and normal pseudo-force feedback on user's fingertip. The tangential feedback is generated by synthesizing two orthogonal asymmetric vibrations, and it simulates frictional force in any desired tangential directions. The normal feedback simulates tactile sensations when pressing various types of button. In addition, by combining the two feedbacks, it can produce frictional force and surface texture simultaneously.	Hapcube: a tactile actuator providing tangential and normal pseudo-force feedback on a fingertip	NA:NA:NA:NA	2018
Shunichi Kasahara	Visual augmentation to the real environment has potential not only to display information but also to provide a new perception of the physical world. However, the currently available mixed reality technologies could not provide enough angle of view. Thus, we introduce "Headlight", a wearable projector system that provides wide egocentric visual augmentation. Our system consists of a small laser projector with a fish-eye wider conversion lens, a headphone and a pose tracker. HeadLight provides projection angle with approx. 105 deg. horizontal and 55 deg. vertical from the point of view of the user. In this system, the three-dimensional virtual space that is consistent with the physical environment is rendered with a virtual camera based on tracking information of the device. By processing inverse correction of the lens distortion and projecting the rendered image from the projector, HeadLight performs consistent visual augmentation in the real world. With Headlight, we envision that physical phenomena that human could not perceive will be perceived through visual augmentation.	Headlight: egocentric visual augmentation by wearable wide projector	NA	2018
Takashi Yamamoto:Tamaki Nishino:Hideki Kajima:Mitsunori Ohta:Koichi Ikeda	There has been an increasing interest in mobile manipulators that is capable of performing physical work in living spaces worldwide, corresponding to population aging with declining birth rates with the expectation of improving quality of life (QOL). Research and development is a must in intelligent sensing and software which enable advanced recognition, judgment, and motion to realize household work by robots. In order to accelerate this research, we have developed a compact and safe research platform, Human Support Robot (HSR), which can be operated in an actual home environment. We assume that overall R&D will accelerate by using a common robot platform among many researchers since that enables them to share their research results. In this paper, we introduce HSR design and its utilization.	Human support robot (HSR)	NA:NA:NA:NA:NA	2018
Tomoya Sasaki:Richard Sahala Hartanto:Kao-Hua Liu:Keitarou Tsuchiya:Atsushi Hiyama:Masahiko Inami	We present LevioPole, a rod-like device that provides mid-air haptic feedback for full-body interaction in virtual reality, augmented reality, or other daily activities. The device is constructed from two rotor units, which are designed using propellers, motors, speed controllers, batteries, and sensors, allowing portability and ease of use. Having each group of rotor units on both ends of the pole, these rotors generate both rotational and linear forces that can be driven according to the target application. In this paper, we introduce example applications in both VR and physical environment; embodied gaming with haptic feedback and walking navigation in a specific direction.	Leviopole: mid-air haptic interactions using multirotor	NA:NA:NA:NA:NA:NA	2018
Yoichi Ochiai:Kazuki Otao:Yuta Itoh:Shouki Imai:Kazuki Takazawa:Hiroyuki Osone:Atsushi Mori:Ippei Suzuki	Retinal projection is required for xR applications that can deliver immersive visual experience throughout the day. If general-purpose retinal projection methods can be realized at a low cost, not only could the image be displayed on the retina using less energy, but there is also a possibility of cutting off the weight of projection unit itself from the AR goggles. Several retinal projection methods have been previously proposed. Maxwellian optics based retinal projection was proposed in 1990s [Kollin 1993]. Laser scanning [Liao and Tsai 2009], laser projection using spatial light modulator (SLM) or holographic optical elements were also explored [Jang et al. 2017]. In the commercial field, QD Laser1 with a viewing angle of 26 degrees is available. However, as the lenses and iris of an eyeball are in front of the retina, which is a limitation of a human eyeball, the proposal of retinal projection is generally fraught with narrow viewing angles and small eyebox problems. Due to these problems, retinal projection displays are still a rare commodity because of their difficulty in optical schematics design.	Make your own retinal projector: retinal near-eye displays via metamaterials	NA:NA:NA:NA:NA:NA:NA:NA	2018
Matthew O'Toole:David B. Lindell:Gordon Wetzstein	Non-line-of-sight (NLOS) imaging aims at recovering the shape of objects hidden outside the direct line of sight of a camera. In this work, we report on a new approach for acquiring time-resolved measurements that are suitable for NLOS imaging. The system uses a confocalized single-photon detector and pulsed laser. As opposed to previously-proposed NLOS imaging systems, our setup is very similar to LIDAR systems used for autonomous vehicles and it facilitates a closed-form solution of the associated inverse problem, which we derive in this work. This algorithm, dubbed the Light Cone Transform, is three orders of magnitude faster and more memory efficient than existing methods. We demonstrate experimental results for indoor and outdoor scenes captured and reconstructed with the proposed confocal NLOS imaging system.	Real-time non-line-of-sight imaging	NA:NA:NA	2018
Takayuki Todo	SEER (Simulative Emotional Expression Robot) is an animatronic humanoid robot that generates gaze and emotional facial expressions to improve animativity, lifelikeness, and impresssiveness by the integrated design of modeling, mechanism, materials, and computing. The robot can simulated a user?s movement, gaze, and facial expressions detected by a camera sensor. This system can be applied to puppetry, telepresence avatar, and interactive automation.	SEER: simulative emotional expression robot	NA	2018
Hiroaki Yano:Tomohiro Yendo	We present an optical system design for a 3D display that is spherical, full-parallax, and occlusion-capable with a wide viewing zone and no head tracking. The proposed system provides a new approach for the 3D display and thereby addresses limitations of the conventional light-field display structure. Specifically, a spherical full-parallax light-field display is difficult to achieve because it is challenging to curve the conventional structure of the light-field displays. The key elements of the system are a specially designed ball mirror and a high-speed projector. The ball mirror uniaxially rotates and reflects rays from the projector to various angles. The intensities of these rays are controlled by the projector. Rays from a virtual object inside the ball mirror are reconstructed, and the system acts as a light-field display based on the time-division multiplexing method. We implemented this ball mirror by 3D printing and metal plating. The prototype successfully displays a 3D image and the system feasibility is confirmed. Our system is thus suitable for displaying 3D images to many viewers simultaneously and it can be effectively employed as in art or advertisement installation.	Spherical full-parallax light-field display using ball of fly-eye mirror	NA:NA	2018
Kishore Rathinavel:Praneeth Chakravarthula:Kaan Akşit:Josef Spjut:Ben Boudaoud:Turner Whitted:David Luebke:Henry Fuchs	The design challenges of see-through near-eye displays can be mitigated by specializing an augmented reality device for a particular application. We present a novel optical design for augmented reality near-eye displays exploiting 3D stereolithography printing techniques to achieve similar characteristics to progressive prescription binoculars. We propose to manufacture inter-changeable optical components using 3D printing, leading to arbitrary shaped static projection screen surfaces that are adaptive to the targeted applications. We identify a computational optical design methodology to generate various optical components accordingly, leading to small compute and power demands. To this end, we introduce our augmented reality prototype with a moderate form-factor, large field of view. We have also presented that our prototype is promising high resolutions for a foveation technique using a moving lens in front of a projection system. We believe our display technique provides a gate-way to application-adaptive, easily replicable, customizable, and cost-effective near-eye display designs.	Steerable application-adaptive near eye displays	NA:NA:NA:NA:NA:NA:NA:NA	2018
Kazuma Aoyama:Kenta Sakurai:Akinobu Morishima:Taro Maeda:Hideyuki Ando	Galvanic tongue stimulation (GTS) is a technology used to change and induce taste sensation with electrical stimulation. It is known from previous studies that cathodal current stimulation induces two types of effects. The first is the taste suppression that renders the taste induced by electrolytic materials weaker during the stimulation. The second is taste enhancement that makes taste stronger shortly after ending the stimulation. These effects stand a better possibility to affect the ability to emulate taste, which can ultimately control the strength of taste sensation with freedom. Taste emulation has been considered in various applications, such as in virtual reality, in diet efforts, and in other applications. However, conventional GTS is associated with some problems. For example, the duration of taste enhancement is too short for use in diet efforts, and it necessitates the attachment of electrodes in the mouth. Moreover, conventional GTS cannot induce taste at the throat but at the mouth instead. Thus, this study and our associated demonstration introduces some approaches to address and solve these problems. Our approaches realize that taste changes voluntarily and the effects persist for lengthy periods of time.	Taste controller: galvanic chin stimulation enhances, inhibits, and creates tastes	NA:NA:NA:NA:NA	2018
Jotaro Shigeyama:Takeru Hashimoto:Shigeo Yoshida:Taiju Aoki:Takuji Narumi:Tomohiro Tanikawa:Michitaka Hirose	We introduce a dynamic weight moving VR controller for 2d haptic shape rendering using a haptic shape illusion. This allows users to perceive the feeling of various shapes in virtual space with a single controller. We designed a device that drives weight on a 2d planar area to alter mass properties of the hand-held controller. Our user study showed that the system succeeded in providing shape perception over a wide range.	Transcalibur: weight moving VR controller for dynamic rendering of 2D shape using haptic shape illusion	NA:NA:NA:NA:NA:NA:NA	2018
Kazuki Otao:Yuta Itoh:Kazuki Takazawa:Hiroyuki Osone:Yoichi Ochiai	We present a transmissive mirror device (TMD) based near-eye see-through displays with a wide viewing angle and high resolution for virtual reality and augmented reality. In past years, many optical elements, such as transmissive liquid-crystal display (LCD), half-mirror, waveguide and holographic optical element (HOE) have been adopted for near-eye see-through displays. However, it is difficult to obtain wide field of view with see-through capability for beginner developer. To accomplish this, we develop a simple see-through display that easily setup from a combination of off-the-shelf HMD and TMD. In the proposed method, we render "virtual lens," which has the same function as the HMD lens in the air. By using TMD, it is possible to shorten the optical length between the virtual lens and the eyeball. Therefore, the aerial lens provides a wide viewing angle with see-through capability. We demonstrate a prototype with a diagonal viewing angle of 100 degrees.	Transmissive mirror device based near-eye displays with wide field of view	NA:NA:NA:NA:NA	2018
Pierre-Yves Laffont:Ali Hasnain:Pierre-Yves Guillemet:Samuel Wirajaya:Joe Khoo:Deng Teng:Jean-Charles Bazin	The vergence-accommodation conflict is a fundamental cause of discomfort in today's Virtual and Augmented Reality (VR/AR). We present a novel software platform and hardware for varifocal head-mounted displays (HMDs) to generate consistent accommodation cues and account for the user's prescription. We investigate multiple varifocal optical systems and propose the world's first varifocal mobile HMD based on Alvarez lenses. We also introduce a varifocal rendering pipeline, which corrects for distortion introduced by the optical focus adjustment, approximates retinal blur, incorporates eye tracking and leverages on rendered content to correct noisy eye tracking results. We demonstrate the platform running in compact VR headsets and present initial results in video pass-through AR.	Verifocal: a platform for vision correction and accommodation in head-mounted displays	NA:NA:NA:NA:NA:NA:NA	2018
Simon Spielmann:Volker Helzle:Andreas Schuster:Jonas Trottnow:Kai Götz:Patricia Rohr	The work on intuitive Virtual Production tools at Filmakademie Baden-Württemberg has focused on an open platform tied to existing film creation pipelines. The Virtual Production Editing Tools (VPET) started in a former project on Virtual Production funded by the European Union and are published and constantly updated on the open source software development platform Github. We introduce an intuitive workflow where Augmented Reality, inside-out tracking and real-time color keying can be applied on the fly to extend a real movie set with editable, virtual extensions in a collaborative setup.	VPET: virtual production editing tools	NA:NA:NA:NA:NA:NA	2018
Theresa-Marie Rhyne:Nicholas (Nick) Bazarian:Jose Echevarria:Michael J. Murdoch:Danielle Feinberg	Designing and capturing a color scheme for a digital media composition is an important step in the creation pipeline. Whether it is an immersive experience, animation or visualization, color selection is key to conveying the message or story. In this panel, we assemble a group of color experts, aka "Color Mavens," to convey and define color appearance and colorization methods. Each panelist represents a particular color advice approach whether it includes a recommended set of guidelines for color appearance, suggested color schemes, a tool for color capture, an application for color palette creation or tips from colorization experiences. Each panelist will highlight their methods with a team discussion about optimal colorization approaches to follow. The panel will also identify gaps in our understanding about the use of color in digital media composition as well as identifying future application and research directions.	Color mavens advise on digital media creation and tools: SIGGRAPH 2018 panel	NA:NA:NA:NA:NA	2018
Dmytro Korolov:Doug Roble:Jean-Charles Bazin:Renaldas Zioma:Rob Pieke:Jeff Kember:David Luebke	This panel discusses trends and prospects for using AI tools in the VFX pipeline. Panel experts will talk about the current AI tools that work in the industry, give answers to questions and their vision of their technology development.	Future of artificial intelligence and deep learning tools for VFX	NA:NA:NA:NA:NA:NA:NA	2018
Ryan Ulyate:David Bianciardi:Judith Crow:Greg Hermanovic	Twenty years ago in Orlando The Interactive Dance Club (IDC) brought together the SIGGRAPH community in a grand social experiment. For four nights conference attendees gathered to participate in creating a dynamic confluence of music, computer graphics and lighting - all driven by those who danced. Computers were big and expensive back then! There were no iPhones, no bluetooth, no wearables lighter than a small car. So why were we crazy enough to think we could pull this off? To what lengths did we go to make this happen?	Interactive dance club '98: a legend in the making!	NA:NA:NA:NA	2018
Matt Pharr:Brent Burley:Per Christensen:Marcos Fajardo:Luca Fascione:Christopher Kulla	Over the past decade, production rendering has moved from primarily using Reyes-based algorithms to being based on path tracing and physically based approaches. The developers of five of the most significant production renderers have each recently written comprehensive systems papers on their renderers, describing the challenges of modern production rendering and the systems they have respectively built to solve them. In May 2018, these papers were published in a special issue of ACM Transactions on Graphics. In this panel, the developers of these renderers will go into depth on how the challenges of production rendering have influenced the systems they've built and how they have developed new techniques to make path tracing viable in practice. These systems are remarkably varied in some of their core design decisions; the panelists will also compare and contrast their own design decisions with respect to topics like precomputation versus runtime computation, RGB versus spectral rendering, and out-of-core rendering versus requiring scene geometry to fit into memory.	Design and implementation of modern production renderers	NA:NA:NA:NA:NA:NA	2018
Henry Fuchs:Ivan E. Sutherland:Robert F. Sproull:Charles L. Seitz:Frederick P. Brooks:H. Quintin Foster, Jr.	NA	[email protected]: celebrating ivan sutherland's 1968 head-mounted 3D display system	NA:NA:NA:NA:NA:NA	2018
Pol Jeremias-Vila:Kim Libreri:Guido Quaroni:Natalya Tatarchuk:Damien Fagnou	The movie frames we see when watching films today are typically generated using an offline renderer, which could take multiple hours per frame to produce. This time-consuming process makes previewing content difficult, so creators have worked around this issue by utilizing real-time graphics to iterate their content more efficiently. While real-time graphics could be used for previewing, the level of quality had not yet reached the standards needed for a final movie frame. However, over the years, the previews generated by real-time graphics have gotten more refined and have even enabled pre-visualizations using virtual reality. This provides even more context to creative minds. In addition to the ever growing use of real-time graphics, the quality of technology has improved, potentially allowing for the generation of final frames, that can start to look like a movie. This panel will bring together engineers, artists and executives representing various areas of expertise to provide information about how real-time graphics are being used in multiple studios today. They will also describe some of their challenges and how they foresee the future of real-time graphics in film.	The present and future of real-time graphics in film	NA:NA:NA:NA:NA	2018
Mark Wiebe:Jason Fotter:Dan Wexler:Panos Zompolas:Phil Peterson	The Visual Effects industry is presently grappling with how to best take advantage of cloud computing, a technology which has transformed the practice of software in many industries. The ability to treat the provisioning and configuration of render farm hardware with the flexibility of software is highly attractive, but the learning curve can be challenging to juggle with busy production schedules. Fully managed web services have also taken hold in some parts of the production pipeline, with more likely to come. Software vendors creating web services need to enable studios with the right combination of security, backwards compatibility, ease of use, and programmability, so they may adopt these technologies without interrupting their Visual Effects production. In this panel, we will discuss current usage of cloud computing in Visual Effects, how it is trending, and how it interacts with other factors like the growth of VFX-oriented open source software. Studios range in their use of render farms from full on-premises setups through hybrid setups blending their premises with the cloud to all-in cloud rendering. We will explore how fast internet connections and efficient streaming desktop technology are enabling full end-to-end production to move to the cloud with Zero Client workstations. Our panel consists of a diverse group of technologists, representing both Visual Effects studios and the creators of software for the industry.	Visual effects in the age of the cloud	NA:NA:NA:NA:NA	2018
Yufeng Zhu:Robert Bridson:Danny M. Kaufman	Optimizing distortion energies over a mesh, in two or three dimensions, is a common and critical problem in physical simulation and geometry processing. We present three new improvements to the state of the art: a barrier-aware line-search filter that cures blocked descent steps due to element barrier terms and so enables rapid progress; an energy proxy model that adaptively blends the Sobolev (inverse-Laplacian-processed) gradient and L-BFGS descent to gain the advantages of both, while avoiding L-BFGS's current limitations in distortion optimization tasks; and a characteristic gradient norm providing a robust and largely mesh- and energy-independent convergence criterion that avoids wrongful termination when algorithms temporarily slow their progress. Together these improvements form the basis for Blended Cured Quasi-Newton (BCQN), a new distortion optimization algorithm. Over a wide range of problems over all scales we show that BCQN is generally the fastest and most robust method available, making some previously intractable problems practical while offering up to an order of magnitude improvement in others.	Blended cured quasi-newton for distortion optimization	NA:NA:NA	2018
Ligang Liu:Chunyang Ye:Ruiqi Ni:Xiao-Ming Fu	We propose a novel approach, called Progressive Parameterizations, to compute foldover-free parameterizations with low isometric distortion on disk topology meshes. Instead of using the input mesh as a reference to define the objective function, we introduce a progressive reference that contains bounded distortion to the parameterized mesh and is as close as possible to the input mesh. After optimizing the bounded distortion energy between the progressive reference and the parameterized mesh, the parameterized mesh easily approaches the progressive reference, thereby also coming close to the input. By iteratively generating the progressive reference and optimizing the bounded distortion energy to update the parameterized mesh, our algorithm achieves high-quality parameterizations with strong practical reliability and high efficiency. We have demonstrated that our algorithm succeeds on a massive test data set containing over 20712 complex disk topology meshes. Compared to the state-of-the-art methods, our method has achieved higher computational efficiency and practical reliability.	Progressive parameterizations	NA:NA:NA:NA	2018
Yue Peng:Bailin Deng:Juyong Zhang:Fanyu Geng:Wenjie Qin:Ligang Liu	Many computer graphics problems require computing geometric shapes subject to certain constraints. This often results in non-linear and non-convex optimization problems with globally coupled variables, which pose great challenge for interactive applications. Local-global solvers developed in recent years can quickly compute an approximate solution to such problems, making them an attractive choice for applications that prioritize efficiency over accuracy. However, these solvers suffer from lower convergence rate, and may take a long time to compute an accurate result. In this paper, we propose a simple and effective technique to accelerate the convergence of such solvers. By treating each local-global step as a fixed-point iteration, we apply Anderson acceleration, a well-established technique for fixed-point solvers, to speed up the convergence of a local-global solver. To address the stability issue of classical Anderson acceleration, we propose a simple strategy to guarantee the decrease of target energy and ensure its global convergence. In addition, we analyze the connection between Anderson acceleration and quasi-Newton methods, and show that the canonical choice of its mixing parameter is suitable for accelerating local-global solvers. Moreover, our technique is effective beyond classical local-global solvers, and can be applied to iterative methods with a common structure. We evaluate the performance of our technique on a variety of geometry optimization and physics simulation problems. Our approach significantly reduces the number of iterations required to compute an accurate result, with only a slight increase of computational cost per iteration. Its simplicity and effectiveness makes it a promising tool for accelerating existing algorithms as well as designing efficient new algorithms.	Anderson acceleration for geometry optimization and physics simulation	NA:NA:NA:NA:NA:NA	2018
Gavin Barill:Neil G. Dickson:Ryan Schmidt:David I. W. Levin:Alec Jacobson	Inside-outside determination is a basic building block for higher-level geometry processing operations. Generalized winding numbers provide a robust answer for triangle meshes, regardless of defects such as self-intersections, holes or degeneracies. In this paper, we further generalize the winding number to point clouds. Previous methods for evaluating the winding number are slow for completely disconnected surfaces, such as triangle soups or-in the extreme case- point clouds. We propose a tree-based algorithm to reduce the asymptotic complexity of generalized winding number computation, while closely approximating the exact value. Armed with a fast evaluation, we demonstrate the winding number in a variety of new applications: voxelization, signing distances, generating 3D printer paths, defect-tolerant mesh booleans and point set surfaces.	Fast winding numbers for soups and clouds	NA:NA:NA:NA:NA	2018
Yajie Yan:David Letscher:Tao Ju	We present a novel algorithm for computing the medial axes of 3D shapes. We make the observation that the medial axis of a voxel shape can be simply yet faithfully approximated by the interior Voronoi diagram of the boundary vertices, which we call the voxel core. We further show that voxel cores can approximate the medial axes of any smooth shape with homotopy equivalence and geometric convergence. These insights motivate an algorithm that is simple, efficient, numerically stable, and equipped with theoretical guarantees. Compared with existing voxel-based methods, our method inherits their simplicity but is more scalable and can process significantly larger inputs. Compared with sampling-based methods that offer similar theoretical guarantees, our method produces visually comparable results but more robustly captures the topology of the input shape.	Voxel cores: efficient, robust, and provably good approximation of 3D medial axes	NA:NA:NA	2018
Yijing Li:Jernej Barbič	Self-intersecting, or nearly self-intersecting, meshes are commonly found in 2D and 3D computer graphics practice. Self-intersections occur, for example, in the process of artist manual work, as a by-product of procedural methods for mesh generation, or due to modeling errors introduced by scanning equipment. If the space bounded by such inputs is meshed naively, the resulting mesh joins ("glues") self-overlapping parts, precluding efficient further modeling and animation of the underlying geometry. Similarly, near self-intersections force the simulation algorithm to employ an unnecessarily detailed mesh to separate the nearly self-intersecting regions. Our work addresses both of these challenges, by giving an algorithm to generate an "un-glued" simulation mesh, of arbitrary user-chosen resolution, that properly accounts for self-intersections and near self-intersections. In order to achieve this result, we study the mathematical concept of immersion, and give a deterministic and constructive algorithm to determine if the input self-intersecting triangle mesh is the boundary of an immersion. For near self-intersections, we give a robust algorithm to properly duplicate mesh elements and correctly embed the underlying geometry into the mesh element copies. Both the self-intersections and near self-intersections are combined into one algorithm that permits successful meshing at arbitrary resolution. Applications of our work include volumetric shape editing, physically based simulation and animation, and volumetric weight and geodesic distance computation on self-intersecting inputs.	Immersion of self-intersecting solids and surfaces	NA:NA	2018
Roee Lazar:Nadav Dym:Yam Kushinsky:Zhiyang Huang:Tao Ju:Yaron Lipman	Surface reconstruction is one of the central problems in computer graphics. Existing research on this problem has primarily focused on improving the geometric aspects of the reconstruction (e.g., smoothness, features, element quality, etc.), and little attention has been paid to ensure it also has desired topological properties (e.g., connectedness and genus). In this paper, we propose a novel and general optimization method for surface reconstruction under topological constraints. The input to our method is a prescribed genus for the reconstructed surface, a partition of the ambient volume into cells, and a set of possible surface candidates and their associated energy within each cell. Our method computes one candidate per cell so that their union is a connected surface with the prescribed genus that minimizes the total energy. We formulate the task as an integer program, and propose a novel solution that combines convex relaxations within a branch and bound framework. As our method is oblivious of the type of input cells, surface candidates, and energy, it can be applied to a variety of reconstruction scenarios, and we explore two of them in the paper: reconstruction from cross-section slices and iso-surfacing an intensity volume. In the first scenario, our method outperforms an existing topology-aware method particularly for complex inputs and higher genus constraints. In the second scenario, we demonstrate the benefit of topology control over classical topology-oblivious methods such as Marching Cubes.	Robust optimization for topological surface reconstruction	NA:NA:NA:NA:NA:NA	2018
Mingming He:Dongdong Chen:Jing Liao:Pedro V. Sander:Lu Yuan	We propose the first deep learning approach for exemplar-based local colorization. Given a reference color image, our convolutional neural network directly maps a grayscale image to an output colorized image. Rather than using hand-crafted rules as in traditional exemplar-based methods, our end-to-end colorization network learns how to select, propagate, and predict colors from the large-scale data. The approach performs robustly and generalizes well even when using reference images that are unrelated to the input grayscale image. More importantly, as opposed to other learning-based colorization methods, our network allows the user to achieve customizable results by simply feeding different references. In order to further reduce manual effort in selecting the references, the system automatically recommends references with our proposed image retrieval algorithm, which considers both semantic and luminance information. The colorization can be performed fully automatically by simply picking the top reference suggestion. Our approach is validated through a user study and favorable quantitative comparisons to the-state-of-the-art methods. Furthermore, our approach can be naturally extended to video colorization. Our code and models are freely available for public use.	Deep exemplar-based colorization	NA:NA:NA:NA:NA	2018
Tae-Hoon Kim:Sang Il Park	A fully automatic method for descreening halftone images is presented based on convolutional neural networks with end-to-end learning. Incorporating context level information, the proposed method not only removes halftone artifacts but also synthesizes the fine details lost during halftone. The method consists of two main stages. In the first stage, intrinsic features of the scene are extracted, the low-frequency reconstruction of the image is estimated, and halftone patterns are removed. For the intrinsic features, the edges and object-categories are estimated and fed to the next stage as strong visual and contextual cues. In the second stage, fine details are synthesized on top of the low-frequency output based on an adversarial generative model. In addition, the novel problem of rescreening is addressed, where a natural input image is halftoned so as to be similar to a separately given reference halftone image. To this end, a two-stage convolutional neural network is also presented. Both networks are trained with millions of before-and-after example image pairs of various halftone styles. Qualitative and quantitative evaluations are provided, which demonstrates the effectiveness of the proposed methods.	Deep context-aware descreening and rescreening of halftone images	NA:NA	2018
Yang Zhou:Zhen Zhu:Xiang Bai:Dani Lischinski:Daniel Cohen-Or:Hui Huang	The real world exhibits an abundance of non-stationary textures. Examples include textures with large scale structures, as well as spatially variant and inhomogeneous textures. While existing example-based texture synthesis methods can cope well with stationary textures, non-stationary textures still pose a considerable challenge, which remains unresolved. In this paper, we propose a new approach for example-based non-stationary texture synthesis. Our approach uses a generative adversarial network (GAN), trained to double the spatial extent of texture blocks extracted from a specific texture exemplar. Once trained, the fully convolutional generator is able to expand the size of the entire exemplar, as well as of any of its sub-blocks. We demonstrate that this conceptually simple approach is highly effective for capturing large scale structures, as well as other non-stationary attributes of the input exemplar. As a result, it can cope with challenging textures, which, to our knowledge, no other existing method can handle.	Non-stationary texture synthesis by adversarial expansion	NA:NA:NA:NA:NA:NA	2018
Nicholas J. Weidner:Kyle Piddington:David I. W. Levin:Shinjiro Sueda	We resolve the longstanding problem of simulating the contact-mediated interaction of cloth and sharp geometric features by introducing an Eulerian-on-Lagrangian (EOL) approach to cloth simulation. Unlike traditional Lagrangian approaches to cloth simulation, our EOL approach permits bending exactly at and sliding over sharp edges, avoiding parasitic locking caused by over-constraining contact constraints. Wherever the cloth is in contact with sharp features, we insert EOL vertices into the cloth, while the rest of the cloth is simulated in the standard Lagrangian fashion. Our algorithm manifests as new equations of motion for EOL vertices, a contact-conforming remesher, and a set of simple constraint assignment rules, all of which can be incorporated into existing state-of-the-art cloth simulators to enable smooth, inequality-constrained contact between cloth and objects in the world.	Eulerian-on-lagrangian cloth simulation	NA:NA:NA:NA	2018
Yun (Raymond) Fei:Christopher Batty:Eitan Grinspun:Changxi Zheng	We propose a method for simulating the complex dynamics of partially and fully saturated woven and knit fabrics interacting with liquid, including the effects of buoyancy, nonlinear drag, pore (capillary) pressure, dripping, and convection-diffusion. Our model evolves the velocity fields of both the liquid and solid relying on mixture theory, as well as tracking a scalar saturation variable that affects the pore pressure forces in the fluid. We consider the porous microstructure implied by the fibers composing individual threads, and use it to derive homogenized drag and pore pressure models that faithfully reflect the anisotropy of fabrics. In addition to the bulk liquid and fabric motion, we derive a quasi-static flow model that accounts for liquid spreading within the fabric itself. Our implementation significantly extends standard numerical cloth and fluid models to support the diverse behaviors of wet fabric, and includes a numerical method tailored to cope with the challenging nonlinearities of the problem. We explore a range of fabric-water interactions to validate our model, including challenging animation scenarios involving splashing, wringing, and collisions with obstacles, along with qualitative comparisons against simple physical experiments.	A multi-scale model for simulating liquid-fabric interactions	NA:NA:NA:NA	2018
Jie Li:Gilles Daviet:Rahul Narain:Florence Bertails-Descoubes:Matthew Overby:George E. Brown:Laurence Boissieux	Cloth dynamics plays an important role in the visual appearance of moving characters. Properly accounting for contact and friction is of utmost importance to avoid cloth-body and cloth-cloth penetration and to capture typical folding and stick-slip behavior due to dry friction. We present here the first method able to account for cloth contact with exact Coulomb friction, treating both cloth self-contacts and contacts occurring between the cloth and an underlying character. Our key contribution is to observe that for a nodal system like cloth, the frictional contact problem may be formulated based on velocities as primary variables, without having to compute the costly Delassus operator. Then, by reversing the roles classically played by the velocities and the contact impulses, conical complementarity solvers of the literature can be adapted to solve for compatible velocities at nodes. To handle the full complexity of cloth dynamics scenarios, we have extended this base algorithm in two ways: first, towards the accurate treatment of frictional contact at any location of the cloth, through an adaptive node refinement strategy; second, towards the handling of multiple constraints at each node, through the duplication of constrained nodes and the adding of pin constraints between duplicata. Our method allows us to handle the complex cloth-cloth and cloth-body interactions in full-size garments with an unprecedented level of realism compared to former methods, while maintaining reasonable computational timings.	An implicit frictional contact solver for adaptive cloth simulation	NA:NA:NA:NA:NA:NA:NA	2018
Huamin Wang	Being able to customize sewing patterns for different human bodies without using any pre-defined adjustment rule will not only improve the realism of virtual humans in the entertainment industry, but also deeply affect the fashion industry by making fast fashion and made-to-measure garments more accessible. To meet the requirement set by the fashion industry, a sewing pattern adjustment system must be both efficient and precise, which unfortunately cannot be achieved by existing techniques. In this paper, we propose to solve sewing pattern adjustment as a nonlinear optimization problem immediately, rather than in two phases: a garment shape optimization phase and an inverse pattern design phase as in previous systems. This allows us to directly minimize the objective function that evaluates the fitting quality of the garment sewn from a pattern, without any compromise caused by the nonexistence of the solution to inverse pattern design. To improve the efficiency of our system, we carry out systematic research on a variety of optimization topics, including pattern parametrization, initialization, an inexact strategy, acceleration, and CPU-GPU implementation. We verify the usability of our system through automatic grading tests and made-to-measure tests. Designers and pattern makers confirm that our pattern results are able to preserve design details and their fitting qualities are acceptable. In our computational experiment, the system further demonstrates its efficiency, reliability, and flexibility of handling various pattern designs. While our current system still needs to overcome certain limitations, we believe it is a crucial step toward fully automatic pattern design and adjustment in the future.	Rule-free sewing pattern adjustment with precision and efficiency	NA	2018
Jingwen Wang:Ravi Ramamoorthi	Spherical Harmonic (SH) lighting is widely used for real-time rendering within Precomputed Radiance Transfer (PRT) systems. SH coefficients are precomputed and stored at object vertices, and combined interactively with SH lighting coefficients to enable effects like soft shadows, interreflections, and glossy reflection. However, the most common PRT techniques assume distant, low-frequency environment lighting, for which SH lighting coefficients can easily be computed once per frame. There is currently limited support for near-field illumination and area lights, since it is non-trivial to compute the SH coefficients for an area light, and the incident lighting (SH coefficients) varies over the object geometry. We present an efficient closed-form solution for projection of uniform polygonal area lights to spherical harmonic coefficients of arbitrary order, enabling easy adoption of accurate area lighting in PRT systems, with no modifications required to the core PRT framework. Our method only requires computing zonal harmonic (ZH) coefficients, for which we introduce a novel recurrence relation. In practice, ZH coefficients are built up iteratively, with computation linear in the desired SH order. General SH coefficients can then be obtained by the recently developed sparse zonal harmonic rotation method.	Analytic spherical harmonic coefficients for polygonal area lights	NA:NA	2018
Thomas Leimkühler:Hans-Peter Seidel:Tobias Ritschel	Simulating combinations of depth-of-field and motion blur is an important factor to cinematic quality in synthetic images but can take long to compute. Splatting the point-spread function (PSF) of every pixel is general and provides high quality, but requires prohibitive compute time. We accelerate this in two steps: In a pre-process we optimize for sparse representations of the Laplacian of all possible PSFs that we call spreadlets. At runtime, spreadlets can be splat efficiently to the Laplacian of an image. Integrating this image produces the final result. Our approach scales faithfully to strong motion and large out-of-focus areas and compares favorably in speed and quality with off-line and interactive approaches. It is applicable to both synthesizing from pinhole as well as reconstructing from stochastic images, with or without layering.	Laplacian kernel splatting for efficient depth-of-field and motion blur synthesis or reconstruction	NA:NA:NA	2018
Masaki Nakada:Tao Zhou:Honglin Chen:Tomer Weiss:Demetri Terzopoulos	We introduce a biomimetic framework for human sensorimotor control, which features a biomechanically simulated human musculoskeletal model actuated by numerous muscles, with eyes whose retinas have nonuniformly distributed photoreceptors. The virtual human's sensorimotor control system comprises 20 trained deep neural networks (DNNs), half constituting the neuromuscular motor subsystem, while the other half compose the visual sensory subsystem. Directly from the photoreceptor responses, 2 vision DNNs drive eye and head movements, while 8 vision DNNs extract visual information required to direct arm and leg actions. Ten DNNs achieve neuromuscular control---2 DNNs control the 216 neck muscles that actuate the cervicocephalic musculoskeletal complex to produce natural head movements, and 2 DNNs control each limb; i.e., the 29 muscles of each arm and 39 muscles of each leg. By synthesizing its own training data, our virtual human automatically learns efficient, online, active visuomotor control of its eyes, head, and limbs in order to perform nontrivial tasks involving the foveation and visual pursuit of target objects coupled with visually-guided limb-reaching actions to intercept the moving targets, as well as to carry out drawing and writing tasks.	Deep learning of biomimetic sensorimotor control for biomechanical human animation	NA:NA:NA:NA:NA	2018
Seunghwan Lee:Ri Yu:Jungnam Park:Mridul Aanjaneya:Eftychios Sifakis:Jehee Lee	We propose a framework for simulation and control of the human musculoskeletal system, capable of reproducing realistic animations of dexterous activities with high-level coordination. We present the first controllable system in this class that incorporates volumetric muscle actuators, tightly coupled with the motion controller, in enhancement of line-segment approximations that prior art is overwhelmingly restricted to. The theoretical framework put forth by our methodology computes all the necessary Jacobians for control, even with the drastically increased dimensionality of the state descriptors associated with three-dimensional, volumetric muscles. The direct coupling of volumetric actuators in the controller allows us to model muscular deficiencies that manifest in shape and geometry, in ways that cannot be captured with line-segment approximations. Our controller is coupled with a trajectory optimization framework, and its efficacy is demonstrated in complex motion tasks such as juggling, and weightlifting sequences with variable anatomic parameters and interaction constraints.	Dexterous manipulation and control with volumetric muscles	NA:NA:NA:NA:NA:NA	2018
Dinesh K. Pai:Austin Rothwell:Pearson Wyder-Hodge:Alistair Wick:Ye Fan:Egor Larionov:Darcy Harrison:Debanga Raj Neog:Cole Shing	Simulating how the human body deforms in contact with external objects, tight clothing, or other humans is of central importance to many fields. Despite great advances in numerical methods, the material properties required to accurately simulate the body of a real human have been sorely lacking. Here we show that mechanical properties of the human body can be directly measured using a novel hand-held device. We describe a complete pipeline for measurement, modeling, parameter estimation, and simulation using the finite element method. We introduce a phenomenological model (the sliding thick skin model) that is effective for both simulation and parameter estimation. Our data also provide new insights into how the human body actually behaves. The methods described here can be used to create personalized models of an individual human or of a population. Consequently, our methods have many potential applications in computer animation, product design, e-commerce, and medicine.	The human touch: measuring contact with real human soft tissues	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Gaspard Zoss:Derek Bradley:Pascal Bérard:Thabo Beeler	In computer graphics the motion of the jaw is commonly modelled by up-down and left-right rotation around a fixed pivot plus a forward-backward translation, yielding a three dimensional rig that is highly suited for intuitive artistic control. The anatomical motion of the jaw is, however, much more complex since the joints that connect the jaw to the skull exhibit both rotational and translational components. In reality the jaw does not move in a three dimensional subspace but on a constrained manifold in six dimensions. We analyze this manifold in the context of computer animation and show how the manifold can be parameterized with three degrees of freedom, providing a novel jaw rig that preserves the intuitive control while providing more accurate jaw positioning. The chosen parameterization furthermore places anatomically correct limits on the motion, preventing the rig from entering physiologically infeasible poses. Our new jaw rig is empirically designed from accurate capture data, and we provide a simple method to retarget the rig to new characters, both human and fantasy.	An empirical rig for jaw animation	NA:NA:NA:NA	2018
Yixin Hu:Qingnan Zhou:Xifeng Gao:Alec Jacobson:Denis Zorin:Daniele Panozzo	We propose a novel tetrahedral meshing technique that is unconditionally robust, requires no user interaction, and can directly convert a triangle soup into an analysis-ready volumetric mesh. The approach is based on several core principles: (1) initial mesh construction based on a fully robust, yet efficient, filtered exact computation (2) explicit (automatic or user-defined) tolerancing of the mesh relative to the surface input (3) iterative mesh improvement with guarantees, at every step, of the output validity. The quality of the resulting mesh is a direct function of the target mesh size and allowed tolerance: increasing allowed deviation from the initial mesh and decreasing the target edge length both lead to higher mesh quality. Our approach enables "black-box" analysis, i.e. it allows to automatically solve partial differential equations on geometrical models available in the wild, offering a robustness and reliability comparable to, e.g., image processing algorithms, opening the door to automatic, large scale processing of real-world geometric data.	Tetrahedral meshing in the wild	NA:NA:NA:NA:NA:NA	2018
Leman Feng:Pierre Alliez:Laurent Busé:Hervé Delingette:Mathieu Desbrun	Meshes with curvilinear elements hold the appealing promise of enhanced geometric flexibility and higher-order numerical accuracy compared to their commonly-used straight-edge counterparts. However, the generation of curved meshes remains a computationally expensive endeavor with current meshing approaches: high-order parametric elements are notoriously difficult to conform to a given boundary geometry, and enforcing a smooth and non-degenerate Jacobian everywhere brings additional numerical difficulties to the meshing of complex domains. In this paper, we propose an extension of Optimal Delaunay Triangulations (ODT) to curved and graded isotropic meshes. By exploiting a continuum mechanics interpretation of ODT instead of the usual approximation theoretical foundations, we formulate a very robust geometry and topology optimization of Bézier meshes based on a new simple functional promoting isotropic and uniform Jacobians throughout the domain. We demonstrate that our resulting curved meshes can adapt to complex domains with high precision even for a small count of elements thanks to the added flexibility afforded by more control points and higher order basis functions.	Curved optimal delaunay triangulation	NA:NA:NA:NA:NA	2018
Zichun Zhong:Wenping Wang:Bruno Lévy:Jing Hua:Xiaohu Guo	This article presents a new method to compute a self-intersection free high-dimensional Euclidean embedding (SIFHDE2) for surfaces and volumes equipped with an arbitrary Riemannian metric. It is already known that given a high-dimensional (high-d) embedding, one can easily compute an anisotropic Voronoi diagram by back-mapping it to 3D space. We show here how to solve the inverse problem, i.e., given an input metric, compute a smooth intersection-free high-d embedding of the input such that the pullback metric of the embedding matches the input metric. Our numerical solution mechanism matches the deformation gradient of the 3D → higher-d mapping with the given Riemannian metric. We demonstrate the applicability of our method, by using it to construct anisotropic Restricted Voronoi Diagram (RVD) and anisotropic meshing, that are otherwise extremely difficult to compute. In SIFHDE2-space constructed by our algorithm, difficult 3D anisotropic computations are replaced with simple Euclidean computations, resulting in an isotropic RVD and its dual mesh on this high-d embedding. Results are compared with the state-of-the-art in anisotropic surface and volume meshings using several examples and evaluation metrics.	Computing a high-dimensional euclidean embedding from an arbitrary smooth riemannian metric	NA:NA:NA:NA:NA	2018
Albert Chern:Felix Knöppel:Ulrich Pinkall:Peter Schröder	We study the isometric immersion problem for orientable surface triangle meshes endowed with only a metric: given the combinatorics of the mesh together with edge lengths, approximate an isometric immersion into R3. To address this challenge we develop a discrete theory for surface immersions into R3. It precisely characterizes a discrete immersion, up to subdivision and small perturbations. In particular our discrete theory correctly represents the topology of the space of immersions, i.e., the regular homotopy classes which represent its connected components. Our approach relies on unit quaternions to represent triangle orientations and to encode, in their parallel transport, the topology of the immersion. In unison with this theory we develop a computational apparatus based on a variational principle. Minimizing a non-linear Dirichlet energy optimally finds extrinsic geometry for the given intrinsic geometry and ensures low metric approximation error. We demonstrate our algorithm with a number of applications from mathematical visualization and art directed isometric shape deformation, which mimics the behavior of thin materials with high membrane stiffness.	Shape from metric	NA:NA:NA:NA	2018
Neal Wadhwa:Rahul Garg:David E. Jacobs:Bryan E. Feldman:Nori Kanazawa:Robert Carroll:Yair Movshovitz-Attias:Jonathan T. Barron:Yael Pritch:Marc Levoy	Shallow depth-of-field is commonly used by photographers to isolate a subject from a distracting background. However, standard cell phone cameras cannot produce such images optically, as their short focal lengths and small apertures capture nearly all-in-focus images. We present a system to computationally synthesize shallow depth-of-field images with a single mobile camera and a single button press. If the image is of a person, we use a person segmentation network to separate the person and their accessories from the background. If available, we also use dense dual-pixel auto-focus hardware, effectively a 2-sample light field with an approximately 1 millimeter baseline, to compute a dense depth map. These two signals are combined and used to render a defocused image. Our system can process a 5.4 megapixel image in 4 seconds on a mobile phone, is fully automatic, and is robust enough to be used by non-experts. The modular nature of our system allows it to degrade naturally in the absence of a dual-pixel sensor or a human subject.	Synthetic depth-of-field with a single-camera mobile phone	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Tinghui Zhou:Richard Tucker:John Flynn:Graham Fyffe:Noah Snavely	The view synthesis problem---generating novel views of a scene from known imagery---has garnered recent attention due in part to compelling applications in virtual and augmented reality. In this paper, we explore an intriguing scenario for view synthesis: extrapolating views from imagery captured by narrow-baseline stereo cameras, including VR cameras and now-widespread dual-lens camera phones. We call this problem stereo magnification, and propose a learning framework that leverages a new layered representation that we call multiplane images (MPIs). Our method also uses a massive new data source for learning view extrapolation: online videos on YouTube. Using data mined from such videos, we train a deep network that predicts an MPI from an input stereo image pair. This inferred MPI can then be used to synthesize a range of novel views of the scene, including views that extrapolate significantly beyond the input baseline. We show that our method compares favorably with several recent view synthesis methods, and demonstrate applications in magnifying narrow-baseline stereo images.	Stereo magnification: learning view synthesis using multiplane images	NA:NA:NA:NA:NA	2018
Eike Langbehn:Frank Steinicke:Markus Lappe:Gregory F. Welch:Gerd Bruder	Immersive computer-generated environments (aka virtual reality, VR) are limited by the physical space around them, e.g., enabling natural walking in VR is only possible by perceptually-inspired locomotion techniques such as redirected walking (RDW). We introduce a completely new approach to imperceptible position and orientation redirection that takes advantage of the fact that even healthy humans are functionally blind for circa ten percent of the time under normal circumstances due to motor processes preventing light from reaching the retina (such as eye blinks) or perceptual processes suppressing degraded visual information (such as blink-induced suppression). During such periods of missing visual input, change blindness occurs, which denotes the inability to perceive a visual change such as the motion of an object or self-motion of the observer. We show that this phenomenon can be exploited in VR by synchronizing the computer graphics rendering system with the human visual processes for imperceptible camera movements, in particular to implement position and orientation redirection. We analyzed human sensitivity to such visual changes with detection thresholds, which revealed that commercial off-the-shelf eye trackers and head-mounted displays suffice to translate a user by circa 4 -- 9 cm and rotate the user by circa 2 -- 5 degrees in any direction, which could be accumulated each time the user blinks. Moreover, we show the potential for RDW, whose performance could be improved by approximately 50% when using our technique.	In the blink of an eye: leveraging blink-induced suppression for imperceptible position and orientation redirection in virtual reality	NA:NA:NA:NA:NA	2018
Qi Sun:Anjul Patney:Li-Yi Wei:Omer Shapira:Jingwan Lu:Paul Asente:Suwen Zhu:Morgan Mcguire:David Luebke:Arie Kaufman	Redirected walking techniques can enhance the immersion and visual-vestibular comfort of virtual reality (VR) navigation, but are often limited by the size, shape, and content of the physical environments. We propose a redirected walking technique that can apply to small physical environments with static or dynamic obstacles. Via a head- and eye-tracking VR headset, our method detects saccadic suppression and redirects the users during the resulting temporary blindness. Our dynamic path planning runs in real-time on a GPU, and thus can avoid static and dynamic obstacles, including walls, furniture, and other VR users sharing the same physical space. To further enhance saccadic redirection, we propose subtle gaze direction methods tailored for VR perception. We demonstrate that saccades can significantly increase the rotation gains during redirection without introducing visual distortions or simulator sickness. This allows our method to apply to large open virtual spaces and small physical environments for room-scale VR. We evaluate our system via numerical simulations and real user studies.	Towards virtual reality infinite walking: dynamic saccadic redirection	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Stephen Lombardi:Jason Saragih:Tomas Simon:Yaser Sheikh	We introduce a deep appearance model for rendering the human face. Inspired by Active Appearance Models, we develop a data-driven rendering pipeline that learns a joint representation of facial geometry and appearance from a multiview capture setup. Vertex positions and view-specific textures are modeled using a deep variational autoencoder that captures complex nonlinear effects while producing a smooth and compact latent representation. View-specific texture enables the modeling of view-dependent effects such as specularity. In addition, it can also correct for imperfect geometry stemming from biased or low resolution estimates. This is a significant departure from the traditional graphics pipeline, which requires highly accurate geometry as well as all elements of the shading model to achieve realism through physically-inspired light transport. Acquiring such a high level of accuracy is difficult in practice, especially for complex and intricate parts of the face, such as eyelashes and the oral cavity. These are handled naturally by our approach, which does not rely on precise estimates of geometry. Instead, the shading model accommodates deficiencies in geometry though the flexibility afforded by the neural network employed. At inference time, we condition the decoding network on the viewpoint of the camera in order to generate the appropriate texture for rendering. The resulting system can be implemented simply using existing rendering engines through dynamic textures with flat lighting. This representation, together with a novel unsupervised technique for mapping images to facial states, results in a system that is naturally suited to real-time interactive settings such as Virtual Reality (VR).	Deep appearance models for face rendering	NA:NA:NA:NA	2018
Kfir Aberman:Jing Liao:Mingyi Shi:Dani Lischinski:Baoquan Chen:Daniel Cohen-Or	Correspondence between images is a fundamental problem in computer vision, with a variety of graphics applications. This paper presents a novel method for sparse cross-domain correspondence. Our method is designed for pairs of images where the main objects of interest may belong to different semantic categories and differ drastically in shape and appearance, yet still contain semantically related or geometrically similar parts. Our approach operates on hierarchies of deep features, extracted from the input images by a pre-trained CNN. Specifically, starting from the coarsest layer in both hierarchies, we search for Neural Best Buddies (NBB): pairs of neurons that are mutual nearest neighbors. The key idea is then to percolate NBBs through the hierarchy, while narrowing down the search regions at each level and retaining only NBBs with significant activations. Furthermore, in order to overcome differences in appearance, each pair of search regions is transformed into a common appearance. We evaluate our method via a user study, in addition to comparisons with alternative correspondence approaches. The usefulness of our method is demonstrated using a variety of graphics applications, including cross-domain image alignment, creation of hybrid images, automatic image morphing, and more.	Neural best-buddies: sparse cross-domain correspondence	NA:NA:NA:NA:NA:NA	2018
Kai Wang:Manolis Savva:Angel X. Chang:Daniel Ritchie	We present a convolutional neural network based approach for indoor scene synthesis. By representing 3D scenes with a semantically-enriched image-based representation based on orthographic top-down views, we learn convolutional object placement priors from the entire context of a room. Our approach iteratively generates rooms from scratch, given only the room architecture as input. Through a series of perceptual studies we compare the plausibility of scenes generated using our method against baselines for object selection and object arrangement, as well as scenes modeled by people. We find that our method generates scenes that are preferred over the baselines, and in some cases are equally preferred to human-created scenes.	Deep convolutional priors for indoor scene synthesis	NA:NA:NA:NA	2018
Matan Atzmon:Haggai Maron:Yaron Lipman	This paper presents Point Convolutional Neural Networks (PCNN): a novel framework for applying convolutional neural networks to point clouds. The framework consists of two operators: extension and restriction, mapping point cloud functions to volumetric functions and vise-versa. A point cloud convolution is defined by pull-back of the Euclidean volumetric convolution via an extension-restriction mechanism. The point cloud convolution is computationally efficient, invariant to the order of points in the point cloud, robust to different samplings and varying densities, and translation invariant, that is the same convolution kernel is used at all points. PCNN generalizes image CNNs and allows readily adapting their architectures to the point cloud setting. Evaluation of PCNN on three central point cloud learning benchmarks convincingly outperform competing point cloud learning methods, and the vast majority of methods working with more informative shape representations such as surfaces and/or normals.	Point convolutional neural networks by extension operators	NA:NA:NA	2018
Yağiz Aksoy:Tae-Hyun Oh:Sylvain Paris:Marc Pollefeys:Wojciech Matusik	Accurate representation of soft transitions between image regions is essential for high-quality image editing and compositing. Current techniques for generating such representations depend heavily on interaction by a skilled visual artist, as creating such accurate object selections is a tedious task. In this work, we introduce semantic soft segments, a set of layers that correspond to semantically meaningful regions in an image with accurate soft transitions between different objects. We approach this problem from a spectral segmentation angle and propose a graph structure that embeds texture and color features from the image as well as higher-level semantic information generated by a neural network. The soft segments are generated via eigendecomposition of the carefully constructed Laplacian matrix fully automatically. We demonstrate that otherwise complex image editing tasks can be done with little effort using semantic soft segments.	Semantic soft segmentation	NA:NA:NA:NA:NA	2018
Laurent Belcour	We derive a novel framework for the efficient analysis and computation of light transport within layered materials. Our derivation consists in two steps. First, we decompose light transport into a set of atomic operators that act on its directional statistics. Specifically, our operators consist of reflection, refraction, scattering, and absorption, whose combinations are sufficient to describe the statistics of light scattering multiple times within layered structures. We show that the first three directional moments (energy, mean and variance) already provide an accurate summary. Second, we extend the adding-doubling method to support arbitrary combinations of such operators efficiently. During shading, we map the directional moments to BSDF lobes. We validate that the resulting BSDF closely matches the ground truth in a lightweight and efficient form. Unlike previous methods we support an arbitrary number of textured layers, and demonstrate a practical and accurate rendering of layered materials with both an offline and real-time implementation that are free from per-material precomputation.	Efficient rendering of layered materials using an atomic decomposition with statistical operators	NA	2018
Tizian Zeltner:Wenzel Jakob	We present a versatile computational framework for modeling the reflective and transmissive properties of arbitrarily layered anisotropic material structures. Given a set of input layers, our model synthesizes an effective BSDF of the entire structure, which accounts for all orders of internal scattering and is efficient to sample and evaluate in modern rendering systems. Our technique builds on the insight that reflectance data is sparse when expanded into a suitable frequency-space representation, and that this property extends to the class of anisotropic materials. This sparsity enables an efficient matrix calculus that admits the entire space of BSDFs and considerably expands the scope of prior work on layered material modeling. We show how both measured data and the popular class of microfacet models can be expressed in our representation, and how the presence of anisotropy leads to a weak coupling between Fourier orders in frequency space. In addition to additive composition, our models supports subtractive composition, a fascinating new operation that reconstructs the BSDF of a material that can only be observed indirectly through another layer with known reflectance properties. The operation produces a new BSDF of the desired layer as if measured in isolation. Subtractive composition can be interpreted as a type of deconvolution that removes both internal scattering and blurring due to transmission through the known layer. We experimentally demonstrate the accuracy and scope of our model and validate both additive and subtractive composition using measurements of real-world layered materials. Both implementation and data will be released to ensure full reproducibility of all of our results.1	The layer laboratory: a calculus for additive and subtractive composition of anisotropic surface reflectance	NA:NA	2018
Ling-Qi Yan:Miloš Hašan:Bruce Walter:Steve Marschner:Ravi Ramamoorthi	Simulation of light reflection from specular surfaces is a core problem of computer graphics. Existing solutions either make the approximation of providing only a large-area average solution in terms of a fixed BRDF (ignoring spatial detail), or are specialized for specific microgeometry (e.g. 1D scratches), or are based only on geometric optics (which is an approximation to more accurate wave optics). We design the first rendering algorithm based on a wave optics model that is also able to compute spatially-varying specular highlights with high-resolution detail on general surface microgeometry. We compute a wave optics reflection integral over the coherence area; our solution is based on approximating the phase-delay grating representation of a micron-resolution surface heightfield using Gabor kernels. We found that the appearance difference between the geometric and wave solution is more dramatic when spatial detail is taken into account. The visualizations of the corresponding BRDF lobes differ significantly. Moreover, the wave optics solution varies as a function of wavelength, predicting noticeable color effects in the highlights. Our results show both single-wavelength and spectral solution to reflection from common everyday objects, such as brushed, scratched and bumpy metals.	Rendering specular microgeometry with wave optics	NA:NA:NA:NA:NA	2018
Károly Zsolnai-Fehér:Peter Wonka:Michael Wimmer	We present a learning-based system for rapid mass-scale material synthesis that is useful for novice and expert users alike. The user preferences are learned via Gaussian Process Regression and can be easily sampled for new recommendations. Typically, each recommendation takes 40-60 seconds to render with global illumination, which makes this process impracticable for real-world workflows. Our neural network eliminates this bottleneck by providing high-quality image predictions in real time, after which it is possible to pick the desired materials from a gallery and assign them to a scene in an intuitive manner. Workflow timings against Disney's "principled" shader reveal that our system scales well with the number of sought materials, thus empowering even novice users to generate hundreds of high-quality material models without any expertise in material modeling. Similarly, expert users experience a significant decrease in the total modeling time when populating a scene with materials. Furthermore, our proposed solution also offers controllable recommendations and a novel latent space variant generation step to enable the real-time fine-tuning of materials without requiring any domain expertise.	Gaussian material synthesis	NA:NA:NA	2018
Oded Stein:Eitan Grinspun:Keenan Crane	Developable surfaces are those that can be made by smoothly bending flat pieces without stretching or shearing. We introduce a definition of developability for triangle meshes which exactly captures two key properties of smooth developable surfaces, namely flattenability and presence of straight ruling lines. This definition provides a starting point for algorithms in developable surface modeling---we consider a variational approach that drives a given mesh toward developable pieces separated by regular seam curves. Computation amounts to gradient descent on an energy with support in the vertex star, without the need to explicitly cluster patches or identify seams. We briefly explore applications to developable design and manufacturing.	Developability of triangle meshes	NA:NA:NA	2018
Christian Schüller:Roi Poranne:Olga Sorkine-Hornung	Fabrication from developable parts is the basis for arts such as papercraft and needlework, as well as modern architecture and CAD in general, and it has inspired much research. We observe that the assembly of complex 3D shapes created by existing methods often requires first fabricating many small parts and then carefully following instructions to assemble them together. Despite its significance, this error prone and tedious process is generally neglected in the discussion. We present the concept of zippables - single, two dimensional, branching, ribbon-like pieces of fabric that can be quickly zipped up without any instructions to form 3D objects. Our inspiration comes from the so-called zipit bags [zipit 2017], which are made of a single, long ribbon with a zipper around its boundary. In order to "assemble" the bag, one simply needs to zip up the ribbon. Our method operates in the same fashion, but it can be used to approximate a wide variety of shapes. Given a 3D model, our algorithm produces plans for a single 2D shape that can be laser cut in few parts from fabric or paper. A zipper can then be attached along the boundary by sewing, or by gluing using a custom-built fastening rig. We show physical and virtual results that demonstrate the capabilities of our method and the ease with which shapes can be assembled.	Shape representation by zippables	NA:NA:NA	2018
Dimitar Dinev:Tiantian Liu:Jing Li:Bernhard Thomaszewski:Ladislav Kavan	We propose a novel projection scheme that corrects energy fluctuations in simulations of deformable objects, thereby removing unwanted numerical dissipation and numerical "explosions". The key idea of our method is to first take a step using a conventional integrator, then project the result back to the constant energy-momentum manifold. We implement this strategy using fast projection, which only adds a small amount of overhead to existing physics-based solvers. We test our method with several implicit integration rules and demonstrate its benefits when used in conjunction with Position Based Dynamics and Projective Dynamics. When added to a dissipative integrator such as backward Euler, our method corrects the artificial damping and thus produces more vivid motion. Our projection scheme also effectively prevents instabilities that can arise due to approximate solves or large time steps. Our method is fast, stable, and easy to implement---traits that make it well-suited for real-time physics applications such as games or training simulators.	FEPR: fast energy projection for real-time simulation of deformable objects	NA:NA:NA:NA:NA	2018
Christopher Brandt:Elmar Eisemann:Klaus Hildebrandt	We present a method for the real-time simulation of deformable objects that combines the robustness, generality, and high performance of Projective Dynamics with the efficiency and scalability offered by model reduction techniques. The method decouples the cost for time integration from the mesh resolution and can simulate large meshes in real-time. The proposed hyper-reduction of Projective Dynamics combines a novel fast approximation method for constraint projections and a scalable construction of sparse subspace bases. The resulting system achieves real-time rates for large sub-spaces enabling rich dynamics and can resolve general user interactions, collision constraints, external forces and changes to the materials. The construction of the hyper-reduced system does not require user-interaction and refrains from using training data or modal analysis, which results in a fast preprocessing stage.	Hyper-reduced projective dynamics	NA:NA:NA	2018
Fernando De Goes:Doug L. James	We introduce Dynamic Kelvinlets, a new analytical technique for real-time physically based animation of virtual elastic materials. Our formulation is based on the dynamic response to time-varying force distributions applied to an infinite elastic medium. The resulting displacements provide the plausibility of volumetric elasticity, the dynamics of compressive and shear waves, and the interactivity of closed-form expressions. Our approach builds upon the work of de Goes and James [2017] by presenting an extension of the regularized Kelvinlet solutions from elastostatics to the elastodynamic regime. To finely control our elastic deformations, we also describe the construction of compound solutions that resolve pointwise and keyframe constraints. We demonstrate the versatility and efficiency of our method with a series of examples in a production grade implementation.	Dynamic kelvinlets: secondary motions based on fundamental solutions of elastodynamics	NA:NA	2018
Adrien Gruson:Binh-Son Hua:Nicolas Vibert:Derek Nowrouzezahrai:Toshiya Hachisuka	Gradient-domain rendering can improve the convergence of surface-based light transport by exploiting smoothness in image space. Scenes with participating media exhibit similar smoothness and could potentially benefit from gradient-domain techniques. We introduce the first gradient-domain formulation of image synthesis with homogeneous participating media, including four novel and efficient gradient-domain volumetric density estimation algorithms. We show that naïve extensions of gradient domain path-space and density estimation methods to volumetric media, while functional, can result in inefficient estimators. Focussing on point-, beam- and plane-based gradient-domain estimators, we introduce a novel shift mapping that eliminates redundancies in the naïve formulations using spatial relaxation within the volume. We show that gradient-domain volumetric rendering improve convergence compared to primal domain state-of-the-art, across a suite of scenes. Our formulation and algorithms support progressive estimation and are easy to incorporate atop existing renderers.	Gradient-domain volumetric photon density estimation	NA:NA:NA:NA:NA	2018
Adrian Jarabo:Carlos Aliaga:Diego Gutierrez	We introduce a non-exponential radiative framework that takes into account the local spatial correlation of scattering particles in a medium. Most previous works in graphics have ignored this, assuming uncorrelated media with a uniform, random local distribution of particles. However, positive and negative correlation lead to slower- and faster-than-exponential attenuation respectively, which cannot be predicted by the Beer-Lambert law. As our results show, this has a major effect on extinction, and thus appearance. From recent advances in neutron transport, we first introduce our Extended Generalized Boltzmann Equation, and develop a general framework for light transport in correlated media. We lift the limitations of the original formulation, including an analysis of the boundary conditions, and present a model suitable for computer graphics, based on optical properties of the media and statistical distributions of scatterers. In addition, we present an analytic expression for transmittance in the case of positive correlation, and show how to incorporate it efficiently into a Monte Carlo renderer. We show results with a wide range of both positive and negative correlation, and demonstrate the differences compared to classic light transport.	A radiative transfer framework for spatially-correlated materials	NA:NA:NA	2018
Syuhei Sato:Yoshinori Dobashi:Theodore Kim:Tomoyuki Nishita	Generating realistic fluid simulations remains computationally expensive, and animators can expend enormous effort trying to achieve a desired motion. To reduce such costs, several methods have been developed in which high-resolution turbulence is synthesized as a post process. Since global motion can then be obtained using a fast, low-resolution simulation, less effort is needed to create a realistic animation with the desired behavior. While much research has focused on accelerating the low-resolution simulation, the problem controlling the behavior of the turbulent, high-resolution motion has received little attention. In this paper, we show that style transfer methods from image editing can be adapted to transfer the turbulent style of an existing fluid simulation onto a new one. We do this by extending example-based image synthesis methods to handle velocity fields using a combination of patch-based and optimization-based texture synthesis. This approach allows us to take into account the incompressibility condition, which we have found to be a important factor during synthesis. Using our method, a user can easily and intuitively create high-resolution fluid animations that have a desired turbulent motion.	Example-based turbulence style transfer	NA:NA:NA:NA	2018
Jonas Zehnder:Rahul Narain:Bernhard Thomaszewski	Advection-projection methods for fluid animation are widely appreciated for their stability and efficiency. However, the projection step dissipates energy from the system, leading to artificial viscosity and suppression of small-scale details. We propose an alternative approach for detail-preserving fluid animation that is surprisingly simple and effective. We replace the energy-dissipating projection operator applied at the end of a simulation step by an energy-preserving reflection operator applied at mid-step. We show that doing so leads to two orders of magnitude reduction in energy loss, which in turn yields vastly improved detail-preservation. We evaluate our reflection solver on a set of 2D and 3D numerical experiments and show that it compares favorably to state-of-the-art methods. Finally, our method integrates seamlessly with existing projection-advection solvers and requires very little additional implementation.	An advection-reflection solver for detail-preserving fluid simulation	NA:NA:NA	2018
Muzaffer Akbay:Nicholas Nobles:Victor Zordan:Tamar Shinar	We present a novel extended partitioned method for two-way solid-fluid coupling, where the fluid and solid solvers are treated as black boxes with limited exposed interfaces, facilitating modularity and code reusability. Our method achieves improved stability and extended range of applicability over standard partitioned approaches through three techniques. First, we couple the black-box solvers through a small, reduced-order monolithic system, which is constructed on the fly from input/output pairs generated by the solid and fluid solvers. Second, we use a conservative, impulse-based interaction term to couple the solid and fluid rather than typical pressure-based forces. We show that both of these techniques significantly improve stability and reduce the number of iterations needed for convergence. Finally, we propose a novel boundary pressure projection method that allows for the partitioned simulation of a fully enclosed fluid coupled to a dynamic solid, a scenario that has been problematic for partitioned methods. We demonstrate the benefits of our extended partitioned method by coupling Eulerian fluid solvers for smoke and water to Lagrangian solid solvers for volumetric and thin deformable and rigid objects in a variety of challenging scenarios. We further demonstrate our method by coupling a Lagrangian SPH fluid solver to a rigid body solver.	An extended partitioned method for conservative solid-fluid coupling	NA:NA:NA:NA	2018
Qiaodong Cui:Pradeep Sen:Theodore Kim	The Laplacian Eigenfunction method for fluid simulation, which we refer to as Eigenfluids, introduced an elegant new way to capture intricate fluid flows with near-zero viscosity. However, the approach does not scale well, as the memory cost grows prohibitively with the number of eigenfunctions. The method also lacks generality, because the dynamics are constrained to a closed box with Dirichlet boundaries, while open, Neumann boundaries are also needed in most practical scenarios. To address these limitations, we present a set of analytic eigenfunctions that supports uniform Neumann and Dirichlet conditions along each domain boundary, and show that by carefully applying the discrete sine and cosine transforms, the storage costs of the eigenfunctions can be made completely negligible. The resulting algorithm is both faster and more memory-efficient than previous approaches, and able to achieve lower viscosities than similar pseudo-spectral methods. We are able to surpass the scalability of the original Laplacian Eigenfunction approach by over two orders of magnitude when simulating rectangular domains. Finally, we show that the formulation allows forward scattering to be directed in a way that is not possible with any other method.	Scalable laplacian eigenfluids	NA:NA:NA	2018
Ke Xie:Hao Yang:Shengqiu Huang:Dani Lischinski:Marc Christie:Kai Xu:Minglun Gong:Daniel Cohen-Or:Hui Huang	Capturing aerial videos with a quadrotor-mounted camera is a challenging creative task, as it requires the simultaneous control of the quadrotor's motion and the mounted camera's orientation. Letting the drone follow a pre-planned trajectory is a much more appealing option, and recent research has proposed a number of tools designed to automate the generation of feasible camera motion plans; however, these tools typically require the user to specify and edit the camera path, for example by providing a complete and ordered sequence of key viewpoints. In this paper, we propose a higher level tool designed to enable even novice users to easily capture compelling aerial videos of large-scale outdoor scenes. Using a coarse 2.5D model of a scene, the user is only expected to specify starting and ending viewpoints and designate a set of landmarks, with or without a particular order. Our system automatically generates a diverse set of candidate local camera moves for observing each landmark, which are collision-free, smooth, and adapted to the shape of the landmark. These moves are guided by a landmark-centric view quality field, which combines visual interest and frame composition. An optimal global camera trajectory is then constructed that chains together a sequence of local camera moves, by choosing one move for each landmark and connecting them with suitable transition trajectories. This task is formulated and solved as an instance of the Set Traveling Salesman Problem.	Creating and chaining camera moves for qadrotor videography	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Nobuyuki Umetani:Bernd Bickel	We present a data-driven technique to instantly predict how fluid flows around various three-dimensional objects. Such simulation is useful for computational fabrication and engineering, but is usually computationally expensive since it requires solving the Navier-Stokes equation for many time steps. To accelerate the process, we propose a machine learning framework which predicts aerodynamic forces and velocity and pressure fields given a three-dimensional shape input. Handling detailed free-form three-dimensional shapes in a data-driven framework is challenging because machine learning approaches usually require a consistent parametrization of input and output. We present a novel PolyCube maps-based parametrization that can be computed for three-dimensional shapes at interactive rates. This allows us to efficiently learn the nonlinear response of the flow using a Gaussian process regression. We demonstrate the effectiveness of our approach for the interactive design and optimization of a car body.	Learning three-dimensional flow for interactive aerodynamic design	NA:NA	2018
Christoph Gebhardt:Stefan Stevšić:Otmar Hilliges	In this paper we first contribute a large scale online study (N ≈ 400) to better understand aesthetic perception of aerial video. The results indicate that it is paramount to optimize smoothness of trajectories across all keyframes. However, for experts timing control remains an essential tool. Satisfying this dual goal is technically challenging because it requires giving up desirable properties in the optimization formulation. Second, informed by this study we propose a method that optimizes positional and temporal reference fit jointly. This allows to generate globally smooth trajectories, while retaining user control over reference timings. The formulation is posed as a variable, infinite horizon, contour-following algorithm. Finally, a comparative lab study indicates that our optimization scheme outperforms the state-of-the-art in terms of perceived usability and preference of resulting videos. For novices our method produces smoother and better looking results and also experts benefit from generated timings.	Optimizing for aesthetically pleasing quadrotor camera motion	NA:NA:NA	2018
Nahum Farchi:Mirela Ben-Chen	We propose a new iterative algorithm for computing smooth cross fields on triangle meshes that is simple, easily parallelizable on the GPU, and finds solutions with lower energy and fewer cone singularities than state-of-the-art methods. Our approach is based on a formal equivalence, which we prove, between two formulations of the optimization problem. This equivalence allows us to eliminate the real variables and design an efficient grid search algorithm for the cone singularities. We leverage a recent graph-theoretical approximation of the resistance distance matrix of the triangle mesh to speed up the computation and enable a trade-off between the computation time and the smoothness of the output.	Integer-only cross field computation	NA:NA	2018
Xianzhong Fang:Hujun Bao:Yiying Tong:Mathieu Desbrun:Jin Huang	We introduce an approach to quadrilateral meshing of arbitrary triangulated surfaces that combines the theoretical guarantees of Morse-based approaches with the practical advantages of parameterization methods. We first construct, through an eigensolver followed by a few Gauss-Newton iterations, a periodic four-dimensional vector field that aligns with a user-provided frame field and/or a set of features over the input mesh. A field-aligned parameterization is then greedily computed along a spanning tree based on the Dirichlet energy of the optimal periodic vector field, from which quad elements are efficiently extracted over most of the surface. The few regions not yet covered by elements are then upsampled and the first component of the periodic vector field is used as a Morse function to extract the remaining quadrangles. This hybrid parameterization- and Morse-based quad meshing method is not only fast (the parameterization is greedily constructed, and the Morse function only needs to be upsampled in the few uncovered patches), but is guaranteed to provide a feature-aligned quad mesh with non-degenerate cells that closely matches the input frame field over an arbitrary surface. We show that our approach is much faster than Morse-based techniques since it does not require a densely tessellated input mesh, and is significantly more robust than parameterization-based techniques on models with complex features.	Quadrangulation through morse-parameterization hybridization	NA:NA:NA:NA:NA	2018
Heng Liu:Paul Zhang:Edward Chien:Justin Solomon:David Bommes	Despite high practical demand, algorithmic hexahedral meshing with guarantees on robustness and quality remains unsolved. A promising direction follows the idea of integer-grid maps, which pull back the Cartesian hexahedral grid formed by integer isoplanes from a parametric domain to a surface-conforming hexahedral mesh of the input object. Since directly optimizing for a high-quality integer-grid map is mathematically challenging, the construction is usually split into two steps: (1) generation of a surface-aligned octahedral field and (2) generation of an integer-grid map that best aligns to the octahedral field. The main robustness issue stems from the fact that smooth octahedral fields frequently exhibit singularity graphs that are not appropriate for hexahedral meshing and induce heavily degenerate integer-grid maps. The first contribution of this work is an enumeration of all local configurations that exist in hex meshes with bounded edge valence, and a generalization of the Hopf-Poincaré formula to octahedral fields, leading to necessary local and global conditions for the hex-meshability of an octahedral field in terms of its singularity graph. The second contribution is a novel algorithm to generate octahedral fields with prescribed hex-meshable singularity graphs, which requires the solution of a large nonlinear mixed-integer algebraic system. This algorithm is an important step toward robust automatic hexahedral meshing since it enables the generation of a hex-meshable octahedral field.	Singularity-constrained octahedral fields for hexahedral meshing	NA:NA:NA:NA:NA	2018
Stefan Jeschke:Tomáš Skřivan:Matthias Müller-Fischer:Nuttapong Chentanez:Miles Macklin:Chris Wojtan	The current state of the art in real-time two-dimensional water wave simulation requires developers to choose between efficient Fourier-based methods, which lack interactions with moving obstacles, and finite-difference or finite element methods, which handle environmental interactions but are significantly more expensive. This paper attempts to bridge this long-standing gap between complexity and performance, by proposing a new wave simulation method that can faithfully simulate wave interactions with moving obstacles in real time while simultaneously preserving minute details and accommodating very large simulation domains. Previous methods for simulating 2D water waves directly compute the change in height of the water surface, a strategy which imposes limitations based on the CFL condition (fast moving waves require small time steps) and Nyquist's limit (small wave details require closely-spaced simulation variables). This paper proposes a novel wavelet transformation that discretizes the liquid motion in terms of amplitude-like functions that vary over space, frequency, and direction, effectively generalizing Fourier-based methods to handle local interactions. Because these new variables change much more slowly over space than the original water height function, our change of variables drastically reduces the limitations of the CFL condition and Nyquist limit, allowing us to simulate highly detailed water waves at very large visual resolutions. Our discretization is amenable to fast summation and easy to parallelize. We also present basic extensions like pre-computed wave paths and two-way solid fluid coupling. Finally, we argue that our discretization provides a convenient set of variables for artistic manipulation, which we illustrate with a novel wave-painting interface.	Water surface wavelets	NA:NA:NA:NA:NA:NA	2018
You Xie:Erik Franz:Mengyu Chu:Nils Thuerey	We propose a temporally coherent generative model addressing the super-resolution problem for fluid flows. Our work represents a first approach to synthesize four-dimensional physics fields with neural networks. Based on a conditional generative adversarial network that is designed for the inference of three-dimensional volumetric data, our model generates consistent and detailed results by using a novel temporal discriminator, in addition to the commonly used spatial one. Our experiments show that the generator is able to infer more realistic high-resolution details by using additional physical quantities, such as low-resolution velocities or vorticities. Besides improvements in the training process and in the generated outputs, these inputs offer means for artistic control as well. We additionally employ a physics-aware data augmentation step, which is crucial to avoid overfitting and to reduce memory requirements. In this way, our network learns to generate adverted quantities with highly detailed, realistic, and temporally coherent features. Our method works instantaneously, using only a single time-step of low-resolution fluid data. We demonstrate the abilities of our method using a variety of complex inputs and applications in two and three dimensions.	tempoGAN: a temporally coherent, volumetric GAN for super-resolution fluid flow	NA:NA:NA:NA	2018
Pingchuan Ma:Yunsheng Tian:Zherong Pan:Bo Ren:Dinesh Manocha	We present a learning-based method to control a coupled 2D system involving both fluid and rigid bodies. Our approach is used to modify the fluid/rigid simulator's behavior by applying control forces only at the simulation domain boundaries. The rest of the domain, corresponding to the interior, is governed by the Navier-Stokes equation for fluids and Newton-Euler's equation for the rigid bodies. We represent our controller using a general neural-net, which is trained using deep reinforcement learning. Our formulation decomposes a control task into two stages: a precomputation training stage and an online generation stage. We utilize various fluid properties, e.g., the liquid's velocity field or the smoke's density field, to enhance the controller's performance. We set up our evaluation benchmark by letting controller drive fluid jets move on the domain boundary and allowing them to shoot fluids towards a rigid body to accomplish a set of challenging 2D tasks such as keeping a rigid body balanced, playing a two-player ping-pong game, and driving a rigid body to sequentially hit specified points on the wall. In practice, our approach can generate physically plausible animations.	Fluid directed rigid body control using deep reinforcement learning	NA:NA:NA:NA:NA	2018
Chenxi Liu:Enrique Rosales:Alla Sheffer	When creating line drawings, artists frequently depict intended curves using multiple, tightly clustered, or overdrawn, strokes. Given such sketches, human observers can readily envision these intended, aggregate, curves, and mentally assemble the artist's envisioned 2D imagery. Algorithmic stroke consolidation---replacement of overdrawn stroke clusters by corresponding aggregate curves---can benefit a range of sketch processing and sketch-based modeling applications which are designed to operate on consolidated, intended curves. We propose StrokeAggregator, a novel stroke consolidation method that significantly improves on the state of the art, and produces aggregate curve drawings validated to be consistent with viewer expectations. Our framework clusters strokes into groups that jointly define intended aggregate curves by leveraging principles derived from human perception research and observation of artistic practices. We employ these principles within a coarse-to-fine clustering method that starts with an initial clustering based on pairwise stroke compatibility analysis, and then refines it by analyzing interactions both within and in-between clusters of strokes. We facilitate this analysis by computing a common 1D parameterization for groups of strokes via common aggregate curve fitting. We demonstrate our method on a large range of line drawings, and validate its ability to generate consolidated drawings that are consistent with viewer perception via qualitative user evaluation, and comparisons to manually consolidated drawings and algorithmic alternatives.	StrokeAggregator: consolidating raw sketches into artist-intended curve drawings	NA:NA:NA	2018
Edgar Simo-Serra:Satoshi Iizuka:Hiroshi Ishikawa	We present an interactive approach for inking, which is the process of turning a pencil rough sketch into a clean line drawing. The approach, which we call the Smart Inker, consists of several "smart" tools that intuitively react to user input, while guided by the input rough sketch, to efficiently and naturally connect lines, erase shading, and fine-tune the line drawing output. Our approach is data-driven: the tools are based on fully convolutional networks, which we train to exploit both the user edits and inaccurate rough sketch to produce accurate line drawings, allowing high-performance interactive editing in real-time on a variety of challenging rough sketch images. For the training of the tools, we developed two key techniques: one is the creation of training data by simulation of vague and quick user edits; the other is a line normalization based on learning from vector data. These techniques, in combination with our sketch-specific data augmentation, allow us to train the tools on heterogeneous data without actual user interaction. We validate our approach with an in-depth user study, comparing it with professional illustration software, and show that our approach is able to reduce inking time by a factor of 1.8X, while improving the results of amateur users.	Real-time data-driven interactive rough sketch inking	NA:NA:NA	2018
Tiziano Portenier:Qiyang Hu:Attila Szabó:Siavash Arjomand Bigdeli:Paolo Favaro:Matthias Zwicker	We present a novel system for sketch-based face image editing, enabling users to edit images intuitively by sketching a few strokes on a region of interest. Our interface features tools to express a desired image manipulation by providing both geometry and color constraints as user-drawn strokes. As an alternative to the direct user input, our proposed system naturally supports a copy-paste mode, which allows users to edit a given image region by using parts of another exemplar image without the need of hand-drawn sketching at all. The proposed interface runs in real-time and facilitates an interactive and iterative workflow to quickly express the intended edits. Our system is based on a novel sketch domain and a convolutional neural network trained end-to-end to automatically learn to render image regions corresponding to the input strokes. To achieve high quality and semantically consistent results we train our neural network on two simultaneous tasks, namely image completion and image translation. To the best of our knowledge, we are the first to combine these two tasks in a unified framework for interactive image editing. Our results show that the proposed sketch domain, network architecture, and training procedure generalize well to real user input and enable high quality synthesis results without additional post-processing.	Faceshop: deep sketch-based face image editing	NA:NA:NA:NA:NA:NA	2018
Guangming Zang:Ramzi Idoughi:Ran Tao:Gilles Lubineau:Peter Wonka:Wolfgang Heidrich	X-ray computed tomography (CT) is a valuable tool for analyzing objects with interesting internal structure or complex geometries that are not accessible with optical means. Unfortunately, tomographic reconstruction of complex shapes requires a multitude (often hundreds or thousands) of projections from different viewpoints. Such a large number of projections can only be acquired in a time-sequential fashion. This significantly limits the ability to use x-ray tomography for either objects that undergo uncontrolled shape change at the time scale of a scan, or else for analyzing dynamic phenomena, where the motion itself is under investigation. In this work, we present a non-parametric space-time tomographic method for tackling such dynamic settings. Through a combination of a new CT image acquisition strategy, a space-time tomographic image formation model, and an alternating, multi-scale solver, we achieve a general approach that can be used to analyze a wide range of dynamic phenomena. We demonstrate our method with extensive experiments on both real and simulated data.	Space-time tomography for continuously deforming objects	NA:NA:NA:NA:NA:NA	2018
Peter Hedman:Johannes Kopf	We present an algorithm for constructing 3D panoramas from a sequence of aligned color-and-depth image pairs. Such sequences can be conveniently captured using dual lens cell phone cameras that reconstruct depth maps from synchronized stereo image capture. Due to the small baseline and resulting triangulation error the depth maps are considerably degraded and contain low-frequency error, which prevents alignment using simple global transformations. We propose a novel optimization that jointly estimates the camera poses as well as spatially-varying adjustment maps that are applied to deform the depth maps and bring them into good alignment. When fusing the aligned images into a seamless mosaic we utilize a carefully designed data term and the high quality of our depth alignment to achieve two orders of magnitude speedup w.r.t. previous solutions that rely on discrete optimization by removing the need for label smoothness optimization. Our algorithm processes about one input image per second, resulting in an end-to-end runtime of about one minute for mid-sized panoramas. The final 3D panoramas are highly detailed and can be viewed with binocular and head motion parallax in VR.	Instant 3D photography	NA:NA	2018
Thomas Whelan:Michael Goesele:Steven J. Lovegrove:Julian Straub:Simon Green:Richard Szeliski:Steven Butterfield:Shobhit Verma:Richard Newcombe	Planar reflective surfaces such as glass and mirrors are notoriously hard to reconstruct for most current 3D scanning techniques. When treated naïvely, they introduce duplicate scene structures, effectively destroying the reconstruction altogether. Our key insight is that an easy to identify structure attached to the scanner---in our case an AprilTag---can yield reliable information about the existence and the geometry of glass and mirror surfaces in a scene. We introduce a fully automatic pipeline that allows us to reconstruct the geometry and extent of planar glass and mirror surfaces while being able to distinguish between the two. Furthermore, our system can automatically segment observations of multiple reflective surfaces in a scene based on their estimated planes and locations. In the proposed setup, minimal additional hardware is needed to create high-quality results. We demonstrate this using reconstructions of several scenes with a variety of real mirrors and glass.	Reconstructing scenes with mirror and glass surfaces	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Bojian Wu:Yang Zhou:Yiming Qian:Minglun Cong:Hui Huang	Numerous techniques have been proposed for reconstructing 3D models for opaque objects in past decades. However, none of them can be directly applied to transparent objects. This paper presents a fully automatic approach for reconstructing complete 3D shapes of transparent objects. Through positioning an object on a turntable, its silhouettes and light refraction paths under different viewing directions are captured. Then, starting from an initial rough model generated from space carving, our algorithm progressively optimizes the model under three constraints: surface and refraction normal consistency, surface projection and silhouette consistency, and surface smoothness. Experimental results on both synthetic and real objects demonstrate that our method can successfully recover the complex shapes of transparent objects and faithfully reproduce their light refraction properties.	Full 3D reconstruction of transparent objects	NA:NA:NA:NA:NA	2018
Ligang Liu:Xi Xia:Han Sun:Qi Shen:Juzhan Xu:Bin Chen:Hui Huang:Kai Xu	To carry out autonomous 3D scanning and online reconstruction of unknown indoor scenes, one has to find a balance between global exploration of the entire scene and local scanning of the objects within it. In this work, we propose a novel approach, which provides object-aware guidance for autoscanning, for exploring, reconstructing, and understanding an unknown scene within one navigation pass. Our approach interleaves between object analysis to identify the next best object (NBO) for global exploration, and object-aware information gain analysis to plan the next best view (NBV) for local scanning. First, an objectness-based segmentation method is introduced to extract semantic objects from the current scene surface via a multi-class graph cuts minimization. Then, an object of interest (OOI) is identified as the NBO which the robot aims to visit and scan. The robot then conducts fine scanning on the OOI with views determined by the NBV strategy. When the OOI is recognized as a full object, it can be replaced by its most similar 3D model in a shape database. The algorithm iterates until all of the objects are recognized and reconstructed in the scene. Various experiments and comparisons have shown the feasibility of our proposed approach.	Object-aware guidance for autonomous scene reconstruction	NA:NA:NA:NA:NA:NA:NA:NA	2018
Yousuf Soliman:Dejan Slepčev:Keenan Crane	Angle-preserving or conformal surface parameterization has proven to be a powerful tool across applications ranging from geometry processing, to digital manufacturing, to machine learning, yet conformal maps can still suffer from severe area distortion. Cone singularities provide a way to mitigate this distortion, but finding the best configuration of cones is notoriously difficult. This paper develops a strategy that is globally optimal in the sense that it minimizes total area distortion among all possible cone configurations (number, placement, and size) that have no more than a fixed total cone angle. A key insight is that, for the purpose of optimization, one should not work directly with curvature measures (which naturally represent cone configurations), but can instead apply Fenchel-Rockafellar duality to obtain a formulation involving only ordinary functions. The result is a convex optimization problem, which can be solved via a sequence of sparse linear systems easily built from the usual cotangent Laplacian. The method supports user-defined notions of importance, constraints on cone angles (e.g., positive, or within a given range), and sophisticated boundary conditions (e.g., convex, or polygonal). We compare our approach to previous techniques on a variety of challenging models, often achieving dramatically lower distortion, and demonstrating that global optimality leads to extreme robustness in the presence of noise or poor discretization.	Optimal cone singularities for conformal flattening	NA:NA:NA	2018
Mina Konaković-Luković:Julian Panetta:Keenan Crane:Mark Pauly	Deployable structures are physical mechanisms that can easily transition between two or more geometric configurations; such structures enable industrial, scientific, and consumer applications at a wide variety of scales. This paper develops novel deployable structures that can approximate a large class of doubly-curved surfaces and are easily actuated from a flat initial state via inflation or gravitational loading. The structures are based on two-dimensional rigid mechanical linkages that implicitly encode the curvature of the target shape via a user-programmable pattern that permits locally isotropic scaling under load. We explicitly characterize the shapes that can be realized by such structures---in particular, we show that they can approximate target surfaces of positive mean curvature and bounded scale distortion relative to a given reference domain. Based on this observation, we develop efficient computational design algorithms for approximating a given input geometry. The resulting designs can be rapidly manufactured via digital fabrication technologies such as laser cutting, CNC milling, or 3D printing. We validate our approach through a series of physical prototypes and present several application case studies, ranging from surgical implants to large-scale deployable architecture.	Rapid deployment of curved surfaces via programmable auxetics	NA:NA:NA:NA	2018
Chi-Han Peng:Helmut Pottmann:Peter Wonka	We present a framework to generate mesh patterns that consist of a hybrid of both triangles and quads. Given a 3D surface, the generated patterns fit the surface boundaries and curvatures. Such regular and near regular triangle-quad hybrid meshes provide two key advantages: first, novel-looking polygonal patterns achieved by mixing different arrangements of triangles and quads together; second, a finer discretization of angle deficits than utilizing triangles or quads alone. Users have controls over the generated patterns in global and local levels. We demonstrate applications of our approach in architectural geometry and pattern design on surfaces.	Designing patterns using triangle-quad hybrid meshes	NA:NA:NA	2018
Nikunj Raghuvanshi:John Snyder	Convincing audio for games and virtual reality requires modeling directional propagation effects. The initial sound's arrival direction is particularly salient and derives from multiply-diffracted paths in complex scenes. When source and listener straddle occluders, the initial sound and multiply-scattered reverberation stream through gaps and portals, helping the listener navigate. Geometry near the source and/or listener reveals its presence through anisotropic reflections. We propose the first precomputed wave technique to capture such directional effects in general scenes comprising millions of polygons. These effects are formally represented with the 9D directional response function of 3D source and listener location, time, and direction at the listener, making memory use the major concern. We propose a novel parametric encoder that compresses this function within a budget of ~100MB for large scenes, while capturing many salient acoustic effects indoors and outdoors. The encoder is complemented with a lightweight signal processing algorithm whose filtering cost is largely insensitive to the number of sound sources, resulting in an immediately practical system.	Parametric directional coding for precomputed sound propagation	NA:NA	2018
Jui-Hsien Wang:Ante Qu:Timothy R. Langlois:Doug L. James	We explore an integrated approach to sound generation that supports a wide variety of physics-based simulation models and computer-animated phenomena. Targeting high-quality offline sound synthesis, we seek to resolve animation-driven sound radiation with near-field scattering and diffraction effects. The core of our approach is a sharp-interface finite-difference time-domain (FDTD) wavesolver, with a series of supporting algorithms to handle rapidly deforming and vibrating embedded interfaces arising in physics-based animation sound. Once the solver rasterizes these interfaces, it must evaluate acceleration boundary conditions (BCs) that involve model-and phenomena-specific computations. We introduce acoustic shaders as a mechanism to abstract away these complexities, and describe a variety of implementations for computer animation: near-rigid objects with ringing and acceleration noise, deformable (finite element) models such as thin shells, bubble-based water, and virtual characters. Since time-domain wave synthesis is expensive, we only simulate pressure waves in a small region about each sound source, then estimate a far-field pressure signal. To further improve scalability beyond multi-threading, we propose a fully time-parallel sound synthesis method that is demonstrated on commodity cloud computing resources. In addition to presenting results for multiple animation phenomena (water, rigid, shells, kinematic deformers, etc.) we also propose 3D automatic dialogue replacement (3DADR) for virtual characters so that pre-recorded dialogue can include character movement, and near-field shadowing and scattering sound effects.	Toward wave-based sound synthesis for computer animation	NA:NA:NA:NA	2018
Gabriel Cirio:Ante Qu:George Drettakis:Eitan Grinspun:Changxi Zheng	Thin shells --- solids that are thin in one dimension compared to the other two --- often emit rich nonlinear sounds when struck. Strong excitations can even cause chaotic thin-shell vibrations, producing sounds whose energy spectrum diffuses from low to high frequencies over time --- a phenomenon known as wave turbulence. It is all these nonlinearities that grant shells such as cymbals and gongs their characteristic "glinting" sound. Yet, simulation models that efficiently capture these sound effects remain elusive. We propose a physically based, multi-scale reduced simulation method to synthesize nonlinear thin-shell sounds. We first split nonlinear vibrations into two scales, with a small low-frequency part simulated in a fully nonlinear way, and a high-frequency part containing many more modes approximated through time-varying linearization. This allows us to capture interesting nonlinearities in the shells' deformation, tens of times faster than previous approaches. Furthermore, we propose a method that enriches simulated sounds with wave turbulent sound details through a phenomenological diffusion model in the frequency domain, and thereby sidestep the expensive simulation of chaotic high-frequency dynamics. We show several examples of our simulations, illustrating the efficiency and realism of our model.	Multi-scale simulation of nonlinear thin-shell sound with wave turbulence	NA:NA:NA:NA:NA	2018
Dingzeyu Li:Timothy R. Langlois:Changxi Zheng	Although 360° cameras ease the capture of panoramic footage, it remains challenging to add realistic 360° audio that blends into the captured scene and is synchronized with the camera motion. We present a method for adding scene-aware spatial audio to 360° videos in typical indoor scenes, using only a conventional mono-channel microphone and a speaker. We observe that the late reverberation of a room's impulse response is usually diffuse spatially and directionally. Exploiting this fact, we propose a method that synthesizes the directional impulse response between any source and listening locations by combining a synthesized early reverberation part and a measured late reverberation tail. The early reverberation is simulated using a geometric acoustic simulation and then enhanced using a frequency modulation method to capture room resonances. The late reverberation is extracted from a recorded impulse response, with a carefully chosen time duration that separates out the late reverberation from the early reverberation. In our validations, we show that our synthesized spatial audio matches closely with recordings using ambisonic microphones. Lastly, we demonstrate the strength of our method in several applications.	Scene-aware audio for 360° videos	NA:NA:NA	2018
Ariel Ephrat:Inbar Mosseri:Oran Lang:Tali Dekel:Kevin Wilson:Avinatan Hassidim:William T. Freeman:Michael Rubinstein	We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to "focus" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).	Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation	NA:NA:NA:NA:NA:NA:NA:NA	2018
David B. Lindell:Matthew O'Toole:Gordon Wetzstein	Sensors which capture 3D scene information provide useful data for tasks in vehicle navigation, gesture recognition, human pose estimation, and geometric reconstruction. Active illumination time-of-flight sensors in particular have become widely used to estimate a 3D representation of a scene. However, the maximum range, density of acquired spatial samples, and overall acquisition time of these sensors is fundamentally limited by the minimum signal required to estimate depth reliably. In this paper, we propose a data-driven method for photon-efficient 3D imaging which leverages sensor fusion and computational reconstruction to rapidly and robustly estimate a dense depth map from low photon counts. Our sensor fusion approach uses measurements of single photon arrival times from a low-resolution single-photon detector array and an intensity image from a conventional high-resolution camera. Using a multi-scale deep convolutional network, we jointly process the raw measurements from both sensors and output a high-resolution depth map. To demonstrate the efficacy of our approach, we implement a hardware prototype and show results using captured data. At low signal-to-background levels, our depth reconstruction algorithm with sensor fusion outperforms other methods for depth estimation from noisy measurements of photon arrival times.	Single-photon 3D imaging with deep sensor fusion	NA:NA:NA	2018
Vincent Sitzmann:Steven Diamond:Yifan Peng:Xiong Dun:Stephen Boyd:Wolfgang Heidrich:Felix Heide:Gordon Wetzstein	In typical cameras the optical system is designed first; once it is fixed, the parameters in the image processing algorithm are tuned to get good image reproduction. In contrast to this sequential design approach, we consider joint optimization of an optical system (for example, the physical shape of the lens) together with the parameters of the reconstruction algorithm. We build a fully-differentiable simulation model that maps the true source image to the reconstructed one. The model includes diffractive light propagation, depth and wavelength-dependent effects, noise and nonlinearities, and the image post-processing. We jointly optimize the optical parameters and the image processing algorithm parameters so as to minimize the deviation between the true and reconstructed image, over a large set of images. We implement our joint optimization method using autodifferentiation to efficiently compute parameter gradients in a stochastic optimization algorithm. We demonstrate the efficacy of this approach by applying it to achromatic extended depth of field and snapshot super-resolution imaging.	End-to-end optimization of optics and image processing for achromatic extended depth of field and super-resolution imaging	NA:NA:NA:NA:NA:NA:NA:NA	2018
Congli Wang:Qiang Fu:Xiong Dun:Wolfgang Heidrich	Adaptive optics has become a valuable tool for correcting minor optical aberrations in applications such as astronomy and microscopy. However, due to the limited resolution of both the wavefront sensing and the wavefront correction hardware, it has so far not been feasible to use adaptive optics for correcting large-scale waveform deformations that occur naturally in regular photography and other imaging applications. In this work, we demonstrate an adaptive optics system for regular cameras. We achieve a significant improvement in focus for large wavefront distortions by improving upon a recently developed high resolution coded wavefront sensor, and combining it with a spatial phase modulator to create a megapixel adaptive optics system with unprecedented capability to sense and correct large distortions.	Megapixel adaptive optics: towards correcting large-scale distortions in computational cameras	NA:NA:NA:NA	2018
Nanxuan Zhao:Ying Cao:Rynson W. H. Lau	Graphic designers often manipulate the overall look and feel of their designs to convey certain personalities (e.g., cute, mysterious and romantic) to impress potential audiences and achieve business goals. However, understanding the factors that determine the personality of a design is challenging, as a graphic design is often a result of thousands of decisions on numerous factors, such as font, color, image, and layout. In this paper, we aim to answer the question of what characterizes the personality of a graphic design. To this end, we propose a deep learning framework for exploring the effects of various design factors on the perceived personalities of graphic designs. Our framework learns a convolutional neural network (called personality scoring network) to estimate the personality scores of graphic designs by ranking the crawled web data. Our personality scoring network automatically learns a visual representation that captures the semantics necessary to predict graphic design personality. With our personality scoring network, we systematically and quantitatively investigate how various design factors (e.g., color, font, and layout) affect design personality across different scales (from pixels, regions to elements). We also demonstrate a number of practical application scenarios of our network, including element-level design suggestion and example-based personality transfer.	What characterizes personalities of graphic designs?	NA:NA:NA	2018
You-En Lin:Yong-Liang Yang:Hung-Kuo Chu	Flat design is a modern style of graphics design that minimizes the number of design attributes required to convey 3D shapes. This approach suits design contexts requiring simplicity and efficiency, such as mobile computing devices. This `less-is-more' design inspiration has posed significant challenges in practice since it selects from a restricted range of design elements (e.g., color and resolution) to represent complex shapes. In this work, we investigate a means of computationally generating a specialized 2D flat representation - image formed by black-and-white patches - from 3D shapes. We present a novel framework that automatically abstracts 3D man-made shapes into 2D binary images at multiple scales. Based on a set of identified design principles related to the inference of geometry and structure, our framework jointly analyzes the input 3D shape and its counterpart 2D representation, followed by executing a carefully devised layout optimization algorithm. The robustness and effectiveness of our method are demonstrated by testing it on a wide variety of man-made shapes and comparing the results with baseline methods via a pilot user study. We further present two practical applications that are likely to benefit from our work.	Scale-aware black-and-white abstraction of 3D shapes	NA:NA:NA	2018
Shayan Hoshyari:Edoardo Alberto Dominici:Alla Sheffer:Nathan Carr:Zhaowen Wang:Duygu Ceylan:I-Chao Shen	Artist-drawn images with distinctly colored, piecewise continuous boundaries, which we refer to as semi-structured imagery, are very common in online raster databases and typically allow for a perceptually unambiguous mental vector interpretation. Yet, perhaps surprisingly, existing vectorization algorithms frequently fail to generate these viewer-expected interpretations on such imagery. In particular, the vectorized region boundaries they produce frequently diverge from those anticipated by viewers. We propose a new approach to region boundary vectorization that targets semi-structured inputs and leverages observations about human perception of shapes to generate vector images consistent with viewer expectations. When viewing raster imagery observers expect the vector output to be an accurate representation of the raster input. However, perception studies suggest that viewers implicitly account for the lossy nature of the rasterization process and mentally smooth and simplify the observed boundaries. Our core algorithmic challenge is to balance these conflicting cues and obtain a piecewise continuous vectorization whose discontinuities, or corners, are aligned with human expectations. Our framework centers around a simultaneous spline fitting and corner detection method that combines a learned metric, that approximates human perception of boundary discontinuities on raster inputs, with perception-driven algorithmic discontinuity analysis. The resulting method balances local cues provided by the learned metric with global cues obtained by balancing simplicity and continuity expectations. Given the finalized set of corners, our framework connects those using simple, continuous curves that capture input regularities. We demonstrate our method on a range of inputs and validate its superiority over existing alternatives via an extensive comparative user study.	Perception-driven semi-structured boundary vectorization	NA:NA:NA:NA:NA:NA:NA	2018
Stephen W. Bailey:Dave Otte:Paul Dilorenzo:James F. O'Brien	Character rigs are procedural systems that compute the shape of an animated character for a given pose. They can be highly complex and must account for bulges, wrinkles, and other aspects of a character's appearance. When comparing film-quality character rigs with those designed for real-time applications, there is typically a substantial and readily apparent difference in the quality of the mesh deformations. Real-time rigs are limited by a computational budget and often trade realism for performance. Rigs for film do not have this same limitation, and character riggers can make the rig as complicated as necessary to achieve realistic deformations. However, increasing the rig complexity slows rig evaluation, and the animators working with it can become less efficient and may experience frustration. In this paper, we present a method to reduce the time required to compute mesh deformations for film-quality rigs, allowing better interactivity during animation authoring and use in real-time games and applications. Our approach learns the deformations from an existing rig by splitting the mesh deformation into linear and nonlinear portions. The linear deformations are computed directly from the transformations of the rig's underlying skeleton. We use deep learning methods to approximate the remaining nonlinear portion. In the examples we show from production rigs used to animate lead characters, our approach reduces the computational time spent on evaluating deformations by a factor of 5X-10X. This significant savings allows us to run the complex, film-quality rigs in real-time even when using a CPU-only implementation on a mobile device.	Fast and deep deformation approximations	NA:NA:NA:NA	2018
Jiong Chen:Hujun Bao:Tianyu Wang:Mathieu Desbrun:Jin Huang	In this paper, an efficient and scalable approach for simulating inhomogeneous and non-linear elastic objects is introduced. Our numerical coarsening approach consists in optimizing non-conforming and matrix-valued shape functions to allow for predictive simulation of heterogeneous materials with non-linear constitutive laws even on coarse grids, thus saving orders of magnitude in computational time compared to traditional finite element computations. The set of local shape functions over coarse elements is carefully tailored in a preprocessing step to balance geometric continuity and local material stiffness. In particular, we do not impose continuity of our material-aware shape functions between neighboring elements to significantly reduce the fictitious numerical stiffness that conforming bases induce; however, we enforce crucial geometric and physical properties such as partition of unity and exact reproduction of representative fine displacements to eschew the use of discontinuous Galerkin methods. We demonstrate that we can simulate, with no parameter tuning, inhomogeneous and non-linear materials significantly better than previous approaches that traditionally try to homogenize the constitutive model instead.	Numerical coarsening using discontinuous shape functions	NA:NA:NA:NA:NA	2018
Seung-Wook Kim:Sun Young Park:Junghyun Han	The goal of this paper is to simulate the interactions between magnetic objects in a physically correct way. The simulation scheme is based on magnetization dynamics, which describes the temporal change of magnetic moments. For magnetization dynamics, the Landau-Lifshitz-Gilbert equation is adopted, which is widely used in micromagnetics. Through effectively-designed novel models of magnets, it is extended into the macro scale so as to be combined with real-time rigid-body dynamics. The overall simulation is stable and enables us to implement mutual induction and remanence that have not been tackled by the state-of-the-art technique in magnet simulation. The proposed method can be applied to various fields including magnet experiments in the virtual world.	Magnetization dynamics for magnetic object interactions	NA:NA:NA	2018
Abe Davis:Maneesh Agrawala	We present a visual analogue for musical rhythm derived from an analysis of motion in video, and show that alignment of visual rhythm with its musical counterpart results in the appearance of dance. Central to our work is the concept of visual beats --- patterns of motion that can be shifted in time to control visual rhythm. By warping visual beats into alignment with musical beats, we can create or manipulate the appearance of dance in video. Using this approach we demonstrate a variety of retargeting applications that control musical synchronization of audio and video: we can change what song performers are dancing to, warp irregular motion into alignment with music so that it appears to be dancing, or search collections of video for moments of accidentally dance-like motion that can be used to synthesize musical performances.	Visual rhythm and beat	NA:NA	2018
Michal Piovarči:David I. W. Levin:Danny M. Kaufman:Piotr Didyk	Digital drawing is becoming a favorite technique for many artists. It allows for quick swaps between different materials, reverting changes, and applying selective modifications to finished artwork. These features enable artists to be more efficient and creative. A significant disadvantage of digital drawing is poor haptic feedback. Artists are usually limited to one surface and a few different stylus nibs, and while they try to find a combination that suits their needs, this is typically challenging. In this work, we address this problem and propose a method for designing, evaluating, and optimizing different stylus designs. We begin with collecting a representative set of traditional drawing tools. We measure their physical properties and conduct a user experiment to build a perceptual space that encodes perceptually-relevant attributes of drawing materials. The space is optimized to both explain our experimental data and correlate it with measurable physical properties. To embed new drawing tool designs into the space without conducting additional experiments and measurements, we propose a new, data-driven simulation technique for characterizing stylus-surface interaction. We finally leverage the perceptual space, our simulation, and recent advancements in multi-material 3D printing to demonstrate the application of our system in the design of new digital drawing tools that mimic traditional drawing materials.	Perception-aware modeling and fabrication of digital drawing tools	NA:NA:NA:NA	2018
Thijs Vogels:Fabrice Rousselle:Brian Mcwilliams:Gerhard Röthlin:Alex Harvill:David Adler:Mark Meyer:Jan Novák	We present a modular convolutional architecture for denoising rendered images. We expand on the capabilities of kernel-predicting networks by combining them with a number of task-specific modules, and optimizing the assembly using an asymmetric loss. The source-aware encoder---the first module in the assembly---extracts low-level features and embeds them into a common feature space, enabling quick adaptation of a trained network to novel data. The spatial and temporal modules extract abstract, high-level features for kernel-based reconstruction, which is performed at three different spatial scales to reduce low-frequency artifacts. The complete network is trained using a class of asymmetric loss functions that are designed to preserve details and provide the user with a direct control over the variance-bias trade-off during inference. We also propose an error-predicting module for inferring reconstruction error maps that can be used to drive adaptive sampling. Finally, we present a theoretical analysis of convergence rates of kernel-predicting architectures, shedding light on why kernel prediction performs better than synthesizing the colors directly, complementing the empirical evidence presented in this and previous works. We demonstrate that our networks attain results that compare favorably to state-of-the-art methods in terms of detail preservation, low-frequency noise removal, and temporal stability on a variety of production and academic datasets.	Denoising with kernel prediction and asymmetric loss functions	NA:NA:NA:NA:NA:NA:NA:NA	2018
Petr Vévoda:Ivo Kondapaneni:Jaroslav Křivánek	Direct illumination calculation is an important component of any physically-based Tenderer with a substantial impact on the overall performance. We present a novel adaptive solution for unbiased Monte Carlo direct illumination sampling, based on online learning of the light selection probability distributions. Our main contribution is a formulation of the learning process as Bayesian regression, based on a new, specifically designed statistical model of direct illumination. The net result is a set of regularization strategies to prevent over-fitting and ensure robustness even in early stages of calculation, when the observed information is sparse. The regression model captures spatial variation of illumination, which enables aggregating statistics over relatively large scene regions and, in turn, ensures a fast learning rate. We make the method scalable by adopting a light clustering strategy from the Lightcuts method, and further reduce variance through the use of control variates. As a main design feature, the resulting algorithm is virtually free of any preprocessing, which enables its use for interactive progressive rendering, while the online learning still enables super-linear convergence.	Bayesian online regression for adaptive direct illumination sampling	NA:NA:NA	2018
Zexiang Xu:Kalyan Sunkavalli:Sunil Hadap:Ravi Ramamoorthi	We present an image-based relighting method that can synthesize scene appearance under novel, distant illumination from the visible hemisphere, from only five images captured under pre-defined directional lights. Our method uses a deep convolutional neural network to regress the relit image from these five images; this relighting network is trained on a large synthetic dataset comprised of procedurally generated shapes with real-world reflectances. We show that by combining a custom-designed sampling network with the relighting network, we can jointly learn both the optimal input light directions and the relighting function. We present an extensive evaluation of our network, including an empirical analysis of reconstruction quality, optimal lighting configurations for different scenarios, and alternative network architectures. We demonstrate, on both synthetic and real scenes, that our method is able to reproduce complex, high-frequency lighting effects like specularities and cast shadows, and outperforms other image-based relighting methods that require an order of magnitude more images.	Deep image-based relighting from optimal sparse samples	NA:NA:NA:NA	2018
Kaizhang Kang:Zimin Chen:Jiaping Wang:Kun Zhou:Hongzhi Wu	We propose a novel framework that automatically learns the lighting patterns for efficient reflectance acquisition, as well as how to faithfully reconstruct spatially varying anisotropic BRDFs and local frames from measurements under such patterns. The core of our framework is an asymmetric deep autoencoder, consisting of a nonnegative, linear encoder which directly corresponds to the lighting patterns used in physical acquisition, and a stacked, nonlinear decoder which computationally recovers the BRDF information from captured photographs. The autoencoder is trained with a large amount of synthetic reflectance data, and can adapt to various factors, including the geometry of the setup and the properties of appearance. We demonstrate the effectiveness of our framework on a wide range of physical materials, using as few as 16 ~ 32 lighting patterns, which correspond to 12 ~ 25 seconds of acquisition time. We also validate our results with the ground truth data and captured photographs. Our framework is useful for increasing the efficiency in both novel and existing acquisition setups.	Efficient reflectance capture using an autoencoder	NA:NA:NA:NA:NA	2018
Valentin Deschaintre:Miika Aittala:Fredo Durand:George Drettakis:Adrien Bousseau	Texture, highlights, and shading are some of many visual cues that allow humans to perceive material appearance in single pictures. Yet, recovering spatially-varying bi-directional reflectance distribution functions (SVBRDFs) from a single image based on such cues has challenged researchers in computer graphics for decades. We tackle lightweight appearance capture by training a deep neural network to automatically extract and make sense of these visual cues. Once trained, our network is capable of recovering per-pixel normal, diffuse albedo, specular albedo and specular roughness from a single picture of a flat surface lit by a hand-held flash. We achieve this goal by introducing several innovations on training data acquisition and network design. For training, we leverage a large dataset of artist-created, procedural SVBRDFs which we sample and render under multiple lighting directions. We further amplify the data by material mixing to cover a wide diversity of shading effects, which allows our network to work across many material classes. Motivated by the observation that distant regions of a material sample often offer complementary visual cues, we design a network that combines an encoder-decoder convolutional track for local feature extraction with a fully-connected track for global feature extraction and propagation. Many important material effects are view-dependent, and as such ambiguous when observed in a single image. We tackle this challenge by defining the loss as a differentiable SVBRDF similarity metric that compares the renderings of the predicted maps against renderings of the ground truth from several lighting and viewing directions. Combined together, these novel ingredients bring clear improvement over state of the art methods for single-shot capture of spatially varying BRDFs.	Single-image SVBRDF capture with a rendering-aware deep network	NA:NA:NA:NA:NA	2018
Jonàs Martínez:Samuel Hornus:Haichuan Song:Sylvain Lefebvre	A critical advantage of additive manufacturing is its ability to fabricate complex small-scale structures. These microstructures can be understood as a metamaterial: they exist at a much smaller scale than the volume they fill, and are collectively responsible for an average elastic behavior different from that of the base printing material making the fabricated object lighter and/or flexible along specific directions. In addition, the average behavior can be graded spatially by progressively modifying the micro structure geometry. The definition of a microstructure is a careful trade-off between the geometric requirements of manufacturing and the properties one seeks to obtain within a shape: in our case a wide range of elastic behaviors. Most existing microstructures are designed for stereolithography (SLA) and laser sintering (SLS) processes. The requirements are however different than those of continuous deposition systems such as fused filament fabrication (FFF), for which there is currently a lack of microstructures enabling graded elastic behaviors. In this work we introduce a novel type of microstructures that strictly enforce all the requirements of FFF-like processes: continuity, self-support and overhang angles. They offer a range of orthotropic elastic responses that can be graded spatially. This allows to fabricate parts usually reserved to the most advanced technologies on widely available inexpensive printers that also benefit from a continuously expanding range of materials.	Polyhedral voronoi diagrams for additive manufacturing	NA:NA:NA:NA	2018
Kui Wu:Xifeng Gao:Zachary Ferguson:Daniele Panozzo:Cem Yuksel	We introduce the first fully automatic pipeline to convert arbitrary 3D shapes into knit models. Our pipeline is based on a global parametrization remeshing pipeline to produce an isotropic quad-dominant mesh aligned with a 2-RoSy field. The knitting directions over the surface are determined using a set of custom topological operations and a two-step global optimization that minimizes the number of irregularities. The resulting mesh is converted into a valid stitch mesh that represents the knit model. The yarn curves are generated from the stitch mesh and the final yarn geometry is computed using a yarn-level relaxation process. Thus, we produce topologically valid models that can be used with a yarn-level simulation. We validate our algorithm by automatically generating knit models from complex 3D shapes and processing over a hundred models with various shapes without any user input or parameter tuning. We also demonstrate applications of our approach for custom knit model generation using fabrication via 3D printing.	Stitch meshing	NA:NA:NA:NA:NA	2018
Adriana Schulz:Harrison Wang:Eitan Grinspun:Justin Solomon:Wojciech Matusik	Typical design for manufacturing applications requires simultaneous optimization of conflicting performance objectives: Design variations that improve one performance metric may decrease another performance metric. In these scenarios, there is no unique optimal design but rather a set of designs that are optimal for different trade-offs (called Pareto-optimal). In this work, we propose a novel approach to discover the Pareto front, allowing designers to navigate the landscape of compromises efficiently. Our approach is based on a first-order approximation of the Pareto front, which allows entire neighborhoods rather than individual points on the Pareto front to be captured. In addition to allowing for efficient discovery of the Pareto front and the corresponding mapping to the design space, this approach allows us to represent the entire trade-off manifold as a small collection of patches that comprise a high-quality and piecewise-smooth approximation. We illustrate how this technique can be used for navigating performance trade-offs in computer-aided design (CAD) models.	Interactive exploration of design trade-offs	NA:NA:NA:NA:NA	2018
Mengqi Peng:Jun Xing:Li-Yi Wei	Digital sculpting is a popular means to create 3D models but remains a challenging task. We propose a 3D sculpting system that assists users, especially novices, in freely creating models with reduced input labor and enhanced output quality. With an interactive sculpting interface, our system silently records and analyzes users' workflows including brush strokes and camera movements, and predicts what they might do in the future. Users can accept, partially accept, or ignore the suggestions and thus retain full control and individual style. They can also explicitly select and clone past workflows over output model regions. Our key idea is to consider how a model is authored via dynamic workflows in addition to what is shaped in static geometry. This allows our method for more accurate analysis of user intentions and more general synthesis of shape structures than prior workflow or geometry methods, such as large overlapping deformations. We evaluate our method via user feedbacks and authored models.	Autocomplete 3D sculpting	NA:NA:NA	2018
Minchen Li:Alla Sheffer:Eitan Grinspun:Nicholas Vining	While folds and pleats add interest to garments and cloth objects, incorporating them into an existing design manually or using existing software requires expertise and time. We present FoldSketch, a new system that supports simple and intuitive fold and pleat design. FoldSketch users specify the fold or pleat configuration they seek using a simple schematic sketching interface; the system then algorithmically generates both the fold-enhanced 3D garment geometry that conforms to user specifications, and the corresponding 2D patterns that reproduce this geometry within a simulation engine. While previous work aspired to compute the desired patterns for a given target 3D garment geometry, our main algorithmic challenge is that we do not have target geometry to start with. Real-life garment folds have complex profile shapes, and their exact geometry and location on a garment are intricately linked to a range of physical factors such as fabric properties and the garment's interaction with the wearer's body; it is therefore virtually impossible to predict the 3D shape of a fold-enhanced garment using purely geometric means. At the same time, using physical simulation to model folds requires appropriate 2D patterns and initial drape, neither of which can be easily provided by the user. We obtain both the 3D fold-enhanced garment and its corresponding patterns and initial drape via an alternating 2D-3D algorithm. We first expand the input patterns by allocating excess material for the expected fold formation; we then use these patterns to produce an estimated fold-enhanced drape geometry that balances designer expectations against physical reproducibility. We use the patterns and the estimated drape as input to a simulation generating an initial reproducible output. We improve the output's alignment with designer expectations by progressively refining the patterns and the estimated drape, converging to a final fully physically reproducible fold-enhanced garment. Our experiments confirm that FoldSketch reliably converges to a desired garment geometry and corresponding patterns and drape, and works well with different physical simulators. We demonstrate the versatility of our approach by showcasing a collection of garments augmented with diverse fold and pleat layouts specified via the FoldSketch interface, and further validate our approach via comparisons to alternative solutions and feedback from potential users.	Foldsketch: enriching garments with physically reproducible folds	NA:NA:NA:NA	2018
Chengkai Dai:Charlie C. L. Wang:Chenming Wu:Sylvain Lefebvre:Guoxin Fang:Yong-Jin Liu	This paper presents a new method to fabricate 3D models on a robotic printing system equipped with multi-axis motion. Materials are accumulated inside the volume along curved tool-paths so that the need of supporting structures can be tremendously reduced - if not completely abandoned - on all models. Our strategy to tackle the challenge of tool-path planning for multi-axis 3D printing is to perform two successive decompositions, first volume-to-surfaces and then surfaces-to-curves. The volume-to-surfaces decomposition is achieved by optimizing a scalar field within the volume that represents the fabrication sequence. The field is constrained such that its iso-values represent curved layers that are supported from below, and present a convex surface affording for collision-free navigation of the printer head. After extracting all curved layers, the surfaces-to-curves decomposition covers them with tool-paths while taking into account constraints from the robotic printing system. Our method successfully generates tool-paths for 3D printing models with large overhangs and high-genus topology. We fabricated several challenging cases on our robotic platform to verify and demonstrate its capabilities.	Support-free volume printing by multi-axis motion	NA:NA:NA:NA:NA:NA	2018
Kazutaka Nakashima:Thomas Auzinger:Emmanuel Iarussi:Ran Zhang:Takeo Igarashi:Bernd Bickel	Molding is a popular mass production method, in which the initial expenses for the mold are offset by the low per-unit production cost. However, the physical fabrication constraints of the molding technique commonly restrict the shape of moldable objects. For a complex shape, a decomposition of the object into moldable parts is a common strategy to address these constraints, with plastic model kits being a popular and illustrative example. However, conducting such a decomposition requires considerable expertise, and it depends on the technical aspects of the fabrication technique, as well as aesthetic considerations. We present an interactive technique to create such decompositions for two-piece molding, in which each part of the object is cast between two rigid mold pieces. Given the surface description of an object, we decompose its thin-shell equivalent into moldable parts by first performing a coarse decomposition and then utilizing an active contour model for the boundaries between individual parts. Formulated as an optimization problem, the movement of the contours is guided by an energy reflecting fabrication constraints to ensure the moldability of each part. Simultaneously the user is provided with editing capabilities to enforce aesthetic guidelines. Our interactive interface provides control of the contour positions by allowing, for example, the alignment of part boundaries with object features. Our technique enables a novel workflow, as it empowers novice users to explore the design space, and it generates fabrication-ready two-piece molds that can be used either for casting or industrial injection molding of free-form objects.	CoreCavity: interactive shell decomposition for fabrication with two-piece rigid molds	NA:NA:NA:NA:NA:NA	2018
Thomas Alderighi:Luigi Malomo:Daniela Giorgi:Nico Pietroni:Bernd Bickel:Paolo Cignoni	We propose a new method for fabricating digital objects through reusable silicone molds. Molds are generated by casting liquid silicone into custom 3D printed containers called metamolds. Metamolds automatically define the cuts that are needed to extract the cast object from the silicone mold. The shape of metamolds is designed through a novel segmentation technique, which takes into account both geometric and topological constraints involved in the process of mold casting. Our technique is simple, does not require changing the shape or topology of the input objects, and only requires of-the-shelf materials and technologies. We successfully tested our method on a set of challenging examples with complex shapes and rich geometric detail.	Metamolds: computational design of silicone molds	NA:NA:NA:NA:NA:NA	2018
Haisen Zhao:Hao Zhang:Shiqing Xin:Yuanmin Deng:Changhe Tu:Wenping Wang:Daniel Cohen-Or:Baoquan Chen	We present an automatic algorithm for subtractive manufacturing of freeform 3D objects using high-speed machining (HSM) via CNC. A CNC machine operates a cylindrical cutter to carve off material from a 3D shape stock, following a tool path, to "expose" the target object. Our method decomposes the input object's surface into a small number of patches each of which is fully accessible and machinable by the CNC machine, in continuous fashion, under a fixed cutter-object setup configuration. This is achieved by covering the input surface with a minimum number of accessible regions and then extracting a set of machinable patches from each accessible region. For each patch obtained, we compute a continuous, space-filling, and iso-scallop tool path which conforms to the patch boundary, enabling efficient carving with high-quality surface finishing. The tool path is generated in the form of connected Fermat spirals, which have been generalized from a 2D fill pattern for layered manufacturing to work for curved surfaces. Furthermore, we develop a novel method to control the spacing of Fermat spirals based on directional surface curvature and adapt the heat method to obtain iso-scallop carving. We demonstrate automatic generation of accessible and machinable surface decompositions and iso-scallop Fermat spiral carving paths for freeform 3D objects. Comparisons are made to tool paths generated by commercial software in terms of real machining time and surface quality.	DSCarver: decompose-and-spiral-carve for subtractive manufacturing	NA:NA:NA:NA:NA:NA:NA:NA	2018
Alex Poms:Will Crichton:Pat Hanrahan:Kayvon Fatahalian	A growing number of visual computing applications depend on the analysis of large video collections. The challenge is that scaling applications to operate on these datasets requires efficient systems for pixel data access and parallel processing across large numbers of machines. Few programmers have the capability to operate efficiently at these scales, limiting the field's ability to explore new applications that leverage big video data. In response, we have created Scanner, a system for productive and efficient video analysis at scale. Scanner organizes video collections as tables in a data store optimized for sampling frames from compressed video, and executes pixel processing computations, expressed as dataflow graphs, on these frames. Scanner schedules video analysis applications expressed using these abstractions onto heterogeneous throughput computing hardware, such as multi-core CPUs, GPUs, and media processing ASICs, for high-throughput pixel processing. We demonstrate the productivity of Scanner by authoring a variety of video processing applications including the synthesis of stereo VR video streams from multi-camera rigs, markerless 3D human pose reconstruction from video, and data-mining big video datasets such as hundreds of feature-length films or over 70,000 hours of TV news. These applications achieve near-expert performance on a single machine and scale efficiently to hundreds of machines, enabling formerly long-running big video data analysis tasks to be carried out in minutes to hours.	Scanner: efficient video analysis at scale	NA:NA:NA:NA	2018
Tzu-Mao Li:Michaël Gharbi:Andrew Adams:Frédo Durand:Jonathan Ragan-Kelley	Gradient-based optimization has enabled dramatic advances in computational imaging through techniques like deep learning and nonlinear optimization. These methods require gradients not just of simple mathematical functions, but of general programs which encode complex transformations of images and graphical data. Unfortunately, practitioners have traditionally been limited to either hand-deriving gradients of complex computations, or composing programs from a limited set of coarse-grained operators in deep learning frameworks. At the same time, writing programs with the level of performance needed for imaging and deep learning is prohibitively difficult for most programmers. We extend the image processing language Halide with general reverse-mode automatic differentiation (AD), and the ability to automatically optimize the implementation of gradient computations. This enables automatic computation of the gradients of arbitrary Halide programs, at high performance, with little programmer effort. A key challenge is to structure the gradient code to retain parallelism. We define a simple algorithm to automatically schedule these pipelines, and show how Halide's existing scheduling primitives can express and extend the key AD optimization of "checkpointing." Using this new tool, we show how to easily define new neural network layers which automatically compile to high-performance GPU implementations, and how to solve nonlinear inverse problems from computational imaging. Finally, we show how differentiable programming enables dramatically improving the quality of even traditional, feed-forward image processing algorithms, blurring the distinction between classical and deep methods.	Differentiable programming for image processing and deep learning in halide	NA:NA:NA:NA:NA	2018
Michael Kenzel:Bernhard Kerbl:Dieter Schmalstieg:Markus Steinberger	In this paper, we present a real-time graphics pipeline implemented entirely in software on a modern GPU. As opposed to previous work, our approach features a fully-concurrent, multi-stage, streaming design with dynamic load balancing, capable of operating efficiently within bounded memory. We address issues such as primitive order, vertex reuse, and screen-space derivatives of dependent variables, which are essential to real-world applications, but have largely been ignored by comparable work in the past. The power of a software approach lies in the ability to tailor the graphics pipeline to any given application. In exploration of this potential, we design and implement four novel pipeline modifications. Evaluation of the performance of our approach on more than 100 real-world scenes collected from video games shows rendering speeds within one order of magnitude of the hardware graphics pipeline as well as significant improvements over previous work, not only in terms of capabilities and performance, but also robustness.	A high-performance software graphics pipeline architecture for the GPU	NA:NA:NA:NA	2018
Yong He:Kayvon Fatahalian:Tim Foley	Designers of real-time rendering engines must balance the conflicting goals of maintaining clear, extensible shading systems and achieving high rendering performance. In response, engine architects have established effective design patterns for authoring shading systems, and developed engine-specific code synthesis tools, ranging from preprocessor hacking to domain-specific shading languages, to productively implement these patterns. The problem is that proprietary tools add significant complexity to modern engines, lack advanced language features, and create additional challenges for learning and adoption. We argue that the advantages of engine-specific code generation tools can be achieved using the underlying GPU shading language directly, provided the shading language is extended with a small number of best-practice principles from modern, well-established programming languages. We identify that adding generics with interface constraints, associated types, and interface/structure extensions to existing C-like GPU shading languages enables real-time Tenderer developers to build shading systems that are extensible, maintainable, and execute efficiently on modern GPUs without the need for additional domain-specific tools. We embody these ideas in an extension of HLSL called Slang, and provide a reference design for a large, extensible shader library implemented using Slang's features. We rearchitect an open source Tenderer to use this library and Slang's compiler services, and demonstrate the resulting shading system is substantially simpler, easier to extend with new features, and achieves higher rendering performance than the original HLSL-based implementation.	Slang: language mechanisms for extensible real-time shading systems	NA:NA:NA	2018
Libin Liu:Jessica Hodgins	Basketball is one of the world's most popular sports because of the agility and speed demonstrated by the players. This agility and speed makes designing controllers to realize robust control of basketball skills a challenge for physics-based character animation. The highly dynamic behaviors and precise manipulation of the ball that occur in the game are difficult to reproduce for simulated players. In this paper, we present an approach for learning robust basketball dribbling controllers from motion capture data. Our system decouples a basketball controller into locomotion control and arm control components and learns each component separately. To achieve robust control of the ball, we develop an efficient pipeline based on trajectory optimization and deep reinforcement learning and learn non-linear arm control policies. We also present a technique for learning skills and the transition between skills simultaneously. Our system is capable of learning robust controllers for various basketball dribbling skills, such as dribbling between the legs and crossover moves. The resulting control graphs enable a simulated player to perform transitions between these skills and respond to user interaction.	Learning basketball dribbling skills using trajectory optimization and deep reinforcement learning	NA:NA	2018
Xue Bin Peng:Pieter Abbeel:Sergey Levine:Michiel van de Panne	A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a physical simulation, thus enabling realistic responses to perturbations and environmental variation. We show that well-known reinforcement learning (RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex recoveries, adapting to changes in morphology, and accomplishing user-specified goals. Our method handles keyframed motions, highly-dynamic actions such as motion-captured flips and spins, and retargeted motions. By combining a motion-imitation objective with a task objective, we can train characters that react intelligently in interactive settings, e.g., by walking in a desired direction or throwing a ball at a user-specified target. This approach thus combines the convenience and motion quality of using motion clips to define the desired style and appearance, with the flexibility and generality afforded by RL methods and physics-based animation. We further explore a number of methods for integrating multiple clips into the learning process to develop multi-skilled agents capable of performing a rich repertoire of diverse skills. We demonstrate results using multiple characters (human, Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including locomotion, acrobatics, and martial arts.	DeepMimic: example-guided deep reinforcement learning of physics-based character skills	NA:NA:NA:NA	2018
Wenhao Yu:Greg Turk:C. Karen Liu	Learning locomotion skills is a challenging problem. To generate realistic and smooth locomotion, existing methods use motion capture, finite state machines or morphology-specific knowledge to guide the motion generation algorithms. Deep reinforcement learning (DRL) is a promising approach for the automatic creation of locomotion control. Indeed, a standard benchmark for DRL is to automatically create a running controller for a biped character from a simple reward function [Duan et al. 2016]. Although several different DRL algorithms can successfully create a running controller, the resulting motions usually look nothing like a real runner. This paper takes a minimalist learning approach to the locomotion problem, without the use of motion examples, finite state machines, or morphology-specific knowledge. We introduce two modifications to the DRL approach that, when used together, produce locomotion behaviors that are symmetric, low-energy, and much closer to that of a real person. First, we introduce a new term to the loss function (not the reward function) that encourages symmetric actions. Second, we introduce a new curriculum learning method that provides modulated physical assistance to help the character with left/right balance and forward movement. The algorithm automatically computes appropriate assistance to the character and gradually relaxes this assistance, so that eventually the character learns to move entirely without help. Because our method does not make use of motion capture data, it can be applied to a variety of character morphologies. We demonstrate locomotion controllers for the lower half of a biped, a full humanoid, a quadruped, and a hexapod. Our results show that learned policies are able to produce symmetric, low-energy gaits. In addition, speed-appropriate gait patterns emerge without any guidance from motion examples or contact planning.	Learning symmetric and low-energy locomotion	NA:NA:NA	2018
He Zhang:Sebastian Starke:Taku Komura:Jun Saito	Quadruped motion includes a wide variation of gaits such as walk, pace, trot and canter, and actions such as jumping, sitting, turning and idling. Applying existing data-driven character control frameworks to such data requires a significant amount of data preprocessing such as motion labeling and alignment. In this paper, we propose a novel neural network architecture called Mode-Adaptive Neural Networks for controlling quadruped characters. The system is composed of the motion prediction network and the gating network. At each frame, the motion prediction network computes the character state in the current frame given the state in the previous frame and the user-provided control signals. The gating network dynamically updates the weights of the motion prediction network by selecting and blending what we call the expert weights, each of which specializes in a particular movement. Due to the increased flexibility, the system can learn consistent expert weights across a wide range of non-periodic/periodic actions, from unstructured motion capture data, in an end-to-end fashion. In addition, the users are released from performing complex labeling of phases in different gaits. We show that this architecture is suitable for encoding the multi-modality of quadruped locomotion and synthesizing responsive motion in real-time.	Mode-adaptive neural networks for quadruped motion control	NA:NA:NA:NA	2018
Hsiao-Yu Chen:Arnav Sastry:Wim M. van Rees:Etienne Vouga	We present a physically accurate low-order elastic shell model that incorporates active material response to dynamically changing stimuli such as heat, moisture, and growth. Our continuous formulation of the geometrically non-linear elastic energy derives from the principles of differential geometry, and as such naturally incorporates shell thickness, non-zero rest curvature, and physical material properties. By modeling the environmental stimulus as local, dynamic changes in the rest metric of the material, we are able to solve for the corresponding shape changes by integrating the equations of motions given this non-Euclidean rest state. We present models for differential growth and shrinking due to moisture and temperature gradients along and across the surface, and incorporate anisotropic growth by defining an intrinsic machine direction within the material. Comparisons with experiments and volumetric finite elements show that our simulations achieve excellent qualitative and quantitative agreement. By combining the reduced-order shell theory with appropriate physical models, our approach accurately captures all the physical phenomena while avoiding expensive volumetric discretization of the shell volume.	Physical simulation of environmentally induced thin shell deformation	NA:NA:NA:NA	2018
Qi Guo:Xuchen Han:Chuyuan Fu:Theodore Gast:Rasmus Tamstorf:Joseph Teran	We present a novel method for simulation of thin shells with frictional contact using a combination of the Material Point Method (MPM) and subdivision finite elements. The shell kinematics are assumed to follow a continuum shell model which is decomposed into a Kirchhoff-Love motion that rotates the mid-surface normals followed by shearing and compression/extension of the material along the mid-surface normal. We use this decomposition to design an elastoplastic constitutive model to resolve frictional contact by decoupling resistance to contact and shearing from the bending resistance components of stress. We show that by resolving frictional contact with a continuum approach, our hybrid Lagrangian/Eulerian approach is capable of simulating challenging shell contact scenarios with hundreds of thousands to millions of degrees of freedom. Without the need for collision detection or resolution, our method runs in a few minutes per frame in these high resolution examples. Furthermore we show that our technique naturally couples with other traditional MPM methods for simulating granular and related materials.	A material point method for thin shells with frictional contact	NA:NA:NA:NA:NA:NA	2018
Christian Schumacher:Steve Marschner:Markus Gross:Bernhard Thomaszewski	We propose a comprehensive approach to characterizing the mechanical properties of structured sheet materials, i.e., planar rod networks whose mechanics and aesthetics are inextricably linked. We establish a connection between the complex mesoscopic deformation behavior of such structures and their macroscopic elastic properties through numerical homogenization. Our approach leverages 3D Kirchhoff rod simulation in order to capture nonlinear effects for both in-plane and bending deformations. We apply our method to different families of structures based on isohedral tilings---a simple yet extensive and aesthetically interesting group of space-filling patterns. We show that these tilings admit a wide range of material properties, and our homogenization approach allows us to create concise and intuitive descriptions of a material's direction-dependent macromechanical behavior that are easy to communicate even to non-experts. We perform this characterization for an extensive set of structures and organize these data in a material browser to enable efficient forward exploration of the aesthetic-mechanical space of structured sheet materials. We also propose an inverse design method to automatically find structure parameters that best approximate a user-specified target behavior.	Mechanical characterization of structured sheet materials	NA:NA:NA:NA	2018
Ming Gao:Andre Pradhana:Xuchen Han:Qi Guo:Grant Kot:Eftychios Sifakis:Chenfanfu Jiang	In this paper, we present a mixed explicit and semi-implicit Material Point Method for simulating particle-laden flows. We develop a Multigrid Preconditioned fluid solver for the Locally Averaged Navier Stokes equation. This is discretized purely on a semi-staggered standard MPM grid. Sedimentation is modeled with the Drucker-Prager elastoplasticity flow rule, enhanced by a novel particle density estimation method for converting particles between representations of either continuum or discrete points. Fluid and sediment are two-way coupled through a momentum exchange force that can be easily resolved with two MPM background grids. We present various results to demonstrate the efficacy of our method.	Animating fluid sediment mixture in particle-laden flows	NA:NA:NA:NA:NA:NA:NA	2018
Yuanming Hu:Yu Fang:Ziheng Ge:Ziyin Qu:Yixin Zhu:Andre Pradhana:Chenfanfu Jiang	In this paper, we introduce the Moving Least Squares Material Point Method (MLS-MPM). MLS-MPM naturally leads to the formulation of Affine Particle-In-Cell (APIC) [Jiang et al. 2015] and Polynomial Particle-In-Cell [Fu et al. 2017] in a way that is consistent with a Galerkin-style weak form discretization of the governing equations. Additionally, it enables a new stress divergence discretization that effortlessly allows all MPM simulations to run two times faster than before. We also develop a Compatible Particle-In-Cell (CPIC) algorithm on top of MLS-MPM. Utilizing a colored distance field representation and a novel compatibility condition for particles and grid nodes, our framework enables the simulation of various new phenomena that are not previously supported by MPM, including material cutting, dynamic open boundaries, and two-way coupling with rigid bodies. MLS-MPM with CPIC is easy to implement and friendly to performance optimization.	A moving least squares material point method with displacement discontinuity and two-way rigid body coupling	NA:NA:NA:NA:NA:NA:NA	2018
Ruizhen Hu:Zihao Yan:Jingwen Zhang:Oliver Van Kaick:Ariel Shamir:Hao Zhang:Hui Huang	Humans can predict the functionality of an object even without any surroundings, since their knowledge and experience would allow them to "hallucinate" the interaction or usage scenarios involving the object. We develop predictive and generative deep convolutional neural networks to replicate this feat. Specifically, our work focuses on functionalities of man-made 3D objects characterized by human-object or object-object interactions. Our networks are trained on a database of scene contexts, called interaction contexts, each consisting of a central object and one or more surrounding objects, that represent object functionalities. Given a 3D object in isolation, our functional similarity network (fSIM-NET), a variation of the triplet network, is trained to predict the functionality of the object by inferring functionality-revealing interaction contexts. fSIM-NET is complemented by a generative network (iGEN-NET) and a segmentation network (iSEG-NET). iGEN-NET takes a single voxelized 3D object with a functionality label and synthesizes a voxelized surround, i.e., the interaction context which visually demonstrates the corresponding functionality. iSEG-NET further separates the interacting objects into different groups according to their interaction types.	Predictive and generative neural networks for object functionality	NA:NA:NA:NA:NA:NA:NA	2018
Kangxue Yin:Hui Huang:Daniel Cohen-Or:Hao Zhang	We introduce P2P-NET, a general-purpose deep neural network which learns geometric transformations between point-based shape representations from two domains, e.g., meso-skeletons and surfaces, partial and complete scans, etc. The architecture of the P2P-NET is that of a bi-directional point displacement network, which transforms a source point set to a prediction of the target point set with the same cardinality, and vice versa, by applying point-wise displacement vectors learned from data. P2P-NET is trained on paired shapes from the source and target domains, but without relying on point-to-point correspondences between the source and target point sets. The training loss combines two uni-directional geometric losses, each enforcing a shape-wise similarity between the predicted and the target point sets, and a cross-regularization term to encourage consistency between displacement vectors going in opposite directions. We develop and present several different applications enabled by our general-purpose bidirectional P2P-NET to highlight the effectiveness, versatility, and potential of our network in solving a variety of point-based shape transformation problems.	P2P-NET: bidirectional point displacement net for shape transform	NA:NA:NA:NA	2018
Max Limper:Nicholas Vining:ALLA SHEFFER	Packed atlases, consisting of 2D parameterized charts, are ubiquitously used to store surface signals such as texture or normals. Tight packing is similarly used to arrange and cut-out 2D panels for fabrication from sheet materials. Packing efficiency, or the ratio between the areas of the packed atlas and its bounding box, significantly impacts downstream applications. We propose Box Cutter, a new method for optimizing packing efficiency suitable for both settings. Our algorithm improves packing efficiency without changing distortion by strategically cutting and repacking the atlas charts or panels. It preserves the local mapping between the 3D surface and the atlas charts and retains global mapping continuity across the newly formed cuts. We balance packing efficiency improvement against increase in chart boundary length and enable users to directly control the acceptable amount of boundary elongation. While the problem we address is NP-hard, we provide an effective practical solution by iteratively detecting large rectangular empty spaces, or void boxes, in the current atlas packing and eliminating them by first refining the atlas using strategically placed axis-aligned cuts and then repacking the refined charts. We repeat this process until no further improvement is possible, or until the desired balance between packing improvement and boundary elongation is achieved. Packed chart atlases are only useful for the applications we address if their charts are overlap-free; yet many popular parameterization methods, used as-is, produce atlases with global overlaps. Our pre-processing step eliminates all input overlaps while explicitly minimizing the boundary length of the resulting overlap-free charts. We demonstrate our combined strategy on a large range of input atlases produced by diverse parameterization methods, as well as on multiple sets of 2D fabrication panels. Our framework dramatically improves the output packing efficiency on all inputs; for instance with boundary length increase capped at 50% we improve packing efficiency by 68% on average.	Box cutter: atlas refinement for efficient packing via void elimination	NA:NA:NA	2018
Fabián Prada:Misha Kazhdan:Ming Chuang:Hugues Hoppe	Processing signals on surfaces often involves resampling the signal over the vertices of a dense mesh and applying mesh-based filtering operators. We present a framework to process a signal directly in a texture atlas domain. The benefits are twofold: avoiding resampling degradation and exploiting the regularity of the texture image grid. The main challenges are to preserve continuity across atlas chart boundaries and to adapt differential operators to the non-uniform parameterization. We introduce a novel function space and multigrid solver that jointly enable robust, interactive, and geometry-aware signal processing. We demonstrate our approach using several applications including smoothing and sharpening, multiview stitching, geodesic distance computation, and line integral convolution.	Gradient-domain processing within a texture atlas	NA:NA:NA:NA	2018
Nico Schertler:Daniele Panozzo:Stefan Gumhold:Marco Tarini	We introduce a practical pipeline to create UV T-layouts for real-world quad dominant semi-regular meshes. Our algorithm creates large rectangular patches by relaxing the notion of motorcycle graphs and making it insensitive to local irregularities in the mesh structure such as non-quad elements, redundant irregular vertices, T-junctions, and others. Each surface patch, which can contain multiple singularities and/or polygonal elements, is mapped to an axis-aligned rectangle, leading to a simple and efficient UV layout, which is ideal for texture mapping (allowing for mipmapping and artifact-free bilinear interpolation). We demonstrate that our algorithm is an ideal solution for both recent semi-regular, quad-dominant meshing methods, and for the low-poly meshes typically used in games and movies.	Generalized motorcycle graphs for imperfect quad-dominant meshes	NA:NA:NA:NA	2018
Nicholas Sharp:Keenan Crane	This paper develops a global variational approach to cutting curved surfaces so that they can be flattened into the plane with low metric distortion. Such cuts are a critical component in a variety of algorithms that seek to parameterize surfaces over flat domains, or fabricate structures from flat materials. Rather than evaluate the quality of a cut solely based on properties of the curve itself (e.g., its length or curvature), we formulate a flow that directly optimizes the distortion induced by cutting and flattening. Notably, we do not have to explicitly parameterize the surface in order to evaluate the cost of a cut, but can instead integrate a simple evolution equation defined on the cut curve itself. We arrive at this flow via a novel application of shape derivatives to the Yamabe equation from conformal geometry. We then develop an Eulerian numerical integrator on triangulated surfaces, which does not restrict cuts to mesh edges and can incorporate user-defined data such as importance or occlusion. The resulting cut curves can be used to drive distortion to arbitrarily low levels, and have a very different character from cuts obtained via purely discrete formulations. We briefly explore potential applications to computational design, as well as connections to space filling curves and the problem of uniform heat distribution.	Variational surface cutting	NA:NA	2018
Alan Brunton:Can Ates Arikan:Tejas Madan Tanksale:Philipp Urban	We present an efficient and scalable pipeline for fabricating full-colored objects with spatially-varying translucency from practical and accessible input data via multi-material 3D printing. Observing that the costs associated with BSSRDF measurement and processing are high, the range of 3D printable BSSRDFs are severely limited, and that the human visual system relies only on simple high-level cues to perceive translucency, we propose a method based on reproducing perceptual translucency cues. The input to our pipeline is an RGBA signal defined on the surface of an object, making our approach accessible and practical for designers. We propose a framework for extending standard color management and profiling to combined color and translucency management using a gamut correspondence strategy we call opaque relative processing. We present an efficient streaming method to compute voxel-level material arrangements, achieving both realistic reproduction of measured translucent materials and artistic effects involving multiple fully or partially transparent geometries.	3D printing spatially varying color and translucency	NA:NA:NA:NA	2018
Kaisei Sakurai:Yoshinori Dobashi:Kei Iwasaki:Tomoyuki Nishita	A great deal of attention has been devoted to the fabrication of reflectors that can display different color images when viewed from different directions not only in industry but also for the arts. Although such reflectors have previously been successfully fabricated, the number of images displayed has been limited to two or they suffer from ghosting artifacts where mixed images appear. Furthermore, the previous methods need special hardware and/or materials to fabricate the reflectors. Thus, those techniques are not suitable for printing reflectors on everyday personal objects made of different materials, such as name cards, letter sheets, envelopes, and plastic cases. To overcome these limitations, we propose a method for fabricating reflectors using a standard ultraviolet printer (UV printer). UV printer can render a specified 2D color pattern on an arbitrary material and by overprinting the printed pattern can be raised, that is, the printed pattern becomes a microstructure having color and height. We propose using these micro structures to formulate a method for designing spatially varying reflections that can display different target images when viewed from different directions. The microstructure is calculated by minimizing an objective function that measures the differences between the intensities of the light reflected from the reflector and that of the target image. We show several fabricated reflectors to demonstrate the usefulness of the proposed method.	Fabricating reflectors for displaying multiple images	NA:NA:NA:NA	2018
Thomas Auzinger:Wolfgang Heidrich:Bernd Bickel	Additive manufacturing has recently seen drastic improvements in resolution, making it now possible to fabricate features at scales of hundreds or even dozens of nanometers, which previously required very expensive lithographic methods. As a result, additive manufacturing now seems poised for optical applications, including those relevant to computer graphics, such as material design, as well as display and imaging applications. In this work, we explore the use of additive manufacturing for generating structural colors, where the structures are designed using a fabrication-aware optimization process. This requires a combination of full-wave simulation, a feasible parameterization of the design space, and a tailored optimization procedure. Many of these components should be re-usable for the design of other optical structures at this scale. We show initial results of material samples fabricated based on our designs. While these suffer from the prototype character of state-of-the-art fabrication hardware, we believe they clearly demonstrate the potential of additive nanofabrication for structural colors and other graphics applications.	Computational design of nanostructural color for additive manufacturing	NA:NA:NA	2018
Moritz Geilinger:Roi Poranne:Ruta Desai:Bernhard Thomaszewski:Stelian Coros	We present a computation-driven approach to design optimization and motion synthesis for robotic creatures that locomote using arbitrary arrangements of legs and wheels. Through an intuitive interface, designers first create unique robots by combining different types of servomotors, 3D printable connectors, wheels and feet in a mix-and-match manner. With the resulting robot as input, a novel trajectory optimization formulation generates walking, rolling, gliding and skating motions. These motions emerge naturally based on the components used to design each individual robot. We exploit the particular structure of our formulation and make targeted simplifications to significantly accelerate the underlying numerical solver without compromising quality. This allows designers to interactively choreograph stable, physically-valid motions that are agile and compelling. We furthermore develop a suite of user-guided, semi-automatic, and fully-automatic optimization tools that enable motion-aware edits of the robot's physical structure. We demonstrate the efficacy of our design methodology by creating a diverse array of hybrid legged/wheeled mobile robots which we validate using physics simulation and through fabricated prototypes.	Skaterbots: optimization-based design and motion synthesis for robotic creatures with legs and wheels	NA:NA:NA:NA:NA	2018
Yang Zhou:Zhan Xu:Chris Landreth:Evangelos Kalogerakis:Subhransu Maji:Karan Singh	We present a novel deep-learning based approach to producing animator-centric speech motion curves that drive a JALI or standard FACS-based production face-rig, directly from input audio. Our three-stage Long Short-Term Memory (LSTM) network architecture is motivated by psycho-linguistic insights: segmenting speech audio into a stream of phonetic-groups is sufficient for viseme construction; speech styles like mumbling or shouting are strongly co-related to the motion of facial landmarks; and animator style is encoded in viseme motion curve profiles. Our contribution is an automatic real-time lip-synchronization from audio solution that integrates seamlessly into existing animation pipelines. We evaluate our results by: cross-validation to ground-truth data; animator critique and edits; visual comparison to recent deep-learning lip-synchronization solutions; and showing our approach to be resilient to diversity in speaker and language.	Visemenet: audio-driven animator-centric speech animation	NA:NA:NA:NA:NA:NA	2018
Shugo Yamaguchi:Shunsuke Saito:Koki Nagano:Yajie Zhao:Weikai Chen:Kyle Olszewski:Shigeo Morishima:Hao Li	We present a deep learning-based technique to infer high-quality facial reflectance and geometry given a single unconstrained image of the subject, which may contain partial occlusions and arbitrary illumination conditions. The reconstructed high-resolution textures, which are generated in only a few seconds, include high-resolution skin surface reflectance maps, representing both the diffuse and specular albedo, and medium- and high-frequency displacement maps, thereby allowing us to render compelling digital avatars under novel lighting conditions. To extract this data, we train our deep neural networks with a high-quality skin reflectance and geometry database created with a state-of-the-art multi-view photometric stereo system using polarized gradient illumination. Given the raw facial texture map extracted from the input image, our neural networks synthesize complete reflectance and displacement maps, as well as complete missing regions caused by occlusions. The completed textures exhibit consistent quality throughout the face due to our network architecture, which propagates texture features from the visible region, resulting in high-fidelity details that are consistent with those seen in visible regions. We describe how this highly underconstrained problem is made tractable by dividing the full inference into smaller tasks, which are addressed by dedicated neural networks. We demonstrate the effectiveness of our network design with robust texture completion from images of faces that are largely occluded. With the inferred reflectance and geometry data, we demonstrate the rendering of high-fidelity 3D avatars from a variety of subjects captured under different lighting conditions. In addition, we perform evaluations demonstrating that our method can infer plausible facial reflectance and geometric details comparable to those obtained from high-end capture devices, and outperform alternative approaches that require only a single unconstrained input image.	High-fidelity facial reflectance and geometry inference from an unconstrained image	NA:NA:NA:NA:NA:NA:NA:NA	2018
Hyeongwoo Kim:Pablo Garrido:Ayush Tewari:Weipeng Xu:Justus Thies:Matthias Niessner:Patrick Pérez:Christian Richardt:Michael Zollhöfer:Christian Theobalt	We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted to manipulations of facial expressions only, we are the first to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye blinking from a source actor to a portrait video of a target actor. The core of our approach is a generative neural network with a novel space-time architecture. The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor. The realism in this rendering-to-video transfer is achieved by careful adversarial training, and as a result, we can create modified target videos that mimic the behavior of the synthetically-created input. In order to enable source-to-target video re-animation, we render a synthetic target video with the reconstructed head animation parameters from a source video, and feed it into the trained network - thus taking full control of the target. With the ability to freely recombine source and target parameters, we are able to demonstrate a large variety of video rewrite applications without explicitly modeling hair, body or background. For instance, we can reenact the full head using interactive user-controlled editing, and realize high-fidelity visual dubbing. To demonstrate the high quality of our output, we conduct an extensive series of experiments and evaluations, where for instance a user study shows that our video edits are hard to detect.	Deep video portraits	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Justus Thies:Michael Zollhöfer:Christian Theobalt:Marc Stamminger:Matthias Niessner	We propose HeadOn, the first real-time source-to-target reenactment approach for complete human portrait videos that enables transfer of torso and head motion, face expression, and eye gaze. Given a short RGB-D video of the target actor, we automatically construct a personalized geometry proxy that embeds a parametric head, eye, and kinematic torso model. A novel realtime reenactment algorithm employs this proxy to photo-realistically map the captured motion from the source actor to the target actor. On top of the coarse geometric proxy, we propose a video-based rendering technique that composites the modified target portrait video via view- and pose-dependent texturing, and creates photo-realistic imagery of the target actor under novel torso and head poses, facial expressions, and gaze directions. To this end, we propose a robust tracking of the face and torso of the source actor. We extensively evaluate our approach and show significant improvements in enabling much greater flexibility in creating realistic reenacted output videos.	Headon: real-time reenactment of human portrait videos	NA:NA:NA:NA:NA	2018
Daniel Holden	Raw optical motion capture data often includes errors such as occluded markers, mislabeled markers, and high frequency noise or jitter. Typically these errors must be fixed by hand - an extremely time-consuming and tedious task. Due to this, there is a large demand for tools or techniques which can alleviate this burden. In this research we present a tool that sidesteps this problem, and produces joint transforms directly from raw marker data (a task commonly called "solving") in a way that is extremely robust to errors in the input data using the machine learning technique of denoising. Starting with a set of marker configurations, and a large database of skeletal motion data such as the CMU motion capture database [CMU 2013b], we synthetically reconstruct marker locations using linear blend skinning and apply a unique noise function for corrupting this marker data - randomly removing and shifting markers to dynamically produce billions of examples of poses with errors similar to those found in real motion capture data. We then train a deep denoising feed-forward neural network to learn a mapping from this corrupted marker data to the corresponding transforms of the joints. Once trained, our neural network can be used as a replacement for the solving part of the motion capture pipeline, and, as it is very robust to errors, it completely removes the need for any manual clean-up of data. Our system is accurate enough to be used in production, generally achieving precision to within a few millimeters, while additionally being extremely fast to compute with low memory requirements.	Robust solving of optical motion capture data by denoising	NA	2018
Shangchen Han:Beibei Liu:Robert Wang:Yuting Ye:Christopher D. Twigg:Kenrick Kin	Optical marker-based motion capture is the dominant way for obtaining high-fidelity human body animation for special effects, movies, and video games. However, motion capture has seen limited application to the human hand due to the difficulty of automatically identifying (or labeling) identical markers on self-similar fingers. We propose a technique that frames the labeling problem as a keypoint regression problem conducive to a solution using convolutional neural networks. We demonstrate robustness of our labeling solution to occlusion, ghost markers, hand shape, and even motions involving two hands or handheld objects. Our technique is equally applicable to sparse or dense marker sets and can run in real-time to support interaction prototyping with high-fidelity hand tracking and hand presence in virtual reality.	Online optical marker-based hand tracking with deep labels	NA:NA:NA:NA:NA:NA	2018
Kazuki Miyazaki:Issei Fujishiro	Photoelasticity is known as one of the phenomena related to polarization and is defined as the change in birefringence of transparent material when internal force is applied. Interference fringes appear by irradiating the material with polarized light when viewing it through the polarizer. In this study, we attempt to apply the concept of photoelasticity to generative art. Assuming there is virtual stress distribution in the two-dimensional material, our method automatically generates artworks with photoelasticity. A GPU-based acceleration of the current implementation is also discussed.	Automatic generation of artworks using virtual photoelastic material	NA:NA	2018
Néill O'Dwyer:Nicholas Johnson:Rafael Pagés:Jan Ondřej:Konstantinos Amplianitis:Enda Bates:David Monaghan:Aljoša Smolić	This poster describes a reinterpretation of Samuel Beckett's theatrical text Play for virtual reality (VR). It is an aesthetic reflection on practice that follows up an a technical project description submitted to ISMAR 2017 [O'Dwyer et al. 2017]. Actors are captured in a green screen environment using free-viewpoint video (FVV) techniques, and the scene is built in a game engine, complete with binaural spatial audio and six degrees of freedom of movement. The project explores how ludic qualities in the original text help elicit the conversational and interactive specificities of the digital medium. The work affirms the potential for interactive narrative in VR, opens new experiences of the text, and highlights the reorganisation of the author-audience dynamic.	Beckett in VR: exploring narrative using free viewpoint video	NA:NA:NA:NA:NA:NA:NA:NA	2018
Seungbae Bang:Sung-Hee Lee	Among many approaches for object and character deformation, closed-form skinning methods, such as Linear Blend Skinning (LBS) and Dual Quaternion Skinning (DQS), are widely used as they are fast and intuitive. The quality of these skinning methods highly depends on specifying appropriate skinning weights to vertices, which requires the intensive efforts of professional artists in production animation.	Computation of skinning weight using spline interface	NA:NA	2018
Zihao Song:Serguei A. Mokhov:Miao Song:Sudhir P. Mudur	Illimitable Space System (ISS) is a real-time interactive configurable toolbox for use by artists to create interactive visual effects in theatre performances and in documentaries through user inputs such as gestures and voice. Kinect has been the primary input device for motion and video data capture. In this work in addition to the existing motion based visual and geometric data processing facilities present in ISSv2, we describe our efforts to incorporate audio processing with the help of Modular Audio Recognition Framework (MARF). The combination of computer vision and audio processing to interpret both music and human motion to create imagery in real time is both artistically interesting and technically challenging. With these additional modules, ISSv2 can help interactive performance authoring that employs visual tracking and signal processing in order to create trackable human-shaped animations in real time. These new modules are incorporated into the Processing software sketchbook and language framework used by ISSv2. We verify the effects of these modules, through live demonstrations which are briefly described.	Creative use of signal processing and MARF in ISSv2 and beyond	NA:NA:NA:NA	2018
Martina R. Fröschl:Alfred Vendl	CRISPR/Cas9-NHEJ: Action in the Nucleus (2017) is derived from an interdisciplinary creative process. This paper discusses the creation of this 210° scientific visualization, the usage of data from the worldwide Protein Data Bank, and the audio-visual presentation in an interactive dome setup. Since the topic is significant for the future of humanity, immersive experiences should be considered to convey tacit knowledge of gene-editing processes to make them approachable for the general public.	CRISPR/Cas9-NHEJ: action in the nucleus	NA:NA	2018
Kohei Ogawa:Kengo Tanaka:Tatsuya Minagawa:Yoichi Ochiai	In this study, We propose a method to develop a spring glass dip pen by using a 3D printer and reproduce different types of writing feeling. There have been several studies on different types of pens to change the feel of writing. For example, EV-Pen [Wang et al. 2016] and haptics pens [Lee et al. 2004] changes the feel of pen writing with using vibration. However, our proposed method does not reproduce tactile sensation of softness by using vibrations.	Design method of digitally fabricated spring glass pen	NA:NA:NA:NA	2018
Christy Spangler:Eric Stolzenberg	The National Transportation Safety Board (NTSB) is an independent agency charged with determining the probable cause of transportation accidents and promoting transportation safety. We collect a large volume of highly complex and diverse data, which is often integrated into digital illustrations to help explain the accident events, probable cause, and relevant safety issues. One major accident investigation in which digital illustrations were key was our investigation into the sinking of the cargo ship SS El Faro in 2015. (Fig. 1)	El faro: developing a digital illustration of hull wreckage 15,400 feet below the surface of the Atlantic ocean	NA:NA	2018
Richard Cottrell	Using commercially available parts, Ephemeral Sandscaper produces complex layered landscapes by semi-randomly selecting predefined elements and sculpting them onto a material field with compelling implications for Soft Architecture.	Ephemeral sandscapes: using robotics to generate temporal landscapes	NA	2018
Tim McGraw	Fractal shapes reflect the behavior of complex natural systems, but can be generated by simple mathematical equations. Images of 3D fractals almost exclusively depict opaque surfaces, and use reflected light and shadows to simulate a physical realization of these virtual objects. But rich inner detail can be revealed by reinterpreting the fractal as a volume and considering material transparency, light absorption and refraction. This work explores the range of images made possible by employing volume rendering techniques inspired by medical image visualization.	Fractal anatomy: imaging internal and ambient structures	NA	2018
Maria Lantin:Simon Lysander Overstall:Hongzhu Zhao	We present a multi-user networked VR application, I Am Afraid, which uses voice as an interface to create sonic objects in a virtual environment. Words are spoken and added to the environment as three-dimensional textual objects. Other vocalizations are rendered as abstract shapes. The sculptural elements embed the sound of the voice that initiated their creation, and can be played as instruments via user-controlled interactions such as scrubbing, shaking, or looping. Multiple users can simultaneously be in the environment, mixing their voices in an evolving, dynamic, sound sculpture. I Am Afraid has been used for fun, performance, and therapeutic purposes.	I am afraid: voice as sonic sculpture	NA:NA:NA	2018
Shinji Mizuno:Yuka Oba:Nao Kotani:Yoichi Shinchi:Kenji Funahashi:Shinya Oguri:Koji Oguri:Takami Yasuda	We introduce interactive projection mappings in a traditional Japanese house. In Japanese traditional houses, sliding doors / windows called shoji are often used. The shoji is a panel stuck with paper on the frame of the tree, and it can be used as a projector screen. We created two types of interactive projection mappings on shoji (Figure 1(a)(b)). Other characteristics of Japanese traditional houses is tatami: straw mats flooring. We also created an interactive projection mapping on tatami flooring (Figure 1(c)).	Interactive projection mappings in a Japanese traditional house	NA:NA:NA:NA:NA:NA:NA:NA	2018
Jaedong Lee:Jehee Lee	The main goal of the crowd simulation is to generate realistic movements of agents. Reproducing the mechanism that seeing the environments, understanding current situation, and deciding where to step is crucial point to simulating crowd movements. We formulate the process of walking mechanism using deep reinforcement learning. And we experiment some typical scenarios.	Learning to move in crowd	NA:NA	2018
Steve Caruso	Most photograph-to-oil-painting algorithms are based off of the techniques described by [Litwinowicz 1997] and improved upon by [Hertzmann 2001]. They are essentially fully-automated processes which place strokes at random, choosing stroke orientations to follow local gradient normals. These strokes are built up over several layers, each layer in a decreasing order of stroke size - painting broad strokes and then filling in details. Outside of the initial chosen parameters and some masking considerations, the user has little agency in how the algorithm chooses stroke placement, nor has the ability to make direct changes or touch-ups until every stroke is laid down on the canvas - essentially it behaves like an "image filter" applied as one would adjust contrast or add texture.	Painting with DEGAS: (digitally extrapolated graphics via algorithmic strokes)	NA	2018
Ya-Bo Huang:Mei-Yun Chen:Ming Ouhyoung	In the poster, we propose a model to predict the mixture of water-color pigments using convolutional neural networks (CNN). With a watercolor dataset, we train our model to minimize the loss function of sRGB differences. In metric of color difference ΔELab, our model achieves 88.7 % of data that ΔELab < 5 on the test set, which means the difference can not easily be detected by human eye. In addition, an interesting phenomenon is found; Even if the reflectance curve of the predicted color is not as smooth as the ground truth curve, the RGB color is still close to the ground truth.	Perceptual-based CNN model for watercolor mixing prediction	NA:NA:NA	2018
Yi-Lung Kao:Yu-Sheng Chen:Ming Ouhyoung	A camera is a good instrument for measuring scene radiance. However, to please the human eye, the resulting image brightness is not linear to the scene radiance, so solving the mapping function between scene radiance and image brightness is very important. We propose a Progressive-CRF-net for radiometric calibration. By stacking multiple networks and using the pre-trained weights, this approach can reduce the training time and reach better performance than that of previous work. Our experiments show a significant improvement based on PSNR and SSIM.	Progressive-CRF-net: single image radiometric calibration using stacked CNNs	NA:NA:NA	2018
Jane Prophet:Yong Ming Kow:Mark Hurry	Our prototype app, Pocket Penjing, built using Unity3D, takes its name from the Chinese "Penjing." These tray plantings of miniature trees pre-date bonsai, often including miniature benches or figures to allude to people's relationship to the tree. App users choose a species, then create and name their tree. Swiping rotates a 3D globe showing flagged locations. Each flag represents a live online air quality monitoring station data stream that the app can scrape. Data is pulled in from the selected station and the AR window loads. The AR tree grows in real-time 3D. Its L-Systems form is determined by the selected live air quality data. We used this prototype as the basis of a two-part formative participatory design workshop with 63 participants.	Small trees, big data: augmented reality model of air quality data via the chinese art of "artificial" tray planting	NA:NA:NA	2018
Yuka Takahashi:Tsukasa Fukusato	This paper presents a system that aims to assist with the design of hand-sewn embroidery. With our system, a user can edit his/her design until he/she is satisfied with the simulated embroidery. We demonstrate the effectiveness of our approach, showing that visually pleasing results can be generated with minimal effort.	Stitch: an interactive design system for hand-sewn embroidery	NA:NA	2018
Predrag K. Nikolić:Hua Yang:Jyunjye Chen:George Peter Stankevich	Project Syntropic Counterpoints has been conceptualized in the form of a series of discussions between artificial intelligence (historical persons) clones, related to topics we want to expose to AI interpretation. The project is an artist response to rising technology singularity and emerging Artificial Intelligence implementation in every aspect of everyday life which changes the social interaction landscape forever. With this project we intend to point to questions such as: Are we using AI to make humans smarter or to create a new living entity equal to us? How will this reflect on human society and its present planetary supremacy? Can we share the world and accept equality with a new AI living entity? What could be the consequences of that decision? We are also trying to point to AI limitations and to examine the cultural, creative, historical and social benefits we can gain by using AI.	Syntropic counterpoints: art of AI sense or machine made context art	NA:NA:NA:NA	2018
Águeda Simó	Stereoscopic techniques help to perceive spatial depth; hence they are used to create realistic representations of the three-dimensional world. They can also be used to manipulate spatial dimensions and create alternative spaces that challenge our understanding of visual reality. The video installation Eccentric Spaces is part of an art project that combines stereoscopic live action video with a Holobench-type display to depict alternative spaces that appear physically real.	The stereoscopic art installation eccentric spaces	NA	2018
Kyoung Lee Swearingen:Scott Swearingen	Wall Mounted Level is a cooperative mixed-reality game that leverages multimodal interactions to support its narrative of 'reconciliation'. In it, players control their digitally projected characters and navigate them across a hand drawn physical sculpture as they collaborate towards a shared goal: finding one another. The digital and physical characteristics of the game are further reflected in the ways in which players interact with it, by making use of digital input devices and physical 'touch'. The abstract and poster discuss the design choices that were made for creating the varying modes of engagement and the motivation behind player collaboration in 'Wall Mounted Level.	Wall mounted level: a cooperative mixed reality game about reconciliation	NA:NA	2018
Gyorgy Denes:Kuba Maruszczyk:Rafał K. Mantiuk	Increasingly higher virtual reality (VR) display resolutions and good-quality anti-aliasing make rendering in VR prohibitively expensive. The generation of these complex frames 90 times per second in a binocular setup demands substantial computational power. Wireless transmission of the frames from the GPU to the VR headset poses another challenge, requiring high-bandwidth dedicated links.	Exploiting the limitations of spatio-temporal vision for more efficient VR rendering	NA:NA:NA	2018
Alberto Badias:Iciar Alfaro:David González:Francisco Chinesta:Elías Cueto	We present a new way of adding augmented information based on the computation of the physical equations that truly govern the behavior of objects. In computer graphics, it is common to use big simplifications to be able to solve this type of equations in real time, obtaining in many occasions behaviors that differ remarkably from reality. However, using model order reduction (MOR) techniques we are able to pre-compute a parametric solution that is only evaluated in the visualization stage, greatly reducing the computation time in this on-line phase. We present also several examples that support our method, showing computational fluid dynamics (CFD) examples and deformable solids with nonlinear material behaviors. Since it is a mixed-reality implementation, we decided to create an interactive poster that allows the visualization of augmented reality videos using augmented reality techniques, what we call (AR)2.	Improving the realism of mixed reality through physical simulation	NA:NA:NA:NA:NA	2018
Hui- Ju Chen:Zi-Xin You:Yun-Ho Yu:Jen-Ming Chen:Chia-Chun Chang:Chien-Hsing Chou	Learning essentials of anatomy and physiology[R. Richardson et al. 2018] can make students knowing more about the connection between bones and muscles of human bodies. In the past, we can only use books, pictures, videos or fixed bone model to teach. This kind of teaching may suit for student over 15. But, for student under 15, it's hard to increase their interest or studying time for learning. If there are some models that can be assembled during the class, as Alison James[A. James et al. 2014] said, building LEGO helps us to think more about the 3D shape of the object. Can also increase student's interest of learning.	Interactive teaching aids design for essentials of anatomy and physiology: using bones and muscles as example	NA:NA:NA:NA:NA:NA	2018
Takuro Nakao:Yun Suen Pai:Megumi Isogai:Hideaki Kimata:Kai Kunze	Current devices aim to be more hands-free by providing users with the means to interact with them using other forms of input, such as voice which can be intrusive. We propose Make-a-Face; a wearable device that allows the user to use tongue, mouth, or cheek gestures via a mask-shaped device that senses muscle movement on the lower half of the face. The significance of this approach is threefold: 1) It allows a more non-intrusive approach to interaction, 2) we designed both the hardware and software from the ground-up to accommodate the sensor electrodes and 3) we proposed several use-case scenarios ranging from smartphones to interactions with virtual reality (VR) content.	Make-a-face: a hands-free, non-intrusive device for tongue/mouth/cheek input using EMG	NA:NA:NA:NA:NA	2018
Yiming Lin:Pieter Peers:Abhijeet Ghosh	We present a novel example-based material appearance modeling method for digital content creation. The proposed method requires a single HDR photograph of an exemplar object made of a desired material under known environmental illumination. While conventional methods for appearance modeling require the object shape to be known, our method does not require prior knowledge of the shape of the exemplar, nor does it require recovering the shape, which improves robustness as well as simplify on-site appearance acquisition by non-expert users.	On-site example-based material appearance digitization	NA:NA:NA	2018
Ping-Hsuan Han:Jia-Wei Lin:Chen-Hsin Hsieh:Jhih-Hong Hsu:Yi-Ping Hung	In the aging society, people are paying more attention to having good exercise habits. The advancement of technology grants the possibility of learning various kinds of exercises using multi-media equipment, for example, watching instruction videos. However, it is difficult for users to learn accurate movements due to lack of feedback information.	tARget: limbs movement guidance for learning physical activities with a video see-through head-mounted display	NA:NA:NA:NA:NA	2018
Katharina Krösl:Anna Felnhofer:Johanna X. Kafka:Laura Schuster:Alexandra Rinnerthaler:Michael Wimmer:Oswald D. Kothgassner	This work presents a virtual reality simulation for training different attentional abilities in children and adolescents. In an interdisciplinary project between psychology and computer science, we developed four mini-games that are used during therapy sessions to battle different aspects of attentional disorders. First experiments show that the immersive game-like application is well received by children. Our tool is also currently part of a treatment program in an ongoing clinical study.	The virtual schoolyard: attention training in virtual reality for children with attentional disorders	NA:NA:NA:NA:NA:NA:NA	2018
Wan-Lun Tsai:Min-Chun Hu	Tactic training plays a crucial role in basketball offensive plays. With the aid of virtual reality, we propose a framework to improve the effectiveness and experience of tactic learning. The framework consists of a tactic input device and a wireless VR interaction system, which allows the user to conveniently input target tactic and practice in a high-fidelity circumstance. By the assistance of our VR training system, the user can vividly experience how the tactics are executed by viewing from the a specific player's viewing direction. Additionally, tactic movement guidance, action hint of how to offense aggressively, and virtual defenders are rendered in our system to make the training more realistic. By using the proposed framework, players can strengthen their tactical nous and improve the efficiency of tactic training.	Training assistant: strengthen your tactical nous with proficient virtual basketball players	NA:NA	2018
Jotaro Shigeyama:Takeru Hashimoto:Shigeo Yoshida:Taiju Aoki:Takuji Narumi:Tomohiro Tanikawa:Michitaka Hirose	We introduce a dynamic weight moving VR controller for 2d haptic shape rendering using a haptic shape illusion. This allows users to perceive the feeling of various shapes in virtual space with a single controller. In this paper, we describe the mechanical design of prototype device that drives weight on a 2d planar area to alter mass properties of the hand-held controller. Based on the experiment, our system succeeded in providing shape perception over a wide range. We discuss limitation and further capability of our device.	Transcalibur: dynamic 2D haptic shape illusion of virtual object by weight moving VR controller	NA:NA:NA:NA:NA:NA:NA	2018
Matthew Justice:Ergun Akleman	In this work, we present a process that use Barycentric shading method to create dynamic landscape paintings that change based on time of the day. Our process can allow creating dynamic paintings for any time of the day using simply a limited number of control paintings. To create a proof of concept, we have used landscape paintings of Edgar Payne, one of the leading landscape painters of the American West. His specific style of painting that blends Impressionism with the style of other painters of the American West is particularly appropriate for the demonstration of the power of our Barycentric shading method.	A process to create dynamic landscape paintings using barycentric shading with control paintings	NA:NA	2018
Naoki Hashiomoto:Kyosuke Hamamoto	In the present study, we propose a new wide-viewing-angle aerial imaging display that can display aerial three-dimensional images to the surroundings. Aerial imaging has a strong visual impact, and significant efforts have been made to realize aerial imaging. In recent years, attention has been drawn to optical elements that realize retroreflective transmission, which makes it possible to easily project such images into the air. However, the viewing angle of the aerial image is narrow, and it is difficult for multiple people to simultaneously observe aerial images or to observe aerial images from all angles. Therefore, in the present study, by symmetrically arranging the mirrors at the end of the retroreflective optical transfer system, the maximum viewing angle of the aerial image is enlarged and observation from the entire circumference becomes possible.	Aerial 3D display using a symmetrical mirror structure	NA:NA	2018
Yoshiki Terashima:Kengo Fujii:Hirotsugu Yamamoto:Masaki Yasugi:Shiro Suyama:Yukihiro Takeda	This paper proposes a novel optical system to show an aerial 3D image for a user in front of the display and to show its 2D image for the surrounding viewers. Our optics forms two-layered aerial images that are visible in a limited viewing area. Outside the viewing area, only the rear aerial 2D image is visible. The viewing area is controlled by the area of a retro-reflector in AIRR (Aerial Imaging by Retro-Reflection). The center perceives depth in the aerial screen based on DFD (Depth-Fused 3D) display.	Aerial 3D/2D composite display: depth-fused 3D for the central user and 2D for surrounding audiences	NA:NA:NA:NA:NA:NA	2018
Martin Ritz:Pedro Santos:Dieter Fellner	We created a fully automatic system for acquisition of spatially varying optical material behavior of real object surfaces under a hemisphere of individual incident light directions. The resulting measured material model is flexibly applicable to arbitrary 3D model geometries, can be photorealistically rendered and interacted with in real-time and is not constrained to isotropic materials.	Automated acquisition and real-time rendering of spatially varying optical material behavior	NA:NA:NA	2018
Yusuke Tokuyoshi:Tomohiro Mizokuchi	This paper presents a pipeline to accelerate frustum traced irregular z-buffers (IZBs). The IZB proposed by Wyman et al. is used to render accurate hard shadows for real-time applications such as video games, while it is expensive compared to shadow mapping. To improve the performance of hard shadows, we use a two-pass visibility test by integrating a conservative shadow map into the pipeline of the IZB. This paper also presents a more precise implementation of the conservative shadow map than the previous implementation. In our experiments for 4K screen resolution, the performance of the hard shadow computation is improved by more than double on average using the two-pass visibility test, though there is still room for optimization.	Conservative Z-prepass for frustum-traced irregular Z-buffers	NA:NA	2018
Vineet Batra:Ankit Phogat:Mridul Kavidayal	We propose a novel and intuitive method for coloring vector graphics which is easy to use and creates richly colored artwork with very little effort. Further, it preserves the underlying geometry of the vector graphic primitives, thereby, making it easy to perform subsequent edits. Our method builds upon the concepts of shape-coverage, color and opacity and thus is applicable to all vector graphics constructs including non-convex paths and text. Furthermore, our method is highly performant and provides real-time results irrespective of the number of coloring primitives used.	General primitives for smooth coloring of vector graphics	NA:NA:NA	2018
Fei Wang:Shujin Lin:Ruomei Wang:Yi Li:Baoquan Zhao:Xiaonan Luo	Our method shortens the time of fluid simulation by coupling the two conditions of density-invariant and divergence-free, and achieves the same simulation effect compared with other methods. Further, we regard the displacement of particles as the only basic variable of the continuity equation, which improves the stability of the fluid to a certain extent.	Improving incompressible SPH simulation efficiency by integrating density-invariant and divergence-free conditions	NA:NA:NA:NA:NA:NA	2018
Jiangyan Han:Ishtiaq Rasool Khan:Susanto Rahardja	We propose an adaptive tone mapping method for displaying HDR images according to ambient light conditions. To compensate the loss of perceived luminance in brighter viewing conditions, we enhance the HDR image by an algorithm based on the Naka-Rushton model. Changes of the HVS response under different adaptation levels are considered and we match the response under the ambient conditions with the plateau response to the original HDR scene. The enhanced HDR image is tone mapped through a tone mapping curve constructed by the original image luminance histogram to produce visually pleasing images under given viewing conditions.	Lighting condition adaptive tone mapping method	NA:NA:NA	2018
Tobias Bertel:Christian Richardt	Capturing 360° panoramas has become straightforward now that this functionality is implemented on every phone. However, it remains difficult to capture immersive 360° panoramas with motion parallax, which provide different views for different viewpoints. Alternatives such as omnidirectional stereo panoramas provide different views for each eye (binocular disparity), but do not support motion parallax, while Casual 3D Photography [Hedman et al. 2017] reconstructs textured 3D geometry that provides motion parallax but suffers from reconstruction artefacts. We propose a new image-based approach for capturing and rendering high-quality 360° panoramas with motion parallax. We use novel-view synthesis with flow-based blending to turn a standard monoscopic video into an enriched 360° panoramic experience that can be explored in real time. Our approach makes it possible for casual consumers to capture and view high-quality 360° panoramas with motion parallax.	MegaParallax: 360° panoramas with motion parallax	NA:NA	2018
Antoine Toisoul:Daljit Singh J. Dhillon:Abhijeet Ghosh	We present a novel approach to measure the appearance of commonly found spatially varying holographic surfaces. Such surfaces are made of one dimensional diffraction gratings that vary in orientations and periodicities over a sample to create impressive visual effects. Our method is able to recover the orientation and periodicity maps simply using a flash illumination and a DSLR camera. We present real-time renderings under environmental illumination using the measured maps that match the observed appearance.	Practical acquisition and rendering of common spatially varying holographic surfaces	NA:NA:NA	2018
Yuliya Gitlina:Daljit Singh J. Dhillon:Jan Hansen:Dinesh K. Pai:Abhijeet Ghosh	Realistic appearance modeling of human skin is an important research topic with a variety of application in computer graphics. Various diffusion based BSSRDF models [Jensen et al. 2001, Donner and Jensen 2005, Donner and Jensen 2006] have been introduced in graphics to efficiently simulate subsurface scattering in skin including modeling its layered structure. These models however assume homogeneous subsurface scattering parameters and produce spatial color variation using an albedo map. In this work, we build upon the spectral scattering model of [Donner and Jensen 2006] and target a practical measurement-based rendering approach for such a spectral BSSRDF. The model assumes scattering in the two primary layers of skin (epidermis and dermis respectively) can be modeled with relative melanin and hemoglobin chromophore concentrations respectively. To drive this model for realistic rendering, we employ measurements of skin patches using an off-the-shelf Miravex Antera 3D camera which provides spatially varying maps of these chromophore concentrations as well as corresponding 3D surface geometry (see Figure 1) using a custom imaging setup.	Practical measurement-based spectral rendering of human skin	NA:NA:NA:NA:NA	2018
Markus Schuetz:Michael Wimmer	Rendering tens of millions of points in real time usually requires either high-end graphics cards, or the use of spatial acceleration structures. We introduce a method to progressively display as many points as the GPU memory can hold in real time by reprojecting what was visible and randomly adding additional points to uniformly converge towards the full result within a few frames. Our method heavily limits the number of points that have to be rendered each frame and it converges quickly and in a visually pleasing way, which makes it suitable even for notebooks with low-end GPUs. The data structure consists of a randomly shuffled array of points that is incrementally generated on-the-fly while points are being loaded. Due to this, it can be used to directly view point clouds in common sequential formats such as LAS or LAZ while they are being loaded and without the need to generate spatial acceleration structures in advance, as long as the data fits into GPU memory.	Progressive real-time rendering of unprocessed point clouds	NA:NA	2018
Anastasia Feygina:Dmitry I. Ignatov:Ilya Makarov	In this talk, we show a realistic post-processing rendering based on generative adversarial network CycleWGAN. We propose to use CycleGAN architecture and Wasserstein loss function with additional identity component in order to transfer graphics from Grand Theft Auto V to the older version of GTA video-game, Grand Theft Auto: San Andreas. We aim to present the application of modern art style transfer and unpaired image-to-image translations methods for graphics improvement using deep neural networks with adversarial loss.	Realistic post-processing of rendered 3D scenes	NA:NA:NA	2018
Yen-Chih Chiang:Shih-Song Cheng:Huei-Siou Chen:Le-Jean Wei:Li-Min Huang:David KT Chu	Currently1, Visual Reality Head-mounted Display has several problems that need to be overcome, such as insufficient resolution of the display, latency, Vergence-accommodation Conflict, etc., while the resolution is not high enough, causing the virtual image of the display to have graininess or Screen-door Effect. These problems have brought VR users an imperfect image quality experience and are unable to achieve a good sense of immersion. Therefore, it is necessary to solve the problem of insufficient display resolution. INT TECH Co., is working towards this goal and has made very good progress.	Retinal resolution display technology brings impact to VR industry	NA:NA:NA:NA:NA:NA	2018
Keiko Nakamoto:Takafumi Koike	We present an improved method for rendering heterogeneous translucent materials with existing BSSRDF models. In the general BSSRDF models, the optical properties of the target object are constant. Sone et al. have proposed a method to combine with existing BSSRDF models for rendering heterogeneous materials. However, the method generates more bright and blurred images compared with correctly simulated images. We have experimented with various BSSRDF models by the method and rendered heterogeneous materials. As a result, the rendered image with the better dipole model is the closest to the result of Monte carlo simulation. If incorporating the better dipole model into the method proposed by Sone et al., we can render more realistic images of heterogeneous materials.	Which BSSRDF model is better for heterogeneous materials?	NA:NA	2018
Nao Asano:Katsutoshi Masai:Yuta Sugiura:Maki Sugimoto	Facial performance capture is used for animation production that projects a performer's facial expression to a computer graphics model. Retro-reflective markers and cameras are widely used for the performance capture. To capture expressions, we need to place markers on the performer's face and calibrate the intrinsic and extrinsic parameters of cameras in advance. However, the measurable space is limited to the calibrated area. In this study, we propose a system to capture facial performance using a smart eyewear with photo-reflective sensors and machine learning technique. Also, we show a result of principal components analysis of facial geometry to determine a good estimation parameter set.	3D facial geometry analysis and estimation using embedded optical sensors on smart eyewear	NA:NA:NA:NA	2018
Paul Canada:George Ventura:Christopher Iossa:Orquidia Moreno:William J. Joel	Motion capture (MoCap) has been one of the leading and most useful tools within the field of animation to capture fluid and detailed motion. However, it can be quite expensive for animators, game developers and educators on a tight budgets. By using Raspberry Pi Zeros, with NoIR cameras and IR LED light rings, the cost of a four-camera system can potentially be reduced to less than 1000 USD. The research described should lead to an effective and useful system, able to detect multiple markers, record their coordinates, and keep track of them as they move. With a setup of three or more cameras, one would be able to triangulate the data on a low-cost host computer. All software and hardware designs will be disseminated open source, providing anyone who is interested in MoCap, whether it be for hobbyist, semi-professional, or educational purposes, a system for a fraction of the typical cost.	Development of an open source motion capture system	NA:NA:NA:NA:NA	2018
Kaizhang Kang:Zimin Chen:Jiaping Wang:Kun Zhou:Hongzhi Wu	Digitally acquiring high-quality material appearance from the real-world is challenging, with applications in visual effects, e-commerce and entertainment. One popular class of existing work is based on hand-derived illumination multiplexing [Ghosh et al. 2009], using hundreds of patterns in the most general case [Chen et al. 2014].	Learning optimal lighting patterns for efficient SVBRDF acquisition	NA:NA:NA:NA:NA	2018
Yoichi Ochiai:Kazuki Otao:Yuta Itoh:Shouki Imai:Kazuki Takazawa:Hiroyuki Osone:Atsushi Mori:Ippei Suzuki	Retinal projection is required for xR applications that can deliver immersive visual experience throughout the day. If general-purpose retinal projection methods can be realized at a low cost, not only could the image be displayed on the retina using less energy, but there is also a possibility of cutting off the weight of projection unit itself from the AR goggles. Several retinal projection methods have been previously proposed. Maxwellian optics based retinal projection was proposed in 1990s [Kollin 1993]. Laser scanning [Liao and Tsai 2009], laser projection using spatial light modulator (SLM) or holographic optical elements were also explored [Jang et al. 2017]. In the commercial field, QD Laser1 with a viewing angle of 26 degrees is available. However, as the lenses and iris of an eyeball are in front of the retina, which is a limitation of a human eyeball, the proposal of retinal projection is generally fraught with narrow viewing angles and small eyebox problems. Due to these problems, retinal projection displays are still a rare commodity because of their difficulty in optical schematics design.	Make your own retinal projector: retinal near-eye displays via metamaterials	NA:NA:NA:NA:NA:NA:NA:NA	2018
Kenta Yamamoto:Kotaro Omomo:Kazuki Takazawa:Yoichi Ochiai	The sun is the most universal, powerful and familiar energy available on the planet. Every organism and plant has evolved over the years, corresponding to the energy brought by the sun. Humanity is no exception. We have invented many artificial lights since Edison invented light bulbs. In recent years, LEDs are one of the most representative examples. Displays and projectors using LEDs are still being actively developed. However, it is difficult to reproduce ideal light with high brightness and wide wavelength like sunlight. Furthermore, considering low energy sustainability and environmental contamination in the manufacturing process, artificial light can not surpass the sunlight. Against this backdrop, projects that utilize sunlight have been actively carried out in the world. Concentrating Solar Power (CSP) generate electricity using the heat of sunlight to turn turbines [Müller-Steinhagen and Trieb 2004]. [Koizumi 2017] is an aerial image presentation system using the sun as a light source. Digital sundials use the shadow of sunlight to inform digital time [Scharstein et al. 1996]. These projects attempt to use the direct sunlight without any conversion and minimize the energy loss.	Solar projector	NA:NA:NA:NA	2018
Simone Barbieri:Tao Jiang:Ben Cawthorne:Zhidong Xiao:Xiaosong Yang	While 3D animation is constantly increasing its popularity, 2D is still largely in use in animation production. In fact, 2D has two main advantages. The first one is economic, as it is more rapid to produce, having a dimension less to consider. The second one is important for the artists, as 2D characters usually have highly distinctive traits, which are lost in a 3D transposition. An iconic example is Mickey Mouse, whom ears appear circular no matter which way he is facing.	3D content creation exploiting 2D character animation	NA:NA:NA:NA:NA	2018
Huiyi Fang:Kenji Funahashi	Human eyes have an adjustment function to adjust for different distances of seeing. However, it becomes weaker as you get older. When you move paper closer to read small letters, it is not in focus. When you move it away to bring it into focus, it is too small to read. This condition is called Presbyopia. People suffering from presbyopia also suffer from this condition when they use a smartphone or tablet. Although they can magnify the display using the pinch operation, it is a bother. A method for automatic display zoom, to see detail and an overview, was proposed in [Satake et al. 2016]. This method measures the distance between a face and a screen to judge whether you want to see detail or an overview. When you move it close to your face, it judges you want to see detail and zooms in. When you move it away from your face, it judges that you want to see overview and zooms out. In this paper, we improve and apply this method for presbyopia. First we observe and analyze the behavior of presbyopic people when trying to read small letters. Then we propose a suitable zooming function, for example, a screen is zoomed in also when it is moved away if the person suffers from presbyopia.	Automatic display zoom for people suffering from Presbyopia	NA:NA	2018
Buck Barbieri:Naomi Hutchens:Kayleigh Harrison	Massive Collaborative Animation Projects (MCAP) was founded in 2016 by Dr. William Joel (Western Connecticut State University) to test students' collaborative abilities and provide experience that will allow them to grow professionally and academically. The MCAP 1 production is a children's ghost story designed to test the massive collaborative structure. The goal of MCAP 2 is to create an animation for use in planetariums worldwide. Currently, there are nearly one hundred student contributors from universities in Alaska, California, Colorado, Connecticut, Japan, Michigan, South Korea, and Taiwan.	Collaborative animation production from students' perspective: creating short 3D CG films through international team-work	NA:NA:NA	2018
Martin Kilian:Hui Wang:Eike Schling:Jonas Schikore:Helmut Pottmann	The computation and construction of curved beams along freeform skins pose many challenges. We show how to use surfaces of constant mean curvature (CMC) to compute beam networks with beneficial properties, both aesthetically and from a fabrication perspective. To explore variations of such networks we introduce a new discretization of CMC surfaces as quadrilateral meshes with spherical vertex stars and right node angles. The computed non-CMC surface variations can be seen as a path in design space - exploring possible solutions in a neighborhood, or represent an actual erection sequence exploiting elastic material behavior.	Curved support structures and meshes with spherical vertex stars	NA:NA:NA:NA:NA	2018
Or Fleisher:Shirin Anlen	This paper presents Volume, a software toolkit that enables users to experiment with expressive reconstructions of archival and/or historical materials as volumetric renderings. Making use of contemporary deep learning methods, Volume re-imagines 2D images as volumetric 3D assets. These assets can then be incorporated into virtual, augmented and mixed reality experiences.	Volume: 3D reconstruction of history for immersive platforms	NA:NA	2018
Vincent Gaubert:Enki Londe:Thibaut Poittevin:Alain Lioret	We propose a new approach to 3D mesh fracturing for the fields of animation and game production. Through the use of machine learning and computer vision to analyze real fractures we produced a solution capable of creating realistic fractures in real-time.	3D-mesh cutting based on fracture photographs	NA:NA:NA:NA	2018
Hye sun Kim:Yun ji Ban:Chang joon Park	We present a technique to generate realistic high quality texture with no seams suitable to reconstruct large-scale 3D terrains. We focused on adjusting color difference caused by camera variations and illumination transition for texture reconstruction pipelines. Seams between separated processing areas should also be considered important in large terrain models. The proposed technique corrects these problems by normalizing texture colors and interpolating texture adjustment colors.	A seamless texture color adjustment method for large-scale terrain reconstruction	NA:NA:NA	2018
Kenta Yamamoto:Riku Iwasaki:Tatsuya Minagawa:Ryota Kawamura:Bektur Ryskeldiev:Yoichi Ochiai	3D printing failures can occur without completion of printing process due to shaking, errors in printer settings, and shape of the support material and 3D model. In such case it could be difficult to restart printing process from the last printed layer in conventional 3D printers, as the printing parts to which the nozzles are supposed to be attached are lost. In order to restart printing from the middle layer, Wu et al.[Wu et al. 2017] proposed a method of printing while rotating the base of a 3D printer. However, such approach required time for two objects to bond after segmentation, with limited availability of methods for adhesion between parts. Wu et al.[Wu et al. 2016] have also proposed a method to print 3D models at any angle through 5-axis rotation of the base of a 3D printer, but the manufacturing cost of such approach was relatively high. Therefore, we propose a system that prints 3D models on existing object by utilizing an infrared depth camera. Our method makes it possible to attach a 3D-printed object into a free-formed object in the middle of printing by recognizing its shape with a depth camera.	BOLCOF: base optimization for middle layer completion of 3D-printed objects without failure	NA:NA:NA:NA:NA:NA	2018
Byungjun Kwon:Moonwon Yu:Hanyoung Jang:KyuHyun Cho:Hyundong Lee:Taesung Hahn	This paper presents a novel motion transfer algorithm that copies content motion into a specific style character. The input consists of two motions. One is a content motion such as walking or running, and the other is movement style such as zombie or Krall. The algorithm automatically generates the synthesized motion such as walking zombie, walking Krall, running zombie, or running Krall. In order to obtain natural results, the method adopts the generative power of deep neural networks. Compared to previous neural approaches, the proposed algorithm shows better quality, runs extremely fast, does not require big data, and supports user-controllable style weights.	Deep motion transfer without big data	NA:NA:NA:NA:NA:NA	2018
Xiaodong Cun:Feng Xu:Chi-Man Pun:Hao Gao	Synthesizing images of novel viewpoints is widely investigated in computer vision and graphics. Most works in this topic focus on using multi-view images to synthesize viewpoints in-between. In this paper, we consider extrapolation, and we take a step further to do extrapolation from one single input image. This task is very challenging for two major reasons. First, some parts of the scene may not be observed in the input viewpoint but are required for novel ones. Second, 3D information is lacking for single view input but is crucial to determine pixel movements between viewpoints. Although very challenging, we observe that human brains are always able to imagine novel viewpoints. The reason is that human brains have learned in our daily lives to understand the depth order of objects in a scene [Chen et al. 2016] and infer what the scene looks like when viewing from another viewpoint.	Depth assisted full resolution network for single image-based view synthesis	NA:NA:NA:NA	2018
Sherzod Salokhiddinov:Seungkyu Lee	Depth estimation from differently focused set of images has been a practical approach for 3D reconstruction with existing color cameras. In this paper, we propose a depth from focus (DFF) method for accurate depth estimation using single commodity color camera. We investigate the appearance changes in spatial and frequency domain along the focused image frames in iterative manner. In order to achieve sub-frame level accuracy in depth estimation, optimal location of in-focus frame is estimated by fitting a parameterized polynomial curve on the dissimilarity measurements of each pixel. Quantitative and qualitative evaluations on various test image sets show promising performance of the proposed method in depth estimation.	Depth from focus for 3D reconstruction by iteratively building uniformly focused image set	NA:NA	2018
Chloe LeGendre:Kalle Bladin:Bipin Kishore:Xinglei Ren:Xueming Yu:Paul Debevec	We propose a variant to polarized gradient illumination facial scanning which uses monochrome instead of color cameras to achieve more efficient and higher-resolution results. In typical polarized gradient facial scanning, sub-millimeter geometric detail is acquired by photographing the subject in eight or more polarized spherical gradient lighting conditions made with white LEDs, and RGB cameras are used to acquire color texture maps of the subject's appearance. In our approach, we replace the color cameras and white LEDs with monochrome cameras and multispectral, colored LEDs, leveraging that color images can be formed from successive monochrome images recorded under different illumination colors. While a naive extension of the scanning process to this setup would require multiplying the number of images by number of color channels, we show that the surface detail maps can be estimated directly from monochrome imagery, so that only an additional n photographs are required, where n is the number of added spectral channels. We also introduce a new multispectral optical flow approach to align images across spectral channels in the presence of slight subject motion. Lastly, for the case where a capture system's white light sources are polarized and its multispectral colored LEDs are not, we introduce the technique of multispectral polarization promotion, where we estimate the cross- and parallel-polarized monochrome images for each spectral channel from their corresponding images under a full sphere of even, unpolarized illumination. We demonstrate that this technique allows us to efficiently acquire a full color (or even multispectral) facial scan using monochrome cameras, unpolarized multispectral colored LEDs, and polarized white LEDs.	Efficient multispectral facial capture with monochrome cameras	NA:NA:NA:NA:NA:NA	2018
Nobuhiko Mukai:Taishi Nishikawa:Youngha Chang	In this paper, we report evaluation of thin stretched thread lengths in spinnability simulations. There are many previous studies related to viscoelastic fluid, however, there are few studies that represent "spinnability", which is a feature that the material is stretched thin and long. Although some studies represented thread-forming property, they did not evaluate the stretched length of the material. We also tried to represent spinnability of viscoelastic fluid, however, the simulation results were not similar to a real material. Therefore, we try to perform spinnability simulations with three kinds of models, and evaluate stretched thread lengths by comparison of simulation results with a literature datum.	Evaluation of stretched thread lengths in spinnability simulations	NA:NA:NA	2018
Yuji Suzuki:Jotaro Shigeyama:Shigeo Yoshida:Takuji Narumi:Tomohiro Tanikawa:Michitaka Hirose	Food texture plays an important role in the experience of food. Researchers have proposed various methods to manipulate the perception of food texture using auditory and physical stimulation. In this paper, we demonstrate a system to present visually modified mastication movements in real-time to manipulate the perception of food texture, because visual stimuli efficiently work to enrich other food-related perceptions and showing someone their deformed posture changes somatosensory perception. The result of our experiments suggested that adding real-time feedback of facial deformation when participants open their mouths can increase the perceived chewiness of foods. Moreover, perceptions of hardness and adhesiveness were improved when the participants saw their modified face or listened to their non-modified chewing sound, while both perceptions were decreased when participants were presented with both stimuli. These results indicate the occurrence of the contrast effect.	Food texture manipulation by face deformation	NA:NA:NA:NA:NA:NA	2018
Quan Qi:Qingde Li	Converting a surface-based objects into a thin-surface solid representation is an essential problem for additive manufacturing. This paper proposes a simple way to thicken surfaces to thin solids based on implicit modelling technique. With the proposed technique, any surface-based object can be converted into a 3D printing friendly form that seamlessly combines both the geometric shape and its interior material structures in one single representation.	From visible to printable: thin surface with implicit interior structures	NA:NA	2018
Ivo Aluízio Stinghen Filho:Estevam Nicolas Chen:Jucimar Maia da Silva Junior:Ricardo da Silva Barboza	In this paper we compare the effectiveness of various methods of machine learning algorithms for real-time hand gesture recognition, in order to find the most optimal way to identify static hand gestures, as well as the most optimal sample size for use during the training step of the algorithms. In our framework, Leap Motion and Unity were used to extract the data. The data was then used to be trained using Python and scikit-learn. Utilizing normalized information regarding the hands and fingers, we managed to get a hit rate of 97% using the decision tree classifier.	Gesture recognition using leap motion: a comparison between machine learning algorithms	NA:NA:NA:NA	2018
Yifan Men:Zeyu Shen:Dawar Khan:Dong-Ming Yan	We present a novel method for valence optimization of the Centroidal Voronoi Tessellation (CVT). We first identify three commonly appeared atomic configurations of local irregular Voronoi cells, and then design specific atomic operations for each configuration to improve the regularity within the CVT framework.	Improving regularity of the centoridal voronoi tessellation	NA:NA:NA:NA	2018
Yeonho Kim:Daijin Kim	This paper presents a dance performance evaluation how well a learner mimics the teacher's dance as follows. We estimate the human skeletons, then extract dance features such as torso and first and second-degree feature, and compute the similarity score between the teacher and the learner dance sequence in terms of timing and pose accuracies. To validate the proposed dance evaluation method, we conducted several experiments on a large K-Pop dance database. The proposed methods achieved 98% concordance with experts' evaluation on dance performance.	Interactive dance performance evaluation using timing and accuracy similarity	NA:NA	2018
Ming-Shiuan Chen:I-Chao Shen:Chun-Kai Hunag:Bing-Yu Chen	In recent years, personalized fabrication has attracted many attentions due to the widespread of consumer-level 3D printers. However, consumer 3D printers still suffer from shortcomings such as long production time and limited output size, which are undesirable factors to large-scale rapid-prototyping. We propose a hybrid 3D fabrication method that combines 3D printing and Zometool structure for both time/cost-effective fabrication of large objects. The key of our approach is to utilize compact, sturdy and re-usable internal structure (Zometool) to infill fabrications and replace both time and material-consuming 3D-printed materials. Unlike the laser-cutted shape used in [Song et al. 2016], we are able to reuse the inner structure. As a result, we can significantly reduce the cost and time by printing thin 3D external shells only.	Large-scale fabrication with interior zometool structure	NA:NA:NA:NA	2018
Ryota Natsume:Tatsuya Yatagawa:Shigeo Morishima	This abstract introduces a generative neural network for face swapping and editing face images. We refer to this network as "region-separative generative adversarial network (RSGAN)". In existing deep generative models such as Variational autoencoder (VAE) and Generative adversarial network (GAN), training data must represent what the generative models synthesize. For example, image inpainting is achieved by training images with and without holes. However, it is difficult or even impossible to prepare a dataset which includes face images both before and after face swapping because faces of real people cannot be swapped without surgical operations. We tackle this problem by training the network so that it synthesizes synthesize a natural face image from an arbitrary pair of face and hair appearances. In addition to face swapping, the proposed network can be applied to other editing applications, such as visual attribute editing and random face parts synthesis.	RSGAN: face swapping and editing using face and hair representation in latent spaces	NA:NA:NA	2018
Danny Huang:Ian Stavness	Many thin tissues, such as leaves and flower petals, exhibit rippling and buckling patterns along their edge as they grow (Figure 1). Experiments with plastic materials have replicated the rippling patterns found in nature and shown that such patterns exhibit a fractal quality of ripples upon ripples --- a so called "buckling cascade" [Eran et al. 2004]. Such patterns are influenced by many physical mechanisms, including stress forces, physical properties of materials (e.g., stiffness), and space constraints [Prusinkiewicz and Barbier de Reuille 2010]. Physics-based computer animation that produces emergent rippling patterns on thin surface can improve the realism of virtual flowers and leaves, and also help to explain which physical mechanisms are most important for controlling the morphology of tissues with buckling cascades.	Simulation of emergent rippling on growing thin-shells	NA:NA	2018
Feier Cao:MHD Yamen Saraiji:Kouta Minamizawa	Wearable technologies have been supporting and augmenting our body and sensory functions for a long time. Skin+ introduces a novel bidirectional on-skin interface that serve not only as haptic feedback to oneself but also as a visual display to mediate touch sensation to others as well. In this paper, we describe the design of Skin+ and its usability in a variety of applications. We use a shape-changing auxetic structure to build this programmable coherent visuo-tactile interface. The combination of shape-memory alloy with an auxetic structure enables a lightweight haptic device that can be worn seamlessly on top of our skin.	Skin+: programmable skin as a visuo-tactile interface	NA:NA:NA	2018
Abdelhak Saouli:Mohamed Chaouki Babahenini	The human brain is constantly solving enormous and challenging optimization problems in vision. Due to the formidable meta-heuristics engine our brain equipped with, in addition to the widespread associative inputs from all other senses that act as the perfect initial guesses for a heuristic algorithm, the produced solutions are guaranteed to be optimal. By the same token, we address the problem of computing the depth and normal maps of a given scene under a natural but unknown illumination utilizing particle swarm optimization (PSO) to maximize a sophisticated photo-consistency function. For each output pixel, the swarm is initialized with good guesses starting with SIFT features as well as the optimal solution (depth, normal) found previously during the optimization. This leads to significantly better accuracy and robustness to textureless or quite specular surfaces.	Towards a stochastic depth maps estimation for textureless and quite specular surfaces	NA:NA	2018
Dominic Branchaud:Walter Muskovic:Maria Kavallaris:Daniel Filonik:Tomasz Bednarz	An innovative fully interactive and ultra-high resolution navigation tool has been developed to browse and analyze gene expression levels from human cancer cells, acting as a visual microscope on data. The tool uses high-performance visualization and computer graphics technology to enable genome scientists to observe the evolution of regulatory elements across time and gain valuable insights from their dataset as never before.	Visual microscope for massive genomics datasets, expanded perception and interaction	NA:NA:NA:NA:NA	2018
Richard Clegg:Richard Hoover:Chris McLaughlin	35 years after the release of the original "Blade Runner" film, the visual effects teams behind "Blade Runner 2049" were tasked with the challenge of crafting a dystopian world in the next phase of one of the most-beloved sci-fi films of all time. Set 30 years after the first film, the sequel follows a new blade runner as he unearths a long-buried secret that has the potential to plunge what's left of society into chaos. From the creation of the LA cityscapes, Las Vegas, and Trash Mesa environments to the development of a holographic Joi and the return of Rachael, join the filmmakers from DNEG, Framestore, and MPC as they discuss their Academy-Award winning work that paid tribute to the original picture while creating a film of the future.	DNEG, framestore, and MPC present: the visual effects of "Blade Runner 2049"	NA:NA:NA	2018
Thomas Hullin:Isabelle Langlois	In this production session, we will share our story of working on the legendary show, "Game of Thrones, "since the series' fourth season, detailing the learnings and knowledge we have gained from our multi-season experience on the groundbreaking show. We will go in depth on two of season 7's most intense sequences, starting from the concept art and working through the processes that got us to the final shots.	"Game of Thrones" season 7: orchestrating sea battles and blowing up a big wall	NA:NA	2018
Ian Failes:Rob Bredow:Matt Estela:Mark Hodgkins:Michael Kaschalk:Andy Hayes	In 1996, SideFX released Houdini version 1.0, bringing the power of procedural methods to visual effects artists around the world. This year, more than two decades since Houdini's original release, SideFX was awarded a Scientific and Technical Academy Award of Merit to recognize its continual innovation and dedication to visual effects artists.	Generations of Houdini in film	NA:NA:NA:NA:NA:NA	2018
Rob Bredow:Patrick Tubach:Greg Kegel:Joseph Kasparian	Join the visual effects team as they take you behind the scenes on one of 2018's biggest films. The team will showcase the innovative shooting techniques developed for the film and the unique collaboration with Director Ron Howard that allowed this chapter in the Star Wars universe to be brought to the screen. The team will also pull back the curtain on how they took old school methodologies and combined them with cutting edge technologies to create the film's groundbreaking visual effects work.	Making the kessel run in less than 12 parsecs: the VFX of "Soloa: Star Wars Story"	NA:NA:NA:NA	2018
Mahyar Abousaeedi:Beth Albright:Evan Bonifacio:Chris Burrows:Gordon Cameron:Ralph Eggleston:Nathan Fariss:Fran Kalal:Paul Kanyuk:Ted Mathot:Philip Metschan:Tom Nettleship:Bret Parker:Darwyn Peachey:Reid Sandros:Rick Sayre:Stephen Schaffer:Erik Smitt:Esdras Varagnolo:Bill Watral:Bill Wise	In a conversation that will not only span multiple disciplines, but also multiple years of technological advancement at Pixar, the team behind "Incredibles 2" - many of whom also worked on the first film - will compare and contrast the filmmaking process then and now. With a sequel, there's always the challenge of making a film true to the original, yet different in every detail. In building the world of "Incredibles 2" the team tackled one of the most technically daunting films in Pixar's canon, all while needing it to hue to the familiar tone established by the first film. Hear from this super group as they examine how they used the past to inform the present and, incredibly, achieved the near-impossible.	The Incredibles 2: suit up, it might get weird!	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Grady Cofer:David Shirk:David Dally:Scott Meadows:Ryan Magid	In this deep dive into Steven Spielberg's "Ready Player One" teams from Industrial Light & Magic and Digital Domain will showcase the break through virtual production techniques and technology deployed for the film and the visual effects involved in bringing the film's dystopian vision of life in 2045 to the screen. In addition, the teams will delve into the immense artistic and technical challenges of designing, building and animating every aspect of the expansive virtual universe known as the OASIS.	Three keys to creating the world of "ready player one" visual effects & virtual production	NA:NA:NA:NA:NA	2018
Koki Nagano:Jaewoo Seo:Kyle San:Aaron Hong:Mclean Goldwhite:Jun Xing:Stuti Rastogi:Jiale Kuang:Aviral Agarwal:Hanwei Kung:Caleb Arthur:Carrie Sun:Stephen Chen:Jens Fursund:Hao Li	A deep learning-based technology for generating photo-realistic 3D avatars with dynamic facial textures from a single input image is presented. Real-time performance-driven animations and renderings are demonstrated on an iPhone X and we show how these avatars can be integrated into compelling virtual worlds and used for 3D chats.	Deep learning-based photoreal avatars for online virtual worlds in iOS	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Cory Strassberger:Remco Sikkema	Kite & Lighting reveals how Xsens inertial mocap technology, used in tandem with an iPhone X, can be used for full body and facial performance capture - wirelessly and without the need for a mocap volume - with the results live-streamed to Autodesk Maya in real time.	Democratising mocap: real-time full-performance motion capture with an iPhone X, Xsens, and Maya	NA:NA	2018
Sam Glassenberg:Matthew Yaeger	Enter Gastro Ex for on smartphones and VR. The entire environment surrounding you is interactable and "squishy," featuring advanced soft-body physics and 3D interactive fluid dynamics. Grab anything. Cut anything. Inject anywhere. Unleash argon plasma. Enjoy emergent surgical gameplay, rendered with breathtaking real-time GI and subsurface scattering.	Gastro Ex: real-time interactive fluids and soft tissues on mobile and VR	NA:NA	2018
Tobias Soffner:Christopher Baumbach	IKEA Immerse is available in select IKEA stores in Germany. This application enables consumers to create, experience, and share their own configurations in a virtual living and kitchen room set. With seamless e-commerce integration, a high level of detail, and real-time interaction, the VR experience represents an engaging, valuable touch-point.	IKEA immerse interior designer	NA:NA	2018
Taehyun Rhee:Andrew Chalmers:Ian Loh:Ben Allen:Lohit Petikam:Stephen Thompson:Tom Revill	An interactive mixed reality system using live streamed 360° panoramic videos is presented. A live demo for real-time image-based lighting, light detection, mixed reality rendering, and composition of 3D objects into a live-streamed 360° video of a real-world environment with dynamically changing real-world lights is shown.	Mixed reality 360 live: live blending of virtual objects into 360° streamed video	NA:NA:NA:NA:NA:NA:NA	2018
Chris Harvey:Mike Blomkamp:Isabelle Riva:Neill Blomkamp	Come see how Oats Studios modified their traditional VFX pipeline to create the breakthrough real-time shorts ADAM Chapter 2 & 3 using Photogrammetry, Alembic, and the Unity real-time engine.	Oats studios VFX workflow for real-time production with photogrammetry, alembic, and unity	NA:NA:NA:NA	2018
Jean-Colas Prunier:Armelle Bauer:Yvain Raeymaekers:Stephane Tayeb	PocketStudio is designed to allow filmmakers to easily create, play, and stream 3D animation sequences in real time using real-time collaborative editing, a unified workflow, and other real-time technologies, such as augmented reality.	The power of real-time collaborative filmmaking	NA:NA:NA:NA	2018
Gavin Moran:Mohen Leo	Epic Games, Nvidia, and ILMxLAB would like to present 2018's GDC demo, "Reflections," set in the "Star Wars" universe. In addition, we will record a character performance live using virtual production/virtual reality directly into Unreal Engine Sequencer, and then play the demo with real-time ray tracing live at 24fps.	The 'reflections' ray-tracing demo presented in real time and captured live using virtual production techniques	NA:NA	2018
Francesco Giordana:Veselin Efremov:Gael Sourimant:Silvia Rasheva:Natasha Tatarchuk:Callum James	We demonstrate a Unity-powered virtual production platform that pushes the boundaries of real-time technologies to empower filmmakers with full multi-user collaboration and live manipulation of whole environments and characters. Special attention is dedicated to high-quality real-time graphics, as evidenced by Unity's "Book of the Dead."	Virtual production in 'book of the dead': technicolor's genesis platform, powered by unity	NA:NA:NA:NA:NA:NA	2018
Ayaka Ebisu:Satoshi Hashizume:Yoichi Ochiai	A sense of rhythm is essential for playing instruments. However, many beginners learning how to play musical instruments have difficulty with rhythm. We have proposed "Stimulated Percussions," which is a musical instrument performance system using electrical muscle stimulation (EMS) in the past. In this study, we apply it to the learning of rhythm. By the movement of muscles stimulated using EMS, users are able to acquire what kind of arms and legs to move at what timing. In addition to small percussion instruments such as castanets, users can play the rhythm patterns of drums that the require the simultaneous movement of their limbs.	Building a feedback loop between electrical stimulation and percussion learning	NA:NA:NA	2018
Matthew Griffin:Lizabeth Arum	Since its release, "The Design Engine" has been played by groups of students, teachers, and individuals looking to spark self-guided training. "The Design Engine" is a direct response to educators' requests for better classroom tools surrounding inspiration and 3D printing. By prompting participants to create their own original, imaginative works-instead of using pre-selected examples-teachers can keep their students better motivated through the process of mastering desktop 3D printing. We are hosting a brand new SIGGRAPH-edition of "The Design Engine," a constantly evolving series of challenges hosted within the Studio. Participants of all backgrounds can join for a short startup round, or stick around to design and develop their projects using the tools available in the SIGGRAPH Studio Workshop.	Design engine community project: generate quick adhoc inventions to explore at SIGGRAPH and in the studio	NA:NA	2018
Kengo Tanaka:Kohei Ogawa:Tatsuya Minagawa:Yoichi Ochiai	In this study, We propose a method to develop a spring glass dip pen by using a 3D printer and reproduce different types of writing feeling. There have been several studies on different types of pens to change the feel of writing. For example, EV-Pen [Wang et al. 2016] and haptics pens [Lee et al. 2004] changes the feel of pen writing with using vibration. However, our proposed method does not reproduce tactile sensation of softness by using vibrations.	Design method of digitally fabricated spring glass pen	NA:NA:NA:NA	2018
Quentin Galvane:I-Sheng Lin:Marc Christie:Tsai-Yen Li	Creatives in animated and real movie productions have been exploring new modalities to visually design filmic sequences before realizing them in studios, through techniques like hand-drawn storyboards, physical mockups or more recently virtual 3D environments. A central issue in using virtual 3D environments is the complexity of content creation tools for non technical film creatives. To overcome this issue, we present One Man Movie, a VR authoring system which enables the crafting of filmic sequences with no prior knowledge in 3D animation. The system is designed to reflect the traditional creative process in film pre-production through stages like (i) scene layout (ii) animation of characters, (iii) placement and control of cameras and (iv) montage of the filmic sequence, while enabling a fully novel and seamless back-and-forth between all stages of the process thanks to real-time engines. This research tool has been designed and evaluated with students and experts from film schools, and should therefore raise a significant interest among Siggraph participants.	Immersive previz: VR authoring for film previsualisation	NA:NA:NA:NA	2018
Robin Mange:Kepa Iturrioz Zabala	NA	IMVERSE livemaker: create a 3D model from a single 2D photo inside VR	NA:NA	2018
Brittany Factura:Laura LaPerche:Phil Reyneri:Brett Jones:Kevin Karsch	Projected augmented reality, also called projection mapping or video mapping, is a form of augmented reality that uses projected light to directly augment 3D surfaces, as opposed to using pass-through screens or headsets. The value of projected AR is its ability to add a layer of digital content directly onto physical objects or environments in a way that can be instantaneously viewed by multiple people, unencumbered by a screen or additional setup.	Lightform: procedural effects for projected AR	NA:NA:NA:NA:NA	2018
Alexandra Ion:Patrick Baudisch	In our hands-on demonstration, we show several objects, the functionality of which is defined by the objects' internal micro-structure. Such metamaterial machines can (1) be mechanisms based on their microstructures, (2) employ simple mechanical computation, or (3) change their outside to interact with their environment. They are 3D printed from one piece and we support their creating by providing interactive software tools.	Metamaterial devices	NA:NA	2018
Wataru Date:Yasuaki Kakehi	In this research, we propose a system which makes paper through additive manufacturing process by using a dispenser mounted on XY plotter. By using our system, graphic designers can design and output paper itself which is hard in an existing paper production process. This time, we designed and implemented a machine for fabricating paper and created several output examples. In SIGGRAPH, we will provide a workshop for participants to design their original paper using our machines.	Paperprinting: a machine for prototyping paper and its applications for graphic design	NA:NA	2018
Kevin Watters:Fernando Ramallo	Raymarching signed distance fields is a technique used by graphics experts and demoscene enthusiasts to construct scenes with features unusual in traditional polygonal workflows-blending shapes, kaleidoscopic patterns, reflections, and infinite fractal detail all become possible and are represented in compact representations that live mostly on the graphics card. Until now these scenes have had to be constructed in shaders by hand, but the Raymarching Toolkit for Unity is an extension that combines Unity's highly visual scene editor with the power of raymarched visuals by automatically generating the raymarching shader for the scene an artist is creating, live.	Raymarching toolkit for unity: a highly interactive unity toolkit for constructing signed distance fields visually	NA:NA	2018
