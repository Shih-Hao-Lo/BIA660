		PhD Symposium Chairs' Welcome		2018
Himan Abdollahpouri	One of the most essential parts of any recommender system is personalization– how acceptable the recommendations are from the user’s perspective. However, in many real-world applications, there are other stakeholders whose needs and interests should be taken into account. In this work, we define the problem of multistakeholder recommendation and we focus on finding algorithms for a special case where the recommender system itself is also a stakeholder. In addition, we will explore the idea of incremental incorporation of system-level objectives into recommender systems over time to tackle the existing problems in the optimization techniques which only look for optimizing the individual users’ lists.	Incorporating System-Level Objectives into Recommender Systems	NA	2018
Kaustav Basu	With billions of users, social networks have become the go to platform for information diffusion for news media outlets. Lately, certain entities (users and/or organizations) have been active in generating misinformation in order to attract users to their respective websites, to generate online advertisement revenues, to increase followers, to create political instability, etc. With the increasing presence of misinformation on social networks, it is becoming increasingly difficult to not only distinguish between information and misinformation, but also, to identify the source(s) of misinformation propagation. This effort reviews my doctoral research on identifying the source(s) of misinformation propagation. Particularly, I utilize the mathematical concept of Identifying Codes to uniquely identify users who become active in propagating misinformation. In this paper, I formally present the computation of the Minimum Identifying Code Set (MICS) as a novel variation of the traditional Graph Coloring problem. Furthermore, I present an Integer Linear Program for the computation of the MICS. I apply the technique on various anonymous Facebook network datasets and show the effectiveness of the approach.	Identification of the Source(s) of Misinformation Propagation Utilizing Identifying Codes	NA	2018
Robin Brochier	The scientific literature is a large information network linking various actors (laboratories, companies, institutions, etc.). The vast amount of data generated by this network constitutes a dynamic heterogeneous attributed network (HAN), in which new information is constantly produced and from which it is increasingly difficult to extract content of interest. In this article, I present my first thesis works in partnership with an industrial company, Digital Scientific Research Technology. This later offers a scientific watch tool, Peerus, addressing various issues, such as the real time recommendation of newly published papers or the search for active experts to start new collaborations. To tackle this diversity of applications, a common approach consists in learning representations of the nodes and attributes of this HAN and use them as features for a variety of recommendation tasks. However, most works on attributed network embedding pay too little attention to textual attributes and do not fully take advantage of recent natural language processing techniques. Moreover, proposed methods that jointly learn node and document representations do not provide a way to effectively infer representations for new documents for which network information is missing, which happens to be crucial in real time recommender systems. Finally, the interplay between textual and graph data in text-attributed heterogeneous networks remains an open research direction.	Representation Learning for Recommender Systems with Application to the Scientific Literature	NA	2018
Marie Destandau		Interactive Visualisation Techniques for the Web of Data	NA	2018
Edoardo Galimberti	Complex networks are a powerful paradigm to model complex systems. Specific network models, e.g., multilayer networks, temporal networks, and signed networks, enrich the standard network representation with additional information to better capture real-world phenomena. Despite the keen interest in a variety of problems, algorithms, and analysis methods for these types of network, the problem of extracting cores and dense structures still has unexplored facets. In this work, we present advancements to the state of the art by the introduction of novel definitions and algorithms for the extraction of dense structures from complex networks, mainly cores. At first, we define core decomposition in multilayer networks together with a series of applications built on top of it, i.e., the extraction of maximal multilayer cores only, densest subgraph in multilayer networks, the speed-up of the extraction of frequent cross-graph quasi-cliques, and the generalization of community search to the multilayer setting. Then, we introduce the concept of core decomposition in temporal networks; also in this case, we are interested in the extraction of maximal temporal cores only. Finally, in the context of discovering polarization in large-scale online data, we study the problem of identifying polarized communities in signed networks. The proposed methodologies are evaluated on a large variety of real-world networks against naïve approaches, non-trivial baselines, and competing methods. In all cases, they show effectiveness, efficiency, and scalability. Moreover, we showcase the usefulness of our definitions in concrete applications and case studies, i.e., the temporal analysis of contact networks, and the identification of polarization in debate networks.	Cores and Other Dense Structures in Complex Networks	NA	2018
Joobin Gharibshah:Michalis Faloutsos	The goal of this work is to systematically extract information from hacker forums, whose information would be in general described as unstructured: the text of a post is not necessarily following any writing rules. By contrast, many security initiatives and commercial entities are harnessing the readily public information, but they seem to focus on structured sources of information. Here, we focus on the problem of analyzing text content in security forums. A key novelty is that we use user profiles and contextual features along with transfer learning approach and also embedding space to help us identify and refine information that we could not get from security forum with trivial analysis. We collect a wealth of data from 5 different security forums. The contribution of our work is twofold; (a) we develop a method to automatically identify through the forums malicious IP addresses (b) we also propose a systematic method to identify and classify user-specified threads of interest into four categories. We further showcase how this information can inform knowledge extraction from the forums. As the cyber-wars are becoming more intense, having early accesses to useful information becomes more imperative to remove the hackers first-move advantage, and our work is a solid step towards this direction.	Extracting actionable information from Security Forums	NA:NA	2018
Jasser Jasser	The study of information-sharing cascades has been a constant endeavor since the emergence of social networks. Internet memes which mostly consist of catchphrases, viral images, or small videos shared over the social network are notorious for attracting the users’ attention and spreading through the web in a fast fashion. Misinformation propagators latch their message to a meme to maximize the influence and spreading of the false news. As a result, the diffusion of misleading content has become a force to be reckoned with in the field of information warfare, as foreign actors seek to change opinions, manipulate ideologies, and create conflicts. In this study, we analyze the rapid dissemination of misinformation, aka, misinformation cascades, focusing on cascade temporal behavior and multi-cascade influence relationships. Twitter data used in this study contains only information associated with the Russian Internet Agency (IRA) and the Iranian Cyber Army (ICA). Our study focuses on analyzing temporal patterns of information dynamics created by these foreign actors for the sole purpose of spreading misinformation. We explore dividing temporal cascades into phases, where each phase differs from the previous regarding the number and characteristics of the information bursts. For this preliminary study, we are focusing on the #Trump and #USA hashtags used by the ICA. By studying the dynamics behind each phase, the forces behind the transition from one phase to another, and the influence relationships between cascades and their phases, we expect to shed some light on the timely subject of how to identify and protect society from information manipulation campaigns.	Dynamics of Misinformation Cascades	NA	2018
Porter Jenkins	The rise of big data frameworks has given website administrators the ability to track user clickstream data with more detail than ever before. These clickstreams can represent the user’s intent and purpose in visiting the site. While existing work has explored methods for predicting future user actions, these methods are limited focus solely on one task at a time, ignore graph structure inherent in clickstreams, or model the conversion of the entire clickstream session, ignoring complexities such as multiple conversions in a single session. In this work, we formulate the novel problem of simultaneously predicting future user actions given a user’s clickstream history. We argue that clickstream data contains important signal for predicting future user action. To tackle this new problem, we propose a novel method called ClickGraph, a recurrent neural network that encodes the graph structure of user click trajectories in the learned representations of web pages. We conduct experiments on a real-world dataset and demonstrate that this multitask learning approach is effective at improving the prediction of form fill conversions over strong baselines. In particular, we demonstrate that the ClickGraph model is effective at reducing false positive rates, increasing F1 scores, and improving recall.	ClickGraph: Web Page Embedding using Clickstream Data for Multitask Learning	NA	2018
Lin Malone	Advancements in technology have enabled society to become increasingly globalised, both with regard to physical migration, as well as through the use of information and communication technologies (ICTs) to enable the maintenance of transnational ties. In particular, transient migration in the area of higher education has seen an increasing number of students migrate overseas for the purpose of their studies. However, research has shown that these international students are often disconnected from their host culture and society, with local-international friendships proving to be uncommon (Baldassar & McKenzie, 2016). Based on interviews with over 200 international students in Australia, Sawir et al. (2008) revealed that two-third of them had suffered or were suffering from loneliness, of which Sawir et al. identified three kinds -personal loneliness from loss of contact with families, social loneliness from the loss of networks, and cultural loneliness from the change in environment. This raises the question of how these students may be better supported, and the international student experience improved upon. Social media has often been positioned as a tool through which users become connected and communities are formed. One of the most popular platforms of social media, Facebook, has become an established part of many lives in modern society. Media and culture have always been interconnected, however, the dominance of the Web in everyday lives means that the role that media plays in cross-cultural communication is more significant than ever and must be researched for a better understanding of this phenomenon. While current research has examined the issues relating to construction of online identities for communication within established social networks, new issues have emerged in relation to collapsed contexts and imagined audiences in today's globalised world, especially as multiple cultures are introduced onto the same platform as a result of migration or relocation. Insufficient research has been done into the influence of technology on transient migration and its potential to support cross-cultural communication. The question thus is how exactly social media may assist transient migrants overcome issues of isolation and loneliness, and provide them with support during their time abroad. This study looks to address the issue of student isolation within host societies by examining how social media may provide spaces for support, self-expression and cross-cultural communication. Through a visual internet ethnographic study, it examines the profile pages of international students on Facebook to better understand their positions in these home and host societies. This research study is supplemented by semi-structured interviews for a thorough examination of international students’ use of social media. As we examine how the Web has developed and changed over the last 30 years, it must be acknowledged that this change cannot be solely attributed to technological advancements, but is also influenced by the actual human users of the Web who participate in it. Web users utilise online tools to produce content of their own, tailoring their online experiences accordingly. While advancements in technology has created a more globalised society, the globalised users within this society have had their own impact on technology. This research looks to create a more in-depth understanding of the ways in which social networking platforms are used by transient migrants to navigate transnational cultural settings. It is aimed at enabling deeper understanding of the complex inter-linkages between cultures, to engender new insights of transnational identities. This is essential to address the global nature of today's society and the role of social media platforms in the spaces they create for transient migrants.	Finding Themselves between Home and Host Cultures	NA	2018
Donya Rooein	There are many online courses and contents on the web, so each learner can find the best one and choose it. However, sometimes many online courses are poorly accessible due to the limits of the search engines on the web. The advent of intelligent systems, and online Chatbots, in particular, has brought improvement in various fields. Education Chatbots improve communication, increase productivity, and simplify learning interaction. This study aims to provide an intelligent Edu Chatbot with a high level of customization to learners who have different needs. This way, they can find their personalized learning path dynamically and their customized content without too much time and effort. This is precisely what e-learning needs because of the enormous amount of material on the web.	Data-Driven Edu Chatbots	NA	2018
Henry Rosales-Méndez	The Entity Linking (EL) task is concerned with linking entity mentions in a text collection with their corresponding knowledge-base entries. Despite the progress made in the evaluation of EL systems, there is still much work to be done, where this Ph.D. research tackles issues concerning EL evaluation. Among these issues, we stress (a) the lack of consensus about the definition of “entity” and the lack of evaluation metrics that allow for different notions of entities, (b) the lack of datasets that allow for cross-language comparison, and (c) the focus on evaluating high-level systems rather than low-level techniques. By addressing these challenges and better understanding the performance of EL systems, our hypothesis is that we can create a more general, more configurable EL framework that can be better adapted to the needs of a particular application. In the early stages of this PhD work, we have identified these problems and begun to address (a) and (b), publishing initial results that constitute a significant step forward in our investigation. However, there are still further challenges that must be addressed before we reach our goal. Our next steps thus involve proposing a more fluid definition of “entity” adaptable to different applications, the definition of quality measures that allow for comparing EL approaches targeting different types of entities, as well as the creation of a customizable EL framework that allows for composing and evaluating individual techniques as appropriate to a particular task.	Towards Better Entity Linking Evaluation	NA	2018
Riccardo Tommasini	It is a streaming world: a new generation of Web Applications is pushing the Web infrastructure to evolve and process data as soon as they arrive. However, the Web of Data is not appealing to the growing number of Web Applications demanding to tame Data Velocity. To solve these issues, we need to introduce new key abstractions, i.e., stream and events. Moreover, we need to investigate how to identify, represent and process streams and events on the Web. In this paper, we discuss why taming Velocity on the Web of Data. We present a Design Science research plan that builds on the state of the art of Stream Reasoning and RDF Stream Processing. Finally, we present our research results, for representing and processing stream and events on the Web.	Velocity on the Web - a PhD Symposium	NA	2018
Mengfan Yao	The potentially detrimental effects of cyberbullying have led to the development of numerous automated, data–driven approaches, with an emphasis on classification accuracy. Cyberbullying, as a form of abusive online behavior, although not well–defined, is a repetitive process, i.e., a sequence of aggressive messages sent from a bully to a victim over a period of time with the intent to harm the victim. Existing work has focused on aggression (i.e., using profanity to classify toxic comments independently) as an indicator of cyberbullying, disregarding the repetitive nature of this harassing process. However, raising a cyberbullying alert immediately after an aggressive comment is detected can lead to a high number of false positives. At the same time, three key practical challenges remain unaddressed: (i) detection timeliness, which is necessary to support victims as early as possible, (ii) scalability to the staggering rates at which content is generated in online social networks, (iii) reliance on high quality annotations from human experts for training of highly accurate supervised classifiers. To overcome the challenges associated with cyberbullying detection in online social networks, my PhD thesis focuses on a novel formulation of the online classification problem as sequential hypothesis testing that seeks to drastically reduce the number of features used while maintaining high classification accuracy. To reduce the dependency on labeled datasets, I seek to develop efficient semisupervised methods that extrapolate from a small seed set of expert annotations. Preliminary results are very encouraging, showing significant improvements over the state–of–the–art.	Robust Detection of Cyberbullying in Social Media	NA	2018
		Attention - from Neuroscience to the Web and Wellbeing		2018
Michael Xuelin Huang	Eye tracking provides an effective solution to users’ attention, interest, and engagement. While gaze estimation based on a standard camera can be versatile, it remains challenging to achieve an accurate, robust, and scalable solution on mobile devices. In this talk, I will describe three studies that aim to address these challenges. Specifically, 1) we found that screen reflection on user's cornea can be leveraged for gaze estimation and it considerably improves the practicability of indoor eye tracking. 2) We exploited gaze-hand coordination and applied interaction data for implicit calibration when a user naturally interacts with the computer. This can prevent users from tedious and intrusive calibration in practice. 3) We also proposed to train a multi-device person-specific gaze estimator to accelerate implicit calibration. It adapts the data from different personal devices to learn the shared mapping from user appearance into eye gaze. Taken together, these studies identify indicative eye gaze features, alleviate user calibration effort, and thus pave the way for scalable eye tracking in daily use.	Scalable Eye Tracking for Mobile Devices	NA	2018
Laurent Itti	Visual attention and eye movements in primates have been widely shown to be guided by a combination of stimulus-dependent or 'bottom-up' cues, as well as task-dependent or 'top-down' cues. Both the bottom-up and top-down aspects of attention and eye movements have been modeled computationally. Yet, it is not until recent work which I will describe that bottom-up models have been strictly put to the test, predicting significantly above chance the eye movement patterns, functional neuroimaging activation patterns, or most recently neural activity in the superior colliculus of human or monkey participants inspecting complex static or dynamic scenes. In recent developments, models that increasingly attempt to capture top-down aspects have been proposed. In one system which I will describe, neuromorphic algorithms of bottom-up visual attention are employed to predict, in a task-independent manner, which elements in a video scene might more strongly attract attention and gaze. These bottom-up predictions have more recently been combined with top-down predictions, which allowed the system to learn from examples (recorded eye movements and actions of humans engaged in 3D video games, including flight combat, driving, first-person, or running a hot-dog stand that serves hungry customers) how to prioritize particular locations of interest given the task. Pushing deeper into real-time, joint online analysis of video and eye movements using neuromorphic models, we have recently been able to predict future gaze locations and intentions of future actions when a player is engaged in a task. In a similar approach where computational models provide a normative gold standard against a particular individual's gaze behavior, machine learning systems have been demonstrated which can predict, from eye movement recordings during as little as only 5 minutes of watching TV, whether a person has ADHD or other neurological disorders. Together, these studies suggest that it is possible to build fully computational models that coarsely capture some aspects of both bottom-up and top-down visual attention.	Lessons from neuroscience	NA	2018
		5th AW4City 2019 Chairs' Welcome		2018
Evagelia Anagnostopoulou:Babis Magoutas:Efthimios Bothos:Gregoris Mentzas	In this paper, we study the effectiveness of personalized persuasive interventions to change urban travelers’ mobility behavior and nudge them towards more sustainable transport choices. More specifically, we embed a set of persuasive design elements in a route planning application and investigate how they affect users’ travel choices. The design elements take into consideration the style, the intensity, the target of persuasive interventions as well as users’ characteristics and the trip purpose. Our results show evidence that our proposed approach motivates users on a personal level to change their mobility behavior and make more sustainable choices. Furthermore, by personalizing the persuasive interventions while considering combinations of interventions styles (in our case messages and visualizations) as well as adjusting the intensity of persuasive interventions according to the trip purpose and the transport modes of the routes which the user is nudged to follow, the effects of the persuasive interventions can be increased.	Persuasive Technologies for Sustainable Smart Cities: The Case of Urban Mobility	NA:NA:NA:NA	2018
Rob Christiaanse	MAAS as a mobility model leans on the idea that a gap between private and public transport systems needs to be bridged as well as on a city, intercity, national and supranational level. The current situation is felt problematic due to the fragmented tools and services often organized in silos to meet a traveler needs to undertake a trip. One of the major concerns designing any platform system like Mobility as a Service is where to start modeling and how to express the notion of the platform system in some language that is understandable for all stakeholders of the platform system. Understandability buttresses the expectation of stakeholders whether some design will probably implement the intended platform services enabling users to actually buy and or use the platform system for what ever purpose. Building on the economic theories of two-sided markets and mechanism design we introduce the concept of value nets extending the Contract Protocol Net. Value net modeling offers a precise abstract representation which provides in the detailed informational requirements in a canonical form and it connects i.e. implements the abstract notion of Service Oriented Architecture characterizing systems without loss of crucial informational elements.	Mobility as a Service	NA	2018
Mengfan Yao:Charalampos Chelmis:Daphney-Stavroula Zois	The potentially detrimental effects of cyberbullying have led to the development of numerous automated, data–driven approaches, with emphasis on classification accuracy. Cyberbullying, as a form of abusive online behavior, although not well–defined, is a repetitive process, i.e., a sequence of aggressive messages sent from a bully to a victim over a period of time with the intent to harm the victim. Existing work has focused on harassment (i.e., using profanity to classify toxic comments independently) as an indicator of cyberbullying, disregarding the repetitive nature of this harassing process. However, raising a cyberbullying alert immediately after an aggressive comment is detected can lead to a high number of false positives. At the same time, two key practical challenges remain unaddressed: (i) timeliness: the state–of–the–art relies on a fixed set of features learned during training for offline detection (i.e., after all correspondence has become available), hindering the ability to respond in a timely manner (i.e., as soon as possible) to cyberbullying events. (ii) scalabilty: the scalability of existing methods to the staggering rates at which content is generated (e.g., 95 million photos and videos are shared on Instagram per day1) has largely remained unaddressed. In my lightning talk, I will introduce CONcISE, a novel approach for timely and accurate Cyberbullying detectiON on Instagram media SEssions, that has been accepted for presentation at the main conference [1]. Specifically, I will present a novel two–stage online approach (illustrated in Figure 1) designed to reduce the time to raise a cyberbullying alert by (i) sequentially examining comments as they become available over time, and (ii) minimizing the number of feature evaluations necessary for a decision to be made for each comment. By formalizing the problem as a sequential hypothesis testing problem, a novel algorithm has been developed that satisfies four key properties: accuracy, repetitiveness, timeliness, and efficiency. Extensive experiments on a real–world Instagram dataset with ∼ 4M users and ∼ 10M comments demonstrate the effectiveness of the proposed approach with respect to accuracy, timeliness, efficiency, and robustness, and show that it consistently outperforms the stat–of–the–art, often by a considerable margin.	Lightning Talk–Towards Robust Detection of Cyberbullying in Social Media	NA:NA:NA	2018
Xinyi Zhou:Reza Zafarani	The explosive growth of fake news and its erosion to democracy, journalism and economy has increased the demand for fake news detection. To achieve efficient and explainable fake news detection, an interdisciplinary approach is required, relying on scientific contributions from various disciplines, e.g., social sciences, engineering, among others. Here, we illustrate how such multidisciplinary contributions can help detect fake news by improving feature engineering, or by providing well-justified machine learning models. We demonstrate how news content, news propagation patterns, and users’ engagements with news can help detect fake news.	Fake News Detection: An Interdisciplinary Research	NA:NA	2018
Omar Alonso	Many data intensive applications that use machine learning or artificial intelligence techniques depend on humans providing the initial dataset, enabling algorithms to process the rest or for other humans to evaluate the performance of such algorithms. There are, however, practical issues with the adoption of human computation and crowdsourcing at scale in the real world. Building systems data processing pipelines that require crowd computing remains difficult. In this tutorial, we present practical considerations for designing and implementing tasks that require the use of humans and machines in combination with the goal of producing high quality labels.	The Practice of Labeling: Everything You Always Wanted to Know About Labeling (But Were Afraid to Ask)	NA	2018
Lora Aroyo:Anca Dumitrache:Oana Inel:Zoltán Szlávik:Benjamin Timmermans:Chris Welty	In this tutorial, we introduce a novel crowdsourcing methodology called CrowdTruth [1, 9]. The central characteristic of CrowdTruth is harnessing the diversity in human interpretation to capture the wide range of opinions and perspectives, and thus provide more reliable, realistic and inclusive real-world annotated data for training and evaluating machine learning components. Unlike other methods, we do not discard dissenting votes, but incorporate them into a richer and more continuous representation of truth. CrowdTruth is a widely used crowdsourcing methodology1 adopted by industrial partners and public organizations such as Google, IBM, New York Times, Cleveland Clinic, Crowdynews, Sound and Vision archive, Rijksmuseum, and in a multitude of domains such as AI, news, medicine, social media, cultural heritage, and social sciences. The goal of this tutorial is to introduce the audience to a novel approach to crowdsourcing that takes advantage of the diversity of opinions and perspectives that is inherent to the Web, as methods that deal with disagreement and diversity in crowdsourcing have become increasingly popular. Creating this more complex notion of truth contributes directly to the larger discussion on how to make the Web more reliable, diverse and inclusive.	Crowdsourcing Inclusivity: Dealing with Diversity of Opinions, Perspectives and Ambiguity in Annotated Data	NA:NA:NA:NA:NA:NA	2018
Ricardo Baeza-Yates:Sharad Goel	Machine learning algorithms increasingly affect both our online and offline experiences. Researchers and policymakers, however, have rightfully raised concerns that these systems might inadvertently exacerbate societal biases. We provide an introduction to fair machine learning, beginning with a general overview of algorithmic fairness, and then discussing these issues specifically in the context of the Web. To measure and mitigate potential bias from machine learning systems, there has recently been an explosion of competing mathematical definitions of what it means for an algorithm to be fair. Unfortunately, as we show, many of the most prominent definitions of fairness suffer from subtle shortcomings that can lead to serious adverse consequences when used as an objective. To illustrate these complications, we draw on a variety of classical and modern ideas from statistics, economics, and legal theory. We further discuss the equity of machine learning algorithms in the specific context of the Web, focusing on search engines and e-commerce websites. We expose the different sources for bias on the Web and how they impact fairness. They include not only data bias, but also biases that are produced by data sampling, the algorithms per-se, user interaction and feedback loops that result from user personalization and content creation. All these lead to a vicious cycle that affects everybody. The content of this tutorial is mainly based in the work of the authors [1,2,3,4].	Designing Equitable Algorithms for the Web	NA:NA	2018
Sarah Bird:Ben Hutchinson:Krishnaram Kenthapadi:Emre Kıcıman:Margaret Mitchell	Researchers and practitioners from different disciplines have highlighted the ethical and legal challenges posed by the use of machine learned models and data-driven systems, and the potential for such systems to discriminate against certain population groups, due to biases in algorithmic decision-making systems. This tutorial aims to present an overview of algorithmic bias / discrimination issues observed over the last few years and the lessons learned, key regulations and laws, and evolution of techniques for achieving fairness in machine learning systems. We will motivate the need for adopting a “fairness-first” approach (as opposed to viewing algorithmic bias / fairness considerations as an afterthought), when developing machine learning based models and systems for different consumer and enterprise applications. Then, we will focus on the application of fairness-aware machine learning techniques in practice, by highlighting industry best practices and case studies from different technology companies. Based on our experiences in industry, we will identify open problems and research challenges for the data mining / machine learning community.	Fairness-Aware Machine Learning: Practical Challenges and Lessons Learned	NA:NA:NA:NA:NA	2018
Somit Gupta:Ronny Kohavi:Alex Deng:Jeff Omhover:Pawel Janowski	The Internet and the general digitalization of products and operations provides an unprecedented opportunity to accelerate innovation while applying a rigorous and trustworthy methodology for supporting key product decisions. Developers of connected software, including web sites, applications, and devices, can now evaluate ideas quickly and accurately using controlled experiments, also known as A/B tests. From front-end user-interface changes to backend algorithms, from search engines (e.g., Google, Bing, Yahoo!) to retailers (e.g., Amazon, eBay, Etsy) to social networking services (e.g., Facebook, LinkedIn, Twitter) to travel services (e.g., Expedia, Airbnb, Booking.com) to many startups, online controlled experiments are now utilized to make data-driven decisions at a wide range of companies. The theory of a controlled experiment is simple, but for the practitioner the deployment and evaluation of online controlled experiments at scale (100’s of concurrently running experiments) across a variety of web sites, mobile apps, and desktop applications presents many pitfalls and new research challenges. In this tutorial, we will introduce the overall A/B testing methodology, walkthrough use cases using real examples, and then focus on practical and research challenges in scaling experimentation. We will share key lessons learned from scaling experimentation at Microsoft to thousands of experiments per year and outline promising directions for future work.	A/B Testing at Scale: Accelerating Software Innovation	NA:NA:NA:NA:NA	2018
Krishna P. Gummadi:Hoda Heidari	Machine Learning is increasingly employed to make consequential decisions for humans. In response to the ethical issues that may ensue, an active area of research in ML has been dedicated to the study of algorithmic unfairness. This tutorial introduces fair-ML to the web conference community and offers a new perspective on it through the lens of the long-established economic theories of distributive justice. Based on our past and ongoing research, we argue that economic theories of equality of opportunity, inequality measurement, and social choice have a lot to offer—in terms of tools and insights—to data scientists and practitioners interested in understanding the ethical implications of their work. We overview these theories and discuss their connections to fair-ML.	Economic Theories of Distributive Justice for Fair Machine Learning	NA:NA	2018
Liangjie Hong:Mounia Lalmas	User engagement plays a central role in companies operating online services, such as search engines, news portals, e-commerce sites, entertainment services, and social networks. A main challenge is to leverage collected knowledge about the daily online behavior of millions of users to understand what engage them short-term and more importantly long-term. Two critical steps of improving user engagement are metrics and their optimization. The most common way that engagement is measured is through various online metrics, acting as proxy measures of user engagement. This tutorial will review these metrics, their advantages and drawbacks, and their appropriateness to various types of online services. Once metrics are defined, how to optimize them will become the key issue. We will survey methodologies including machine learning models and experimental designs that are utilized to optimize these metrics via direct or indirect ways. As case studies, we will focus on four types of services, news, search, entertainment, and e-commerce. We will end with lessons learned and a discussion on the most promising research directions.	Tutorial on Online User Engagement: Metrics and Optimization	NA:NA	2018
Rushed Kanawati:Martin Atzmueller	In the field of web mining and web science, as well as data science and data mining there has been a lot of interest in the analysis of (social) networks. With the growing complexity of heterogeneous data, feature-rich networks have emerged as a powerful modeling approach: They capture data and knowledge at different scales from multiple heterogeneous data sources, and allow the mining and analysis from different perspectives. The challenge is to devise novel algorithms and tools for the analysis of such networks. This tutorial provides a unified perspective on feature-rich networks, focusing on different modeling approaches, in particular multiplex and attributed networks. It outlines important principles, methods, tools and future research directions in this emerging field.	Modeling and Mining Feature-Rich Networks	NA:NA	2018
Krishnaram Kenthapadi:Ilya Mironov:Abhradeep Thakurta	Preserving privacy of users is a key requirement of web-scale data mining applications and systems such as web search, recommender systems, crowdsourced platforms, and analytics applications, and has witnessed a renewed focus in light of recent data breaches and new regulations such as GDPR. In this tutorial, we will first present an overview of privacy breaches over the last two decades and the lessons learned, key regulations and laws, and evolution of privacy techniques leading to differential privacy definition / techniques. Then, we will focus on the application of privacy-preserving data mining techniques in practice, by presenting case studies such as Apple’s differential privacy deployment for iOS / macOS, Google’s RAPPOR, LinkedIn Salary, and Microsoft’s differential privacy deployment for collecting Windows telemetry. We will conclude with open problems and challenges for the data mining / machine learning community, based on our experiences in industry.	Privacy-preserving Data Mining in Industry	NA:NA:NA	2018
Luca Pappalardo:Gianni Barlacchi:Roberto Pellungrini:Filippo Simini	The inclusion of tracking technologies in personal devices opened the doors to the analysis of large sets of mobility data like GPS traces and call detail records. This tutorial presents an overview of both modeling principles of human mobility and machine learning models applicable to specific problems. We review the state of the art of five main aspects in human mobility: (1) human mobility data landscape; (2) key measures of individual and collective mobility; (3) generative models at the level of individual, population and mixture of the two; (4) next location prediction algorithms; (5) applications for social good. For each aspect, we show experiments and simulations using the Python library ”scikit-mobility” developed by the presenters of the tutorial.	Human Mobility from theory to practice:Data, Models and Applications	NA:NA:NA:NA	2018
Vayianos Pertsas:Panos Constantopoulos	Understanding and extracting knowledge contained in text and encoding it as linked data for the WEB is a highly complex task that poses several challenges, requiring expertise from different fields such as conceptual modeling, natural language processing and web technologies including web mining, linked data generation and publishing, etc. When it comes to the scholarly domain, the transformation of human readable research articles into machine comprehensible knowledge bases is considered of high importance and necessity today due to the explosion of scientific publications in every major discipline, that makes it increasingly difficult for experts to maintain an overview of their domain or relate ideas from different domains. This situation could be significantly alleviated by knowledge bases capable of supporting queries such as: find all papers that address a given problem; how was the problem solved; which methods are employed by whom in addressing particular tasks; etc. that currently cannot be addressed by commonly used search engines such as Google Scholar or Semantic Scholar. This tutorial addresses the above challenge by introducing the participants to methods required in order to model knowledge regarding a given domain, extract information from available texts using advanced machine learning techniques, associate it with other information mined from the web in order to infer new knowledge and republish everything as linked open data on the Web. To this end, we will use a specific use case – that of the scholarly domain, and will show how to model research processes, extract them from research articles, associate them with contextual information from article metadata and other linked repositories and create knowledge bases available as linked data. Our aim is to show how methodologies from different computer science fields, namely natural language processing, machine learning and conceptual modeling, can be combined with Web technologies in a single meaningful workflow.	From Research Articles to Knowledge Graphs	NA:NA	2018
Massimo Quadrana:Dietmar Jannach:Paolo Cremonesi	Recommender systems are widely used in online applications to help users find items of interest and help them deal with information overload. In this tutorial, we discuss the class of sequence-aware recommender systems. Differently from the traditional problem formulation based on a user-item rating matrix, the input to such systems is a sequence of logged user interactions. Likewise, sequence-aware recommender systems implement alternative computational tasks, such as predicting the next items a user will be interested in an ongoing session or creating entire sequences of items to present to the user. We propose a problem formulation, sketch a number of computational tasks, review existing algorithmic approaches, and finally discuss evaluation aspects of sequence-aware recommender systems.	Tutorial: Sequence-Aware Recommender Systems	NA:NA:NA	2018
Comandur Seshadhri:Srikanta Tirthapura	Subgraph counting is a fundamental problem in graph analysis that finds use in a wide array of applications. The basic problem is to count or approximate the occurrences of a small subgraph (the pattern) in a large graph (the dataset). Subgraph counting is a computationally challenging problem, and the last few years have seen a rich literature develop around scalable solutions for it. However, these results have thus far appeared as a disconnected set of ideas that are applied separately by different research groups. We observe that there are a few common algorithmic building blocks that most subgraph counting results build on. In this tutorial, we attempt to summarize current methods through distilling these basic algorithmic building blocks. The tutorial will also cover methods for subgraph analysis on “big data” computational models such as the streaming model and models of parallel and distributed computation.	Scalable Subgraph Counting: The Methods Behind The Madness	NA:NA	2018
Omprakash Sonie:Muthusamy Chelliah:Shamik Sural	Deep Learning has shown significant results in various domains. In this tutorial, we provide conceptual understanding of embedding methods, Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNNs). We present fashion use case and apply these techniques for modeling image, text as well as sequence data to figure out user profiles, give personalized recommendations tailored to changing user taste and interest. Given the image of a fashion item, recommending complementary matches is a challenge. Users’ taste evolves over time and depends on persona. Humans relate objects based on their appearance and non-visual factors of lifestyle merchandise which further complicate recommendation task. Composing outfits in addition necessitates constituent items to be compatible - similar in some but different in other aspects.	Concept to Code: Deep Learning for Fashion Recommendation	NA:NA:NA	2018
Jie Tang:Yuxiao Dong	Network representation learning offers a revolutionary paradigm for mining and learning with network data. In this tutorial, we will give a systematic introduction for representation learning on networks. We will start the tutorial with industry examples from Alibaba, AMiner, Microsoft Academic, WeChat, and XueTangX to explain how network analysis and graph mining on the Web are benefiting from representation learning. Then we will comprehensively introduce both the history and recent advances on network representation learning, such as network embedding and graph neural networks. Uniquely, this tutorial aims to provide the audience with the underlying theories in network representation learning, as well as our experience in translating this line of research into real-world applications on the Web. Finally, we will release public datasets and benchmarks for open and reproducible network representation learning research. The tutorial accompanying page is at https://aminer.org/nrl_www2019.	Representation Learning on Networks: Theories, Algorithms, and Applications	NA:NA	2018
Riccardo Tommasini:Robin Keskisärkkä:Jean-Paul Calbimonte:Eva Blomqvist:Emanuele Della Valle:Albert Bifet	This half-day tutorial provides a comprehensive introduction to web stream processing, including the fundamental stream reasoning concepts, as well as an introduction to practical implementations and how to use them in concrete web applications. To this extent, we intend to (1) survey existing research outcomes from Stream Reasoning / RDF Stream Processing that arise in querying, reasoning on and learning from a variety of highly dynamic data, (2) introduce deductive and inductive stream reasoning techniques as powerful tools to use when addressing a data-centric problem characterized both by variety and velocity, (3) present a relevant use-case, which requires to address data velocity and variety simultaneously on the web, and guide the participants in developing a web stream processing application.	Continuous Analytics of Web Streams	NA:NA:NA:NA:NA:NA	2018
Yulia Tsvetkov:Vinodkumar Prabhakaran:Rob Voigt	As language technologies have become increasingly prevalent in analyzing online data, there is a growing awareness that decisions we make about our data, methods, and tools often have immense impact on people and societies. This tutorial will provide an overview of real-world applications of Natural Language Processing technologies and their potential ethical implications. We intend to provide the researchers with an overview of tools to ensure that the data, algorithms, and models that they build are socially responsible. These tools will include a checklist of common pitfalls that one should avoid, as well as methods to mitigate these issues. Issues of bias, ethics, and impact are often not clear-cut; this tutorial will also discuss the complexities inherent in this area.	Socially Responsible Natural Language Processing	NA:NA:NA	2018
Erik Wilde:Mike Amundsen	The Web as the world’s largest information system has largely settled on a solid foundation of HTTP-based connectivity, and the representation of User Interface (UI) information resources through a mix of HTML and scripting. In contrast, the similarly rapidly evolving “Web of Services” is still based on a more diverse and more quickly evolving set of approaches and technologies. This can make architectural decisions harder when it comes to choosing on how to expose information and services through an Application Programming Interface (API). This challenge becomes even more pronounced when organizations are faced with developing strategies for managing constantly growing and evolving API landscapes. This tutorial takes participants through two different journeys. The first one is a journey discussing API styles and API technologies, comparing and contrasting them as a way to highlight the fact that there is no such thing as the one best choice. The goal of this first journey is to provide an overview of how APIs are used nowadays in research and in industry. The second journey discusses the question of how to define an API strategy, which focuses both on helping teams to make effective choices about APIs in a given context, and on how to manage that context over time when large organizations nowadays have thousands of APIs, which will continue to evolve constantly.	The Challenge of API Management: API Strategies for Decentralized API Landscapes	NA:NA	2018
Wei Wu:Rui Yan	The tutorial is based on our long-term research on open domain conversation and rich hands-on experience on development of Microsoft XiaoIce. We will summarize the recent achievements made by both academia and industry on chatbots, and give a thorough and systematic introduction to state-of-the-art methods for open domain conversation modeling including both retrieval-based methods and generation-based methods. In addition to these, our tutorial will also cover some new trends of research of chatbots, such as how to design a reasonable evaluation metric for open domain dialogue generation, how to build conversation models with multiple modalities, and how to conduct dialogue management in open domain conversation systems.	Deep Chit-Chat: Deep Learning for Chatbots	NA:NA	2018
Hinnerk Eißfeldt	Looking ahead, in about 20 years, there is likely to be urban air mobility in larger cities across the globe. If economic predictions come true, thousands of air taxi flights will take place daily in capital cities – not only in megacities. Noise generated by urban flight mobility has been identified as a critical factor in this development. A concept is proposed to help raise the tolerance level for urban air noise among communities as well as of individual residents by means of transparency. This concept views residents as stakeholders in urban air mobility and widens the call for continuous noise measurements of vertical take-off and landing operations on individual site basis [1] by residents voluntary on-site data collection enabled by smartphone-based participatory noise sensing (PNS). In the presentation of this concept, this discussion paper describes important aspects of social acceptance of urban air mobility.	Supporting Urban Air Mobility with Citizen Participatory Noise Sensing: A Concept	NA	2018
Daniel Herzog:Sherjeel Sikander:Wolfgang Wörndl	Tourist trip recommender systems (RSs) support travelers in identifying the most attractive points of interests (POIss) and combine the POIss along a route for single- or multi-day trips. Most RSs consider only the quality of POIss when searching for the best recommendation. In this work, we introduce a novel approach that also considers the attractiveness of the routes between POIss. For this purpose, we identify a list of important attributes of route attractiveness and explain how to implement our approach using three exemplary attributes. We develop a web application for demonstration purposes and apply it in a small preliminary user study with 16 participants. The results show that the integration of route attractiveness attributes makes most people choose the more attractive route over the shortest path between two POIss. This paper highlights how tourist trip RSs can support smart tourism. Our work aims to encourage further discussion on collecting and providing environmental data in cities to enable such applications.	Integrating Route Attractiveness Attributes into Tourist Trip Recommendations	NA:NA:NA	2018
Eddy Maddalena:Luis-Daniel Ibáñez:Elena Simperl:Richard Gomer:Mattia Zeni:Donglei Song:Fausto Giunchiglia	Sustainable mobility is one of the main goals of both European and United Nations plans for 2030. The concept of Smart Cities has arisen as a way to achieve this goal by leveraging IoT interconnected devices to collect and analyse large quantities of data. However, several works have pointed out the importance of including the human factor, and in particular, citizens, to make sense of the collected data and ensure their engagement along the data value chain. This paper presents the design and implementation of two end-to-end hybrid human-machine workflows for solving two mobility problems: modal split estimation, and mapping mobility infrastructure. For modal split, we combine the use of i-Log, an app to collect data and interact with citizens, with reinforcement learning classifiers to continuously improve the accuracy of the classification, aiming at reducing the required interactions from citizens. For mobility infrastructure, we developed a system that uses remote crowdworkers to explore the city looking for Points of Interest, that is more scalable than sending agents on the field. Crowdsourced maps are then fused with existing maps (if available) to create a final map that then is validated on the field by citizens engaged through the i-Log app.	Hybrid Human Machine workflows for mobility management	NA:NA:NA:NA:NA:NA:NA	2018
Md Ashifuddin Mondal:Zeenat Rehena	Managing the ever increasing road traffic congestion due to enormous vehicular growth is a big concern all over the world. Tremendous air pollution, loss of valuable time and money are the common consequences of traffic congestion in urban areas. IoT based Intelligent Transportation System (ITS) can help in managing the road traffic congestion in an efficient way. Estimation and classification of the traffic congestion state of different road segments is one of the important aspects of intelligent traffic management. Traffic congestion state recognition of different road segments helps the traffic management authority to optimize the traffic regulation of a transportation system. The commuters can also decide their best possible route to the destination based on traffic congestion state of different road segments. This paper aims to estimate and classify the traffic congestion state of different road segments within a city by analyzing the road traffic data captured by in-road stationary sensors. The Artificial Neural Network (ANN) based system is used to classify traffic congestion states. Based on traffic congestion status, ITS will automatically update the traffic regulations like, changing the queue length in traffic signal, suggesting alternate routes. It also helps the government to device policies regarding construction of flyover/alternate route for better traffic management.	Intelligent Traffic Congestion Classification System using Artificial Neural Network	NA:NA	2018
William H. Money:Stephen Cohen	Cities have entered the age of the sensor and located sensors everywhere over and under cities. The sensors monitor a host of factors that assess City operations and life such as air quality, noise, city services and traffic. Further, the sensors have “gone mobile” with announcements of situation aware mobile sensor platforms designed for city-level security and public safety. These wearable sensor platforms combine video, audio, and location data with Internet of Things (IoT) capabilities. However, the many sensors and functional platforms have not yet made the cities employing these many diverse sensors truly Smart. We are analyzing why the success toward the Smart city is limited, or late in coming. The explanations for the constrained effectiveness are assigned to many factors, but one of significance can be teased from a long-accepted explanation that associates data, information, and knowledge. Smart Cities need to effectively use the sensor data and the information assembled from these interpreted and organized data to create knowledge that serves the city and its people by answering and resolving key problems and questions. But the systems and analytic models needed to associate these data from many sensors have yet to be designed, constructed, and proven in the complex cities of today. Thus, the data (and information from the diverse sensors) lacks crucial integration and coordination for decisions and sense-making. While these sensor-based systems were, and in many cases are meeting some intended functionally discrete goals, they appear to be better described as data collection tools feeding centralized analytical engines. They are point solutions with specialized or targeted sensors feeding specialized solutions. This is a significant limiting factor in a city's drive to improve the quality of life and the efficiency of the services a city provides to its stakeholders. In this paper we present current trends in Smart City development, emerging issues with data and complexity growth, and proposes a mean to leverage the advancing technologies to address the integration problem.	Leveraging AI and Sensor Fabrics to Evolve Smart City Solution Designs	NA:NA	2018
Vaia Moustaka:Athena Vakali:Nikos Zikos:Theoklis Tsirakidis:Leonidas G. Anthopoulos	Smart technologies advancements, emerging markets competition and sustainability needs have radically changed tourism and transport sectors. The key features of this change are the exploitation of evolving Big Data in the business intelligence context, the development of customized services tailored to the needs of consumers with the purpose of improving their experience, and the development of new business models based on the interaction between business and consumers. This is due to the capacity of smart transport technologies to integrate customers sensing and in this way a novel framework aimed at: i) developing personalized transport services in the tourism sector and ii) creating and delivering patterns of tourist consumer behavior according to specific target groups and market segments at tourist destination or country level is designed and outlined. The proposed “TΟMI” framework, exploits tour data analytics, in order to enable the deployment of personalized tour services that will be beneficial for tour operators, travellers and any other interested parties (local stakeholders, tourism entrepreneurs, etc.). The exploitation of the “TOMI” framework for the purpose of organizing tours in a city is also addressed through a case study on the city of Thessaloniki.	TOMI: A Framework for Smart Tourism on the Move Innovation	NA:NA:NA:NA:NA	2018
Daksh Shah:Aravinda Kumaran:Rijurekha Sen:Ponnurangam Kumaraguru	Travel time estimates are highly useful in planning urban mobility events. This paper investigates the quality of travel time estimates in the Indian capital city of Delhi and the National Capital Region (NCR). Using Uber mobile and web applications, we collect data about 610 trips from 34 Uber users. We empirically show the unpredictability of travel time estimates for Uber cabs. We also discuss the adverse effects of such unpredictability on passengers waiting for the cabs, leading to a whopping 28.4% of the requested trips being cancelled. Our empirical observations differ significantly from the high accuracies reported in travel time estimation literature. These pessimistic results will hopefully trigger useful investigations in future on why the travel time estimates are mismatching the high accuracy levels reported in literature - (a) is it a lack of training data issue for developing countries or (b) an algorithmic shortcoming that cannot capture the (lack of) historical patterns in developing region travel times or (c) a conscious policy decision by Uber platform or Uber drivers, to mismatch the correctly predicted travel time estimates and increase cab cancellation fees? In the context of smartphone apps extensively generating and utilizing travel time information for urban commute, this paper identifies and discusses the important problem of travel time estimation inaccuracies in developing countries.	Travel Time Estimation Accuracy in Developing Regions: An Empirical Case Study with Uber Data in Delhi-NCR✱	NA:NA:NA:NA	2018
Nicolas Tempelmeier:Yannick Rietz:Iryna Lishchuk:Tina Kruegel:Olaf Mumm:Vanessa Miriam Carlow:Stefan Dietze:Elena Demidova	With the increasing availability of mobility-related data, such as GPS-traces, Web queries and climate conditions, there is a growing demand to utilize this data to better understand and support urban mobility needs. However, data available from the individual actors, such as providers of information, navigation and transportation systems, is mostly restricted to isolated mobility modes, whereas holistic data analytics over integrated data sources is not sufficiently supported. In this paper we present our ongoing research in the context of holistic data analytics to support urban mobility applications in the Data4UrbanMobility (D4UM) project. First, we discuss challenges in urban mobility analytics and present the D4UM platform we are currently developing to facilitate holistic urban data analytics over integrated heterogeneous data sources along with the available data sources. Second, we present the MiC app - a tool we developed to complement available datasets with intermodal mobility data (i.e. data about journeys that involve more than one mode of mobility) using a citizen science approach. Finally, we present selected use cases and discuss our future work.	Data4UrbanMobility: Towards Holistic Data Analytics for Mobility Applications in Urban Regions	NA:NA:NA:NA:NA:NA:NA:NA	2018
Richard Han:Neil Shah		Cybersafety 2019: The 4th Workshop on Computational Methods in Online Misbehavior	NA:NA	2018
Seyed Ali Alhosseini:Raad Bin Tareaf:Pejman Najafi:Christoph Meinel	Spam Bots have become a threat to online social networks with their malicious behavior, posting misinformation messages and influencing online platforms to fulfill their motives. As spam bots have become more advanced over time, creating algorithms to identify bots remains an open challenge. Learning low-dimensional embeddings for nodes in graph structured data has proven to be useful in various domains. In this paper, we propose a model based on graph convolutional neural networks (GCNN) for spam bot detection. Our hypothesis is that to better detect spam bots, in addition to defining a features set, the social graph must also be taken into consideration. GCNNs are able to leverage both the features of a node and aggregate the features of a node’s neighborhood. We compare our approach, with two methods that work solely on a features set and on the structure of the graph. To our knowledge, this work is the first attempt of using graph convolutional neural networks in spam bot detection.	Detect Me If You Can: Spam Bot Detection Using Inductive Representation Learning	NA:NA:NA:NA	2018
Hamidreza Alvari:Elham Shaabani:Soumajyoti Sarkar:Ghazaleh Beigi:Paulo Shakarian	Recent years have witnessed a surge of manipulation of public opinion and political events by malicious social media actors. These users are referred to as “Pathogenic Social Media (PSM)” accounts. PSMs are key users in spreading misinformation in social media to viral proportions. These accounts can be either controlled by real users or automated bots. Identification of PSMs is thus of utmost importance for social media authorities. The burden usually falls to automatic approaches that can identify these accounts and protect social media reputation. However, lack of sufficient labeled examples for devising and training sophisticated approaches to combat these accounts is still one of the foremost challenges facing social media firms. In contrast, unlabeled data is abundant and cheap to obtain thanks to massive user-generated data. In this paper, we propose a semi-supervised causal inference PSM detection framework, SemiPsm, to compensate for the lack of labeled data. In particular, the proposed method leverages unlabeled data in the form of manifold regularization and only relies on cascade information. This is in contrast to the existing approaches that use exhaustive feature engineering (e.g., profile information, network structure, etc.). Evidence from empirical experiments on a real-world ISIS-related dataset from Twitter suggests promising results of utilizing unlabeled instances for detecting PSMs.	Less is More: Semi-Supervised Causal Inference for Detecting Pathogenic Users in Social Media	NA:NA:NA:NA:NA	2018
Adam Badawy:Kristina Lerman:Emilio Ferrara	Social media, once hailed as a vehicle for democratization and the promotion of positive social change across the globe, are under attack for becoming a tool of political manipulation and spread of disinformation. A case in point is the alleged use of trolls by Russia to spread malicious content in Western elections. This paper examines the Russian interference campaign in the 2016 US presidential election on Twitter. Our aim is twofold: first, we test whether predicting users who spread trolls’ content is feasible in order to gain insight on how to contain their influence in the future; second, we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content. We collected a dataset with over 43 million election-related posts shared on Twitter between September 16 and November 9, 2016, by about 5.7 million users. This dataset includes accounts associated with the Russian trolls identified by the US Congress. Proposed models are able to very accurately identify users who spread the trolls’ content (average AUC score of 96%, using 10-fold validation). We show that political ideology, bot likelihood scores, and some activity-related account meta data are the most predictive features of whether a user spreads trolls’ content or not.	Who Falls for Online Political Manipulation?	NA:NA:NA	2018
Lu Cheng:Ruocheng Guo:Huan Liu	Cyberbullying poses serious threats to preteens and teenagers, therefore, understanding the incentives behind cyberbullying is critical to prevent its happening and mitigate the impact. Most existing work towards cyberbullying detection has focused on the accuracy, and overlooked causes of the outcome. Discovering the causes of cyberbullying from observational data is challenging due to the existence of confounders, variables that can lead to spurious causal relationships between covariates and the outcome. This work studies the problem of robust cyberbullying detection with causal interpretation and proposes a principled framework to identify and block the influence of the plausible confounders, i.e., p-confounders. The de-confounded model is causally interpretable and is more robust to the changes in data distribution. We test our approach using the state-of-the-art evaluation method, causal transportability. The experimental results corroborate the effectiveness of our proposed algorithm. The purpose of this study is to provide a computational means to understanding cyberbullying behavior from observational data. This improves our ability to predict and to facilitate effective strategies or policies to proactively mitigate the impact of cyberbullying.	Robust Cyberbullying Detection with Causal Interpretation	NA:NA:NA	2018
Joobin Gharibshah:Zhabiz Gharibshah:Evangelos E. Papalexakis:Michalis Faloutsos	How useful is the information that a security analyst can extract from a security forum? We focus on threads of interest, which we define as: (i) alerts of worrisome events, such as attacks, (ii) offering of malicious services and products, (iii) hacking information to perform malicious acts, and (iv) useful security-related experiences. The analysis of security forums is in its infancy despite several promising recent works. Here, we leverage our earlier work in thread analysis, and ask the question: what kind of information do these malicious threads provide. Specifically, we analyze threads in three dimensions: (a) temporal characteristics, (b) user-centric characteristics (c) content-centric properties. We study threads pulled from three security forums spanning the period 2012-2016. First, we show that 53% of the users asking/selling malicious Services on average has 3 posts and initiate 1 thread and 1 day lifetime. Second, we argue that careful analysis can help to identify emerging threats reported in security forums through Services and Alerts threads and potentially help security analysts prevent attacks. We see this study as a first attempt to argue for the wealth and type of information that can be extracted from security forums.	An Empirical Study of Malicious Threads in Security Forums	NA:NA:NA:NA	2018
Hsien-Te Kao:Shen Yan:Di Huang:Nathan Bartley:Homa Hosseinmardi:Emilio Ferrara	Cyberbullying is a major issue on online social platforms, and can have prolonged negative psychological impact on both the bullies and their targets. Users can be characterized by their involvement in cyberbullying according to different social roles including victim, bully, and victim supporter. In this work, we propose a social role detection framework to understand cyberbullying on online social platforms, and select a dataset that contains users’ records on both Instagram and Ask.fm as a case study. We refine the traditional victim-bully framework by constructing a victim-bully-supporter network on Instagram. These social roles are automatically identified via ego comment networks and linguistic cues of comments. Additionally, we analyze the consistency of users’ social role within Instagram and compare users’ behaviors on Ask.fm. Our analysis reveals the inconsistency of social roles both within and across platforms, which suggests social roles in cyberbullying are not invariant by conversation, person, or social platform.	Understanding Cyberbullying on Instagram and Ask.fm via Social Role Detection	NA:NA:NA:NA:NA:NA	2018
Shuaiji Li:Tao Huang:Zhiwei Qin:Fanfang Zhang:Yinhong Chang	Digital threats such as backdoors, trojans, info-stealers and bots can be especially damaging nowadays as they actively steal information or allow remote control for nefarious purposes. A common attribute amongst such malware is the need for network communication and many of them use domain generation algorithms (DGAs) to pseudo-randomly generate numerous domains to communicate with each other to avoid being take-down by blacklisting method. DGAs are constantly evolving and these generated domains are mixed with benign queries in network communication traffic each day, which raises a high demand for an efficient real-time DGA classifier on domains in DNS log. Previous works either rely on group contextual/statistical features or extra host-based information and thus need long time window, or depend on lexical features extracted from domain strings to build real-time classifiers, or directly build an end-to-end deep neural network to make prediction from domain strings. Pros and cons exist for either way in experiments. In this paper, we propose several new real-time detection models and frameworks which utilize meta-data generated from domains and combine the advantages of a deep neural network model and a lexical features based model using the ensemble technique. Our proposed model obtains performance higher than all state-of-art methods so far to the best knowledge of the authors, with both precision and recall at 99.8% on a widely used public dataset.	Domain Generation Algorithms detection through deep neural network and ensemble	NA:NA:NA:NA:NA	2018
Jan Pennekamp:Martin Henze:Oliver Hohlfeld:Andriy Panchenko	Public opinion manipulation is a serious threat to society, potentially influencing elections and the political situation even in established democracies. The prevalence of online media and the opportunity for users to express opinions in comments magnifies the problem. Governments, organizations, and companies can exploit this situation for biasing opinions. Typically, they deploy a large number of pseudonyms to create an impression of a crowd that supports specific opinions. Side channel information (such as IP addresses or identities of browsers) often allows a reliable detection of pseudonyms managed by a single person. However, while spoofing and anonymizing data that links these accounts is simple, a linking without is very challenging. In this paper, we evaluate whether stylometric features allow a detection of such doppelgängers within comment sections on news articles. To this end, we adapt a state-of-the-art doppelgänger detector to work on small texts (such as comments) and apply it on three popular news sites in two languages. Our results reveal that detecting potential doppelgängers based on linguistics is a promising approach even when no reliable side channel information is available. Preliminary results following an application in the wild shows indications for doppelgängers in real world data sets.	Hi Doppelgänger : Towards Detecting Manipulation in News Comments	NA:NA:NA:NA	2018
Nazgol Tavabi:Nathan Bartley:Andres Abeliuk:Sandeep Soni:Emilio Ferrara:Kristina Lerman	The deep and darkweb (d2web) refers to limited access web sites that require registration, authentication, or more complex encryption protocols to access them. These web sites serve as hubs for a variety of illicit activities: to trade drugs, stolen user credentials, hacking tools, and to coordinate attacks and manipulation campaigns. Despite its importance to cyber crime, the d2web has not been systematically investigated. In this paper, we study a large corpus of messages posted to 80 d2web forums over a period of more than a year. We identify topics of discussion using LDA and use a non-parametric HMM to model the evolution of topics across forums. Then, we examine the dynamic patterns of discussion and identify forums with similar patterns. We show that our approach surfaces hidden similarities across different forums and can help identify anomalous events in this rich, heterogeneous data.	Characterizing Activity on the Deep and Dark Web	NA:NA:NA:NA:NA:NA	2018
Kai-Cheng Yang:Pik-Mai Hui:Filippo Menczer	It has been widely recognized that automated bots may have a significant impact on the outcomes of national events. It is important to raise public awareness about the threat of bots on social media during these important events, such as the 2018 US midterm election. To this end, we deployed a web application to help the public explore the activities of likely bots on Twitter on a daily basis. The application, called Bot Electioneering Volume (BEV), reports on the level of likely bot activities and visualizes the topics targeted by them. With this paper we release our code base for the BEV framework, with the goal of facilitating future efforts to combat malicious bots on social media.	Bot Electioneering Volume: Visualizing Social Bot Activity During Elections	NA:NA:NA	2018
Savvas Zannettou:Tristan Caulfield:Emiliano De Cristofaro:Michael Sirivianos:Gianluca Stringhini:Jeremy Blackburn	Over the past couple of years, anecdotal evidence has emerged linking coordinated campaigns by state-sponsored actors with efforts to manipulate public opinion on the Web, often around major political events, through dedicated accounts, or “trolls.” Although they are often involved in spreading disinformation on social media, there is little understanding of how these trolls operate, what type of content they disseminate, and most importantly their influence on the information ecosystem. In this paper, we shed light on these questions by analyzing 27K tweets posted by 1K Twitter users identified as having ties with Russia’s Internet Research Agency and thus likely state-sponsored trolls. We compare their behavior to a random set of Twitter users, finding interesting differences in terms of the content they disseminate, the evolution of their account, as well as their general behavior and use of Twitter. Then, using Hawkes Processes, we quantify the influence that trolls had on the dissemination of news on social platforms like Twitter, Reddit, and 4chan. Overall, our findings indicate that Russian trolls managed to stay active for long periods of time and to reach a substantial number of Twitter users with their tweets. When looking at their ability of spreading news content and making it viral, however, we find that their effect on social platforms was minor, with the significant exception of news published by the Russian state-sponsored news outlet RT (Russia Today).	Disinformation Warfare: Understanding State-Sponsored Trolls on Twitter and Their Influence on the Web	NA:NA:NA:NA:NA:NA	2018
		Data Science for Social Good Chairs' Welcome		2018
Leo Ferres, Universidad del Desarrollo, Santiago	Since 2016, through an association between Telefónica R&D and the Institute of Data Science in Chile, a group of researchers and myself have been working with trillions of digital traces left behind when people use their mobile phones. All of this work has been done under the general umbrella term of "data science for social good", and we have worked on anything from population displacement after external events like earthquakes, how people started using public spaces after the introduction of a popular mobile game, to actual social inclusion of people of different socio-economic backgrounds mixing in shopping malls or reading certain kinds of news, or patterns arising from gendered data sets. We will show how data in the private sector made us learn important social lessons such as how parks can become more secure when people went out to play Pokemon Go, how certain malls are hubs of social inclusion, how gender segregates the city and how different demographics keep themselves in their own informational filter bubble. However, even after all this benefits, the relationship with industry has never been fluid, and involves a lot of small and not so small compromises and "battles". In this talk, I will present a technical history of the work we've done with X/CDRs for social good including practical aspects of accessing and sharing data, the balance of research and industrial innovation, and issues of transactions costs while still providing value for the company itself, government, the university and society. I will also recount experiences about what it meant for a company like Telefónica and a research university like us to travel together in a very interesting context of huge data, incredible insights, privacy considerations, money, corporate interests, university expectations, and data-driven discovery.	Problems and Opportunities of Working with a Telco's Large Data Sets of Mobile Data*	NA	2018
Alex Jaimes	In most emergencies, people use social media platforms to publicly share information. Such data, from multiple sources is extremely useful for emergency response and public safety: the more knowledge that is gathered, the better the response can be. When an emergency event first occurs, getting the right information as quickly as possible is critical in saving lives. When an emergency event is ongoing, information on what is happening can be critical in making decisions to keep people safe and take control of the particular situation unfolding. In both cases, first responders have to quickly make decisions that include what resources to deploy and where. In this talk, I will describe challenges in emergency response and how a computational platform that leverages public data can address them. A platform to detect emergency situations and deliver the right information to first responders has to deal with ingesting thousands of data points per second: sifting through and identifying relevant information, from different sources, in different formats, with varying levels of detail, in real time, so that first responders and others can be alerted at the right level and at the right time. I will describe technical challenges in processing vast amounts of heterogenous data in real time, highlighting the importance of interdisciplinary research and a human-centered approach to address problems in emergency response. I will give specific examples and discuss relevant research topics in Machine Learning, NLP, Information Retrieval, Computer Vision and other fields.	AI for Good at Scale: Challenges in Real Time Emergency Response	NA	2018
Charalampos Chelmis:Mengfan Yao	Human service providers play a critical role in improving well–being in the United States. However, little is know about (i) how service seekers find the services they are looking for by navigating among available service providers, and (ii) how such organizations collaborate to meet human needs. In this paper, we report the first outcomes of our ongoing project. Specifically, we first describe a data acquisition engine, designed around the particular challenges of capturing, maintaining, and updating data pertaining to human service organizations from semistructured Web sources. We then proceed to illustrate the potential of the resulting comprehensive repository of human service providers through a case study showcasing a mobile app prototype designed to provide a one–stop shop for human service seekers.	Creating Public Value by Democratizing the Ecosystem of Human Service Providers	NA:NA	2018
Ashok Deb:Luca Luceri:Adam Badaway:Emilio Ferrara	One of the hallmarks of a free and fair society is the ability to conduct a peaceful and seamless transfer of power from one leader to another. Democratically, this is measured in a citizen population’s trust in the electoral system of choosing a representative government. In view of the well documented issues of the 2016 US Presidential election, we conducted an in-depth analysis of the 2018 US Midterm elections looking specifically for voter fraud or suppression. The Midterm election occurs in the middle of a 4 year presidential term. For the 2018 midterms, 35 Senators and all the 435 seats in the House of Representatives were up for re-election, thus, every congressional district and practically every state had a federal election. In order to collect election related tweets, we analyzed Twitter during the month prior to, and the two weeks following, the November 6, 2018 election day. In a targeted analysis to detect statistical anomalies or election interference, we identified several biases that can lead to wrong conclusions. Specifically, we looked for divergence between actual voting outcomes and instances of the #ivoted hashtag on the election day. This analysis highlighted three states of concern: New York, California, and Texas. We repeated our analysis discarding malicious accounts, such as social bots. Upon further inspection and against a backdrop of collected general election-related tweets, we identified some confounding factors, such as population bias, or bot and political ideology inference, that can lead to false conclusions. We conclude by providing an in-depth discussion of the perils and challenges of using social media data to explore questions about election manipulation.	Perils and Challenges of Social Media and Election Manipulation Analysis: The 2018 US Midterms	NA:NA:NA:NA	2018
Kyriaki Kalimeri:Mariano G. Beiró:Alessandra Urbinati:Andrea Bonanomi:Alessandro Rosina:Ciro Cattuto	Psychological, political, cultural, and even societal factors are entangled in the reasoning and decision-making process towards vaccination, rendering vaccine hesitancy a complex issue. Here, administering a series of surveys via a Facebook-hosted application, we study the worldviews of people that “Liked” supportive or vaccine resilient Facebook Pages. In particular, we assess differences in political viewpoints, moral values, personality traits, and general interests, finding that those sceptical about vaccination, appear to trust less the government, are less agreeable, while they are emphasising more on anti-authoritarian values. Exploring the differences in moral narratives as expressed in the linguistic descriptions of the Facebook Pages, we see that pages that defend vaccines prioritise the value of the family while the vaccine hesitancy pages are focusing on the value of freedom. Finally, creating embeddings based on the health-related likes on Facebook Pages, we explore common, latent interests of vaccine-hesitant people, showing a strong preference for natural cures. This exploratory analysis aims at exploring the potentials of a social media platform to act as a sensing tool, providing researchers and policymakers with insights drawn from the digital traces, that can help design communication campaigns that build confidence, based on the values that also appeal to the socio-moral criteria of people.	Human Values and Attitudes towards Vaccination in Social Media	NA:NA:NA:NA:NA:NA	2018
Johnny Torres, Carmen Vaca	The role of social networks during natural disasters is becoming crucial to share relevant information and coordinate relief actions. With the reach of the social networks, any user around the world has the possibility of interact in crisis-events as these unfold. A large part of the information posted during a disaster uses the native language where the disaster occurred. However, there are also users from other parts of the world who can comment about the event, often in another language. In this work, we conducted a study of crisis-related tweets about the earthquake that occurred in Ecuador in April 2016. To that end, we introduce a new annotated dataset in both Spanish and English languages with approximately 8K tweets; half of them belong to conversations. We evaluate several neural architectures to identify crisis-related tweets in a multi-lingual setting, and we found that deep contextual multi-lingual embeddings outperform other strong baseline models. We then explore the type of conversations that occur from the perspective of different languages. The results show that certain types of conversations occur more in the native language and others in a foreign language. Conversations from foreign countries seek to gather situation awareness and give emotional support, while in the affected country the conversations aim mainly to humanitarian aid.	Cross-lingual Perspectives about Crisis-Related Conversations on Twitter	NA	2018
Antonio Vazquez Brust:Tomás Olego:Germán Rosati:Carolina Lang:Guillermo Bozzoli:Diego Weinberg:Roberto Chuit:Martin Minnoni:Carlos Sarraute	A map of potential prevalence of Chagas disease (ChD) with high spatial disaggregation is presented. It aims to detect areas outside the Gran Chaco ecoregion (hyperendemic for the ChD), characterized by high affinity with ChD and high health vulnerability. To quantify potential prevalence, we developed several indicators: an Affinity Index which quantifies the degree of linkage between endemic areas of ChD and the rest of the country. We also studied favorable habitability conditions for Triatoma infestans, looking for areas where the predominant materials of floors, roofs and internal ceilings favor the presence of the disease vector. We studied determinants of a more general nature that can be encompassed under the concept of Health Vulnerability Index. These determinants are associated with access to health providers and the socio-economic level of different segments of the population. Finally we constructed a Chagas Potential Prevalence Index (ChPPI) which combines the affinity index, the health vulnerability index, and the population density. We show and discuss the maps obtained. These maps are intended to assist public health specialists, decision makers of public health policies and public officials in the development of cost-effective strategies to improve access to diagnosis and treatment of ChD.	Detecting Areas of Potential High Prevalence of Chagas in Argentina	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
		DL4G-SDE 2019 Chairs' Welcome		2018
Oghenemaro Anuyah:Ion Madrazo Azpiazu:Maria Soledad Pera	To capitalize on the benefits associated with word embeddings, researchers working with data from domains such as medicine, sentiment analysis, or finance, have dedicated efforts to either taking advantage of popular, general-purpose embedding-learning strategies, such as Word2Vec, or developing new ones that explicitly consider domain knowledge in order to generate new domain-specific embeddings. In this manuscript, we instead propose a mixed strategy to generate enriched embeddings specifically designed for the educational domain. We do so by leveraging FastText embeddings pre-trained using Wikipedia, in addition to established educational standards that serve as structured knowledge sources to identify terms, topics, and subjects for each school grade. The results of an initial empirical analysis reveal that the proposed embedding-learning strategy, which infuses limited structured knowledge currently available for education into pre-trained embeddings, can better capture relationships and proximity among education-related terminology. Further, these results demonstrate the advantages of using domain-specific embeddings over general-purpose counterparts for capturing information that pertains to the educational area, along with potential applicability implications when it comes to text processing and analysis for K–12 curriculum-related tasks.	Using structured knowledge and traditional word embeddings to generate concept representations in the educational domain	NA:NA:NA	2018
Robin Brochier:Adrien Guille:Julien Velcin	In this extended abstract, we present an algorithm that learns a similarity measure between documents from the network topology of a structured corpus. We leverage the Scaled Dot-Product Attention, a recently proposed attention mechanism, to design a mutual attention mechanism between pairs of documents. To train its parameters, we use the network links as supervision. We provide preliminary experiment results with a citation dataset on two prediction tasks, demonstrating the capacity of our model to learn a meaningful textual similarity.	Link Prediction with Mutual Attention for Text-Attributed Networks	NA:NA:NA	2018
Xi Liu:Ping-Chun Hsieh:Nick Duffield:Rui Chen:Muhe Xie:Xidao Wen	Recently, considerable research attention has been paid to graph embedding, a popular approach to construct representations of vertices in latent space. Due to the curse of dimensionality and sparsity in graphical datasets, this approach has become indispensable for machine learning tasks over large networks. The majority of the existing literature has considered this technique under the assumption that the network is static. However, networks in many applications, including social networks, collaboration networks, and recommender systems, nodes, and edges accrue to a growing network as streaming. A small number of very recent results have addressed the problem of embedding for dynamic networks. However, they either rely on knowledge of vertex attributes, suffer high-time complexity or need to be re-trained without closed-form expression. Thus the approach of adapting the existing methods designed for static networks or dynamic networks to the streaming environment faces non-trivial technical challenges. These challenges motivate developing new approaches to the problems of streaming graph embedding. In this paper, we propose a new framework that is able to generate latent representations for new vertices with high efficiency and low complexity under specified iteration rounds. We formulate a constrained optimization problem for the modification of the representation resulting from a stream arrival. We show this problem has no closed-form solution and instead develop an online approximation solution. Our solution follows three steps: (1) identify vertices affected by newly arrived ones, (2) generating latent features for new vertices, and (3) updating the latent features of the most affected vertices. The new representations are guaranteed to be feasible in the original constrained optimization problem. Meanwhile, the solution only brings about a small change to existing representations and only slightly changes the value of the objective function. Multi-class classification and clustering on five real-world networks demonstrate that our model can efficiently update vertex representations and simultaneously achieve comparable or even better performance compared with model retraining.	Real-Time Streaming Graph Embedding Through Local Actions	NA:NA:NA:NA:NA:NA	2018
Jinhua Peng:Zongyang Ma:Di Jiang:Hua Wu	In dialogue systems, discourse coherence is an important concept that measures semantic relevance between an utterance and its context. It plays a critical role in determining the inappropriate reply of dialogue systems with regard to a given dialogue context. In this paper, we present a novel framework for evaluating discourse coherence by seamlessly integrating Bayesian and neural networks. The Bayesian network corresponds to Coherence-Pivoted Latent Dirichlet Allocation (cpLDA). cpLDA concentrates on generating the fine-grained topics from dialogue data and takes both local and global semantics into account. The neural network corresponds to Multi-Hierarchical Coherence Network (MHCN). Coupled with cpLDA, MHCN quantifies discourse coherence between an utterance and its context by comprehensively utilizing original texts, topic distribution and topic embedding. Extensive experiments show that the proposed framework yields superior performance comparing with the state-of-the-art methods.	Integrating Bayesian and Neural Networks for Discourse Coherence	NA:NA:NA:NA	2018
Aynaz Taheri:Kevin Gimpel:Tanya Berger-Wolf	Graph representation learning for static graphs is a well studied topic. Recently, a few studies have focused on learning temporal information in addition to the topology of a graph. Most of these studies have relied on learning to represent nodes and substructures in dynamic graphs. However, the representation learning problem for entire graphs in a dynamic context is yet to be addressed. In this paper, we propose an unsupervised representation learning architecture for dynamic graphs, designed to learn both the topological and temporal features of the graphs that evolve over time. The approach consists of a sequence-to-sequence encoder-decoder model embedded with gated graph neural networks (GGNNs) and long short-term memory networks (LSTMs). The GGNN is able to learn the topology of the graph at each time step, while LSTMs are leveraged to propagate the temporal information among the time steps. Moreover, an encoder learns the temporal dynamics of an evolving graph and a decoder reconstructs the dynamics over the same period of time using the encoded representation provided by the encoder. We demonstrate that our approach is capable of learning the representation of a dynamic graph through time by applying the embeddings to dynamic graph classification using a real world dataset of animal behaviour.	Learning to Represent the Evolution of Dynamic Graphs with Recurrent Models	NA:NA:NA	2018
Anton Tsitsulin:Davide Mottin:Panagiotis Karras:Alexander Bronstein:Emmanuel Müller	Sadly, an empty abstract.	Spectral Graph Complexity	NA:NA:NA:NA:NA	2018
Jianyu Wang:Rui Wen:Chunming Wu:Yu Huang:Jian Xion	Online review system enables users to submit reviews about the products. However, the openness of Internet and monetary rewards for crowdsourcing tasks stimulate a large number of fraudulent users to write fake reviews and post advertisements to interfere the rank of apps. Existing methods for detecting spam reviews have been successful but they usually aims at e-commerce (e.g. Amazon, eBay) and recommendation (e.g. Yelp, Dianping) systems. Since the behaviors of fraudulent users are complexity and varying across different review platforms, existing methods are not suitable for fraudster detection in online app review system. To shed light on this question, we are among the first to analyze the intentions of fraudulent users from different review platforms and categorize them by utilizing characteristics of contents (similarity, special symbols) and behaviors (timestamps, device, login status). With a comprehensive analysis of spamming activities and relationships between normal and malicious users, we design and present FdGars, the first graph convolutional network approach for fraudster detection in online app review system. Then we evaluate FdGars on real-world large-scale dataset (with 82,542 nodes and 42,433,134 edges) from Tencent App Store. The result demonstrates that F1-score of FdGars can achieve 0.938+, which outperforms several baselines and state-of-art fraudsters detecting methods. Moreover, we deploy FdGars on Tencent Beacon Anti-Fraud Platform to show its effectiveness and scalability. To the best of our knowledge, this is the first work to use graph convolutional networks for fraudster detection in the large-scale online app review system. It is worth to mention that FdGars can uncover malicious accounts even the data lack of labels in anti-spam tasks.	FdGars: Fraudster Detection via Graph Convolutional Networks in Online App Review System	NA:NA:NA:NA:NA	2018
Dongsheng Wang:Qiuchi Li:Lucas Chaves Lima:Jakob Grue Simonsen:Christina Lioma	When the meaning of a phrase cannot be inferred from the individual meanings of its words (e.g., hot dog), that phrase is said to be non-compositional. Automatic compositionality detection in multi-word phrases is critical in any application of semantic processing, such as search engines [9]; failing to detect non-compositional phrases can hurt system effectiveness notably. Existing research treats phrases as either compositional or non-compositional in a deterministic manner. In this paper, we operationalize the viewpoint that compositionality is contextual rather than deterministic, i.e., that whether a phrase is compositional or non-compositional depends on its context. For example, the phrase “green card” is compositional when referring to a green colored card, whereas it is non-compositional when meaning permanent residence authorization. We address the challenge of detecting this type of contextual compositionality as follows: given a multi-word phrase, we enrich the word embedding representing its semantics with evidence about its global context (terms it often collocates with) as well as its local context (narratives where that phrase is used, which we call usage scenarios). We further extend this representation with information extracted from external knowledge bases. The resulting representation incorporates both localized context and more general usage of the phrase and allows to detect its compositionality in a non-deterministic and contextual way. Empirical evaluation of our model on a dataset of phrase compositionality1, manually collected by crowdsourcing contextual compositionality assessments, shows that our model outperforms state-of-the-art baselines notably on detecting phrase compositionality.	Contextual Compositionality Detection with External Knowledge Bases and Word Embeddings	NA:NA:NA:NA:NA	2018
		First International Workshop on e-Commerce and NLP (ECNLP) Chairs' Welcome		2018
David Carmel	In this talk we consider the domain of voice shopping in Alexa, Amazon’s voice assistant. In this domain, search scenarios are integral part of the shopping sessions, where users seek for a product to buy, or for some information about a product. The fact that in voice search, both the input and output are spoken, involves many challenges in automatic speech recognition, natural language understanding, question answering, and new user experiences. We will elaborate on customers’ behavior in voice shopping, where we have observed an interesting and surprising phenomenon that many customers purchase or engage with irrelevant search results. The term “irrelevance” may mislead, since a relevant item is typically interpreted as “anything that satisfies the user needs”. Thus, the title of this work may look as an oxymoron – the purchase of a product is a strong signal of relevance to the customer. In the context of this work we take a simplified approach. We mark product items as relevant or irrelevant to the user query based on the relevance judgments of several human annotators. However, even in the context of objective relevance judgments, it is still surprising that so many customers engage with irrelevant results. We will analyze this phenomenon and demonstrate its significance. We will offer several hypotheses as to the reasons behind customers’ purchase and engagement with irrelevant results, including customers’ personal preferences, trendiness of the products and their relatedness to the query, the query intent and the product price.	On the Relation Between Products’ Relevance and Customers’ Satisfaction in Voice Shopping	NA	2018
G Mohammed Abdulla:Shreya Singh:Sumit Borar	Size selection is a critical step while purchasing fashion products. Unlike offline, in online fashion shopping, customers don’t have the luxury of trying a product and have to rely on the product images and size charts to select a product that fits well. As a result of this gap, online shopping yields a large percentage of returns due to size and fit. Hence providing size recommendation for customers enhances their buying experience and also reduces operational costs incurred during exchanges and returns. In this paper, we present a robust personalized size recommendation system which predicts the most appropriate size for users based on their order history and product data. We embed both users and products in a size and fit space using skip-gram based Word2Vec model and employ GBM classifier to predict the fit likelihood. We describe the architecture of the system and challenges we encountered while developing it. Further we also analyze the performance of our system through extensive offline and online testing, compare our technique with another state-of-art technique and share our findings.	Shop your Right Size: A System for Recommending Sizes for Fashion products	NA:NA:NA	2018
Georgios Alexandridis:Thanos Tagaris:Giorgos Siolas:Andreas Stafylopatis	Recent theoretical and practical advances have led to the emergence of review-based recommender systems, where user preference data is encoded in at least two dimensions; the traditional rating scores in a predefined discrete scale and the user-generated reviews in the form of free-text. The main contribution of this work is the presentation of a new technique of incorporating those reviews into collaborative filtering matrix factorization algorithms. The text of each review, of arbitrary length, is mapped to a continuous feature space of fixed length, using neural language models and more specifically, the Paragraph Vector model. Subsequently, the resulting feature vectors (the neural embeddings) are used in combination with the rating scores in a hybrid probabilistic matrix factorization algorithm, based on maximum a-posteriori estimation. The proposed methodology is then compared to three other similar approaches on six datasets in order to assess its performance. The obtained results demonstrate that the new formulation outperforms the other systems on a set of two metrics, thereby indicating the robustness of the idea.	From Free-text User Reviews to Product Recommendation using Paragraph Vectors and Matrix Factorization	NA:NA:NA:NA	2018
Omar Alonso:Vasileios Kandylas:Serge-Eric Tremblay	We describe a system that organizes search results in the context of an exploratory product search session where the user is researching goods. Compared to existing approaches that use predefined categories to filter results by attributes, we organize information needs based on queries instead of documents. The idea is to organize queries around the same topic and produce a hierarchical representation of intents that describe information about a product from different perspectives. We present a prototype implementation using a real-world data set of 24M queries.	Exploration of Product Search Intents via Clustering of Query Clusters	NA:NA:NA	2018
Anjan Goswami:Prasant Mohapatra:Chengxiang Zhai	The demand generation and assortment planning are two critical components of running a retail business. Traditionally, retail companies use the historical sales data for modeling and optimization of assortment selection, and they use a marketing strategy for demand generation. However, today, most retail businesses have e-commerce sites with rapidly growing online sales. An e-commerce site typically has to maintain a large amount of digitized product data, and it also keeps a vast amount of historical customer interaction data that includes search, browse, click, purchase and many other different interactions. In this paper, we show how this digitized product data and the historical search logs can be used in understanding and quantifying the gap between the supply and demand side of a retail market. This gap helps in making an effective strategy for both demand generation and assortment selection. We construct topic models of the historical search queries and the digitized product data from the catalog. We use the former to model the customer demand and the later to model the supply side of the retail business. We then create a tool to visualize the topic models to understand the differences between the supply and demand side. We also quantify the supply and demand gap by defining a metric based on Kullback-Leibler (KL) divergence of topic distributions of queries and the products. The quantification helps us identifying the topics related to excess or less demand and thereby in designing effective strategies for demand generation and assortment selection. Application of this work by e-Commerce retailers can result in the development of product innovations that can be utilized to achieve economic equilibrium. We can identify the excess demand and can provide insight to the teams responsible for improving assortment and catalog quality. Similarly, we can also identify excess supply and can provide that intelligence to the teams responsible for demand generation. Tools of this nature can be developed to systematically drive efficiency in achieving better economic gains for the entire e-commerce engine. We conduct several experiments collecting data from Walmart.com to validate the effectiveness of our approach.	Quantifying and Visualizing the Demand and Supply Gap from E-commerce Search Data using Topic Models	NA:NA:NA	2018
Ashish Kulkarni:Kartik Mehta:Shweta Garg:Vidit Bansal:Nikhil Rasiwasia:Srinivasan Sengamedu	Product pages on e-commerce websites often overwhelm their customers with a wealth of data, making discovery of relevant information a challenge. Motivated by this, here, we present a novel framework to answer both factoid and non-factoid user questions on product pages. We propose several question-answer matching models leveraging both deep learned distributional semantics and semantics imposed by a structured resource like a domain specific ontology. The proposed framework supports the use of a combination of these models and we show, through empirical evaluation, that a cascade of these models does much better in meeting the high precision requirements of such a question-answering system. Evaluation on user asked questions shows that the proposed system achieves 66% higher precision1 as compared to IDF-weighted average of word vectors baseline [1].	ProductQnA: Answering User Questions on E-Commerce Product Pages	NA:NA:NA:NA:NA:NA	2018
Yuan Lyu:Daichi Amagata:Takuya Maekawa:Takahiro Hara:Hao Niu:Kei Yonekawa:Mori Kurokawa	With the recent proliferation of e-commerce services, online shopping has become more and more popular among customers. Because it is necessary to recommend proper items to customers, to improve the accuracy of recommendation, high-performance recommender systems are required. However, current recommender systems are mainly based on information of their own domain, resulting in low accurate recommendation for customers with limited purchasing histories. The accuracy may suffer due to a lack of information. In order to use information from other domains, it is necessary to associate behaviors in different domains of the behaviorally related users. This paper presents a preliminary analysis of matching behaviors of the behaviorally related users in different domains. The result shows that we got a better prediction rate than linear regression.	Behavior Matching between Different Domains based on Canonical Correlation Analysis	NA:NA:NA:NA:NA:NA:NA	2018
Alessandro Magnani:Feng Liu:Min Xie:Somnath Banerjee	For an E-commerce website like Walmart.com, search is one of the most critical channel for engaging customer. Most existing works on search are composed of two steps, a retrieval step which obtains the candidate set of matching items, and a re-rank step which focuses on fine-tuning the ranking of candidate items. Inspired by latest works in the domain of neural information retrieval (NIR), we discuss in this work our exploration of various product retrieval models which are trained on search log data. We discuss a set of lessons learned in our empirical result section, and these results can be applied to any product search engine which aims at learning a good product retrieval model based on search log data.	Neural Product Retrieval at Walmart.com	NA:NA:NA:NA	2018
Dina Pisarevskaya:Boris Galitsky:Jay Taylor:Andrey Ozerov	Automated detection of text with misrepresentations such as fake reviews is an important task for online reputation management. The dataset of customer complaints - emotionally charged texts which are very similar to reviews and include descriptions of problems customers experienced with certain businesses – is presented. It contains 2746 complaints about banks and provides clear ground truth, based on available factual knowledge about the financial domain. Among them, 400 texts were manually tagged. Initial experiments were performed in order to explore the links between implicit cues of the rhetoric structure of texts and the validity of arguments, and also how truthful/deceptive are these texts.	An Anatomy of a Lie:	NA:NA:NA:NA	2018
Anna Primpeli:Ralph Peeters:Christian Bizer	A current research question in the area of entity resolution (also called link discovery or duplicate detection) is whether and in which cases embeddings and deep neural network based matching methods outperform traditional symbolic matching methods. The problem with answering this question is that deep learning based matchers need large amounts of training data. The entity resolution benchmark datasets that are currently available to the public are too small to properly evaluate this new family of matching methods. The WDC Training Dataset for Large-Scale Product Matching fills this gap. The English language subset of the training dataset consists of 20 million pairs of offers referring to the same products. The offers were extracted from 43 thousand e-shops which provide schema.org annotations including some form of product ID such as a GTIN or MPN. We also created a gold standard by manually verifying 2200 pairs of offers belonging to four product categories. Using a subset of our training dataset together with this gold standard, we are able to publicly replicate the recent result of Mudgal et al. that embeddings and deep neural network based matching methods outperform traditional symbolic matching methods on less structured data.	The WDC Training Dataset and Gold Standard for Large-Scale Product Matching	NA:NA:NA	2018
Daniel Stein:Dimitar Shterionov:Andy Way	The quality of e-Commerce services largely depends on the accessibility of product content as well as its completeness and correctness. Nowadays, many sellers target cross-country and cross-lingual markets via active or passive cross-border trade, fostering the desire for seamless user experiences. While machine translation (MT) is very helpful for crossing language barriers, automatically matching existing items for sale (e.g. the smartphone in front of me) to the same product (all smartphones of the same brand/type/colour/condition) can be challenging, especially because the seller’s description can often be erroneous or incomplete. This task we refer to as item alignment in multilingual e-commerce catalogues. To facilitate this task, we develop a pipeline of tools for item classification based on cross-lingual text similarity, exploiting recurrent neural networks (RNNs) with and without pre-trained word-embeddings. Furthermore, we combine our language agnostic RNN classifiers with an in-domain MT system to further reduce the linguistic and stylistic differences between the investigated data, aiming to boost our performance. The quality of the methods as well as their training speed is compared on an in-domain data set for English–German products.	Towards language-agnostic alignment of product titles and descriptions: a neural approach	NA:NA:NA	2018
Artem Vovk:Dmitrii Tochilkin:Pradyumna Narayana:Kazoo Sone:Sugato Basu	Analyzing commercial pages to infer the products or services being offered by a web-based business is a task central to product search, product recommendation, ad placement and other e-commerce tasks. What makes this task challenging is that there are two types of e-commerce product pages. One is the single-product (SP) page where one product is featured primarily and users are able to buy that product or add to cart on the page. The other is the multi-product (MP) page, where users are presented with multiple (often 10-100) choices of products within a same category, often with thumbnail pictures and brief descriptions — users browse through the catalogue until they find a product they want to learn more about, and subsequently purchase the product of their choice on a corresponding SP page. In this paper, we take a two-step approach to identifying product phrases from commercial pages. First we classify whether a commercial web page is a SP or MP page. To that end, we introduce two different image recognition based models to differentiate between these two types of pages. If the page is determined to be SP, we identify the main product featured in that page. We compare the two types of image recognition models in terms of trade-offs between accuracy and latency, and empirically demonstrate the efficacy of our overall approach.	Product Phrase Extraction from e-Commerce Pages	NA:NA:NA:NA:NA	2018
Hanxin Wang:Daichi Amagata:Takuya Maekawa:Takahiro Hara:Hao Niu:Kei Yonekawa:Mori Kurokawa	Many current applications use recommender systems to predict user preferences, aiming at improving user experience and increasing the amount of sales and the usage time that users spent on the application. However, it is not an easy task to recommend items to new users accurately because of the user cold-start problem, which means that recommendation performance will degrade on users with little interaction, particularly for latent users who have never used the service before. In this work, we combine an online shopping domain with information from an ad platform, and then apply deep learning to build a cross-domain recommender system based on shared users of these two domains, to alleviate the user cold-start problem. Experimental results show the effectiveness of our deep cross-domain recommender system on handling user cold-start problem. By our framework, it is possible to recommend products to users of other domain through ad distribution in a more accurate level, and to increase sales amount of online shopping.	Preliminary Investigation of Alleviating User Cold-Start Problem in E-commerce with Deep Cross-Domain Recommender System	NA:NA:NA:NA:NA:NA:NA	2018
Chu Wang:Lei Tang:Shujun Bian:Da Zhang:Zuohua Zhang:Yongning Wu	For a product of interest, we propose a search method to surface a set of reference products. The reference products can be used as candidates to support downstream modeling tasks and business applications. The search method consists of product representation learning and fingerprint-type vector searching. The product catalog information is transformed into a high-quality embedding of low dimensions via a novel attention auto-encoder neural network, and the embedding is further coupled with a binary encoding vector for fast retrieval. We conduct extensive experiments to evaluate the proposed method, and compare it with peer services to demonstrate its advantage in terms of search return rate and precision.	Reference Product Search	NA:NA:NA:NA:NA:NA	2018
Chu Wang:Lei Tang:Yang Lu:Shujun Bian:Hirohisa Fujita:Da Zhang:Zuohua Zhang:Yongning Wu	ProductNet is a collection of high-quality product datasets for better product understanding. Motivated by ImageNet, ProductNet aims at supporting product representation learning by curating product datasets of high quality with properly chosen taxonomy. In this paper, the two goals of building high-quality product datasets and learning product representation support each other in an iterative fashion: the product embedding is obtained via a multi-modal deep neural network (master model) designed to leverage product image and catalog information; and in return, the embedding is utilized via active learning (local model) to vastly accelerate the annotation process. For the labeled data, the proposed master model yields high categorization accuracy (94.7% top-1 accuracy for 1240 classes), which can be used as search indices, partition keys, and input features for machine learning models. The product embedding, as well as the fined-tuned master model for a specific business task, can also be used for various transfer learning tasks.	ProductNet: a Collection of High-Quality Datasets for Product Representation Learning	NA:NA:NA:NA:NA:NA:NA:NA	2018
		Emoji2019 Workshop Chairs' Welcome		2018
Qiaozhu Mei	Emojis have quickly become a universal language that is used by worldwide users, for everyday tasks, across language barriers, and in different apps and platforms. The prevalence of emojis has quickly attracted great attentions from various research communities such as natural language processing, Web mining, ubiquitous computing, and human-computer interaction, as well as other disciplines including social science, arts, psychology, and linguistics. This talk summarizes the recent efforts made by my research group and our collaborators on analyzing large-scale emoji data. The usage of emojis by worldwide users presents interesting commonality as well as divergence. In our analysis of emoji usage by millions of smartphone users in 212 countries, we show that the different preferences and usage of emojis provide rich signals for understanding the cultural differences of Internet users, which correlate with the Hofstede’s cultural dimensions [4]. Emojis play different roles when used alongside text. Through jointly learning the embeddings and topological structures of words and emojis, we reveal that emojis present both complementary and supplementary relations to words. Based on the structural properties of emojis in the semantic spaces, we are able to untangle several factors behind the popularity of emojis [1]. This talk also highlights the utility of emojis. In general, emojis have been used by Internet users as text supplements to describe objects and situations, express sentiments, or express humor and sarcasm; they are also used as communication tools to attract attention, adjust tones, or establish personal relationships. The benefit of using emojis goes beyond these intentions. In particular, we show that including emojis in the description of an issue report on GitHub results in the issue being responded to by more users and resolved sooner. Large-scale emoji data can also be utilized by AI systems to improve the quality of Web mining services. In particular, a smart machine learning system can infer the latent topics, sentiments, and even demographic information of users based on how they use emojis online. Our analysis reveals a considerable difference between female and male users of emojis, which is big enough for a machine learning algorithm to accurately predict the gender of a user. In Web services that are customized for gender groups, gender inference models built upon emojis can complement those based on text or behavioral traces with fewer privacy concerns [2]. Emojis can be also used as an instrument to bridge Web mining tasks across language barriers, especially to transfer sentiment knowledge from a language with rich training labels (e.g., English) to languages that have been difficult for advanced natural language processing tasks [3]. Through this bridge, developers of AI systems and Web services are able to reduce the inequality in the quality of services received by the international users that has been caused by the imbalance of available human annotations in different languages. In general, emojis have evolved from visual ideograms to a brand-new world language in the era of AI and a new Web. The popularity, roles, and utility of emojis have all gone beyond people’s original intentions, which have created a huge opportunity for future research that calls for joint efforts from multiple disciplines.	Decoding the New World Language: Analyzing the Popularity, Roles, and Utility of Emojis	NA	2018
Wesley Brants:Bonita Sharif:Alexander Serebrenik	Emojis are increasingly being used in today’s social communication - both formally in team messaging systems as well as informally via text messages on phones. Besides being used in social communication, emojis might also be a suitable mechanism for emotion (self-)assessment. Indeed, emojis can be expected to be familiar to people of different social groups and do not depend on the mastery of a specific language. However, emojis could be interpreted very differently from their actual intent. In order to determine whether people interpret emojis (specific to emotional states) in a consistent manner, we conducted an online survey on nine emojis with 386 people. The results show that the emojis representing anger, sadness, joy, surprise, and neutral state are interpreted as they were intended, independent of age and gender. Interpretations of other emojis such as Unamused Face and Face Screaming in Fear depend on age, and thus are not as useful for probing for emotion in a study setting unless all participants belong to the same age category. The Face with Rolling Eyes emoji is interpreted differently by gender and finally the Nauseated Face emoji resulted in no conclusive interpretation.	Assessing the Meaning of Emojis for Emotional Awareness - A Pilot Study	NA:NA:NA	2018
Jennifer Daniel	Currently, to support gender inclusive codepoints (for example, gender inclusive can be defined as male/female to an equal degree, can neither be confidently identified as male/female, etc.) all major platforms default to a male or a female design. So, if someone were to send a text to their friend from a Microsoft device, “Love a good mansplain 1F925” their friend, if reading from an iPhone will see, “Love a good mansplain 1F926” even though both of these emojis map to U+1F926. This creates all kinds of cross platform inconsistencies and in some cases reinforces stereotypes.1	When a Merperson is a Merman	NA	2018
Jing Ge	Focusing on a Chinese social media platform, this study adopts computer-mediated discourse analysis to examine how users employ emoji sequences to construct their personal identity through the expression of stance and engagement. Seven types of linguistic elements were identified by conducting stance and engagement analysis on emoji sequences in posts by social media influencers. Stance was more frequent than engagement. Attitude markers were the most common element used to convey stance, whereas directive was the most prevalent element used to express engagement. In addition, emoji sequences that did not convey stance and engagement were coded as n/a. This study also observed creative usages in the composition of emoji sequences that compensate for the lack of a prescribed emoji sequence grammar. Based on these findings, it advances recommendations for the design of emoji and of social media platforms grounded in linguistic principles.	Emoji Sequence Use in Enacting Personal Identity	NA	2018
Anurag Illendula:Amit Sheth	Most NLP and Computer Vision tasks are limited to scarcity of labelled data. In social media emotion classification and other related tasks, hashtags have been used as indicators to label data. With the rapid increase in emoji usage of social media, emojis are used as an additional feature for major social NLP tasks. However, this is less explored in case of multimedia posts on social media where posts are composed of both image and text. At the same time, w.e have seen a surge in the interest to incorporate domain knowledge to improve machine understanding of text. In this paper, we investigate whether domain knowledge for emoji can improve the accuracy of emotion classification task. We exploit the importance of different modalities from social media post for emotion classification task using state-of-the-art deep learning architectures. Our experiments demonstrate that the three modalities (text, emoji and images) encode different information to express emotion and therefore can complement each other. Our results also demonstrate that emoji sense depends on the textual context, and emoji combined with text encodes better information than considered separately. The highest accuracy of 71.98% is achieved with a training data of 550k posts.	Multimodal Emotion Classification	NA:NA	2018
Sujay Khandekar:Joseph Higg:Yuanzhe Bian:Chae Won Ryu:Jerry O. Talton Iii:Ranjitha Kumar	In the last two decades, Emoji have become a mainstay of digital communication, allowing ordinary people to convey ideas, concepts, and emotions with just a few Unicode characters. While emoji are most often used to supplement text in digital communication, they comprise a powerful and expressive vocabulary in their own right. In this paper, we study the affordances of “emoji-first” communication, in which sequences of emoji are used to describe concepts without textual accompaniment. To investigate the properties of emoji-first communication, we built and released Opico, a social media mobile app that allows users to create reactions — sequences of between one and five emoji — and share them with a network of friends. We then leveraged Opico to collect a repository of more than 3700 emoji reactions from more than 1000 registered users, each tied to one of 2441 physical places. We describe the design and architecture of the Opico app, present a qualitative and quantitative analysis of Opico’s reaction dataset, and discuss the implications of Emoji-first communication for future social platforms.	Opico: A Study of Emoji-first Communication in a Mobile Social App	NA:NA:NA:NA:NA:NA	2018
Jennifer 8. Lee:Jason Li:An Xiao Mina	Chinese characters predate the introduction of digital emoji by approximately 3,000 years. Despite the temporal gap, there are striking parallels between the canonical set of Chinese radicals and the set of emoji that have currently been approved by the Unicode Consortium. Comparing the 214 Kangxi Chinese radicals with the 3,019 emojis in the Unicode 12.0 set can reveal semantic gaps and provide directions for new emoji. Our analysis found that 72.4% of radicals have reasonable emoji equivalents, while only 17.8% of radicals lack any emoji equivalent that we could determine.	Hanmoji: What Chinese Characters and Emoji Reveal About Each Other	NA:NA:NA	2018
Mingyang Li:Sharath Guntuku:Vinit Jakhetiya:Lyle Ungar	Emojis have gained widespread acceptance, globally and cross-culturally. However, Emoji use may also be nuanced due to differences across cultures, which can play a significant role in shaping emotional life. In this paper, we a) present a methodology to learn latent emotional components of Emojis, b) compare Emoji-Emotion associations across cultures, and c) discuss how they may reflect emotion expression in these platforms. Specifically, we learn vector space embeddings with more than 100 million posts from China (Sina Weibo) and the United States (Twitter), quantify the association of Emojis with 8 basic emotions, demonstrate correlation between visual cues and emotional valence, and discuss pairwise similarities between emotions. Our proposed Emoji-Emotion visualization pipeline for uncovering latent emotional components can potentially be used for downstream applications such as sentiment analysis and personalized text recommendations.	Exploring (Dis-)Similarities in Emoji-Emotion Association on Twitter and Weibo	NA:NA:NA:NA	2018
Chaya Liebeskind:Shmuel Liebeskind	In this study, we aim to predict the most likely emoji given only a short text as an input. We extract a Hebrew political dataset of user comments for emoji prediction. Then, we investigate highly sparse n-grams representations as well as denser character n-grams representations for emoji classification. Since the comments in social media are usually short, we also investigate four dimension reduction methods, which associates similar words to similar vectorial representation. We demonstrate that the common Word Embedding dimension reduction method is not optimal. We also show that the character n-grams representations outperform all the other representation for the task of emoji prediction for Hebrew political domain.	Emoji Prediction for Hebrew Political Domain	NA:NA	2018
Benjamin Weissman	This paper discusses two multi-experiment studies using the ERP methodology to investigate neural correlates of processing linguistic emojis. The first study examined the use of wink emojis used to mark irony and found the same ERP response complex that has been found in response to word-generated irony. Contingent upon individual differences in interpretation, these emojis are processed the same way as ironic words. The second study investigated the prediction of non-face emojis substituted for nouns. When predictability was high, unexpected emojis elicited the same ERP response patterns as words. Overall, the results of these two studies suggest that emojis used linguistically are processed in the same way as words and that individuals can integrate input from multiple modalities into a holistic representation of a single utterance.	Emojis in Sentence Processing: An Electrophysiological Approach	NA	2018
		FATES on the WEB Chairs' Welcome		2018
Frauke Kreuter	The sharing of data is at the core of many technology companies. Data sharing is also increasingly important for government decision-making, as stated by the Commission on Evidenced-Based Policymaking, which led to the Foundations for Evidence-Based Policymaking Act. However, in many instances, data used for decision-making is generated by people, and needs to be explicitly shared by the data subject with those wanting to use the data. The decision-making process behind sharing (private) information needs to be understood to assess (and circumvent) potential biases in the resulting data. When assessing bias in algorithmic decision-making, awareness of biases in the training data is essential. This presentation will review social science theories behind data sharing decision-making, highlight a series of experimental studies designed to affect sharing decisions, and present a framework design to detect sources of bias in various data sources.	The Social Science of Privacy - Effects on Industry and Government Data Use	NA	2018
Giorgio Barnabò:Adriano Fazzone:Stefano Leonardi:Chris Schwiegelshohn	As freelancing work keeps on growing almost everywhere due to a sharp decrease in communication costs and to the widespread of Internet-based labour marketplaces (e.g., guru.com, feelancer.com, mturk.com, upwork.com), many researchers and practitioners have started exploring the benefits of outsourcing and crowdsourcing  [13, 14, 16, 23, 25, 29]. Since employers often use these platforms to find a group of workers to complete a specific task, researchers have focused their efforts on the study of team formation and matching algorithms and on the design of effective incentive schemes  [2, 3, 4, 17]. Nevertheless, just recently, several concerns have been raised on possibly unfair biases introduced through the algorithms used to carry out these selection and matching procedures. For this reason, researchers have started studying the fairness of algorithms related to these online marketplaces  [8, 19], looking for intelligent ways to overcome the algorithmic bias that frequently arises. Broadly speaking, the aim is to guarantee that, for example, the process of hiring workers through the use of machine learning and algorithmic data analysis tools does not discriminate, even unintentionally, on grounds of nationality or gender. In this short paper, we define the Fair Team Formation problem in the following way: given an online labour marketplace where each worker possesses one or more skills, and where all workers are divided into two or more not overlapping classes (for examples, men and women), we want to design an algorithm that is able to find a team with all the skills needed to complete a given task, and that has the same number of people from all classes. We provide inapproximability results for the Fair Team Formation problem together with four algorithms for the problem itself. We also tested the effectiveness of our algorithmic solutions by performing experiments using real data from an online labor marketplace.	Algorithms for Fair Team Formation in Online Labour Marketplaces✱	NA:NA:NA:NA	2018
Daniel Borkan:Lucas Dixon:Jeffrey Sorensen:Nithum Thain:Lucy Vasserman	Unintended bias in Machine Learning can manifest as systemic differences in performance for different demographic groups, potentially compounding existing challenges to fairness in society at large. In this paper, we introduce a suite of threshold-agnostic metrics that provide a nuanced view of this unintended bias, by considering the various ways that a classifier’s score distribution can vary across designated groups. We also introduce a large new test set of online comments with crowd-sourced annotations for identity references. We use this to show how our metrics can be used to find new and potentially subtle unintended bias in existing public models.	Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification	NA:NA:NA:NA:NA	2018
Josep Domingo-Ferrer:Cristina Pérez-Solà:Alberto Blanco-Justicia	An ever increasing number of decisions affecting our lives are made by algorithms. For this reason, algorithmic transparency is becoming a pressing need: automated decisions should be explainable and unbiased. A straightforward solution is to make the decision algorithms open-source, so that everyone can verify them and reproduce their outcome. However, in many situations, the source code or the training data of algorithms cannot be published for industrial or intellectual property reasons, as they are the result of long and costly experience (e.g. this is typically the case in banking or insurance). We present an approach whereby individual subjects on whom automated decisions are made can elicit in a collaborative and privacy-preserving manner a rule-based approximation of the model underlying the decision algorithm, based on limited interaction with the algorithm or even only on how they have been classified. Furthermore, being rule-based, the approximation thus obtained can be used to detect potential discrimination. We present empirical work to demonstrate the practicality of our ideas.	Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation	NA:NA:NA	2018
Javier D. Fernández:Fajar J. Ekaputra:Peb Ruswono Aryan:Amr Azzam:Elmar Kiesling	The European General Data Protection Regulation (GDPR) brings new challenges for companies, who must demonstrate that their systems and business processes comply with usage constraints specified by data subjects. However, due to the lack of standards, tools, and best practices, many organizations struggle to adapt their infrastructure and processes to ensure and demonstrate that all data processing is in compliance with users’ given consent. The SPECIAL EU H2020 project has developed vocabularies that can formally describe data subjects’ given consent as well as methods that use this description to automatically determine whether processing of the data according to a given policy is compliant with the given consent. Whereas this makes it possible to determine whether processing was compliant or not, integration of the approach into existing line of business applications and ex-ante compliance checking remains an open challenge. In this short paper, we demonstrate how the SPECIAL consent and compliance framework can be integrated into Linked Widgets, a mashup platform, in order to support privacy-aware ad-hoc integration of personal data. The resulting environment makes it possible to create data integration and processing workflows out of components that inherently respect usage policies of the data that is being processed and are able to demonstrate compliance. We provide an overview of the necessary meta data and orchestration towards a privacy-aware linked data mashup platform that automatically respects subjects’ given consents. The evaluation results show the potential of our approach for ex-ante usage policy compliance checking within the Linked Widgets Platforms and beyond.	Privacy-aware Linked Widgets	NA:NA:NA:NA:NA	2018
Spyros Giotakis:Nikos Pelekis	Nowadays, human trajectories are enriched with semantic information having multiple aspects, such as by using background geographic information, by user-provided data via location-based social media, as well as by data coming from various kind of sensing devices. This new type of multiple aspects representation of personal movements as sequences of places visited by a person during his/her movement poses even greater privacy violation threats. This paper provides the blueprint of a semantic-aware Moving Object Database (MOD) engine for privacy-aware sharing of such enriched mobility data and introduces an attack prevention mechanism where all potential privacy breaches that may occur when answering a query, are prevented through an auditing methodology. Towards enhancing the user-friendliness of our approach, we propose a mechanism whose objective is to modify the user queries that cannot be answered due to possible privacy violation, to ‘similar’ queries that can be answered without exposing sensitive information.	On Preserving Sensitive Information of Multiple Aspect Trajectories In-House	NA:NA	2018
Jeanna Matthews:Matt Goerzen	In this paper, we analogize the practice of trolling to the practice of hacking. Just as hacking often involves the discovery and exploitation of vulnerabilities in a computer security landscape, trolling frequently involves the discovery and exploitation of vulnerabilities in a media or attention landscape to amplify messages and direct attention. Also like with hacking, we consider the possibility for a range of trolling personas: from black hat trolls who push an agenda that is clearly counter to the interests of the target, to gray hat trolls who exploit vulnerabilities to draw critical attention to unaddressed issues, and white hat trolls who could help proactively disclose vulnerabilities so that attack surface can be reduced. We discuss a variety of trolling techniques from dogpiling to sockpuppetry and also a range of possible interventions.	Black Hat Trolling, White Hat Trolling, and Hacking the Attention Landscape	NA:NA	2018
Taha Hassan	The prevalence of misinformation on online social media has tangible empirical connections to increasing political polarization and partisan antipathy in the United States. Ranking algorithms for social recommendation often encode broad assumptions about network structure (like homophily) and group cognition (like, social action is largely imitative). Assumptions like these can be naïve and exclusionary in the era of fake news and ideological uniformity towards the political poles. We examine these assumptions with aid from the user-centric framework of trustworthiness in social recommendation. The constituent dimensions of trustworthiness (diversity, transparency, explainability, disruption) highlight new opportunities for discouraging dogmatization and building decision-aware, transparent news recommender systems.	Trust and Trustworthiness in Social Recommender Systems	NA	2018
Lanna Lima:Vasco Furtado:Elizabeth Furtado:Virgilio Almeida	Voice-based assistants are becoming increasingly widespread all over the world. However, the performance of these assistants in the interaction with users of languages and accents of developing countries is not clear yet. Eventual bias against specific language or accent of different groups of people in developing countries is maybe a factor to increase the digital gap in these countries. Our research aims at analysing the presence of bias in the interaction via audio. We carried out experiments to verify the quality of the recognition of phrases spoken by different groups of people. We evaluated the behaviour of Google Assistant and Siri for groups of people formed according to gender and regions that have different accents. Preliminary results indicate that accent and mispronunciation due to regional differences are not being properly considered by the assistants we have analyzed.	Empirical Analysis of Bias in Voice-based Personal Assistants	NA:NA:NA:NA	2018
Drew Roselli:Jeanna Matthews:Nisha Talagala	Recent awareness of the impacts of bias in AI algorithms raises the risk for companies to deploy such algorithms, especially because the algorithms may not be explainable in the same way that non-AI algorithms are. Even with careful review of the algorithms and data sets, it may not be possible to delete all unwanted bias, particularly because AI systems learn from historical data, which encodes historical biases. In this paper, we propose a set of processes that companies can use to mitigate and manage three general classes of bias: those related to mapping the business intent into the AI implementation, those that arise due to the distribution of samples used for training, and those that are present in individual input samples. While there may be no simple or complete solution to this issue, best practices can be used to reduce the effects of bias on algorithmic outcomes.	Managing Bias in AI	NA:NA:NA	2018
Eric Ferreira Dos Santos:Danilo Carvalho:Livia Ruback:Jonice Oliveira	As the Online Social Networks (OSNs) presence continues to grow as a form of mass communication, tensions regarding their usage and perception by different social groups are reaching a turning point. The number of messages that are exchanged between users in these environments are vast and brought a trust problem, where it is difficult to know if the information is from a real person and if what was said is true. Automated users (bots) are part of this issue, as they may be used to spread false and/or harmful messages through an OSN while pretending to be a person. New attempts to automatically identify bots are in constant development, but so are the mechanisms to elude detection. We believe that teaching the user to identify a bot message is an important step in maintaining the credibility of content on social media. In this study, we developed an analysis tool, based on media literacy considerations, that helps the ordinary user to recognize a bot message using only textual features. Instead of simply classifying a user as a bot or human, this tool presents an interpretable reasoning path that helps to educate the user into recognizing suspicious activity. Experimental evaluation is conducted to test the tool’s primary effectiveness (classification) and results are presented. The secondary effectiveness (interpretability) is discussed in qualitative terms.	Uncovering Social Media Bots: a Transparency-focused Approach	NA:NA:NA:NA	2018
Piotr Sapiezynski:Wesley Zeng:Ronald E Robertson:Alan Mislove:Christo Wilson	In this work, we introduce a novel metric for auditing group fairness in ranked lists. Our approach offers two benefits compared to the state of the art. First, we offer a blueprint for modeling of user attention. Rather than assuming a logarithmic loss in importance as a function of the rank, we can account for varying user behaviors through parametrization. For example, we expect a user to see more items during a viewing of a social media feed than when they inspect the results list of a single web search query. Second, we allow non-binary protected attributes to enable investigating inherently continuous attributes (e.g., political alignment on the liberal to conservative spectrum) as well as to facilitate measurements across aggregated sets of search results, rather than separately for each result list. By combining these two elements into our metric, we are able to better address the human factors inherent in this problem. We measure the whole sociotechnical system, consisting of a ranking algorithm and individuals using it, instead of exclusively focusing on the ranking algorithm. Finally, we use our metric to perform three simulated fairness audits. We show that determining fairness of a ranked output necessitates knowledge (or a model) of the end-users of the particular service. Depending on their attention distribution function, a fixed ranking of results can appear biased both in favor and against a protected group1.	Quantifying the Impact of User Attentionon Fair Group Representation in Ranked Lists	NA:NA:NA:NA:NA	2018
David Sarne:Jonathan Schler:Alon Singer:Ayelet Sela:Ittai Bar Siman Tov	This paper suggests the use of automatic topic modeling for large-scale corpora of privacy policies using unsupervised learning techniques. The advantages of using unsupervised learning for this task are numerous. The primary advantages include the ability to analyze any new corpus with a fraction of the effort required by supervised learning, the ability to study changes in topics of interest along time, and the ability to identify finer-grained topics of interest in these privacy policies. Based on general principles of document analysis we synthesize a cohesive framework for privacy policy topic modeling and apply it over a corpus of 4,982 privacy policies of mobile applications crawled from the Google Play Store. The results demonstrate that even with this relatively moderate-size corpus quite comprehensive insights can be attained regarding the focus and scope of current privacy policy documents. The topics extracted, their structure and the applicability of the unsupervised approach for that matter are validated through an extensive comparison to similar findings reported in prior work that uses supervised learning (which heavily depends on manual annotation of experts). The comparison suggests a substantial overlap between the topics found and those reported in prior work, and also unveils some new topics of interest.	Unsupervised Topic Extraction from Privacy Policies	NA:NA:NA:NA:NA	2018
Ana-Andreea Stoica:Augustin Chaintreau	Algorithms for social influence maximization have been extensively studied for the purpose of strategically choosing an initial set of individuals in a social network from which information gets propagated. With many applications in advertisement, news spread, vaccination, and online trend-setting, this problem is a central one in understanding how information flows in a network of individuals. As human networks may encode historical biases, algorithms performing on them might capture and reproduce such biases when automating outcomes. In this work, we study the social influence maximization problem for the purpose of designing fair algorithms for diffusion, aiming to understand the effect of communities in the creation of disparate impact among network participants based on demographic attributes (gender, race etc). We propose a set of definitions and models for assessing the fairness-utility tradeoff in designing algorithms that maximize influence through a mathematical model of diffusion and an empirical analysis of a collected dataset from Instagram. Our work shows that being feature-aware can lead to more diverse outcomes in outreach and seed selection, as well as better efficiency, than being feature-blind.	Fairness in Social Influence Maximization	NA:NA	2018
Ana-Andreea Stoica:Augustin Chaintreau	As today’s media landscape is carved by social media endorsements and built on automated recommendations, both of these are often criticized for inducing vicious dynamics, such as the filter bubble effect, echo chamber, or polarization. We introduce a new model featuring a mild version of homophily and two well-known popularity dynamics. These broadly reproduce the organic activity and the algorithmic filtering, respectively, of which the latter is now commonplace within social media or other online services. Surprisingly, we show this is all that is needed to create hegemony: a single viewpoint (or side) not only receives undue attention, but it also captures all the attention given to “top trending” items.	Hegemony in Social Media and the effect of recommendations	NA:NA	2018
Bianca Teixeira:Daniel Schwabe:Flavia Santoro:Fernanda Baião:Maria Luiza Campos:Leticia Verona:Carlos Laufer:Simone Barbosa:Sérgio Lifschitz:Rosa Costa	The Fourth Industrial Revolution (4IR) is characterized by a fusion of technologies, which is blurring the lines between the physical, digital, and biological spheres. In this context, two fundamental characteristics emerge: transparency and privacy. From one side, transparency can be seen as the quality that allows participants of a community to know which particular processes are being applied, by which agents, and on which data items. It is generally regarded as a means to enable checks and balances within this community, so as to provide a basis for trust among its participants. Privacy, on the other side, essentially refers to the right of an individual to control how information about her/him is used by others. The issue of public transparency versus individual privacy has long been discussed, and within already existing 4IR scenarios, it became clear that the free flow of information fostered by transparency efforts poses serious conflicting issues to privacy assurance. In order to deal with the myriad of often conflicting cross-cutting concerns, Internet applications and systems must incorporate adequate mechanisms to ensure compliance of both ethical and legal principles. In this paper, we use the OurPrivacy Framework as a conceptual framework to precisely characterize where in the design process the decisions must be made to handle both transparency and privacy concerns.	Privacy and Transparency within the 4IR: Two faces of the same coin	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Depeng Xu:Shuhan Yuan:Xintao Wu	Machine learning algorithms are used to make decisions in various applications. These algorithms rely on large amounts of sensitive individual information to work properly. Hence, there are sociological concerns about machine learning algorithms on matters like privacy and fairness. Currently, many studies focus on only protecting individual privacy or ensuring fairness of algorithms. However, how to meet both privacy and fairness requirements simultaneously in machine learning algorithms is under exploited. In this paper, we focus on one classic machine learning model, logistic regression, and develop differentially private and fair logistic regression models by combining functional mechanism and decision boundary fairness in a joint form. Theoretical analysis and empirical evaluations demonstrate our approaches effectively achieve both differential privacy and fairness while preserving good utility.	Achieving Differential Privacy and Fairness in Logistic Regression	NA:NA:NA	2018
		Augmenting Intelligence with Bias-aware Humans-in-the-loop (HumBL2019)		2018
Jon Chamberlain	The online game-with-a-purpose Phrase Detectives (https://www.phrasedetectives.com) [1] has been collecting decisions about anaphoric coreference in human language for over 10 years (4 million judgements from 40,000 players). The game was originally designed to collect multiple valid solutions for a single task, which complicated aggregation but created a very rich (and noisy) dataset [2]. Analysis of the ambiguous player decisions highlights the need for understanding and resolving disagreement that is inherent in language interpretation. This talk will present some of the interesting cases of ambiguity found by the players of Phrase Detectives (a dataset that will be made available to the research community later this year [3]) and discuss the statistical methods we have been working on to harness crowds that disagree with each other [4, 5].	Are Two Heads Better Than One? An Exploration of Ambiguity in Crowd-Collected Language Decisions from the Phrase Detectives Game.	NA	2018
Alfredo Alba:Chad DeLuca:Anna Lisa Gentile:Daniel Gruhl:Linda Kato:Chris Kau:Petar Ristoski:Steve Welch	Many real world analytics problems examine multiple entities or classes that may appear in a corpus. For example, in a customer satisfaction survey analysis there are over 60 categories of (somewhat overlapping) concerns. Each of these is backed by a lexicon of terminology associated with the concern (e.g., “Easy, user friendly process” or ”Process confusing, too many handoffs”). These categories need to be expanded by a subject matter expert as the terminology is not always straight forward (e.g., “handoffs” may also include “ping-pong” and “hot potato” as relevant terms). But given that Subject Matter Expert time is costly, which of the 60+ lexicons should we expand first? We propose a metric for evaluating an existing set of lexicons and providing guidance on which are likely to benefit most from human-in-the-loop expansion. Using our ranking results we achieved ≈ 4 × improvement in impact when expanding the first few lexicons off our suggested list as compared to a random selection.	Identifying High Value Opportunities for Human in the Loop Lexicon Expansion	NA:NA:NA:NA:NA:NA:NA:NA	2018
Alfredo Alba:Chad DeLuca:Anna Lisa Gentile:Daniel Gruhl:Linda Kato:Chris Kau:Petar Ristoski:Steve Welch	Data exploration is a task that inherently requires high human interaction. The subject matter expert looks at the data to identify a hypothesis, potential questions, and where to look for answers in the data. Virtually all data exploration scenarios can benefit from a tight human-in-the-loop paradigm, where data can be visualized and reshaped, but also augmented with missing semantic information - that the subject matter expert can supplement in itinere. In this demo we show a novel graph-based data exploration model where the subject matter expert can annotate and maneuver the data to answer specific questions. This demo specifically focuses on the task of migrating data centers, logically and/or physically, where the subject matter expert needs to identify the function of each node - a server, a virtual machine, a printer, etc - in the data center, which is not necessarily directly available in the data and to be able to plan a safe switch-off and relocation of a cluster of nodes. We show how the novel human-in-the-loop data exploration and enrichment paradigm helps designing the data center migration plan.	Task Oriented Data Exploration with Human-in-the-Loop. A Data Center Migration Use Case.	NA:NA:NA:NA:NA:NA:NA:NA	2018
Marc Bron:Ke Zhou:Andy Haines:Mounia Lalmas	Electronic publishers and other web-companies are starting to collect user feedback on ads with the aim of using this signal to maintain the quality of ads shown on their sites. However, users are not randomly sampled to provide feedback on ads, but targeted. Furthermore some users who provide feedback may be prone to dislike ads more than the general user. This raises questions about the reliability of ad feedback as a signal for measuring ad quality and whether it can be used in ad ranking. In this paper we start by gaining insights to such signals by analyzing the feedback event logs attributed to users of a popular mobile news app. We then propose a model to reduce potential biases in ad feedback data. Finally, we conclude by comparing the effectiveness of reducing the bias in ad feedback data using existing ad ranking methods along with a new and novel approach we propose that takes revenue considerations into account.	Uncovering Bias in Ad Feedback Data Analyses & Applications✱	NA:NA:NA:NA	2018
Gianluca Demartini	Collaborative creation of knowledge is an approach which has been successfully demonstrated by crowdsourcing project like Wikipedia. Similar techniques have recently been adopted for the creation of collaboratively generated Knowledge Graphs like, for example, Wikidata. While such an approach enables the creation of high quality structured content, it also comes with the challenge of introducing contributors’ implicit bias in the generated Knowledge Graph. In this paper, we investigate how paid crowdsourcing can be used to understand contributor bias for controversial facts to be included into collaborative Knowledge Graphs. We propose methods to trace the provenance of crowdsourced fact checking thus enabling bias transparency rather than aiming at eliminating bias from the Knowledge Graph.	Implicit Bias in Crowdsourced Knowledge Graphs	NA	2018
Lauren Fratamico:Deb Roy	Loneliness is becoming a global epidemic. As many as 33% of Americans report being chronically lonely, with similar percentages being reported in countries around the world. Additionally, this is a percentage that has risen in recent years. Many are turning to online forums as a way to connect with others about their feelings of loneliness and to begin to reduce these feelings. However, oftentimes, posts go unresponded to and online conversations do not take place, perhaps because those conversing did not find a connection with each other, potentially leaving the poster feeling even more lonely. This paper introduces a human-in-the-loop approach so that computers can mediate interactions online about loneliness and facilitate more intimate interactions. We also discuss ways to mitigate the bias when creating this system. The artificial intelligence in this approach takes into account the homophilous characteristics of the conversations that are taking place online by examining the homophily of the participants. Initial findings related to the correlation between homophily and successful conversations about loneliness on reddit are presented, and we lay the groundwork for being able to facilitate finding optimal conversation partners for those that are feeling lonely by proposing a human-in-the-loop approach.	I’m Lonely. Who should I talk to?	NA:NA	2018
Shima Imani:Sara Alaee:Eamonn Keogh	Time series are one of the most common data types in nature. Given this fact, there are dozens of query-by-sketching/ query-by-example/ query-algebra systems proposed to allow users to search large time series collections. However, none of these systems have seen widespread adoption. We argue that there are two reasons why this is so. The first reason is that these systems are often complex and unintuitive, requiring the user to understand complex syntax/interfaces to construct high-quality queries. The second reason is less well appreciated. The expressiveness of most query-by-content systems is surprisingly limited. There are well defined, simple queries that cannot be answered by any current query-by-content system, even if it uses a state-of-the-art distance measure such as Dynamic Time Warping. In this work, we propose a natural language search mechanism for searching time series. We show that our system is expressive, intuitive, and requires little space and time overhead. Because our system is text-based, it can leverage decades of research text retrieval, including ideas such as relevance feedback. Moreover, we show that our system subsumes both motif/discord discovery and most existing query-by-content systems in the literature. We demonstrate the utility of our system with case studies in domains as diverse as animal motion studies, medicine and industry.	Putting the Human in the Time Series Analytics Loop	NA:NA:NA	2018
Wenlong Sun:Sami Khenissi:Olfa Nasraoui:Patrick Shafto	Recommender Systems (RSs) are widely used to help online users discover products, books, news, music, movies, courses, restaurants, etc. Because a traditional recommendation strategy always shows the most relevant items (thus with highest predicted rating), traditional RS’s are expected to make popular items become even more popular and non-popular items become even less popular which in turn further divides the haves (popular) from the have-nots (unpopular). Therefore, a major problem with RSs is that they may introduce biases affecting the exposure of items, thus creating a popularity divide of items during the feedback loop that occurs with users, and this may lead the RS to make increasingly biased recommendations over time. In this paper, we view the RS environment as a chain of events that are the result of interactions between users and the RS. Based on that, we propose several debiasing algorithms during this chain of events, and evaluate how these algorithms impact the predictive behavior of the RS, as well as trends in the popularity distribution of items over time. We also propose a novel blind-spot-aware matrix factorization (MF) algorithm to debias the RS. Results show that propensity matrix factorization achieved a certain level of debiasing of the RS while active learning combined with the propensity MF achieved a higher debiasing effect on recommendations.	Debiasing the Human-Recommender System Feedback Loop in Collaborative Filtering	NA:NA:NA:NA	2018
		First Workshop on Hypermedia Multi-Agent Systems (HyperAgents 2019) Workshop Chairs' Welcome & Organization Listing		2018
Rem W. Collier:Eoin O'Neill:David Lillis:Gregory O'Hare	This paper explores the intersection between microservices and Multi-Agent Systems (MAS), introducing the notion of a new approach to building MAS known as Multi-Agent MicroServices (MAMS). Our approach is illustrated through a worked example of a Vickrey Auction implemented as a microservice.	MAMS: Multi-Agent MicroServices✱	NA:NA:NA:NA	2018
Mareike Kritzler:Jack Hodges:Dan Yu:Kimberly Garcia:Hemant Shukla:Florian Michahelles	The industrial domain offers a high degree of standardization, a variety of very specialized use cases, and an abundance of resources. These characteristics provide perfect conditions for Digital Companion systems. A Digital Companion is a cognitive agent that assists human users by taking on three roles: as guardians, assistants or mentors, and partners. This paper describes the characteristics, conceptual architecture, use cases and open challenges regarding Digital Companions for industry.	Digital Companion for Industry	NA:NA:NA:NA:NA:NA	2018
Alexandru Sorici:Andrei Olaru:Adina Magda Florea	Deploying context management systems at a global scale comes with a number of challenges and requirements. We argue that the hypermedia model and the agent-oriented paradigm help achieve the vision of Context-as-a-Service. We categorize challenges according to context processing concerns and use a scenario to exemplify how the proposed architectural principles help overcome the challenges.	Towards Enabling Internet-Scale Context-as-a-Service: A Position Paper	NA:NA:NA	2018
		The First International Workshop on Knowledge Graph Technology and Applications Chairs' Welcome		2018
Denny Vrandecic:Jure Leskovec	Decentralised data solutions bring their own sets of capabilities, requirements and issues not necessarily present in centralised solutions. In order to compare the properties of different approaches or tools for management of decentralised data, it is important to have a common evaluation framework. We present a set of dimensions relevant to data management in decentralised contexts and use them to define principles extending the FAIR framework, initially developed for open research data. By characterising a range of different data solutions or approaches by how TRusted, Autonomous, Distributed and dEcentralised, in addition to how Findable, Accessible, Interoperable and Reusable, they are, we show that our FAIR TRADE framework is useful for describing and evaluating the management of decentralised data solutions, and aim to contribute to the development of best practice in a developing field.	The First International Workshop on Knowledge Graph Technology and Applications	NA:NA	2018
Yuqing Gao	Decentralised data solutions bring their own sets of capabilities, requirements and issues not necessarily present in centralised solutions. In order to compare the properties of different approaches or tools for management of decentralised data, it is important to have a common evaluation framework. We present a set of dimensions relevant to data management in decentralised contexts and use them to define principles extending the FAIR framework, initially developed for open research data. By characterising a range of different data solutions or approaches by how TRusted, Autonomous, Distributed and dEcentralised, in addition to how Findable, Accessible, Interoperable and Reusable, they are, we show that our FAIR TRADE framework is useful for describing and evaluating the management of decentralised data solutions, and aim to contribute to the development of best practice in a developing field.	The First International Workshop on Knowledge Graph Technology and Applications	NA	2018
Joshua Shinavier:Kim Branson:Wei Zhang:Shima Dastgheib:Yuqing Gao:Bogdan Arsintescu:Fatma Özcan:Edgar Meij	This panel will focus on industry applications related to knowledge graph and showcase how knowledge graph transforms the conventional and unconventional industries to the new era of AI, ranging from innovations in medicine and healthcare, literature search, e-commerce, professional connections, to getting a ride. Panelists are: Senior Software Engineer/Research Scientist at Uber, Co-founder of Tinkerpop, specialized on real-time semantics, RDF streams and graph databases Head of AI at Genentech, passionate about modeling, and currently developing a general medical inference engine that can be applied to a wide variety of areas, from point of care decision support, triage, insurance risk managment to name a few A senior staff engineer/director at Business Platform Unit, Alibaba, leading product knowledge graph (PKG) team and Business Platform AI team. He and his team have built a huge PKG with 10 billion of entities. Data Scientist at Numedii, previously postdoctoral research fellow at Stanford University School of Medicine, developing novel methods to integrate and explore a broad set of biological and clinical data for scientific reproducibility and biomedical discovery. An accomplished technical scientist, innovator and R&D leader in cutting-edge technology research and product development. Proven track record of success (20+ years of successful professional career in leading R&D organizations) in leading rapid technological advancement, innovation, and highly competitive environments. Broad range of skills from initiating research breakthroughs to achieving marketable product development. Renowned expert and technological visionary in the fields of enterprise middleware, cloud computing, data centric computing, workload optimized systems and appliances, business analytics, big data, social media and multimedia, speech and natural language processing. Extended technical leadership in systems design for LinkedIn Economic Graph, Google Search and Google Research. Breadth and depth of expertise in building data systems and platforms. Technical leadership and management of software development in both start-up and large companies. A principal research staff member, and a senior manager at IBM Almaden Research Center. I manage the information management department, working on HTAP (hybrid transactional and analytical processing) systems, large scale machine learning, and natural language querying of data. A team lead and senior scientist at Bloomberg. He holds a PhD in computer science from the University of Amsterdam and has an extensive track record in artificial intelligence, information retrieval, knowledge graphs, natural language processing, and machine learning. Before joining Bloomberg he worked at Yahoo Labs on semantic search at web scale using the Yahoo Knowledge Graph. At Bloomberg he leads the team that is responsible for leveraging knowledge graph technology to drive advanced financial insights.	Panel: Knowledge Graph Industry Applications	NA:NA:NA:NA:NA:NA:NA:NA	2018
Jie Tang:Jure Leskovec:Kuansan Wang:Deborah McGuinness:Hassan Sawaf	This panel will focus on the cutting-edge computation methods, which can be applied to knowledge graph, such as latest NLP technologies to extract entities and relationships to build knowledge graphs, machine learning or deep learning methods on mining knowledge graph, and intelligent search or recommendations powered by knowledge graphs. Panelists are: Professor and the Vice Chair of the Department of Computer Science and Technology of Tsinghua University. I obtained my Ph.D. in DCST of Tsinghua University in 2006. My research interests include artificial intelligence, data mining, social networks, machine learning and knowledge graph, with an emphasis on designing new algorithms for mining social and knowledge networks. Associate Professor of Computer Science at Stanford University. My research focuses on mining and modeling large social and information networks, their evolution, and diffusion of information and influence over them. Problems I investigate are motivated by large scale data, the web and on-line media Managing Director of MSR Outreach, an organization with the mission to serve the research community. In addition to applying the intelligent technologies to make Bing and Cortana smarter in gathering and serving academic knowledge, we are also starting an experimental website, academic.microsoft.com (powered by Academic API), and mobile apps dedicated to exploring new service scenarios for active researchers like myself A leading expert in knowledge representation and reasoning languages and systems and has worked in ontology creation and evolution environments for over 20 years. Most recently, McGuinness is best known for her leadership role in semantic web research, and for her work on explanation, trust, and applications of semantic web technology, particularly for scientific applications. Director of Artificial Intelligence at Amazon Web Services, passionate about opening up new markets and opportunities with smart application of Artificial Intelligence and application-driven research in AI, in particular in language technology, speech processing, computer vision and computational reasoning.	Panel: Computational Methods about Knowledge Graph	NA:NA:NA:NA:NA	2018
Shumin Deng:Ningyu Zhang:Wen Zhang:Jiaoyan Chen:Jeff Z. Pan:Huajun Chen	Deep neural networks have achieved promising results in stock trend prediction. However, most of these models have two common drawbacks, including (i) current methods are not sensitive enough to abrupt changes of stock trend, and (ii) forecasting results are not interpretable for humans. To address these two problems, we propose a novel Knowledge-Driven Temporal Convolutional Network (KDTCN) for stock trend prediction and explanation. Firstly, we extract structured events from financial news, and utilize external knowledge from knowledge graph to obtain event embeddings. Then, we combine event embeddings and price values together to forecast stock trend. We evaluate the prediction accuracy to show how knowledge-driven events work on abrupt changes. We also visualize the effect of events and linkage among events based on knowledge graph, to explain why knowledge-driven events are common sources of abrupt changes. Experiments demonstrate that KDTCN can (i) react to abrupt changes much faster and outperform state-of-the-art methods on stock datasets, as well as (ii) facilitate the explanation of prediction particularly with abrupt changes.	Knowledge-Driven Stock Trend Prediction and Explanation via Temporal Convolutional Network	NA:NA:NA:NA:NA:NA	2018
Matthew Horridge:Rafael S. Gonçalves:Csongor I. Nyulas:Tania Tudorache:Mark A. Musen	We present WebProtégé, a tool to develop ontologies represented in the Web Ontology Language (OWL). WebProtégé is a cloud-based application that allows users to collaboratively edit OWL ontologies, and it is available for use at https://webprotege.stanford.edu. WebProtégé currently hosts more than 68,000 OWL ontology projects and has over 50,000 user accounts. In this paper, we detail the main new features of the latest version of WebProtégé.	WebProtégé: A Cloud-Based Ontology Editor	NA:NA:NA:NA:NA	2018
Kevin Joseph:Hui Jiang	Content-based news recommendation systems need to recommend news articles based on the topics and content of articles without using user specific information. Many news articles describe the occurrence of specific events and named entities including people, places or objects. In this paper, we propose a graph traversal algorithm as well as a novel weighting scheme for cold-start content based news recommendation utilizing these named entities. Seeking to create a higher degree of user-specific relevance, our algorithm computes the shortest distance between named entities, across news articles, over a large knowledge graph. Moreover, we have created a new human annotated data set for evaluating content based news recommendation systems. Experimental results show our method is suitable to tackle the hard cold-start problem and it produces stronger Pearson correlation to human similarity scores than other cold-start methods. Our method is also complementary and a combination with the conventional cold-start recommendation methods may yield significant performance gains. The dataset, CNRec, is available at: https://github.com/kevinj22/CNRec	Content based News Recommendation via Shortest Entity Distance over Knowledge Graphs	NA:NA	2018
Richard M. Keller	Historically, most of the focus in the knowledge graph community has been on the support for web, social network, or product search applications. This paper describes some of our experience in developing a large-scale applied knowledge graph for a more technical audience with more specialized information access and analysis needs – the air traffic management community. We describe ATMGRAPH, a knowledge graph created by integrating various sources of structured aviation data, provided in large part by US federal agencies. We review some of the practical challenges we faced in creating this knowledge graph.	Building a Knowledge Graph for the Air Traffic Management Community	NA	2018
Aman Mehta:Aashay Singhal:Kamalakar Karlapalem	Automatic extraction of information from text and its transformation into a structured format is an important goal in both Semantic Web Research and computational linguistics. Knowledge Graphs (KG) serve as an intuitive way to provide structure to unstructured text. A fact in a KG is expressed in the form of a triple which captures entities and their interrelationships (predicates). Multiple triples extracted from text can be semantically identical but they may have a vocabulary gap which could lead to an explosion in the number of redundant triples. Hence, to get rid of this vocabulary gap, there is a need to map triples to a homogeneous namespace. In this work, we present an end-to-end KG construction system, which identifies and extracts entities and relationships from text and maps them to the homogenous DBpedia namespace. For Predicate Mapping, we propose a Deep Learning architecture to model semantic similarity. This mapping step is computation heavy, owing to the large number of triples in DBpedia. We identify and prune unnecessary comparisons to make this step scalable. Our experiments show that the proposed approach is able to construct a richer KG at a significantly lower computation cost with respect to previous work.	Scalable Knowledge Graph Construction over Text using Deep Learning based Predicate Mapping	NA:NA:NA	2018
Tomer Sagi:Yael Wolf:Katja Hose	Linked Open Data and the RDF format have become the premier method of publishing structured data representing entities and facts. Specifically, media organizations, such as the New York Times and the BBC, have embraced Linked Open Data as a way of providing structured access to traditional media content, including articles, images, and video. To ground RDF entities and predicates in existing Linked Open Data sources, dataset curators provide links for some entities to existing general purpose repositories, such as YAGO and DBpedia, using entity extraction and linking tools. However, these state-of-the-art tools rely on the entities to exist in the knowledge base. How much of the information is actually new and thus unable to be grounded is unclear. In this work, we empirically investigate the prevalence of new entities in news feeds with respect to both public and commercial knowledge graphs.	How New is the (RDF) News?	NA:NA:NA	2018
Daniel Schwabe	This paper presents the KG Usage framework, which allows the introduction of KG features to support Trust, Privacy and Transparency concerns regarding the use of its contents by applications. A real-world example is presented and used to illustrate how the framework can be used.	Trust and Privacy in Knowledge Graphs	NA	2018
Yan Zhou:Longtao Huang:Tao Guo:Songlin Hu:Jizhong Han	Extracting entities and relations is critical to the understanding of massive text corpora. Recently, neural joint models have shown promising results for this task. However, the entity features are not effectively used in these joint models. In this paper, we propose an approach to utilize the implicit entity features in the joint model and show these features can facilitate the joint extraction task. Particularly, we use the hidden-layer vectors extracted from a pre-trained named entity recognition model as the entity features. Thus, our method does not need to design the entity features by hand and can benefit from the new development of named entity recognition task. In addition, we introduce an attention mechanism in our model which can select the informative parts of the input sentence to the prediction. We conduct a series of experiments on a public dataset and the results show the effectiveness of our model.	An Attention-based Model for Joint Extraction of Entities and Relations with Implicit Entity Features	NA:NA:NA:NA:NA	2018
		LA-Web 2019 Chairs' Welcome		2018
Jussara Almeida	Misinformation dissemination is a topic that has gained a lot of attention from academia and public media, in general. Despite a rich literature on strategies to detect and mitigate this phenomenon, the problem still persists with impact on several sectors of the society. In this talk, I will discuss the problem, revise existing approaches as well as discuss challenges to properly address it. I will also discuss recent results of our group on the investigation of misinformation spread on WhatsApp.	Misinformation Dissemination on the Web	NA	2018
Carlos Castillo	In this talk we refer to bias in its everyday sense, as a prejudice against a person or a group, and ask whether an algorithm, particularly a ranking algorithm, can be biased. We begin by defining under which conditions this can happen. Next, we describe key results from research on algorithmic fairness, much of which studies automatic classification by a supervised learning method. Finally, we attempt to map these concepts to rankings and to introduce new, ranking-specific ways of looking at algorithmic bias.	Algorithmic Bias in Rankings	NA	2018
Victoria Patricia Aires:Fabiola G. Nakamura:Eduardo F. Nakamura	News websites are currently one of the main sources of information. Like traditional media, these sources can have a bias in how they report news. This media bias can influence how people perceive events, political decisions, or discussions. In this paper, we describe a link-based approach to identify news websites with the same political orientation, i.e., characterize the bias of news websites, using network analysis techniques. After constructing a graph from a few seeds with previously known bias, we show that a community detection algorithm can identify groups formed by sources with the same political orientation.	A Link-based Approach to Detect Media Bias in News Websites	NA:NA:NA	2018
Harold Aragon:Samuel Braganza:Edwin Boza:Jonathan Parrales:Cristina Abad	We study the workload of an Online Invoicing application with clients in the Andean region in South America. The application is offered to clients with a Software-as-a-Service model, has a microservices architecture and runs on a containerized environment on a public cloud provider. The cloud application workload described in this paper can be used as part of a workload suite comprised of different application workloads, when evaluating microservices architectures. To the best of our knowledge, this is a novel workload in the web domain and it complements other workloads publicly available. Though we make no claim of the general applicability of this workload as a “microservices benchmark”, its inclusion in evaluations could aid researchers and practitioners enrich their evaluations with tests based on a real microservices-based web application. Finally, we provide some insights regarding best-practices in microservice design, as a result of the observed workload.	Workload Characterization of a Software-as-a-Service Web Application Implemented with a Microservices Architecture	NA:NA:NA:NA:NA	2018
Maria Soledad Bucalo:Luz Calvo:Fernando Cucchietti:David Garcia Povedano:Artur Garcia-Sáez:Juan Felipe Gómez:Camilo Arcadio González:Guillermo Marin:Irene Meta:Patricio Reyes:Feliu Serra:Diana Fernanda Vélez	In this work, we analyze content and structure of the Twitter trending topic #cuentalo with the purpose of providing a visualization of the movement. A supervised learning methodology is used to train the classifying algorithms with hand-labeled observations. The methodology allows us to classify each tweet according to its role in the movement.	A Constellation of Horrors: Analysis and Visualization of the #Cuéntalo Movement	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Germán Cheuque Cerda:Juan L. Reutter	The Bitcoin protocol and its underlying cryptocurrency have started to shape the way we view digital currency, and opened up a large list of new and interesting challenges. Amongst them, we focus on the question of how is the price of digital currencies affected, which is a natural question especially when considering the price rollercoaster we witnessed for bitcoin in 2017-2018. We work under the hypothesis that price is affected by the web footprint of influential people, we refer to them as crypto-influencers. In this paper we provide neural models for predicting bitcoin price. We compare what happens when the model is fed only with recent price history versus what happens when fed, in addition, with a measure of the positivity or negativity of the sayings of these influencers, measured through a sentiment analysis of their twitter posts. We show preliminary evidence that twitter data should indeed help to predict the price of bitcoin, even though the measures we use in this paper have a lot of room for refinement. In particular, we also discuss the challenges of measuring the correct sensation of these posts, and discuss the work that should help improving our discoveries even further.	Bitcoin Price Prediction Through Opinion Mining	NA:NA	2018
Germán Cheuque:José Guzmán:Denis Parra	The world of video games has changed considerably over the recent years. Its diversification has dramatically increased the number of users engaged in online communities of this entertainment area, and consequently, the number and types of games available. This context of information overload underpins the development of recommender systems that could leverage the information that the video game platforms collect, hence following the trend of new games coming out every year. In this work we test the potential of state-of-the-art recommender models based respectively on Factorization Machines (FM), deep neural networks (DeepNN) and one derived from the mixture of both (DeepFM), chosen for their potential of receiving multiple inputs as well as different types of input variables. We evaluate our results measuring the ranking accuracy of the recommendation and the diversity/novelty of a recommendation list. All the algorithms achieve better results than a baseline based on implicit feedback (Alternating Least Squares model). The best performing algorithm is DeepNN, the high order interactions are more important than the low order ones for this recommendation task. We also analyze the effect of the sentiment extracted directly from game reviews, and find that it is not as relevant for recommendation as one might expect. We are the first in studying the aforementioned recommender systems over the context of online video game platforms, reporting novel results which could be used as baseline in future works.	Recommender Systems for Online Video Game Platforms: the Case of STEAM	NA:NA:NA	2018
Tiago de Melo:Altigran S. da Silva:Edleno S. de Moura:Pável Calado	When making purchasing decisions, customers usually rely on information from two types of sources: product specifications, provided by manufacturers, and reviews, posted by other customers. Both kinds of information are often available on e-commerce websites. While researchers have demonstrated the importance of product specifications and reviews as separate and valuable sources to support purchase decision-making, a largely uninvestigated issue is what is the relationship between these two kinds of information. In this paper we present an empirical study on the use of direct and indirect mentions to canonical product attributes, that is, those defined by manufactures in product specifications, in the reviews written by customers. For this study, we analyzed more than 1,100,000 opinionated sentences available in about 650,000 user reviews from Amazon.com across five product categories. Our results indicate that user opinions are indeed guided by the attributes from product specifications and highlight the influence of canonical attributes on the user reviews.	The Importance of Canonical Product Attributes on User Opinions: an Empirical Investigation	NA:NA:NA:NA	2018
Juglar Diaz:Barbara Poblete	Complex human behaviors related to crime require multiple sources of information to understand them. Social Media is a place where people share opinions and news. This allows events in the physical world like crimes to be reflected on Social Media. In this paper we study crimes from the perspective of Social Media, specifically car theft and Twitter. We use data of car theft reports from Twitter and car insurance companies in Chile to perform a temporal analysis. We found that there is an increasing correlation in recent years between the number of car theft reports in Twitter and data collected from insurance companies. We performed yearly, monthly, daily and hourly analyses. Though Twitter is an unstructured source and very noisy, it allows you to estimate the volume of thefts that are reported by the insurers. We experimented with a Moving Average to predict the tendency in the number of car theft reported to insurances using Twitter data and found that one month is the best time window for prediction.	Car Theft Reports: a Temporal Analysis from a Social Media Perspective	NA:NA	2018
Yerka Freire-Vidal:Eduardo Graells-Garrido	Migration is a worldwide phenomenon that may generate different reactions in the population. Attitudes vary from those that support multiculturalism and communion between locals and foreigners, to contempt and hatred toward immigrants. Since anti-immigration attitudes are often materialized in acts of violence and discrimination, it is important to identify factors that characterize these attitudes. However, doing so is expensive and impractical, as traditional methods require enormous efforts to collect data. In this paper, we propose to leverage Twitter to characterize local attitudes toward immigration, with a case study on Chile, where immigrant population has drastically increased in recent years. Using semi-supervised topic modeling, we situated 49K users into a spectrum ranging from in-favor to against immigration. We characterized both sides of the spectrum in two aspects: the emotions and lexical categories relevant for each attitude, and the discussion network structure. We found that the discussion is mostly driven by Haitian immigration; that there are temporal trends in tendency and polarity of discussion; and that assortative behavior on the network differs with respect to attitude. These insights may inform policy makers on how people feel with respect to migration, with potential implications on communication of policy and the design of interventions to improve inter-group relations.	Characterization of Local Attitudes Toward Immigration Using Social Media	NA:NA	2018
Manuela Garretón:Andrea Rihm:Denis Parra	The last decades have shown us a growing interest in different fields of how interactive art transforms the position of the viewer into a participant and how audiences engage and relate to interactive artwork. This article presents a visual analysis of the content shared on Instagram by the audience of Default –an interactive art installation presented in Santiago, Chile in 2017. The analysis shows that people reacted and engaged differently with various aspects of the installation, as shown by the strategies they used to share it. We argue that the analysis of the visual content of Instagram posts opens avenues to understanding the relationship between installation and audience, giving clues on the audience experience and, therefore, providing feedback for developers, who could use them in the design process of future installations.	#Default #Interactiveart #Audiencexperience	NA:NA:NA	2018
Felipe González:Yihan Yu:Andrea Figueroa:Claudia López:Cecilia Aragon	Currently, there is a limited understanding of how data privacy concerns vary across the world. The Cambridge Analytica scandal triggered a wide-ranging discussion on social media about user data collection and use practices. We conducted a cross-language study of this online conversation to compare how people speaking different languages react to data privacy breaches. We collected tweets about the scandal written in Spanish and English between April and July 2018. We used the Meaning Extraction Method in both datasets to identify their main topics. They reveal a similar emphasis on Zuckerberg’s hearing in the US Congress and the scandal’s impact on political issues. However, our analysis also shows that while English speakers tend to attribute responsibilities to companies, Spanish speakers are more likely to connect them to people. These findings show the potential of cross-language comparisons of social media data to deepen the understanding of cultural differences in data privacy perspectives.	Global Reactions to the Cambridge Analytica Scandal: A Cross-Language Social Media Study	NA:NA:NA:NA:NA	2018
Bernadette Farias Lóscio:Caroline Burle:Newton Calegari	The Best Practices described on the Data on the Web Best Practices (DWBP) document [3] encourages and enables the continued expansion of the Web as a medium for the exchange of data. In this context, this paper focus on two cases of implementing the DWBP. The first one concerns data published by The Regional Center for Studies on the Development of the Information Society (Cetic.br) of The Brazilian Network Information Center (NIC.br). The second use case shows the experience of the Judiciary Department of Costa Rica (Justicia Abierta) with applying the DWBP Recommendation to publish their data on the Web.	Implementation of the Best Practices for Data on the Web in Brazil and Costa Rica	NA:NA:NA	2018
Karen Oróstica:Barbara Poblete	Recent work suggests that certain places can be more attractive for car theft based how many people regularly visit them, as well as other factors. In this sense, we must also consider the city or district itself where vehicles are stolen. All cities have different cultural and socioeconomic characteristics that influence car theft patterns. In particular, the distribution of public services and places attract a large crowd could play a key role in the occurrence of car theft. Santiago, a city that displays drastic socioeconomic differences among its districts, presents increasingly-high car theft rates. This represents a serious issue for the city, as for any other major city, which –at least for Santiago– has not been analyzed in depth using quantitative approaches. In this work, we present a preliminary study of how places that create social interest, such as restaurants, bars, schools, and shopping malls, increase car theft frequency in Santiago. We also study if some types of places are more attractive than others for this type of crime. To evaluate this, we propose to analyze car theft points (CTP) from insurance companies and their relationship with places of social interest (PSI) extracted from Google Maps, using a proximity based approach. Our findings show a high correlation between CTP and PSI for all of the social interest categories that we studied in the different districts of the Santiago. In particular our work contributes to the understanding of the social factors that are associated to car thefts.	Mining the Relationship BetweenCar Theft and Places of Social Interest in Santiago Chile	NA:NA	2018
Henry Rosales-Méndez:Aidan Hogan:Barbara Poblete	The Entity Linking (EL) task identifies entity mentions in a text corpus and associates them with a corresponding unambiguous entry in a Knowledge Base. The evaluation of EL systems relies on the comparison of their results against gold standards. A common format used to represent gold standard datasets is the NLP Interchange Format (NIF), which uses RDF as a data model. However, creating gold standard datasets for EL is a time-consuming and error-prone process. In this paper we propose a tool called NIFify to help manually generate, curate, visualize and validate EL annotations; the resulting tool is useful, for example, in the creation of gold standard datasets. NIFify also serves as a benchmark tool that enables the assessment of EL results. Using the validation features of NIFify, we further explore the quality of popular EL gold standards.	NIFify: Towards Better Quality Entity Linking Datasets	NA:NA:NA	2018
Jefferson Silva:Newton Calegari:Eduardo Gomes	Decentralized web applications do not offer fine-grained access controls to users’ data, which potentially creates openings for data breaches. For software companies that need to comply with Brazil’s General Data Protection Law (LGPD), data breaches not only might harm application users but also could expose the companies to hefty fines. In this context, engineering fine-grained authorization controls (that comply with the LGPD) to decentralized web application requires creating audit trails, possibly in the source code. Although the literature offers some solutions, they are scattered. We present Esfinge Guardian, an authorization framework that completely separates authorization from other concerns, which increases compliance with the LGPD. We conclude the work with a brief discussion.	After Brazil’s General Data Protection Law: Authorization in Decentralized Web Applications	NA:NA:NA	2018
Maribel Acosta:Tim Berners-Lee:Stefan Dietze:Anastasia Dimou:John Domingue:Luis-Daniel Ibánez:Krzysztof Janowicz:Maria-Esther Vidal:Amrapali Zaveri	Decentralised data solutions bring their own sets of capabilities, requirements and issues not necessarily present in centralised solutions. In order to compare the properties of different approaches or tools for management of decentralised data, it is important to have a common evaluation framework. We present a set of dimensions relevant to data management in decentralised contexts and use them to define principles extending the FAIR framework, initially developed for open research data. By characterising a range of different data solutions or approaches by how TRusted, Autonomous, Distributed and dEcentralised, in addition to how Findable, Accessible, Interoperable and Reusable, they are, we show that our FAIR TRADE framework is useful for describing and evaluating the management of decentralised data solutions, and aim to contribute to the development of best practice in a developing field.	Linked Data on theWeb and its Relationship with Distributed Ledgers (LDOW/LDDL)	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Themis Beris:Iosif Angelidis:Ilias Chalkidis:Charalampos Nikolaou:Christos Papaloukas:Panagiotis Soursos:Manolis Koubarakis	This paper is a progress report on our recent work on two applications that use Linked Data and Distributed Ledger technologies and aim to transform the Greek public sector into a decentralized, trusted, intelligent and linked organization. The first application is a re-engineering of Diavgeia, the Greek government portal for open and transparent public administration. The second application is Nomothesia, a new portal that we have built, which makes Greek legislation available on the Web as linked data to enable its effective use by citizens, legal professionals and software developers who would like to build new applications that utilize Greek legislation. The presented applications have been implemented without funding from any source and are available for free to any part of the Greek public sector that may want to use them. An important goal of this paper is to present the lessons learned from this effort.	Towards a Decentralized, Trusted, Intelligent and Linked Public Sector: A Report from the Greek Trenches	NA:NA:NA:NA:NA:NA:NA	2018
Jeremy Debattista:Judie Attard:Rob Brennan:Declan O'Sullivan	The Linked Open Data (LOD) cloud has been around since 2007. Throughout the years, this prominent depiction served as the epitome for Linked Data and acted as a starting point for many. In this article we perform a number of experiments on the dataset metadata provided by the LOD cloud, in order to understand better whether the current visualised datasets are accessible and with an open license. Furthermore, we perform quality assessment of 17 metrics over accessible datasets that are part of the LOD cloud. These experiments were compared with previous experiments performed on older versions of the LOD cloud. The results showed that there was no improvement on previously identified problems. Based on our findings, we therefore propose a strategy and architecture for a potential collaborative and sustainable LOD cloud.	Is the LOD cloud at risk of becoming a museum for datasets? Looking ahead towards a fully collaborative and sustainable LOD cloud	NA:NA:NA:NA	2018
Ahmed El Amine Djebri:Andrea G.B. Tettamanzi:Fabien Gandon	There is no credibility insurance measure for the information provided by the Web. In most cases, information cannot be checked for accuracy. Semantic Web technologies aimed to give structure and sense to information published on the Web and to provide us with a machine-readable data format for interlinked data. However, Semantic Web standards do not offer the possibility to represent and attach uncertainty to such data in a way allowing the reasoning over the latter. Moreover, uncertainty is context-dependent and may be represented by multiple theories which apply different calculi. In this paper, we present a new vocabulary and a framework for handling generic uncertainty representation and reasoning. The meta-Uncertainty vocabulary offers a way to represent uncertainty theories and annotate Linked Data with uncertainty information. We provide the tools to represent uncertainty calculi linked to the previous theories using the LDScript function scripting language. Moreover, we describe the semantics of contexts in uncertainty reasoning with meta-uncertainty. We describe the mapping between RDF triples and their uncertainty information, and we demonstrate the effect on the query writing process in Corese. We discuss the translatability of uncertainty theories and, finally, the negotiation of an answer annotated with uncertainty information.	Linking and Negotiating Uncertainty Theories Over Linked Data	NA:NA:NA	2018
John Domingue:Allan Third:Manoharan Ramachandran	Decentralised data solutions bring their own sets of capabilities, requirements and issues not necessarily present in centralised solutions. In order to compare the properties of different approaches or tools for management of decentralised data, it is important to have a common evaluation framework. We present a set of dimensions relevant to data management in decentralised contexts and use them to define principles extending the FAIR framework, initially developed for open research data. By characterising a range of different data solutions or approaches by how TRusted, Autonomous, Distributed and dEcentralised, in addition to how Findable, Accessible, Interoperable and Reusable, they are, we show that our FAIR TRADE framework is useful for describing and evaluating the management of decentralised data solutions, and aim to contribute to the development of best practice in a developing field.	The FAIR TRADE Framework for Assessing Decentralised Data Solutions	NA:NA:NA	2018
Franck Michel:Catherine Faron-Zucker:Olivier Corby:Fabien Gandon	To help in making sense of the ever-increasing number of data sources available on the Web, in this article we tackle the problem of enabling automatic discovery and querying of data sources at Web scale. To pursue this goal, we suggest to (1) provision rich descriptions of data sources and query services thereof, (2) leverage the power of Web search engines to discover data sources, and (3) rely on simple, well-adopted standards that come with extensive tooling. We apply these principles to the concrete case of SPARQL micro-services that aim at querying Web APIs using SPARQL. The proposed solution leverages SPARQL Service Description, SHACL, DCAT, VoID, Schema.org and Hydra to express a rich functional description that allows a software agent to decide whether a micro-service can help in carrying out a certain task. This description can be dynamically transformed into a Web page embedding rich markup data. This Web page is both a human-friendly documentation and a machine-readable description that makes it possible for humans and machines alike to discover and invoke SPARQL micro-services at Web scale, as if they were just another data source. We report on a prototype implementation that is available on-line for test purposes, and that can be effectively discovered using Google’s Dataset Search engine.	Enabling Automatic Discovery and Querying of Web APIs at Web Scale using Linked Data Standards	NA:NA:NA:NA	2018
		LocWeb2019 Chairs' Welcome		2018
Leo Ferres	In this keynote, I will present some results we have obtained using location data from mobile phones interacting with information (news) websites, mobile apps like Pokemon GO and Twitter, and large physical spaces like shopping malls. I will draw some conclusions and generally discuss about the properties of mobile phone data for location-based research to finally close with some remarks about privacy and data security.	Indoor Towers, DPIs, and More People in Parks at Night: New Trends in Mobile Phone Location Research	NA	2018
Amine Dadoun:Raphaël Troncy:Olivier Ratier:Riccardo Petitti	The amount of information available in social media and specialized blogs has become useful for a user to plan a trip. However, the user is quickly overwhelmed by the list of possibilities offered to him, making his search complex and time-consuming. Recommender systems aim to provide personalized suggestions to users by leveraging different type of information, thus assisting them in their decision-making process. Recently, the use of neural networks and knowledge graphs have proven to be efficient for items recommendation. In our work, we propose an approach that leverages contextual, collaborative and content information in order to recommend personalized destinations to travelers. We compare our approach with a set of state of the art collaborative filtering methods and deep learning based recommender systems.	Location Embeddings for Next Trip Recommendation	NA:NA:NA:NA	2018
Kris McGlinn:Darragh Blake:Declan O'Sullivan	Linked data (LD) is a technology to support publishing structured data on the web so that it may be interlinked. Building Information Modelling (BIM) is a key enabler to support integration of building data within the buildings life cycle (BLC). LD can therefore provide better access and more semantically useful querying of BIM data. The integration of BIM into the geospatial domain provides much needed contextual information about the building and its surroundings, and can support geospatial querying over BIM data. Creating GeoSPARQL queries for users who are non experts in semantic web technologies can be a challenge. In this paper we present a visualization tool built upon HTML5 and WebGL technologies that supports queries over linked data without the need to understand the resulting SPARQL queries. The interactive web interface can be quickly extended to support new use cases, for example, related to 3D geometries. The paper discusses the underlying data management, the methodology for uplifting several open data sources into Resource Description Framework (RDF), and the front-end implementation tested over a sample use case. Finally some discussion and future work is given, with a focus on how this tool can potentially support BIM integration.	GViz - An Interactive WebApp to Support GeoSPARQL over Integrated Building Information	NA:NA:NA	2018
Thomas Steiner:Anssi Kostiainen:Marijn Kruisselbrink	Geolocation is arguably one of the most powerful capabilities of smartphones and a lot of attention has been paid to native applications that make use of it. The discontinued Google Gears plugin was one of the first approaches to access exact location data on the Web as well, apart from server-side coarse location lookups based on Internet Protocol (ip) addresses; and the plugin led directly to the now widely implemented Geolocation api. The World Wide Web Consortium (w3c) Geolocation api specification defines a standard for accessing location services in the browser via JavaScript. For a long time, developers have also demanded more advanced features like background geolocation tracking and geofencing. The w3c Geolocation and the Devices and Sensors Working Groups, as well as the Web Incubator Community Group (wicg), have addressed these demands with the no longer maintained Geofencing api specification for the former, and—with now (early 2019) resumed efforts—the in-flight Geolocation Sensors specification for the latter two groups. This paper first provides an in-depth overview of the historical development of geolocation in the browser and motivates privacy decisions that were made at the time, and then gives an outlook on current and future efforts, challenges, and use cases from both a technology as well as from a privacy angle.	Geolocation in the Browser	NA:NA:NA	2018
Helge Reelfs:Timon Mohaupt:Oliver Hohlfeld:Niklas Henckell	This paper studies for the first time the usage and propagation of hashtags in a new and fundamentally different type of social media that is i) without profiles and ii) location-based to only show nearby posted content. Our study is based on analyzing the mobile-only Jodel microblogging app, which has an established user base in several European countries and Saudi Arabia. All posts are user to user anonymous (i.e., no displayed user handles) and are only displayed in the proximity of the user’s location (up to 20 km). It thereby forms local communities and opens the question of how information propagates within and between these communities. We tackle this question by applying established metrics for Twitter hashtags to a ground-truth data set of Jodel posts within Germany that spans three years. We find the usage of hashtags in Jodel to differ from Twitter; despite embracing local communication in its design, Jodel hashtags are mostly used country-wide.	Hashtag Usage in a Geographically-Local Microblogging App	NA:NA:NA:NA	2018
		MDML '™2019 Chairs' Welcome		2018
David Pennock	Billions of dollars in financial securities exchange hands every day in independent continuous double auctions. Although the auctions are automated, fast, open 24-7, and have worldwide scope and massive scale, the underlying auction rules have not changed much for over 100 years. Advertisement auctions, on the other hand, have rapidly evolved, incorporating optimization and machine learning directly into their allocation rules. The downside is a less-transparent auction, but the upsides for efficiency and expressiveness are tremendous. The trend toward smarter markets will expand into finance and well beyond, pervading how markets are designed. I will discuss markets that optimize and learn, using prediction markets and advertising markets as key examples.	Smarter Markets: Bringing Intelligence into the Exchange	NA	2018
Suju Rajan:Noureddine El Karoui	Much of classical auction theory has been developed from the standpoint of the seller, trying to understand how to optimize auctions to maximize seller revenue for instance. This is still a source of very active current research. Billions of auctions are now run on the Internet everyday between the same sellers and bidders and this creates a need to better understand auctions from the bidders’ perspective. In this talk we will present some recent results on this question, showing for instance that auctions that are reputed to be truthful are not truthful anymore when the seller optimizes the auction format based on bidders’ past bids, provide explicit and simple to implement shading strategies that improve bidders’ utility (on and off equilibrium) and are robust to various forms of estimation error and mechanism changes. We will also discuss various equilibrium questions. We take a mostly functional analytic point of view on these problems. If time permits, we will discuss ongoing work on a machine-learning-based perspective. Joint work with Thomas Nedelec, Marc Abeille, Clément Calauzènes, Benjamin Heymann and Vianney Perchet while doing research at Criteo.	Auction Theory from the Bidder Standpoint	NA:NA	2018
Yash Kanoria:Hamid Nazerzadeh	Motivated by online advertising market, we consider a seller who repeatedly sells ex ante identical items via the second-price auction. Buyers’ valuations for each item are drawn i.i.d. from a distribution F that is unknown to the seller. We find that if the seller attempts to dynamically update a common reserve price based on the bidding history, this creates an incentive for buyers to shade their bids, which can hurt revenue. When there is more than one buyer, incentive compatibility can be restored by using personalized reserve prices, where the personal reserve price for each buyer is set using the historical bids of other buyers. In addition, we use a lazy allocation rule, so that buyers do not benefit from raising the prices of their competitors. Such a mechanism asymptotically achieves the expected revenue obtained under the static Myerson optimal auction for F. Further, if valuation distributions differ across bidders, the loss relative to the Myerson benchmark is only quadratic in the size of such differences. We extend our results to a contextual setting where the valuations of the buyers depend on observed features of the items.	Incentive-Compatible Learning of Reserve Prices for Repeated Auctions	NA:NA	2018
Thomas Nedelec:Noureddine El Karoui:Vianney Perchet	We consider the problem of the optimization of bidding strategies in prior-dependent revenue-maximizing auctions, when the seller fixes the reserve prices based on the bid distributions. Our study is done in the setting where one bidder is strategic. Using a variational approach, we study the complexity of the original objective and we introduce a relaxation of the objective functional in order to use gradient descent methods. Our approach is simple, general and can be applied to various value distributions and revenue-maximizing mechanisms. The new strategies we derive yield massive uplifts compared to the traditional truthfully bidding strategy.	Learning to Bid in Revenue Maximizing Auction	NA:NA:NA	2018
		MEPDaW Chairs' Welcome		2018
Laure Berti-Equille	With the success of machine learning (ML) techniques, ML has already proved a tremendous potential to impact the foundations, algorithms, and models of several data management tasks, such as error detection, data quality assessment, data cleaning, and data integration. In Knowledge Graphs, part of the data preparation and cleaning processes, such as data linking, identity disambiguation, or missing value inference and completion could be automated by making a ML model “learn” and predict the matches routinely with different degrees of supervision. This talk will survey the recent trends of applying machine learning solutions to improve and facilitate Knowledge Graph curation and enrichment, as one of the most critical tasks impacting Web search and query-answering. Finally, the talk will discuss the next research challenges in the convergence of machine learning and management of Knowledge Graph evolution and preservation.	ML-Based Knowledge Graph Curation: Current Solutions and Challenges	NA	2018
James Anderson	This report describes a way to represent and operate on an RDF dataset such the it behaves as an instance of a conflict free replicable datatype. In this industry presentation, we describe how we accomplish this for the Dydra RDF graph storage service in a manner compatible with the SPARQL Graph Store HTTP Protocol (GSP). The standard GSP concerns the current store state only. Dydra retains previous store states as active addressable aspects analogous to named graphs in a quad store. It incorporates and addresses arbitrary revisions of target datasets according to ETag and Content-Disposition specifications in HTTP headers. Appropriate interpretation of these arguments permits to replicate datasets among cooperating participants.	RDF Graph Stores as Convergent Datatypes	NA	2018
Natanael Arndt:Norman Radtke	The Semantic Web is about collaboration and exchange of information. While the data on the Semantic Web is constantly evolving and meant to be collaboratively edited there is no practical transactional concept or method to control concurrent writes to a dataset and avoid conflicts. Thus, we follow the question, how can we ensure a controlled state of a SPARQL Store when performing non transactional write operations? Based on the Distributed Version Control System for RDF data implemented in the Quit Store we present the Quit Editor Interface Concurrency Control (QEICC). QEICC provides a protocol on top of the SPARQL 1.1 standard to identify, avoid, and resolve conflicts. The strategies reject, branch, and merge are presented to allow different levels of control over the conflict resolution. While the reject strategy gives full control to the client, with branch and merge it is even possible to postpone the conflict resolution and integrate it into the date engineering process.	Conflict Detection, Avoidance, and Resolution in a Non-Linear RDF Version Control System	NA:NA	2018
Natanael Arndt:Michael Martin	Apart from documents, datasets are gaining more attention on the World Wide Web. An increasing number of the datasets on the Web are available as Linked Data, also called the Linked Open Data Cloud1 or Giant Global Graph2. Collaboration of people and machines is a major aspect of the World Wide Web and as well of the Semantic Web. Currently, the access to RDF data on the Semantic Web is possible by applying the Linked Data principles3, and the SPARQL specification4, which enables clients to access and retrieve data stored and published via SPARQL endpoints. RDF resources in the Semantic Web are interconnected and often correspond to previously created vocabularies and patterns. This way of reusing existing knowledge facilitates the modeling and representation of information and may optimally reduce the development costs of a knowledge base. As a result of the collaborative reuse process, structural and content interferences as well as varying models and contradictory statements are inevitable.	Decentralized Collaborative Knowledge Management using Git	NA:NA	2018
Lucy McKenna:Christophe Debruyne:Declan O'Sullivan	As the Web of Data grows, so does the need to establish the quality and trustworthiness of its contents. Increasing numbers of libraries are publishing their metadata as Linked Data (LD). As these institutions are considered authoritative sources of information, it is likely that library LD will be treated with increased credibility over data published by other sources. However, in order to establish this trust, the provenance of library LD must be provided. In 2018 we conducted a survey which explored the position of Information Professionals (IPs), such as librarians, archivists and cataloguers, with regards to LD. Results indicated that IPs find the process of LD interlinking to be a particularly challenging. In order to publish authoritative interlinks, provenance data for the description and justification of the links is required. As such, the goal of this research is to provide a provenance model for the LD interlinking process that meets the requirements of library metadata standards. Many current LD technologies are not accessible to non-technical experts or attuned to the needs of the library domain. By designing a model specifically for libraries, with input from IPs, we aim to facilitate this domain in the process of creating interlink provenance data.	Modelling the Provenance of Linked Data Interlinks for the Library Domain	NA:NA:NA	2018
Anisa Rula:Matteo Palmonari:Simone Rubinacci:Axel-Cyrille Ngonga Ngomo:Jens Lehmann:Andrea Maurino:Diego Esteves	Some facts in the Web of Data are only valid within a certain time interval. However, most of the knowledge bases available on the Web of Data do not provide temporal information explicitly. Hence, the relationship between facts and time intervals is often lost. A few solutions are proposed in this field. Most of them are concentrated more in extracting facts with time intervals rather than trying to map facts with time intervals. This paper studies the problem of determining the temporal scopes of facts, that is, deciding the time intervals in which the fact is valid. We propose a generic approach which addresses this problem by curating temporal information of facts in the knowledge bases. Our proposed framework, Temporal Information Scoping (TISCO) exploits evidence collected from the Web of Data and the Web. The evidence is combined within a three-step approach which comprises matching, selection and merging. This is the first work employing matching methods that consider both a single fact or a group of facts at a time. We evaluate our approach against a corpus of facts as input and different parameter settings for the underlying algorithms. Our results suggest that we can detect temporal information for facts from DBpedia with an f-measure of up to 80%.	TISCO: Temporal Scoping of Facts	NA:NA:NA:NA:NA:NA:NA	2018
Mayesha Tasnim:Diego Collarana:Damien Graux:Fabrizio Orlandi:Maria-Esther Vidal	Knowledge graphs are dynamic in nature, new facts about an entity are added or removed over time. Therefore, multiple versions of the same knowledge graph exist, each of which represents a snapshot of the knowledge graph at some point in time. Entities within the knowledge graph undergo evolution as new facts are added or removed. The problem of automatically generating a summary out of different versions of a knowledge graph is a long-studied problem. However, most of the existing approaches are limited to a pairwise version comparison. This limitation makes it difficult to capture a complete evolution out of several versions of the same knowledge graph. To overcome this limitation, we envision an approach to create a summary graph capturing temporal evolution of entities across different versions of a knowledge graph. The entity summary graphs may then be used for documentation generation, profiling or visualization purposes. First, we take different temporal versions of a knowledge graph and convert them into RDF molecules. Secondly, we perform Formal Concept Analysis on these molecules to generate summary information. Finally, we apply a summary fusion policy in order to generate a compact summary graph which captures the evolution of entities.	Summarizing Entity Temporal Evolution in Knowledge Graphs	NA:NA:NA:NA:NA	2018
Brecht Van de Vyvere:Pieter Colpaert:Erik Mannens:Ruben Verborgh	For better traffic flow and making better policy decisions, the city of Antwerp is connecting traffic lights to the Internet. The live “time to green” only tells a part of the story: also the historical values need to be preserved and need to be made accessible to everyone. We propose (i) an ontology for describing the topology of an intersection and the signal timing of traffic lights, (ii) a specification to publish these historical and live data with Linked Data Fragments and (iii) a method to preserve the published data in the long-term. We showcase the applicability of our specification with the opentrafficlights.org project where an end-user can see the live count-down as well as a line chart showing the historic “time to green” of a traffic light. We found that publishing traffic lights data as time sorted Linked Data Fragments allow synchronizing and reusing an archive to retrieve historical observations. Long-term preservation with tape storage becomes feasible when archives shift from byte preservation to knowledge preservation by combining Linked Data Fragments. This is a print-version of a paper first written for the Web. The Web-version is available at https://brechtvdv.github.io/Article-Open-Traffic-Lights	Open traffic lights: a strategy for publishing and preserving traffic lights data	NA:NA:NA:NA	2018
		MisInfo '19 Chairs' Welcome		2018
Miriam Metzger, UC Santa Barbara	Credibility research in the social sciences has a long history that may be particularly informative to today's efforts to combat misinformation and disinformation online. This keynote address will discuss the ways in which credibility has been studied in the disciplines of communication and psychology, including both how this notion has been conceptually and operationally defined. Key research findings will be presented with the aim of understanding what kinds of computational algorithms, tools, systems, and applications for tackling misinformation and disinformation are more versus less likely to be effective. This will help to answer a very important yet often overlooked question, which is: To what extent is misinformation a problem of information or a problem of human information processing? The answer to this question is crucial for both software developers and designers of educational intervention efforts to minimize the negative societal impacts of fabricated news and information flowing over the Internet and on social media. • Applied computing∼Psychology	Looking Backward to See a Way Forward	NA	2018
Sonia Castelo:Thais Almeida:Anas Elghafari:Aécio Santos:Kien Pham:Eduardo Nakamura:Juliana Freire	Fake news and misinformation have been increasingly used to manipulate popular opinion and influence political processes. To better understand fake news, how they are propagated, and how to counter their effect, it is necessary to first identify them. Recently, approaches have been proposed to automatically classify articles as fake based on their content. An important challenge for these approaches comes from the dynamic nature of news: as new political events are covered, topics and discourse constantly change and thus, a classifier trained using content from articles published at a given time is likely to become ineffective in the future. To address this challenge, we propose a topic-agnostic (TAG) classification strategy that uses linguistic and web-markup features to identify fake news pages. We report experimental results using multiple data sets which show that our approach attains high accuracy in the identification of fake news, even as topics evolve over time.	A Topic-Agnostic Approach for Identifying Fake News Pages	NA:NA:NA:NA:NA:NA:NA	2018
Sameer Dhoju:Md Main Uddin Rony:Muhammad Ashad Kabir:Naeemul Hassan	The spread of ‘fake’ health news is a big problem with even bigger consequences. In this study, we examine a collection of health-related news articles published by reliable and unreliable media outlets. Our analysis shows that there are structural, topical, and semantic patterns which are different in contents from reliable and unreliable media outlets. Using machine learning, we leverage these patterns and build classification models to identify the source (reliable or unreliable) of a health-related news article. Our model can predict the source of an article with an F-measure of 96%. We argue that the findings from this study will be useful for combating the health disinformation problem.	Differences in Health News from Reliable and Unreliable Media	NA:NA:NA:NA	2018
Ludivine Duroyon:François Goasdoué:Ioana Manolescu:François Goasdoué:Ioana Manolescu	A frequent journalistic fact-checking scenario is concerned with the analysis of statements made by individuals, whether in public or in private contexts, and the propagation of information and hearsay (“who said/knew what when”). Inspired by our collaboration with fact-checking journalists from Le Monde, France’s leading newspaper, we describe here a Linked Data (RDF) model, endowed with formal foundations and semantics, for describing facts, statements, and beliefs. Our model combines temporal and belief dimensions to trace propagation of knowledge between agents along time, and can answer a large variety of interesting questions through RDF query evaluation. A preliminary feasibility study of our model incarnated in a corpus of tweets demonstrates its practical interest.	A Linked Data Model for Facts, Statements and Beliefs	NA:NA:NA:NA:NA	2018
Casper Hansen:Christian Hansen:Stephen Alstrup:Jakob Grue Simonsen:Christina Lioma	Automatic fact-checking systems detect misinformation, such as fake news, by (i) selecting check-worthy sentences for fact-checking, (ii) gathering related information to the sentences, and (iii) inferring the factuality of the sentences. Most prior research on (i) uses hand-crafted features to select check-worthy sentences, and does not explicitly account for the recent finding that the top weighted terms in both check-worthy and non-check-worthy sentences are actually overlapping [15]. Motivated by this, we present a neural check-worthiness sentence ranking model that represents each word in a sentence by both its embedding (aiming to capture its semantics) and its syntactic dependencies (aiming to capture its role in modifying the semantics of other terms in the sentence). Our model is an end-to-end trainable neural network for check-worthiness ranking, which is trained on large amounts of unlabelled data through weak supervision. Thorough experimental evaluation against state of the art baselines, with and without weak supervision, shows our model to be superior at all times (+13% in MAP and +28% at various Precision cut-offs from the best baseline with statistical significance). Empirical analysis of the use of weak supervision, word embedding pretraining on domain-specific data, and the use of syntactic dependencies of our model reveals that check-worthy sentences contain notably more identical syntactic dependencies than non-check-worthy sentences.	Neural Check-Worthiness Ranking with Weak Supervision: Finding Sentences for Fact-Checking	NA:NA:NA:NA:NA	2018
Naeemul Hassan:Mohammad Yousuf:Md Mahfuzul Haque:Javier A. Suarez Rivas:Md Khadimul Islam	This study explores an online fact-checking community called politicalfactchecking on reddit.com that relies on crowdsourcing to find and verify check-worthy facts relating to U.S. politics. The community embodies a network journalism model in which the process of finding and verifying check-worthy facts through crowdsourcing is coordinated by a team of moderators. Applying the concepts of connective journalism, this study analyzed the posts (N = 543) and comments (N = 10, 221) on the community’s Reddit page to understand differences in roles of the community members and the moderators. A mixed-method approach was used to analyze the data. The authors also developed an automated argument classification model to analyze the contents and identify ways to automate parts of the process. The findings suggest that a model consisting of crowds, professionals, and computer-assisted analysis could increase efficiency and decrease costs in news organizations that involve fact-checking.	Examining the Roles of Automation, Crowds and Professionals Towards Sustainable Fact-checking	NA:NA:NA:NA:NA	2018
Luca Luceri:Ashok Deb:Adam Badawy:Emilio Ferrara	Recent research brought awareness of the issue of bots on social media and the significant risks of mass manipulation of public opinion in the context of political discussion. In this work, we leverage Twitter to study the discourse during the 2018 US midterm elections and analyze social bot activity and interactions with humans. We collected 2.6 million tweets for 42 days around the election day from nearly 1 million users. We use the collected tweets to answer three research questions: (i) Do social bots lean and behave according to a political ideology? (ii) Can we observe different strategies among liberal and conservative bots? (iii) How effective are bot strategies in engaging humans? We show that social bots can be accurately classified according to their political leaning and behave accordingly. Conservative bots share most of the topics of discussion with their human counterparts, while liberal bots show less overlap and a more inflammatory attitude. We studied bot interactions with humans and observed different strategies. Finally, we measured bots embeddedness in the social network and the extent of human engagement with each group of bots. Results show that conservative bots are more deeply embedded in the social network and more effective than liberal bots at exerting influence on humans.	Red Bots Do It Better:Comparative Analysis of Social Bot Partisan Behavior	NA:NA:NA:NA	2018
Caio Machado:Beatriz Kira:Vidya Narayanan:Bence Kollanyi:Philip Howard	There are rising concerns over the spread of misinformation in WhatsApp groups and the potential impact on political polarization, hindrance of public debate and fostering acts of political violence. As social media use becomes increasingly widespread, it becomes imperative to study how these platforms can be used to as a tool to spread propaganda and manipulate audience groups ahead of important political events. In this paper, we present a grounded typology to classify links to news sources into different categories including ‘junk’ news sources that deliberately publish or aggregate misleading, deceptive or incorrect information packaged as real news about politics, economics or culture obtained from public WhatsApp groups. Further, we examine a sample of 200 videos and images, extracted from a sample of WhatsApp groups and develop a new typology to classify this media content. For our analysis, we have used data from 130 public WhatsApp groups in the period leading up to the two rounds of the 2018 Brazilian presidential elections.	A Study of Misinformation in WhatsApp groups with a focus on the Brazilian Presidential Elections.	NA:NA:NA:NA:NA	2018
Jonathan Stray	How should an organized response to disinformation proceed in a 21st century democratic society? At the highest level, what strategies are available? This paper attempts to answer these questions by looking at what three contemporary counter-disinformation organizations are actually doing, then analyzing their tactics. The EU East StratCom Task Force is a contemporary government counter-propaganda agency. Facebook has made numerous changes to its operations to try to combat disinformation, and is a good example of what platforms can do. The Chinese information regime is a marvel of networked information control, and provokes questions about what a democracy should and should not do. The tactics used by these organizations can be grouped into six high level strategies: refutation, exposure of inauthenticity, alternative narratives, algorithmic filter manipulation, speech laws, and censorship. I discuss the effectiveness and political legitimacy of these approaches when used within a democracy with an open Internet and a free press.	Institutional Counter-disinformation Strategies in a Networked Democracy	NA	2018
Christopher R. Walker:Sara-Jayne Terp:Pablo C. Breuer:Courtney L. Crooks, PhD	State actors, private influence operators and grassroots groups are all exploiting the openness and reach of the Internet to manipulate populations at a distance, extending their decades-long struggle for “hearts and minds” via propaganda, influence operations and information warfare. Computational propaganda fueled by AI makes matters worse. The structure and propagation patterns of these attacks have many similarities to those seen in information security and computer hacking. The Credibility Coalition's MisinfosecWG working group is analyzing those similarities, including information security frameworks that could give the truth-based community better ways to describe, identify and counter misinformation-based attacks.  Specifically, we place misinformation components into a framework commonly used to describe information security incidents. We anticipate that our work will give responders the ability to transfer other information security principles to the misinformation sphere, and to plan defenses and countermoves .	Misinfosec	NA:NA:NA:NA	2018
Martin Atzmueller:Alvin Chin:Christoph Trattner		International Workshop on Modeling Social Media (MSM'2019) Chairs' Welcome	NA:NA:NA	2018
Alessia Antelmi:Delfina Malandrino:Vittorio Scarano	Online Social Networks (OSNs) represent a fertile field to collect real user data and to explore OSNs user behavior. Recently, two topics are drawing the attention of researchers: the evolution of online social roles and the question of participation inequality. In this work, we bring these two fields together to study and characterize the behavioral evolution of OSNs users according to the quantity and the typology of their social interactions. We found that online participation on the microblogging platform can be categorized into four different activity levels. Furthermore, we empirically verified that the 90-9-1 rule of thumb about participation inequality is not an accurate representation of reality. Findings from our analysis reveal that lurkers are less than expected: they are not 9 out of 10 as suggested by Nielsen, but 3 out of 4. This represents a significant result that can give new insights on how users relate with social media and how their use is evolving towards a more active interaction with the new generation of consumers.	Characterizing the Behavioral Evolution of Twitter Users and The Truth Behind the 90-9-1 Rule	NA:NA:NA	2018
Makan Arastuie:Kevin S. Xu	Understanding mechanisms driving link formation in dynamic social networks is a long-standing problem that has implications to understanding social structure as well as link prediction and recommendation. Social networks exhibit a high degree of transitivity, which explains the successes of common neighbor-based methods for link prediction. In this paper, we examine mechanisms behind link formation from the perspective of an ego node. We introduce the notion of personalized degree for each neighbor node of the ego, which is the number of other neighbors a particular neighbor is connected to. From empirical analyses on four on-line social network datasets, we find that neighbors with higher personalized degree are more likely to lead to new link formations when they serve as common neighbors with other nodes, both in undirected and directed settings. This is complementary to the finding of Adamic and Adar [1] that neighbor nodes with higher (global) degree are less likely to lead to new link formations. Furthermore, on directed networks, we find that personalized out-degree has a stronger effect on link formation than personalized in-degree, whereas global in-degree has a stronger effect than global out-degree. We validate our empirical findings through several link recommendation experiments and observe that incorporating both personalized and global degree into link recommendation greatly improves accuracy.	Personalized Degrees: Effects on Link Formation in Dynamic Networks from an Egocentric Perspective	NA:NA	2018
Martin Atzmueller	The detection of anomalies and exceptional patterns in social interaction networks is a prominent research direction in data mining and network science. For anomaly detection, typically two questions need to be addressed and defined: (1) What is an anomaly? (2) How do we identify an anomaly? This paper discusses model-based approaches and methods for addressing and formalizing these issues in the context of feature-rich social interaction networks. It provides a categorization of model-based approaches and provides perspectives and first promising directions for its implementation.	Onto Model-based Anomalous Link Pattern Mining on Feature-Rich Social Interaction Networks	NA	2018
Johannes Beck:Roberta Huang:David Lindner:Tian Guo:Zhang Ce:Dirk Helbing:Nino Antulov-Fantulin	The ability to track and monitor relevant and important news in real-time is of crucial interest in multiple industrial sectors. In this work, we focus on cryptocurrency news, which recently became of emerging interest to the general and financial audience. In order to track popular news in real-time, we (i) match news from the web with tweets from social media, (ii) track their intraday tweet activity and (iii) explore different machine learning models for predicting the number of article mentions on Twitter after its publication. We compare several machine learning models, such as linear extrapolation, linear and random forest autoregressive models, and a sequence-to-sequence neural network.	Sensing Social Media Signals for Cryptocurrency News	NA:NA:NA:NA:NA:NA:NA	2018
Adele Lu Jia:Xiaoxue Shen:Siqi Shen:Yongquan Fu:Liwen Peng	User generated video systems like YouTube and Twitch.tv have been a major internet phenomenon. They have attracted a vast user base with their many and varied contents provided by their users, and a series of social features tailored for online viewing. In hoping for building a more lively community and encouraging the content creators to share more, recently many such systems have introduced crowdsourcing mechanisms wherein creators get tangible rewards through user donations. User donation is a very special form of user relationships. It influences user engagement in the community, and has a great impact on the success of these systems. However, user donations and donation relationships remain trade secrets for most enterprises and to date are still unexplored. It is not clear at what scale are the donations or how users donate in these systems. In this work, we attempt to fill this gap. We obtain and provide a publicly available dataset on user donations in BiliBili, a popular user generated video system in China with 76.4 million average monthly active users. Based on detailed information on over 5 million videos, over 700 thousand content creators, and over 1.5 million user donations, we quantitatively reveal the characteristics of user donations, we examine their correlations with the upload behavior and content popularity of the creators, and we adopt machine-learned classifiers to accurately predict the creators who will receive donations and who will donate in the future.	User Donations in a User Generated Video System	NA:NA:NA:NA:NA	2018
Ke Li:Bin Guo:Qiuyun Zhang:Jianping Yuan:Zhiwen Yu	Recently, content polluters post malicious information in Online Social Networks (OSNs), which is a more and more serious problem that poses a serious threat to the privacy information, account security, user experience, etc. They continuously simulate the behaviors of legitimate accounts in various ways, and evade detection systems against them. In this paper, we focus on one kind of content polluter, namely collective content polluter (hereinafter referred to as CCP). Existing works either focus on individual polluters or require long periods of data records for detection, making their detection methods less robust and lagging behind. It is thus necessary to analyze the characteristics of collective content polluters and study the methods for early detection. This paper proposes a CCP early detection method called CrowdGuard. It analyzes the crowd behaviors of collective content polluters and legitimate accounts, extracts distinctive features, and leverages the Gaussian Mixture Model (GMM) method to cluster the two groups of accounts (legitimate users and polluters) to achieve early detection. Using the public dataset including thousands of collective content polluters on Twitter about a political election, we design an experimental scenario simulating early detection and evaluate the performance of CrowdGuard. The results show that CrowdGuard outperforms existing methods and is adequate for early detection.	CrowdGuard: Characterization and Early Detection of Collective Content Polluters in Online Social Networks	NA:NA:NA:NA:NA	2018
Alberto Purpura:Chiara Masiero:Gianmaria Silvello:Gian Antonio Susto	Emotion Classification (EC) aims at assigning an emotion label to a textual document with two inputs – a set of emotion labels (e.g. anger, joy, sadness) and a document collection. The best performing approaches for EC are dictionary-based and suffer from two main limitations: (i) the out-of-vocabulary (OOV) keywords problem and (ii) they cannot be used across heterogeneous domains. In this work, we propose a way to overcome these limitations with a supervised approach based on TF-IDF indexing and Multinomial Linear Regression with Elastic-Net regularization to extract an emotion lexicon and classify short documents from diversified domains. We compare the proposed approach to state-of-the-art methods for document representation and classification by running an extensive experimental study on two shared and heterogeneous data sets.	Supervised Lexicon Extraction for Emotion Classification	NA:NA:NA:NA	2018
Soumajyoti Sarkar:Hamidreza Alvari:Paulo Shakarian	Information diffusion mechanisms based on social influence models are mainly studied using likelihood of adoption when active neighbors expose a user to a message. The problem arises primarily from the fact that for the most part, this explicit information of who-exposed-whom among a group of active neighbors in a social network, before a susceptible node is infected is not available. In this paper, we attempt to understand the diffusion process through information cascades by studying the temporal network structure of the cascades. In doing so, we accommodate the effect of exposures from active neighbors of a node through a network pruning technique that leverages network motifs to identify potential infectors responsible for exposures from among those active neighbors. We attempt to evaluate the effectiveness of the components used in modeling cascade dynamics and especially whether the additional effect of the exposure information is useful. Following this model, we develop an inference algorithm namely InferCut, that uses parameters learned from the model and the exposure information to predict the actual parent node of each potentially susceptible user in a given cascade. Empirical evaluation on a real world dataset from Weibo social network demonstrate the significance of incorporating exposure information in recovering the exact parents of the exposed users at the early stages of the diffusion process.	Leveraging Motifs to Model the Temporal Dynamics of Diffusion Networks	NA:NA:NA	2018
Zizhu Zhang:Weiliang Zhao:Jian Yang:Cecile Paris:Surya Nepal	Influence diffusion has been widely studied in social networks for applications such as service promotion and marketing. There are two challenging issues here: (1) how we measure people’s influence on others; (2) how we predict whom would be influenced by a particular person and when people would be influenced. Existing works have not captured the temporal and structural characteristics of influence diffusion in Twitter. In this paper, we firstly develop a model to learn influence probabilities between users in Twitter from their action history; secondly, we introduce diffusion models that are used to predict how information is propagated in Twitter. Experiment results show that our proposed models outperform existing models in terms of the balanced precision and recall.	Learning Influence Probabilities and Modelling Influence Diffusion in Twitter	NA:NA:NA:NA:NA	2018
		Workshop on Subjectivity, Ambiguity and Disagreement on the Web (SAD2019)		2018
Brad Klingenberg	Most recommendation algorithms produce results without humans-in-the-loop. Combining algorithms with expert human curation can make recommendations much more effective, especially in hard-to-quantify domains like fashion. But it also makes things more complicated, introducing new sources of statistical bias and challenges for traditional approaches to training and evaluating algorithms. Humans and machines can also disagree, further complicating the design of production systems. In this talk I'll share lessons from combining algorithms and human judgement for personal styling recommendations at Stitch Fix, an online personal styling service that commits committed to its recommendations through the physical delivery of merchandise to clients.	Humans, Machines and Disagreement	NA	2018
Maria Stone	To understand why and how subjectivity and disagreement in label collection matters or doesn't matter I examine the history of systems evaluations and measurement performed by humans and trace the roots of human computation/crowdsourcing and the context in which it arose.   Before we can begin fruitful discussions about subjectivity and disagreement we need to ask ourselves what/who it is that human raters are supposed to represent. I offer multiple different perspectives and scenarios that showcase just how varied and ill-defined the role of a human rater can be. I will conclude with some practical recommendations with respect to the questions researchers and practitioners ought to ask themselves before employing human raters, and some challenges with both methodology of such data collection and subsequent analysis of such data.	What we talk about when we talk about crowdsourcing	NA	2018
Lora Aroyo:Lucas Dixon:Nithum Thain:Olivia Redfield:Rachel Rosen	Discussing things you care about can be difficult, especially via online platforms, where sharing your opinion leaves you open to the real and immediate threats of abuse and harassment. Due to these threats, people stop expressing themselves and give up on seeking different opinions. Recent research efforts focus on examining the strengths and weaknesses (e.g. potential unintended biases) of using machine learning as a support tool to facilitate safe space for online discussions; for example, through detecting various types of negative online behaviors such as hate speech, online harassment, or cyberbullying. Typically, these efforts build upon sentiment analysis or spam detection in text. However, the toxicity of the language could be a strong indicator for the intensity of the negative behavior. In this paper, we study the topic of toxicity in online conversations by addressing the problems of subjectivity, bias, and ambiguity inherent in this task. We start with an analysis of the characteristics of subjective assessment tasks (e.g. relevance judgment, toxicity judgment, sentiment assessment, etc). Whether we perceive something as relevant or as toxic can be influenced by almost infinite amounts of prior or current context, e.g. culture, background, experiences, education, etc. We survey recent work that tries to understand this phenomenon, and we outline a number of open questions and challenges which shape the research perspectives in this multi-disciplinary field.	Crowdsourcing Subjective Tasks: The Case Study of Understanding Toxicity in Online Discussions	NA:NA:NA:NA:NA	2018
Alyssa Lees:Chris Welty	Crowdsourcing systems increasingly rely on users to provide more subjective ground truth for intelligent systems - e.g. ratings, aspect of quality and perspectives on how expensive or lively a place feels, etc. We focus on the ubiquitous implementation of online user ordinal voting (e.g 1-5, 1 star-4 stars) on some aspect of an entity, to extract a relative truth, measured by a selected metric such as vote plurality or mean. We argue that this methodology can aggregate results that yield little information to the end user. In particular, ordinal user rankings often converge to a indistinguishable rating. This is demonstrated by the trend in certain cities for the majority of restaurants to all have a 4 star rating. Similarly, the rating of an establishment can be significantly affected by a few users [10]. User bias in voting is not spam, but rather a preference that can be harnessed to provide more information to users. We explore notions of both global skew and user bias. Leveraging these bias and preference concepts, the paper suggests explicit models for better personalization and more informative ratings.	Discovering User Bias in Ordinal Voting Systems	NA:NA	2018
Tong Liu:Akash Venkatachalam:Pratik Sanjay Bongale:Christopher Homan	Machine learning problems are often subjective or ambiguous. That is, humans solving the same problems might come to legitimate but completely different conclusions, based on their personal experiences and beliefs. In supervised learning, particularly when using crowdsourced training data, multiple annotations per data item are usually reduced to a single label representing ground truth. This hides a rich source of diversity and subjectivity of opinions about the labels. Label distribution learning associates for each data item a probability distribution over the labels for that item, thus can preserve the diversity that conventional learning hides or ignores. We introduce a strategy for learning label distributions with only five-to-ten labels per item by aggregating human-annotated labels over multiple, semantically related data items. Our results suggest that specific label aggregation methods can help provide reliable representative semantics at the population level.	Learning to Predict Population-Level Label Distributions	NA:NA:NA:NA	2018
V. K. Chaithanya Manam:Dwarakanath Jampani:Mariam Zaim:Meng-Han Wu:Alexander J. Quinn	Developing instructions for microtask crowd workers requires time to ensure consistent interpretations by crowd workers. Even with substantial effort, workers may still misinterpret the instructions due to ambiguous language and structure in the task design. Prior work demonstrated methods for facilitating iterative improvement with help from the requester. However, any participation by the requester reduces the time saved by delegating the work—and hence the utility of using crowdsourcing. We present TaskMate, a system for facilitating worker-led refinement of task instructions with minimal involvement by the requester. Small teams of workers search for ambiguities and vote on the interpretation they believe the requester intended. This paper describes the workflow, our implementation, and our preliminary evaluation.	TaskMate: A Mechanism to Improve the Quality of Instructions in Crowdsourcing	NA:NA:NA:NA:NA	2018
Mike Schaekermann:Graeme Beaton:Minahz Habib:Andrew Lim:Kate Larson:Edith Law	Group-based discussion among human graders can be a useful tool to capture sources of disagreement in ambiguous classification tasks and to adjudicate any resolvable disagreements. Existing workflows for panel-based adjudication, however, capture graders’ arguments and rationales in a free-form, unstructured format, limiting the potential for automatic analysis of the discussion contents. We designed and implemented a structured adjudication system that collects graders’ arguments in a machine-readable format without limiting graders’ abilities to provide free-form justifications for their classification decisions. Our system enables graders to cite instructions from a set of labeling guidelines, specified in the form of discrete classification rules and conditions that need to be met in order for each rule to be applicable. In the present work, we outline the process of designing and implementing this adjudication system, and report preliminary findings from deploying our system in the context of medical time series analysis for sleep stage classification.	Capturing Expert Arguments from Medical Adjudication Discussions in a Machine-readable Format	NA:NA:NA:NA:NA:NA	2018
Rafael Zequeira Jiménez:Anna Llagostera:Babak Naderi:Sebastian Möller:Jens Berger	Crowdsourcing is a great tool for conducting subjective user studies with large amounts of users. Collecting reliable annotations about the quality of speech stimuli is challenging. The task itself is of high subjectivity and users in crowdsourcing work without supervision. This work investigates the intra- and inter-listener agreement withing a subjective speech quality assessment task. To this end, a study has been conducted in the laboratory and in crowdsourcing in which listeners were requested to rate speech stimuli with respect to their overall quality. Ratings were collected on a 5-point scale in accordance with the ITU-T Rec. P.800 and P.808, respectively. The speech samples were taken from the database ITU-T Rec. P.501 Annex D, and were presented four times to the listeners. Finally, the crowdsourcing results were contrasted to the ratings collected in the laboratory. Strong and significant Spearman’s correlation was achieved when contrasting the ratings collected in both environments. Our analysis show that while the inter-rater agreement increased the more the listeners conducted the assessment task, the intra-rater reliability remained constant. Our study setup helped to overcome the subjectivity of the task and we found that disagreement can represent a source of information to some extent.	Intra- and Inter-rater Agreement in a Subjective Speech Quality Assessment Task in Crowdsourcing	NA:NA:NA:NA:NA	2018
		TempWeb 2019 Chairs' Welcome		2018
Omar Alonso	Twitter and Facebook continue to be top destinations for information consumption on the Internet. The ever-expanding social graph based enables the implementation of traditional features like item recommendation and selection of trending content that rely on human input and other behavioral data. However, given the enormous amount of human sensing in the world at any given moment in any platform, there is a lot of untapped potential that goes beyond simple applications on top of atomic level content like a post or tweet. In this talk we describe a social knowledge graph that discover relationships as they occur over time and how it can be used to capture the evolution of events or stories.	Large Scale Human Sensing Over Time. Challenges and Lessons Learned	NA	2018
Melisachew Wudage Chekol:Giuseppe Pirrò:Heiner Stuckenschmidt	Knowledge graphs enriched with temporal information are becoming more and more common. As an example, the Wikidata KG contains millions of temporal facts associated with validity intervals (i.e., start and end time) covering a variety of domains. While these facts are interesting, computing temporal relations between their intervals allows to discover temporal relations holding between facts (e.g., “football players that get divorced after moving from a team to another”). In this paper we study the problem of computing different kinds of interval joins in temporal KGs. In principle, interval joins can be computed by resorting to query languages like SPARQL. However, this language is not optimized for such a task, which makes it hard to answer real-world queries. For instance, the query “find players that were married while being member of a team” times out on Wikidata. We present efficient algorithms to compute interval joins for the main Allen’s relations (e.g., before, after, during, meets). We also address the problem of interval coalescing, which is used for merging contiguous or overlapping intervals of temporal facts, and propose an efficient algorithm. We integrate our interval joins and coalescing algorithms into a light SPARQL extension called iSPARQL. We evaluated the performance of our algorithms on real-world temporal kgs.	Fast Interval Joins for Temporal SPARQL Queries	NA:NA:NA	2018
Ioannis Dikeoulias:Jannik Strötgen:Simon Razniewski	Knowledge bases (KBs) contain huge amounts of facts about entities, their properties, and relations between them. They are thus the key asset in any intelligent system for tasks such as structured search and question answering. However, due to dynamics in the real world, properties and relations change over time, and stored knowledge may become outdated. While KB information evolves steadily, there is no information whether or not a KB property might be subject to change with high probability or whether it is likely to be stable. Systems exploiting KB information, however, could benefit a lot if they had access to this kind of information. In this paper, we analyze and predict the stability of KB entries, which allows to accompany entries with stability scores. Our predictive model exploits entity-based features and learns through historic data. A particular challenge to determine stability scores is that KB entries are not only added or modified due to real-world changes but also to reduce the incompleteness of KBs in general. Nevertheless, our evaluation of sample properties demonstrates the effectiveness of our method for predicting the one-year stability of KB properties.	Epitaph or Breaking News? Analyzing and Predicting the Stability of Knowledge Base Properties	NA:NA:NA	2018
Lukas Lange:Omar Alonso:Jannik Strötgen	Temporal information extracted from texts and normalized to some standard format has been exploited in a variety of tasks such as information retrieval and question answering. Classifying documents into categories using temporal features has not yet been tried. Such a method might be particularly valuable when classifying sensitive texts such as patient records, i.e., whenever the pure content of the documents should not be used for the classification. In this paper, we describe, as a proof-of-concept, our work on classifying news articles exploiting only features defined over extracted and normalized temporal expressions. Our evaluation of two classification models on large German and English news archives shows promising results and demonstrates the discriminative power of temporal features for topically classifying text documents.	The Power of Temporal Features for Classifying News Articles	NA:NA:NA	2018
Behrooz Mansouri:Mohammad Sadegh Zahedi:Ricardo Campos:Mojgan Farhoodi	As video games are developing fast, many users issue queries related to video games in a daily fashion. While there were a few attempts to understand their behavior, little is known on how the video game-related searches are done. Digesting and analyzing this search behavior may thus be faced as an important contribution for search engines to provide better results and search services for their users. To overcome this lack of knowledge and to gain more insight into how video game searches are done, we analyze in this paper, a number of game search queries submitted to a general search engine named Parsijoo. The analysis conducted was performed on top of 372,508 game search records extracted from the query logs within 253,516 different search sessions. Different aspects of video game searches are studied, including, their temporal distribution, game version specification, popular game categories, popular game platforms, game search sessions and clicked pages. Overall, the experimental analysis on video game searches shows that the current retrieval methods used by traditional search engines cannot be applied for game searches, thus, different retrieval and search services should be considered for these searches in the future.	Exploring Video Game Searches on the Web	NA:NA:NA:NA	2018
		Wiki Workshop Chairs' Welcome		2018
Chuankai An:Daniel N. Rockmore	In this paper we present a proof-of-concept of a visual navigation tool for a personalized “sandbox” of Wiki pages. The navigation tool considers multiple groups of algorithmic parameters and adapts to user activity via graphical user interfaces. The output is a 2D map of a subset of Wikipedia pages network which provides a different and broader visual representation – a map – in the neighborhood (according to some metric) of the pages around the page currently displayed in a browser. The representation schema includes the incorporation of a kind of transparency in the algorithmic parameters affecting the presentation of the landscape visualization, which in turn enables the delivery of a personalized canvas, designed by the user. A case study shows the combination of four different sourcing (i.e., identification and extraction of the neighboring pages) rules and three layouts over the same Wikipedia subnetwork. The basic schema is readily adapted to other search experiences and contexts.	Open Personalized Navigation on the Sandbox of Wiki Pages	NA:NA	2018
James Ashford:Liam Turner:Roger Whitaker:Alun Preece:Diane Felmlee:Don Towsley	Wikipedia serves as a good example of how editors collaborate to form and maintain an article. The relationship between editors, derived from their sequence of editing activity, results in a directed network structure called the revision network, that potentially holds valuable insights into editing activity. In this paper we create revision networks to assess differences between controversial and non-controversial articles, as labelled by Wikipedia. Originating from complex networks, we apply motif analysis, which determines the under or over-representation of induced sub-structures, in this case triads of editors. We analyse 21,631 Wikipedia articles in this way, and use principal component analysis to consider the relationship between their motif subgraph ratio profiles. Results show that a small number of induced triads play an important role in characterising relationships between editors, with controversial articles having a tendency to cluster. This provides useful insight into editing behaviour and interaction capturing counter-narratives, without recourse to semantic analysis. It also provides a potentially useful feature for future prediction of controversial Wikipedia articles.	Understanding the Signature of Controversial Wikipedia Articles through Motifs in Editor Revision Networks	NA:NA:NA:NA:NA:NA	2018
Nicolas Aspert:Volodymyr Miz:Benjamin Ricaud:Pierre Vandergheynst	Wikipedia is a rich and invaluable source of information. Its central place on the Web makes it a particularly interesting object of study for scientists. Researchers from different domains used various complex datasets related to Wikipedia to study language, social behavior, knowledge organization, and network theory. While being a scientific treasure, the large size of the dataset hinders pre-processing and may be a challenging obstacle for potential new studies. This issue is particularly acute in scientific domains where researchers may not be technically and data processing savvy. On one hand, the size of Wikipedia dumps is large. It makes the parsing and extraction of relevant information cumbersome. On the other hand, the API is straightforward to use but restricted to a relatively small number of requests. The middle ground is at the mesoscopic scale, when researchers need a subset of Wikipedia ranging from thousands to hundreds of thousands of pages but there exists no efficient solution at this scale. In this work, we propose an efficient data structure to make requests and access subnetworks of Wikipedia pages and categories. We provide convenient tools for accessing and filtering viewership statistics or “pagecounts” of Wikipedia web pages. The dataset organization leverages principles of graph databases that allows rapid and intuitive access to subgraphs of Wikipedia articles and categories. The dataset and deployment guidelines are available on the LTS2 website https://lts2.epfl.ch/Datasets/Wikipedia/.	A Graph-Structured Dataset for Wikipedia Research	NA:NA:NA:NA	2018
Preeti Bhargava:Nemanja Spasojevic:Sarah Ellinger:Adithya Rao:Abhinand Menon:Saul Fuhrmann:Guoning Hu	Recently much progress has been made in entity disambiguation and linking systems (EDL). Given a piece of text, EDL links words and phrases to entities in a knowledge base, where each entity defines a specific concept. Although extracted entities are informative, they are often too specific to be used directly by many applications. These applications usually require text content to be represented with a smaller set of predefined concepts or topics, belonging to a topical taxonomy, that matches their exact needs. In this study, we aim to build a system that maps Wikidata entities to such predefined topics. We explore a wide range of methods that map entities to topics, including GloVe similarity, Wikidata predicates, Wikipedia entity definitions, and entity-topic co-occurrences. These methods often predict entity-topic mappings that are reliable, i.e., have high precision, but tend to miss most of the mappings, i.e., have low recall. Therefore, we propose an ensemble system that effectively combines individual methods and yields much better performance, comparable with human annotators.	Learning to Map Wikidata Entities To Predefined Topics	NA:NA:NA:NA:NA:NA:NA	2018
Gil Domingues:Carla Teixeira Lopes	Wikipedia is the largest on-line collaborative encyclopedia, containing information from a plethora of fields, including medicine. It has been shown that Wikipedia is one of the top visited sites by readers looking for information on this topic. The large reliance on Wikipedia for this type of information drives research towards the analysis of the quality of its articles. In this work, we evaluate and compare the quality of medicine-related articles in the English and Portuguese Wikipedia. For that we use metrics such as authority, completeness, complexity, informativeness, consistency, currency and volatility, and domain-specific measurements, in order to evaluate and compare the quality of medicine related articles in the English and Portuguese Wikipedia. We were able to conclude that the English articles score better across most metrics than the Portuguese articles.	Characterizing and comparing Portuguese and English Wikipedia medicine-related articles	NA:NA	2018
Swati Goel:Ashton Anderson:Leila Zia	The Thanks feature on Wikipedia, also known as “Thanks”, is a tool with which editors can quickly and easily send one other positive feedback [1]. The aim of this project is to better understand this feature: its scope, the characteristics of a typical “Thanks” interaction, and the effects of receiving a thank on individual editors. We study the motivational impacts of “Thanks” because maintaining editor engagement is a central problem for crowdsourced repositories of knowledge such as Wikimedia. Our main findings are that most editors have not been exposed to the Thanks feature (meaning they have never given nor received a thank), thanks are typically sent upwards (from less experienced to more experienced editors), and receiving a thank is correlated with having high levels of editor engagement. Though the prevalence of “Thanks” usage varies by editor experience, the impact of receiving a thank seems mostly consistent for all users. We empirically demonstrate that receiving a thank has a strong positive effect on short-term editor activity across the board and provide preliminary evidence that thanks could compound to have long-term effects as well.	Thanks for Stopping By: A Study of “Thanks” Usage on Wikimedia	NA:NA:NA	2018
Chander J Iyer:Srinath Ravindran	Developing a deeper understanding of the travel domain is helpful for presenting users with consistent and reliable information, and few sources of data are able to achieve that. Further, such information can serve as background knowledge for evaluating machine learning algorithms. In this paper, we present part of our work towards developing such an understanding. We demonstrate a simple extraction technique and how the extracted data can be used to evaluate an unsupervised embedding model built on search queries with travel intent.	Understanding Travel from Web Queries using Domain Knowledge from Wikipedia	NA:NA	2018
Ali Javanmardi:Lu Xiao		What's in the Content of Wikipedia's Article for Deletion Discussions?	NA:NA	2018
Shaunak Mishra:Aasish Pappu:Narayan Bhamidipati	Online advertising platforms in partnerships with media companies typically have access to an online user’s history of viewed articles. If a concerned brand (advertiser) plans to run advertisement campaigns on users exposed to negative articles, it is essential to first identify articles with negative sentiment about the brand. For an advertising platform, scalable identification of such articles with little human-annotation effort is necessary for launching campaigns soon after an advertiser signs up. In this context, generic sentiment analysis tools suffer from the lack of contextual world knowledge associated with the advertiser. Human annotation of articles for supervised approaches is laborious and painstaking. To address these problems, we propose the use of publicly available Wikipedia footnote references for an advertiser, and propagate their sentiment to several articles related to the advertiser. In particular, our proposed approach has three components: (i) automatically find Wikipedia references which have negative sentiment about an advertiser, (ii) learn distributed representations (doc2vec) of article texts referred in footnotes and other unlabeled articles, and (iii) inferring sentiment in unlabeled articles using label propagation (from references) in the doc2vec space. Our experiments spanning three real brands, and data from a major advertising platform (Yahoo Gemini) show significant lifts in sentiment inference compared to existing baselines. In addition, we share valuable insights on how article sentiment influences the online activities of a user with respect to a brand.	Inferring Advertiser Sentiment in Online Articles using Wikipedia Footnotes	NA:NA:NA	2018
Charlotte Rudnik:Thibault Ehrhart:Olivier Ferret:Denis Teyssou:Raphael Troncy:Xavier Tannier	News agencies produce thousands of multimedia stories describing events happening in the world that are either scheduled such as sports competitions, political summits and elections, or breaking events such as military conflicts, terrorist attacks, natural disasters, etc. When writing up those stories, journalists refer to contextual background and to compare with past similar events. However, searching for precise facts described in stories is hard. In this paper, we propose a general method that leverages the Wikidata knowledge base to produce semantic annotations of news articles. Next, we describe a semantic search engine that supports both keyword based search in news articles and structured data search providing filters for properties belonging to specific event schemas that are automatically inferred.	Searching News Articles Using an Event Knowledge Graph Leveraged by Wikidata	NA:NA:NA:NA:NA:NA	2018
Mohsen Sayyadiharikandeh:Jonathan Gordon:Jose-Luis Ambite:Kristina Lerman	The increased availability of online learning resources in the form of courses, videos, and tutorials has created new opportunities for independent learners, but it has also increased the difficulty of planning a course of study. Where should the learner start? What should the learner know before tackling a new course? Manually identifying these prerequisite relations between learning resources or concepts is expensive in terms of time and expertise, and it is particularly difficult to do so for new or rapidly changing areas of knowledge. To address this challenge, we present a new method for identifying prerequisite relations based on naturally occurring data, namely the navigation patterns of users on the Wikipedia online encyclopedia. Our supervised learning approach shows that the navigation network structure can be used to identify dependencies among concepts in several domains.	Finding Prerequisite Relations using the Wikipedia Clickstream	NA:NA:NA:NA	2018
Khonzodakhon Umarova:Eni Mustafaraj	Increased polarization and partisanship have become a consistent state of politics, media, and society, especially in the United States. As many news publishers are perceived as “biased” and some others have come under attack as being “fake news”, efforts to make such labels stick have increased too. In some cases (e.g., InfoWars), the use of such labels is legitimate, because some online publishers deliberately spread conspiracy theories and false stories. Other news publishers are perceived as partisan and biased, in ways that damages their reporting credibility. Whether political bias affects journalism standards appears to be a debated topic with no clear consensus. Meanwhile, labels such as “far-left” or “alt-right” are highly contested and may become cause for prolonged edit wars on the Wikipedia pages of some news sources. In this paper, we try to shine a light into this phenomenon and its extent, in order to start a conversation within the Wikipedia community about transparent processes for assigning political orientation and journalistic reliability labels to news sources, especially to unfamiliar ones, which users would be more likely to verify by looking them up. As more of Wikipedia’s content is used outside Wikipedia’s “container” (e.g., in search results or by voice personal assistants), the issue of where certain statements appear in the Wikipedia page and their verifiability becomes an urgent one to consider not only by Wikipedia editors, but by third-party information providers too.	How Partisanship and Perceived Political Bias Affect Wikipedia Entries of News Sources	NA:NA	2018
Xiaoxi Chelsy Xie:Isaac Johnson:Anne Gomez	Understanding how various external campaigns or events affect readership on Wikipedia is important to efforts aimed at improving awareness and access to its content. In this paper, we consider how to build time-series models aimed at predicting page views on Wikipedia with the goal of detecting whether there are significant changes to the existing trends. We test these models on two different events: a video campaign aimed at increasing awareness of Hindi Wikipedia in India and the page preview feature roll-out—a means of accessing Wikipedia content without actually visiting the pages—on English and German Wikipedia. Our models effectively estimate the impact of page preview roll-out, but do not detect a significant change following the video campaign in India. We also discuss the utility of other geographies or language editions for predicting page views from a given area on a given language edition.	Detecting and Gauging Impact on Wikipedia Page Views	NA:NA:NA	2018
		The Third Workshop on Women in Web Data Science (WinDS '19) Chairs' Welcome & Organization Listing		2018
Anna Lisa Gentile	Information Extraction (IE) techniques enables us to distill Knowledge from the abundantly available unstructured content. Some of the basic IE methods include the automatic extraction of relevant entities from text (e.g. places, dates, people, ...), understanding relations among them, building semantic resources (dictionaries, ontologies) to inform the extraction tasks, connecting extraction results to standard classification resources. IE techniques cannot decouple from human input - at bare minimum some of the data needs to be manually annotated by a human so that automatic methods can learn patterns to recognize certain type of information. The human-in-the-loop paradigm applied to IE techniques focuses on how to better take advantage of human annotations (the recorded observations), how much interaction with the human is needed for each specific extraction task.	Information Extraction with Humans in the Loop	NA	2018
Lise Getoor	Data science is an emerging discipline that offers both promise and peril. Responsible data science refers to efforts that address both the technical and societal issues in emerging data-driven technologies. How can machine learning and AI systems reason effectively about complex dependencies and uncertainty? Furthermore, how do we understand the ethical and societal issues involved in data-driven decision-making? There is a pressing need to integrate algorithmic and statistical principles, social science theories, and basic humanist concepts so that we can think critically and constructively about the socio-technical systems we are building. In this talk, I will overview this emerging area.	Responsible Data Science	NA	2018
Oghenemaro Anuyah:Ion Madrazo Azpiazu:Nevena Dragovic:Maria Soledad Pera	As one of the largest communities that search for online resources, children are introduced to the Web at increasingly young ages [1]. However, popular search tools are not explicitly designed with children in mind nor do their retrieved results explicitly target children. Consequently, many young users struggle in completing successful searches, especially since most search engines (SE) do not directly support, or offer weak support, for children’s inquiry approaches [2]. Even though children, as inexperienced users, struggle with describing their information needs in a concise query, they still expect SEs to retrieve relevant information in response to their requirements. As part of their capabilities, SEs often suggest queries to aid users in better defining their information needs. In fact, a recent study conducted by Gossen et al. [3] shows that children pay more attention to suggested queries than adults. Unfortunately, these suggestions are not specifically tailored towards children and thus need improvement [5]. While there exist multiple query suggestion modules, only few specifically target children. To address this problem, along with a need for more children-related tools, we rely on ReQuIK (Recommendations based on Query Intention for Kids), a query suggestion module tailored towards 6-to-13 year old children (introduced in [4]). ReQuIK informs its suggestion process by applying (i) a strategy based on search intent to capture the purpose of a query [1], (ii) a ranking strategy based on a wide and deep neural network that considers both raw text and traits commonly associated with kid-related queries, (iii) a filtering strategy based on the readability levels of documents potentially retrieved by a query to favor suggestions that trigger the retrieval of documents matching children’s reading skills, and (iv) a content-similarity strategy to ensure diversity among suggestions. For assessing the quality of the system, we conducted initial offline and online experiments based on 591 queries written by 97 children, ages 6 to 13. The results of this assessment verified the correctness of ReQuIK’s recommendation strategy, the fact that it provides suggestions that appeal to children and ReQuIK’s ability to recommend queries that lead to the retrieval of materials with readability levels that correlate with children’s reading skills To the best of our knowledge, ReQuIK is the only available system that can be coupled with SEs to generate query recommendations for children, favoring those that lead to easier-to-read, child-related resources, which can improve SEs’ performance. The design of the proposed tool explicitly considers different patterns children use while searching the Web to adequately capture the intended meaning of their original queries. For example, if a child submits the query “elsa”, ReQuIK aims to prioritize query suggestions such as “elsa coloring papers” or “elsa dress up games” that correlate better with topics of interest to children rather than “elsa pataky”, as suggested by Google, which is more appealing to mature users. Other contributions of our work include a novel ranking model inspired by a deep-and-wide architecture that, while successfully applied for ranking purposes [6], has never been used in the query suggestions domain; a strategy to overcome the lack of queries written by children by taking advantage of general purpose children-oriented phrases; and a newly-created dataset [4].	Lightning Talk - Looking for the Movie Seven or Sven from the Movie Frozen?	NA:NA:NA:NA	2018
Valeriya Baranova-Bolotova:Vladislav Blinov:Pavel Braslavski	In this lighting talk paper, we present a dataset of jokes in Russian and deep learning model for solving humor recognition task. The new large dataset was collected from various online resources and complemented carefully with unfunny texts with similar lexical properties. In total, there are more than 300,000 short texts, which is significantly larger than any previous humor-related corpus. Manual annotation of 2,000 items proved the reliability of corpus construction approach. Further, we applied language model fine-tuning for text classification and obtained an F1 score of 0.91, which constitutes a considerable gain over baseline methods.	Lightning Talk - Humor Recognition in Russian Language	NA:NA:NA	2018
Ghazaleh Beigi:Suhas Ranganath:Huan Liu	Predicting signed links in social networks often faces the problem of signed link data sparsity, i.e., only a small percentage of signed links are given. The problem is exacerbated when the number of negative links is much smaller than that of positive links. Boosting signed link prediction necessitates additional information to compensate for data sparsity. According to psychology theories, one rich source of such information is user’s personality such as optimism and pessimism that can help determine her propensity in establishing positive and negative links. In this study, we investigate how personality information can be obtained, and if personality information can help alleviate the data sparsity problem for signed link prediction. We propose a novel signed link prediction model that enables empirical exploration of user personality via social media data. We evaluate our proposed model on two datasets of real-world signed link networks. The results demonstrate the complementary role of personality information in the signed link prediction problem. Experimental results also indicate the effectiveness of different levels of personality information for signed link data sparsity problem.	Signed Link Prediction with Sparse Data: The Role of Personality Information	NA:NA:NA	2018
Samaa Gazzaz:Praveen Rao	It is estimated that merely 4% of the world's population reside on US soil. Remarkably, 43% of the entire population of prominent websites are hosted in the United States (Fig. 1). Even though most data content on the Web is unstructured, the US government has had big contributions in producing and actively releasing structured datasets related to different fields such as health, education, safety and finance. Aforementioned datasets are referred to as Open Government Data (OGD) and are aimed at increasing the structured data pool in conjunction with promoting government transparency and accountability. In this paper, we present a new system “OGDXplor” which processes raw OGD through a well-defined procedure leveraging machine learning algorithms and produces meaningful insights. The novelty of this work is encompassed by the collective approach utilized in developing the system and tackling challenges. First by addressing arising challenges due to data being collected and aggregated from heterogeneous sources that otherwise would have been impossible to acquire as a comprehensive unit. moreover, classification and comparisons are drawn on a much finer level that we refer to as zone level. Zones are the areas encompassed and defined by zip codes and are seldomly used in classifying and extracting insights as presented here. OGDXplor facilitates comparing and classifying zones located in different cities or zones within an individual city. The system is presented to end-users as a web application allowing users to elect zones and features relevant to their use case. Results are presented in both chart and map formats which aids the decision-making process.	Extracting Meaningful Insights on City and Zone Levels Utilizing US Open Government Data	NA:NA	2018
Meghana Joshi:Sonam Damani:Khyatti Gupta:Nitya Raviprakash	Dialogue systems and conversational agents are becoming increasingly popular in the modern society but building an agent capable of holding intelligent conversation with its users is a challenging problem for artificial intelligence. In this talk, we share challenges and learnings from our journey of building a deep learning based conversational social agent called ”Ruuh” (m.me/Ruuh) developed by a team at Microsoft India to converse on a wide range of topics. The authors are co-creators of Ruuh and the original paper was presented in NeurIPS 2018 Demonstration Track by two of the authors. As a social agent, Ruuh needs to think beyond the utilitarian notion of merely generating ”relevant” responses and meet a wider range of user social needs. The agent also needs to detect and respond to abusive language, sensitive topics and trolling behavior of users. Some of the above objectives pose significant research challenges in the areas of NLP, IR and AI. Our agent has interacted with over 2 million real world users till date which has generated over 150 million user conversations. We intend to walk the audience through our journey of overcoming several research challenges to become the most popular social agent in India.	Lightning Talk - Ruuh: A Conversational Social Agent	NA:NA:NA:NA	2018
Shirin Nilizadeh:Hojjat Aghakhani:Eric Gustafson:Christopher Kruegel:Giovanni Vigna	Many crowd-sourced review platforms, such as Yelp, TripAdvisor, and Foursquare, have sprung up to provide a shared space for people to write reviews and rate local businesses. With the substantial impact of businesses’ online ratings on their selling [2], many businesses add themselves to multiple websites to more easily be discovered. Some might also engage in reputation management, which could range from rewarding their customers for a favorable review, or a complex review campaign, where armies of accounts post reviews to influence a business’ average review score. Most of previous work use supervised machine learning, and only focus on textual and stylometry features [1, 3, 4, 7]. Their obtained ground truth data is not large and comprehensive [4, 5, 6, 7, 8, 10]. These works also assume a limited threat model, e.g., an adversary’s activity is assumed to be found near sudden shifts in the data [8], or focused on positive campaigns. We propose OneReview , a system for finding fraudulent content on a crowd-sourced review site, leveraging correlations with other independent review sites, and the use of textual and contextual features. We assume that an attacker may not be able to exert the same influence over a business’ reputation on several websites, due to increased cost. OneReview focuses on isolating anomalous changes in a business’ reputation across multiple review sites, to locate malicious activity without relying on specific patterns. Our intuition is that a business’s reputation should not be very different in multiple review sites; e.g., if a restaurant changes its chef or manager, then the impact of these changes should appear on reviews across all the websites. OneReview utilizes Change Point Analysis method on the reviews of every business independently on every website, and then uses our proposed Change Point Analyzer to evaluate change-points, detect those that do not match across the websites, and identify them as suspicious. Then, it uses supervised machine learning, utilizing a combination of textual and metadata features to locate fraudulent reviews among the suspicious reviews. We evaluated our approach, using data from two reviewing websites, Yelp and TripAdvisor, to find fraudulent activity on Yelp. We obtained Yelp reviews, through the Yelp Data Challenge [9], and used our Change Point Analyzer to correlate this with data crawled from TripAdvisor. Since realistic and varied ground truth data is not currently available, we used a combination of our change point analysis and crowd-labeling to create a set of 5,655 labeled reviews. We used k-cross validation (k=5) on our ground truth and obtained 97% (+/- 0.01) accuracy, 91% (+/- 0.03) precision and 90% (+/- 0.06) recall. The model was used on the suspicious reviews, which classified 61,983 reviews, about 8% of all reviews, as fraudulent. We further detected fraudulent campaigns that are actively initiated by or targeted toward specific businesses. We identified 3,980 businesses with fraudulent reviews, as well as, 14,910 suspected spam, where at least 40% of their reviews are classified as fraudulent. We also used community detection algorithms to locate several large astroturfing campaigns. These results show the effectiveness of OneReview in detecting fraudulent campaigns.	Lightning Talk - Think Outside the Dataset: Finding Fraudulent Reviews using Cross-Dataset Analysis	NA:NA:NA:NA:NA	2018
Melanie Johnston-Hollitt	Modern Astrophysics is one of the most data intensive research fields in the world and is driving many of the required innovations in the "big data" space. Foremost in astronomy in terms of data generation is radio astronomy, and in the last decade an increase in global interest and investment in the field had led to a large number of new or upgraded facilities which are each currently generating petabytes of data per annum. The peak of this so-called 'radio renaissance' will be the Square Kilometre Array (SKA) -- a global observatory designed to uncover the mysteries of the Universe. The SKA will create the highest resolution, fastest frame rate movie of the evolving Universe ever and in doing so will generate 160 terrabytes of data a day, or close to 5 zettabytes of data per annum. Furthermore, due to the extreme faintness of extraterrestrial radio signals, the telescope elements for the SKA must be located in radio quite parts of the world with very low population density. Thus the project aims to build the most data intensive scientific experiment ever, in some of the most remote places on Earth. Generating and serving scientific data products of this scale to a global community of researchers from remote locations is just the first of the "big data" challenges the project faces. Coordination of a global network of tiered data resources will be required along with software tools to exploit the vast sea of results generated. In fact, to fully realize the enormous scientific potential of this project, we will need not only better data distribution and coordination mechanisms, but also improved algorithms, artificial intelligence and ontologies to extract knowledge in an automated way at a scale not yet attempted in science. In this keynote I will present an overview of the SKA project, outline the "big data" challenges the project faces and discuss some of the approaches we are taking to tame the astronomical data deluge we face.	Taming the Data Deluge to Unravel the Mysteries of the Universe	NA	2017
Mark Pesce	The great project of the World Wide Web has succeeded - a large portion of the world's information is now instantly accessible through open protocols and open presentation formats. The Web is as Sir Tim Berners-Lee envisioned it, a vast resource of interconnected knowledge. Yet that resource exists in a universe of its own. Meanwhile the real world has become crowded with connected devices, none more significant than the smartphone - bringing the Web to eighty percent of the planet's adult population by the end of this decade. Smartphones have become fantastically adept at navigating cyberspace, but - with the singular exception of maps - have few real connections to the world immediately at hand. In 2017 we live in two worlds: the Web, and the real. The time has come to knit these two together. To begin that integration, our first step must be a deep moment of contemplation about what the Web and the real world have to offer one another. How can each amplify the value and capacity of the other? Because of the Web, the real world is pregnant with data and knowledge - what does that world look like? How do we use it? How does it change the way we think and behave? In this simple act of design thinking - toward a "Web-wide world" - we can reframe the possibilities of what both the Web and the real world can offer - and what we can offer both. This is the next great project for the Web - finding its place in the world.	The Web-Wide World	NA	2017
Yoelle Maarek	Many have noticed that personal communications have slowly moved from mail to social media and instant messaging platforms, especially with younger generation [6]. Yet Web Mail traffic continues to steadily grow. A paradox? Not really. We have observed at Yahoo Research that the nature of email traffic has significantly changed in the last two decades, and it is now dominated by machine-generated messages. These messages include hotel newsletters, from which users forgot to unsubscribe, repeated, and often annoying, notifications from a social media site, or critical information such as a flight e-ticket, a purchase invoice, or a telephone bill. In this talk, I first share some elements of this journey that led us to this critical finding that 90% of today's Web Mail is sent by automatic scripts [1]. I then discuss the challenges and opportunities this drastic change offers. First the key challenge: namely, the need for Web mail services to revisit their usage assumptions and their traditional features in light of this change. An obvious example is the "reply" button being displayed by default below messages sent from a "[email protected]" sender. Another feature is mail classification, which has finally experienced some changes in the last few years, [4]. I then discuss the opportunities in this era of big data. One first insight is that messages that have been generated by a same script, share some semantic commonality. Being able to automatically cluster such messages, and map such clusters into "templates" brings great value for discovering meaning, for generalizing findings and predicting behaviors [5]. A second insight is that within this commonality, the differences bring even more value, which allows highlighting what makes individuals unique within a crowd. In particular we discuss extraction techniques that automatically identify these unique elements [2]. Yet, they also present a clear risk in terms of privacy and I describe the absolute need for guaranteeing k-anonymity in our mining techniques, [3]. I conclude by encouraging the research community to explore this new domain of Web mail search and data mining.	Web Mail is not Dead!: It's Just Not Human Anymore	NA	2017
Vahab Mirrokni:Hamid Nazerzadeh	Billions of dollars worth of display advertising are sold via contracts and deals. This paper presents a formal study of preferred deals, a new generation of contracts for selling online advertisement, that generalize the traditional reservation contracts; these contracts are suitable for advertisers with advanced targeting capabilities. We propose a constant-factor approximation algorithm for maximizing the revenue that can be obtained from these deals. We show, both theoretically and via data analysis, that deals, with appropriately chosen minimum-purchase guarantees, can yield significantly higher revenue than auctions. We evaluate our algorithm using data from Google's ad exchange platform. Our algorithm obtains about 90% of the optimal revenue where the second-price auction, even with personalized reserve, obtains at most 52% of the benchmark.	Deals or No Deals: Contract Design for Online Advertising	NA:NA	2017
Santiago Balseiro:Anthony Kim:Mohammad Mahdian:Vahab Mirrokni	In online advertising, advertisers purchase ad placements by participating in a long sequence of repeated auctions. One of the most important features advertising platforms often provide, and advertisers often use, is budget management, which allows advertisers to control their cumulative expenditures. Advertisers typically declare the maximum daily amount they are willing to pay, and the platform adjusts allocations and payments to guarantee that cumulative expenditures do not exceed budgets. There are multiple ways to achieve this goal, and each one, when applied to all budget-constrained advertisers simultaneously, steers the system toward a different equilibrium. While previous research focused on online stochastic optimization techniques or game-theoretic equilibria of such settings, our goal in this paper is to compare the ``system equilibria'' of a range of budget management strategies in terms of the seller's profit and buyers' utility. In particular, we consider six different budget management strategies including probabilistic throttling, thresholding, bid shading, reserve pricing, and multiplicative boosting. We show these methods admit a system equilibrium in a rather general setting, and prove dominance relations between them in a simplified setting. Our study sheds light on the impact of budget management strategies on the tradeoff between the seller's profit and buyers' utility. Finally, we also empirically compare the system equilibria of these strategies using real ad auction data in sponsored search and randomly generated bids. The empirical study confirms our theoretical findings about the relative performances of budget management strategies.	Budget Management Strategies in Repeated Auctions	NA:NA:NA:NA	2017
Christopher A. Wilkens:Ruggiero Cavallo:Rad Niazadeh	Nearly fifteen years ago, Google unveiled the generalized second price (GSP) auction. By all theoretical accounts including their own [Varian 14], this was the wrong auction --- the Vickrey-Clarke-Groves (VCG) auction would have been the proper choice --- yet GSP has succeeded spectacularly. We give a deep justification for GSP's success: advertisers' preferences map to a model we call value maximization; they do not maximize profit as the standard theory would believe. For value maximizers, GSP is the truthful auction [Aggarwal 09]. Moreover, this implies an axiomatization of GSP --- it is an auction whose prices are truthful for value maximizers --- that can be applied much more broadly than the simple model for which GSP was originally designed. In particular, applying it to arbitrary single-parameter domains recovers the folklore definition of GSP. Through the lens of value maximization, GSP metamorphosizes into a powerful auction, sound in its principles and elegant in its simplicity.	GSP: The Cinderella of Mechanism Design	NA:NA:NA	2017
Alexey Drutsa	We study revenue optimization learning algorithms for repeated posted-price auctions where a seller interacts with a (truthful or strategic) buyer that holds a fixed valuation. We focus on a practical situation in which the seller does not know in advance the number of played rounds (the time horizon) and has thus to use a horizon-independent pricing. First, we consider straightforward modifications of previously best known algorithms and show that these horizon-independent modifications have worser or even linear regret bounds. Second, we provide a thorough theoretical analysis of some broad families of consistent algorithms and show that there does not exist a no-regret horizon-independent algorithm in those families. Finally, we introduce a novel deterministic pricing algorithm that, on the one hand, is independent of the time horizon T and, on the other hand, has an optimal strategic regret upper bound in O(log log T). This result closes the logarithmic gap between the previously best known upper and lower bounds on strategic regret.	Horizon-Independent Optimal Pricing in Repeated Auctions with Truthful and Strategic Buyers	NA	2017
Ruggiero Cavallo:Prabhakar Krishnamurthy:Maxim Sviridenko:Christopher A. Wilkens	The generalized second price (GSP) auction has served as the core selling mechanism for sponsored search ads for over a decade. However, recent trends expanding the set of allowed ad formats---to include a variety of sizes, decorations, and other distinguishing features---have raised critical problems for GSP-based platforms. Alternatives such as the Vickrey-Clarke-Groves (VCG) auction raise different complications because they fundamentally change the way prices are computed. In this paper we report on our efforts to redesign a search ad selling system from the ground up in this new context, proposing a mechanism that optimizes an entire slate of ads globally and computes prices that achieve properties analogous to those held by GSP in the original, simpler setting of uniform ads. A careful algorithmic coupling of allocation-optimization and pricing-computation allows our auction to operate within the strict timing constraints inherent in real-time ad auctions. We report performance results of the auction in Yahoo's Gemini Search platform.	Sponsored Search Auctions with Rich Ads	NA:NA:NA:NA	2017
Zhixuan Fang:Longbo Huang:Adam Wierman	The growth of the sharing economy is driven by the emergence of sharing platforms, e.g., Uber and Lyft, that match owners looking to share their resources with customers looking to rent them. The design of such platforms is a complex mixture of economics and engineering, and how to "optimally" design such platforms is still an open problem. In this paper, we focus on the design of prices and subsidies in sharing platforms. Our results provide insights into the tradeoff between revenue maximizing prices and social welfare maximizing prices. Specifically, we introduce a novel model of sharing platforms and characterize the profit and social welfare maximizing prices in this model. Further, we bound the efficiency loss under profit maximizing prices, showing that there is a strong alignment between profit and efficiency in practical settings. Our results highlight that the revenue of platforms may be limited in practice due to supply short- ages; thus platforms have a strong incentive to encourage sharing via subsidies. We provide an analytic characterization of when such subsidies are valuable and show how to optimize the size of the subsidy provided. Finally, we validate the insights from our analysis using data from Didi Chuxing, the largest ridesharing platform in China.	Prices and Subsidies in the Sharing Economy	NA:NA:NA	2017
Siddhartha Banerjee:Sreenivas Gollapudi:Kostas Kollias:Kamesh Munagala	Recent years have witnessed the rise of many successful e-commerce marketplace platforms like the Amazon marketplace, AirBnB, Uber/Lyft, and Upwork, where a central platform mediates economic transactions between buyers and sellers. A common feature of many of these two-sided marketplaces is that the platform has full control over search and discovery, but prices are determined by the buyers and sellers. Motivated by this, we study the algorithmic aspects of market segmentation via directed discovery in two-sided markets with endogenous prices. We consider a model where an online platform knows each buyer/seller's characteristics, and associated demand/supply elasticities. Moreover, the platform can use discovery mechanisms (search, recommendation, etc.) to control which buyers/sellers are visible to each other. We develop efficient algorithms for throughput (i.e. volume of trade) and welfare maximization with provable guarantees under a variety of assumptions on the demand and supply functions. We also test the validity of our assumptions on demand curves inferred from NYC taxicab log-data, as well as show the performance of our algorithms on synthetic experiments.	Segmenting Two-Sided Markets	NA:NA:NA:NA	2017
Noam Nisan:Gali Noti	Using data obtained in a controlled ad-auction experiment that we ran, we evaluate the regret-based approach to econometrics that was recently suggested by Nekipelov, Syrgkanis, and Tardos (EC 2015). We found that despite the weak regret-based assumptions, the results were (at least) as accurate as those obtained using classic equilibrium-based assumptions. En route we studied to what extent humans actually minimize regret in our ad auction, and found a significant difference between the ``high types'' (players with a high valuation) who indeed rationally minimized regret and the ``low types'' who significantly overbid. We suggest that correcting for these biases and adjusting the regret-based econometric method may improve the accuracy of estimated values.	An Experimental Evaluation of Regret-Based Econometrics	NA:NA	2017
Cinar Kilcioglu:Justin M. Rao:Aadharsh Kannan:R. Preston McAfee	We examine the economics of demand and supply in cloud computing. The public cloud offers three main benefits to firms: 1) utilization can be scaled up or down easily; 2) capital expenditure (on-premises servers) can be converted to operating expenses, with the capital incurred by a specialist; 3) software can be ``pay-as-you-go.'' These benefits increase with the firm's ability to dynamically scale resource utilization and thus point to the need for dynamic prices to shape demand to the (short-run) fixed datacenter supply. Detailed utilization analysis reveals the large swings in utilization at the hourly, daily or weekly level are very rare at the customer level and non-existent at the datacenter level. Furthermore, few customers show volatility patterns that are excessively correlated with the market. These results explain why fixed prices currently prevail despite the seeming need for time-varying dynamics. Examining the actual CPU utilization provides a lens into the future. Here utilization varies by order half the datacenter capacity, but most firms are not dynamically scaling their assigned resources at-present to take advantage of these changes. If these gains are realized, demand fluctuations would be on par with the three classic industries where dynamic pricing is important (hotels, electricity, airlines) and dynamic prices would be essential for efficiency.	Usage Patterns and the Economics of the Public Cloud	NA:NA:NA:NA	2017
Yilin Wang:Jiliang Tang:Jundong Li:Baoxin Li:Yali Wan:Clayton Mellina:Neil O'Hare:Yi Chang	Studies suggest that self-harm users found it easier to discuss self-harm-related thoughts and behaviors using social media than in the physical world. Given the enormous and increasing volume of social media data, on-line self-harm content is likely to be buried rapidly by other normal content. To enable voices of self-harm users to be heard, it is important to distinguish self-harm content from other types of content. In this paper, we aim to understand self-harm content and provide automatic approaches to its detection. We first perform a comprehensive analysis on self-harm social media using different input cues. Our analysis, the first of its kind in large scale, reveals a number of important findings. Then we propose frameworks that incorporate the findings to discover self-harm content under both supervised and unsupervised settings. Our experimental results on a large social media dataset from Flickr demonstrate the effectiveness of the proposed frameworks and the importance of our findings in discovering self-harm content.	Understanding and Discovering Deliberate Self-harm Content in Social Media	NA:NA:NA:NA:NA:NA:NA:NA	2017
Sandra Servia-Rodríguez:Kiran K. Rachuri:Cecilia Mascolo:Peter J. Rentfrow:Neal Lathia:Gillian M. Sandstrom	Measuring mental well-being with mobile sensing has been an increasingly active research topic. Pervasiveness of smartphones combined with the convenience of mobile app distribution platforms (e.g., Google Play) provide a tremendous opportunity to reach out to millions of users. However, the studies at the confluence of mental health and mobile sensing have been longitudinally limited, controlled, or confined to a small number of participants. In this paper we report on what we believe is the largest longitudinal in-the-wild study of mood through smartphones. We describe an Android app to collect participants' self-reported moods and system triggered experience sampling data while passively measuring their physical activity, sociability, and mobility via their device's sensors. We report the results of a large-scale analysis of the data collected for about three years from 18,000 users. The paper makes three primary contributions. First, we show how we used physical and software sensors in smartphones to automatically and accurately identify routines. Then, we demonstrate the strong correlation between these routines and users' personality, well-being perception, and other psychological variables. Finally, we explore predictability of users' mood using their passive sensing data. Our findings show that, especially for weekends, mobile sensing can be used to predict users' mood with an accuracy of about 70%. These results have the potential to impact the design of future mobile apps for mood/behavior tracking and interventions.	Mobile Sensing at the Service of Mental Well-being: a Large-scale Longitudinal Study	NA:NA:NA:NA:NA:NA	2017
Tim Althoff:Eric Horvitz:Ryen W. White:Jamie Zeitzer	Human cognitive performance is critical to productivity, learning, and accident avoidance. Cognitive performance varies throughout each day and is in part driven by intrinsic, near 24-hour circadian rhythms. Prior research on the impact of sleep and circadian rhythms on cognitive performance has typically been restricted to small-scale laboratory-based studies that do not capture the variability of real-world conditions, such as environmental factors, motivation, and sleep patterns in real-world settings. Given these limitations, leading sleep researchers have called for larger in situ monitoring of sleep and performance. We present the largest study to date on the impact of objectively measured real-world sleep on performance enabled through a reframing of everyday interactions with a web search engine as a series of performance tasks. Our analysis includes 3 million nights of sleep and 75 million interaction tasks. We measure cognitive performance through the speed of keystroke and click interactions on a web search engine and correlate them to wearable device-defined sleep measures over time. We demonstrate that real-world performance varies throughout the day and is influenced by both circadian rhythms, chronotype (morning/evening preference), and prior sleep duration and timing. We develop a statistical model that operationalizes a large body of work on sleep and performance and demonstrates that our estimates of circadian rhythms, homeostatic sleep drive, and sleep inertia align with expectations from laboratory-based sleep studies. Further, we quantify the impact of insufficient sleep on real-world performance and show that two consecutive nights with less than six hours of sleep are associated with decreases in performance which last for a period of six days. This work demonstrates the feasibility of using online interactions for large-scale physiological sensing.	Harnessing the Web for Population-Scale Physiological Sensing: A Case Study of Sleep and Performance	NA:NA:NA:NA	2017
Shaodian Zhang:Tian Kang:Lin Qiu:Weinan Zhang:Yong Yu:Noémie Elhadad	A large number of patients discuss treatments in online health communities (OHCs). One research question of interest to health researchers is whether treatments being discussed in OHCs are eventually used by community members in their real lives. In this paper, we rely on machine learning methods to automatically identify attributions of mentions of treatments from an online autism community. The context of our work is online autism communities, where parents exchange support for the care of their children with autism spectrum disorder. Our methods are able to distinguish discussions of treatments that are associated with patients, caregivers, and others, as well as identify whether a treatment is actually taken. We investigate treatments that are not just discussed but also used by patients according to two types of content analysis, cross-sectional and longitudinal. The treatments identified through our content analysis help create a catalogue of real-world treatments. This study results lay the foundation for future research to compare real-world drug usage with established clinical guidelines.	Cataloguing Treatments Discussed and Used in Online Autism Communities	NA:NA:NA:NA:NA:NA	2017
Jin-woo Kwon:Soo-Mook Moon	Due to its high portability and simplicity, web application (app) based on HTML/JavaScript/CSS has been widely used for various smart-device platforms. To take advantage of its wide platform pool, a new idea called app migration has been proposed for the web platform. Web app migration is a framework to serialize a web app running on a device and restore it in another device to continue its execution. In JavaScript semantics, one of the language features that does not allow easy app migration is a closure. A JavaScript function can access variables defined in its outer function even if the execution of the outer function is terminated. It is allowed because the inner function is created as a closure such that it contains the outer function's environment. This feature is widely used in web app development because it is the most common way to implement data encapsulation in web programming. Closures are not easy to serialize because environments can be shared by a number of closures and environments can be created in a nested way. In this paper, we propose a novel approach to fully serialize closures. We created mechanisms to extract information from a closure's environment through the JavaScript engine and to serialize the information in a proper order so that the original relationship between closures and environments can be restored properly. We implemented our mechanism on the WebKit browser and successfully migrated Octane benchmarks and seven real web apps which heavily exploit closures. We also show that our mechanism works correctly even for some extreme, closure-heavy cases.	Web Application Migration with Closure Reconstruction	NA:NA	2017
Mengwei Xu:Yun Ma:Xuanzhe Liu:Felix Xiaozhu Lin:Yunxin Liu	Background activities on smartphones are essential to today's "always-on" mobile device experience. Yet, there lacks a clear understanding of the cooperative behaviors among background activities as well as a quantification of the consequences. In this paper, we present the first in-depth study of app collusion, in which one app surreptitiously launches others in the background without user's awareness. To enable the study, we develop AppHolmes, a static analysis tool for detecting app collusion by examining the app binaries. By analyzing 10,000 apps from top third-party app markets, we found that i) covert, cooperative behaviors in background app launch are surprisingly pervasive, ii) most collusion is caused by shared services, libraries, or common interest among apps, and iii) collusion has serious impact on performance, efficiency, and security. Overall, our work presents a strong implication on future mobile system design.	AppHolmes: Detecting and Characterizing App Collusion among Third-Party Android Markets	NA:NA:NA:NA:NA	2017
Elias P. Papadopoulos:Michalis Diamantaris:Panagiotis Papadopoulos:Thanasis Petsas:Sotiris Ioannidis:Evangelos P. Markatos	The vast majority of online services nowadays, provide both a mobile friendly website and a mobile application to their users. Both of these choices are usually released for free, with their developers, usually gaining revenue by allowing advertisements from ad networks to be embedded into their content. In order to provide more personalized and thus more effective advertisements, ad networks usually deploy pervasive user tracking, raising this way significant privacy concerns. As a consequence, the users do not have to think only their convenience before deciding which choice to use while accessing a service: web or app, but also which one harms their privacy the least. In this paper, we aim to respond to this question: which of the two options protects the users' privacy in the best way apps or browsers? To tackle this question, we study a broad range of privacy related leaks in a comparison of several popular apps and their web counterpart. These leaks may contain not only personally identifying information (PII) but also device-specific information, able to cross-application and cross-site track the user into the network, and allow third parties to link web with app sessions. Finally, we propose an anti-tracking mechanism that enable the users to access an online service through a mobile app without risking their privacy. Our evaluation shows that our approach is able to preserve the privacy of the user by reducing the leaking identifiers of apps by 27.41% on average, while it imposes a practically negligible latency of less than 1 millisecond per request.	The Long-Standing Privacy Debate: Mobile Websites vs Mobile Apps	NA:NA:NA:NA:NA:NA	2017
Haoyu Wang:Zhe Liu:Yao Guo:Xiangqun Chen:Miao Zhang:Guoai Xu:Jason Hong	With the prevalence of smartphones, app markets such as Apple App Store and Google Play has become the center stage in the mobile app ecosystem, with millions of apps developed by tens of thousands of app developers in each major market. This paper presents a study of the mobile app ecosystem from the perspective of app developers. Based on over one million Android apps and 320,000 developers from Google Play, we analyzed the Android app ecosystem from different aspects. Our analysis shows that while over half of the developers have released only one app in the market, many of them have released hundreds of apps. We classified developers into different groups based on the number of apps they have released, and compared their characteristics. Specially, we have analyzed the group of aggressive developers who have released more than 50 apps, trying to understand how and why they create so many apps. We also investigated the privacy behaviors of app developers, showing that some developers have a habit of producing apps with low privacy ratings. Our study shows that understanding the behavior of mobile developers can be helpful to not only other app developers, but also to app markets and mobile users.	An Explorative Study of the Mobile App Ecosystem from App Developers' Perspective	NA:NA:NA:NA:NA:NA:NA	2017
Xiangnan He:Lizi Liao:Hanwang Zhang:Liqiang Nie:Xia Hu:Tat-Seng Chua	In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation --- collaborative filtering --- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering --- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.	Neural Collaborative Filtering	NA:NA:NA:NA:NA:NA	2017
Peizhe Cheng:Shuaiqiang Wang:Jun Ma:Jiankai Sun:Hui Xiong	In this study, we investigate diversified recommendation problem by supervised learning, seeking significant improvement in diversity while maintaining accuracy. In particular, we regard each user as a training instance, and heuristically choose a subset of accurate and diverse items as ground-truth for each user. We then represent each user or item as a vector resulted from the factorization of the user-item rating matrix. In our paper, we try to discover a factorization for matching the following supervised learning task. In doing this, we define two coupled optimization problems, parameterized matrix factorization and structural learning, to formulate our task. And we propose a diversified collaborative filtering algorithm (DCF) to solve the coupled problems. We also introduce a new pairwise accuracy metric and a normalized topic coverage diversity metric to measure the performance of accuracy and diversity respectively. Extensive experiments on benchmark datasets show the performance gains of DCF in comparison with the state-of-the-art algorithms.	Learning to Recommend Accurate and Diverse Items	NA:NA:NA:NA:NA	2017
Cheng-Kang Hsieh:Longqi Yang:Yin Cui:Tsung-Yi Lin:Serge Belongie:Deborah Estrin	Metric learning algorithms produce distance metrics that capture the important relationships among data. In this work, we study the connection between metric learning and collaborative filtering. We propose Collaborative Metric Learning (CML) which learns a joint metric space to encode not only users' preferences but also the user-user and item-item similarity. The proposed algorithm outperforms state-of-the-art collaborative filtering algorithms on a wide range of recommendation tasks and uncovers the underlying spectrum of users' fine-grained preferences. CML also achieves significant speedup for Top-K recommendation tasks using off-the-shelf, approximate nearest-neighbor search, with negligible accuracy reduction.	Collaborative Metric Learning	NA:NA:NA:NA:NA:NA	2017
Alex Beutel:Ed H. Chi:Zhiyuan Cheng:Hubert Pham:John Anderson	When building a recommender system, how can we ensure that all items are modeled well? Classically, recommender systems are built, optimized, and tuned to improve a global prediction objective, such as root mean squared error. However, as we demonstrate, these recommender systems often leave many items badly-modeled and thus under-served. Further, we give both empirical and theoretical evidence that no single matrix factorization, under current state-of-the-art methods, gives optimal results for each item. As a result, we ask: how can we learn additional models to improve the recommendation quality for a specified subset of items? We offer a new technique called focused learning, based on hyperparameter optimization and a customized matrix factorization objective. Applying focused learning on top of weighted matrix factorization, factorization machines, and LLORMA, we demonstrate prediction accuracy improvements on multiple datasets. For instance, on MovieLens we achieve as much as a 17% improvement in prediction accuracy for niche movies, cold-start items, and even the most badly-modeled items in the original model.	Beyond Globally Optimal: Focused Learning for Improved Recommendations	NA:NA:NA:NA:NA	2017
Tsubasa Takahashi:Bryan Hooi:Christos Faloutsos	Given a collection of seasonal time-series, how can we find regular (cyclic) patterns and outliers (i.e. rare events)? These two types of patterns are hidden and mixed in the time-varying activities. How can we robustly separate regular patterns and outliers, without requiring any prior information? We present CycloneM, a unifying model to capture both cyclic patterns and outliers, and CycloneFact, a novel algorithm which solves the above problem. We also present an automatic mining framework AutoCyclone, based on CycloneM and CycloneFact. Our method has the following properties; (a) effective: it captures important cyclic features such as trend and seasonality, and distinguishes regular patterns and rare events clearly; (b) robust and accurate: it detects the above features and patterns accurately against outliers; (c) fast: CycloneFact takes linear time in the data size and typically converges in a few iterations; (d) parameter free: our modeling framework frees the user from having to provide parameter values. Extensive experiments on 4 real datasets demonstrate the benefits of the proposed model and algorithm, in that the model can capture latent cyclic patterns, trends and rare events, and the algorithm outperforms the existing state-of-the-art approaches. CycloneFact was up to 5 times more accurate and 20 times faster than top competitors.	AutoCyclone: Automatic Mining of Cyclic Online Activities with Robust Tensor Factorization	NA:NA:NA	2017
Yuan Lin:Wei Chen:Zhongzhi Zhang	Percolation threshold of a network is the critical value such that when nodes or edges are randomly selected with probability below the value, the network is fragmented but when the probability is above the value, a giant component connecting a large portion of the network would emerge. Assessing the percolation threshold of networks has wide applications in network reliability, information spread, epidemic control, etc. The theoretical approach so far to assess the percolation threshold is mainly based on spectral radius of adjacency matrix or non-backtracking matrix, which is limited to dense graphs or locally treelike graphs, and is less effective for sparse networks with non-negligible amount of triangles and loops. In this paper, we study high-order non-backtracking matrices and their application to assessing percolation threshold. We first define high-order non-backtracking matrices and study the properties of their spectral radii. Then we focus on the 2nd-order non-backtracking matrix and demonstrate analytically that the reciprocal of its spectral radius gives a tighter lower bound than those of adjacency and standard non-backtracking matrices. We further build a smaller size matrix with the same largest eigenvalue as the 2nd-order non-backtracking matrix to improve computation efficiency. Finally, we use both synthetic networks and 42 real networks to illustrate that the use of the 2nd-order non-backtracking matrix does give better lower bound for assessing percolation threshold than adjacency and standard non-backtracking matrices.	Assessing Percolation Threshold Based on High-Order Non-Backtracking Matrices	NA:NA:NA	2017
Maximilien Danisch:T.-H. Hubert Chan:Mauro Sozio	Algorithms for finding dense regions in an input graph have proved to be effective tools in graph mining and data analysis. Recently, Tatti and Gionis [WWW 2015] presented a novel graph decomposition (known as the locally-dense decomposition) that is similar to the well-known k-core decomposition, with the additional property that its components are arranged in order of their densities. Such a decomposition provides a valuable tool in graph mining. Unfortunately, their algorithm for computing the exact decomposition is based on a maximum-flow algorithm which cannot scale to massive graphs, while the approximate decomposition defined by the same authors misses several interesting properties. This calls for scalable algorithms for computing such a decomposition. In our work, we devise an efficient algorithm which is able to compute exact locally-dense decompositions in real-world graphs containing up to billions of edges. Moreover, we provide a new definition of approximate locally-dense decomposition which retains most of the properties of an exact decomposition, for which we devise an algorithm that can scale to real-world graphs containing up to tens of billions of edges. Our algorithm is based on the classic Frank-Wolfe algorithm which is similar to gradient descent and can be efficiently implemented in most of the modern architectures dealing with massive graphs. We provide a rigorous study of our algorithms and their convergence rates. We conduct an extensive experimental evaluation on multi-core architectures showing that our algorithms converge much faster in practice than their worst-case analysis. Our algorithm is even more efficient for the more specialized problem of computing a densest subgraph.	Large Scale Density-friendly Graph Decomposition via Convex Programming	NA:NA:NA	2017
Xinsheng Li:K. Selçuk Candan:Maria Luisa Sapino	Tensor decomposition is used for many web and user data analysis operations from clustering, trend detection, anomaly detection, to correlation analysis. However, many of the tensor decomposition schemes are sensitive to noisy data, an inevitable problem in the real world that can lead to false conclusions. The problem is compounded by over-fitting when the user data is sparse. Recent research has shown that it is possible to avoid over-fitting by relying on probabilistic techniques. However, these have two major deficiencies: (a) firstly, they assume that all the data and intermediary results can fit in the main memory, and (b) they treat the entire tensor uniformly, ignoring potential non-uniformities in the noise distribution. In this paper, we propose a Noise-Profile Adaptive Tensor Decomposition (nTD) method, which aims to tackle both of these challenges. In particular, nTD leverages a grid-based two-phase decomposition strategy for two complementary purposes: firstly, the grid partitioning helps ensure that the memory footprint of the decomposition is kept low; secondly (and perhaps more importantly) any a priori knowledge about the noise profiles of the grid partitions enable us to develop a sample assignment strategy (or s-strategy) that best suits the noise distribution of the given tensor. Experiments show that nTD's performance is significantly better than conventional CP decomposition techniques on noisy user data tensors.	nTD: Noise-Profile Adaptive Tensor Decomposition	NA:NA:NA	2017
Osama Haq:Mamoon Raja:Fahad R. Dogar	Many popular cloud applications use inter-data center paths; yet, little is known about the characteristics of these ``cloud paths''. Over an eighteen month period, we measure the inter-continental cloud paths of three providers (Amazon, Google, and Microsoft) using client side (VM-to-VM) measurements. We find that cloud paths are more predictable compared to public Internet paths, with an order of magnitude lower loss rate and jitter at the tail (95th percentile and beyond) compared to public Internet paths. We also investigate the nature of packet losses on these paths (e.g., random vs. bursty) and potential reasons why these paths may be better in quality. Based on our insights, we consider how we can further improve the quality of these paths with the help of existing loss mitigation techniques. We demonstrate that using the cloud path in conjunction with a detour path can mask most of the cloud losses, resulting in up to five 9's of network availability for applications.	Measuring and Improving the Reliability of Wide-Area Cloud Paths	NA:NA:NA	2017
Henrique Moniz:João Leitão:Ricardo J. Dias:Johannes Gehrke:Nuno Preguiça:Rodrigo Rodrigues	Most geo-replicated storage systems use weak consistency to avoid the performance penalty of coordinating replicas in different data centers. This departure from strong semantics poses problems to application programmers, who need to address the anomalies enabled by weak consistency. In this paper we use a recently proposed isolation level, called Non-Monotonic Snapshot Isolation, to achieve ACID transactions with low latency. To this end, we present Blotter, a geo-replicated system that leverages these semantics in the design of a new concurrency control protocol that leaves a small amount of local state during reads to make commits more efficient, which is combined with a configuration of Paxos that is tailored for good performance in wide area settings. Read operations always run on the local data center, and update transactions complete in a small number of message steps to a subset of the replicas. We implemented Blotter as an extension to Cassandra. Our experimental evaluation shows that Blotter has a small overhead at the data center scale, and performs better across data centers when compared with our implementations of the core Spanner protocol and of Snapshot Isolation on the same codebase.	Blotter: Low Latency Transactions for Geo-Replicated Storage	NA:NA:NA:NA:NA:NA	2017
Charles L.A. Clarke:Gordon V. Cormack:Jimmy Lin:Adam Roegiest	This paper explores a simple question: How would we provide a high-quality search experience on Mars, where the fundamental physical limit is speed-of-light propagation delays on the order of tens of minutes? On Earth, users are accustomed to nearly instantaneous responses from web services. Is it possible to overcome orders-of-magnitude longer latency to provide a tolerable user experience on Mars? In this paper, we formulate the searching from Mars problem as a tradeoff between "effort" (waiting for responses from Earth) and "data transfer" (pre-fetching or caching data on Mars). The contribution of our work is articulating this design space and presenting two case studies that explore the effectiveness of baseline techniques, using publicly available data from the TREC Total Recall and Sessions Tracks. We intend for this research problem to be aspirational as well as inspirational---even if one is not convinced by the premise of Mars colonization, there are Earth-based scenarios such as searching from rural villages in India that share similar constraints, thus making the problem worthy of exploration and attention from researchers.	Ten Blue Links on Mars	NA:NA:NA:NA	2017
Albert van der Linde:Pedro Fouto:João Leitão:Nuno Preguiça:Santiago Castiñeira:Annette Bieniusa	Many web applications are built around direct interactions among users, from collaborative applications and social networks to multi-user games. Despite being user-centric, these applications are usually supported by services running on servers that mediate all interactions among clients. When users are in close vicinity of each other, relying on a centralized infrastructure for mediating user interactions leads to unnecessarily high latency while hampering fault-tolerance and scalability. In this paper, we propose to extend user-centric Internet services with peer-to-peer interactions. We have designed a framework named Legion that enables client web applications to securely replicate data from servers, and synchronize these replicas directly among them. Legion allows for client-side modules, that we dub adapters, to leverage existing web platforms for storing data and to assist in Legion operation. Using these adapters, legacy applications accessing directly the web platforms can co-exist with new applications that use our framework, while accessing the same shared objects.Our experimental evaluation shows that, besides supporting direct client interactions, even when disconnected from the servers, Legion provides lower latency for update propagation with decreased network traffic for servers.	Legion: Enriching Internet Services with Peer-to-Peer Interactions	NA:NA:NA:NA:NA:NA	2017
Luca Soldaini:Elad Yom-Tov	Internet data has surfaced as a primary source for investigation of different aspects of human behavior. A crucial step in such studies is finding a suitable cohort (i.e., a set of users) that shares a common trait of interest to researchers. However, direct identification of users sharing this trait is often impossible, as the data available to researchers is usually anonymized to preserve user privacy. To facilitate research on specific topics of interest, especially in medicine, we introduce an algorithm for identifying a trait of interest in anonymous users. We illustrate how a small set of labeled examples, together with statistical information about the entire population, can be aggregated to obtain labels on unseen examples. We validate our approach using labeled data from the political domain. We provide two applications of the proposed algorithm to the medical domain. In the first, we demonstrate how to identify users whose search patterns indicate they might be suffering from certain types of cancer. This shows, for the first time, that search queries can be used as a screening device for diseases that are currently often discovered too late, because no early screening tests exists. In the second, we detail an algorithm to predict the distribution of diseases given their incidence in a subset of the population at study, making it possible to predict disease spread from partial epidemiological data.	Inferring Individual Attributes from Search Engine Queries and Auxiliary Information	NA:NA	2017
Daniela Perrotta:Michele Tizzoni:Daniela Paolotti	Traditional surveillance of seasonal influenza is generally affected by reporting lags of at least one week and by continuous revisions of the numbers initially released. As a consequence, influenza forecasts are often limited by the time required to collect new and accurate data. On the other hand, the availability of novel data streams for disease detection can help in overcoming these issues by capturing an additional surveillance signal that can be used to complement data collected by public health agencies. In this study, we investigate how combining both traditional and participatory Web-based surveillance data can provide accurate predictions for seasonal influenza in real-time fashion. To this aim, we use two data sources available in Italy from two different monitoring systems: traditional surveillance data based on sentinel doctors reports and digital surveillance data deriving from a participatory system that monitors the influenza activity through Internet-based surveys. We integrate such digital component in a linear autoregressive exogenous (ARX) model based on traditional surveillance data and evaluate its predictive ability over the course of four influenza seasons in Italy, from 2012-2013 to 2015-2016, for each of the four weekly time horizons. Our results show that by using data extracted from a Web-based participatory surveillance system, which are usually available one week in advance with respect to traditional surveillance, it is possible to obtain accurate weekly predictions of influenza activity at national level up to four weeks in advance. Compared to a model that is only based on data from sentinel doctors, our approach significantly improves real-time forecasts of influenza activity, by increasing the Pearson's correlation up to 30% and by reducing the Mean Absolute Error up to 43% for the four weekly time horizons.	Using Participatory Web-based Surveillance Data to Improve Seasonal Influenza Forecasting in Italy	NA:NA:NA	2017
Qian Zhang:Nicola Perra:Daniela Perrotta:Michele Tizzoni:Daniela Paolotti:Alessandro Vespignani	The availability of novel digital data streams that can be used as proxy for monitoring infectious disease incidence is ushering in a new era for real-time forecast approaches to disease spreading. Here, we propose the first seasonal influenza forecast framework based on a stochastic, spatially structured mechanistic model (individual level microsimulation) initialized with geo-localized microblogging data. The framework provides for more than 600 census areas in the United States, Italy and Spain, the initial conditions for a stochastic epidemic computational model that generates an ensemble of forecasts for the main indicators of the epidemic season: peak time and intensity. We evaluate the forecasts accuracy and reliability by comparing the results with the data from the official influenza surveillance systems in the US, Italy and Spain in the seasons 2014/15 and 2015/16. In all countries studied, the proposed framework provides reliable results with leads of up to 6 weeks that became more stable and accurate with progression of the season. The results for the United States have been generated in real-time in the context of the Centers for Disease Control and Prevention ``Forecasting the Influenza Season Challenge''. A characteristic feature of the mechanistic modeling approach is in the explicit estimate of key epidemiological parameters relevant for public health decision-making that cannot be achieved with statistical models that do not consider the disease dynamic. Furthermore, the presented framework allows the fusion of multiple data streams in the initialization stage and can be enriched with census, weather and socioeconomic data.	Forecasting Seasonal Influenza Fusing Digital Indicators and a Mechanistic Disease Model	NA:NA:NA:NA:NA:NA	2017
Maulik R. Kamdar:Mark A. Musen	Integrated approaches for pharmacology are required for the mechanism-based predictions of adverse drug reactions that manifest due to concomitant intake of multiple drugs. These approaches require the integration and analysis of biomedical data and knowledge from multiple, heterogeneous sources with varying schemas, entity notations, and formats. To tackle these integrative challenges, the Semantic Web community has published and linked several datasets in the Life Sciences Linked Open Data (LSLOD) cloud using established W3C standards. We present the PhLeGrA platform for Linked Graph Analytics in Pharmacology in this paper. Through query federation, we integrate four sources from the LSLOD cloud and extract a drug-reaction network, composed of distinct entities. We represent this graph as a hidden conditional random field (HCRF), a discriminative latent variable model that is used for structured output predictions. We calculate the underlying probability distributions in the drug-reaction HCRF using the datasets from the U.S. Food and Drug Administration's Adverse Event Reporting System. We predict the occurrence of 146 adverse reactions due to multiple drug intake with an AUROC statistic greater than 0.75. The PhLeGrA platform can be extended to incorporate other sources published using Semantic Web technologies, as well as to discover other types of pharmacological associations.	PhLeGrA: Graph Analytics in Pharmacology over the Web of Life Sciences Linked Open Data	NA:NA	2017
Min Hong Yun:Songtao He:Lin Zhong	Drawing or dragging an object on a mobile device is annoying today because the latency is manifested spatially with an obvious gap between the touch point and the line head or dragged object. This work identifies the multiple synchronization points in the input to display path of modern mobile systems as a major source of latency, contributing about 30 ms to the overall latency. We present Presto, an asynchronous design of the input to display path. By focusing on the main application and relaxing conventional requirements of no frame drop and no tearing effects, Presto is able to eliminate much of the latency due to synchrony. By carefully guarding against consecutive frame drops and limiting the risk of tearing to a small region around the touch point, Presto is able to reduce their visual impact to barely noticeable. Using a prototype based on Android 5, we are able to quantify the effectiveness, overhead and user experience of Presto through both objective measurements and subjective user assessment. We show that Presto is able to reduce the latency of legacy Android applications by close to half; and more importantly, we show this reduction is orthogonal to that by other popular approaches. When combined with touch prediction, Presto is able to reduce the touch latency below 10 ms, a remarkable achievement without any hardware support.	Reducing Latency by Eliminating Synchrony	NA:NA:NA	2017
Giovanni Campagna:Rakesh Ramesh:Silei Xu:Michael Fischer:Monica S. Lam	This paper presents the architecture of Almond, an open, crowdsourced, privacy-preserving and programmable virtual assistant for online services and the Internet of Things (IoT). Included in Almond is Thingpedia, a crowdsourced public knowledge base of natural language interfaces and open APIs. Our proposal addresses four challenges in virtual assistant technology: generality, interoperability, privacy, and usability. Generality is addressed by crowdsourcing Thingpedia, while interoperability is provided by ThingTalk, a high-level domain-specific language that connects multiple devices or services via open APIs. For privacy, user credentials and user data are managed by our open-source ThingSystem, which can be run on personal phones or home servers. Finally, we address usability by providing a natural language interface, whose capability can be extended via training with the help of a menu-driven interface. We have created a fully working prototype, and crowdsourced a set of 187 functions across 45 different kinds of devices. Almond is the first virtual assistant that lets users specify trigger-action tasks in natural language. Despite the lack of real usage data, our experiment suggests that Almond can understand about 40% of the complex tasks when uttered by a user familiar with its capability.	Almond: The Architecture of an Open, Crowdsourced, Privacy-Preserving, Programmable Virtual Assistant	NA:NA:NA:NA:NA	2017
Shuochao Yao:Shaohan Hu:Yiran Zhao:Aston Zhang:Tarek Abdelzaher	Mobile sensing and computing applications usually require time-series inputs from sensors, such as accelerometers, gyroscopes, and magnetometers. Some applications, such as tracking, can use sensed acceleration and rate of rotation to calculate displacement based on physical system models. Other applications, such as activity recognition, extract manually designed features from sensor inputs for classification. Such applications face two challenges. On one hand, on-device sensor measurements are noisy. For many mobile applications, it is hard to find a distribution that exactly describes the noise in practice. Unfortunately, calculating target quantities based on physical system and noise models is only as accurate as the noise assumptions. Similarly, in classification applications, although manually designed features have proven to be effective, it is not always straightforward to find the most robust features to accommodate diverse sensor noise patterns and heterogeneous user behaviors. To this end, we propose DeepSense, a deep learning framework that directly addresses the aforementioned noise and feature customization challenges in a unified manner. DeepSense integrates convolutional and recurrent neural networks to exploit local interactions among similar mobile sensors, merge local interactions of different sensory modalities into global interactions, and extract temporal relationships to model signal dynamics. DeepSense thus provides a general signal estimation and classification framework that accommodates a wide range of applications. We demonstrate the effectiveness of DeepSense using three representative and challenging tasks: car tracking with motion sensors, heterogeneous human activity recognition, and user identification with biometric motion analysis. DeepSense significantly outperforms the state-of-the-art methods for all three tasks. In addition, we show that DeepSense is feasible to implement on smartphones and embedded devices thanks to its moderate energy consumption and low latency.	DeepSense: A Unified Deep Learning Framework for Time-Series Mobile Sensing Data Processing	NA:NA:NA:NA:NA	2017
Chao Zhang:Keyang Zhang:Quan Yuan:Haoruo Peng:Yu Zheng:Tim Hanratty:Shaowen Wang:Jiawei Han	With the ever-increasing urbanization process, systematically modeling people's activities in the urban space is being recognized as a crucial socioeconomic task. This task was nearly impossible years ago due to the lack of reliable data sources, yet the emergence of geo-tagged social media (GTSM) data sheds new light on it. Recently, there have been fruitful studies on discovering geographical topics from GTSM data. However, their high computational costs and strong distributional assumptions about the latent topics hinder them from fully unleashing the power of GTSM. To bridge the gap, we present CrossMap, a novel cross-modal representation learning method that uncovers urban dynamics with massive GTSM data. CrossMap first employs an accelerated mode seeking procedure to detect spatiotemporal hotspots underlying people's activities. Those detected hotspots not only address spatiotemporal variations, but also largely alleviate the sparsity of the GTSM data. With the detected hotspots, CrossMap then jointly embeds all spatial, temporal, and textual units into the same space using two different strategies: one is reconstruction-based and the other is graph-based. Both strategies capture the correlations among the units by encoding their co-occurrence and neighborhood relationships, and learn low-dimensional representations to preserve such correlations. Our experiments demonstrate that CrossMap not only significantly outperforms state-of-the-art methods for activity recovery and classification, but also achieves much better efficiency.	Regions, Periods, Activities: Uncovering Urban Dynamics via Cross-Modal Representation Learning	NA:NA:NA:NA:NA:NA:NA:NA	2017
Dimitris Serbos:Shuyao Qi:Nikos Mamoulis:Evaggelia Pitoura:Panayiotis Tsaparas	Recommending packages of items to groups of users has several applications, including recommending vacation packages to groups of tourists, entertainment packages to groups of friends, or sets of courses to groups of students. In this paper, we focus on a novel aspect of package-to-group recommendations, that of fairness. Specifically, when we recommend a package to a group of people, we ask that this recommendation is fair in the sense that every group member is satisfied by a sufficient number of items in the package. We explore two definitions of fairness and show that for either definition the problem of finding the most fair package is NP-hard. We exploit the fact that our problem can be modeled as a coverage problem, and we propose greedy algorithms that find approximate solutions within reasonable time. In addition, we study two extensions of the problem, where we impose category or spatial constraints on the items to be included in the recommended packages. We evaluate the appropriateness of the fairness models and the performance of the proposed algorithms using real data from Yelp, and a user study.	Fairness in Package-to-Group Recommendations	NA:NA:NA:NA:NA	2017
Shiyu Chang:Yang Zhang:Jiliang Tang:Dawei Yin:Yi Chang:Mark A. Hasegawa-Johnson:Thomas S. Huang	The increasing popularity of real-world recommender systems produces data continuously and rapidly, and it becomes more realistic to study recommender systems under streaming scenarios. Data streams present distinct properties such as temporally ordered, continuous and high-velocity, which poses tremendous challenges to traditional recommender systems. In this paper, we investigate the problem of recommendation with stream inputs. In particular, we provide a principled framework termed sRec, which provides explicit continuous-time random process models of the creation of users and topics, and of the evolution of their interests. A variational Bayesian approach called recursive meanfield approximation is proposed, which permits computationally efficient instantaneous on-line inference. Experimental results on several real-world datasets demonstrate the advantages of our sRec over other state-of-the-arts.	Streaming Recommender Systems	NA:NA:NA:NA:NA:NA:NA	2017
Suhang Wang:Yilin Wang:Jiliang Tang:Kai Shu:Suhas Ranganath:Huan Liu	The rapid growth of Location-based Social Networks (LBSNs) provides a vast amount of check-in data, which facilitates the study of point-of-interest (POI) recommendation. The majority of the existing POI recommendation methods focus on four aspects, i.e., temporal patterns, geographical influence, social correlations and textual content indications. For example, user's visits to locations have temporal patterns and users are likely to visit POIs near them. In real-world LBSNs such as Instagram, users can upload photos associating with locations. Photos not only reflect users' interests but also provide informative descriptions about locations. For example, a user who posts many architecture photos is more likely to visit famous landmarks; while a user posts lots of images about food has more incentive to visit restaurants. Thus, images have potentials to improve the performance of POI recommendation. However, little work exists for POI recommendation by exploiting images. In this paper, we study the problem of enhancing POI recommendation with visual contents. In particular, we propose a new framework Visual Content Enhanced POI recommendation (VPOI), which incorporates visual contents for POI recommendations. Experimental results on real-world datasets demonstrate the effectiveness of the proposed framework.	What Your Images Reveal: Exploiting Visual Contents for Point-of-Interest Recommendation	NA:NA:NA:NA:NA:NA	2017
Tuan-Anh Nguyen Pham:Xutao Li:Gao Cong	With the rapid growth of location-based social networks (LBSNs), it is now available to analyze and understand user mobility behavior in real world. Studies show that users usually visit nearby points of interest (POIs), located in small regions, especially when they travel out of their hometowns. However, previous out-of-town recommendation systems mainly focus on recommending individual POIs that may reside far from each other, which makes the recommendation results less useful. In this paper, we introduce a novel problem called Region Recommendation, which aims to recommend an out-of-town region of POIs that are likely to be visited by a user. The proximity characteristic of user mobility behavior implies that the probability of visiting one POI depends on those of nearby POIs. Thus, to make accurate region recommendation, our proposed model exploits the influence between POIs, instead of treating them individually. Moreover, to overcome the efficiency problem of searching the best region, we propose a sweeping line-based method, and subsequently an constant-bounded algorithm for better efficiency. Experiments on two real-world datasets demonstrate the improved effectiveness of our models over baseline methods and efficiency of the approximate algorithm.	A General Model for Out-of-town Region Recommendation	NA:NA:NA	2017
Ravi Kumar:Maithra Raghu:Tamás Sarlós:Andrew Tomkins	We introduce LAMP: the Linear Additive Markov Process. Transitions in LAMP may be influenced by states visited in the distant history of the process, but unlike higher-order Markov processes, LAMP retains an efficient parameterization. LAMP also allows the specific dependence on history to be learned efficiently from data. We characterize some theoretical properties of LAMP, including its steady-state and mixing time. We then give an algorithm based on alternating minimization to learn LAMP models from data. Finally, we perform a series of real-world experiments to show that LAMP is more powerful than first-order Markov processes, and even holds its own against deep sequential models (LSTMs) with a negligible increase in parameter complexity.	Linear Additive Markov Processes	NA:NA:NA:NA	2017
Alessandro Epasto:Silvio Lattanzi:Sergei Vassilvitskii:Morteza Zadimoghaddam	Maximizing submodular functions under cardinality constraints lies at the core of numerous data mining and machine learning applications, including data diversification, data summarization, and coverage problems. In this work, we study this question in the context of data streams, where elements arrive one at a time, and we want to design low-memory and fast update-time algorithms that maintain a good solution. Specifically, we focus on the sliding window model, where we are asked to maintain a solution that considers only the last W items. In this context, we provide the first non-trivial algorithm that maintains a provable approximation of the optimum using space sublinear in the size of the window. In particular we give a 1/3 - ε approximation algorithm that uses space polylogarithmic in the spread of the values of the elements, δ, and linear in the solution size k for any constant ε > 0. At the same time, processing each element only requires a polylogarithmic number of evaluations of the function itself. When a better approximation is desired, we show a different algorithm that, at the cost of using more memory, provides a 1/2 - ε approximation, and allows a tunable trade-off between average update time and space. This algorithm matches the best known approximation guarantees for submodular optimization in insertion-only streams, a less general formulation of the problem. We demonstrate the efficacy of the algorithms on a number of real world datasets, showing that their practical performance far exceeds the theoretical bounds. The algorithms preserve high quality solutions in streams with millions of items, while storing a negligible fraction of them.	Submodular Optimization Over Sliding Windows	NA:NA:NA:NA	2017
Aneesh Sharma:C. Seshadhri:Ashish Goel	Finding similar user pairs is a fundamental task in social networks, with numerous applications in ranking and personalization tasks such as link prediction and tie strength detection. A common manifestation of user similarity is based upon network structure: each user is represented by a vector that represents the user's network connections, where pairwise cosine similarity among these vectors defines user similarity. The predominant task for user similarity applications is to discover all similar pairs that have a pairwise cosine similarity value larger than a given threshold τ. In contrast to previous work where τ is assumed to be quite close to 1, we focus on recommendation applications where τ is small, but still meaningful. The all pairs cosine similarity problem is computationally challenging on networks with billions of edges, and especially so for settings with small τ. To the best of our knowledge, there is no practical solution for computing all user pairs with, say τ = 0.2 on large social networks, even using the power of distributed algorithms. Our work directly addresses this challenge by introducing a new algorithm --- WHIMP --- that solves this problem efficiently in the MapReduce model. The key insight in WHIMP is to combine the ``wedge-sampling" approach of Cohen-Lewis for approximate matrix multiplication with the SimHash random projection techniques of Charikar. We provide a theoretical analysis of WHIMP, proving that it has near optimal communication costs while maintaining computation cost comparable with the state of the art. We also empirically demonstrate WHIMP's scalability by computing all highly similar pairs on four massive data sets, and show that it accurately finds high similarity pairs. In particular, we note that WHIMP successfully processes the entire Twitter network, which has tens of billions of edges.	When Hashes Met Wedges: A Distributed Algorithm for Finding High Similarity Vectors	NA:NA:NA	2017
Shweta Jain:C. Seshadhri	Clique counts reveal important properties about the structure of massive graphs, especially social networks. The simple setting of just 3-cliques (triangles) has received much attention from the research community. For larger cliques (even, say 6-cliques) the problem quickly becomes intractable because of combinatorial explosion. Most methods used for triangle counting do not scale for large cliques, and existing algorithms require massive parallelism to be feasible. We present a new randomized algorithm that provably approximates the number of k-cliques, for any constant k. The key insight is the use of (strengthenings of) the classic Turán's theorem: this claims that if the edge density of a graph is sufficiently high, the k-clique density must be non-trivial. We define a combinatorial structure called a Turàn shadow, the construction of which leads to fast algorithms for clique counting. We design a practical heuristic, called TURÀN-SHADOW, based on this theoretical algorithm, and test it on a large class of test graphs. In all cases, TURÀN-SHADOW has less than 2% error, in a fraction of the time used by well-tuned exact algorithms. We do detailed comparisons with a range of other sampling algorithms, and find that TURÀN-SHADOW is generally much faster and more accurate. For example, TURÀN-SHADOW estimates all cliques numbers up to size 10 in social network with over a hundred million edges. This is done in less than three hours on a single commodity machine.	A Fast and Provable Method for Estimating Clique Counts Using Turán's Theorem	NA:NA	2017
Gareth Tyson:Shan Huang:Felix Cuadrado:Ignacio Castro:Vasile C. Perta:Arjuna Sathiaseelan:Steve Uhlig	Headers are a critical part of HTTP, and it has been shown that they are increasingly subject to middlebox manipulation. Although this is well known, little is understood about the general regional and network trends that underpin these manipulations. In this paper, we collect data on thousands of networks to understand how they intercept HTTP headers in-the-wild. Our analysis reveals that 25% of measured ASes modify HTTP headers. Beyond this, we witness distinct trends among different regions and AS types; e.g., we observe high numbers of cache headers in poorly connected regions. Finally, we perform an in-depth analysis of the types of manipulations and how they differ across regions.	Exploring HTTP Header Manipulation In-The-Wild	NA:NA:NA:NA:NA:NA:NA	2017
Sanae Rosen:Bo Han:Shuai Hao:Z. Morley Mao:Feng Qian	In HTTP/1.1, it is necessary for the client to request an object (e.g. an image in a page) in order for the server to send it, even if the server knows in advance what the client will need. Server Push is a feature introduced in HTTP/2 that promises to improve page load times (PLT) by having the server push content to the browser in advance. In this paper, we investigate the benefits and challenges of using Server Push on mobile devices. We first examine whether pushing all content or just the CSS and Javascript files performs better, and find the former leads to much better web performance. Also, we find that sites making use of domain sharding or which otherwise have content divided across many servers do not benefit much from Server Push, a major challenge for Server Push going forward. Network performance characteristics also play a major role. Server Push is especially effective at improving performance at high loss rates (16% median PLT reduction with a 2% loss rate) and high latencies (14% PLT reduction with 100 ms latency), and has little benefit for high-speed Ethernet connections. This motivates its use on mobile devices, although we also find the limited processing power of these devices limits the benefits of Server Push. Server Push also offers modest energy benefits, with energy savings of 9% on LTE for one device. Overall, Server Push is a promising approach for improving web performance in mobile networks, but there are a number of challenges in achieving the full benefits of Server Push.	Push or Request: An Investigation of HTTP/2 Server Push for Improving Mobile Performance	NA:NA:NA:NA:NA	2017
Hiranya Jayathilaka:Chandra Krintz:Rich Wolski	In this paper, we describe Roots - a system for automatically identifying the "root cause" of performance anomalies in web applications deployed in Platform-as-a-Service (PaaS) clouds. Roots does not require application-level instrumentation. Instead, it tracks events within the PaaS cloud that are triggered by application requests using a combination of metadata injection and platform-level instrumentation. We describe the extensible architecture of Roots, a prototype implementation of the system, and a statistical methodology for performance anomaly detection and diagnosis. We evaluate the efficacy of Roots using a set of PaaS-hosted web applications, and detail the performance overhead and scalability of the implementation.	Performance Monitoring and Root Cause Analysis for Cloud-hosted Web Applications	NA:NA:NA	2017
Valentin Dalibard:Michael Schaarschmidt:Eiko Yoneki	Due to their complexity, modern systems expose many configuration parameters which users must tune to maximize performance. Auto-tuning has emerged as an alternative in which a black-box optimizer iteratively evaluates configurations to find efficient ones. Unfortunately, for many systems, such as distributed systems, evaluating performance takes too long and the space of configurations is too large for the optimizer to converge within a reasonable time. We present BOAT, a framework which allows developers to build efficient bespoke auto-tuners for their system, in situations where generic auto-tuners fail. At BOAT's core is structured Bayesian optimization (SBO), a novel extension of the Bayesian optimization algorithm. SBO leverages contextual information provided by system developers, in the form of a probabilistic model of the system's behavior, to make informed decisions about which configurations to evaluate. In a case study, we tune the scheduling of a neural network computation on a heterogeneous cluster. Our auto-tuner converges within ten iterations. The optimized configurations outperform those found by generic auto-tuners in thirty iterations by up to 2X.	BOAT: Building Auto-Tuners with Structured Bayesian Optimization	NA:NA:NA	2017
Christoph Trattner:David Elsweiler	Food recommenders have the potential to positively influence the eating habits of users. To achieve this, however, we need to understand how healthy recommendations are and the factors which influence this. Focusing on two approaches from the literature (single item and daily meal plan recommendation) and utilizing a large Internet sourced dataset from Allrecipes.com, we show how algorithmic solutions relate to the healthiness of the underlying recipe collection. First, we analyze the healthiness of Allrecipes.com recipes using nutritional standards from the World Health Organisation and the United Kingdom Food Standards Agency. Second, we investigate user interaction patterns and how these relate to the healthiness of recipes. Third, we experiment with both recommendation approaches. Our results indicate that overall the recipes in the collection are quite unhealthy, but this varies across categories on the website. Users in general tend to interact most often with the least healthy recipes. Recommender algorithms tend to score popular items highly and thus on average promote unhealthy items. This can be tempered, however, with simple post-filtering approaches, which we show by experiment are better suited to some algorithms than others. Similarly, we show that the generation of meal plans can dramatically increase the number of healthy options open to users. One of the main findings is, nevertheless, that the utility of both approaches is strongly restricted by the recipe collection. Based on our findings we draw conclusions how researchers should attempt to make food recommendation systems promote healthy nutrition.	Investigating the Healthiness of Internet-Sourced Recipes: Implications for Meal Planning and Recommender Systems	NA:NA	2017
Deepika Yadav:Pushpendra Singh:Kyle Montague:Vijay Kumar:Deepak Sood:Madeline Balaam:Drishti Sharma:Mona Duggal:Tom Bartindale:Delvin Varghese:Patrick Olivier	The Healthcare system of India provides outreach services to the rural population with a key focus on the maternal and child health through its flagship program of Community Health Workers (CHWs). The program since its launch has reached a scale of over 900000 health workers across the country and observed significant benefits on the health indicators. However, traditional face to face training mechanisms face persistent challenge in providing adequate training and capacity building opportunities to CHWs which leads to their sub-optimal knowledge and skill sets. In this paper, we propose Sangoshthi, a low-cost mobile based training and learning platform that fits well into the environment of low-Internet access. Sangoshthi leverages the architecture that combines Internet and IVR technology to host real time training sessions with the CHWs having access to basic phones only. We present our findings of a four week long field deployment with 40 CHWs using both qualitative and quantitative methods. Sangoshthi offers a lively environment of peer learning that was well received by the CHW community and resulted into their knowledge gains (16%) and increased confidence levels to handle the cases. Our study highlights the potential of complementary training platforms that can empower CHWs in-situ without the need of additional infrastructure.	Sangoshthi: Empowering Community Health Workers through Peer Learning in Rural India	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2017
Ferda Ofli:Yusuf Aytar:Ingmar Weber:Raggi al Hammouri:Antonio Torralba	Food is an integral part of our life and what and how much we eat crucially affects our health. Our food choices largely depend on how we perceive certain characteristics of food, such as whether it is healthy, delicious or if it qualifies as a salad. But these perceptions differ from person to person and one person's "single lettuce leaf" might be another person's "side salad". Studying how food is perceived in relation to what it actually is typically involves a laboratory setup. Here we propose to use recent advances in image recognition to tackle this problem. Concretely, we use data for 1.9 million images from Instagram from the US to look at systematic differences in how a machine would objectively label an image compared to how a human subjectively does. We show that this difference, which we call the "perception gap", relates to a number of health outcomes observed at the county level. To the best of our knowledge, this is the first time that image recognition is being used to study the "misalignment" of how people describe food images vs. what they actually depict.	Is Saki #delicious?: The Food Perception Gap on Instagram and Its Relation to Health	NA:NA:NA:NA:NA	2017
David Stück:Haraldur Tómas Hallgrímsson:Greg Ver Steeg:Alessandro Epasto:Luca Foschini	Many behaviors that lead to worsened health outcomes are modifiable, social, and visible. Social influence has thus the potential to foster adoption of habits that promote health and improve disease management. In this study, we consider the evolution of the physical activity of 44.5 thousand Fitbit users as they interact on the Fitbit social network, in relation to their health status. The users collectively recorded 9.3 million days of steps over the period of a year through a Fitbit device. 7,515 of the users also self-reported whether they were diagnosed with a major chronic condition. A time-aggregated analysis shows that ego net size, average alter physical activity, gender, and body mass index (BMI) are significantly predictive of ego physical activity. For users who self-reported chronic conditions, the direction and effect size of associations varied depending on the condition, with diabetic users specifically showing almost a 6-fold increase in additional daily steps for each additional social tie. Subsequently, we consider the co-evolution of activity and friendship longitudinally on a month by month basis. We show that the fluctuations in average alter activity significantly predict fluctuations in ego activity. By leveraging a class of novel non-parametric statistical tests we investigate the causal factors in these fluctuations. We find that under certain stationarity assumptions, non-null causal dependence exists between ego and alter's activity, even in the presence of unobserved stationary individual traits. We believe that our findings provide evidence that the study of online social networks have the potential to improve our understanding of factors affecting adoption of positive habits, especially in the context of chronic condition management.	The Spread of Physical Activity Through Social Networks	NA:NA:NA:NA:NA	2017
David Goldberg:Andrew Trotman:Xiao Wang:Wei Min:Zongru Wan	The quality of a search engine is typically evaluated using hand-labeled data sets, where the labels indicate the relevance of documents to queries. Often the number of labels needed is too large to be created by the best annotators, and so less accurate labels (e.g. from crowdsourcing) must be used. This introduces errors in the labels, and thus errors in standard precision metrics (such as [email protected] and DCG); the lower the quality of the judge, the more errorful the labels, consequently the more inaccurate the metric. We introduce equations and algorithms that can adjust the metrics to the values they would have had if there were no annotation errors. This is especially important when two search engines are compared by comparing their metrics. We give examples where one engine appeared to be statistically significantly better than the other, but the effect disappeared after the metrics were corrected for annotation error. In other words the evidence supporting a statistical difference was illusory, and caused by a failure to account for annotation error.	Drawing Sound Conclusions from Noisy Judgments	NA:NA:NA:NA:NA	2017
Liangda Li:Hongbo Deng:Anlei Dong:Yi Chang:Ricardo Baeza-Yates:Hongyuan Zha	Contextual data plays an important role in modeling search engine users' behaviors on both query auto-completion (QAC) log and normal query (click) log. User's recent search history on each log has been widely studied individually as the context to benefit the modeling of users' behaviors on that log. However, there is no existing work that explores or incorporates both logs together for contextual data. As QAC and click logs actually record users' sequential behaviors while interacting with a search engine, the available context of a user's current behavior based on the same type of log can be strengthened from the user's recent search history shown on the other type of log. Our paper proposes to model users' behaviors on both QAC and click logs simultaneously by utilizing both logs as the contextual data of each other. The key idea is to capture the correlation between users' behavior patterns on both logs. We model such correlation through a novel probabilistic model based on the Latent Dirichlet allocation (LDA) model. The learned users' behavior patterns on both logs are utilized to address not only the application of query auto-completion on QAC logs, but also the click prediction and relevance ranking of web documents on click logs. Experiments on real-world logs demonstrate the effectiveness of the proposed model on both applications.	Exploring Query Auto-Completion and Click Logs for Contextual-Aware Web Search and Query Suggestion	NA:NA:NA:NA:NA:NA	2017
Adam Fourney:Meredith Ringel Morris:Ryen W. White	Many people rely on web search engines to check the spelling or grammatical correctness of input phrases. For example, one might search [recurring or reoccurring] to decide between these similar words. While language-related queries are common, they have low click-through rates, lack a strong intent signal, and are generally challenging to study. Perhaps for these reasons, they have yet to be characterized in the literature. In this paper we report the results of two surveys that investigate how, when, and why people use web search to support low-level, language-related tasks. The first survey was distributed by email, and asked participants to reflect on a recent search task. The second survey was embedded directly in search result pages, and captured information about searchers' intents in-situ. Our analysis confirms that language-related search tasks are indeed common, accounting for at least 2.7% of all queries posed by our respondents. Survey responses also reveal: (1) the range of language-related tasks people perform with search, (2) the contexts in which these tasks arise, and (3), the reasons why people elect to use web search rather than relying on traditional proofing tools (e.g., spelling and grammar checkers).	Web Search as a Linguistic Tool	NA:NA:NA	2017
Yashen Wang:Heyan Huang:Chong Feng	We tackle the problem of improving microblog retrieval algorithms by proposing a Feedback Concept Model for query expansion. In particular, we expand the query using knowledge information derived from Probase so that the expanded one could better reflect users' search intent, which allows for microblog retrieval at a concept-level, rather than term-level. In the proposed feedback concept model: (i) we mine the concept information implicit in short-texts based on the external knowledge bases; (ii) with the relevant concepts associated with short-texts, a mixture model is generated to estimate a concept language model; (iii) finally, we utilize the concept language model for query expansion. Moreover, we incorporate temporal prior into the proposed query expansion method to satisfy real-time information need. Finally, we test the generalization power of the feedback concept model on the TREC Microblog corpora. The experimental results demonstrate that the proposed model outperforms the previous methods for microblog retrieval significantly.	Query Expansion Based on a Feedback Concept Model for Microblog Retrieval	NA:NA:NA	2017
Karol Wegrzycki:Piotr Sankowski:Andrzej Pacuk:Piotr Wygocki	We introduce random directed acyclic graph and use it to model the information diffusion network. Subsequently, we analyze the cascade generation model (CGM) introduced by Leskovec et al. [19]. Until now only empirical studies of this model were done. In this paper, we present the first theoretical proof that the sizes of cascades generated by the CGM follow the power-law distribution, which is consistent with multiple empirical analysis of the large social networks. We compared the assumptions of our model with the Twitter social network and tested the goodness of approximation.	Why Do Cascade Sizes Follow a Power-Law?	NA:NA:NA:NA	2017
Cheng Li:Jiaqi Ma:Xiaoxiao Guo:Qiaozhu Mei	Information cascades, effectively facilitated by most social network platforms, are recognized as a major factor in almost every social success and disaster in these networks. Can cascades be predicted? While many believe that they are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted by a machine learning algorithm that combines many features. These predictors all depend on a bag of hand-crafting features to represent the cascade network and the global network structures. Such features, always carefully and sometimes mysteriously designed, are not easy to extend or to generalize to a different platform or domain. Inspired by the recent successes of deep learning in multiple data mining tasks, we investigate whether an end-to-end deep learning approach could effectively predict the future size of cascades. Such a method automatically learns the representation of individual cascade graphs in the context of the global network structure, without hand-crafted features or heuristics. We find that node embeddings fall short of predictive power, and it is critical to learn the representation of a cascade graph as a whole. We present algorithms that learn the representation of cascade graphs in an end-to-end manner, which significantly improve the performance of cascade prediction over strong baselines including feature based methods, node embedding methods, and graph kernel methods. Our results also provide interesting implications for cascade prediction in general.	DeepCas: An End-to-end Predictor of Information Cascades	NA:NA:NA:NA	2017
Rahmtin Rotabi:Krishna Kamath:Jon Kleinberg:Aneesh Sharma	Cascades on social and information networks have been a tremendously popular subject of study in the past decade, and there is a considerable literature on phenomena such as diffusion mechanisms, virality, cascade prediction, and peer network effects. Against the backdrop of this research, a basic question has received comparatively little attention: how desirable are cascades on a social media platform from the point of view of users' While versions of this question have been considered from the perspective of the producers of cascades, any answer to this question must also take into account the effect of cascades on their audience --- the viewers of the cascade who do not directly participate in generating the content that launched it. In this work, we seek to fill this gap by providing a consumer perspective of information cascades. Users on social and information networks play the dual role of producers and consumers, and our work focuses on how users perceive cascades as consumers. Starting from this perspective, we perform an empirical study of the interaction of Twitter users with retweet cascades. We measure how often users observe retweets in their home timeline, and observe a phenomenon that we term the Impressions Paradox: the share of impressions for cascades of size k decays much more slowly than frequency of cascades of size k. Thus, the audience for cascades can be quite large even for rare large cascades. We also measure audience engagement with retweet cascades in comparison to non-retweeted or organic content. Our results show that cascades often rival or exceed organic content in engagement received per impression. This result is perhaps surprising in that consumers didn't opt in to see tweets from these authors. Furthermore, although cascading content is widely popular, one would expect it to eventually reach parts of the audience that may not be interested in the content. Motivated by the tension in these empirical findings, we posit a simple theoretical model that focuses on the effect of cascades on the audience (rather than the cascade producers). Our results on this model highlight the balance between retweeting as a high-quality content selection mechanism and the role of network users in filtering irrelevant content. In particular, the results suggest that together these two effects enable the audience to consume a high quality stream of content in the presence of cascades.	Cascades: A View from Audience	NA:NA:NA:NA	2017
Karthik Subbian:B. Aditya Prakash:Lada Adamic	Detecting large reshare cascades is an important problem in online social networks. There are a variety of attempts to model this problem, from using time series analysis methods to stochastic processes. Most of these approaches heavily depend on the underlying network features and use network information to detect the virality of cascades. In most cases, however, getting such detailed network information can be hard or even impossible. In contrast, in this paper, we propose SANSNET, a network-agnostic approach instead. Our method can be used to answer two important questions: (1) Will a cascade go viral? and (2) How early can we predict it? We use techniques from survival analysis to build a supervised classifier in the space of survival probabilities and show that the optimal decision boundary is a survival function. A notable feature of our approach is that it does not use any network-based features for the prediction tasks, making it very cheap to implement. Finally, we evaluate our approach on several real-life data sets, including popular social networks like Facebook and Twitter, on metrics like recall, F-measure and breakout coverage. We find that network agnostic SANSNET classifier outperforms several non-trivial competitors and baselines which utilize network information.	Detecting Large Reshare Cascades in Social Networks	NA:NA:NA	2017
Zhijing Li:Ana Nika:Xinyi Zhang:Yanzi Zhu:Yuanshun Yao:Ben Y. Zhao:Haitao Zheng	While crowdsourcing is an attractive approach to collect large-scale wireless measurements, understanding the quality and variance of the resulting data is difficult. Our work analyzes the quality of crowdsourced cellular signal measurements in the context of basestation localization, using large international public datasets (419M signal measurements and 1M cells) and corresponding ground truth values. Performing localization using raw received signal strength (RSS) data produces poor results and very high variance. Applying supervised learning improves results moderately, but variance remains high. Instead, we propose feature clustering, a novel application of unsupervised learning to detect hidden correlation between measurement instances, their features, and localization accuracy. Our results identify RSS standard deviation and RSS-weighted dispersion mean as key features that correlate with highly predictive measurement samples for both sparse and dense measurements respectively. Finally, we show how optimizing crowdsourcing measurements for these two features dramatically improves localization accuracy and reduces variance.	Identifying Value in Crowdsourced Wireless Signal Measurements	NA:NA:NA:NA:NA:NA:NA	2017
Nikhil Garg:Vijay Kamble:Ashish Goel:David Marn:Kamesh Munagala	Many societal decision problems lie in high-dimensional continuous spaces not amenable to the voting techniques common for their discrete or single-dimensional counterparts. These problems are typically discretized before running an election or decided upon through negotiation by representatives. We propose a meta-algorithm called Iterative Local Voting for collective decision-making in this setting, in which voters are sequentially sampled and asked to modify a candidate solution within some local neighborhood of its current value, as defined by a ball in some chosen norm. In general, such schemes do not converge, or, when they do, the resulting solution does not have a natural description. We first prove the convergence of this algorithm under appropriate choices of neighborhoods to plausible solutions in certain natural settings: when the voters' utilities can be expressed in terms of some form of distance from their ideal solution, and when these utilities are additively decomposable across dimensions. In many of these cases, we obtain convergence to the societal welfare maximizing solution. We then describe an experiment in which we test our algorithm for the decision of the U.S. Federal Budget on Mechanical Turk with over 4,000 workers, employing neighborhoods defined by §L1, §L2 and §L∞ balls. We make several observations that inform future implementations of such a procedure.	Collaborative Optimization for Collective Decision-making in Continuous Spaces	NA:NA:NA:NA:NA	2017
Leye Wang:Dingqi Yang:Xiao Han:Tianben Wang:Daqing Zhang:Xiaojuan Ma	In traditional mobile crowdsensing applications, organizers need participants' precise locations for optimal task allocation, e.g., minimizing selected workers' travel distance to task locations. However, the exposure of their locations raises privacy concerns. Especially for those who are not eventually selected for any task, their location privacy is sacrificed in vain. Hence, in this paper, we propose a location privacy-preserving task allocation framework with geo-obfuscation to protect users' locations during task assignments. Specifically, we make participants obfuscate their reported locations under the guarantee of differential privacy, which can provide privacy protection regardless of adversaries' prior knowledge and without the involvement of any third-part entity. In order to achieve optimal task allocation with such differential geo-obfuscation, we formulate a mixed-integer non-linear programming problem to minimize the expected travel distance of the selected workers under the constraint of differential privacy. Evaluation results on both simulation and real-world user mobility traces show the effectiveness of our proposed framework. Particularly, our framework outperforms Laplace obfuscation, a state-of-the-art differential geo-obfuscation mechanism, by achieving 45% less average travel distance on the real-world data.	Location Privacy-Preserving Task Allocation for Mobile Crowdsensing with Differential Geo-Obfuscation	NA:NA:NA:NA:NA:NA	2017
Yaguang Li:Han Su:Ugur Demiryurek:Bolong Zheng:Tieke He:Cyrus Shahabi	The turn-by-turn directions provided in existing navigation applications are exclusively derived from underlying road network topology information, i.e., the connectivity of edges to each other. Therefore, the turn-by-turn directions are simplified as metric translation of physical world (e.g. distance/time to turn) to spoken language. Such translation - that ignores human cognition of the geographic space - is often verbose and redundant for the drivers who have knowledge about the geographical areas. In this paper, we study a Personalized RoutE Guidance System dubbed PaRE - with which the goal is to generate more customized and intuitive directions based on user generated content. PaRE utilizes a wealth of user generated historical trajectory data to extract namely "landmarks" (e.g., point of interests or intersections) and frequently visited routes between them from the road network. The extracted information is used to obtain cognitive customized directions for each user. We formalize this task as a problem of finding the optimal partition for a given route that maximizes the familiarity while minimizing the number of segments in the partition, and propose two efficient algorithms to solve it. For empirical study, we apply our solution to both real and synthetic trajectory datasets to evaluate the performance and effectiveness of PaRE.	PaRE: A System for Personalized Route Guidance	NA:NA:NA:NA:NA:NA	2017
Milivoj Simeonovski:Giancarlo Pellegrino:Christian Rossow:Michael Backes	The Internet is built on top of intertwined network services, e.g., email, DNS, and content distribution networks operated by private or governmental organizations. Recent events have shown that these organizations may, knowingly or unknowingly, be part of global-scale security incidents including state-sponsored mass surveillance programs and large-scale DDoS attacks. For example, in March 2015 the Great Cannon attack has shown that an Internet service provider can weaponize millions of Web browsers and turn them into DDoS bots by injecting malicious JavaScript code into transiting TCP connections. While attack techniques and root cause vulnerabilities are routinely studied, we still lack models and algorithms to study the intricate dependencies between services and providers, reason on their abuse, and assess the attack impact. To close this gap, we present a technique that models services, providers, and dependencies as a property graph. Moreover, we present a taint-style propagation-based technique to query the model, and present an evaluation of our framework on the top 100k Alexa domains.	Who Controls the Internet?: Analyzing Global Threats using Property Graph Traversals	NA:NA:NA:NA	2017
Rebecca S. Portnoff:Sadia Afroz:Greg Durrett:Jonathan K. Kummerfeld:Taylor Berg-Kirkpatrick:Damon McCoy:Kirill Levchenko:Vern Paxson	Underground forums are widely used by criminals to buy and sell a host of stolen items, datasets, resources, and criminal services. These forums contain important resources for understanding cybercrime. However, the number of forums, their size, and the domain expertise required to understand the markets makes manual exploration of these forums unscalable. In this work, we propose an automated, top-down approach for analyzing underground forums. Our approach uses natural language processing and machine learning to automatically generate high-level information about underground forums, first identifying posts related to transactions, and then extracting products and prices. We also demonstrate, via a pair of case studies, how an analyst can use these automated approaches to investigate other categories of products and transactions. We use eight distinct forums to assess our tools: Antichat, Blackhat World, Carders, Darkode, Hack Forums, Hell, L33tCrew and Nulled. Our automated approach is fast and accurate, achieving over 80% accuracy in detecting post category, product, and prices.	Tools for Automated Analysis of Cybercriminal Markets	NA:NA:NA:NA:NA:NA:NA:NA	2017
Qian Cui:Guy-Vincent Jourdan:Gregor V. Bochmann:Russell Couturier:Iosif-Viorel Onut	The so-called ``phishing'' attacks are one of the important threats to individuals and corporations in today's Internet. Combatting phishing is thus a top-priority, and has been the focus of much work, both on the academic and on the industry sides. In this paper, we look at this problem from a new angle. We have monitored a total of 19,066 phishing attacks over a period of ten months and found that over 90% of these attacks were actually replicas or variations of other attacks in the database. This provides several opportunities and insights for the fight against phishing: first, quickly and efficiently detecting replicas is a very effective prevention tool. We detail one such tool in this paper. Second, the widely held belief that phishing attacks are dealt with promptly is but an illusion. We have recorded numerous attacks that stay active throughout our observation period. This shows that the current prevention techniques are ineffective and need to be overhauled. We provide some suggestions in this direction. Third, our observation give a new perspective into the modus operandi of attackers. In particular, some of our observations suggest that a small group of attackers could be behind a large part of the current attacks. Taking down that group could potentially have a large impact on the phishing attacks observed today.	Tracking Phishing Attacks Over Time	NA:NA:NA:NA:NA	2017
Deepak Kumar:Zane Ma:Zakir Durumeric:Ariana Mirian:Joshua Mason:J. Alex Halderman:Michael Bailey	Over the past 20 years, websites have grown increasingly complex and interconnected. In 2016, only a negligible number of sites are dependency free, and over 90% of sites rely on external content. In this paper, we investigate the current state of web dependencies and explore two security challenges associated with the increasing reliance on external services: (1) the expanded attack surface associated with serving unknown, implicitly trusted third-party content, and (2) how the increased set of external dependencies impacts HTTPS adoption. We hope that by shedding light on these issues, we can encourage developers to consider the security risks associated with serving third-party content and prompt service providers to more widely deploy HTTPS.	Security Challenges in an Increasingly Tangled Web	NA:NA:NA:NA:NA:NA:NA	2017
Xiaohan Li:Shu Wu:Liang Wang	Recently, the percentage of people with hypertension is increasing, and this phenomenon is widely concerned. At the same time, wireless home Blood Pressure (BP) monitors become accessible in people's life. Since machine learning methods have made important contributions in different fields, many researchers have tried to employ them in dealing with medical problems. However, the existing studies for BP prediction are all based on clinical data with short time ranges. Besides, there do not exist works which can jointly make use of historical measurement data (e.g. BP and heart rate) and contextual data (e.g. age, gender, BMI and altitude). Recurrent Neural Networks (RNNs), especially those using Long Short-Term Memory (LSTM) units, can capture long range dependencies, so they are effective in modeling variable-length sequences. In this paper, we propose a novel model named recurrent models with contextual layer, which can model the sequential measurement data and contextual data simultaneously to predict the trend of users' BP. We conduct our experiments on the BP data set collected from a type of wireless home BP monitors, and experimental results show that the proposed models outperform several competitive compared methods.	Blood Pressure Prediction via Recurrent Models with Contextual Layer	NA:NA:NA	2017
Vasileios Lampos:Bin Zou:Ingemar Johansson Cox	Health surveillance systems based on online user-generated content often rely on the identification of textual markers that are related to a target disease. Given the high volume of available data, these systems benefit from an automatic feature selection process. This is accomplished either by applying statistical learning techniques, which do not consider the semantic relationship between the selected features and the inference task, or by developing labour-intensive text classifiers. In this paper, we use neural word embeddings, trained on social media content from Twitter, to determine, in an unsupervised manner, how strongly textual features are semantically linked to an underlying health concept. We then refine conventional feature selection methods by a priori operating on textual variables that are sufficiently close to a target concept. Our experiments focus on the supervised learning problem of estimating influenza-like illness rates from Google search queries. A "flu infection" concept is formulated and used to reduce spurious and potentially confounding features that were selected by previously applied approaches. In this way, we also address forms of scepticism regarding the appropriateness of the feature space, alleviating potential cases of overfitting. Ultimately, the proposed hybrid feature selection method creates a more reliable model that, according to our empirical analysis, improves the inference performance (Mean Absolute Error) of linear and nonlinear regressors by 12% and 28.7%, respectively.	Enhancing Feature Selection Using Word Embeddings: The Case of Flu Surveillance	NA:NA:NA	2017
Kathy Lee:Ashequl Qadir:Sadid A. Hasan:Vivek Datla:Aaditya Prakash:Joey Liu:Oladimeji Farri	Current Adverse Drug Events (ADE) surveillance systems are often associated with a sizable time lag before such events are published. Online social media such as Twitter could describe adverse drug events in real-time, prior to official reporting. Deep learning has significantly improved text classification performance in recent years and can potentially enhance ADE classification in tweets. However, these models typically require large corpora with human expert-derived labels, and such resources are very expensive to generate and are hardly available. Semi-supervised deep learning models, which offer a plausible alternative to fully supervised models, involve the use of a small set of labeled data and a relatively larger collection of unlabeled data for training. Traditionally, these models are trained on labeled and unlabeled data from similar topics or domains. In reality, millions of tweets generated daily often focus on disparate topics, and this could present a challenge for building deep learning models for ADE classification with random Twitter stream as unlabeled training data. In this work, we build several semi-supervised convolutional neural network (CNN) models for ADE classification in tweets, specifically leveraging different types of unlabeled data in developing the models to address the problem. We demonstrate that, with the selective use of a variety of unlabeled data, our semi-supervised CNN models outperform a strong state-of-the-art supervised classification model by +9.9% F1-score. We evaluated our models on the Twitter data set used in the PSB 2016 Social Media Shared Task. Our results present the new state-of-the-art for this data set.	Adverse Drug Event Detection in Tweets with Semi-Supervised Convolutional Neural Networks	NA:NA:NA:NA:NA:NA:NA	2017
Yoshihiko Suhara:Yinzhan Xu:Alex 'Sandy' Pentland	Depression is a prevailing issue and is an increasing problem in many people's lives. Without observable diagnostic criteria, the signs of depression may go unnoticed, resulting in high demand for detecting depression in advance automatically. This paper tackles the challenging problem of forecasting severely depressed moods based on self-reported histories. Despite the large amount of research on understanding individual moods including depression, anxiety, and stress based on behavioral logs collected by pervasive computing devices such as smartphones, forecasting depressed moods is still an open question. This paper develops a recurrent neural network algorithm that incorporates categorical embedding layers for forecasting depression. We collected large-scale records from 2,382 self-declared depressed people to conduct the experiment. Experimental results show that our method forecast the severely depressed mood of a user based on self-reported histories, with higher accuracy than SVM. The results also showed that the long-term historical information of a user improves the accuracy of forecasting depressed mood.	DeepMood: Forecasting Depressed Mood Based on Self-Reported Histories via Recurrent Neural Networks	NA:NA:NA	2017
Minh X. Hoang:Xuan-Hong Dang:Xiang Wu:Zhenyu Yan:Ambuj K. Singh	Predicting the popularity of online content in social networks is important in many applications, ranging from ad campaign design, web content caching and prefetching, to web-search result ranking. Earlier studies target this problem by learning models that either generalize behaviors of the entire network population or capture behaviors of each individual user. In this paper, we claim that a novel approach based on group-level popularity is necessary and more practical, given that users naturally organize themselves into clusters and that users within a cluster react to online content in a uniform manner. We develop a novel framework by first grouping users into cohesive clusters, and then adopt tensor decomposition to make predictions. In order to minimize the impact of noisy data and be more flexible in capturing changes in users' interests, our framework exploits both the network topology and interaction among users in learning a robust user clustering. The PARAFAC tensor decomposition is adapted to work with hierarchical constraint over user groups, and we show that optimizing this constrained function via gradient descent achieves faster convergence and leads to more stable solutions. Extensive experimental results over two social networks demonstrate that our framework is scalable, finds meaningful user groups, and significantly outperforms eight baseline methods in terms of prediction accuracy.	GPOP: Scalable Group-level Popularity Prediction for Online Content in Social Networks	NA:NA:NA:NA:NA	2017
Marian-Andrei Rizoiu:Lexing Xie:Scott Sanner:Manuel Cebrian:Honglin Yu:Pascal Van Hentenryck	Modeling and predicting the popularity of online content is a significant problem for the practice of information dissemination, advertising, and consumption. Recent work analyzing massive datasets advances our understanding of popularity, but one major gap remains: To precisely quantify the relationship between the popularity of an online item and the external promotions it receives. This work supplies the missing link between exogenous inputs from public social media platforms, such as Twitter, and endogenous responses within the content platform, such as YouTube. We develop a novel mathematical model, the Hawkes intensity process, which can explain the complex popularity history of each video according to its type of content, network of diffusion, and sensitivity to promotion. Our model supplies a prototypical description of videos, called an endo-exo map. This map explains popularity as the result of an extrinsic factor -- the amount of promotions from the outside world that the video receives, acting upon two intrinsic factors -- sensitivity to promotion, and inherent virality. We use this model to forecast future popularity given promotions on a large 5-months feed of the most-tweeted videos, and found it to lower the average error by 28.6% from approaches based on popularity history. Finally, we can identify videos that have a high potential to become viral, as well as those for which promotions will have hardly any effect.	Expecting to be HIP: Hawkes Intensity Processes for Social Media Popularity	NA:NA:NA:NA:NA:NA	2017
Andrés Abeliuk:Gerardo Berbeglia:Pascal Van Hentenryck:Tad Hogg:Kristina Lerman	Unpredictability is often portrayed as an undesirable outcome of social influence in cultural markets. Unpredictability stems from the "rich get richer" effect, whereby small fluctuations in the market share or popularity of products are amplified over time by social influence. In this paper, we report results of an experimental study that shows that unpredictability is not an inherent property of social influence. We investigate strategies for creating markets in which the popularity of products is better-and more predictably-aligned with their underlying quality. For our study, we created a cultural market of science stories and conducted randomized experiments on different policies for presenting the stories to study participants. Specifically, we varied how the stories were ranked, and whether or not participants were shown the ratings these stories received from others. We present a policy that leverages social influence and product positioning to help distinguish the product's market share (popularity) from underlying quality. Highlighting products with the highest estimated quality reduces the "rich get richer" effect highlighting popular products. We show that this policy allows us to more robustly and predictably identify high quality products and promote blockbusters. The policy can be used to create more efficient online cultural markets with a better allocation of resources to products.	Taming the Unpredictability of Cultural Markets with Social Influence	NA:NA:NA:NA:NA	2017
Julia Proskurnia:Przemyslaw Grabowicz:Ryota Kobayashi:Carlos Castillo:Philippe Cudré-Mauroux:Karl Aberer	Applying classical time-series analysis techniques to online content is challenging, as web data tends to have data quality issues and is often incomplete, noisy, or poorly aligned. In this paper, we tackle the problem of predicting the evolution of a time series of user activity on the web in a manner that is both accurate and interpretable, using related time series to produce a more accurate prediction. We test our methods in the context of predicting signatures for online petitions using data from thousands of petitions posted on The Petition Site - one of the largest platforms of its kind. We observe that the success of these petitions is driven by a number of factors, including promotion through social media channels and on the front page of the petitions platform. We propose an interpretable model that incorporates seasonality, aging effects, self-excitation, and external effects. The interpretability of the model is important for understanding the elements that drives the activity of an online content. We show through an extensive empirical evaluation that our model is significantly better at predicting the outcome of a petition than state-of-the-art techniques.	Predicting the Success of Online Petitions Leveraging Multidimensional Time-Series	NA:NA:NA:NA:NA:NA	2017
Jiani Zhang:Xingjian Shi:Irwin King:Dit-Yan Yeung	Knowledge Tracing (KT) is a task of tracing evolving knowledge state of students with respect to one or more concepts as they engage in a sequence of learning activities. One important purpose of KT is to personalize the practice sequence to help students learn knowledge concepts efficiently. However, existing methods such as Bayesian Knowledge Tracing and Deep Knowledge Tracing either model knowledge state for each predefined concept separately or fail to pinpoint exactly which concepts a student is good at or unfamiliar with. To solve these problems, this work introduces a new model called Dynamic Key-Value Memory Networks (DKVMN) that can exploit the relationships between underlying concepts and directly output a student's mastery level of each concept. Unlike standard memory-augmented neural networks that facilitate a single memory matrix or two static memory matrices, our model has one static matrix called key, which stores the knowledge concepts and the other dynamic matrix called value, which stores and updates the mastery levels of corresponding concepts. Experiments show that our model consistently outperforms the state-of-the-art model in a range of KT datasets. Moreover, the DKVMN model can automatically discover underlying concepts of exercises typically performed by human annotations and depict the changing knowledge state of a student.	Dynamic Key-Value Memory Networks for Knowledge Tracing	NA:NA:NA:NA	2017
Simon Walk:Lisette Esín-Noboa:Denis Helic:Markus Strohmaier:Mark A. Musen	Ontologies in the biomedical domain are numerous, highly specialized and very expensive to develop. Thus, a crucial prerequisite for ontology adoption and reuse is effective support for exploring and finding existing ontologies. Towards that goal, the National Center for Biomedical Ontology (NCBO) has developed BioPortal---an online repository containing more than 500 biomedical ontologies. In 2016, BioPortal represents one of the largest portals for exploration of semantic biomedical vocabularies and terminologies, which is used by many researchers and practitioners. While usage of this portal is high, we know very little about how exactly users search and explore ontologies and what kind of usage patterns or user groups exist in the first place. Deeper insights into user behavior on such portals can provide valuable information to devise strategies for a better support of users in exploring and finding existing ontologies, and thereby enable better ontology reuse. To that end, we study and group users according to their browsing behavior on BioPortal and use data mining techniques to characterize and compare exploration strategies across ontologies. In particular, we were able to identify seven distinct browsing types, all relying on different functionality provided by BioPortal. For example, Search Explorers extensively use the search functionality while Ontology Tree Explorers mainly rely on the class hierarchy for exploring ontologies. Further, we show that specific characteristics of ontologies influence the way users explore and interact with the website. Our results may guide the development of more user-oriented systems for ontology exploration on the Web.	How Users Explore Ontologies on the Web: A Study of NCBO's BioPortal Usage Logs	NA:NA:NA:NA:NA	2017
HyeongSik Kim:Padmashree Ravindra:Kemafor Anyanwu	Scalable query processing relies on early and aggressive determination and pruning of query-irrelevant data. Besides the traditional space-pruning techniques such as indexing, type-based optimizations that exploit integrity constraints defined on the types can be used to rewrite queries into more efficient ones. However, such optimizations are only applicable in strongly-typed data and query models which make it a challenge for semi-structured models such as RDF. Consequently, developing techniques for enabling typebased query optimizations will contribute new insight to improving the scalability of RDF processing systems. In this paper, we address the challenge of type-based query optimization for RDF graph pattern queries. The approach comprises of (i) a novel type system for RDF data induced from data and ontologies and (ii) a query optimization and evaluation framework for evaluating graph pattern queries using type-based optimizations. An implementation of this approach integrated into Apache Pig is presented and evaluated. Comprehensive experiments conducted on real-world and synthetic benchmark datasets show that our approach is up to 500X faster than existing approaches	Type-based Semantic Optimization for Scalable RDF Graph Pattern Matching	NA:NA:NA	2017
Marco Brambilla:Stefano Ceri:Emanuele Della Valle:Riccardo Volonterio:Felix Xavier Acero Salazar	Massive data integration technologies have been recently used to produce very large ontologies. However, knowledge in the world continuously evolves, and ontologies are largely incomplete for what concerns low-frequency data, belonging to the so-called long tail. Socially produced content is an excellent source for discovering emerging knowledge: it is huge, and immediately reflects the relevant changes which hide emerging entities. Thus, we propose a method for discovering emerging entities by extracting them from social content. Once instrumented by experts through very simple initialization, the method is capable of finding emerging entities; we use a purely syntactic method as a baseline, and we propose several semantics-based variants. The method uses seeds, i.e. prototypes of emerging entities provided by experts, for generating candidates; then, it associates candidates to feature vectors, built by using terms occurring in their social content, and then ranks the candidates by using their distance from the centroid of seeds, returning the top candidates as result. The method can be continuously or periodically iterated, using the results as new seeds. We validate our method by applying it to a set of diverse domain-specific application scenarios, spanning fashion, literature, and exhibitions.	Extracting Emerging Knowledge from Social Media	NA:NA:NA:NA:NA	2017
Cuong Xuan Chu:Niket Tandon:Gerhard Weikum	Knowledge graphs have become a fundamental asset for search engines. A fair amount of user queries seek information on problem-solving tasks such as building a fence or repairing a bicycle. However, knowledge graphs completely lack this kind of how-to knowledge. This paper presents a method for automatically constructing a formal knowledge base on tasks and task-solving steps, by tapping the contents of online communities such as WikiHow. We employ Open-IE techniques to extract noisy candidates for tasks, steps and the required tools and other items. For cleaning and properly organizing this data, we devise embedding-based clustering techniques. The resulting knowledge base, HowToKB, includes a hierarchical taxonomy of disambiguated tasks, temporal orders of sub-tasks, and attributes for involved items. A comprehensive evaluation of HowToKB shows high accuracy. As an extrinsic use case, we evaluate automatically searching related YouTube videos for HowToKB tasks.	Distilling Task Knowledge from How-To Communities	NA:NA:NA	2017
Seongsoon Kim:Seongwoon Lee:Donghyeon Park:Jaewoo Kang	Opinion spam, intentionally written by spammers who do not have actual experience with services or products, has recently become a factor that undermines the credibility of information online. In recent years, studies have attempted to detect opinion spam using machine learning algorithms. However, limitations of gold-standard spam datasets still prove to be a major obstacle in opinion spam research. In this paper, we introduce a novel dataset called Paraphrased OPinion Spam (POPS), which contains a new type of review spam that imitates real human opinions using crowdsourcing. To create such a seemingly truthful review spam dataset, we asked task participants to paraphrase truthful reviews, and include factual information and domain knowledge in their reviews. The classification experiments and semantic analysis results show that our POPS dataset most linguistically and semantically resembles truthful reviews. We believe that our new deceptive opinion spam dataset will help advance opinion spam research.	Constructing and Evaluating a Novel Crowdsourcing-based Paraphrased Opinion Spam Dataset	NA:NA:NA:NA	2017
Abhijnan Chakraborty:Saptarshi Ghosh:Niloy Ganguly:Krishna P. Gummadi	Online news media sites are emerging as the primary source of news for a large number of users. The selection of 'front-page' stories on these media sites usually takes into consideration several crowdsourced popularity metrics, such as number of views or shares by the readers. In this work, we focus on automatically recommending front-page stories in such media websites. When recommending news stories, there are two basic metrics of interest - recency and relevancy. Ideally, recommender systems should recommend the most relevant stories soon after they are published. However, the relevancy of a story only becomes evident as the story ages, thereby creating a tension between recency and relevancy. A systematic analysis of popular recommendation strategies in use today reveals that they lead to poor trade-offs between recency and relevancy in practice. So, in this paper, we propose a new recommendation strategy (called Highest Future-Impact) which attempts to optimize on both the axes. To implement our proposed strategy in practice, we develop an optimization framework combining the predicted future-impact of the stories with the uncertainties in the predictions. Evaluations over three real-world news datasets show that our implementation achieves good performance trade-offs between recency and relevancy.	Optimizing the Recency-Relevancy Trade-off in Online News Recommendations	NA:NA:NA:NA	2017
Behzad Tabibian:Isabel Valera:Mehrdad Farajtabar:Le Song:Bernhard Schölkopf:Manuel Gomez-Rodriguez	Online knowledge repositories typically rely on their users or dedicated editors to evaluate the reliability of their contents. These explicit feedback mechanisms can be viewed as noisy measurements of both information reliability and information source trustworthiness. Can we leverage these noisy measurements, often biased, to distill a robust, unbiased and interpretable measure of both notions? In this paper, we argue that the large volume of digital traces left by the users within knowledge repositories also reflect information reliability and source trustworthiness. In particular, we propose a temporal point process modeling framework which links the temporal behavior of the users to information reliability and source trustworthiness. Furthermore, we develop an efficient convex optimization procedure to learn the parameters of the model from historical traces of the evaluations provided by these users. Experiments on real-world data gathered from Wikipedia and Stack Overflow show that our modeling framework accurately predicts evaluation events, provides an interpretable measure of information reliability and source trustworthiness, and yields interesting insights about real-world events.	Distilling Information Reliability and Source Trustworthiness from Digital Traces	NA:NA:NA:NA:NA:NA	2017
Srijan Kumar:Justin Cheng:Jure Leskovec:V.S. Subrahmanian	In online discussion communities, users can interact and share information and opinions on a wide variety of topics. However, some users may create multiple identities, or sockpuppets, and engage in undesired behavior by deceiving others or manipulating discussions. In this work, we study sockpuppetry across nine discussion communities, and show that sockpuppets differ from ordinary users in terms of their posting behavior, linguistic traits, as well as social network structure. Sockpuppets tend to start fewer discussions, write shorter posts, use more personal pronouns such as ``I'', and have more clustered ego-networks. Further, pairs of sockpuppets controlled by the same individual are more likely to interact on the same discussion at the same time than pairs of ordinary users. Our analysis suggests a taxonomy of deceptive behavior in discussion communities. Pairs of sockpuppets can vary in their deceptiveness, i.e., whether they pretend to be different users, or their supportiveness, i.e., if they support arguments of other sockpuppets controlled by the same user. We apply these findings to a series of prediction tasks, notably, to identify whether a pair of accounts belongs to the same underlying user or not. Altogether, this work presents a data-driven view of deception in online discussion communities and paves the way towards the automatic detection of sockpuppets.	An Army of Me: Sockpuppets in Online Discussion Communities	NA:NA:NA:NA	2017
Chaoshun Zuo:Zhiqiang Lin	Server URLs including domain names, resource path, and query parameters are important to many security applications such as hidden service identification, malicious website detection, and server vulnerability fuzzing. Unlike traditional desktop web apps in which server URLs are often directly visible, the server URLs of mobile apps are often hidden, only being exposed when the corresponding app code gets executed. Therefore, it is important to automatically analyze the mobile app code to expose the server URLs and enable the security applications with them. We have thus developed SMARTGEN to feature selective symbolic execution for the purpose of automatically generate server request messages to expose the server URLs by extracting and solving user input constraints in mobile apps. Our evaluation with 5,000 top-ranked mobile apps (each with over one million installs) in Google Play shows that with SMARTGEN we are able to reveal 297,780 URLs in total for these apps. We have then submitted all of these exposed URLs to a harmful URL detection service provided by VirusTotal, which further identified 8634 URLs being harmful. Among them, Phising belong to phishing sites, 3,722 malware sites and 3,228 malicious sites (there are 387 overlapped sites between malware and malicious sites).	SMARTGEN: Exposing Server URLs of Mobile Apps With Selective Symbolic Execution	NA:NA	2017
Dolière Francis Some:Nataliia Bielova:Tamara Rezk	Modern browsers implement different security policies such as the Content Security Policy (CSP), a mechanism designed to mitigate popular web vulnerabilities, and the Same Origin Policy (SOP), a mechanism that governs interactions between resources of web pages. In this work, we describe how CSP may be violated due to the SOP when a page contains an embedded iframe from the same origin. We analyse 1 million pages from 10,000 top Alexa sites and report that at least 31.1% of current CSP-enabled pages are potentially vulnerable to CSP violations. Further considering real-world situations where those pages are involved in same-origin nested browsing contexts, we found that in at least 23.5% of the cases, CSP violations are possible. During our study, we also identified a divergence among browsers implementations in the enforcement of CSP in srcdoc sandboxed iframes, which actually reveals a problem in Gecko-based browsers CSP implementation. To ameliorate the problematic conflicts of the security mechanisms, we discuss measures to avoid CSP violations.	On the Content Security Policy Violations due to the Same-Origin Policy	NA:NA:NA	2017
Adam Bates:Wajih Ul Hassan:Kevin Butler:Alin Dobra:Bradley Reaves:Patrick Cable:Thomas Moyer:Nabil Schear	Detecting and explaining the nature of attacks in distributed web services is often difficult -- determining the nature of suspicious activity requires following the trail of an attacker through a chain of heterogeneous software components including load balancers, proxies, worker nodes, and storage services. Unfortunately, existing forensic solutions cannot provide the necessary context to link events across complex workflows, particularly in instances where application layer semantics (e.g., SQL queries, RPCs) are needed to understand the attack. In this work, we present a transparent provenance-based approach for auditing web services through the introduction of Network Provenance Functions (NPFs). NPFs are a distributed architecture for capturing detailed data provenance for web service components, leveraging the key insight that mediation of an application's protocols can be used to infer its activities without requiring invasive instrumentation or developer cooperation. We design and implement NPF with consideration for the complexity of modern cloud-based web services, and evaluate our architecture against a variety of applications including DVDStore, RUBiS, and WikiBench to show that our system imposes as little as 9.3% average end-to-end overhead on connections for realistic workloads. Finally, we consider several scenarios in which our system can be used to concisely explain attacks. NPF thus enables the hassle-free deployment of semantically rich provenance-based auditing for complex applications workflows in the Cloud.	Transparent Web Service Auditing via Network Provenance Functions	NA:NA:NA:NA:NA:NA:NA:NA	2017
Kyungtae Kim:I Luk Kim:Chung Hwan Kim:Yonghwi Kwon:Yunhui Zheng:Xiangyu Zhang:Dongyan Xu	Web-based malware equipped with stealthy cloaking and obfuscation techniques is becoming more sophisticated nowadays. In this paper, we propose J-FORCE, a crash-free forced JavaScript execution engine to systematically explore possible execution paths and reveal malicious behaviors in such malware. In particular, J-FORCE records branch outcomes and mutates them for further explorations. J-FORCE inspects function parameter values that may reveal malicious intentions and expose suspicious DOM injections. We addressed a number of technical challenges encountered. For instance, we keep track of missing objects and DOM elements, and create them on demand. To verify the efficacy of our techniques, we apply J-FORCE to detect Exploit Kit (EK) attacks and malicious Chrome extensions. We observe that J-FORCE is more effective compared to the existing tools.	J-Force: Forced Execution on JavaScript	NA:NA:NA:NA:NA:NA:NA	2017
Zheqian Chen:Ben Gao:Huimin Zhang:Zhou Zhao:Haifeng Liu:Deng Cai	Community question answering(CQA) services have arisen as a popular knowledge sharing pattern for netizens. With abundant interactions among users, individuals are capable of obtaining satisfactory information. However, it is not effective for users to attain satisfying answers within minutes. Users have to check the progress over time until the appropriate answers submitted. We address this problem as a user personalized satisfaction prediction task. Existing methods usually exploit manual feature selection. It is not desirable as it requires careful design and is labor intensive. In this paper, we settle this issue by developing a new multiple instance deep learning framework. Specifically, in our settings, each question follows a multiple instance learning assumption, where its obtained answers can be regarded as instance sets in a bag and we define the question resolved with at least one satisfactory answer. We design an efficient framework exploiting multiple instance learning property with deep learning tactic to model the question-answer pairs relevance and rank the asker's satisfaction possibility. Extensive experiments on large-scale datasets from different forums of Stack Exchange demonstrate the feasibility of our proposed framework in predicting asker personalized satisfaction.	User Personalized Satisfaction Prediction via Multiple Instance Deep Learning	NA:NA:NA:NA:NA:NA	2017
Dimitar Dimitrov:Philipp Singer:Florian Lemmerich:Markus Strohmaier	While a plethora of hypertext links exist on the Web, only a small amount of them are regularly clicked. Starting from this observation, we set out to study large-scale click data from Wikipedia in order to understand what makes a link successful. We systematically analyze effects of link properties on the popularity of links. By utilizing mixed-effects hurdle models supplemented with descriptive insights, we find evidence of user preference towards links leading to the periphery of the network, towards links leading to semantically similar articles, and towards links in the top and left-side of the screen. We integrate these findings as Bayesian priors into a navigational Markov chain model and by doing so successfully improve the model fits. We further adapt and improve the well-known classic PageRank algorithm that assumes random navigation by accounting for observed navigational preferences of users in a weighted variation. This work facilitates understanding navigational click behavior and thus can contribute to improving link structures and algorithms utilizing these structures.	What Makes a Link Successful on Wikipedia?	NA:NA:NA:NA	2017
Jack Hessel:Lillian Lee:David Mimno	The content of today's social media is becoming more and more rich, increasingly mixing text, images, videos, and audio. It is an intriguing research question to model the interplay between these different modes in attracting user attention and engagement. But in order to pursue this study of multimodal content, we must also account for context: timing effects, community preferences, and social factors (e.g., which authors are already popular) also affect the amount of feedback and reaction that social-media posts receive. In this work, we separate out the influence of these non-content factors in several ways. First, we focus on ranking pairs of submissions posted to the same community in quick succession, e.g., within 30 seconds; this framing encourages models to focus on time-agnostic and community-specific content features. Within that setting, we determine the relative performance of author vs. content features. We find that victory usually belongs to "cats and captions," as visual and textual features together tend to outperform identity-based features. Moreover, our experiments show that when considered in isolation, simple unigram text features and deep neural network visual features yield the highest accuracy individually, and that the combination of the two modalities generally leads to the best accuracies overall.	Cats and Captions vs. Creators and the Clock: Comparing Multimodal Content to Context in Predicting Relative Popularity	NA:NA:NA	2017
Lin Gong:Benjamin Haines:Hongning Wang	We propose to capture humans' variable and idiosyncratic sentiment via building personalized sentiment classification models at a group level. Our solution roots in the social comparison theory that humans tend to form groups with others of similar minds and ability, and the cognitive consistency theory that mutual influence inside groups will eventually shape group norms and attitudes, with which group members will all shift to align. We formalize personalized sentiment classification as a multi-task learning problem. In particular, to exploit the clustering property of users' opinions, we impose a non-parametric Dirichlet Process prior over the personalized models, in which group members share the same customized sentiment model adapted from a global classifier. Extensive experimental evaluations on large collections of Amazon and Yelp reviews confirm the effectiveness of the proposed solution: it outperformed user-independent classification solutions, and several state-of-the-art model adaptation and multi-task learning algorithms.	Clustered Model Adaption for Personalized Sentiment Analysis	NA:NA:NA	2017
Takanori Maehara:Hirofumi Suzuki:Masakazu Ishihata	Evaluating influence spread in social networks is a fundamental procedure to estimate the word-of-mouth effect in viral marketing. There are enormous studies about this topic; however, under the standard stochastic cascade models, the exact computation of influence spread is known to be #P-hard. Thus, the existing studies have used Monte-Carlo simulation-based approximations to avoid exact computation. We propose the first algorithm to compute influence spread exactly under the independent cascade model. The algorithm first constructs binary decision diagrams (BDDs) for all possible realizations of influence spread, then computes influence spread by dynamic programming on the constructed BDDs. To construct the BDDs efficiently, we designed a new frontier-based search-type procedure. The constructed BDDs can also be used to solve other influence-spread related problems, such as random sampling without rejection, conditional influence spread evaluation, dynamic probability update, and gradient computation for probability optimization problems. We conducted computational experiments to evaluate the proposed algorithm. The algorithm successfully computed influence spread on real-world networks with a hundred edges in a reasonable time, which is quite impossible by the naive algorithm. We also conducted an experiment to evaluate the accuracy of the Monte-Carlo simulation-based approximation by comparing exact influence spread obtained by the proposed algorithm.	Exact Computation of Influence Spread by Binary Decision Diagrams	NA:NA:NA	2017
Gilad Asharov:Francesco Bonchi:David Garcia-Soriano:Tamir Tassa	Consider a multi-layered graph, where the different layers correspond to different proprietary social networks on the same ground set of users. Suppose that the owners of the different networks (called hosts) are mutually non-trusting parties: how can they compute a centrality score for each of the users using all the layers, but without disclosing information about their private graphs? Under this setting we study a suite of three centrality measures whose algebraic structure allows performing that computation with provable security and efficiency. The first measure counts the nodes reachable from a node within a given radius. The second measure extends the first one by counting the number of paths between any two nodes. The final one is a generalization to the multi-layered graph case: not only the number of paths is counted, but also the multiplicity of these paths in the different layers is considered. We devise a suite of multiparty protocols to compute those centrality measures, which are all provably secure in the information-theoretic sense. One typical challenge and limitation of secure multiparty computation protocols is their scalability. We tackle this problem and devise a protocol which is highly scalable and still provably secure. We test our protocols on several real-world multi-layered graphs: interestingly, the protocol to compute the most sensitive measure (i.e., the multi-layered centrality) is also the most scalable one and can be efficiently run on very large networks.	Secure Centrality Computation Over Multiple Networks	NA:NA:NA:NA	2017
Wei Chen:Shang-Hua Teng	We study network centrality based on dynamic influence propagation models in social networks. To illustrate our integrated mathematical-algorithmic approach for understanding the fundamental interplay between dynamic influence processes and static network structures, we focus on two basic centrality measures: (a) Single Node Influence (SNI) centrality, which measures each node's significance by its influence spread; and (b) Shapley Centrality, which uses the Shapley value of the influence spread function --- formulated based on a fundamental cooperative-game-theoretical concept --- to measure the significance of nodes. We present a comprehensive comparative study of these two centrality measures. Mathematically, we present axiomatic characterizations, which precisely capture the essence of these two centrality measures and their fundamental differences. Algorithmically, we provide scalable algorithms for approximating them for a large family of social-influence instances. Empirically, we demonstrate their similarity and differences in a number of real-world social networks, as well as the efficiency of our scalable algorithms. Our results shed light on their applicability: SNI centrality is suitable for assessing individual influence in isolation while Shapley centrality assesses individuals' performance in group influence settings.	Interplay between Social Influence and Network Centrality: A Comparative Study on Shapley Centrality and Single-Node-Influence Centrality	NA:NA	2017
Naoto Ohsaka:Yuichi Yoshida	Motivated by viral marketing, stochastic diffusion processes that model influence spread on a network have been studied intensively. The primary interest in such models has been to find a seed set of a fixed size that maximizes the expected size of the cascade from it. Practically, however, it is not desirable to have the risk of ending with a small cascade, even if the expected size of the cascade is large. To address this issue, we adopt conditional value at risk (CVaR) as a risk measure, and propose an algorithm that computes a portfolio over seed sets with a provable guarantee on its CVaR. Using real-world social networks, we demonstrate that the portfolio computed by our algorithm has a significantly better CVaR than seed sets computed by other baseline methods.	Portfolio Optimization for Influence Spread	NA:NA	2017
Ido Guy:Avihai Mejer:Alexander Nus:Fiana Raiber	User-generated reviews are a key driving force behind some of the leading websites, such as Amazon, TripAdvisor, and Yelp. Yet, the proliferation of user reviews in such sites also poses an information overload challenge: many items, especially popular ones, have a large number of reviews, which cannot all be read by the user. In this work, we propose to extract short practical tips from user reviews. We focus on tips for travel attractions extracted from user reviews on TripAdvisor. Our method infers a list of templates from a small gold set of tips and applies them to user reviews to extract tip candidates. For each attraction, the associated candidates are then ranked according to their predicted usefulness. Evaluation based on labeling by professional annotators shows that our method produces high-quality tips, with good coverage of cities and attractions.	Extracting and Ranking Travel Tips from User-Generated Reviews	NA:NA:NA:NA	2017
Mayank Kejriwal:Pedro Szekely	Extracting useful entities and attribute values from illicit domains such as human trafficking is a challenging problem with the potential for widespread social impact. Such domains employ atypical language models, have 'long tails' and suffer from the problem of concept drift. In this paper, we propose a lightweight, feature-agnostic Information Extraction (IE) paradigm specifically designed for such domains. Our approach uses raw, unlabeled text from an initial corpus, and a few (12-120) seed annotations per domain-specific attribute, to learn robust IE models for unobserved pages and websites. Empirically, we demonstrate that our approach can outperform feature-centric Conditional Random Field baselines by over 18% F-Measure on five annotated sets of real-world human trafficking datasets in both low-supervision and high-supervision settings. We also show that our approach is demonstrably robust to concept drift, and can be efficiently bootstrapped even in a serial computing environment.	Information Extraction in Illicit Web Domains	NA:NA	2017
Alexander Konovalov:Benjamin Strauss:Alan Ritter:Brendan O'Connor	Broad-coverage knowledge bases (KBs) such as Wikipedia, Freebase, Microsoft's Satori and Google's Knowledge Graph contain structured data describing real-world entities. These data sources have become increasingly important for a wide range of intelligent systems: from information retrieval and question answering, to Facebook's Graph Search, IBM's Watson, and more. Previous work on learning to populate knowledge bases from text has, for the most part, made the simplifying assumption that facts remain constant over time. But this is inaccurate -- we live in a rapidly changing world. Knowledge should not be viewed as a static snapshot, but instead a rapidly evolving set of facts that must change as the world changes. In this paper we demonstrate the feasibility of accurately identifying entity-transition-events, from real-time news and social media text streams, that drive changes to a knowledge base. We use Wikipedia's edit history as distant supervision to learn event extractors, and evaluate the extractors based on their ability to predict online updates. Our weakly supervised event extractors are able to predict 10 KB revisions per month at 0.8 precision. By lowering our confidence threshold, we can suggest 34.3 correct edits per month at 0.4 precision. 64% of predicted edits were detected before they were added to Wikipedia. The average lead time of our forecasted knowledge revisions over Wikipedia's editors is 40 days, demonstrating the utility of our method for suggesting edits that can be quickly verified and added to the knowledge graph.	Learning to Extract Events from Knowledge Base Revisions	NA:NA:NA:NA	2017
Xiang Ren:Zeqiu Wu:Wenqi He:Meng Qu:Clare R. Voss:Heng Ji:Tarek F. Abdelzaher:Jiawei Han	Extracting entities and relations for types of interest from text is important for understanding massive text corpora. Traditionally, systems of entity relation extraction have relied on human-annotated corpora for training and adopted an incremental pipeline. Such systems require additional human expertise to be ported to a new domain, and are vulnerable to errors cascading down the pipeline. In this paper, we investigate joint extraction of typed entities and relations with labeled data heuristically obtained from knowledge bases (i.e., distant supervision). As our algorithm for type labeling via distant supervision is context-agnostic, noisy training data poses unique challenges for the task. We propose a novel domain-independent framework, called CoType, that runs a data-driven text segmentation algorithm to extract entity mentions, and jointly embeds entity mentions, relation mentions, text features and type labels into two low-dimensional spaces (for entity and relation mentions respectively), where, in each space, objects whose types are close will also have similar representations. CoType, then using these learned embeddings, estimates the types of test (unlinkable) mentions. We formulate a joint optimization problem to learn embeddings from text corpora and knowledge bases, adopting a novel partial-label loss function for noisy labeled data and introducing an object "translation" function to capture the cross-constraints of entities and relations on each other. Experiments on three public datasets demonstrate the effectiveness of CoType across different domains (e.g., news, biomedical), with an average of 25% improvement in F1 score compared to the next best method.	CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases	NA:NA:NA:NA:NA:NA:NA:NA	2017
Nate Veldt:Anthony I. Wirth:David F. Gleich	Correlation clustering is a technique for aggregating data based on qualitative information about which pairs of objects are labeled `similar' or `dissimilar.' Because the optimization problem is NP-hard, much of the previous literature focuses on finding approximation algorithms. In this paper we explore how to solve the correlation clustering objective exactly when the data to be clustered can be represented by a low-rank matrix. We prove in particular that correlation clustering can be solved in polynomial time when the underlying matrix is positive semidefinite with small constant rank, but that the task remains NP-hard in the presence of even one negative eigenvalue. Based on our theoretical results, we develop an algorithm for efficiently ``solving'' low-rank positive semidefinite correlation clustering by employing a procedure for zonotope vertex enumeration. We demonstrate the effectiveness and speed of our algorithm by using it to solve several clustering problems on both synthetic and real-world data.	Correlation Clustering with Low-Rank Matrices	NA:NA:NA	2017
Wei Wu:Bin Li:Ling Chen:Chengqi Zhang	Min-Hash, which is widely used for efficiently estimating similarities of bag-of-words represented data, plays an increasingly important role in the era of big data. It has been extended to deal with real-value weighted sets -- Improved Consistent Weighted Sampling (ICWS) is considered as the state-of-the-art for this problem. In this paper, we propose a Practical CWS (PCWS) algorithm. We first transform the original form of ICWS into an equivalent expression, based on which we find some interesting properties that inspire us to make the ICWS algorithm simpler and more efficient in both space and time complexities. PCWS is not only mathematically equivalent to ICWS and preserves the same theoretical properties, but also saves 20% memory footprint and substantial computational cost compared to ICWS. The experimental results on a number of real-world text data sets demonstrate that PCWS obtains the same (even better) classification and retrieval performance as ICWS with 1/5~1/3 reduced empirical runtime.	Consistent Weighted Sampling Made More Practical	NA:NA:NA:NA	2017
Jan Deriu:Aurelien Lucchi:Valeria De Luca:Aliaksei Severyn:Simon Müller:Mark Cieliebak:Thomas Hofmann:Martin Jaggi	This paper presents a novel approach for multi-lingual sentiment classification in short texts. This is a challenging task as the amount of training data in languages other than English is very limited. Previously proposed multi-lingual approaches typically require to establish a correspondence to English for which powerful classifiers are already available. In contrast, our method does not require such supervision. We leverage large amounts of weakly-supervised data in various languages to train a multi-layer convolutional network and demonstrate the importance of using pre-training of such networks. We thoroughly evaluate our approach on various multi-lingual datasets, including the recent SemEval-2016 sentiment prediction benchmark (Task 4), where we achieved state-of-the-art performance. We also compare the performance of our model trained individually for each language to a variant trained for all languages at once. We show that the latter model reaches slightly worse - but still acceptable - performance when compared to the single language model, while benefiting from better generalization properties across languages.	Leveraging Large Amounts of Weakly Supervised Data for Multi-Language Sentiment Classification	NA:NA:NA:NA:NA:NA:NA:NA	2017
Ping Li:Cun-Hui Zhang	In web search, data mining, and machine learning, two popular measures of data similarity are the cosine and the resemblance (the latter is for binary data). In this study, we develop theoretical results for both the cosine and the GMM (generalized min-max) kernel, which is a generalization of the resemblance. GMM has direct applications in machine learning as a positive definite kernel and can be efficiently linearized via probabilistic hashing to handle big data. Owing to its discrete nature, the hashed values can also be used to build hash tables for efficient near neighbor search. We prove the theoretical limit of GMM and the consistency result, assuming that the data follow an elliptical distribution, which is a general family of distributions and includes the multivariate normal and t-distribution as special cases. The consistency result holds as long as the data have bounded first moment (an assumption which typically holds for data commonly encountered in practice). Furthermore, we establish the asymptotic normality of GMM. We also prove the limit of cosine under elliptical distributions. In comparison, the consistency of GMM requires much weaker conditions. For example, when data follow a t-distribution with ν degrees of freedom, GMM typically provides a better estimate of similarity than cosine when ν < 8 (ν = 8 means the distribution is very close to normal). These theoretical results help explain the recent success of GMM and lay the foundation for further research.	Theory of the GMM Kernel	NA:NA	2017
Huayi Li:Geli Fei:Shuai Wang:Bing Liu:Weixiang Shao:Arjun Mukherjee:Jidong Shao	Online reviews play a crucial role in helping consumers evaluate and compare products and services. This critical importance of reviews also incentivizes fraudsters (or spammers) to write fake or spam reviews to secretly promote or demote some target products and services. Existing approaches to detecting spam reviews and reviewers employed review contents, reviewer behaviors, star rating patterns, and reviewer-product networks for detection. In this research, we further discovered that reviewers' posting rates (number of reviews written in a period of time) also follow an interesting distribution pattern, which has not been reported before. That is, their posting rates are bimodal. Multiple spammers also tend to collectively and actively post reviews to the same set of products within a short time frame, which we call co-bursting. Furthermore, we found some other interesting patterns in individual reviewers' temporal dynamics and their co-bursting behaviors with other reviewers. Inspired by these findings, we first propose a two-mode Labeled Hidden Markov Model to model spamming using only individual reviewers' review posting times. We then extend it to the Coupled Hidden Markov Model to capture both reviewer posting behaviors and co-bursting signals. Our experiments show that the proposed model significantly outperforms state-of-the-art baselines in identifying individual spammers. Furthermore, we propose a co-bursting network based on co-bursting relations, which helps detect groups of spammers more effectively than existing approaches.	Bimodal Distribution and Co-Bursting in Review Spam Detection	NA:NA:NA:NA:NA:NA:NA	2017
Yuli Liu:Yiqun Liu:Ke Zhou:Min Zhang:Shaoping Ma	Community Question Answering (CQA) portals provide rich sources of information on a variety of topics. However, the authenticity and quality of questions and answers (Q&As) has proven hard to control. In a troubling direction, the widespread growth of crowdsourcing websites has created a large-scale, potentially difficult-to-detect workforce to manipulate malicious contents in CQA. The crowd workers who join the same crowdsourcing task about promotion campaigns in CQA collusively manipulate deceptive Q&As for promoting a target (product or service). The collusive spamming group can fully control the sentiment of the target. How to utilize the structure and the attributes for detecting manipulated Q&As? How to detect the collusive group and leverage the group information for the detection task? To shed light on these research questions, we propose a unified framework to tackle the challenge of detecting collusive spamming activities of CQA. First, we interpret the questions and answers in CQA as two independent networks. Second, we detect collusive question groups and answer groups from these two networks respectively by measuring the similarity of the contents posted within a short duration. Third, using attributes (individual-level and group-level) and correlations (user-based and content-based), we proposed a combined factor graph model to detect deceptive Q&As simultaneously by combining two independent factor graphs. With a large-scale practical data set, we find that the proposed framework can detect deceptive contents at early stage, and outperforms a number of competitive baselines.	Detecting Collusive Spamming Activities in Community Question Answering	NA:NA:NA:NA:NA	2017
Neil Shah	Livestreaming platforms have become increasingly popular in recent years as a means of sharing and advertising creative content. Popular content streamers who attract large viewership to their live broadcasts can earn a living by means of ad revenue, donations and channel subscriptions. Unfortunately, this incentivized popularity has simultaneously resulted in incentive for fraudsters to provide services to astroturf, or artificially inflate viewership metrics by providing fake ``live'' views to customers. Our work provides a number of major contributions: (a) formulation: we are the first to introduce and characterize the viewbot fraud problem in livestreaming platforms, (b) methodology: we propose FLOCK, a principled and unsupervised method which efficiently and effectively identifies botted broadcasts and their constituent botted views, and (c) practicality: our approach achieves over 98% precision in identifying botted broadcasts and over 90% precision/recall against sizable synthetically generated viewbot attacks on a real-world livestreaming workload of over 16 million views and 92 thousand broadcasts. FLOCK successfully operates on larger datasets in practice and is regularly used at a large, undisclosed livestreaming corporation.	FLOCK: Combating Astroturfing on Livestreaming Platforms	NA	2017
David Mandell Freeman	Online social networks (OSNs) are appealing platforms for spammers and fraudsters, who typically use fake or compromised accounts to connect with and defraud real users. To combat such abuse, OSNs allow users to report fraudulent profiles or activity. The OSN can then use reporting data to review and/or limit activity of reported accounts. Previous authors have suggested that an OSN can augment its takedown algorithms by identifying a "trusted set" of users whose reports are weighted more heavily in the disposition of flagged accounts. Such identification would allow the OSN to improve both speed and accuracy of fake account detection and thus reduce the impact of spam on users. In this work we provide the first public, data-driven assessment of whether the above assumption is true: are some users better at reporting than others? Specifically, is reporting skill both measurable, i.e., possible to distinguish from random guessing; and repeatable, i.e., persistent over repeated sampling? Our main contributions are to develop a statistical framework that describes these properties and to apply this framework to data from LinkedIn, the professional social network. Our data includes member reports of fake profiles as well as the more voluminous, albeit weaker, signal of member responses to connection requests. We find that members demonstrating measurable, repeatable skill in identifying fake profiles do exist but are rare: at most 2.4% of those reporting fakes and at most 1.3% of those rejecting connection requests. We conclude that any reliable "trusted set" of members will be too small to have noticeable impact on spam metrics.	Can You Spot the Fakes?: On the Limitations of User Feedback in Online Social Networks	NA	2017
Mengting Wan:Di Wang:Matt Goldman:Matt Taddy:Justin Rao:Jie Liu:Dimitrios Lymberopoulos:Julian McAuley	In order to match shoppers with desired products and provide personalized promotions, whether in online or offline shopping worlds, it is critical to model both consumer preferences and price sensitivities simultaneously. Personalized preferences have been thoroughly studied in the field of recommender systems, though price (and price sensitivity) has received relatively little attention. At the same time, price sensitivity has been richly explored in the area of economics, though typically not in the context of developing scalable, working systems to generate recommendations. In this study, we seek to bridge the gap between large-scale recommender systems and established consumer theories from economics, and propose a nested feature-based matrix factorization framework to model both preferences and price sensitivities. Quantitative and qualitative results indicate the proposed personalized, interpretable and scalable framework is capable of providing satisfying recommendations (on two datasets of grocery transactions) and can be applied to obtain economic insights into consumer behavior.	Modeling Consumer Preferences and Price Sensitivities from Large-Scale Grocery Shopping Transaction Logs	NA:NA:NA:NA:NA:NA:NA:NA	2017
Chanyoung Park:Donghyun Kim:Jinoh Oh:Hwanjo Yu	For online product recommendation engines, learning high-quality product embedding that captures various aspects of the product is critical to improving the accuracy of user rating prediction. In recent research, in conjunction with user feedback, the appearance of a product as side information has been shown to be helpful for learning product embedding. However, since a product has a variety of aspects such as functionality and specifications, taking into account only its appearance as side information does not suffice to accurately learn its embedding. In this paper, we propose a matrix co-factorization method that leverages information hidden in the so-called "also-viewed" products, i.e., a list of products that has also been viewed by users who have viewed a target product. "Also-viewed" products reflect various aspects of a given product that have been overlooked by visually-aware recommendation methods proposed in past research. Experiments on multiple real-world datasets demonstrate that our proposed method outperforms state-of-the-art baselines in terms of user rating prediction. We also perform classification on the product embedding learned by our method, and compare it with a state-of-the-art baseline to demonstrate the superiority of our method in generating high-quality product embedding that better represents the product.	Do "Also-Viewed" Products Help User Rating Prediction?	NA:NA:NA:NA	2017
Ying-Chun Lin:Chi-Hsuan Huang:Chu-Cheng Hsieh:Yu-Chen Shu:Kun-Ta Chuang	The effectiveness of monetary promotions has been well reported in the literature to affect shopping decisions for products in real life experience. Nowadays, e-commerce retailers are facing more fierce competition on price promotion in that consumers can easily use a search engine to find another merchant selling an identical product for comparing price. To achieve more effectiveness on real-time promotion in pursuit of better profits, we propose two discount-giving strategies: an algorithm based on Kernel density estimation, and the other algorithm based on Thompson sampling strategy. We show that, given a pre-determined discount budget, our algorithms can significantly acquire better revenue in return than classical strategies with simply fixed discount on label price. We then demonstrate its feasibility to be a promising deployment in e-commerce services for real-time promotion.	Monetary Discount Strategies for Real-Time Promotion Campaign	NA:NA:NA:NA:NA	2017
Chao-Yuan Wu:Amr Ahmed:Gowtham Ramani Kumar:Ritendra Datta	In online shopping, users usually express their intent through search queries. However, these queries are often ambiguous. For example, it is more likely (and easier) for users to write a query like "high-end bike" than "21 speed carbon frames jamis or giant road bike". It is challenging to interpret these ambiguous queries and thus search result accuracy suffers. A user oftentimes needs to go through the frustrating process of refining search queries or self-teaching from possibly unstructured information. However, shopping is indeed a structured domain, that is composed of category hierarchy, brands, product lines, features, etc. It would be much better if a shopping site could understand users' intent through this structure, present organized information, and then find the items with the right categories, brands or features. In this paper we study the problem of inferring the latent intent from unstructured queries and mapping them to structured attributes. We present a novel framework that jointly learns this knowledge from user consumption behaviors and product metadata. We present a hybrid Long Short-term Memory (LSTM) joint model that is accurate and robust, even though user queries are noisy and product catalog is rapidly growing. Our study is conducted on a large-scale dataset from Google Shopping, that is composed of millions of items and user queries along with their click responses. Extensive qualitative and quantitative evaluation shows that the proposed model is more accurate, concise, and robust than multiple possible alternatives. In terms of information retrieval (IR) performance, our model is able to improve the quality of current Google Shopping production system, which is a very strong baseline.	Predicting Latent Structured Intents from Shopping Queries	NA:NA:NA:NA	2017
Zhengxing Chen:Su Xue:John Kolen:Navid Aghdaie:Kazi A. Zaman:Yizhou Sun:Magy Seif El-Nasr	Matchmaking connects multiple players to participate in online player-versus-player games. Current matchmaking systems depend on a single core strategy: create fair games at all times. These systems pair similarly skilled players on the assumption that a fair game is best player experience. We will demonstrate, however, that this intuitive assumption sometimes fails and that matchmaking based on fairness is not optimal for engagement. In this paper, we propose an Engagement Optimized Matchmaking (EOMM) framework that maximizes overall player engagement. We prove that equal-skill based matchmaking is a special case of EOMM on a highly simplified assumption that rarely holds in reality. Our simulation on real data from a popular game made by Electronic Arts, Inc. (EA) supports our theoretical results, showing significant improvement in enhancing player engagement compared to existing matchmaking methods.	EOMM: An Engagement Optimized Matchmaking Framework	NA:NA:NA:NA:NA:NA:NA	2017
Brunella Spinelli:L. Elisa Celis:Patrick Thiran	Source localization, the act of finding the originator of a disease or rumor in a network, has become an important problem in sociology and epidemiology. The localization is done using the infection state and time of infection of a few designated sensor nodes; however, maintaining sensors can be very costly in practice. We propose the first online approach to source localization: We deploy a priori only a small number of sensors (which reveal if they are reached by an infection) and then iteratively choose the best location to place a new sensor in order to localize the source. This approach allows for source localization with a very small number of sensors; moreover, the source can be found while the epidemic is still ongoing. Our method applies to a general network topology and performs well even with random transmission delays.	Back To The Source: An Online Approach for Sensor Placement and Source Localization	NA:NA:NA	2017
Enrico Mariconti:Jeremiah Onaolapo:Syed Sharique Ahmad:Nicolas Nikiforou:Manuel Egele:Nick Nikiforakis:Gianluca Stringhini	Users on Twitter are commonly identified by their profile names. These names are used when directly addressing users on Twitter, are part of their profile page URLs, and can become a trademark for popular accounts, with people referring to celebrities by their real name and their profile name, interchangeably. Twitter, however, has chosen to not permanently link profile names to their corresponding user accounts. In fact, Twitter allows users to change their profile name, and afterwards makes the old profile names available for other users to take. In this paper, we provide a large-scale study of the phenomenon of profile name reuse on Twitter. We show that this phenomenon is not uncommon, investigate the dynamics of profile name reuse, and characterize the accounts that are involved in it. We find that many of these accounts adopt abandoned profile names for questionable purposes, such as spreading malicious content, and using the profile name's popularity for search engine optimization. Finally, we show that this problem is not unique to Twitter (as other popular online social networks also release profile names) and argue that the risks involved with profile-name reuse outnumber the advantages provided by this feature.	What's in a Name?: Understanding Profile Name Reuse on Twitter	NA:NA:NA:NA:NA:NA:NA	2017
Muhammad Bilal Zafar:Isabel Valera:Manuel Gomez Rodriguez:Krishna P. Gummadi	Automated data-driven decision making systems are increasingly being used to assist, or even replace humans in many settings. These systems function by learning from historical decisions, often taken by humans. In order to maximize the utility of these systems (or, classifiers), their training involves minimizing the errors (or, misclassifications) over the given historical data. However, it is quite possible that the optimally trained classifier makes decisions for people belonging to different social groups with different misclassification rates (e.g., misclassification rates for females are higher than for males), thereby placing these groups at an unfair disadvantage. To account for and avoid such unfairness, in this paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in terms of misclassification rates. We then propose intuitive measures of disparate mistreatment for decision boundary-based classifiers, which can be easily incorporated into their formulation as convex-concave constraints. Experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment, often at a small cost in terms of accuracy.	Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment	NA:NA:NA:NA	2017
Claudia Wagner:Philipp Singer:Fariba Karimi:Jürgen Pfeffer:Markus Strohmaier	Sampling from large networks represents a fundamental challenge for social network research. In this paper, we explore the sensitivity of different sampling techniques (node sampling, edge sampling, random walk sampling, and snowball sampling) on social networks with attributes. We consider the special case of networks (i) where we have one attribute with two values (e.g., male and female in the case of gender), (ii) where the size of the two groups is unequal (e.g., a male majority and a female minority), and (iii) where nodes with the same or different attribute value attract or repel each other (i.e., homophilic or heterophilic behavior). We evaluate the different sampling techniques with respect to conserving the position of nodes and the visibility of groups in such networks. Experiments are conducted both on synthetic and empirical social networks. Our results provide evidence that different network sampling techniques are highly sensitive with regard to capturing the expected centrality of nodes, and that their accuracy depends on relative group size differences and on the level of homophily that can be observed in the network. We conclude that uninformed sampling from social networks with attributes thus can significantly impair the ability of researchers to draw valid conclusions about the centrality of nodes and the visibility or invisibility of groups in social networks.	Sampling from Social Networks with Attributes	NA:NA:NA:NA:NA	2017
Abdalghani Abujabal:Mohamed Yahya:Mirek Riedewald:Gerhard Weikum	Templates are an important asset for question answering over knowledge graphs, simplifying the semantic parsing of input utterances and generating structured queries for interpretable answers. State-of-the-art methods rely on hand-crafted templates with limited coverage. This paper presents QUINT, a system that automatically learns utterance-query templates solely from user questions paired with their answers. Additionally, QUINT is able to harness language compositionality for answering complex questions without having any templates for the entire question. Experiments with different benchmarks demonstrate the high quality of QUINT.	Automated Template Generation for Question Answering over Knowledge Graphs	NA:NA:NA:NA	2017
Long Chen:Joemon M. Jose:Haitao Yu:Fajie Yuan	In the age of Web 2.0, a substantial amount of unstructured content are distributed through multiple text streams in an asynchronous fashion, which makes it increasingly difficult to glean and distill useful information. An effective way to explore the information in text streams is topic modelling, which can further facilitate other applications such as search, information browsing, and pattern mining. In this paper, we propose a semantic graph based topic modelling approach for structuring asynchronous text streams. Our model integrates topic mining and time synchronization, two core modules for addressing the problem, into a unified model. Specifically, for handling the lexical gap issues, we use global semantic graphs of each timestamp for capturing the hidden interaction among entities from all the text streams. For dealing with the sources asynchronism problem, local semantic graphs are employed to discover similar topics of different entities that can be potentially separated by time gaps. Our experiment on two real-world datasets shows that the proposed model significantly outperforms the existing ones.	A Semantic Graph-Based Approach for Mining Common Topics from Multiple Asynchronous Text Streams	NA:NA:NA:NA	2017
Denis Lukovnikov:Asja Fischer:Jens Lehmann:Sören Auer	Question Answering (QA) systems over Knowledge Graphs (KG) automatically answer natural language questions using facts contained in a knowledge graph. Simple questions, which can be answered by the extraction of a single fact, constitute a large part of questions asked on the web but still pose challenges to QA systems, especially when asked against a large knowledge resource. Existing QA systems usually rely on various components each specialised in solving different sub-tasks of the problem (such as segmentation, entity recognition, disambiguation, and relation classification etc.). In this work, we follow a quite different approach: We train a neural network for answering simple questions in an end-to-end manner, leaving all decisions to the model. It learns to rank subject-predicate pairs to enable the retrieval of relevant facts given a question. The network contains a nested word/character-level question encoder which allows to handle out-of-vocabulary and rare word problems while still being able to exploit word-level semantics. Our approach achieves results competitive with state-of-the-art end-to-end approaches that rely on an attention mechanism.	Neural Network-based Question Answering over Knowledge Graphs on Word and Character Level	NA:NA:NA:NA	2017
Wei Emma Zhang:Quan Z. Sheng:Jey Han Lau:Ermyas Abebe	Programming community-based question-answering (PCQA) websites such as Stack Overflow enable programmers to find working solutions to their questions. Despite detailed posting guidelines, duplicate questions that have been answered are frequently created. To tackle this problem, Stack Overflow provides a mechanism for reputable users to manually mark duplicate questions. This is a laborious effort, and leads to many duplicate questions remain undetected. Existing duplicate detection methodologies from traditional community based question-answering (CQA) websites are difficult to be adopted directly to PCQA, as PCQA posts often contain source code which is linguistically very different from natural languages. In this paper, we propose a methodology designed for the PCQA domain to detect duplicate questions. We model the detection as a classification problem over question pairs. To extract features for question pairs, our methodology leverages continuous word vectors from the deep learning literature, topic model features and phrases pairs that co-occur frequently in duplicate questions mined using machine translation systems. These features capture semantic similarities between questions and produce a strong performance for duplicate detection. Experiments on a range of real-world datasets demonstrate that our method works very well; in some cases over 30% improvement compared to state-of-the-art benchmarks. As a product of one of the proposed features, the association score feature, we have mined a set of associated phrases from duplicate questions on Stack Overflow and open the dataset to the public.	Detecting Duplicate Posts in Programming QA Communities via Latent Semantics and Association Rules	NA:NA:NA:NA	2017
Camille Cobb:Tadayoshi Kohno	Online dating services let users expand their dating pool beyond their social network and specify important characteristics of potential partners. To assess compatibility, users share personal information -- e.g., identifying details or sensitive opinions about sexual preferences or worldviews -- in profiles or in one-on-one communication. Thus, participating in online dating poses inherent privacy risks. How people reason about these privacy risks in modern online dating ecosystems has not been extensively studied. We present the results of a survey we designed to examine privacy-related risks, practices, and expectations of people who use or have used online dating, then delve deeper using semi-structured interviews. We additionally analyzed 400 Tinder profiles to explore how these issues manifest in practice. Our results reveal tensions between privacy and competing user values and goals, and we demonstrate how these results can inform future designs.	How Public Is My Private Life?: Privacy in Online Dating	NA:NA	2017
Fengli Xu:Zhen Tu:Yong Li:Pengyu Zhang:Xiaoming Fu:Depeng Jin	Human mobility data has been ubiquitously collected through cellular networks and mobile applications, and publicly released for academic research and commercial purposes for the last decade. Since releasing individual's mobility records usually gives rise to privacy issues, datasets owners tend to only publish aggregated mobility data, such as the number of users covered by a cellular tower at a specific timestamp, which is believed to be sufficient for preserving users' privacy. However, in this paper, we argue and prove that even publishing aggregated mobility data could lead to privacy breach in individuals' trajectories. We develop an attack system that is able to exploit the uniqueness and regularity of human mobility to recover individual's trajectories from the aggregated mobility data without any prior knowledge. By conducting experiments on two real-world datasets collected from both mobile application and cellular network, we reveal that the attack system is able to recover users' trajectories with accuracy about 73%~91% at the scale of tens of thousands to hundreds of thousands users, which indicates severe privacy leakage in such datasets. Through the investigation on aggregated mobility data, our work recognizes a novel privacy problem in publishing statistic data, which appeals for immediate attentions from both academy and industry.	Trajectory Recovery From Ash: User Privacy Is NOT Preserved in Aggregated Mobility Data	NA:NA:NA:NA:NA:NA	2017
Iskander Sanchez-Rola:Davide Balzarotti:Igor Santos	Tor is a well known and widely used darknet, known for its anonymity. However, while its protocol and relay security have already been extensively studied, to date there is no comprehensive analysis of the structure and privacy of its Web Hidden Service. To fill this gap, we developed a dedicated analysis platform and used it to crawl and analyze over 1.5M URLs hosted in 7257 onion domains. For each page we analyzed its links, resources, and redirections graphs, as well as the language and category distribution. According to our experiments, Tor hidden services are organized in a sparse but highly connected graph, in which around 10% of the onions sites are completely isolated. Our study also measures for the first time the tight connection that exists between Tor hidden services and the Surface Web. In fact, more than 20% of the onion domains we visited imported resources from the Surface Web, and links to the Surface Web are even more prevalent than to other onion domains. Finally, we measured for the first time the prevalence and the nature of web tracking in Tor hidden services, showing that, albeit not as widespread as in the Surface Web, tracking is notably present also in the Dark Web: more than 40% of the scripts are used for this purpose, with the 70% of them being completely new tracking scripts unknown by existing anti-tracking solutions.	The Onions Have Eyes: A Comprehensive Structure and Privacy Analysis of Tor Hidden Services	NA:NA:NA	2017
Jessica Su:Ansh Shukla:Sharad Goel:Arvind Narayanan	Can online trackers and network adversaries de-anonymize web browsing data readily available to them? We show---theoretically, via simulation, and through experiments on real user data---that de-identified web browsing histories can be linked to social media profiles using only publicly available data. Our approach is based on a simple observation: each person has a distinctive social network, and thus the set of links appearing in one's feed is unique. Assuming users visit links in their feed with higher probability than a random user, browsing histories contain tell-tale marks of identity. We formalize this intuition by specifying a model of web browsing behavior and then deriving the maximum likelihood estimate of a user's social profile. We evaluate this strategy on simulated browsing histories, and show that given a history with 30 links originating from Twitter, we can deduce the corresponding Twitter profile more than 50% of the time.To gauge the real-world effectiveness of this approach, we recruited nearly 400 people to donate their web browsing histories, and we were able to correctly identify more than 70% of them. We further show that several online trackers are embedded on sufficiently many websites to carry out this attack with high accuracy. Our theoretical contribution applies to any type of transactional data and is robust to noisy observations, generalizing a wide range of previous de-anonymization attacks. Finally, since our attack attempts to find the correct Twitter profile out of over 300 million candidates, it is---to our knowledge---the largest-scale demonstrated de-anonymization to date.	De-anonymizing Web Browsing Data with Social Networks	NA:NA:NA:NA	2017
Chenyan Xiong:Russell Power:Jamie Callan	This paper introduces Explicit Semantic Ranking (ESR), a new ranking technique that leverages knowledge graph embedding. Analysis of the query log from our academic search engine, SemanticScholar.org, reveals that a major error source is its inability to understand the meaning of research concepts in queries. To addresses this challenge, ESR represents queries and documents in the entity space and ranks them based on their semantic connections from their knowledge graph embedding. Experiments demonstrate ESR's ability in improving Semantic Scholar's online production system, especially on hard queries where word-based ranking fails.	Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding	NA:NA:NA	2017
Sourav Dutta:Pratik Nayek:Arnab Bhattacharya	Labeled graphs provide a natural way of representing entities, relationships and structures within real datasets such as knowledge graphs and protein interactions. Applications such as question answering, semantic search, and motif discovery entail efficient approaches for subgraph matching involving both label and structural similarities. Given the NP-completeness of subgraph isomorphism and the presence of noise, approximate graph matching techniques are required to handle queries in a robust and real-time manner. This paper presents a novel technique to characterize the subgraph similarity based on statistical significance captured by chi-square statistic. The statistical significance model takes into account the background structure and label distribution in the neighborhood of vertices to obtain the best matching subgraph and, therefore, robustly handles partial label and structural mismatches. Based on the model, we propose two algorithms, VELSET and NAGA, that, given a query graph, return the top-k most similar subgraphs from a (large) database graph. While VELSET is more accurate and robust to noise, NAGA is faster and more applicable for scenarios with low label noise. Experiments on large real-life graph datasets depict significant improvements in terms of accuracy and running time in comparison to the state-of-the-art methods.	Neighbor-Aware Search for Approximate Labeled Graph Matching using the Chi-Square Statistics	NA:NA:NA	2017
Bhaskar Mitra:Fernando Diaz:Nick Craswell	Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favourable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or 'duet' performs significantly better than either neural network individually on a Web page ranking task, and significantly outperforms traditional baselines and other recently proposed models based on neural networks.	Learning to Match using Local and Distributed Representations of Text for Web Search	NA:NA:NA	2017
Alexey Drutsa:Gleb Gusev:Pavel Serdyukov	State-of-the-art user engagement metrics (such as session-per-user) are widely used by modern Internet companies to evaluate ongoing updates of their web services via A/B testing. These metrics are predictive of companies' long-term goals, but suffer from this property due to slow user learning of an evaluated treatment, which causes a delay in the treatment effect. That, in turn, causes low sensitivity of the metrics and requires to conduct A/B experiments with longer duration or larger set of users from a limited traffic. In this paper, we study how the delay property of user learning can be used to improve sensitivity of several popular metrics of user loyalty and activity. We consider both novel and previously known modifications of these metrics, including different methods of quantifying a trend in a metric's time series and delaying its calculation. These modifications are analyzed with respect to their sensitivity and directionality on a large set of A/B tests run on real users of Yandex. We discover that mostly loyalty metrics gain profit from the considered modifications. We find such modifications that both increase sensitivity of the source metric and are consistent with the sign of its average treatment effect as well.	Using the Delay in a Treatment Effect to Improve Sensitivity and Preserve Directionality of Engagement Metrics in A/B Experiments	NA:NA:NA	2017
Qian Zhao:Yue Shi:Liangjie Hong	Latent factor models and decision tree based models are widely used in tasks of prediction, ranking and recommendation. Latent factor models have the advantage of interpreting categorical features by a low-dimensional representation, while such an interpretation does not naturally fit numerical features. In contrast, decision tree based models enjoy the advantage of capturing the nonlinear interactions of numerical features, while their capability of handling categorical features is limited by the cardinality of those features. Since in real-world applications we usually have both abundant numerical features and categorical features with large cardinality (e.g. geolocations, IDs, tags etc.), we design a new model, called GB-CENT, which leverages latent factor embedding and tree components to achieve the merits of both while avoiding their demerits. With two real-world data sets, we demonstrate that GB-CENT can effectively (i.e. fast and accurately) achieve better accuracy than state-of-the-art matrix factorization, decision tree based models and their ensemble.	GB-CENT: Gradient Boosted Categorical Embedding and Numerical Trees	NA:NA:NA	2017
Jun Hu:Ping Li	We propose a new pointwise collaborative ranking approach for recommender systems, which focuses on improving ranking performance at the top of recommended list. Our approach is different from common pointwise methods in that we consider user ratings as ordinal rather than viewing them as real values or categorical labels. In addition, positively rated items (higher rating scores) are emphasized more in our method in order to improve the performance at the top of recommended list. In our method, user ratings are modeled based on an ordinal classification framework, which is made up of a sequence of binary classification problems in which one discriminates between ratings no less than a specific ordinal category c and ratings below that category ({̥ c}vs.{< c}). The results are used subsequently to generate a ranking score that puts higher weights on the output of those binary classification problems concerning high values of c so as to improve the ranking performance at the top of list. As our method crucially builds on a decomposition into binary classification problems, we call our proposed method as Decoupled Collaborative Ranking (DCR). As an extension, we impose pairwise learning on DCR, which yields further improvement with regard to the ranking performance of the proposed method. We demonstrate through extensive experiments on benchmark datasets that our method outperforms many considered state-of-the-art collaborative ranking algorithms in terms of the NDCG metric.	Decoupled Collaborative Ranking	NA:NA	2017
Avradeep Bhowmik:Joydeep Ghosh	Learning the true rank ordering among objects by aggregating a set of expert opinion rank order lists is an important and ubiquitous problem in many applications ranging from social choice theory to recommendation systems and search aggregation. We study the problem of unsupervised rank aggregation where no ground truth ordering information in available, neither about the true preference ordering among any set of objects nor about the quality of individual rank lists. Aggregating the often inconsistent and poor quality rank lists in such an unsupervised manner is a highly challenging problem, and standard consensus-based methods fall short in terms of both quality and scalability. In this manuscript we propose a novel framework to bypass these issues by using object attributes to augment the standard rank aggregation framework. We design algorithms that learn joint models on both rank lists and object features to obtain an aggregated rank ordering that is more accurate and robust, and also helps weed out rank lists of dubious validity. We validate our techniques on synthetic datasets where our algorithm is able to estimate the true rank ordering even when the rank lists are corrupted. Experiments on three real datasets, MQ2008, MQ2007 and OHSUMED, show that using object features can result in significant improvement in performance over existing rank aggregation methods that do not use object information. Furthermore, when at least some of the rank lists are of high quality, our methods are able to effectively exploit such information to output an aggregated rank ordering of high accuracy.	LETOR Methods for Unsupervised Rank Aggregation	NA:NA	2017
Immanuel Bayer:Xiangnan He:Bhargav Kanagal:Steffen Rendle	In recent years, interest in recommender research has shifted from explicit feedback towards implicit feedback data. A diversity of complex models has been proposed for a wide variety of applications. Despite this, learning from implicit feedback is still computationally challenging. So far, most work relies on stochastic gradient descent (SGD) solvers which are easy to derive, but in practice challenging to apply, especially for tasks with many items. For the simple matrix factorization model, an efficient coordinate descent (CD) solver has been previously proposed. However, efficient CD approaches have not been derived for more complex models. In this paper, we provide a new framework for deriving efficient CD algorithms for complex recommender models. We identify and introduce the property of k-separable models. We show that k-separability is a sufficient property to allow efficient optimization of implicit recommender problems with CD. We illustrate this framework on a variety of state-of-the-art models including factorization machines and Tucker decomposition. To summarize, our work provides the theory and building blocks to derive efficient implicit CD algorithms for complex recommender models.	A Generic Coordinate Descent Framework for Learning from Implicit Feedback	NA:NA:NA:NA	2017
Roy Ka-Wei Lee:Tuan-Anh Hoang:Ee-Peng Lim	Topic modeling has traditionally been studied for single text collections and applied to social media data represented in the form of text documents. With the emergence of many social media platforms, users find themselves using different social media for posting content and for social interaction. While many topics may be shared across social media platforms, users typically show preferences of certain social media platform(s) over others for certain topics. Such platform preferences may even be found at the individual level. To model social media topics as well as platform preferences of users, we propose a new topic model known as MultiPlatform-LDA (MultiLDA). Instead of just merging all posts from different social media platforms into a single text collection, MultiLDA keeps one text collection for each social media platform but allowing these platforms to share a common set of topics. MultiLDA further learns the user-specific platform preferences for each topic. We evaluate MultiLDA against TwitterLDA, the state-of-the-art method for social media content modeling, on two aspects: (i) the effectiveness in modeling topics across social media platforms, and (ii) the ability to predict platform choices for each post. We conduct experiments on three real-world datasets from Twitter, Instagram and Tumblr sharing a set of common users. Our experiments results show that the MultiLDA outperforms in both topic modeling and platform choice prediction tasks. We also show empirically that among the three social media platforms, "Daily matters" and "Relationship matters" are dominant topics in Twitter, "Social gathering", "Outing" and "Fashion" are dominant topics in Instagram, and "Music", "Entertainment" and "Fashion" are dominant topics in Tumblr.	On Analyzing User Topic-Specific Platform Preferences Across Multiple Social Media Sites	NA:NA:NA	2017
Rahmtin Rotabi:Cristian Danescu-Niculescu-Mizil:Jon Kleinberg	In many domains, a latent competition among different conventions determines which one will come to dominate. One sees such effects in the success of community jargon, of competing frames in political rhetoric, or of terminology in technical contexts. These effects have become widespread in the on-line domain, where the ease of information transmission makes them particularly forceful, and where the available data offers the potential to study competition among conventions at a fine-grained level. In analyzing the dynamics of conventions over time, however, even with detailed on-line data, one encounters two significant challenges. First, as conventions evolve, the underlying substance of their meaning tends to change as well; and such substantive changes confound investigations of social effects. Second, the selection of a convention takes place through the complex interactions of individuals within a community, and contention between the users of competing conventions plays a key role in the convention's evolution. Any analysis of the overall dynamics must take place in the presence of these two issues. In this work we study a setting in which we can cleanly track the competition among conventions while explicitly taking these sources of complexity into account. Our analysis is based on the spread of low-level authoring conventions in the e-print arXiv over 24 years and roughly a million posted papers: by tracking the spread of macros and other author-defined conventions, we are able to study conventions that vary even as the underlying meaning remains constant. We find that the interaction among co-authors over time plays a crucial role in the selection of conventions; the distinction between more and less experienced members of the community, and the distinction between conventions with visible versus invisible effects, are both central to the underlying processes. Through our analysis we make predictions at the population level about the ultimate success of different synonymous conventions over time --- and at the individual level about the outcome of ``fights'' between people over convention choices.	Competition and Selection Among Conventions	NA:NA:NA	2017
George Berry:Sean J. Taylor	Studies of online social influence have demonstrated that friends have important effects on many types of behavior in a wide variety of settings. However, we know much less about how influence works among relative strangers in digital public squares, despite important conversations happening in such spaces. We present the results of a study on large public Facebook Pages where we randomly used two different methods---most recent and social feedback---to order comments on posts. We find that the social feedback condition results in higher quality viewed comments and response comments. After measuring the average quality of comments written by users before the study, we find that social feedback has a positive effect on response quality for both low and high quality commenters. We draw on a theoretical framework of social norms to explain this empirical result. In order to examine the influence mechanism further, we measure the similarity between comments viewed and written during the study, finding that similarity increases for the highest quality contributors under the social feedback condition. This suggests that, in addition to norms, some individuals may respond with increased relevance to high-quality comments.	Discussion Quality Diffuses in the Digital Public Square	NA:NA	2017
Liye Fu:Lillian Lee:Cristian Danescu-Niculescu-Mizil	Group discussions are a way for individuals to exchange ideas and arguments in order to reach better decisions than they could on their own. One of the premises of productive discussions is that better solutions will prevail, and that the idea selection process is mediated by the (relative) competence of the individuals involved. However, since people may not know their actual competence on a new task, their behavior is influenced by their self-estimated competence -- that is, their confidence -- which can be misaligned with their actual competence. Our goal in this work is to understand the effects of confidence-competence misalignment on the dynamics and outcomes of discussions. To this end, we design a large-scale natural setting, in the form of an online team-based geography game, that allows usto disentangle confidence from competence and thus separate their effects. We find that in task-oriented discussions, the more-confident individuals have a larger impact on the group's decisions even when these individuals are at the same level of competence as their teammates. Furthermore, this unjustified role of confidence in the decision-making process often leads teams to under-perform. We explore this phenomenon by investigating the effects of confidence on conversational dynamics. For example, we take up the question: do more-confident people introduce more ideas than the less-confident, or do they introduce the same number of ideas but their ideas get more uptake? Moreover, we show that the language people use is more predictive of a person's confidence level than their actual competence. This also suggests potential practical applications, given that in many settings, true competence cannot be assessed before the task is completed, whereas the conversation can be tracked during the course of the problem-solving process.	When Confidence and Competence Collide: Effects on Online Decision-Making Discussions	NA:NA:NA	2017
Ellery Wulczyn:Nithum Thain:Lucas Dixon	The damage personal attacks cause to online discourse motivates many platforms to try to curb the phenomenon. However, understanding the prevalence and impact of personal attacks in online platforms at scale remains surprisingly difficult. The contribution of this paper is to develop and illustrate a method that combines crowdsourcing and machine learning to analyze personal attacks at scale. We show an evaluation method for a classifier in terms of the aggregated number of crowd-workers it can approximate. We apply our methodology to English Wikipedia, generating a corpus of over 100k high quality human-labeled comments and 63M machine-labeled ones from a classifier that is as good as the aggregate of 3 crowd-workers, as measured by the area under the ROC curve and Spearman correlation. Using this corpus of machine-labeled scores, our methodology allows us to explore some of the open questions about the nature of online personal attacks. This reveals that the majority of personal attacks on Wikipedia are not the result of a few malicious users, nor primarily the consequence of allowing anonymous contributions from unregistered users.	Ex Machina: Personal Attacks Seen at Scale	NA:NA:NA	2017
Dominik Kowald:Subhash Chandra Pujari:Elisabeth Lex	Hashtags have become a powerful tool in social platforms such as Twitter to categorize and search for content, and to spread short messages across members of the social network. In this paper, we study temporal hashtag usage practices in Twitter with the aim of designing a cognitive-inspired hashtag recommendation algorithm we call BLLi,s. Our main idea is to incorporate the effect of time on (i) individual hashtag reuse (i.e., reusing own hashtags), and (ii) social hashtag reuse (i.e., reusing hashtags, which has been previously used by a followee) into a predictive model. For this, we turn to the Base-Level Learning (BLL) equation from the cognitive architecture ACT-R, which accounts for the time-dependent decay of item exposure in human memory. We validate BLLI,S using two crawled Twitter datasets in two evaluation scenarios. Firstly, only temporal usage patterns of past hashtag assignments are utilized and secondly, these patterns are combined with a content-based analysis of the current tweet. In both evaluation scenarios, we find not only that temporal effects play an important role for both individual and social hashtag reuse but also that our BLLI,S approach provides significantly better prediction accuracy and ranking results than current state-of-the-art hashtag recommendation methods.	Temporal Effects on Hashtag Reuse in Twitter: A Cognitive-Inspired Hashtag Recommendation Approach	NA:NA:NA	2017
Sihem Amer-Yahia:Sofia Kleisarchaki:Naresh Kumar Kolloju:Laks V.S. Lakshmanan:Ruben H. Zamar	Online rated datasets have become a source for large-scale population studies for analysts and a means for end-users to achieve routine tasks such as finding a book club. Existing systems however only provide limited insights into the opinions of different segments of the rater population. In this paper, we develop a framework for finding and exploring population segments and their opinions. We propose rating maps, a collection of (population segment, rating distribution) pairs, where a segment, e.g., {18-29 year old males in CA} has a rating distribution in the form of a histogram that aggregates its ratings for a set of items (e.g., movies starring Russel Crowe). We formalize the problem of building rating maps dynamically given desired input distributions. Our problem raises two challenges: (i) the choice of an appropriate measure for comparing rating distributions, and (ii) the design of efficient algorithms to find segments. We show that the Earth Mover's Distance (EMD) is well-adapted to comparing rating distributions and prove that finding segments whose rating distribution is close to input ones is NP-complete. We propose an efficient algorithm for building Partition Decision Trees and heuristics for combining the resulting partitions to further improve their quality. Our experiments on real and synthetic datasets validate the utility of rating maps for both analysts and end-users.	Exploring Rated Datasets with Rating Maps	NA:NA:NA:NA:NA	2017
Charalampos Mavroforakis:Isabel Valera:Manuel Gomez-Rodriguez	People are increasingly relying on social media and the Web to find solutions to their problems in a wide range of domains. In this setting, closely related problems often lead to the same characteristic learning pattern --- people sharing a similar problem visit closely related pieces of information, perform almost identical queries or, more generally, take a series of similar actions at a similar pace. In this paper, we introduce a novel modeling framework for clustering continuous-time grouped streaming data, the Hierarchical Dirichlet Hawkes process (HDHP), which allows us to automatically uncover a wide variety of learning patterns from detailed traces of learning activity. Our model allows for efficient inference, scaling to millions of actions and thousands of users. Experiments on real data from Stack Overflow reveal that our framework recovers meaningful learning patterns, accurately tracks users' interests and goals over time and achieves better predictive performance than the state of the art.	Modeling the Dynamics of Learning Activity on the Web	NA:NA:NA	2017
Ali Pinar:C. Seshadhri:Vaidyanathan Vishal	Counting the frequency of small subgraphs is a fundamental technique in network analysis across various domains, most notably in bioinformatics and social networks. The special case of triangle counting has received much attention. Getting results for 4-vertex or 5-vertex patterns is highly challenging, and there are few practical results known that can scale to massive sizes. We introduce an algorithmic framework that can be adopted to count any small pattern in a graph and apply this framework to compute exact counts for all 5-vertex subgraphs. Our framework is built on cutting a pattern into smaller ones, and using counts of smaller patterns to get larger counts. Furthermore, we exploit degree orientations of the graph to reduce runtimes even further. These methods avoid the combinatorial explosion that typical subgraph counting algorithms face. We prove that it suffices to enumerate only four specific subgraphs (three of them have less than 5 vertices) to exactly count all 5-vertex patterns. We perform extensive empirical experiments on a variety of real-world graphs. We are able to compute counts of graphs with tens of millions of edges in minutes on a commodity machine. To the best of our knowledge, this is the first practical algorithm for 5-vertex pattern counting that runs at this scale. A stepping stone to our main algorithm is a fast method for counting all 4-vertex patterns. This algorithm is typically ten times faster than the state of the art 4-vertex counters.	ESCAPE: Efficiently Counting All 5-Vertex Subgraphs	NA:NA:NA	2017
Priya Govindan:Chenghong Wang:Chumeng Xu:Hongyu Duan:Sucheta Soundarajan	The structure of real-world complex networks has long been an area of interest, and one common way to describe the structure of a network has been with the k-core decomposition. The core number of a node can be thought of as a measure of its centrality and importance, and is used by applications such as community detection, understanding viral spreads, and detecting fraudsters. However, we observe that the k-core decomposition suffers from an important flaw: namely, it is calculated globally, and so if the network contains distinct regions of different densities, the sparser among these regions may be neglected. To resolve this issue, we propose the k-peak graph decomposition method, based on the k-core algorithm, which finds the centers of distinct regions in the graph. Our contributions are as follows: (1) We present a novel graph decomposition- the k-peak decomposition- and corresponding algorithm, and perform a theoretical analysis of its properties. (2) We describe a new visualization method, the "Mountain Plot", which can be used to better understand the global structure of a graph. (3) We perform an extensive empirical analysis of real-world graphs, including technological, social, biological, and collaboration graphs, and show how the k-peak decomposition gives insight into the structures of these graphs. (4) We demonstrate the advantage of using the k-peak decomposition in various applications, including community detection, contagion and identifying essential proteins.	The k-peak Decomposition: Mapping the Global Structure of Graphs	NA:NA:NA:NA:NA	2017
Charalampos E. Tsourakakis:Jakub Pachocki:Michael Mitzenmacher	We develop new methods based on graph motifs for graph clustering, allowing more efficient detection of communities within networks. We focus on triangles within graphs, but our techniques extend to other clique motifs as well. Our intuition, which has been suggested but not formalized similarly in previous works, is that triangles are a better signature of community than edges. We therefore generalize the notion of conductance for a graph to triangle conductance, where the edges are weighted according to the number of triangles containing the edge. This methodology allows us to develop variations of several existing clustering techniques, including spectral clustering, that minimize triangles split by the cluster instead of edges cut by the cluster. We provide theoretical results in a planted partition model to demonstrate the potential for triangle conductance in clustering problems. We then show experimentally the effectiveness of our methods to multiple applications in machine learning and graph mining.	Scalable Motif-aware Graph Clustering	NA:NA:NA	2017
Aaron Archer:Silvio Lattanzi:Peter Likarish:Sergei Vassilvitskii	We consider the reachability indexing problem for private-public directed graphs. In these graphs nodes come in three flavors: public--nodes visible to all users, private--nodes visible to a specific set of users, and protected--nodes visible to any user who can see at least one of the node's parents. We are interested in computing the set of nodes visible to a specific user online. There are two obvious algorithms: precompute the result for every user, or run a reachability algorithm at query time. This paper explores the trade-off between these two strategies. Our approach is to identify a set of additional visible seed nodes for each user. The online reachability algorithm explores the graph starting at these nodes. We first formulate the problem as asymmetric k-center with outliers, and then give an efficient and practical algorithm. We prove new theoretical guarantees for this problem and show empirically that it performs very well in practice.	Indexing Public-Private Graphs	NA:NA:NA:NA	2017
Danny Yuxing Huang:Doug Grundman:Kurt Thomas:Abhishek Kumar:Elie Bursztein:Kirill Levchenko:Alex C. Snoeren	In this paper, we investigate a new form of blackhat search engine optimization that targets local listing services like Google Maps. Miscreants register abusive business listings in an attempt to siphon search traffic away from legitimate businesses and funnel it to deceptive service industries---such as unaccredited locksmiths---or to traffic-referral scams, often for the restaurant and hotel industry. In order to understand the prevalence and scope of this threat, we obtain access to over a hundred-thousand business listings on Google Maps that were suspended for abuse. We categorize the types of abuse affecting Google Maps; analyze how miscreants circumvented the protections against fraudulent business registration such as postcard mail verification; identify the volume of search queries affected; and ultimately explore how miscreants generated a profit from traffic that necessitates physical proximity to the victim. This physical requirement leads to unique abusive behaviors that are distinct from other online fraud such as pharmaceutical and luxury product scams.	Pinning Down Abuse on Google Maps	NA:NA:NA:NA:NA:NA:NA	2017
Oleksii Starov:Nick Nikiforakis	Users have come to rely on browser extensions to realize features that are not implemented by browser vendors. Extensions offer users the ability to, among others, block ads, de-clutter websites, enrich pages with third-party content, and take screenshots. At the same time, because of their privileged position inside a user's browser, extensions have access to content and functionality that is not available to webpages, such as, the ability to conduct and read cross-origin requests, as well as get access to a browser's history and cookie jar. In this paper, we report on the first large-scale study of privacy leakage enabled by extensions. By using dynamic analysis and simulated user interactions, we investigate the leaking happening by the 10,000 most popular browser extensions of Google Chrome and find that a non-negligible fraction leaks sensitive information about the user's browsing habits, such as, their browsing history and search-engine queries. We identify common ways that extensions use to obfuscate this leakage and discover that, while some leakage happens on purpose, a large fraction of it is accidental because of the way that extensions attempt to introduce third-party content to a page's DOM. To counter the inference of a user's interests and private information enabled by this leakage, we design, implement, and evaluate BrowsingFog, a browser extension that automatically browses the web in a way that conceals a user's true interests, from a vantage point of history-stealing, third-party trackers.	Extended Tracking Powers: Measuring the Privacy Diffusion Enabled by Browser Extensions	NA:NA	2017
Li Chang:Hsu-Chun Hsiao:Wei Jeng:Tiffany Hyun-Jin Kim:Wei-Hsi Lin	URL redirection is a popular technique that automatically navigates users to an intended destination webpage with- out user awareness. However, such a seemingly advantageous feature may offer inadequate protection from security vulnerabilities unless every redirection is performed over HTTPS. Even worse, as long as the final redirection to a website is performed over HTTPS, the browser's URL bar indicates that the website is secure regardless of the security of prior redirections, which may provide users with a false sense of security. This paper reports a well-rounded investigation to analyze the wellness of URL redirection security. As an initial large-scale investigation, we screened the integrity and consistency of URL redirections for the Alexa top one million (1M) websites, and further examined 10,000 (10K) websites with their login features. Our results suggest that 1) the majority (83.3% in the 1M dataset and 78.6% in the 10K dataset) of redirection trails among web- sites that support only HTTPS are vulnerable to attacks, and 2) current incoherent practices (e.g., naked domains and www subdomains being redirected to different destinations with varying security levels) undermine the security guarantees provided by HTTPS and HSTS.	Security Implications of Redirection Trail in Popular Websites Worldwide	NA:NA:NA:NA:NA	2017
Milijana Surbatovich:Jassim Aljuraidan:Lujo Bauer:Anupam Das:Limin Jia	The use of end-user programming, such as if-this-then-that (IFTTT), is becoming increasingly common. Services like IFTTT allow users to easily create new functionality by connecting arbitrary Internet-of-Things (IoT) devices and online services using simple if-then rules, commonly known as recipes. However, such convenience at times comes at the cost of security and privacy risks for end users. To gain an in-depth understanding of the potential security and privacy risks, we build an information-flow model to analyze how often IFTTT recipes involve potential integrity or secrecy violations. Our analysis finds that around 50% of the 19,323 unique recipes we examined are potentially unsafe, as they contain a secrecy violation, an integrity violation, or both. We next categorize the types of harm that these potentially unsafe recipes can cause to users. After manually examining a random selection of potentially unsafe recipes, we find that recipes can not only lead to harms such as personal embarrassment but can also be exploited by an attacker, e.g., to distribute malware or carry out denial-of-service attacks. The use of IoT devices and services like IFTTT is expected only to grow in the near future; our analysis suggests users need to be both informed about and protected from these emerging threats to which they could be unwittingly exposing themselves.	Some Recipes Can Do More Than Spoil Your Appetite: Analyzing the Security and Privacy Risks of IFTTT Recipes	NA:NA:NA:NA:NA	2017
Qingyao Ai:Susan T. Dumais:Nick Craswell:Dan Liebling	As the number of email users and messages continues to grow, search is becoming more important for finding information in personal archives. In spite of its importance, email search is much less studied than web search, particularly using large-scale behavioral log analysis. In this paper we report the results of a large-scale log analysis of email search and complement this with a survey to better understand email search intent and success. We characterize email search behaviors and highlight differences from web search. When searching for email, people know many attributes about what they are looking for; they often look for specific known items; their queries are shorter and they click on fewer items than in web search. Although repeat queries are common in both email and web search, repeat visits to the same search result are much less common in email search suggesting that the same query is used for different search intents over time. We consider search intent from multiple angles. In email search logs, we find that people use email search not just to find information but also to perform tasks such as cleanup or organization, and that the distribution of actions they perform depends on the type of query. In our survey, people reported that they looked for specific information in both email search and web search, but they were much less likely to search for general information on a topic in email. The differences in overall behavior, re-finding patterns and search intents we observed between email and web search have important implications for the design of email search algorithms and interfaces.	Characterizing Email Search using Large-scale Behavioral Logs and Surveys	NA:NA:NA:NA	2017
Julia Proskurnia:Marc-Allen Cartright:Lluis Garcia-Pueyo:Ivo Krka:James B. Wendt:Tobias Kaufmann:Balint Miklos	Unsupervised template induction over email data is a central component in applications such as information extraction, document classification, and auto-reply. The benefits of automatically generating such templates are known for structured data, e.g. machine generated HTML emails. However much less work has been done in performing the same task over unstructured email data. We propose a technique for inducing high quality templates from plain text emails at scale based on the suffix array data structure. We evaluate this method against an industry-standard approach for finding similar content based on shingling, running both algorithms over two corpora: a synthetically created email corpus for a high level of experimental control, as well as user-generated emails from the well-known Enron email corpus. Our experimental results show that the proposed method is more robust to variations in cluster quality than the baseline and templates contain more text from the emails, which would benefit extraction tasks by identifying transient parts of the emails. Our study indicates templates induced using suffix arrays contain approximately half as much noise (measured as entropy) as templates induced using shingling. Furthermore, the suffix array approach is substantially more scalable, proving to be an order of magnitude faster than shingling even for modestly-sized training clusters. Public corpus analysis shows that email clusters contain on average 4 segments of common phrases, where each of the segments contains on average 9 words, thus showing that templatization could help users reduce the email writing effort by an average of 35 words per email in an assistance or auto-reply related task.	Template Induction over Unstructured Email Corpora	NA:NA:NA:NA:NA:NA:NA	2017
Hamed Zamani:Michael Bendersky:Xuanhui Wang:Mingyang Zhang	Modern search engines leverage a variety of sources, beyond the conventional query-document content similarity, to improve their ranking performance. Among them, query context has attracted attention in prior work. Previously, query context was mainly modeled by user search history, either long-term or short-term, to help the ranking of future queries. In this paper, we focus on situational context, i.e., the contextual features of the current search request that are independent from both query content and user history. As an example, situational context can depend on search request time and location. We propose two context-aware ranking models based on neural networks. The first model learns a low-dimensional deep representation from the combination of contextual features. The second model extends the first one by leveraging binarized contextual features in addition to the high-level abstractions learned using a deep network. The existing context-aware ranking models are mainly based on search history, especially click data that can be gathered from the search engine logs. Although context-aware models have been widely explored in web search, their influence on search scenarios where click data is highly sparse is relatively unstudied. The focus of this paper, personal search (e.g., email search or on-device search), is one of such scenarios. We evaluate our models using the click data collected from one of the world's largest personal search engines. The experiments demonstrate that the proposed models significantly outperform the baselines which do not take context into account. These results indicate the importance of situational context for personal search, and open up a venue for further exploration of situational context in other search scenarios.	Situational Context for Ranking in Personal Search	NA:NA:NA:NA	2017
David Carmel:Liane Lewin-Eytan:Alex Libov:Yoelle Maarek:Ariel Raviv	Web mail search is an emerging topic, which has not been the object of as many studies as traditional Web search. In particular, little is known about the characteristics of mail searchers and of the queries they issue. We study here the characteristics of Web mail searchers, and explore how demographic signals such as location, age, gender, and inferred income, influence their search behavior. We try to understand for instance, whether women exhibit different mail search patterns than men, or whether senior people formulate more precise queries than younger people. We compare our results, obtained from the analysis of a Yahoo Web mail search query log, to similar work conducted in Web and Twitter search. In addition, we demonstrate the value of the user's personal query log, as well as of the global query log and of the demographic signals, in a key search task: dynamic query auto-completion. We discuss how going beyond users' personal query logs (their search history) significantly improves the quality of suggestions, in spite of the fact that a user's mailbox is perceived as being highly personal. In particular, we note the striking value of demographic features for queries relating to companies/organizations, thus verifying our assumption that query completion benefits from leveraging queries issued by ``people like me". We believe that demographics and other such global features can be leveraged in other mail applications, and hope that this work is a first step in this direction.	The Demographics of Mail Search and their Application to Query Suggestion	NA:NA:NA:NA:NA	2017
David Carmel:Liane Lewin-Eytan:Alex Libov:Yoelle Maarek:Ariel Raviv	Mail search has traditionally served time-ranked results, even if it has been shown that relevance ranking provides higher retrieval quality on average. Some Web mail services have recently started to provide relevance ranking options such as the relevance toggle in the search results page of Yahoo Mail, or the ``top results" section in Inbox by Gmail. Yet, ranking results by relevance is not accepted by all, either in mail search, or in in other domains such as social media, where it has even triggered some public outcry. Given the sensitivity of the topic, we propose here to investigate a mixed approach of promoting the most relevant results, to which we refer as ``heroes'', on top of time-ranked results. We argue that this approach represents a good compromise to mail searchers, supporting on one hand the time sorted paradigm they are familiar with, while being almost as effective as full relevance ranking view that Web mail users seem to be reluctant to adopt. We describe three hero-selection algorithms we have devised and the associated experiments we have conducted in Yahoo mail. We measure retrieval success via two metrics: MRR (Mean Reciprocal Rank) and [email protected], and verify agreement between these metrics and users' direct feedback. We demonstrate that supplementing time-sorted results with hero results leads to a higher MRR than the traditional time-sorted view. We additionally show that MRR better reflects users' perception of quality than [email protected] Finally, we report on online results following the successful launch of one of our hero-selection algorithms for all Yahoo enterprise mail users and a few million Yahoo Web mail users.	Promoting Relevant Results in Time-Ranked Mail Search	NA:NA:NA:NA:NA	2017
Jinyuan Jia:Binghui Wang:Le Zhang:Neil Zhenqiang Gong	In the attribute inference problem, we aim to infer users' private attributes (e.g., locations, sexual orientation, and interests) using their public data in online social networks. State-of-the-art methods leverage a user's both public friends and public behaviors (e.g., page likes on Facebook, apps that the user reviewed on Google Play) to infer the user's private attributes. However, these methods suffer from two key limitations: 1) suppose we aim to infer a certain attribute for a target user using a training dataset, they only leverage the labeled users who have the attribute, while ignoring the label information of users who do not have the attribute; 2) they are inefficient because they infer attributes for target users one by one. As a result, they have limited accuracies and applicability in real-world social networks. In this work, we propose AttriInfer, a new method to infer user attributes in online social networks. AttriInfer can leverage both friends and behaviors, as well as the label information of training users who have an attribute and who do not have the attribute. Specifically, we model a social network as a pairwise Markov Random Field (pMRF). Given a training dataset, which consists of some users who have a certain attribute and some users who do not have a certain attribute, we compute the posterior probability that a target user has the attribute and use the posterior probability to infer attributes. In the basic version of AttriInfer, we use Loopy Belief Propagation (LBP) to compute the posterior probability. However, LBP is not scalable to very large-scale real-world social networks and not guaranteed to converge. Therefore, we further optimize LBP to be scalable and guaranteed to converge. We evaluated our method and compare it with state-of-the-art methods using a real-world Google+ dataset with 5.7M users. Our results demonstrate that our method substantially outperforms state-of-the-art methods in terms of both accuracy and efficiency.	AttriInfer: Inferring User Attributes in Online Social Networks Using Markov Random Fields	NA:NA:NA:NA	2017
Ajaya Neupane:Nitesh Saxena:Leanne Hirshfield	In this paper, we study the neural underpinnings relevant to user-centered web security through the lens of functional near-infrared spectroscopy (fNIRS). Specifically, we design and conduct an fNIRS study to pursue a thorough investigation of users' processing of legitimate vs. illegitimate and familiar vs. unfamiliar websites. We pinpoint the neural activity in these tasks as well as the brain areas that control such activity. We show that, at the neurological level, users process the legitimate websites differently from the illegitimate websites when subject to phishing attacks. Similarly, we show that users exhibit marked differences in the way their brains process the previously familiar websites from unfamiliar websites. These findings have several defensive and offensive implications. In particular, we discuss how these differences may be used by the system designers in the future to differentiate between legitimate and illegitimate websites automatically based on neural signals. Similarly, we discuss the potential for future malicious attackers, with access to neural signals, in compromising the privacy of users by detecting whether a website is previously familiar or unfamiliar to the user. Compared to prior research, our novelty lies in several aspects. First, we employ a neuroimaging methodology (fNIRS) not tapped into by prior security research for the problem domain we are studying. Second, we provide a focused study design and comprehensive investigation of the neural processing underlying the specific tasks of legitimate vs. illegitimate and familiar vs. unfamiliar websites. Third, we use an experimental set-up much more amenable to real-world settings, compared to previous fMRI studies. Beyond these scientific innovations, our work also serves to corroborate and extend several of the findings of the prior literature with independent methodologies, tools, and settings.	Neural Underpinnings of Website Legitimacy and Familiarity Detection: An fNIRS Study	NA:NA:NA	2017
Sungchul Kim:Nikhil Kini:Jay Pujara:Eunyee Koh:Lise Getoor	Personalization -- the customization of experiences, interfaces, and content to individual users -- has catalyzed user growth and engagement for many web services. A critical prerequisite to personalization is establishing user identity. However the variety of devices, including mobile phones, appliances, and smart watches, from which users access web services from both anonymous and logged-in sessions poses a significant obstacle to user identification. The resulting entity resolution task of establishing user identity across devices and sessions is commonly referred to as ``visitor stitching.'' We introduce a general, probabilistic approach to visitor stitching using features and attributes commonly contained in web logs. Using web logs from two real-world corporate websites, we motivate the need for probabilistic models by quantifying the difficulties posed by noise, ambiguity, and missing information in deployment. Next, we introduce our approach using probabilistic soft logic (PSL), a statistical relational learning framework capable of capturing similarities across many sessions and enforcing transitivity. We present a detailed description of model features and design choices relevant to the visitor stitching problem. Finally, we evaluate our PSL model on binary classification performance for two real-world visitor stitching datasets. Our model demonstrates significantly better performance than several state-of-the-art classifiers, and we show how this advantage results from collective reasoning across sessions.	Probabilistic Visitor Stitching on Cross-Device Web Logs	NA:NA:NA:NA:NA	2017
Philipp Singer:Florian Lemmerich:Robert West:Leila Zia:Ellery Wulczyn:Markus Strohmaier:Jure Leskovec	Wikipedia is one of the most popular sites on the Web, with millions of users relying on it to satisfy a broad range of information needs every day. Although it is crucial to understand what exactly these needs are in order to be able to meet them, little is currently known about why users visit Wikipedia. The goal of this paper is to fill this gap by combining a survey of Wikipedia readers with a log-based analysis of user activity. Based on an initial series of user surveys, we build a taxonomy of Wikipedia use cases along several dimensions, capturing users' motivations to visit Wikipedia, the depth of knowledge they are seeking, and their knowledge of the topic of interest prior to visiting Wikipedia. Then, we quantify the prevalence of these use cases via a large-scale user survey conducted on live Wikipedia with almost 30,000 responses. Our analyses highlight the variety of factors driving users to Wikipedia, such as current events, media coverage of a topic, personal curiosity, work or school assignments, or boredom. Finally, we match survey responses to the respondents' digital traces in Wikipedia's server logs, enabling the discovery of behavioral patterns associated with specific use cases. For instance, we observe long and fast-paced page sequences across topics for users who are bored or exploring randomly, whereas those using Wikipedia for work or school spend more time on individual articles focused on topics such as science. Our findings advance our understanding of reader motivations and behavior on Wikipedia and can have implications for developers aiming to improve Wikipedia's user experience, editors striving to cater to their readers' needs, third-party services (such as search engines) providing access to Wikipedia content, and researchers aiming to build tools such as recommendation engines.	Why We Read Wikipedia	NA:NA:NA:NA:NA:NA:NA	2017
Xin Wang:Steven C.H. Hoi:Martin Ester:Jiajun Bu:Chun Chen	Recent years have seen a surge of research on social recommendation techniques for improving recommender systems due to the growing influence of social networks to our daily life. The intuition of social recommendation is that users tend to show affinities with items favored by their social ties due to social influence. Despite the extensive studies, no existing work has attempted to distinguish and learn the personalized preferences between strong and weak ties, two important terms widely used in social sciences, for each individual in social recommendation. In this paper, we first highlight the importance of different types of ties in social relations originated from social sciences, and then propose anovel social recommendation method based on a new Probabilistic Matrix Factorization model that incorporates the distinction of strong and weak ties for improving recommendation performance. The proposed method is capable of simultaneously classifying different types of social ties in a social network w.r.t. optimal recommendation accuracy, and learning a personalized tie type preference for each user in addition to other parameters. We conduct extensive experiments on four real-world datasets by comparing our method with state-of-the-art approaches, and find encouraging results that validate the efficacy of the proposed method in exploiting the personalized preferences of strong and weak ties for social recommendation.	Learning Personalized Preference of Strong and Weak Ties for Social Recommendation	NA:NA:NA:NA:NA	2017
Xiaokai Wei:Linchuan Xu:Bokai Cao:Philip S. Yu	Link Prediction has been an important task for social and information networks. Existing approaches usually assume the completeness of network structure. However, in many real-world networks, the links and node attributes can usually be partially observable. In this paper, we study the problem of Cross View Link Prediction (CVLP) on partially observable networks, where the focus is to recommend nodes with only links to nodes with only attributes (or vice versa). We aim to bridge the information gap by learning a robust consensus for link-based and attribute-based representations so that nodes become comparable in the latent space. Also, the link-based and attribute-based representations can lend strength to each other via this consensus learning. Moreover, attribute selection is performed jointly with the representation learning to alleviate the effect of noisy high-dimensional attributes. We present two instantiations of this framework with different loss functions and develop an alternating optimization framework to solve the problem. Experimental results on four real-world datasets show the proposed algorithm outperforms the baseline methods significantly for cross-view link prediction.	Cross View Link Prediction by Learning Noise-resilient Representation Consensus	NA:NA:NA:NA	2017
Xiang Li:Yao Wu:Martin Ester:Ben Kao:Xin Wang:Yudian Zheng	A heterogeneous information network (HIN) is one whose nodes model objects of different types and whose links model objects' relationships. In many applications, such as social networks and RDF-based knowledge bases, information can be modeled as HINs. To enrich its information content, objects (as represented by nodes) in an HIN are typically associated with additional attributes. We call such an HIN an Attributed HIN or AHIN. We study the problem of clustering objects in an AHIN, taking into account objects' similarities with respect to both object attribute values and their structural connectedness in the network. We show how supervision signal, expressed in the form of a must-link set and a cannot-link set, can be leveraged to improve clustering results. We put forward the SCHAIN algorithm to solve the clustering problem. We conduct extensive experiments comparing SCHAIN with other state-of-the-art clustering algorithms and show that SCHAIN outperforms the others in clustering quality.	Semi-supervised Clustering in Attributed Heterogeneous Information Networks	NA:NA:NA:NA:NA:NA	2017
Gianluca Stringhini:Jeff Z. Pan	It is our great pleasure to welcome you to the Poster Track, associated with WWW 2016. The poster track is a forum to foster interactions among researchers and practitioners, by allowing them to present their new and innovative work in-progress. By presenting their ideas to the WWW 2016, researchers will have a chance to collect feedback from the WWW community and start fruitful conversations. We have a wide variety of topics in the poster track, which cover many topics of interest to the WWW community. We hope that you will enjoy attending the track, and you will find it useful for your future research. The call for papers attracted submissions from United States and Europe. The program committee reviewed and accepted the following: Full Technical Papers Reviewed 182 Accepted 72.	Session details: Posters	NA:NA	2016
Behnoush Abdollahi:Olfa Nasraoui	Explanations have been shown to increase the user's trust in recommendations in addition to providing other benefits such as scrutability, which is the ability to verify the validity of recommendations. Most explanation methods are designed for classical neighborhood-based Collaborative Filtering (CF) or rule-based methods. For the state of the art Matrix Factorization (MF) recommender systems, recent explanation methods, require an additional data source, such as item content data, in addition to rating data. In this paper, we address the case where no such additional data is available and propose a new Explainable Matrix Factorization (EMF) technique that computes an accurate top-$n$ recommendation list of items that are explainable. We also introduce new explanation quality metrics, that we call Mean Explainability Precision (MEP) and Mean Explainability Recall (MER).	Explainable Matrix Factorization for Collaborative Filtering	NA:NA	2016
Medha Atre	Evaluating SPARQL queries with the DISTINCT clause may become memory intensive due to the requirement of additional auxiliary data structures, like hash-maps, to discard the duplicates. DISTINCT queries make up to 16% of all the queries (e.g., DBPedia), and thus are non-negligible. In this poster we propose a novel method for such queries, by just manipulating the compressed bit-vector indexes called BitMats, for acyclic basic graph pattern (BGP) queries.	For the DISTINCT Clause of SPARQL Queries	NA	2016
Xiaomei Bai:Jun Zhang:Hai Cui:Zhaolong Ning:Feng Xia	Evaluating the impact of an article is a significant topic and has attracted extensive attention. Citation-based assessment methods currently face a limitation, i.e. the anomalous citations patterns still remain poorly understand. To remedy this drawback, we propose a Positive and Negative Conflict of Interest (COI)-based Rank algorithm, named PNCOIRank, to acquire positive COI, negative COI, positive suspected COI and negative suspected COI relationships. We investigate the citation relationships by the following scholarly factors: citing times, the interval of citing time, collaboration times, the interval of collaboration time, and team of citing authors with the purpose of weakening the COI relationships in citation network. A weighted PageRank is finally constructed and employed, with HITS algorithm to assess the impact of articles. Through experiments on American Physical Society (APS) dataset, we show that PNCOIRank significantly outperforms the existing methods in terms of recommendation intensity.	PNCOIRank: Evaluating the Impact of Scholarly Articles with Positive and Negative Citations	NA:NA:NA:NA:NA	2016
Rajesh Basak:Niloy Ganguly:Shamik Sural:Soumya K. Ghosh	Online social networks (OSNs) are often flooded with scathing remarks against individuals or businesses on their perceived wrongdoing. This paper studies three such events to get insight into various aspects of shaming done through twitter. An important contribution of our work is categorization of shaming tweets, which helps in understanding the dynamics of spread of online shaming events. It also facilitates automated segregation of shaming tweets from non-shaming ones.	Look Before You Shame: A Study on Shaming Activities on Twitter	NA:NA:NA:NA	2016
Amna Basharat:I. Budak Arpinar:Khaled Rasheed	In this paper, we illustrate how we leverage crowdsourcing to create workflows for knowledge engineering in specialized and knowledge intensive domains. We undertake the special case of the Arabic script of the Qur'an, a widely studied manuscript, and attempt to employ crowdsourcing methods for its thematic annotation at the sub-verse level, for which, there is no standardized knowledge model available to date. We demonstrate that our proposed method presents feasibility to achieve reliable annotations in an efficient and scalable manner. The proposed methodology and framework is meant to be generalizable to other knowledge intensive and specialized domains.	Leveraging Crowdsourcing for the Thematic Annotation of the Qur'an	NA:NA:NA	2016
Murat Ali Bayir:Ismail Hakki Toroslu	This paper introduces a new method for the session construction problem, which is the first main step of the web usage mining process. The proposed method is capable of extracting all possible maximal navigation sequences of web users. Through experiments, it is shown that when our new technique is used, it outperforms previous approaches in web usage mining applications such as next-page prediction.	Finding All Maximal Paths In Web User Sessions	NA:NA	2016
Martin Becker:Hauke Mewes:Andreas Hotho:Dimitar Dimitrov:Florian Lemmerich:Markus Strohmaier	HypTrails is a bayesian approach for comparing different hypotheses about human trails on the web. While a standard implementation exists, it exposes performance issues when working with large-scale data. In this paper, we propose a distributed implementation of HypTrails based on Apache Spark taking advantage of several structural properties inherent to HypTrails. The performance improves substantially. Our implementation is publicly available.	SparkTrails: A MapReduce Implementation of HypTrails for Comparing Hypotheses About Human Trails	NA:NA:NA:NA:NA:NA	2016
Atef Chaudhury:Myunghwan Kim:Mitul Tiwari	In both the online and offline world, networks that people form within certain communities are critical for their engagement and growth in those communities. In this work, we analyze the growth of ego-networks on LinkedIn for new employees of companies, and study how the pattern of network formation in the new company affects one's growth and engagement in that company. We observe that the initial state of ego-network growth in a newly joined company shows strong correlations with the future status in the company -- such as network size, network diversity, and retention. We also present some key patterns that demonstrate the importance of the first few connections in the new company as well as how they lead to the phenomena we observed.	Importance of First Steps in a Community for Growth, Diversity, and Engagement	NA:NA:NA	2016
Xu Chen:Pengfei Wang:Zheng Qin:Yongfeng Zhang	Bayesian Personal Ranking(BPR) method is a well-known model due to its high performance in the task of item recommendation. However, this method fail to distinguish user preference among the non-interacted items. In this paper, to enhance traditional BPR's performance, we introduce and analyse a hybrid method, namely Hybrid Local Bayesian Personal Ranking method(HLBPR for short). Our main idea is to construct additional item preference pairs among the products which haven't been purchased, and then utilize the extened pairs to optimize the ranking object. Experiments on two real-world transaction datasets demonstrated the effectiveness of our approach as compared with the state-of-the-art methods.	HLBPR: A Hybrid Local Bayesian Personal Ranking Method	NA:NA:NA:NA	2016
Hélène de Ribaupierre:Francesco Osborne:Enrico Motta	The natural language processing (NLP) community has developed a variety of methods for extracting and disambiguating information from research publications. However, they usually focus only on standard research entities such as authors, affiliations, venues, references and keywords. We propose a novel approach, which combines NLP and semantic technologies for generating from the text of research publications an OWL ontology describing software technologies used or introduced by researchers, such as applications, systems, frameworks, programming languages, and formats. The method was tested on a sample of 300 publications in the Semantic Web field, yielding promising results.	Combining NLP And Semantics For Mining Software Technologies From Research Publications	NA:NA:NA	2016
Angelo Di Iorio:Alejandra Gonzalez-Beltran:Francesco Osborne:Silvio Peroni:Francesco Poggi:Fabio Vitali	In this poster paper we introduce the RASH Online Conversion Service, i.e., a Web application that allows the conversion of ODT documents into RASH, a HTML-based markup language for writing scholarly articles, and from RASH into LaTeX according to Springer LNCS and ACM ICPS.	It ROCS!: The RASH Online Conversion Service	NA:NA:NA:NA:NA:NA	2016
Dimitar Dimitrov:Philipp Singer:Florian Lemmerich:Markus Strohmaier	In this work, we study the visual position of links and their clicks on Wikipedia, particularly where links are visually located, at which screen positions users click on links, and which areas on the screen exhibit more or less clicks per links. For that purpose, we introduce a novel dataset containing the on-screen coordinate position for all links between pages in the English Wikipedia and additionally resort to navigation logs of Wikipedia users. Using this data, we can observe a preference of certain link and click locations on Wikipedia including first evidence of positional click bias. For example, our results suggest that users have a tendency to prefer to click on the left side of the screen which exceeds what one would expect from the presence of links on pages. We believe that presented data and research can be useful for optimizing the process of link creation and link consumption on Wikipedia and other Web platforms.	Visual Positions of Links and Clicks on Wikipedia	NA:NA:NA:NA	2016
Hui Du:Xueke Xu:Xueqi Cheng:Dayong Wu:Yue Liu:Zhihua Yu	Recently, Deep Convolutional Neural Networks (CNNs) have been widely applied to sentiment analysis of short texts. Naturally, word embedding techniques are used to learn continuous word representations for constructing sentence matrix as input to CNN. As for sentiment analysis of customer reviews, we argue that it is problematic to learn a single representation for a word while ignoring sentiment information and the discussed aspects. In this poster, we propose a novel word embedding model to learn sentimental word embedding given specific aspects by modeling both sentiment and syntactic context under the specific aspects. We apply our method as input to CNN for sentiment analysis in multiple domains. Experiments show that the CNN based on the proposed model can consistently achieve superior performance compared to CNN based on traditional word embedding method.	Aspect-Specific Sentimental Word Embedding for Sentiment Analysis of Online Reviews	NA:NA:NA:NA:NA:NA	2016
Nemanja Duric:Mihajlo Grbovic:Vladan Radosavljevic:Jaikit Savla:Varun Bhagwan:Doug Sharp	Tourism industry has grown tremendously in the previous several decades. Despite its global impact, there still remain a number of open questions related to better understanding of tourists and their habits. In this work we analyze the largest data set of travel receipts considered thus far, and focus on exploring and modeling booking behavior of online customers. We extract useful, actionable insights into the booking behavior, and tackle the task of predicting the booking time. The presented results can be directly used to improve booking experience of customers and optimize targeting campaigns of travel operators.	Travel the World: Analyzing and Predicting Booking Behavior using E-mail Travel Receipts	NA:NA:NA:NA:NA:NA	2016
Jinhua Gao:Huawei Shen:Shenghua Liu:Xueqi Cheng	Modeling and predicting retweeting dynamics in social media has important implications to an array of applications. Existing models either fail to model the triggering effect of retweeting dynamics, e.g., the model based on reinforced Poisson process, or are hard to be trained using only the retweeting dynamics of individual tweet, e.g., the model based on self-exciting Hawkes process. In this paper, motivated by the observation that each retweeting dynamics is generally dominated by a handful of key nodes that separately trigger a high number of retweets, we propose a mixture process to model and predict retweeting dynamics, with each subprocess capturing the retweeting dynamics initiated by a key node. Experiments demonstrate that the proposed model outperforms the state-of-the-art model.	Modeling and Predicting Retweeting Dynamics via a Mixture Process	NA:NA:NA:NA	2016
Shreya Ghosh:Soumya K. Ghosh	Exploring human movement pattern from raw GPS traces is an interesting and challenging task. This paper aims at analysing a large volume of GPS data in spatio-temporal context, clustering trajectories using geographic and semantic location information and identifying different categories of people. It tries to exploit the fact that human moves with an intent. The proposed framework yields encouraging results using a large scale GPS dataset of Microsoft GeoLife.	THUMP: Semantic Analysis on Trajectory Traces to Explore Human Movement Pattern	NA:NA	2016
Zafar Gilani:Liang Wang:Jon Crowcroft:Mario Almeida:Reza Farahbakhsh	The WWW has seen a massive growth in variety and usage of OSNs. The rising population of users on Twitter and its open nature has made it an ideal platform for various kinds of opportunistic pursuits, such as news and emergency communication, business promotion, political campaigning, spamming and spreading malicious content. Most of these opportunistic pursuits are exploited through automated programs, known as bots. In this study we propose a framework (Stweeler) to study bot impact and influence on Twitter from systems and social media perspectives.	Stweeler: A Framework for Twitter Bot Analysis	NA:NA:NA:NA:NA	2016
Michel Héon:Roger Nkambou:Christian Langheit	The Web Ontology Language (OWL-2) aims at offering a family of syntax such as RDF/XML, Manchester Turtle and others, for building ontologies. Ontology engineering is a complex task that requires skills that are rarely accessible to content experts. On the other hand, to model contents pertaining to a specific domain, graphical modeling is a technique that is often used to offer a knowledge representation tool to content experts that are not well acquainted with the process of formal ontology design. In this paper, we present the way in which the usage of polymorphism and symbol typing of graphical vocabulary have allowed us to design the G-OWL syntax, a graphical syntax that aims to graphically represent domain-specific knowledge using the OWL-2.	Toward G-OWL: A Graphical, Polymorphic And Typed Syntax For Building Formal OWL2 Ontologies	NA:NA:NA	2016
Aurelie Herbelot	This paper presents PeARS (Peer-to-peer Agent for Reciprocated Search), an algorithm for distributed Web search that emulates the offline behaviour of a human with an information need. Specifically, the algorithm models the process of 'calling a friend', i.e. directing one's query to the knowledge holder most likely to answer it. The system allows network users to index and share part of their browsing history in a way that makes them 'experts' on some topics. A layer of distributional semantics agents then performs targeted information retrieval in response to user queries, in a fully automated fashion.	PeARS: a Peer-to-peer Agent for Reciprocated Search	NA	2016
Manel Hmimida:Rushed Kanawati	In this paper we propose a new graph-based tag recommendation approach. The approach is structured into an offline step and an online one. Offline, the hypergraph depicting the history of tags assignment by users to resources is abstracted. On online, for a given target user and a resource, we first compute the set of recommended abstract tags (i.e tag clusters) applying a basic graph-based approach to the abstract graph. A new reduced graph is computed by unfolding the abstract subgraph composed of the set of recommended abstract tags and nodes representing the cluster of users (resp. resources) to which the target user (resp. resource) belongs to. Again the same basic graph-based tag recommendation approach is applied to this new reduced graph in order to compute the final set of tags to recommend. Experiments on real dataset show the effectiveness of the proposed approach.	A Graph-Coarsening Approach for Tag Recommendation	NA:NA	2016
Tin Kam Ho:Luis A. Lastras:Oded Shmueli	We propose a method for a concept-centric semantic analysis of an evolving corpus, highlighting the persistent concepts, emergence of new concepts, and the changes in the semantic associations between concepts. We report our findings on a corpus of computer science literature that spans six decades, revealing interesting patterns about the progress of the discipline.	Concept Evolution Modeling Using Semantic Vectors	NA:NA:NA	2016
Kai Hui:Klaus Berberich	Offline evaluation for information retrieval aims to compare the performance of retrieval systems based on relevance judgments for a set of test queries. Since manual judgments are expensive, selective labeling has been developed to semi-automatically label documents, in the wake of the similarity relationship among retrieved documents. Intuitively, the agreement w.r.t the cluster hypothesis can directly determine the amount of manual judgments that can be saved by creating labels with a semi-automatic method. Meanwhile, in representing documents, certain information is lost. We argue that better document representation can lead to better agreement with the cluster hypothesis. To this end, we investigate different document representations on established benchmarks in the context of low-cost evaluation, showing that different document representations vary in how well they capture document similarity relative to a query.	Cluster Hypothesis in Low-Cost IR Evaluation with Different Document Representations	NA:NA	2016
Ganesh J:Soumyajit Ganguly:Manish Gupta:Vasudeva Varma:Vikram Pudi	In this paper, we consider the problem of learning representations for authors from bibliographic co-authorship networks. Existing methods for deep learning on graphs, such as DeepWalk, suffer from link sparsity problem as they focus on modeling the link information only. We hypothesize that capturing both the content and link information in a unified way will help mitigate the sparsity problem. To this end, we present a novel model 'Author2Vec', which learns low-dimensional author representations such that authors who write similar content and share similar network structure are closer in vector space. Such embeddings are useful in a variety of applications such as link prediction, node classification, recommendation and visualization. The author embeddings we learn are empirically shown to outperform DeepWalk by 2.35% and 0.83% for link prediction and clustering task respectively.	Author2Vec: Learning Author Representations by Combining Content and Link Information	NA:NA:NA:NA:NA	2016
Takamu Kaneko:Keiji Yanai	Twitter is a unique microblog, which is different from conventional social media in terms of its quickness and on-the-spot-ness. Many Twitter's users send messages, which is commonly called "tweets", to Twitter on the spot with mobile phones or smart phones, and some of them send photos and geotags as well as tweets. Most of the photos are sent to Twitter soon after taken. In case of photos related to some events, most of them are taken during the events. We think that Twitter event photo mining is more useful to under- stand what happens currently over the world than only text-based Twitter event mining. In this paper, we propose a system to mine events visually from the Twitter stream. To do that, we use not only tweets having both geotags and photos but also tweets having geotags or photos for textual analysis or visual analysis. Although there exist many works related to Twitter mining using only text analysis such as typhoon and earthquake detection by Sakaki et al. [1], only a limited number of works exist on Twitter mining using image analysis. Nakaji et al. [2] proposed a system to mine representative photos related to the given keyword or term from a large number of geo-tweet photos. They extracted representative photos related to events such as "typhoon" and "New Year's Day". They used only geotagged photo tweets the number of which are limited compared to all the photo tweets. Gao et al. [3] proposed a method to mine brand product photos fromWeibo which employs supervised image recognition, which is different from event detection. They integrated visual features and social factors (users, relations, and locations) as well as textual features for brand product photo mining. In this paper, we detect visual events using geotagged non-photo tweets and non-geotagged photo tweets as well as geotagged photo tweets. In the experiments, we show some examples of detected events and their photos such as "rainbow", "fireworks" and "festival".	Visual Event Mining from the Twitter Stream	NA:NA	2016
Fariba Karimi:Claudia Wagner:Florian Lemmerich:Mohsen Jadidi:Markus Strohmaier	Computational social scientists often harness the Web as a "societal observatory" where data about human social behavior is collected. This data enables novel investigations of psychological, anthropological and sociological research questions. However, in the absence of demographic information, such as gender, many relevant research questions cannot be addressed. To tackle this problem, researchers often rely on automated methods to infer gender from name information provided on the web. However, little is known about the accuracy of existing gender-detection methods and how biased they are against certain sub-populations. In this paper, we address this question by systematically comparing several gender detection methods on a random sample of scientists for whom we know their full name, their gender and the country of their workplace. We further suggest a novel method that employs web-based image retrieval and gender recognition in facial images in order to augment name-based approaches. Our findings show that the performance of name-based gender detection approaches can be biased towards countries of origin and such biases can be reduced by combining name-based an image-based gender detection methods.	Inferring Gender from Names on the Web: A Comparative Evaluation of Gender Detection Methods	NA:NA:NA:NA:NA	2016
Noriaki Kawamae	We present a time series analysis employing natural language processing (NLP) techniques, and show the effect of N-gram over Context (NOC), that is a one of topic models that enjoy success in NLP, in this analysis.	Time Series Analysis Using NOC	NA	2016
Sungchul Kim:Jinyoung Yeo:Eunyee Koh:Nedim Lipka	Web logs in e-commerce sites consist of user actions on items such as visiting an item description page, adding an item to a wishlist, and purchasing an item. Those items could be represented as nodes in a graph while viewing their relationships as edges according to the user actions. Based on the item graph, identifying items that attract users to purchase the target item could be practically used for supporting business decisions. To do this, we introduce a new task, called `Purchase Influence Mining', that finds the top-k items (PIM-items) maximizing the estimated purchase influence from them to a target item. We solve this problem by modeling the purchase influence as the shortest path between item pair. According to the result, our approach more consistently finds the k PIM-items than the baseline.	Purchase Influence Mining: Identifying Top-k Items Attracting Purchase of Target Item	NA:NA:NA:NA	2016
Yun-Yong Ko:Dong-Kyu Chae:Sang-Wook Kim	This paper proposes a novel approach to target-oriented influence estimation, which remedies the drawback of state-of-the-art, thereby understanding information diffusion more accurately in a social network.	Accurate Path-based Methods for Influence Maximization in Social Networks	NA:NA:NA	2016
Peter Kratky:Daniela Chuda	Biometric data are affected by physiological properties of people, including gender or age. The paper describes an experiment of discovering gender and age in computer mouse movement data that might be notably beneficial for profiling anonymous visitors browsing the Web. The proposed method extracts features, such as velocity, path straightness or pauses duration, that are used by a multiclassifier system to make an estimate. Age category estimation shows encouraging results of the early method, especially for statistical analysis of a website audience.	Estimating Gender and Age of Web Page Visitors from the Way They Use Their Mouse	NA:NA	2016
Srijan Kumar	Citations are important to track and understand the evolution of human knowledge. At the same time, it is widely accepted that all the citations made in a paper are not equal. However, there is no thorough understanding of how citations are created that explicitly criticize or endorse others. In this paper, we do a detailed study of such citations made within the NLP community by differentiating citations into endorsement (positive), criticism (negative) and neutral categories. We analyse this signed network created between papers and between authors for the first time from a social networks perspective. We make many observations - we find that the citations follow a heavy-tailed distribution and they are created in a way that follows weak balance theory and status theories. Moreover, we find that authors do not change their opinion towards others over time and rarely reciprocate the opinion that they receive. Overall, the paper builds the understanding of the structure and dynamics of positive, negative and neutral citations.	Structure and Dynamics of Signed Citation Networks	NA	2016
Hemank Lamba:Vaishnavh Nagarajan:Kijung Shin:Naji Shajarisales	Matrix and tensor completion techniques have proven useful in many applications such as recommender systems, image/video restoration, and web search. We explore the idea of using external information in completing missing values in tensors. In this work, we present a framework that employs side information as kernel matrices for tensor factorization. We apply our framework to problems of recommender systems and video restoration and show that our framework effectively deals with the cold-start problem.	Incorporating Side Information in Tensor Completion	NA:NA:NA:NA	2016
Hemank Lamba:Jürgen Pfeffer	Influence maximization has found applications in various fields such as sensor placement, viral marketing, controlling rumor outbreak, etc. In this paper, we propose a targeted approach to influence maximization in polarized networks i.e. networks where we already know or can predict node's opinion about a product or topic. The goal is to find a set of individuals to target, such that positive opinion about a specific topic or the product to be launched is maximized. Another key aspect that is present in most of the existing viral marketing algorithms is that they do not take into account the timeliness of the product adoption. In this paper, we present a framework where we infer the polarity, activity levels of the users, and then select seeds to launch viral marketing campaigns such that positive influence about the product is maximized by the given deadline.	Maximizing the Spread of Positive Influence by Deadline	NA:NA	2016
Kevin Lange Di Cesare:Michel Gagnon:Amal Zouaq:Ludovic Jean-Louis	The TAC KBP English slot filling track is an evaluation campaign that targets the extraction of 41 pre-identified relations related to specific named entities. In this work, we present a machine learning filter whose aim is to enhance the precision of relation extractors while minimizing the impact on recall. Our approach aims at filtering relation extractors' output using a binary classifier based on a wide array of features including syntactic, lexical and statistical features. We experimented the classifier on 14 of the 18 participating systems in the TAC KBP English slot filling track 2013. The results show that our filter is able to improve the precision of the best 2013 system by nearly 20\% and improve the F1-score for 17 relations out of 33 considered.	A Machine learning Filter for Relation Extraction	NA:NA:NA:NA	2016
Yang-Yin Lee:Hao Ke:Hen-Hsen Huang:Hsin-Hsi Chen	GloVe, global vectors for word representation, performs well in some word analogy and semantic relatedness tasks. However, we find that some dimensions of the trained word embedding are abnormal. We verify our conjecture via removing these abnormal dimensions using Kolmogorov-Smimov test and experiment on several benchmark datasets for semantic relatedness measurement. The experimental results confirm our finding. Interestingly, some of the tasks outperform the state-of-the-art model SensEmbed by simply removing these abnormal dimensions. The novel rule of thumb technique which leads to better performance is expected to be useful in practice.	Less is More: Filtering Abnormal Dimensions in GloVe	NA:NA:NA:NA	2016
Yang-Yin Lee:Hao Ke:Hen-Hsen Huang:Hsin-Hsi Chen	While many traditional studies on semantic relatedness utilize the lexical databases, such as WordNet or Wikitionary, the recent word embedding learning approaches demonstrate their abilities to capture syntactic and semantic information, and outperform the lexicon-based methods. However, word senses are not disambiguated in the training phase of both Word2Vec and GloVe, two famous word embedding algorithms, and the path length between any two senses of words in lexical databases cannot reflect their true semantic relatedness. In this paper, a novel approach that linearly combines Word2Vec and GloVe with the lexical database WordNet is proposed for measuring semantic relatedness. The experiments show that the simple method outperforms the state-of-the-art model SensEmbed.	Combining Word Embedding and Lexical Database for Semantic Relatedness Measurement	NA:NA:NA:NA	2016
Oliver Lehmberg:Dominique Ritze:Robert Meusel:Christian Bizer	NA	A Large Public Corpus of Web Tables containing Time and Context Metadata	NA:NA:NA:NA	2016
Manling Li:Yantao Jia:Yuanzhuo Wang:Jingyuan Li:Xueqi Cheng	Link prediction over a knowledge graph aims to predict the missing entity h or t for a triple (h,r,t). Existing knowledge graph embedding based predictive methods represent entities and relations in knowledge graphs as elements of a vector space, and employ the structural information for link prediction. However, knowledge graphs contain many hierarchical relations, which existing methods have pay little attention to. In this paper, we propose a hierarchy-constrained locally adaptive knowledge graph embedding based link prediction method, called hTransA, by integrating hierarchical structures into the predictive work. Experiments over two benchmark data sets demonstrate the superiority of hTransA.	Hierarchy-Based Link Prediction in Knowledge Graphs	NA:NA:NA:NA:NA	2016
Yuchen Liu:Dmitry Chechik:Junghoo Cho	This paper introduces human curation signals and demonstrates incorporating human curation signals improves the relevance of state-of-art recommendation system models by up to 30% by experiments on a large-scale Pinterest dataset.	Power of Human Curation in Recommendation System	NA:NA:NA	2016
Fred Morstatter:Harsh Dani:Justin Sampson:Huan Liu	While social media mining continues to be an active area of research, obtaining data for research is a perennial problem. Even more, obtaining unbiased data is a challenge for researchers who wish to study human behavior, and not technical artifacts induced by the sampling algorithm of a social media site. In this work, we evaluate one social media data outlet that gives data to its users in the form of a stream: Twitter's Sample API. We show that in its current form, this API can be poisoned by bots or spammers who wish to promote their content, jeopardizing the credibility of the data collected through this API. We design a proof-of-concept algorithm that shows how malicious users could increase the probability of their content appearing in the Sample API, thus biasing the content towards spam and bot content and harming the representativity of this data outlet.	Can One Tamper with the Sample API?: Toward Neutralizing Bias from Spam and Bot Content	NA:NA:NA:NA	2016
Eric Nalisnick:Bhaskar Mitra:Nick Craswell:Rich Caruana	This paper investigates the popular neural word embedding method Word2vec as a source of evidence in document ranking. In contrast to NLP applications of word2vec, which tend to use only the input embeddings, we retain both the input and the output embeddings, allowing us to calculate a different word similarity that may be more suitable for document ranking. We map the query words into the input space and the document words into the output space, and compute a relevance score by aggregating the cosine similarities across all the query-document word pairs. We postulate that the proposed Dual Embedding Space Model (DESM) provides evidence that a document is about a query term, in addition to and complementing the traditional term frequency based approach.	Improving Document Ranking with Dual Word Embeddings	NA:NA:NA:NA	2016
Luan Minh Nguyen	In this paper, we propose CaTER, which learns a novel context-aware joint representation of text and user by incorporating semantic text embedding of unlabeled tweets as well as social relation information. CaTER leverages the wealth of user contextual information available apart from user's utterances for sentiment analysis. Our approach is inspired by social science about emotional behaviors of connected users, who perhaps more likely to consensus on similar opinions. Our method outperforms numerous baselines on two real-world Twitter datasets.	Context-Aware Text Representation for Social Relation Aided Sentiment Analysis	NA	2016
Shumpei Okura:Yukihiro Tagami:Akira Tajima	In news recommendation systems, eliminating redundant information is important as well as providing interesting articles for users. We propose a method that quantifies the similarity of articles based on their distributed representation, learned with the category information as weak supervision. This method is useful for evaluation under tight time constraints, since it only requires low-dimensional inner product calculation for estimating similarities. The experimental results from human evaluation and online performance in A/B testing suggest the effectiveness of our proposed method, especially for quantifying middle-level similarities. Currently, this method is used on Yahoo!\ JAPAN's front page, which has millions of users per day and billions of page views per month.	Article De-duplication Using Distributed Representations	NA:NA:NA	2016
Chanyoung Park:Donghyun Kim:Jinoh Oh:Hwanjo Yu	Due to the data sparsity problem, social network information is often additionally used to improve the performance of recommender system. While most existing works exploit social information to reduce the rating prediction error, e.g., RMSE, a few had aimed to improve the top-k ranking prediction accuracy. This paper proposes a novel top-k oriented recommendation method, TRecSo, which incorporates social information into recommendation by modeling two different roles of users as trusters and trustees while considering the structural information of the network. Empirical studies on real-world datasets demonstrate that TRecSo leads to remarkable improvement compared to previous methods in top-k recommendation.	TRecSo: Enhancing Top-k Recommendation With Social Information	NA:NA:NA:NA	2016
Dinesh Pradhan:Tanmoy Chakraborty:Saswata Pandit:Subrata Nandi	Understanding the qualitative patterns of research endeavor of scientific authors in terms of publication count and their impact (citation) is important in order to quantify success trajectories. Here, we examine the career profile of authors in computer science and physics domains and discover at least six different success trajectories in terms of normalized citation count in longitudinal scale. Initial observations of individual trajectories lead us to characterize the authors in each category. We further leverage this trajectory information to build a two-stage stratification model to predict future success of an author at the early stage of her career. Our model outperforms the baseline with an average improvement of 15.68% for both the datasets.	On the Discovery of Success Trajectories of Authors	NA:NA:NA:NA	2016
Vladan Radosavljevic:Mihajlo Grbovic:Nemanja Djuric:Narayan Bhamidipati:Daneo Zhang:Jack Wang:Jiankai Dang:Haiying Huang:Ananth Nagarajan:Peiji Chen	Last decade has witnessed a tremendous expansion of mobile devices, which brought an unprecedented opportunity to reach a large number of mobile users at any point in time. This resulted in a surge of interest of mobile operators and ad publishers to understand usage patterns of mobile apps and allow more relevant content recommendations. Due to a large input space, a critical step in understanding app usage patterns is reducing sparseness by classifying apps into predefined interest taxonomies. However, besides short name and noisy description majority of apps have very limited information available, which makes classification a challenging task. We address this issue and present a novel method to classify apps into interest categories by: 1) embedding apps into low-dimensional space using a neural language model applied on smartphone logs; and 2) applying k-nearest-neighbors classification in the embedding space. To validate the method we run experiments on more than one billion device logs covering hundreds of thousands of apps. To the best of our knowledge this is the first app categorization study at this scale. Empirical results show that the proposed method outperforms the current state-of-the-art.	Smartphone App Categorization for Interest Targeting in Advertising Marketplace	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2016
Jing Ren:Jialie Shen:Robert J. Kauffman	Tens of thousands of music tracks are uploaded to the Internet every day through social networks that focus on music and videos, as well as portal websites. While some of the content has been popular for decades, some tracks that have just been released have been completely ignored. So what makes a music track popular? Can we predict the popularity of a music track before it is released? In this research, we will focus on an online music social network, Last.fm, and investigate three key factors of a music track that may have impact on its popularity. They include: the music content, the artist reputation and the social context of the music. The results suggest that we can predict the future popularity of music with around 80% accuracy using just these three factors. We also found out that in the social networks scenario, the content of the music seems to be an surprisingly important factor that determines the popularity of a track online.	What Makes a Music Track Popular in Online Social Networks?	NA:NA:NA	2016
Surendra Sedhai:Aixin Sun	Presence of spam tweets in a dataset may affect the choices of feature selection, algorithm formulation, and system evaluation for many applications. However, most existing studies have not considered the impact of spam tweets. In this paper, we study the impact of spam tweets on hashtag recommendation for hyperlinked tweets (i.e., tweets containing URLs) in HSpam14 dataset. HSpam14 is a collection of 14 million tweets with annotations of being spam and ham (i.e., non-spam). In our experiments, we observe that it is much easier to recommend "correct" hashtags for spam tweets than ham tweets, because of the near duplicates in spam tweets. Simple approaches like recommending most popular hashtags achieves very good accuracy on spam tweets. On the other hand, features that are highly effective on ham tweets may not be effective on spam tweets. Our findings suggest that without removing spam tweets from the data collection (as in most studies), the results obtained could be misleading for hashtag recommendation tasks.	Effect of Spam on Hashtag Recommendation for Tweets	NA:NA	2016
Wafa Shafqat:Seunghun Lee:Sehrish Malik:Hyun-chul Kim	Crowdfunding sites with recent explosive growth are equally attractive platforms for swindlers or scammers. Though the growing number of articles on crowdfunding scams indicate that the fraud threats are accelerating, there has been little knowledge on the scamming practices and patterns. The key contribution of this research is to discover the hidden clues in the text by exploring linguistic features to distinguish scam campaigns from non-scams. Our results indicate that by providing less information and writing more carefully (and less informally), scammers deliberately try to deceive people; (i) they use less number of words, verbs, and sentences in their campaign pages. (ii) scammers make less typographical errors, 4.5-4.7 times lower than non-scammers.(iii) Expressivity of scams is 2.6-8.5 times lower as well.	The Language of Deceivers: Linguistic Features of Crowdfunding Scams	NA:NA:NA:NA	2016
Baoxu Shi:Tim Weninger	Traditional fact checking by experts and analysts cannot keep pace with the volume of newly created information. It is important and necessary, therefore, to enhance our ability to computationally determine whether some statement of fact is true or false. We view this problem as a link-prediction task in a knowledge graph, and show that a new model of the top discriminative meta paths is able to understand the meaning of some statement and accurately determine its veracity. We evaluate our approach by examining thousands of claims related to history, geography, biology, and politics using public, million node knowledge graphs extracted from Wikipedia and SemMedDB. Not only does our approach significantly outperform related models, we also find that the discriminative path model is easily interpretable and provides sensible reasons for the final determination.	Fact Checking in Heterogeneous Information Networks	NA:NA	2016
Alexander Shishkin:Ekaterina Gladkikh:Aleksandr Vorobev	Implicit user feedback is known to be a strong signal of user preferences in web search. Hence, solving the exploration-exploitation dilemma [5] became an important direction of improvement of ranking algorithms in the last years. In this poster, in the case of commercial queries, we consider a new negative effect of exploration on the user utility -- distracting and confusing users by shifting well-known documents from their common positions -- and propose an approach to take it into account within Multi-Armed Bandit algorithms, usually applied to solve the dilemma.	Selective Exploration of Commercial Documents in Web Search	NA:NA:NA	2016
Rina Singh:Jeffrey A. Graves:Douglas A. Talbert	In recent years, there has been huge growth in the amount of graph data generated from various sources. These types of data are often represented by vertices and edges in a graph with real-valued attributes, topological properties, and temporal information associated with the vertices. Until recently, most pattern mining techniques focus solely on vertex attributes, topological properties, or a combination of these in a static sense; mining attribute and topological changes simultaneously over time has largely been overlooked. In this work-in-progress paper, we propose to extend an existing state-of-the-art technique to mine for patterns in dynamic attributed graphs which appear to trigger changes in attribute values.	Complex Patterns in Dynamic Attributed Graphs	NA:NA:NA	2016
Marcin Skowron:Marko Tkalčič:Bruce Ferwerda:Markus Schedl	Incorporating users' personality traits has shown to be instrumental in many personalized retrieval and recommender systems. Analysis of users' digital traces has become an important resource for inferring personality traits. To date, the analysis of users' explicit and latent characteristics is typically restricted to a single social networking site (SNS). In this work, we propose a novel method that integrates text, image, and users' meta features from two different SNSs: Twitter and Instagram. Our preliminary results indicate that the joint analysis of users' simultaneous activities in two popular SNSs seems to lead to a consistent decrease of the prediction errors for each personality trait.	Fusing Social Media Cues: Personality Prediction from Twitter and Instagram	NA:NA:NA:NA	2016
Sucheta Soundarajan:Acar Tamersoy:Elias B. Khalil:Tina Eliassi-Rad:Duen Horng Chau:Brian Gallagher:Kevin Roundy	We study the problem of determining the proper aggregation granularity for a stream of time-stamped edges. Such streams are used to build time-evolving networks, which are subsequently used to study topics such as network growth. Currently, aggregation lengths are chosen arbitrarily, based on intuition or convenience. We describe ADAGE, which detects the appropriate aggregation intervals from streaming edges and outputs a sequence of structurally mature graphs. We demonstrate the value of ADAGE in automatically finding the appropriate aggregation intervals on edge streams for belief propagation to detect malicious files and machines.	Generating Graph Snapshots from Streaming Edge Data	NA:NA:NA:NA:NA:NA:NA	2016
Ajitesh Srivastava:Charalampos Chelmis:Viktor K. Prasanna	Several applications including community detection in social networks and discovering correlated genes involve finding large subgraphs of high density. We propose the problem of finding the largest subgraph of a given density. The problem is a generalization of the Max-Clique problem which seeks the largest subgraph that has an edge density of 1. We define an objective function and prove that its optimization results in the largest graph of given density. We propose an algorithm that finds the subgraph by running multiple local search heuristics with random restarts. For massive graphs, where running the algorithm directly may be intractable, we use a sampling technique that reduces the graph to a smaller one which is likely to contain large dense subgraphs. We evaluate our algorithm on multiple real life and synthetic datasets. Our experiments show that our algorithm performs as well as the state-of-the-art for finding large subgraphs of high density, while providing density guarantees.	Mining Large Dense Subgraphs	NA:NA:NA	2016
Io Taxidou:Peter M. Fischer:Tom De Nies:Erik Mannens:Rik Van de Walle	This paper sheds light on the different interaction types among social media users that benefit information diffusion and provenance analysis. In particular, we identify explicit and implicit interactions in Twitter, including informal conventions applied by users. In our empirical evaluation considering only retweets, the most common means of information propagation in Twitter, we can infer 50% of message provenance. However, if we consider other types of interactions, we can explain another 13%. Accordingly, we enrich the PROV-SAID model for information diffusion, which extends the W3C PROV standard for provenance.	Information Diffusion and Provenance of Interactions in Twitter: Is it only about Retweets?	NA:NA:NA:NA:NA	2016
Ramine Tinati:Markus Luczak-Roesch:Wendy Hall:Nigel Shadbolt	NA	More than an Edit: Using Transcendental Information Cascades to Capture Hidden Structure in Wikipedia	NA:NA:NA:NA	2016
Fujio Toriumi:Seigo Baba	During a disaster, appropriate information must be collected quickly. For example, residents along the coast require information about tsunamis and those who have lost their houses need information about shelters. Twitter can attract more attention than other forms of mass media under these circumstances because it can quickly provide such information. Since Twitter has an enormous amount of tweets, they must be classified to provide users with the information they need. Previous works on extracting information from Twitter focused on the text data of tweets. However, in some cases, text mining has difficulty extracting information. For example, it might be difficult for text mining to group tweets with URLs. On the other hand, by assuming that users who retweet the same tweet are interested in the same topic, we can classify tweets that are required by users with similar interests based on retweets. Thus, we employ the tweet classification method that focuses on retweets. In this paper, we demonstrated that our method works quickly in disaster situations and that it can quickly classify the required information based on the needs in disaster situations and is helpful for collecting information under them.	Real-time Tweet Classification in Disaster Situation	NA:NA	2016
Herbert Van de Sompel:Martin Klein:Shawn M. Jones	We quantify the extent to which references to papers in scholarly literature use persistent HTTP URIs that leverage the Digital Object Identifier infrastructure. We find a significant number of references that do not, speculate why authors would use brittle URIs when persistent ones are available, and propose an approach to alleviate the problem.	Persistent URIs Must Be Used To Be Persistent	NA:NA:NA	2016
Bo Wang:Yanshu Yu:Peng Zhang	Instead of studying the properties of social relationship from an objective view, in this paper, we focus on the two individuals' subjective and asymmetric opinions on their interrelationships. The sociolinguistics theories propose to characterize the individuals' opinions of their interrelationship with interactive language features. With this inspiration, we investigate the subjective asymmetry of the interrelationship with the asymmetry of the interactive language features including the frequency, quantity, quality and emotion. Experimental results with Enron email corpus provide suggestive evidences and thus reveal that the pair-wise language styles on an interrelationship are asymmetric, and this asymmetry can be a joint effect of the individuals' opinions of the interrelationship and their personal language habits. The results also indicate that the degree of the asymmetry could be related to the individuals' personality traits.	Investigation of the Subjective Asymmetry of Social Interrelationship with Interactive Language	NA:NA:NA	2016
Chengyu Wang:Rong Zhang:Xiaofeng He:Aoying Zhou	While most of the entity ranking research focuses on Web corpora with user queries as input, little has been done to rank entities directly from documents. We propose a ranking algorithm NERank to address this issue. NERank employs a random walk process on a weighted tripartite graph mined from the document collection. We evaluate NERank over real-life document datasets and compare it with baselines. Experimental results show the effectiveness of our method.	NERank: Ranking Named Entities in Document Collections	NA:NA:NA:NA	2016
Dongjing Wang:Shuiguang Deng:Songguo Liu:Guandong Xu	In this paper, a music recommendation approach based on distributed representation is presented. The proposed approach firstly learns the distributed representations of music pieces and acquires users' preferences from listening records. Then, it recommends appropriate music pieces whose distributed representations are in accordance with target users' preferences. Experiments on a real world dataset demonstrate that the proposed approach outperforms the state-of-the-art methods.	Improving Music Recommendation Using Distributed Representation	NA:NA:NA:NA	2016
Chao-Yuan Wu:Alex Beutel:Amr Ahmed:Alexander J. Smola	Understanding a user's motivations provides valuable information beyond the ability to recommend items. Quite often this can be accomplished by perusing both ratings and review texts. Unfortunately matrix factorization approaches to recommendation result in large, complex models that are difficult to interpret. In this paper, we attack this problem through succinct additive co-clustering on both ratings and reviews. Our model yields accurate and interpretable recommendations.	Explaining Reviews and Ratings with PACO: Poisson Additive Co-Clustering	NA:NA:NA:NA	2016
Kohei Yamamoto:Hayato Kobayashi:Yukihiro Tagami:Hideki Nakayama	News article recommendation has the key problem of needing to eliminate the redundant information in a ranked list in order to provide more relevant information within a limited time and space. In this study, we tackle this problem by using image thumbnailing, which can be regarded as the summarization of news images. We propose a multimodal image thumbnailing method considering news text as well as images themselves. We evaluate this approach on a real data set based on news articles that appeared on Yahoo! JAPAN. Experimental results demonstrate the effectiveness of our proposed method.	Multimodal Content-Aware Image Thumbnailing	NA:NA:NA:NA	2016
Tomoya Yamazaki:Nobuyuki Shimizu:Hayato Kobayashi:Satoshi Yamauchi	We propose a simple and scalable method for soft community detection that makes use of both graph structures and vertex attributes. Our method is based on micro-clustering, which is a scalable and efficient clique-based method for detecting overlapping communities in unweighted graphs. We extend this method to graphs with vertex attributes so that we can make use of information supplied by vertex attributes. Our method still requires the same time complexity as micro-clustering. We confirm the validity and efficiency of our method by applying it to a large-scale co-purchasing network of real online auction data.	Weighted Micro-Clustering: Application to Community Detection in Large-Scale Co-Purchasing Networks with User Attributes	NA:NA:NA:NA	2016
Jinyoung Yeo:Sungchul Kim:Eunyee Koh:Seung-won Hwang:Nedim Lipka	This paper covers a sales forecasting problem on e-commerce sites. To predict product sales, we need to understand customers' browsing behavior and identify whether it is for purchase purpose or not. For this goal, we propose a new customer model, B2P, of aggregating predictive features extracted from customers' browsing history. We perform experiments on a real world e-commerce site and show that sales predictions by our model are consistently more accurate than those by existing state-of-the-art baselines.	Browsing2purchase: Online Customer Model for Sales Forecasting in an E-Commerce Site	NA:NA:NA:NA:NA	2016
Li'ang Yin:Jianhua Han:Yong Yu	Label aggregation is one of the key topics in crowdsourcing research. Most researchers make their efforts in modeling ability of users and difficulty of instances. In this paper, we consider label aggregation from the view of grouping instances. We assume instances are sampled from latent groups and they share the same true label with their corresponding groups. We construct a graphical model named InGroup(Instance Grouping model) to infer latent group assignment as well as true labels. The experimental results show the advantages of our model compared with baselines.	Label Aggregation with Instance Grouping Model	NA:NA:NA	2016
Haochao Ying:Liang Chen:Yuwen Xiong:Jian Wu	Point-of-interest (POI) recommendation has become more and more important, since it could discover user behavior pattern and find interesting venues for them. To address this problem, we propose a rank-based method, PGRank, which integrates user geographical preference and latent preference into Bayesian personalized ranking framework. The experimental results on a real dataset show its effective.	PGRank: Personalized Geographical Ranking for Point-of-Interest Recommendation	NA:NA:NA:NA	2016
Takeru Yokoi:Masato Fukuchi:Michihiro Kobayakawa:Roliana Ibrahim:Ali Selamat	News articles are a type of instantaneous and regional media, and provide the daily concerns of its publication area. This work proposes an analytical framework of the relations among nations using the similarities in the content of news articles published in different nations. Our key idea is that those relations exist in the news articles miss-classified by different nations from their original publication area. In order to clearly illustrate those relations, the classification results are visualized as bar graphs. We also carried out some experiments for the proposed framework using a small collection of news articles.	Analytical Framework of Relations among Nations using News Articles	NA:NA:NA:NA:NA	2016
Shuangfei Zhai:Keng-hao Chang:Ruofei Zhang:Zhongfei Zhang	We investigate the use of recurrent neural networks (RNNs) in the context of online advertising, where we use RNNs to map both query and ads to real valued vectors. In addition, we propose an attention network that assigns scores to different word locations according to their intent importance. The vector output is computed by a weighted sum of the vectors at each word. We perform end-to-end training of both the RNN and attention network under the guidance of user click logs. We show that the attention network improves the quality of learned vector representations evaluated by AUC on a manually labeled dataset. Moreover, we show that keywords extracted according to the attention scores are easy to interpret and significantly outperform the state-of-the-art query intent extraction methods.	Attention Based Recurrent Neural Networks for Online Advertising	NA:NA:NA:NA	2016
Zhongyi Zhai:Bo Cheng:Zhaoning Wang:Xuan Liu:Meng Liu:Junliang Chen	In this paper, we present an ecosystem for mobile application development by end-users. An advantage of this ecosystem is that the graphical user interface (GUI), as well as the application logic, can both be developed in a rapid and simple way. This ecosystem is mainly implemented through development and integration of two sub-systems, namely EasyApp and LSCE. EasyApp is responsible for developing the mobile app with the compatibility of multiple mobile platforms, while LSCE is in charge of creating the service process that can be invoked by mobile app directly. A case study is presented to illustrate the development process using this ecosystem.	Design and Implementation: the End User Development Ecosystem for Cross-platform Mobile Applications	NA:NA:NA:NA:NA:NA	2016
Shanshan Zhang:Slobodan Vucetic	This paper describes a case study of sampling bias in LinkedIn, a major professional social network. The study collected a sample of 1,989 STEM students who graduated from a major public university between 2002 and 2014. Overall, 40\% of the graduates had a LinkedIn profile in summer of 2015. It was observed that LinkedIn participation significantly fluctuated among different majors, and ranged from 30\% for biochemistry majors to 51\% for information science majors. Year of graduation, gender, and grade point average surprisingly did not seem to create a large difference in LinkedIn participation. These results should be useful for design and interpretation of empirical studies which use LinkedIn data or select participants from LinkedIn social network.	Sampling Bias in LinkedIn: A Case Study	NA:NA	2016
Matteo Zignani:Sabrina Gaito:Gian Paolo Rossi	Measurements of online social networks (OSNs) support the common fact that not all links carry the same social value, and that the strength of each link is strictly related to the frequency of interactions between the connected users. In this paper, we investigate the predictability of the interactions on OSN links by wondering if it is possible to categorize interactive or non-interactive links at their creation time. We turn the problem into a binary classification task and introduce a set of features which leverage the temporal and topological properties of the social and interaction networks, without requiring the knowledge of the interaction history of the link. The best classifier trained on a Facebook dataset obtained 0.72 as AUC. The above performance suggests that we can distinguish between interactive/non-interactive links at the time of link creation.	Predicting the Link Strength of "Newborn" Links	NA:NA:NA	2016
Alfredo Cuzzocrea:Abdulmotaleb El Saddik	It is our great pleasure to welcome you to the Demo Track of WWW 2016, The 25th International World Wide Web Conference, held in Montreal, Canada, during April 11-15, 2016. The WWW 2016 Demo Track, like in the tradition of WWW Demo conference series, allows researchers and practitioners to demonstrate new systems in a dedicated session. Demo contributions are based on an implemented and tested system that pursues one or more innovative ideas in the interest areas of Web data and information management, Web search, Web intelligence tools, Web mining, social network applications and so forth. Topics of interest for the 2016 edition's conference include (but are not limited to) the following ones: Behavioral Analysis and Personalization Big Data on the Web Crowdsourcing Systems and Social Media Content Analysis Graph Data Management and Mining High-Performance Infrastructures for Data- Intensive Web Tasks Internet Economics and Monetization Pervasive Web and Mobility Security and Privacy Semantic Web Social Networks and Graph Analysis Web Information Retrieval Web Infrastructure: Datacenters, Content Delivery Networks, and Cloud Computing Web Mining Web Science Web Search Systems and Applications Demo contributions come from academic researchers, industrial practitioners with prototypes or inproduction deployments, as well as from any W3C-related activities. All have in common to show innovative use of Web-based techniques. The WWW 2016 Demo Track call for papers attracted 65 submissions from all over the world (USA, North America, South America, Europe, Australia, Asia, Africa). The program committee reviewed and accepted a very selected collection of 29 papers, and the final statistics is the following: WWW 2016 Demo Track Statistics Number of Submitted Papers 65 Number of Accepted Papers 29.	Session details: Demonstrations	NA:NA	2016
Marie Al-Ghossein:Talel Abdessalem	This demo presents SoMap, a web-based platform that provides new scalable methods to aggregate, analyse and valorise large collections of heterogeneous social data in urban contexts. The platform relies on geotagged data extracted from social networks and microblogging applications such as Instagram, Flickr and Twitter and on Points Of Interest gathered from OpenStreetMap. It could be very insightful and interesting for data scientists and decision-makers. SoMap enables dynamic clustering of filtered social data in order to display it on a map in a combined form. The key components of this platform are the clustering module, which relies on a scalable algorithm described in this paper, and the ranking algorithm that combines the popularity of the posts, their location and their link to the points of interest found in the neighbourhood. The system further detects mobility patterns by identifying and aggregating trajectories for all the users. SoMap will be demonstrated through several examples that highlight all of its functionalities and reveal its effectiveness and usefulness.	SoMap: Dynamic Clustering and Ranking of Geotagged Posts	NA:NA	2016
Ana Paula Appel:Heloisa Candello:Beatriz S.R. de Souza:Bruna D. Andrade	In this work we present Destiny a cognitive mobile guide for Olympics games in Brazil that identifies user characteristics to deliver content. It will help visitors, athletes, and athletes' parents to localize themselves and receive tailored historical, cultural and entertainment information at a particular point of interest (POI). The application will recommend places to go and show POI contents according to user context based on several attributes such as personality (closeness, curiosity, adventurous), nationality, favorite places already visited, trip length, price range and favorite sports	Destiny: A Cognitive Mobile Guide for the Olympics	NA:NA:NA:NA	2016
Mouhamadou Lamine Ba:Laure Berti-Equille:Kushal Shah:Hossam M. Hammady	Social networks and the Web in general are characterized by multiple information sources often claiming conflicting data values. Data veracity is hard to estimate, especially when there is no prior knowledge about the sources or the claims and in time-dependent scenarios where initially very few observers can report first information. Despite the wide set of recently proposed truth discovery approaches, "no-one-fits-all" solution emerges for estimating the veracity of on-line information in open contexts. However, analyzing the space of conflicting information and disagreeing sources might be relevant, as well as ensembling multiple truth discovery methods. This demonstration presents VERA, a Web-based platform that supports information extraction from Web textual data and micro-texts from Twitter and estimates data veracity. Given a user query, VERA systematically extracts entities and relations from Web content, structures them as claims relevant to the query and gathers more conflicting/corroborating information. VERA combines multiple truth discovery algorithms through ensembling returns the veracity label and score of each data value and the trustworthiness scores of the sources. VERA will be demonstrated through several real-world scenarios to show its potential value for fact-checking from Web data.	VERA: A Platform for Veracity Estimation over Web Data	NA:NA:NA:NA	2016
Ciro Baron Neto:Kay Müller:Martin Brümmer:Dimitris Kontokostas:Sebastian Hellmann	The Linked Open Data (LOD) cloud is in danger of becoming a black box. Simple questions such as "What kind of datasets are in the LOD cloud?", "In what way(s) are these datasets connected?" -- albeit frequently asked -- are at the moment still difficult to answer due to the lack of proper tooling support. The infrequent update of the static LOD cloud diagram adds to the current dilemma, since there is neither reliable nor timely-updated information to perform an interactive search, analysis or in particular visualization in order to gain insight into the current state of Linked Open Data. In this paper, we propose a new hybrid system which combines LOD Visualisation, Analytics and DiscovERy (LODVader) to aid in answering the above questions. LODVader is equipped with (1) a multi-layer LOD cloud visualization component comprising datasets, subsets and vocabularies, (2) dataset analysis components that extend the state of the art with new similarity measures and efficient link extracting techniques and (3) a fast search index that is an entry point for dataset discovery. At its core, LODVader employs a timely-updated index using a complex cluster of Bloom filters as a fast search index with low memory footprint. This BF cluster is able to efficiently perform analysis on link and dataset similarities based on stored predicate and object information, which -- once inverted -- can be employed to discover invalid links by displaying the Dark LOD Cloud. By combining all these features, we allow for an up-to-date, multi-dimensional LOD cloud analysis, which -- to the best of our knowledge -- was not possible before.	LODVader: An Interface to LOD Visualization, Analyticsand DiscovERy in Real-time	NA:NA:NA:NA:NA	2016
Emre Çelikten:Géraud Le Falher:Michael Mathioudakis	We demonstrate Geotopics, a system to explore geographical patterns of urban activity. The system collects publicly shared check-ins generated by Foursquare users, that reveal who spends time where, when, and on what type of activity. It then employs sparse probabilistic modeling techniques to learn associations between different regions of a city and multi-feature descriptions of urban activity. Through a web interface, users of the system can select a city of interest and explore visualizations that highlight how different types of activity are spatially and temporally distributed in the city. We discuss the opportunities that web data offer to understand urban activity and the challenges one faces in that task. We then describe our approach and the architecture of Geotopics. Finally, we lay out the demonstration scenario.	"What Is the City but the People?": Exploring Urban Activity Using Social Web Traces	NA:NA:NA	2016
Diego Collarana:Christoph Lange:Sören Auer	The increasing amount of structured and semi-structured information available on the Web and in distributed information systems, as well as the Web's diversification into different segments such as the Social Web, the Deep Web, or the Dark Web, requires new methods for horizontal search. FuhSen is a federated, RDF-based, hybrid search platform that searches, integrates and summarizes information about entities from distributed heterogeneous information sources using Linked Data. As a use case, we present scenarios where law enforcement institutions search and integrate data spread across these different Web segments to identify cases of organized crime. We present the architecture and implementation of FuhSen and explain the queries that can be addressed with this new approach.	FuhSen: A Platform for Federated, RDF-based Hybrid Search	NA:NA:NA	2016
Achille Fokoue:Oktie Hassanzadeh:Mohammad Sadoghi:Ping Zhang	Drug-Drug Interactions (DDIs) are a major cause of preventable adverse drug reactions and a huge burden on public health and the healthcare system. On the other hand, there is a large amount of drug-related (open) data published on the Web, describing various properties of drugs and their relationships to other drugs, genes, diseases, and related concepts and entities. In this demonstration, we describe an end-to-end system we have designed to take in various Web data sources as input and provide as output a prediction of DDIs along with an explanation of why two drugs may interact. The system first creates a knowledge graph out of input data sources through large-scale semantic integration, and then performs link prediction among drug entities in the graph through large-scale similarity analysis and machine learning. The link prediction is performed using a logistic regression model over several similarity matrices built using different drug similarity measures. We present both the efficient link prediction framework implemented in Apache Spark, and our APIs and Web interface for predicting DDIs and exploring their potential causes and nature.	Predicting Drug-Drug Interactions Through Similarity-Based Link Prediction Over Web Data	NA:NA:NA:NA	2016
Michele M. Franceschini:Livio B. Soares:Luis A. Lastras Montaño	Watson Concept Insights (WCI) is a service that was recently made publicly available by IBM. WCI provides an information retrieval framework that is designed to facilitate search and exploration of text documents, and is particularly effective on sparse data sets. Its methodology consists of first defining a dictionary of concepts which are interconnected in a concept graph and then modeling a document by predicting its relevance to any given concept in the concept graph using the concepts that are directly mentioned in the document itself. This technique in effect increases the document recall for any given query, even for very sparse data sets, exposing the user to a variety of connections between their query and a data set of interest.	Watson Concept Insights: A Conceptual Association Framework	NA:NA:NA	2016
Andrea Gallidabino:Cesare Pautasso	We are heading toward an era in which users own more than one single Web-enabled device. These devices range from smart phones, tablets and personal computers to smart Web-enabled devices found in houses and cars. The access mechanisms and usage patterns of Web applications are changing accordingly, as users interact more and more with Web applications through all their devices, even if the majority of Web applications are not ready to offer a good user experience taking full advantage of multiple devices. In this demonstration we introduce Liquid.js, a framework whose goal is to enable Web developers to take advantage of multiple heterogeneous devices and offer to their users a liquid user experience, whereby any device can be used sequentially or concurrently with Web applications that can effortlessly roam from one device to another. This way, as highlighted in the demonstration users do not need to stop and resume their work on their Web application as they migrate and clone them across different devices. The demo will also show how developers can easily add such liquid behavior to any Polymer Web component.	The Liquid.js Framework for Migrating and Cloning Stateful Web Components across Multiple Devices	NA:NA	2016
Stefan Hagedorn:Kai-Uwe Sattler	Data analytics has gained more and more focus during recent years and many data processing platforms have been developed. They all provide a powerful but often complex API that users have to learn. Furthermore, results can only be stored or printed, without any possibility for visualization. In this paper we present Piglet, a compiler for the high-level Pig Latin script language that generates code for various platforms like Spark, Flink, Storm, and PipeFabric. Piglet lets users write elegant code with extensions for SPARQL and RDF, as well as support for streaming data. An integration into the notebook-based frontend Zeppelin provides a homogeneous and interactive user interface for exploring, analyzing, and visualizing data from different sources and lets users share their scripts and results.	Piglet: Interactive and Platform Transparent Analytics for RDF & Dynamic Data	NA:NA	2016
Aisha Hasan:Mohammad Hammoud:Reza Nouri:Sherif Sakr	RDF and SPARQL query language are gaining wide popularity and acceptance. This demonstration paper presents DREAM, a hybrid RDF system, which combines the advantages and averts the disadvantages of the centralized and distributed RDF schemes. In particular, DREAM avoids partitioning RDF datasets and reversely partitions SPARQL queries. By not partitioning datasets, DREAM offers a general paradigm for different types of pattern matching queries and entirely precludes intermediate data shuffling (only auxiliary data are shuffled). By partitioning only queries, DREAM suggests an adaptive scheme, which runs queries on different numbers of machines depending on their complexities. DREAM achieves these goals and significantly outperforms related systems via employing a novel graph-based, rule-oriented query planner and a new cost model. This paper proposes demonstrating DREAM live over the cloud using a friendly graphical user interface (GUI). The GUI allows participants to execute and visualize pre-defined and user-defined (which can be written by participants on-the-fly) SPARQL queries over various real-world and synthetic RDF datasets. Furthermore, participants can empirically compare and contrast DREAM against three state-of-the-art RDF systems.	DREAM in Action: A Distributed and Adaptive RDF System on the Cloud	NA:NA:NA:NA	2016
Viet Ha-Thuc:Ye Xu:Satya Pradeep Kanduri:Xianren Wu:Vijay Dialani:Yan Yan:Abhishek Gupta:Shakti Sinha	One key challenge in talent search is how to translate complex criteria of a hiring position into a search query. This typically requires deep knowledge on which skills are typically needed for the position, what are their alternatives, which companies are likely to have such candidates, etc. However, listing examples of suitable candidates for a given position is a relatively easy job. Therefore, in order to help searchers overcome this challenge, we design a next generation of talent search paradigm at LinkedIn: Search by Ideal Candidates. This new system only needs the searcher to input one or several examples of suitable candidates for the position. The system will generate a query based on the input candidates and then retrieve and rank results based on the query as well as the input candidates. The query is also shown to the searcher to make the system transparent and to allow the searcher to interact with it. As the searcher modifies the initial query and makes it deviate from the ideal candidates, the search ranking function dynamically adjusts an refreshes the ranking results balancing between the roles of query and ideal candidates. As of writing this paper, the new system is being launched to our customers.	Search by Ideal Candidates: Next Generation of Talent Search at LinkedIn	NA:NA:NA:NA:NA:NA:NA:NA	2016
Ruining He:Chunbin Lin:Julian McAuley	To build a fashion recommendation system, we need to help users retrieve fashionable items that are visually similar to a particular query, for reasons ranging from searching alternatives (i.e., substitutes), to generating stylish outfits that are visually consistent, among other applications. In domains like clothing and accessories, such considerations are particularly paramount as the visual appearance of items is a critical feature that guides users' decisions. However, existing systems like Amazon and eBay still rely mainly on keyword search and recommending loosely consistent items (e.g. based on co-purchasing or browsing data), without an interface that makes use of visual information to serve the above needs. In this paper, we attempt to fill this gap by designing and implementing an image-based query system, called Fashionista, which provides a graphical interface to help users efficiently explore those items that are not only visually similar to a given query, but which are also fashionable, as determined by visually-aware recommendation approaches. Methodologically, Fashionista learns a low-dimensional visual space as well as the evolution of fashion trends from large corpora of binary feedback data such as purchase histories of Women's Clothing & Accessories from Amazon, which we use for this demonstration.	Fashionista: A Fashion-aware Graphical System for Exploring Visually Similar Items	NA:NA:NA	2016
Johannes Hoffart:Dragan Milchevski:Gerhard Weikum:Avishek Anand:Jaspreet Singh	Entity search over news, social media and the Web allows users to precisely retrieve concise information about specific people, organizations, movies and their characters, and other kinds of entities. This expressive search mode builds on two major assets: 1) a knowledge base (KB) that contains the entities of interest and 2) entity markup in the documents of interest derived by automatic disambiguation of entity names (NED) and linking names to the KB. These prerequisites are not easily available, though, in the important case when a user is interested in a newly emerging entity (EE) such as new movies, new songs, etc. Automatic methods for detecting and canonicalizing EEs are not nearly at the same level as the NED methods for prominent entities that have rich descriptions in the KB. To overcome this major limitation, we have developed an approach and prototype system that allows searching for EEs in a user-friendly manner. The approach leverages the human in the loop by prompting for user feedback on candidate entities and on characteristic keyphrases for EEs. For convenience and low burden on users, this process is supported by the automatic harvesting oftentative keyphrases. Our demo system shows this interactive process and its high usability.	The Knowledge Awakens: Keeping Knowledge Bases Fresh with Emerging Entities	NA:NA:NA:NA:NA	2016
Helge Holzmann:Avishek Anand	Limited search and access patterns over Web archives have been well documented. One of the key reasons is the lack of understanding of the user access patterns over such collections, which in turn is attributed to the lack of effective search interfaces. Current search interfaces for Web archives are (a) either purely navigational or (b) have sub-optimal search experience due to ineffective retrieval models or query modeling. We identify that external longitudinal resources, such as social bookmarking data, are crucial sources to identify important and popular websites in the past. To this extent we present Tempas, a tag-based temporal search engine for Web archives. Websites are posted at specific times of interest on several external platforms, such as bookmarking sites like Delicious. Attached tags not only act as relevant descriptors useful for retrieval, but also encode the time of relevance. With Tempas we tackle the challenge of temporally searching a Web archive by indexing tags and time. We allow temporal selections for search terms, rank documents based on their popularity and also provide meaningful query recommendations by exploiting tag-tag and tag-document co-occurrence statistics in arbitrary time windows. Finally, Tempas operates as a fairly non-invasive indexing framework. By not dealing with contents from the actual Web archive it constitutes an attractive and low-overhead approach for quick access into Web archives.	Tempas: Temporal Archive Search Based on Tags	NA:NA	2016
Salman Hooshmand:Akib Mahmud:Gregor V. Bochmann:Muhammad Faheem:Guy-Vincent Jourdan:Russ Couturier:Iosif-Viorel Onut	We present D-ForenRIA, a distributed forensic tool to automatically reconstruct user-sessions in Rich Internet Applications (RIAs), using solely the full HTTP traces of the sessions as input. D-ForenRIA recovers automatically each browser state, reconstructs the DOMs and re-creates screenshots of what was displayed to the user. The tool also recovers every action taken by the user on each state, including the user-input data. Our application domain is security forensics, where sometimes months-old sessions must be quickly reconstructed for immediate inspection. We will demonstrate our tool on a series of RIAs, including a vulnerable banking application created by IBM Security for testing purposes. In that case study, the attacker visits the vulnerable web site, and exploits several vulnerabilities (SQL-injections, XSS...) to gain access to private information and to perform unauthorized transactions. D-ForenRIA can reconstruct the session, including screenshots of all pages seen by the hacker, DOM of each page and the steps taken for unauthorized login and the inputs hacker exploited for the SQL-injection attack. D-ForenRIA is made efficient by applying advanced reconstruction techniques and by using several browsers concurrently to speed up the reconstruction process. Although we developed D-ForenRIA in the context of security forensics, the tool can also be useful in other contexts such as aided RIAs debugging and automated RIAs scanning.	D-ForenRIA: Distributed Reconstruction of User-Interactions for Rich Internet Applications	NA:NA:NA:NA:NA:NA:NA	2016
Thomas Kowark:Keven Richly:Matthias Uflacker:Hasso Plattner	Ontology matching enables applications, such as automated data transformation or query rewriting. As it requires domain knowledge, it needs to be carried out by expert users, whose time is scarce and, therefore, should be used efficiently. To this end, the RepMine system presented in this paper does not treat ontology matching as a task of its own, but integrates it into a semi-automated query translation process. By that, users perform a task with immediate benefit for them and simultaneously contribute to alignments between ontologies. Furthermore, the overall task of matching two ontologies is split on a per-query basis and, thus, can be performed incrementally by all system users	Incremental, Per-Query Ontology Matching with RepMine	NA:NA:NA:NA	2016
Jukyoung Lee:Yonghwa Choi:Suhkyung Kim:Seongsoon Kim:Jaewoo Kang	Many people seek majority opinions by searching for question-answers that are uploaded by others or uploading their own questions on social media sites. However, people have to read through a large number of documents returned by search services to find the majority opinions. Moreover, even when users upload questions on social media sites, they cannot immediately obtain answers. To address these problems, we present Searching Majority Opinions System (SEMO), a novel majority opinion-based search system that uses QA threads uploaded on SNS and cQA websites. SEMO returns entities based on majority opinions for opinion-finding queries in real time. We also tackled a data sparsity problem using a novel query component expansion approach. To prove SEMO's usefulness in finding majority opinions, we implemented a prototype of SEMO for the movie domain. We believe that our method can cause a paradigm shift in opinion-finding query search and help people make decisions. SEMO is available at http://semo.korea.ac.kr/	SEMO: Searching Majority Opinions on Movies using SNS QA Threads	NA:NA:NA:NA:NA	2016
Essam Mansour:Andrei Vlad Sambra:Sandro Hawke:Maged Zereba:Sarven Capadisli:Abdurrahman Ghanem:Ashraf Aboulnaga:Tim Berners-Lee	Solid is a decentralized platform for social Web applications. In the Solid platform, users' data is managed independently of the applications that create and consume this data. Each user stores their data in a Web-accessible personal online datastore (or pod). Each user can have one or more pods from different pod providers, and can easily switch between providers. Applications access data in users' pods using well defined protocols, and a decentralized authentication and access control mechanism guarantees the privacy of the data. In this decentralized architecture, applications can operate on users' data wherever it is stored. Users control access to their data, and have the option to switch between applications at any time. We will demonstrate the utility of Solid and how it is experienced from the point of view of end users and application developers. For this, we will use a set of Solid servers and multiple Web applications that use these servers. We believe that experience with a concrete platform such as Solid is highly valuable in truly appreciating the power of a decentralized social Web.	A Demonstration of the Solid Platform for Social Web Applications	NA:NA:NA:NA:NA:NA:NA:NA	2016
Luis G. Moyano:Ana Paula Appel:Vagner F. de Santana:Marcia Ito:Thiago D. dos Santos	Healthcare insurance data represent a rich source of information and has the potential to contribute significantly in guiding business decision making. In this work we present GraPhys, a Graph Analysis platform designed for exploration, visualization and analysis of healthcare insurance data and its corresponding metadata. By taking advantage of relationships contained in healthcare claims data, we are able to apply Graph Analytics methods and algorithms in order to devise useful business metrics to guide data analysis and exploration. Our tool focuses in better understanding physicians, patients and their practices. We illustrate our approach by demonstrating two use cases where we show how graph analytics metrics, combined with other data, may lead to useful insights not directly available to traditional Business Analytics.	GraPhys: Understanding Health Care Insurance Data through Graph Analytics.	NA:NA:NA:NA:NA	2016
Brice Nédelec:Pascal Molli:Achour Mostefaoui	Real-time collaborative editors are common tools for distributing work across space, time, and organizations. Unfortunately, mainstream editors such as Google Docs rely on central servers and raise privacy and scalability issues. CRATE is a real-time decentralized collaborative editor that runs directly in web browsers thanks to WebRTC. Compared to state-of-the-art, CRATE is the first real-time editor that only requires browsers in order to support collaborative editing and to transparently handle from small to large groups of users. Consequently, CRATE can also be used in massive online lectures, TV shows or large conferences to allow users to share their notes. CRATE's properties rely on two scientific results: (i) a replicated sequence structure with sub-linear upper bound on space complexity; this prevents the editor from running costly distributed garbage collectors, (ii) an adaptive peer sampling protocol; this prevent the editor from oversizing routing tables, hence from letting small networks pay the price of large networks. This paper describes CRATE, its properties and its usage.	CRATE: Writing Stories Together with our Browsers	NA:NA:NA	2016
Giuseppe Pirrò:Alfredo Cuzzocrea	We describe RECAP, a tool that, given a pair of entities defined in some Knowledge Graph (KG), builds an explanation, that is, a graph (of manageable size) reflecting their relatedness. Explanations enable to discover new knowledge and browse toward other entities of interest. We discuss different kinds of explanations based on information theory and diversity. The KG-agnostic approach adopted by RECAP, which retrieves the necessary information via SPARQL queries, makes it readily usable on a variety of KGs.	RECAP: Building Relatedness Explanations on the Web	NA:NA	2016
Vinodkumar Prabhakaran:MIchael Saltzman:Owen Rambow	We present the SPIN system, a computational tool to detect linguistic and dialog structure patterns in a social interaction that reveal the underlying power relations between its participants. The SPIN system labels sentences in an interaction with their dialog acts (i.e., communicative intents), detects instances of overt display of power, and predicts social power relations between its participants. We also describe a Google Chrome browser extension, namely gSPIN, to illustrate an exciting use-case of the SPIN system, which will be demonstrated at the demo session during the conference.	How Powerful are You?: gSPIN: Bringing Power Analysis to Your Finger Tips	NA:NA:NA	2016
Christopher Riederer:Daniel Echickson:Stephanie Huang:Augustin Chaintreau	The ubiquitous availability of location data to smartphone apps and online social networks has caused the collection of such information to grow at an unprecedented rate. However, the discriminative power and potential uses of this data collection is not always clear to the end user. In this work, we present FindYou, a web-based application that gives users the ability to perform a location data privacy audit. FindYou lets users import and visualize the location data collected by popular web services in order to understand what these companies know or can easily infer about them. Additionally, FindYou gives users the option to donate their data to the scientific community, creating new mobile datasets linked to user properties that will be open to use by academic institutions. We hope that FindYou will increase awareness of the privacy issues surrounding the collection and use of location data, the potential problem of "digital red-lining", and also create valuable new datasets with the full informed consent of interested users.	FindYou: A Personal Location Privacy Auditing Tool	NA:NA:NA:NA	2016
Martin Saveski:Eric Chu:Soroush Vosoughi:Deb Roy	Most social network analyses focus on online social networks. While these networks encode important aspects of our lives they fail to capture many real-world social connections. Most of these connections are, in fact, public and known to the members of the community. Mapping them is a task very suitable for crowdsourcing: it is easily broken down in many simple and independent subtasks. Due to the nature of social networks-presence of highly connected nodes and tightly knit groups-if we allow users to map their immediate connections and the connections between them, we will need few participants to map most connections within a community. To this end, we built the Human Atlas, a web-based tool for mapping social networks. To test it, we partially mapped the social network of the MIT Media Lab. We ran a user study and invited members of the community to use the tool. In 4.6 man-hours, 22 participants mapped 984 connections within the lab, demonstrating the potential of the tool.	Human Atlas: A Tool for Mapping Social Networks	NA:NA:NA:NA	2016
Stephan Seufert:Patrick Ernst:Srikanta J. Bedathur:Sarath Kumar Kondreddi:Klaus Berberich:Gerhard Weikum	We demonstrate InstantEspresso, a system to explain the relationship between two sets of entities in knowledge graphs. Instant-Espresso answers questions of the form. Which European politicians are related to politicians in the United States, and how? or How can one summarize the relationship between China and countries from the Middle East? Each question is specified by two sets of query entities. These sets (e.g. European politicians or United States politicians) can be determined by an initial graph query over a knowledge graph capturing relationships between real-world entities. Instant-Espresso analyzes the (indirect) relationships that connect entities from both sets and provides a user-friendly explanation of the answer in the form of concise subgraphs. These so-called relatedness cores correspond to important event complexes involving entities from the two sets. Our system provides a user interface for the specification of entity sets and displays a visually appealing visualization of the extracted subgraph to the user. The demonstrated system can be used to provide background information on the current state-of-affairs between real-world entities such as politicians, organizations, and the like, e.g. to a journalist preparing an article involving the entities of interest. InstantEspresso is available for an online demonstration at the URL http://espresso.mpi-inf.mpg.de/.	Instant Espresso: Interactive Analysis of Relationships in Knowledge Graphs	NA:NA:NA:NA:NA:NA	2016
Kaisong Song:Ling Chen:Wei Gao:Shi Feng:Daling Wang:Chengqi Zhang	Microblogging services are playing increasingly important roles in our daily life today. It is useful for microblog users to instantly understand the sentiment of a large number of microblogs posted by their friends and make appropriate response. Despite considerable progress on microblog sentiment classification, most of the existing works ignore the influence of personal distinctions of different microblog users on the sentiments they convey, and none of them has provided real-world personalized sentiment classification systems. Considering personal distinctions in sentiment analysis is natural and necessary as different people have different language habits, personal characters, opinion bias and so on. In this demonstration, we present a live system based on Twitter called PerSentiment, an individuality-dependent sentiment classification system which makes the first attempt to analyze the personalized sentiment of recent tweets and retweets posted by the authenticated user and the users he/she follows. Our system consists of four steps, i.e., requesting tweets via Twitter API, preprocessing collected tweets for extracting features, building personalized sentiment classifier based on a novel and extensible Latent Factor Model (LFM) trained on emoticon-tagged tweets, and finally visualizing the sentiment of friends' tweets to provide a guide for better sentiment understanding.	PerSentiment: A Personalized Sentiment Classification System for Microblog Users	NA:NA:NA:NA:NA:NA	2016
Mehdi Terdjimi:Lionel Médini:Michael Mrissa	Web applications that rely on datasets of limited sizes to handle small but frequent updates and numerous queries have no simple way to define where data should be stored and processed. We propose a reasoning framework that can be integrated in Web applications and is able to perform the same reasoning tasks on both client or server sides. This framework embeds a rule-based reasoning engine that uses an algorithm relying on both incremental reasoning and named graphs. We evaluate the performance of our approach and compare the effects of incremental reasoning and named graphs in different experimental conditions. Results show that our reasoner can significantly reduce response times to INSERT and DELETE queries. During the demo we will exhibit how it can be used to perform reasoning tasks based on client-generated information and improve Web applications with location-agnostic reasoning.	HyLAR+: Improving Hybrid Location-Agnostic Reasoning with Incremental Rule-based Update	NA:NA:NA	2016
Shoko Wakamiya:Adam Jatowt:Yukiko Kawai:Toyokazu Akiyama	Microblogging has been recently used for detecting common opinions of users at different geographic places. In this paper we propose a novel spatial visualization system for uncovering collective spatial attention and interest of users not at but rather towards different locations. In other words, we aim to answer questions of the type: what do users collectively talk about when they refer to certain geographical places? In addition, we analyze relations between geographical locations from where Twitter users issue messages and the locations they tweet about. This allows answering questions such as: what do users at a certain place commonly talk about when they refer to another geographical place? We demonstrate an online visualization system that supports the interactive analysis of collective spatial attention over time using 4 months' long collection of tweets in USA.	Analyzing Global and Pairwise Collective Spatial Attention for Geo-social Event Detection in Microblogs	NA:NA:NA:NA	2016
Haofen Wang:Zhijia Fang:Tong Ruan	Recently Web search engines have built knowledge graphs to support entity search and to provide structural summaries called \emph{knowledge cards} for entities mentioned in queries. Different knowledge cards might be complementary or even have conflicts on values of the equivalent property. Thus, it is essential to achieve a more comprehensive fused card from those individual cards representing the same entity. In this paper, we present a system with technical details of card disambiguation, property alignment, value deduplication and card ranking to fuse knowledge cards from various search engines. We further develop a Javascript library called KCF.js based on the card fusion engine and demonstrates its usability via three possible applications.	KCF.js: A Javascript Library for Knowledge Cards Fusion	NA:NA:NA	2016
Oshani Seneviratne:Lalana Kagal:Andrei Sambra	It is our great pleasure to welcome you to the rebooted Developers Day, at with WWW 2016. The success of the World Wide Web depends on its developers. From Hypertext and Web Browsers to the APIs that extend the capabilities of the Web, developers have played a very important role in making the Web as ubiquitous as it is today. To celebrate this great driving force behind the Web, we have rebooted the Developers Day at the World Wide Web Conference. The presenters at Developers Day are developers who have interesting open-source software to showcase to the Web developer community. The submissions that were accepted for presentation clearly showed an innovative use of technology, a potential to advance the state of the art of the Web, and a real-world application of the software. Some of the presentations include social bots, Web-based payment protocols, collaborative web environments, tracking changes in large knowledge organization systems, and testing frameworks suited for the Web. All of these topics are timely and are of interest to the general Web developer community. The program for the Developers Day consists of a round of lightning talks followed by Birds of a Feather (BoF) sessions where attendees are able to get an in-depth look at the software presented in the form of a hands-on tutorial as well as a discussion of open challenges, next steps, and application areas. We believe that such developer discourse is essential for the advancement of the Web.	Session details: Developers Track	NA:NA:NA	2016
Clayton Allen Davis:Onur Varol:Emilio Ferrara:Alessandro Flammini:Filippo Menczer	While most online social media accounts are controlled by humans, these platforms also host automated agents called social bots or sybil accounts. Recent literature reported on cases of social bots imitating humans to manipulate discussions, alter the popularity of users, pollute content and spread misinformation, and even perform terrorist propaganda and recruitment actions. Here we present BotOrNot, a publicly-available service that leverages more than one thousand features to evaluate the extent to which a Twitter account exhibits similarity to the known characteristics of social bots. Since its release in May 2014, BotOrNot has served over one million requests via our website and APIs.	BotOrNot: A System to Evaluate Social Bots	NA:NA:NA:NA:NA	2016
Joachim Neubert	"What's new?" and "What has changed?" are questions users of knowledge organizations systems, such as thesauri, classifications or taxonomies, ask, when new versions of such vocabularies are published. Until recently, it had been difficult for the publishers to provide this information (normally resorting to custom change logging within maintenance applications), and almost impossible for anybody else. With the widespread acceptance of SKOS as the standard publication and exchange format, the situation has changed fundamentally: Exact deltas between two sets of RDF triples can be computed, and the differences can be organized in a meaningful way through SPARQL queries, taking advantage of regular SKOS structures. skos-history combines a script for creating a "version store" with queries accessing this store to generate standard reports such as "added concepts" or "changed notations", or more subtle changes like concept splits, as well as aggregated statistics on certain change types, or a complete change history for a single concept across multiple versions. To allow for interactive sorting, filtering and downloading these reports, and for editing the queries in an IDE-like environment to adapt them to different vocabularies or user needs, other open source libraries are integrated.	skos-history: Exploiting Web Standards for Change Tracking in Knowledge Organization Systems	NA	2016
Michael Nolting:Jan Eike von Seggern	Data-driven and continuous development and deployment of modern web applications depend critically on registering changes as fast aspossible, paving the way for short innovation cycles. A/B testing is a popular tool for comparing the performance of different variants. Despite the widespread use of A/B tests, there is little research on how to assert the validity of such tests. Even small changes in the application's user base, hard- or software stack not related to the variants under test can transform on possibly hidden paths into significant disturbances of the overall evaluation criterion (OEC) of an A/B test and, hence, invalidate such a test. Therefore, the highly dynamic server and client run-time environments of modern web applications make it difficult to assert correctly the validity of an A/B test. We propose the concept of test context to capture data relevant for the chosen OEC. We use pre-test data for dynamic base-lining of the target space of the system under test and to increase the statistical power. During an A/B experiment, the contexts of each variant are compared to the pre-test context to ensure the validity of the test. We have implemented this method using a generic parameter-free statistical test based on the bootstrap method focussing on frontend performance metrics.	Context-based A/B Test Validation	NA:NA	2016
Evan Schwartz	The history of the Web is full of attempts to enable micropayments for content and services. All have failed to achieve widespread adoption. This has fueled recurring debates about the merits or fundamental flaws of the concept of asking users to pay small amounts for what they use online. As a result, however, of the Web's lack of native payment infrastructure, the only viable business models concentrate earnings and power in a small group of content and advertising aggregators and increase demand for privacy-infringing technologies. We need to learn from the failures of previous micropayment schemes and we need to create a payment protocol that is of the Web, for the Web. We present a demo browser extension that uses the new Interledger Protocol (ILP) to demonstrate how payments and micropayments can be seamlessly built into the Web. ILP is an open payment protocol for payments across different payment networks that is being developed in the W3C Interledger Community Group. It enables new possibilities for developers and a better experience for users of the Open Web Platform.	A Payment Protocol of the Web, for the Web: Or, Finally Enabling Web Micropayments with the Interledger Protocol	NA	2016
Adrian Hope-Bailie:Stefan Thomas	The web has enabled free and open information exchange for a vast number of users around the world. However, it has so far failed to do the same for payments. Instead of finding the cheapest route for each payment from a competitive network of providers, we rely on a small number of proprietary operators with global reach. The work happening at the Web Payments Working Group at W3C is attempting to remove some of the friction in performing payments on the Web by defining a standard payment API and messaging in browsers. This will make payments on the Web easier but not entirely frictionless or integrated. As active participants in the W3C's Web Payments Working Group we present a browser polyfill of one of the prosed payment APIs and will walk the audience through the goals of the WG and vision of how payments will work on the Web in the future. Building on this, we will introduce the Interledger Protocol (ILP), a new neutral payments protocol being incubated in the Interledger Payments Community Group, also at the W3C. We will demonstrate how, in the future, the combination of the W3C's Web Payments APIs and the power of ILP payments will not only be frictionless but fully integrated into how we use the Web. Ubiquitous payments in an Internet of Value.	Interledger: Creating a Standard for Payments	NA:NA	2016
Tobias Weller:Maria Maleshkova	Existing standards in capturing processes concentrate on client tools. Furthermore, semantic information are often available that cannot be captured in a structured way with the proposed standard formats. In addition, processes are usually used and maintained by multiple persons. Therefore, a collaborative platform to discuss and share information about processes is valuable. In order to address the challenge of maintaining and sharing knowledge about processes, we provide a tool to capture and annotate processes using Semantic MediaWiki as a collaborative platform. We demonstrate the practical applicability of our tool by presenting a demo available in the World Wide Web.	Capturing and Annotating Processes using a Collaborative Platform	NA:NA	2016
Eyhab Al-Masri:Tie-Yan Liu	It is our great pleasure to welcome you to the PhD Symposium that is held in conjunction with the 25th International World Wide Web Conference, April 11 -- April 15, 2016, Montreal, Canada. The PhD Symposium of WWW2016 provides an excellent opportunity for PhD students at different stages in their research to present their ideas, and receive feedback on their work by experienced researchers and other PhD students working in research areas related to the World Wide Web. The call for papers attracted 16 submissions from Brazil, Canada, China, France, Germany, Greece, India, Ireland, United Kingdom, and the United States. The program committee reviewed and accepted 7 papers that cover a variety of topics including search and recommendation, web mining, social networks and graph analysis, crowdsourcing analysis, semantics and big data, among others. We hope that the program will serve as a valuable reference for researchers and developers in the field of World Wide Web. Putting together the WWW2016 PhD Symposium was a team effort. We first thank the authors for their contributions to the program. We must also thank the program committee members for their invaluable efforts in reviewing papers and providing constructive feedback to authors. We are also grateful to the General Chairs, James Hendler and Roger Nkambou, the Local Organization Committee Members and ACM SIGs for their guidance, support and great help in the preparation and organization of this program. We hope that you will find this program interesting and thought-provoking and that the PhD Symposium will continue its excellence and serve as an important forum for PhD candidates around the world to share their original research results in the field of World Wide Web.	Session details: PhD Symposium	NA:NA	2016
Andrejs Abele	Since the beginning of the Linked Open Data initiative, the number of published open datasets has gradually increased, but the datasets often do not contain description about content such as the dataset domain (e.g., medicine, cancer), when this information is available, it is usually coarse-grained e.g. organic-edunet contains the metadata about a collection of learning objects exposed through the Organic.Edunet portal, but it is classified as Life science. In this work we propose approaches that will provide a detailed description of existing datasets as well as linking assistance when publishing new datasets by generating detailed descriptions of the publishers dataset.	Linked Data Profiling: Identifying the Domain of Datasets Based on Data Content and Metadata	NA	2016
Anupama Aggarwal	In recent times, online social networks (OSNs) are being used not only to communicate but to also create a public/social image. Artists, celebrities and even common people are using social networks to build their brand value and gain more visibility either amongst a restricted set of people or public. In order to enable user to connect to other users in the OSN and gain following and appreciation from them, various OSNs provide different social metrics to the user such as Facebook likes, Twitter followers and Tumblr reblogs. Hence, these metrics give a sense of social reputation to the OSN user. As more users are trying to leverage social media to create a brand value and become more influential, spammers are luring such users to help manipulate their social reputation with the help of paid service (black markets) or collusion networks. In this work, we aim to build a robust alternate social reputation system and detect users with manipulated social reputation. In order to do so, we first start by understanding the underlying structure of various sources of crowdsourced social reputation manipulation like blackmarkets, supply-driven microtask websites and collusion networks. We then build a mechanism for an early detection of users with manipulated social reputation. Our initial results are encouraging and substantiate the possibility of a robust social reputation system.	Detecting and Mitigating the Effect of Manipulated Reputation on Online Social Networks	NA	2016
Hend Alrasheed	In graph theory, the δ-hyperbolicity is a global property that shows how close a given graph's structure is to the tree's structure metrically. It embeds multiple properties that facilitate solving several problems that found to be hard in the general graph form. Interestingly, not only that δ-hyperbolicity provides an idea about the structure of the graph, but also it explains how information navigates throughout the network. Therefore, δ-hyperbolicity has several applications in diverse applied fields. My PhD dissertation focuses on analyzing and exploiting structural properties of hyperbolic networks for different applications.	Structural Properties in δ-Hyperbolic Networks: Algorithmic Analysis and Implications	NA	2016
Álvaro García-Recuero	In this position paper we present the challenge of detecting abuse in a modern Online Social Network (OSN) while balancing data utility and privacy, with the goal of limiting the amount of user sensitive information processed during data collection, extraction and analysis. While we are working with public domain data available in a contemporary OSN, our goal is to design a thorough method for future alternative OSN designs that both protect user's sensitive information and discourage abuse. In this summary, we present initial results for detecting abusive behavior on Twitter. We plan to further investigate the impact of reducing input metadata on the quality of the abuse detection. In addition, we will consider defeating Byzantine behavior by opponents in the system.	Discouraging Abusive Behavior in Privacy-Preserving Online Social Networking Applications	NA	2016
Lu Jiang	The World Wide Web has been witnessing an explosion of video content. Video data are becoming one of the most valuable sources to assess insights and information. However, existing video search methods are still based on text matching (text-to-text search), and could fail for the huge volumes of videos that have little relevant metadata or no metadata at all. In this paper, we propose an accurate, efficient and scalable semantic search method for Internet videos that allows for intelligent and flexible search schemes over the video content (text-to-video search and text&video-to-video search). To achieve this ambitious goal, we propose several novel methods to improve accuracy and efficiency. The extensive experiments demonstrate that the proposed methods are able to surpass state-of-the-art accuracy and efficiency on multiple datasets. Based on the proposed methods, we implement E-Lamp Lite, the first of its kind large-scale semantic search engine for Internet videos. According to National Institute of Standards and Technology (NIST), it achieved the best accuracy in the TRECVID Multimedia Event Detection (MED) 2013, 2014 and 2015, one of the most representative task for content-based video search. To the best of our knowledge, E-Lamp Lite is the first content-based semantic search system that is capable of indexing and searching a collection of 100 million videos.	Web-scale Multimedia Search for Internet Video Content	NA	2016
Julien Plu	We identify two main factors that can cause numerous difficulties when developing a generic entity linking system: i) the amount of data currently available on the Web that do not stop to increase and where a large part comes in the form of natural language texts; ii) the velocity at which data is published that may impose to process streams of text in near real-time. Social media platforms such as Twitter, Facebook or LinkedIn become a reliable source of news and play a key role for being aware of events around the world. Encyclopedia and newspaper articles contain general knowledge of our world and they can be used to explain concepts and known entities. Videos can be associated with subtitles and images may have captions. Depending on where a text comes from, it can have different properties such as a specific language, style of writing or topic. In this research, we present a preliminary framework based on a novel hybrid architecture for an entity linking system, that combines methods from the Natural Language Processing (NLP), information retrieval and semantic fields. In particular, we propose a modular approach in order to be as independent as possible of the text to be processed. Our evaluation suggests that this framework can outperform the state-of-the-art systems or show encouraging results on three datasets: OKE2015, #Micropost 2014 and #Micropost 2015. We identify the current limitations and we provide promising future research directions.	Knowledge Extraction in Web Media: At The Frontier of NLP, Machine Learning and Semantics	NA	2016
Julio Vega	Parkinson's Disease (PD) affects patients' motor and non-motor functionality. Traditional assessment techniques are inaccurate because PD symptoms vary throughout the day and are evaluated in sporadic and subjective sessions. Although recent works have utilised wearable devices to try to overcome these issues, most are unsuitable for following patients regularly for a long time. In contrast, my approach aims to monitor PD continuously in a longitudinal, naturalistic, non-disruptive and non-intrusive way. It uses smartphones to log and transmit over the Internet social, environmental, and interaction data about patients and their surroundings. This data is complemented with other web data sources (i.e., geographical and weather data) and then processed to infer a set of metrics (a latent behavioural variable or LBV) of people's activities and habits. Then, the LBV's trends are measured and mapped to the progression of the disease. As a part of the pilot study to test the proposed methodology, I have collected ~290 million records from 2 patients, making this dataset 34.5x bigger and 4x richer than state-of-the-art sets. I used the collected data to identify six possible PD-related LBVs. This project aims to get a more accurate disease picture and to reduce the physical and psychological burden of traditional assessment methods. Ultimately, the work has the potential to save patients' time and improve the efficiency and effectiveness of health services.	Monitoring Parkinson's Disease Progression Using Behavioural Inferences, Mobile Devices and Web Technologies	NA	2016
Marijn Janssen:Leonidas Anthopoulos:Vishanth Weerakkody	It is our great pleasure to welcome you to the 2016 ACM Workshop on WEB APPLICATIONS AND SMART CITIES -- AW4City'16, in conjunction with WWW 2016. This second event follows up last year's setup of a premier forum to address web-based application and Apps' design and development in the smart city context, which is a rapidly emerging domain and suggests a steadily evolving dominant market. Such applications are crucial, since they deliver smart services of all types to smart city habitants, visitors and businesses, while they create opportunities for new business installation and growth. Various exemplars are well known across the globe and they usually enable transactions between physical and virtual worlds. The mission of this year's workshop, is to emphasize on the contribution of web applications and Apps to current sustainability smart city challenges like urban efficiency against climate change, economic viability, adoption etc. Our short call for papers attracted submissions from Asia, Europe, and the United States. The program committee, with the contribution of additional scholars performed a blind peer-review process and accepted the following: Venue or Track - Reviewed - 8 Accepted - 6.	Session details: AW4City'16	NA:NA:NA	2016
Beth Coleman	I argue for a vital relationship between current understanding of Big Data and how we might move toward new modes of mobile application design that draw on affective and generative "data sets". Toward this end, I look at projects that use the city as a living laboratory, including mobile apps, social media mapping, and various civic uses of Internet of Things (IoT) and surveillance technology. I chose these case studies for their appropriation of psychogeographic "small data" to map a city as they engage affective and immersive aspects of interaction and spatial design.	Generative Mappings of New Data Publics	NA	2016
Alois Paulin	The paper argues that the Smart City idea lacks grounding in shared base technology and instead yields black-box artefacts. The reliance on black-box systems in public governance is considered a great hazard since it may result in sinecures, stifles democratic control of the public domain, and results in neo-feudal monopolies. Base technology (such as the WWW technology stack) on the other hand is use-neutral, implementation-neutral, open, and teach-/learn-able, thus enabling the emergence of cascading technological ecosystems, which can drive large-scale economic and societal progress. The concepts of a primary, secondary, and tertiary technological ecosystem are introduced to delineate the role and importance of base technology. The paper calls for stronger focus on Smart City foundational research and a change in culture from quick fixes to solutions that would survive generations.	Technological Ecosystems' Role in Preventing Neo-Feudalism in Smart-City Informatization	NA	2016
Iraklis Argyriou	This paper offers a brief review of basic literature on smart cities to outline research issues in this field that require further attention from a governance perspective. It then maps the current urban planning landscape in China, a country which puts increasing emphasis on smart cities as a mechanism to promote sustainable development, in order to elicit key policy aspects that need to be considered in empirical analysis when "planning the Chinese smart city". The paper concludes by introducing a case of local innovation in the area of digital economy, the so-called Dream Town, undertaken in the city of Hangzhou as an illustrative example of urban China's current efforts on planning for smart city development.	Planning the Smart City in China: Key Policy Issues and the Case of Dream Town in the City of Hangzhou	NA	2016
João Pedro Gouveia:Júlia Seixas:George Giannakidis	This paper presents an innovative analytical framework to address incomplete interpretations and dispersed data of the energy system in cities, which usually generate multiple inefficiencies. Integrative city planning takes the city energy system from the supply to the demand while considering its spatial representativeness, and drives optimal cost-efficient assessment towards future sustainable energy targets. This holistic approach delivers more adequate policies and measures towards higher energy use efficiency. The proposed analytical framework has been developed within the INSMART EU funded project and focuses on data gathering procedures and data processing tools and models, covering a wide range of city's energy consumers, as residential buildings, transport and utilities. The results, mapped into a GIS, can be further exploited either for awareness increase of citizens and for decision support of city energy planners.	Smart City Energy Planning: Integrating Data and Tools	NA:NA:NA	2016
Leonidas G. Anthopoulos:Christopher G. Reddick	Smart City is an emerging and multidisciplinary domain. It has been recently defined as innovation, not necessarily but mainly through information and communications technologies (ICT), which enhance urban life in terms of people, living, economy, mobility and governance. Smart government is also an emerging topic, which attracts increasing attention from scholars who work in public administration, political and information sciences. There is no widely accepted definition for smart government, but it appears to be the next step of e-government with the use of technology and innovation by governments for better performance. However, it is not clear whether these two terms co-exist or concern different domains. The aim of this paper is to investigate the term smart government and to clarify its meaning in relationship to the smart city. In this respect this paper performed a comprehensive literature review analysis and concluded that smart government is shown not to be synonymous with smart city. Our findings show that smart city has a dimension of smart government, and smart government uses smart city as an area of practice. The authors conclude that smart city is complimentary, part of larger smart government movement.	Smart City and Smart Government: Synonymous or Complementary?	NA:NA	2016
Leonidas Anthopoulos:Marijn Janssen:Vishanth Weerakkody	Smart services concern the core element of a smart city, since they support the realization of urban "intelligence" in terms of people, economy, governance, environment, mobility and leaving. Smart services aim to enhance quality of life within a city and in this respect to improve "livability". The types and purposes of smart services cannot be easily pre-defined, since they are the outcome of innovation, which cannot be pre-defined either, but instead it is the product of citizens' and businesses' creativity. However, standard bodies that work on smart city definition have described smart city portfolios, which are suggested to city policy makers and potential entrepreneurs. The aim of this paper is to validate whether standardized smart service portfolios are being followed by smart cities in practice. In this regard, a set of more than 70 smart cities are examined and their smart services are matched to these portfolios. The outcomes are extremely important and leave space for future research in this regard.	Smart Service Portfolios: Do the Cities Follow Standards?	NA:NA:NA	2016
Sudha Ram:Yun Wang:Faiz Currim:Fan Dong:Ezequiel Dantas:Luiz Alberto Sabóia	Systematic evaluation is crucial to the management and development of smart urban transportation, as it allows transportation planners to better understand the impact of their decisions and design targeted interventions to improve efficiency. Implementation of smart and adaptable public transportation is an important challenge in developing cities and newly industrialized economies where growth characteristics contribute to and can be impacted by factors like overcrowding and travel delays. In this paper, we focus on bus transportation, and present the design and implementation of a 3-layer web-based system for performance evaluation and decision support. This is part of a "Smart Cities" initiative, which is an international collaboration between academia and government. The first layer estimates fundamental indicators such as bus travel time and passenger demands by integrating heterogeneous data sources. A novel bus-stop network is then designed in the second layer, which enables the derivation of passenger patterns in public transit using network analysis. The third layer provides decision support by analyzing causal relationships between indicators. The proposed web-based system called SMARTBUS is being developed and validated with the city of Fortaleza in Brazil. We believe the use of generally available urban transportation data makes our methodology adaptable and customizable for other cities.	SMARTBUS: A Web Application for Smart Urban Mobility and Transportation	NA:NA:NA:NA:NA:NA	2016
Feng Xia:Huan Liu:Irwin King:Kuansan Wang	It is our great pleasure to welcome you to BigScholar 2016, The Third WWW Workshop on Big Scholarly Data: Towards the Web of Scholars. The workshop is held in Montreal, Canada, April 2016, as part of the 25th International World Wide Web Conference (WWW 2016). The BigScholar workshop aims at bringing together researchers and practitioners working on Big Scholarly Data to discuss what are emerging research issues and how to explore the Web of Scholars. Several core challenges, such as the tools and methods for analyzing and mining scholarly data will be the main center of discussions at the workshop. The goal is to contribute to the birth of a community having a shared interest around the Web of Scholars and exploring it using data mining, recommender systems, social network analysis and other appropriate technologies. In response to the call-for-papers, this third edition of the workshop received 22 submissions from Asia, Europe, South America, Canada, and the United States of America. Each paper was reviewed by at least two members of the program committee. As a result of the rigorous review process, 12 high-quality papers were accepted for presentation at the workshop and inclusion in the proceedings. In addition to paper presentations, the workshop also features two Invited Keynote Speeches delivered by Prof. C. Lee Giles from Pennsylvania State University and Prof. Jie Tang from Tsinghua University, respectively.	Session details: BigScholar'16	NA:NA:NA:NA	2016
C. Lee Giles	NA	Scholarly Big Data Knowledge and Semantics	NA	2016
Jie Tang	AMiner is the second generation of the ArnetMiner system. We focus on developing author-centric analytic and mining tools for gaining a deep understanding of the large and heterogeneous networks formed by authors, papers, venues, and knowledge concepts. One fundamental goal is how to extract and integrate semantics from different sources. We have developed algorithms to automatically extract researchers' profiles from the Web and re- solve the name ambiguity problem, and connect different professional networks. We also developed methodologies to incorporate knowledge from the Wikipedia and other sources into the system to bridge the gap between network science and the web mining research. In this talk, I will focus on answering two fundamental questions for author-centric network analysis: who is who? and who are similar to each other? The system has been in operation since 2006 and has collected more than 100,000,000 author profiles, 100,000,000 publication papers, and 7,800,000 knowledge concepts. It has been widely used for collaboration recommendation, similarity analysis, and community evolution.	AMiner: Mining Deep Knowledge from Big Scholar Data	NA	2016
Chun-Hua Tsai:Yu-Ru Lin	Academic publication is a key indicator for measuring scholars' scientific productivity and has a crucial impact on their future career. Previous work has identified the positive association between the number of collaborators and academic productivity, which motivates the problem of tracing and predicting potential collaborators for junior scholars. Nevertheless, the insufficient publication record makes current approaches less effective for junior scholars. In this paper, we present an exploratory study of predicting junior scholars' future co-authorship in three different network density. By combining features based on affiliation, geographic and content information, the proposed model significantly outperforms the baseline methods by 12% in terms of sensitivity. Furthermore, the experiment result shows the association between network density and feature selection strategy. Our study sheds light on the re-evaluation of existing approaches to connect scholars in the emerging worldwide Web of Scholars.	Tracing and Predicting Collaboration for Junior Scholars	NA:NA	2016
Hui Shi:Kurt Maly:Dazhi Chong:Gongjun Yan:Wu He	In the semantic web, content is tagged with "meaning" or "semantics" to facilitate machine processing and web searching. In general, question answering systems that are built on top of reasoning and inference face a number of difficult issues. In this paper, we analyze scalability issues faced by a question answering system used by a knowledge base with science information that has been harvested from the web. Using this system, we will be able to answer questions that contain qualitative descriptors such as "groundbreaking", "top researcher", and "tenurable at university x". This question answering system has been built using ontologies, reasoning systems and custom based rules for the reasoning system. Furthermore, we evaluated the performance of our optimized backward chaining engine on supporting custom rules and designed the experimental environment including scalable datasets, rule sets, query sets and metrics and compared the experimental results with other in-memory ontology reasoning systems. The results show that our developed backward chaining ontology reasoning system has better scalability than in-memory reasoning systems.	Backward Chaining Ontology Reasoning Systems with Custom Rules	NA:NA:NA:NA:NA	2016
Ian Wesley-Smith:Jevin D. West	The body of scientific literature is growing at an exponential rate. This expansion of scientific knowledge has increased the need for tools to help users find relevant articles. However, researchers developing new scholarly article recommendation algorithms face two substantial hurdles: acquiring high-quality, large-scale scholarly metadata and mechanisms for evaluating their recommendation algorithms. To address these problems we created Babel---an open-source platform uniting publisher, researchers, and users. Babel includes tens of millions of scholarly articles, several recommendation algorithms, and tools for integrating recommendations into publisher websites and other scholarly platforms.	Babel: A Platform for Facilitating Research in Scholarly Article Discovery	NA:NA	2016
Jun Zhang:Feng Xia:Wei Wang:Xiaomei Bai:Shuo Yu:Teshome Megersa Bekele:Zhong Peng	Evaluating the scientific impact of scholars has been studied by researchers from various disciplines for a long time. However, very few efforts have been devoted to evaluate the future potential of researchers based on their performance at the initial stage of scientific careers. Academic rising stars represent junior researchers who may not be very outstanding among the peers at the initial stage of their careers, but tend to become influential scholars in the future. In this paper, we propose a novel method named CocaRank, which integrates our proposed new indicator called the collaboration caliber, the typical indicator citation counts and hybrid calculation results on heterogeneous academic networks, to find academic rising stars. In addition, we investigate the appropriate time interval for the prediction of rising stars. The experimental results on real datasets demonstrate that our method can find more top ranked rising stars with higher average citation counts than other state-of-art methods.	CocaRank: A Collaboration Caliber-based Method for Finding Academic Rising Stars	NA:NA:NA:NA:NA:NA:NA	2016
Luam C. Totti:Prasenjit Mitra:Mourad Ouzzani:Mohammed J. Zaki	Finding a relevant set of publications for a given topic of interest is a challenging problem. We propose a two-stage query-dependent approach for retrieving relevant papers given a keyword-based query. In the first stage, we utilize content similarity to select an initial seed set of publications; we then augment them by citation links weighted with information such as citation context relevance and age-based attenuation. In the second stage, we construct a multi-layer graph that expands the publications subgraph by including links to the authors, venues, and keywords. This allows us to return recommendations that are both highly authoritative, and also textually related to the query. We show that our staged approach gives superior results on three different benchmark query sets.	A Query-oriented Approach for Relevance in Citation Networks	NA:NA:NA:NA	2016
Marlies Olensky:Tsung-Han Tsai:Kuan-Ta Chen	This study presents the first analysis of h-index sequences on a larger scale. Exemplarily, we investigated researchers from three different fields within Computer Science. We use Google Scholar citation profiles as data source to construct the h-index sequences of individual researchers. Our ultimate goal is to develop a self-evaluation tool, to assess one's own development of the h-index in comparison to other researchers in the same field, maybe identify career role models in the field and assess career development with future chances of success. The results of this study show that the average h-index sequences behave differently for the datasets, which is partly due to the different sample sizes. Hence, further research will be needed to confirm if every research field behaves differently. In addition, we applied the algorithm developed by Wu et al. to our data to classify the h-index sequences of individual authors according to five different shape categories. The majority of researchers has an S-shaped h-index sequence, followed by IS-shaped and linear sequences. Purely concave or convex sequences hardly ever occur. The researchers with the highest h-indices after 10 career years respectively belong to the S-shaped and IS-shaped categories with a few linear category occurrences. Hence, having a linear h-index is not only very hard to achieve, it is also not a guaranty to be the researcher with the highest h-index in a field.	H-index Sequences across Fields: A Comparative Analysis	NA:NA:NA	2016
Po-shen Lee:Jevin D. West:Bill Howe	We present VizioMetrix, a platform that extracts visual information from the scientific literature and makes it available for use in new information retrieval applications and for studies that look at patterns of visual information across millions of papers. New ideas are conveyed visually in the scientific literature through figures --- diagrams, photos, visualizations, tables --- but these visual elements remain ensconced in the surrounding paper and difficult to use directly to facilitate information discovery tasks or longitudinal analytics. Very few applications in information retrieval, academic search, or bibliometrics make direct use of the figures, and none attempt to recognize and exploit the type of figure, which can be used to augment interactions with a large corpus of scholarly literature. The VizioMetrix platform processes a corpus of documents, classifies the figures, organizes the results into a cloud-hosted databases, and drives three distinct applications to support bibliometric analysis and information retrieval. The first application supports information retrieval tasks by allowing rapid browsing of classified figures. The second application supports longitudinal analysis of visual patterns in the literature and facilitates data mining of these figures. The third application supports crowdsourced tagging of figures to improve classification, augment search, and facilitate new kinds of analyses. Our initial corpus is the entirety of PubMed Central (PMC), and will be released to the public alongside this paper; we welcome other researchers to make use of these resources.	VizioMetrix: A Platform for Analyzing the Visual Information in Big Scholarly Data	NA:NA:NA	2016
Ryan Whalen:Yun Huang:Craig Tanis:Anup Sawant:Brian Uzzi:Noshir Contractor	Using latent semantic analysis on the full text of scientific articles, we measure the distance between 36 million citing/cited article pairs and chart changes in citation proximity over time. The analysis shows that the mean distance between citing and cited articles has steadily increased since 1990. This demonstrates that current scholars are more likely to cite distantly related research than their peers of 20 years ago who tended to cite more proximate work. These changes coincide with the introduction of new information technologies like the Internet, and the increasing popularity of interdisciplinary and multidisciplinary research. The "citation distance" measure shows promise in improving our understanding of the evolution of knowledge. It also offers a method to add nuance to scholarly impact measures by assessing the extent to which an article influences proximate or distant future work.	Citation Distance: Measuring Changes in Scientific Search Strategies	NA:NA:NA:NA:NA:NA	2016
Suhendry Effendy:Roland H.C. Yap	The rating of Computer Science (CS) conferences are important as it influences how papers published at the conferences and may also be used to evaluate research. In this paper, we proposed a method, \rsit{}, based on a small given set of top conference ({\em pivots}) and a relatedness measure based this set as well as basic baseline methods using citation count and field rating. We experimented with a snapshot dataset from Microsoft Academic Graph together with conference data from Microsoft Academic Search. We evaluated the conference ratings from our methods with the CCF conference rating list. We showed that \rsit{} correlates well with CCF rating and correlates better than ratings from using a baseline ranking with citation count or field rating.	Investigations on Rating Computer Sciences Conferences: An Experiment with the Microsoft Academic Graph Dataset	NA:NA	2016
Yan Wu:Srini Venkat:Dah Ming Chiu	Citations serve as an important metric for identifying experts and opinion leaders in academic communities. In this paper, we analyze the evolution of yearly rankings of top-cited (C-list) authors in the domain of Computer Science. In searching for factors that help authors become top-cited, we also gather authors in the top-collaboration list (A-list) and top-publication list (P-list) for each year, and analyze cross-correlation of the C-list with the corresponding A-list and P-list for each year. Results show that the A-list and P-list serve as (unreliable) indicators for appearance on the C-list, but their effect is quick and short-lived. Through further case studies we find other key factors, such as the seminal importance of an author's publication and the association on an author's work with hot topic trends, may significantly affect rank dynamics. Based on the study of citation rank dynamics in academia, we then discuss the modeling of rank dynamics, specifically a model based on item visibility and item strength, and the general applicability of such a model.	Get To the Top and Stay There: A Study of Citation Rank Dynamics in Academia	NA:NA:NA	2016
Zhaowei Tan:Changfeng Liu:Yuning Mao:Yunqi Guo:Jiaming Shen:Xinbing Wang	A large number of papers are being published every year, which makes it difficult for researchers to grasp the relationship among the scientific literatures and the big picture of academic fields. The new challenges have thus been raised, such as analyzing the complicated citation and author network, mining valuable scientific knowledge, and visualizing big scholarly data. The existing academic systems, such as Google Scholar and DBLP have mainly adopted text-based methods, while some other systems make attempts to better navigate the literatures, for example, AMiner and Science Navigation Map. Although these systems show improvements, they fail to present the academic data in a holistic way, and also have limited functions. Therefore, we need to develop new tools which can realize more modules and further explore the academic literatures. In this paper, we conceptualize and design a novel academic system, AceMap, to analyze the big scholarly data and present the results through a ``map'' approach. AceMap integrates several algorithms in the field of network analysis and data mining, and then displays the information in a clear and intuitive way, aiming to help the researchers facilitate their work. After describing the big picture, we present achieved results and our work in progress. By far, AceMap has implemented the following functions: dynamic citation network display, paper clustering, academic genealogy, author and conference homepage, etc. We have also designed and performed distributed network analysis algorithms in a cutting-edge Spark system and utilized modern visualization tools to present the results. Finally, we conclude our paper by proposing the future outlooks.	AceMap: A Novel Approach towards Displaying Relationship among Academic Literatures	NA:NA:NA:NA:NA:NA	2016
Zhiya Zuo:Xi Wang:David Eichmann:Kang Zhao	Although closely related, multidisciplinarity and interdisciplinarity are different. The former indicates the co-existence of multiple disciplines while the latter is more about the integration among various areas. As collaboration between researchers from different areas is one of the major approaches for interdisciplinarity, this research investigated whether higher levels of multidisciplinarity in academic institutions are related to more collaborations, especially more interdisciplinary collaborations, among its faculty members. Using U.S. iSchools as a case study, we applied social network analysis and text mining techniques to faculty members' educational background and publication data, and proposed metrics for multidisciplinarity and collaboration interdisciplinarity. Our analysis results revealed that the multidisciplinarity of an iSchool is actually negatively correlated with the frequency and interdisciplinarity of research collaborations among its faculty members. This finding suggests that having a multidisciplinary environment alone is not sufficient to promote collaborations, nor interdisciplinary collaborations.	Research Collaborations in Multidisciplinary Institutions: a Case Study of iSchools	NA:NA:NA:NA	2016
Michael Huth:Lotfi ben Othmane:Martin Gilje Jaatun:Edgar Weippl	It is our great pleasure to welcome you to the Workshop on Empirical Research Methods in Information Security, associated with WWW 2016. This workshop seeks to engage empirical researchers in information security to think critically about the nusage of methods that support such empirical research, to evaluate current practice, and to offer new insights or proposals for establishing methods that enable a fairer and more robust evaluation of empirical research results, even when tensioned with particular constraints of information security such as the restricted access to meaningful data. The call for papers attracted submissions from United States, Asia and Europe. The program committee reviewed and accepted the following: Full Technical Papers Venue or Track Reviewed - 11 Accepted - 5.	Session details: ERMIS'16	NA:NA:NA:NA	2016
Omar Alrawi:Aziz Mohaisen	Digital certificates are key component of trust used by many operating systems. Modern operating systems implement a form of digital signature verification for various applications, including kernel driver installation, software execution, etc. Digital signatures rely on digital certificates that authenticate the signature, which then verify the validity of a given signature for a signed binary. Malware attempts to subvert the chain of trust through several techniques to achieve execution, evasion, and persistence. In this paper, we examine a large corpus of malware ($3.3$ million samples) to extract digital signatures and their corresponding certificates. We examine several characteristics of the digital certificates to study features in the process of malware authorship that will potentially be used for characterizing and classifying malware. We look at many features including the certificate's chain length, the issue and expiration year, the validity duration of a certificate, the issuing country, validity, top issuing certificate authorities (CAs), and others, highlighting potentially discriminatory features.	Chains of Distrust: Towards Understanding Certificates Used for Signing Malicious Applications	NA:NA	2016
Ping Chen:Lieven Desmet:Christophe Huygens:Wouter Joosen	As the web rapidly expands and gets integrated into all kinds of business, browsing the web has become an important part of people's daily lives. With the rising importance of various web applications sit in a browser, attackers also shifted their focus towards client-side attacks. To defend against these attacks, numerous client-side security mechanisms for the browser are proposed. The presence of these mechanisms on a website can be used as an indicator of the security awareness and practices of that website. In this paper, through a large-scale analysis of more than 18,000 European websites over two years, we analyze the longitudinal trends of the adoption of client-side security mechanisms. We validate that the most popular websites were adopting new security features quicker that less popular websites in the two year timeframe. By examining the websites based on their business vertical, we observe that the websites in the Finance and Education category are outperforming other verticals in the data set, with respect to the usage of client-side security mechanisms.	Longitudinal Study of the Use of Client-side Security Mechanisms on the European Web	NA:NA:NA:NA	2016
Gokhan Kul:Duc Luong:Ting Xie:Patrick Coonan:Varun Chandola:Oliver Kennedy:Shambhu Upadhyaya	Insider threats to databases in the financial sector have become a very serious and pervasive security problem. This paper proposes a framework to analyze access patterns to databases by clustering SQL queries issued to the database. Our system Ettu works by grouping queries with other similarly structured queries. The small number of intent groups that result can then be efficiently labeled by human operators. We show how our system is designed and how the components of the system work. Our preliminary results show that our system accurately models user intent.	Ettu: Analyzing Query Intents in Corporate Databases	NA:NA:NA:NA:NA:NA:NA	2016
Stefan Marschalek:Manfred Kaiser:Robert Luh:Sebastian Schrittwieser	Behavioural analysis has become an important method of today's malware research. Malicious software is executed inside a controlled environment where its runtime behaviour can be studied. Recently, we proposed the concept of not only observing individual executables but a computer system as a whole. The basic idea is to identify malware by detecting anomalies in the way a system behaves. In this paper we discuss our methodology for empirical malware research and highlight its strengths and limitations. Furthermore, we explain the challenges we faced during our research and describe our lessons learned.	Empirical Malware Research through Observation of System Behaviour	NA:NA:NA:NA	2016
Wilfried Mayer:Martin Schmiedecker	For billions of users, today's Internet has become a critical infrastructure for information retrieval, social interaction and online commerce. However, in recent years research has shown that mechanisms to increase security and privacy like HTTPS are seldomly employed by default. With the exception of some notable key players like Google or Facebook, the transition to protecting not only sensitive information flows but all communication content using TLS is still in the early stages. While non-significant portion of the web can be reached securely using an open-source browser extension called HTTPS Everywhere by the EFF, the rules fueling it are so far manually created and maintained by a small set of people. In this paper we present our findings in creating and validating rules for HTTPS Everywhere using crowdsourcing approaches. We created a publicly reachable platform at tlscompare.org to validate new as well as existing rules at large scale. Over a period of approximately 5 months we obtained results for more than 7,500 websites, using multiple seeding approaches. In total, the users of TLScompare spent more than 28 hours of comparing time to validate existing and new rules for HTTPS Everywhere. One of our key findings is that users tend to disagree even regarding binary decisions like whether two websites are similar over port 80 and 443.	TLScompare: Crowdsourcing Rules for HTTPS Everywhere	NA:NA	2016
Martin Pirker:Andreas Nusser	As the internet continuously expands, information security research is a never ending challenge. It is impossible to know all internet participants, protocols and their applications. Instead, security research focuses on empirically collected real-world data; stores, processes, transforms and analyses the data, in order to learn from it and its anomalies --- security issues --- as they happen. This paper presents one practical work-flow for collection and processing of security related data. It present a hard- and software setup, experiences made, and estimates future developments. This gives others the opportunity to learn and identify areas for improvement, especially those in the early stages of setting up a research project based on empirically gathered data.	A Work-Flow for Empirical Exploration of Security Events	NA:NA	2016
Stefan Dietze:Mathieu d'Aquin:Eelco Herder:Dragan Gasevic:Harald Sack	Distance teaching and openly available educational resources on the Web are becoming common practices. Public higher education institutions as well as private training organisations increasingly realise the benefits of online resources. In addition, informal learning and knowledge exchange are inherent to the online interactions found on the Web in general. These interactions involve, for instance, learning and knowledgecentric social networks -- such as Bibsonomy, Slideshare or Videolectures -- but also general-purpose social environments such as LinkedIn, where matters related to skills, competence development or training are central concerns of involved stakeholders. These interactions generate a vast amount of informal knowledge resources of varying granularity, as well as indicators for learning and competences, which are currently under-investigated. On the other hand, the widespread adoption of Linked Data principles [2], as well as the more recent widespread adoption of embedded annotations through schema.org, Microformats and RDFa, has led to the availability of vast amounts of semi-structured data [1], which facilitates interpretation and reuse of Web content and data [4]. This includes schemas and vocabularies directly focused on learning (e.g., LRMI, AAISO, BIBO) and more knowledge-oriented datasets, such as the ones gathered by LinkedEducation.org1, LinkedUniversities.org2 and LinkedUp3. These repositories offer data from The Open University (UK), Learning Analytics datasets and resources [3], or the mEducator Linked Educational Resources [5] , as well as general purpose knowledge graphs, such as DBpedia, WordNet RDF. This has led to the creation of an embryonic "Web of Educational Data", which is largely focused on sharing semi-structured metadata about resources, but which still lacks sufficient recognition of learning-related activities and knowledge resources that are prevalent in less structured and informal online settings. On the other hand, progress in methods and tools for Entity-centric approaches for analysing and understanding the wealth of data on the Web -- such as entity extraction, linking and retrieval -- have paved the way for the exploration of Web data and knowledge relevant to learning and education. The widespread analysis of both informal and formal learning activities and resources has the potential to fundamentally aid and transform the production, recommendation and consumption of learning services and content. However, widespread take-up of such approaches is still hindered by issues that are both technical as well interdisciplinary. Building on the success of previous editions (LILE 2011-2015) 4, LILE20165 addresses such challenges by providing a forum for researchers and practitioners who make innovative use of Web Data for educational purposes. After extensive peer review (each submission was reviewed by at least three independent reviewers) we were able to select 7 papers for presentation in the program. The workshop would not have been possible without contributions of many people and institutions. We are very thankful to the organizers of the WWW 2016 conference for providing us with the opportunity to organize the workshop, for their excellent collaboration, and for looking after many important logistic issues. We are also very grateful to the members of the program committee for their commitment in reviewing the papers and assuring the good quality of the workshop program. We also thank all authors and invited speakers for their invaluable contributions to the workshop. Of course, great appreciation goes to our sponsors GNOSS6, AFEL and EATEL. We thank all supporters of LILE2016 for making this event possible.	Session details: LILE'16	NA:NA:NA:NA:NA	2016
Rafa Absar:Anatoliy Gruzd:Caroline Haythornthwaite:Drew Paulin	In this paper, we examine how multiple social media platforms are being used for formal and informal learning by examining data from two connectivist MOOCs (or cMOOCs). Our overarching goal is to develop and evaluate methods for learning analytics to detect and study collaborative learning processes. For this paper, we focus on how to link multiple online identities of learners and their contributions across several social media platforms in order to study their learning behaviours in open online environments. Many challenges were found in collection, processing, and analyzing the data; results are presented here to provide others with insight into such issues for examining data across multiple open media platforms.	Linking Online Identities and Content in Connectivist MOOCs across Multiple Social Media Platforms	NA:NA:NA:NA	2016
Guillaume Durand:Nabil Belacel:Cyril Goutte	In this paper, we present, discuss and summarize different research works we carried out toward the exploitation of the Web of data for learning and training purpose (Web of learning data). For several years now, we have conducted efforts to explore this main objective through two complementary directions. The first direction is the scalability and particularly the need to develop methods able to provide learners with adequate learning path in the world of big data. The second direction is related to the transition from Web data to Web of learning data and particularly the extraction of cognitive attributes from Web content. For this purpose, we proposed different text mining techniques as well as the development of competency framework engineering tools. Resulting evidence-based techniques allow us to properly evaluate and improve the relationships between learning materials, performance records and student competencies. Although some questions remain unanswered and challenging technology improvements are still required, promising results and developments are arising.	Competency Based Learning in the Web of Learning Data	NA:NA:NA	2016
Yang Liu:Carey Williamson	Modern educational Web sites often feature a rich assortment of linked media content. In this paper, we present a workload study of such an educational Web site hosted at the University of Calgary. Three main insights emerge from our study. First, educational Web sites can generate large volumes of Internet traffic, even when the number of users is limited. Second, network usage is highly influenced by course-related events, such as midterms and finals. Third, the approach used by the site for displaying videos can have adverse impacts on user experience and network traffic. We demonstrate these effects with active measurement of different Web browser and video player implementations.	Workload Study of a Media-Rich Educational Web Site	NA:NA	2016
Dmitry Mouromtsev:Aleksei Romanov:Dmitry Volchek:Fedor Kozlov	The paper describes use cases and architecture of the course extraction plugin for the Open edX platform build upon Linked Open Data. The issue of frequent repetitions of educational materials within the MOOC and relativity of recommendation tools for course developers is considered. Comprehensive review of the designed ontology and mapping, as well as evaluation using test courses are given. The last part of the paper discusses new possibilities and opportunities for the future work.	Metadata Extraction from Open edX Online Courses Using Dynamic Mapping of NoSQL Queries	NA:NA:NA:NA	2016
Bruno Elias Penteado	The advance in quality of public education is a challenge to public managers in contemporary society. In this sense, many studies point to the strong influence of socioeconomical factors in school performance but it is a challenge to select proper data to perform analyses on this matter. In tandem, it has happening a growth in provision of big quantities of educational indicators data, but in isolate cases, and by different agencies of Brazilian government. For this work, we use both education and economic indicators for analysis. The following socioeconomical indicators were selected: municipal human development index (MHDI), social vulnerability index (SVI), Gini coefficient and variables extracted from DBpedia, as part of the connection of this data to the Web of data: GDP per capita and municipal population. These data were used as independent variables to look into their correlations with Brazilian Basic Education Development Index (IDEB) performances at municipal level, supported by the application of linked open data principles. OpenRefine was used to extract the data from different sources, convert to RDF triples and then the mapping of the variables to existing ontologies and vocabularies in this domain, aiming at the reuse of existing semantics. The correlational analysis of the variables showed coherence with the literature about the theme, with significative magnitude between IDEB performances and the indicators related to income and parent education (SVI and HDI), besides moderate relations with the other varibles, except for the municipal population. Finally, the consolidated dataset, enriched by information extracted DBpedia was made available by a SPARQL endpoint for queries of humans and software agents, allowing other applications and researchers to explore the data from other platforms.	Correlational Analysis Between School Performance and Municipal Indicators in Brazil Supported by Linked Open Data	NA	2016
Davide Taibi:Stefan Dietze	Embedded markup of Web pages have emerged as a significant source of structured data on the Web. In this context, the LRMI initiative has provided a set of vocabulary terms, now part of the schema.org vocabulary, to enable the markup of resources of educational value. In this paper we present a preliminary analysis of the use of LRMI terms on the Web by assessing LRMI-based statements extracted from the Web Data Commons dataset.	Towards Embedded Markup of Learning Resources on the Web: An Initial Quantitative Analysis of LRMI Terms Usage	NA:NA	2016
Shuting Wang:Lei Liu	Traditional assessment modes usually give identical set of questions to each student, thus are inefficient for students to fix their problems. In order to perform an efficient assessment, we utilize prerequisite concept maps to find students' learning gaps and work on closing these gaps and proposed a two-phase model for concept map construction. Experiments on concept pairs with prerequisite relationships which are manually created show the promise of our proposed method. In order to meet the challenge of using concept maps in automatic assessments, we also derive a top-k concept selection algorithm which allows students to view different numbers of concepts.	Prerequisite Concept Maps Extraction for AutomaticAssessment	NA:NA	2016
Dirk Ahlers:Erik Wilde:Bruno Martins	It is our great pleasure to welcome you to the 6th International Workshop on Location and the Web, associated with WWW 2016. This is the sixth workshop in its series, having previously been held at WWW (LocWeb 2008), CHI (LocWeb 2009), IoT (LocWeb 2010), CIKM (LocWeb 2014), and WWW (LocWeb 2015). LocWeb continues following its main objective of bringing together a community of researchers at the intersection of location and the Web, serving as a unique venue to integrate different backgrounds and stimulating the exchange of ideas and fostering closer cooperation. LocWeb will provide a topic-specific venue where researchers from different fields, be it data mining, recommendation, search, systems, services, social media, applications, or standards, can discuss and develop the role of location. Its focus lies in Web-scale services and systems facilitating location-aware information access. We aim for a highly interactive, collaborative workshop with ample room for discussion that will explore and advance the geospatial topic. The location topic is understood as a crosscutting issue equally concerning information access, semantics and standards, and Web-scale systems and services. The workshop establishes an integrated venue where the location aspect can be discussed in depth within an interested community. LocWeb follows the main theme of Location-Aware Information Access, with subtopics related to Search, Analytics, Mobility, Apps, Services, and Systems. It is designed to reflect the multitude of fields that demand and utilize location features from an interdisciplinary perspective. The call for papers attracted submissions from Europe, the Americas, Asia, and the Middle East. The program committee reviewed and accepted the following: Venue or Track Reviewed - 5, Accepted - 3. We encourage WWW attendees to attend the three accepted paper presentations, a keynote talk, and a discussion session. The detailed programme will be available on the workshop website.	Session details: LocWeb'16	NA:NA:NA	2016
Luca Maria Aiello	Our daily urban experiences are the product of our perceptions and senses, yet the complete sensorial range is strikingly absent from urban studies. Sight has been historically privileged over the other senses and urban studies. However, smell and sound have also a huge influence over how we perceive places, they impact our behavior, attitudes and health. Yet, city planning is concerned only with a few bad smells and with limiting noise levels. We propose a new way of capturing nuanced sensorial perceptions of cities from data implicitly generated by social media users and of producing detailed sensorial maps of our cities.	The Sensorial Map of the City	NA	2016
Hakan Bagci:Pinar Karagoz	The location-based social networks (LBSN) facilitate users to check-in their current location and share it with other users. The accumulated check-in data can be employed for the benefit of users by providing personalized recommendations. In this paper, we propose a random walk based context-aware friend recommendation algorithm (RWCFR). RWCFR considers the current context (i.e. current social relations, personal preferences and current location) of the user to provide personalized recommendations. Our LBSN model is an undirected unweighted graph model that represents users, locations, and their relationships. We build a graph according to the current context of the user depending on this LBSN model. In order to rank the recommendation scores of the users for friend recommendation, a random walk with restart approach is employed. We compare RWCFR with popularity-based, friend-based and expert-based baseline approaches. According to the results, our friend recommendation algorithm outperforms these approaches in all the tests.	Context-Aware Friend Recommendation for Location Based Social Networks using Random Walk	NA:NA	2016
Lisette Espín Noboa:Florian Lemmerich:Philipp Singer:Markus Strohmaier	Nowadays, human movement in urban spaces can be traced digitally in many cases. It can be observed that movement patterns are not constant, but vary across time and space. In this work, we characterize such spatio-temporal patterns with an innovative combination of two separate approaches that have been utilized for studying human mobility in the past. First, by using non-negative tensor factorization (NTF), we are able to cluster human behavior based on spatio-temporal dimensions. Second, for characterizing these clusters, we propose to use HypTrails, a Bayesian approach for expressing and comparing hypotheses about human trails. To formalize hypotheses, we utilize publicly available Web data (i.e., Foursquare and census data). By studying taxi data in Manhattan, we can discover and characterize human mobility patterns that cannot be identified in a collective analysis. As one example, we find a group of taxi rides that end at locations with a high number of party venues on weekend nights. Our findings argue for a more fine-grained analysis of human mobility in order to make informed decisions for e.g., enhancing urban structures, tailored traffic control and location-based recommender systems.	Discovering and Characterizing Mobility Patterns in Urban Spaces: A Study of Manhattan Taxi Data	NA:NA:NA:NA	2016
Peter Rushforth	The Web has a long history of Web mapping, being originally described at the first WWW Conference in 1994. Web maps have evolved significantly since then, and patterns have developed. The patterns of modern Web mapping underlie mapping programs across economic sectors. Key issues which remain are that Web standards do not directly address the needs of mapping, and Web mapping standards do not rely on Web architecture. As a result of this lack of collaboration, Web mapping and Web standards continue to evolve independently, with little coordination based on a broader interest. This has led to a situation where newcomers to Web mapping are faced with the problem as to what technologies to use for creating and publishing Web maps, of which the unintended consequence is increasing centralization of Web mapping. This paper documents the results of design and development done by Natural Resources Canada within the scope of the Maps For HTML Community Group. A declarative Web map extension to the HTML standard is proposed, together with a new supporting hypermedia type. Taken together, the proposed standards will progressively support simple to advanced Web map applications, including considerations of layers, projections and feature styling. If widely implemented, the proposed Web standards could help realize the value of the substantial investments in spatial data across all sectors of society. The paper concludes with several propositions drawn from the discussion, and proposed actions to be undertaken by the Maps for HTML Community Group, in which the reader is invited to participate.	Maps for HTML: A New Media Type and Prototype Client for Web Mapping	NA	2016
Martin Atzmueller:Alvin Chin:Christoph Trattner	In our first workshop on Modeling Social Media (MSM 2010 in Toronto, Canada), we explored various different models of social media ranging from user modeling, hypertext models, software engineering models, sociological models and framework models. In our second workshop (MSM 2011 in Boston, USA), we addressed the user interface aspects of modeling social media. In our third workshop (MSM 2012 in Milwaukee, USA), we looked at the collective intelligence in social media, i.e. making sense of the content and context from social media websites such as Facebook, Twitter, Google+ and Foursquare by banalyzing tweets, tags, blog posts, likes, posts and check-ins, in order to create a new knowledge and semantic meaning. Our fourth workshop (MSM 2013 in Paris, France) then especially considered "recommender systems" for social media, also tackling the increasing information overload problem for recommending "things" in social media. The workshop in the last two years (MSM 2014 in Seoul, Korea and MSM 2015 in Florence, Italy) focused on mining Big Data on social media and the web. Behavioral analytics is an important topic, e.g., concerning web applications as well as mobile and ubiquitous applications, for understanding user behavior. Following the discussion at our workshop at WWW 2015 we aim to continue our focus on behavioral analytics on social media and the web, however, with a special focus: We aim to go beyond standard analytics approaches and try to answer the "why" question, which is often missing in analytical papers. The call for papers attracted 17 submissions, from which we were able to accept 8 submissions (five full papers and three short papers) based on a rigorous reviewing process. The accepted papers cover a variety of topics, including social media and dynamic behavioral analytics, usage analysis, recommendation, and behavior prediction. We hope that these proceedings will serve as a valuable reference for researchers and developers.	Session details: MSM'16	NA:NA:NA	2016
Anatoliy Gruzd	Computational techniques such as Behavioural Analytics (BA) have been extremely effective at transforming social media data into useful insights for applications such as recommender systems [1] and customer relation management [2]. However, due to the rise of smarter, more sophisticated social bots [3] and the increasing reliance on algorithmic filtering (which nudges online users to make certain choices and take specific actions) [4], it begs the question, if we are using data from social media for modelling, are we modelling human behavior in social media or simply reverse engineering how bots and other algorithms operate?	Who are We Modelling: Bots or Humans?	NA	2016
Martin Atzmueller:Andreas Schmidt:Mark Kibanov	The analysis of sequential trails and patterns is a prominent research topic. However, typically only explicitly observed trails are considered. In contrast, this paper proposes the DASHTrails approach that enables the modeling and analysis of distribution-adapted sequential trails and hypotheses. It presents a method for deriving transition matrices given a probability distribution over certain events. We demonstrate the applicability of the proposed approach using real-world data in the mobility domain, i.e., car trajectories and spatio-temporal distributions on car accidents.	DASHTrails: An Approach for Modeling and Analysis of Distribution-Adapted Sequential Hypotheses and Trails	NA:NA:NA	2016
Lukas Eberhard:Christoph Trattner	Social information such as stated interests or geographic check-ins in social networks has shown to be useful in many recommender tasks recently. Although many successful examples exist, not much attention has been put on exploring the extent to which social impact is useful for the task of recommending sellers to buyers in virtual marketplaces. To contribute to this sparse field of research we collected data of a marketplace and a social network in the virtual world of Second Life and introduced several social features and similarity metrics that we used as input for a user-based $k$-nearest neighbor collaborative filtering method. As our results reveal, most of the types of social information and features which we used are useful to tackle the problem we defined. Social information such as joined groups or stated interests are more useful, while others such as places users have been checking in, do not help much for recommending sellers to buyers. Furthermore, we find that some of the features significantly vary in their predictive power over time, while others show more stable behaviors. This research is relevant for researchers interested in recommender systems and online marketplace research as well as for engineers interested in feature engineering.	Recommending Sellers to Buyers in Virtual Marketplaces Leveraging Social Information	NA:NA	2016
Bruce Ferwerda:Markus Schedl:Marko Tkalcic	Applications increasingly use personality traits to provide a personalized service to the user. To acquire personality, social media trails showed to be a reliable source. However, until now, analysis of social media trails have been focusing on \textit{what} has been disclosed: content of disclosed items. These methods fail to acquire personality when there is a lack of content (non-disclosure). In this study we do not look at the disclosed content, but whether disclosure occurred or not. We extracted 40 items of different Facebook profile sections that users can disclose or not disclose. We asked participants to indicate to which extent they disclose the items in an online survey, and additionally asked them to fill in a personality questionnaire. Among 100 participants we found that users' personality can be predicted by solely looking at whether they disclose particular sections of their profiles. This allows for personality acquisition when content is missing.	Personality Traits and the Relationship with (Non-) Disclosure Behavior on Facebook	NA:NA:NA	2016
Luciano Gallegos:Kristina Lerman:Arhur Huang:David Garcia	During the last years, researchers explored the geographic and environmental factors that affect happiness. More recently, location-sharing services provided by the social media has given an unprecedented access to geo-located data for studying the interplay between these factors on a much bigger scale. Do location-sharing services help in turn at distinguishing emotions in places within a city? Which aspects contribute better at understanding happier places? To answer these questions, we use data from Foursquare location-sharing service to identify areas within a major US metropolitan area with many check-ins, i.e., areas that people like to use. We then use data from the Twitter microblogging platform to analyze the properties of these areas. Specifically, we have extracted a large corpus of geo-tagged messages, called tweets, from a major metropolitan area and linked them US Census data through their locations. This allows us to measure the sentiment expressed in tweets that are posted from a specific area, and also use that area's demographic properties in analysis. Our results reveal that areas with many check-ins are different from other areas within the metropolitan region. In particular, these areas have happier tweets, which also encourage people living in it or from other areas to commute longer distances to these places. These findings shed light on the influence certain places play within a city regarding people's emotions and mobility, which in turn can be used for city planners for designing happier and more equitable cities.	Geography of Emotion: Where in a City are People Happier?	NA:NA:NA:NA	2016
Taraneh Khazaei:Lu Xiao:Robert Mercer:Atif Khan	The dichotomy between users' privacy behaviours and their privacy attitudes is a widely observed phenomenon in online social media. Such a disparity can be mainly attributed to the users' lack of awareness about the default privacy setting in social networking websites, which is often open and permissive. This problem has led to a large number of publicly available accounts that may belong to privacy-concerned users. As an initial step toward addressing this issue, we examined whether profile attributes of Twitter users with varying privacy settings are configured differently. As a result of the analysis, a set of features is identified and used to predict user privacy settings. For our best classifier, we obtained an F-score of 0.71, which outperforms the baselines considerably. Hence, profile attributes proved valuable for our task and suggest the possibility of the automatic detection of public accounts intended to be private based on online social footprints.	Privacy Behaviour and Profile Configuration in Twitter	NA:NA:NA:NA	2016
Maja R. Rudolph:Matthew Hoffman:Aaron Hertzmann	Good recommendations are a key tool to increase user engagement and user satisfaction on many social networks. Here we focus on Behance, a social network for artist from various fields such as typography, street art, industrial design, and fashion. On Behance, the artists can connect by following each other, display their work in online portfolios, and brows each other's work. Each user has a personalized dashboard which is an integral part of the Behance experience. In this work we create a joint behavior model which jointly models the users' viewing behavior and the social network. The joint model which we fit with variational inference is capable of producing both who-to-follow and what-to-view recommendations. We show on real data from Behance that the joint behavior model outperforms a Poisson factorization approach which treats both data sources separately.	A Joint Model for Who-to-Follow and What-to-View Recommendations on Behance	NA:NA:NA	2016
Steffen Schnitzer:Svenja Neitzel:Sebastian Schmidt:Christoph Rensing	Crowdsourcing platforms support the assignment of jobs while relying on the workers' search capabilities. Recommenders can support the workers' decisions to improve quality and outcome for both worker and requester. A precedent study showed, that many workers expect to get tasks recommended, which are similar to previously finished ones. In order to create genuine task recommendation, similarities between tasks have to be identified and analyzed. Therefore, this work provides an empirical study about how workers perceive task similarities. The perceived task similarities may vary between workers with different cultural background and may depend e.g. on the complexity, required action or the requester of the task.	Perceived Task Similarities for Task Recommendation in Crowdsourcing Systems	NA:NA:NA:NA	2016
Pengyu Wei:Ning Wang	Vast volumes of online information related to news stories, blogs and online social media have an observable effect on investor's opinions towards financial markets. But do these particular information reflect or impact people's decision-making in investment? This paper investigates whether data generated from Internet usage can be used to predict the movements in the financial market. We provide evidence that data on how often a company's Wikipedia page is being viewed is linked to its subsequent performance in the stock market. We then develop a portfolio in line with the Wikipedia usages and demonstrate that our investment strategy based on Wikipedia views is profitable both financially and statistically. Our finding implies that online web data such as Wikipedia presents an alternative insights on collecting and quantifying investor's sentiments towards financial markets, which can be further employed as a timely approximation of investor's behaviours in decision-making.	Wikipedia and Stock Return: Wikipedia Usage Pattern Helps to Predict the Individual Stock Movement	NA:NA	2016
Mena B. Habib:Florian Kunneman:Maurice van Keulen	It is our great pleasure to welcome you to the 2nd International Workshop on Natural Language Processing for Informal Text (NLPIT), associated with WWW 2016. The rapid growth of Internet usage in the last two decades adds new challenges to understand the informal user generated content (UGC) on the Internet. Textual UGC refers to textual posts on social media, blogs, emails, chat conversations, instant messages, forums, reviews, or advertisements that are created by endusers of an online system. A large portion of language used on textual UGC is informal. Informal text is the style of writing that disregards language grammars and uses a mixture of abbreviations and context dependent terms. The straightforward application of state-of-the-art Natural Language Processing approaches on informal text typically results in a significantly degraded performance due to the following reasons: the lack of sentence structure; the lack of enough context required; the seldom entities involved; the noisy sparse contents of users' contributions; and the untrusted facts contained. The NLPIT workshop hopes to bring opportunities and challenges involved in informal text processing under the attention of researchers. In particular, we are interested in discussing informal text modeling, normalization, mining, and understanding in addition to various application areas in which UGC is involved. The workshop is a follow-up of the first NLPIT workshop that was held in conjunction with ICWE: the International Conference on Web Engineering held in Rotterdam, The Netherlands, from 23rd to 26th of July 2015. The call for papers attracted submissions from 15 different countries. The program committee reviewed and accepted the following: Venue or Track: Reviewed - 16 Accepted - 6. The workshop started with a keynote presentation given by Raphaël Troncy from EURECOM, France entitled "Linking Entities for Enriching and Structuring Social Media Content". The keynote is followed by 6 research presentations. The common theme of the research presentations is, analogous to the first edition of NLPIT, NLP for a multitude of languages. Among them the papers and presentations feature Arabic, Spanish, Russian, Chinese, Yoruba (West Africa), and various variations of English.	Session details: NLPIT'16	NA:NA:NA	2016
Raphaël Troncy	Social media platforms such as Twitter, Facebook or LinkedIn become a reliable source of news and play a key role for being aware of events around the world. Using social media to recognize, enrich or summarize events is however very challenging. In the first part of this talk, we will present ADEL, a novel hybrid architecture for an adaptive entity linking system, that combines methods from the natural language processing, information retrieval and semantic fields. The framework enables to link all the mentions occurring in a text to their entity counterparts in a knowledge base. It is modular and adaptive since it enables to process text written in different languages and of different kind (newswire, tweets, blog posts, etc.) while entities can be of common types (PERSON, LOCATION and ORGANIZATION) or specific ones (dates, numbers) and be disambiguated in generic or specialized knowledge bases. We will show how ADEL can outperform the state-of-the-art systems on the reference NEEL challenges that happens in the yearly #Micropost workshop (2014-2016). In the second part of this talk, we will present a framework that can collect microposts from more than 12 social platforms and that contain media items, as a result of a query -- for example a trending event. We will then show how we can automatically create different visual storyboards that reflect what users have shared about this particular event. The visualization emphasizes the different aspects of storyboards. A graph view shows the relationships between microposts and topics that we automatically extract, while the timeline view emphasizes the time dimension. The user can watch and interact with the summarized view of all the topics or select a particular one with the additional details. In addition, the states of different views are persistent through the URLs which makes easy sharing possible.	Linking Entities for Enriching and Structuring Social Media Content	NA	2016
Tunde Adegbola	The Unsupervised induction of morphological rules from a simple list of words in a language of interest is a productive approach to Computational Morphology. The most popular algorithms used for this purpose in the literature are based on the assumption that the relatively high occurrence frequencies of certain word segments described as recurrent partials in a lexicon suggests the existence of morpheme boundaries around such high frequency word segments. Even though this word-segment-frequency approach works well for concatenative morphology, it does not cater for some of the most productive morphological processes in Yorùbá and some other African languages. In this paper, unsupervised induction of the morphological rules of Yorùbá was achieved based on a word-pattern-frequency rather than a word-segment-frequency approach. Words in a Yorùbá lexicon were clustered according to the morphological processes on which their formation are based, producing results that hitherto were achievable only by painstaking rule-based manual classification.	Pattern-based Unsupervised Induction Of Yorùbá Morphology	NA	2016
Jhon Adrián Cerón-Guzmán:Elizabeth León-Guzmán	Twitter data have brought new opportunities to know what happens in the world in real-time, and conduct studies on the human subjectivity on a diversity of issues and topics at large scale, which would not be feasible using traditional methods. However, as well as these data represent a valuable source, a vast amount of noise can be found in them. Because of the brevity of texts and the widespread use of mobile devices, non-standard word forms abound in tweets, which degrade the performance of Natural Language Processing tools. In this paper, a lexical normalization system of tweets written in Spanish is presented. The system suggests normalization candidates for out-of-vocabulary (OOV) words based on similarity of graphemes or phonemes. Using contextual information, the best correction candidate for a word is selected. Experimental results show that the system correctly detects OOV words and the most of cases suggests the proper corrections. Together with this, results indicate a room for improvement in the correction candidate selection. Compared with other methods, the overall performance of the system is above-average and competitive to different approaches in the literature.	Lexical Normalization of Spanish Tweets	NA:NA	2016
Pedro Miguel Dias Cardoso:Anindya Roy	Conversations on social media and microblogging websites such as Twitter typically consist of short and noisy texts. Due to the presence of slang, misspellings, and special elements such as hashtags, user mentions and URLs, such texts present a challenging case for the task of language identification. Furthermore, the extensive use of transliteration for languages such as Arabic and Russian that do not use Latin script raises yet another problem. This work studies the performance of language identification algorithms applied to tweets, i.e. short messages on Twitter. It uses a previously trained general purpose language identification model to semi-automatically label a large corpus of tweets - in order to train a tweet-specific language identification model. It gives special attention to text written in transliterated Arabic and Russian.	Language Identification for Social Media: Short Messages and Transliteration	NA:NA	2016
Anna Jørgensen:Anders Søgaard	We present a suite of 12 datasets for evaluating POS taggers across varieties of English to enable researchers to evaluate the robustness of their models. The suite includes three new datasets, sampled from lyrics from black American hip-hop artists, southeastern American Twitter, and the subtitles from the TV series The Wire. We present an example eval- uation of an off-the-shelf POS tagger across these datasets.	A Test Suite for Evaluating POS Taggers across Varieties of English	NA:NA	2016
Ming Yang:William H. Hsu	We present a new approach towards capturing topic interests corresponding to all the observed latent topics generated by an author in documents to which he or she has contributed. Topic models based on Latent Dirichlet Allocation (LDA) have been built for this purpose but are brittle as to the number of topics allowed for a collection and for each author of documents within the collection. Meanwhile, topic models based upon Hierarchical Dirichlet Processes (HDPs) allow an arbitrary number of topics to be discovered and generative distributions of interest inferred from text corpora, but this approach is not directly extensible to generative models of authors as contributors to documents with variable topical expertise. Our approach combines an existing HDP framework for learning topics from free text with latent authorship learning within a generative model using author list information. This model adds another layer into the current hierarchy of HDPs to represent topic groups shared by authors, and the document topic distribution is represented as a mixture of topic distribution of its authors. Our model automatically learns author contribution partitions for documents in addition to topics.	HDPauthor: A New Hybrid Author-Topic Model using Latent Dirichlet Allocation and Hierarchical Dirichlet Processes	NA:NA	2016
Qian Zhang:Bruno Goncalves	Sina Weibo, China's most popular microblogging platform, is considered to be a proxy of Chinese social life. In this study, we contrast the discussions occurring on Sina Weibo and on Chinese language Twitter in order to observe two different strands of Chinese culture: people within China who use Sina Weibo with its government imposed restrictions and those outside that are free to speak completely anonymously. We first propose a simple ad-hoc algorithm to identify topics of Tweets and Weibos. Different from previous works on micro-message topic detection, our algorithm considers topics of the same contents but with different #tags. Our algorithm can also detect topics for Tweets and Weibos without any #tags. Using a large corpus of Weibo and Chinese language tweets, covering the entire year of 2012, we obtain a list of topics using clustered #tags and compare them on two platforms. Surprisingly, we find that there are no common entries among the Top 100 most popular topics. Only 9.2% of tweets correspond to the Top 1000 topics of Weibo, and conversely only 4.4% of weibos were found to discuss the most popular Twitter topics. Our results reveal significant differences in social attention on the two platforms, with most popular topics on Weibo relating to entertainment while most tweets corresponded to cultural or political contents that is practically non existent in Weibo.	Topical differences between Chinese language Twitter and Sina Weibo	NA:NA	2016
Eric Charton:Nizar Ghoula:Marie-Jean Meurs	It is our great pleasure to welcome you to the 1st Workshop on Open Data for Local Search, associated with WWW 2016. Local search engines are specialized information retrieval systems enabling users to discover amenities and services in their neighborhood (schools, businesses, hospitals, etc.). Developing a local search system still raises scientific questions, as well as very specific technical issues. One of the main problems encountered is the partial availability or even the absence of informative contents related to local actors, merchants or service providers. Introducing open data in the architecture of local search engines supports the identification and collection of structured content. Collaborative data such as those made available by the OpenStreetMap Foundation can be of help to identify new dealers, and improve their geolocation. Semantic Web resources such as DBpedia contain keywords or content for enriching ontologies associated with a local search service. Open data provided by cities or national organizations, such as descriptions of public institutions, opening hours, location of shopping centers are other usable resources. Available open data can be exploited to dramatically improve the design of local search engines and their contents. The aim of this workshop is to explore new fields of investigation both in terms of algorithmic approaches as well as originality of usable data. The workshop focuses on how open data can be used to enhance the capabilities of local search engines. Target audience will include researchers, and professionals interested in: semantic web and open data usage to improve local search, enhance ontologies and their alignment, and discover keywords and concepts geo-content improvement using open data to develop geo-search algorithms, build maps and content, improve and enrich geo-data. information extraction involving open data for knowledge management, named entity, business, and content discovery. The call for papers attracted submissions from the United States, Canada, Switzerland, India, and China. The program committee reviewed and accepted the following: Venue or Track Reviewed - 10 Accepted - 6. We encourage attendees to attend demonstrations, list of which will be available on the website http://od4ls.uqam.ca	Session details: OD4LS'16	NA:NA:NA	2016
Mazen AlObaidi:Khalid Mahmood:Susan Sabra	Local search engines are a vital part of presence of local businesses on the Internet. Local search engines improvement is an important element to ensure that local businesses can be found by millions of people whom are using web to find services. However, web presence can be disguised or not properly documented. Our approach will improve the effectiveness of local search, and increase the ranking of local businesses. We introduce an approach for enhancing local search engines efficiency in returning more accurate results. Our approach consists of semantically enriching the results of a query using Linked Open Data (LOD) web content. Our preliminary evaluation demonstrated evidence that our approach has a better search with more accurate results.	Semantic Enrichment for Local Search Engine using Linked Open Data	NA:NA:NA	2016
Chuankai An:Dan Rockmore	Local search helps users find certain types of business units (restaurant, gas stations, hospitals, etc.) in the surrounding area. However, some merchants might not have much online content (e.g. customer reviews, business descriptions, opening hours, telephone numbers, etc.). This can pose a problem for traditional local search algorithms such as vector space based approaches. With this difficulty in mind, in this paper we present an approach to local search that incorporates geographic open data. Using the publicly available {\em Yelp} dataset we are able to uncover patterns that link geographic features and user preferences. From this, we propose a model to infer user preferences that integrates geographic parameters. Through this model and estimation of user preference, we develop a new framework for ``local'' (in the sense of geography) search that offsets the absence of contexts regarding physical business units. Our initial analysis points to the meaningful integration of open geographic data in local search and points out several directions for further research.	Improving Local Search with Open Geographic Data	NA:NA	2016
Eric Charton:Nizar Ghoula:Marie-Jean Meurs	Local search engines are specialized information retrieval systems enabling users to discover amenities and services in their neighbourhood. Developing a local search system still raises scientific questions, as well as very specific technical issues. Those issues come for example from the lack of information about local events and actors, or the specific form taken by the indexable data. Available open data can be exploited to dramatically improve the design of local search engines and their content. The purpose of this workshop is to explore new fields of investigation both in terms of algorithmic approaches as well as originality of usable data. The workshop focuses on how open data can be used to enhance the capabilities of local search engines.	Open Data for Local Search: Challenges and Perspectives	NA:NA:NA	2016
Robert F. Lytle	This paper addresses the use of Open Data business licence records in the course of local web search. It assesses the feasibility of increasing the search ranking of authorised service providers and rank reduction or removal of providers with an invalid licence status. A case study was conducted to identify usable results returned with a local search across multiple providers. Result records were then analysed against an applicable Open Data set to determine the usability of the search results for end-users seeking service. A model for adjusting search result ranking is proposed for further analysis. Finally, findings based on the analysis lead to additional proposed research efforts in this space.	Open Data Business Licence Usage to Improve Local Search Engine Content Ranking	NA	2016
Jonathan Milot:Patrick Munroe:Eric Beaudry:Francois Grondin:Guillaume Bourdeau	Lookupia is an intelligent real estate search engine for finding houses optimally geolocated to reach points of interest. It uses data from OpenStreetMap and most of public transit corporations that publish transit schedules in the General Transit Feed Specification (GTFS) format.	Lookupia: An Intelligent Real Estate Search Engine for Finding Houses Optimally Geolocated to Reach Points of Interest	NA:NA:NA:NA:NA	2016
Michael Peterman:Omar Benomar:Hacene Mechedou:Felix-Herve Bachand	Geocoding is the process of converting addresses to geocoordinates. It is widely used in several fields such as public health to monitor socioeconomic inequalities for example or in Geographical Information Systems (GIS) to be able to use with its provided features. In this work, we describe a method to create an address geocoder from a free and open government street lines data source. The address geocoder transforms a street address into a location typically measured in latitude-longitude coordinates. The address geocoder is used in a search engine to relate spatial data to search results and improve accuracy.	Address Geocoding using Street Profiles for Local Search	NA:NA:NA:NA	2016
Camille Tardy:Laurent Moccozet:Gilles Falquet	There exist many popular crowdsourcing and social services (Volunteered Geographic Information (VGI)) to share information and documents such as Flickr, Foursquare, Twitter , Facebook, etc. They all use metadata, folksonomy and more importantly a geographic axis with GPS coordinates and/or geographic tags. Using this available folksonomy in VGI services we propose a logical approach to highlight and possibly discover the characteristics of geographic places. The approach is based on the notion of spatial coverage and a model of tags categorization and on their semantic identification, using semantic services such as GeoNames, OpenStreetMap or WordNet. We illustrate our model with Flickr to retrieve the characteristics (function, usage?) of places even if those places have a small number of related photos. Those found characteristics allow tag disambiguation and can be use to complete the semantic gap on places and POIs such as the function of buildings, which can exist in geographic services.	A Simple Tags Categorization Framework Using Spatial Coverage to Discover Geospatial Semantics	NA:NA:NA	2016
Philipp Cimiano:Jean-Michel Dalle:Fabien Gandon	It is our great pleasure to welcome you to the 1st Workshop on Question Answering And Activity Analysis in Participatory Sites (Q4APS) associated with WWW 2016. This workshop intends to bring together researchers and practitioners of Question Answering sites and services on the Web to present and discuss latest advances in analyzing, supporting and automating tasks of the life-cycle of such applications. The goal is to cover and bring together the different approaches existing in managing and answering natural language questions of users on the Web. This includes methods, models and algorithms from automated question-answering, for question-answering forums mining, as well as for monitoring and management automation. Topics included in the call for papers were the following: question routing, question answering question and answer recommendation question and need analysis, question modeling expert finding, expertise categorization, expertise labelling debate analysis, argument mining, argument schemes moderating support and automation animation fostering, targeted solicitation and notification answer generation, multiple and/or heterogeneous answer sources answer detection and ranking, best answer identification answer building and answer improving questions and expert topic labelling role detection (questioner, answerer, editor, etc.), user modelling social aspects of question answering fact checking, cross-validation, supporting evidence spam or abuse prevention in questions and answers answer personalization challenges, datasets, benchmarks for question-answering evaluation We received seven submissions for the workshop. Each submission received at least three reviews by members of the program committee. On the basis of these reviews, we decided to accept six papers to be presented at the workshop and to be included in the conference proceedings. The paper by Burel et al. "Structural Normalisation Methods for Improving Best Answer Identification in Question Answering Communities" investigates the problem of how to identify the best answers in community-based Q/A portals. They propose to apply structural normalization techniques to feature-based best answer identification models. The paper by Jenders et al. "Which Answer is Best? Predicting Accepted Answers in MOOC Forums" deals with a similar topic, and investigates the identification of best answers in the context of MOOCS. They present a method that exploits historical data to find best answers for a question. In their paper "Using Semantics to Search Answers for Unanswered Questions in Q&A Forums", Singh et al. present an approach that tackles the problem that in many Q/A community sites there are many unanswered questions. To mitigate this problem, they propose a system that finds answered questions that are similar to unanswered questions. Sandor et al. present an approach to the detection of user issues and request types in technical forum question posts. They propose a system that categorizes posts into these types using techniques from discourse analysis. In their paper "Enriching Topic Modelling with Users' histories for Improving Tag Prediction in Q&A Systems", Loeckx et al. describe an approach to predict the tags for questions in social Q/A sites such as StackExchange. They propose a model that factors in the user context and show that this extension improves upon a purely textual content-based baseline. Finally, Shekarpour et al., in their paper "Question Answering on Linked Data: Challenges and Future Directions", present an overview of open and future challenges in the field of question answering from Linked Data. Presenters are encouraged to bring demos to the workshop to enhance oral discussions and presentations. Acknowledgment. We thank the ANR for the ANR-12-CORD-0026 grant supporting the Ocktopus project and this workshop.	Session details: Q4APS'16	NA:NA:NA	2016
Xavier Amatriain	Q&A sites like Quora aim at growing the world's knowledge. In order to do this, they need to get the right questions to the right people to answer them, but also the existing answers to people who are interested in them. In order to accomplish this, they need to build a complex ecosystem where issues such as content quality, engagement, demand, interests, or reputation are taken into account. It is not possible to build a system like this unless most of the process are highly automated and scalable. The good news is that using high-quality data you can build machine learning solutions that can help address all of the previous requirements. In this talk I will describe some interesting uses of machine learning for Q&A that range from different recommendation approaches such as personalized ranking to classifiers built to detect duplicate questions or spam. I will describe some of the modeling and feature engineering approaches that go into building these systems. I will also share some of the challenges faced when building such a large-scale knowledge base of human-generated knowledge. Finally, I will describe some of the unresolved research challenges in the Q&A Space. I will use my experience at Quora as the main driving example. Quora is a Q&A site that despite having over 80 million unique visitors a month, it is known for keeping a high-quality of answers and content in general.	Machine Learning for Q&A Sites: State of the Art and Research Directions	NA	2016
Jean-Michel Dalle:Catherine Faron-Zucker:Fabien Gandon:Mathieu Lacage:Zide Meng	This position paper provides an overview of the OCKTOPUS project whose goal is to increase the social and economic benefit of user-generated content, by transforming it into knowledge which can be shared and reused broadly.	Online Knowledge Triage: Searching, Detecting, Labelling and Orienting User Generated Content	NA:NA:NA:NA:NA	2016
Glenn Boudaer:Johan Loeckx	The automatic attribution of tags in Question & Answering (Q&A) systems like Stack Exchange can significantly reduce the human effort in tagging as well as improve the consistency among users. Existing approaches typically either rely on Natural Language Processing solely or employ collaborative filtering techniques. In this paper, we attempt to combine the best of both worlds by investigating whether incorporating a personal profile, consisting of a user's history or its social network can significantly improve the predictions of state-of-the-art text-based methods. Our research has found that enriching content-based text features with this personal profile allows to trade-off the precision of predictions for recall and as such improve the "exact match" (predicting the number of tags and the tags themselves correctly) in a multi-label setting from a baseline of 18.2% text-only to 54.3%.	Enriching Topic Modelling with Users' Histories for Improving Tag Prediction in Q&A Systems	NA:NA	2016
Gregoire Burel:Paul Mulholland:Harith Alani	Nowadays, Question Answering (Q&A) websites are popular source of information for finding answers to all kind of questions. Due to this popularity it is critical to help the identification of best answers to existing questions for simplifying the access to relevant information. Although it is possible to identify relatively accurately best answers by using binary classifiers coupled with user, content and thread} features, existing works have generally ignored to incorporate the thread-like structure of Q&A communities in the design of best answer identification predictors and algorithms. This paper investigates this particular issue by studying structural normalisation techniques for improving the accuracy of feature based best answer identification models. Thread-based normalisation methods are introduced for improving the accuracy of identification models by introducing a systematic normalisation approach that normalise predictors by taking into account relations between features and the thread-like structure of Q&A communities. Compared to similar non normalised models, better results are obtained for each of the three communities studied. These results show that structural normalisation methods can improve the identification of best answers compared to non-normalised models.	Structural Normalisation Methods for Improving Best Answer Identification in Question Answering Communities	NA:NA:NA	2016
Maximilian Jenders:Ralf Krestel:Felix Naumann	Massive Open Online Courses (MOOCs) have grown in reach and importance over the last few years, enabling a vast userbase to enroll in online courses. Besides watching videos, user participate in discussion forums to further their understanding of the course material. As in other community-based question-answering communities, in many MOOC forums a user posting a question can mark the answer they are most satisfied with. In this paper, we present a machine learning model that predicts this accepted answer to a forum question using historical forum data.	Which Answer is Best?: Predicting Accepted Answers in MOOC Forums	NA:NA:NA	2016
Agnes Sandor:Nikolaos Lagos:Ngoc-Phuoc-An Vo:Caroline Brun	In this paper we propose the detection of user issues and request types in technical forum question posts with a twofold purpose: supporting up-to-date knowledge generation in organizations that provide (semi-) automated customer-care services, and enriching forum metadata in order to enhance the effectiveness of search. We present a categorization system for detecting the proposed question post types based on discourse analysis, and show the advantage of using discourse patterns compared to a baseline relying on standard linguistic features. Besides the detailed description of our method, we also release our annotated corpus to the community.	Identifying User Issues and Request Types in Forum Question Posts Based on Discourse Analysis	NA:NA:NA:NA	2016
Saeedeh Shekarpour:Kemele M. Endris:Ashwini Jaya Kumar:Denis Lukovnikov:Kuldeep Singh:Harsh Thakkar:Christoph Lange	Question Answering (QA) systems are becoming the inspiring model for the future of search engines. While, recently, datasets underlying QA systems have been promoted from unstructured datasets to structured datasets with semantically highly enriched metadata, question answering systems are still facing serious challenges and are therefore not meeting users' expectations. This paper provides an exhaustive insight of challenges known so far for building QA systems, with a special focus on employing structured data (i.e. knowledge graphs).It thus helps researchers to easily spot gaps to fill with their future research agendas.	Question Answering on Linked Data: Challenges and Future Directions	NA:NA:NA:NA:NA:NA:NA	2016
Priyanka Singh:Elena Simperl	The expert based question and answering forums are crowdsourced and rely on people to provide answers for questions. This paper focuses on technology based Q&A systems like StackOverflow and Reddit. These websites are popular and yet many questions remain unanswered. The Suman system uses semantic keyword search in combination with traditional text search techniques to find similar questions with answers for unanswered questions. Furthermore, the Suman system also recommends experts who can answer those questions. This helps to narrow down the long tail of unanswered questions. The Suman system utilises Semantic Web and Linked Data technologies to integrate the datasets from two websites, structure them and link them to Linked Data Cloud. It uses available tools to solve name entity disambiguation problem and expands the query term with added semantics. The Suman system was evaluated and results were analysed to show its viability.	Using Semantics to Search Answers for Unanswered Questions in Q&A Forums	NA:NA	2016
Gianmarco De Francisci Morales:Luca Maria Aiello:Symeon Papadopoulos:Haewoon Kwak	It is our great pleasure to welcome you to the Third Workshop on Social News on the Web -- SNOW 2016 -- held in conjunction with the WWW 2016 conference on April 12th 2016 in Montreal, Canada. In recent years, the topics addressed by SNOW have become very popular among diverse scientific communities. Computer scientists have studied how information spreads and diffuses in social networks. Journalists, social scientists, and economists are typical professionals who can improve their respective fields and practice by adopting technologies of interest to SNOW. In addition, there are emerging regulatory and legal issues that are of interest to law researchers and practitioners. The workshop aims to provide a forum to foster communication between these communities. In particular, the workshop has attracted a number of high-quality submissions on the relationships between online news and social media. With this workshop we aim at continuing a tradition of interdisciplinary exchange and cross-domain fertilization among different research communities. The goal of the workshop is to share novel ideas and to discuss future directions in the emerging areas of news search, news mining, news verification, and news recommendation. It especially focuses on the interplay between news content, generated by professional journalists, and social media content, generated by millions of users in real time and subject to social media dynamics. We also welcome investigations on the topics of citizen journalism and computational journalism. SNOW aspires to give researchers and practitioners a unique opportunity to share their perspectives with others interested in news and social media. The call for papers attracted 11 submissions from Asia, America, and Europe. The program committee accepted 6 papers that cover a variety of topics, including news credibility and verification, robot journalism, news consumption and social sharing, and news production on social media. The major criterion for the selection of papers was their potential to generate discussion and influence future research directions. The program also includes four keynote talks from distinguished experts in the field, with themes ranging from event understanding and narration to tracking misinformation, from curation of online news comments to the role of journalism on social media. We hope you will find this program interesting and thought provoking, and that the workshop will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.	Session details: SNOW'16	NA:NA:NA:NA	2016
Nicholas Diakopoulos	National news outlets routinely publish articles that attract hundreds and even thousands of user comments. These comments often provide valuable feedback and critique, personal perspectives, new information and expertise, and opportunities for discussion (not to mention profanity and vitriol). The varying quality of comments demands a high level of moderation and curatorial attention in order to cultivate a successful online community around news. Amongst publishers there is a growing awareness that finding and publicly highlighting high quality comments can in turn promote the general quality of the discourse. Further journalistic value can be gleaned by identifying and developing new sources of information and expertise from comments. In this talk I will present an editorially-aware visual analytics system called CommentIQ that supports moderators in curating high quality news comments at scale. The possibilities and ramifications of algorithmically infused social media moderation will be discussed in terms of journalistic ideals and norms of free speech and inclusion.	CommentIQ: Enhancing Journalistic Curation of Online News Comments	NA	2016
Filippo Menczer	As social media become major channels for the diffusion of news and information, they are also increasingly attractive and targeted for abuse and manipulation. This talk overviews ongoing network analytics, data mining, and modeling efforts to understand the spread of misinformation online and offline. I present machine learning methods to detect astroturf and social bots, and outline initial steps toward computational fact checking, as well as theoretical models to study how truthful and truthy facts compete for our collective attention. These efforts will be framed by a case study in which, ironically, our own research became the target of a coordinated disinformation campaign.	The Spread of Misinformation in Social Media	NA	2016
Devipsita Bhattacharya:Sudha Ram	Social media has emerged as a mechanism for online news propagation. This in turn has changed the competitive landscape of news providers, a landscape that was previously partitioned based on the traditional channels of news dispersion. The channels of news distribution refer to - television, newspaper, magazine, radio, news agency and online only. In this paper, we examine similarities and differences in news propagation patterns on social media based on the primary channel of a news provider. We collected news article propagation activity data from Twitter for 32 news providers over a three-week period and analyzed their propagation networks. Our analysis shows that the structural properties of the propagation networks are statistically different based on the type of primary channel. Our study has useful implications for understanding the competition between news providers in an online environment.	Understanding the Competitive Landscape of News Providers on Social Media	NA:NA	2016
Ram Meshulam:Roy Sasson	This paper analyzes two types of user interactions with online content: (1) private engagement with content, measured by page-views and click-through rate; and (2) social engagement, measured by the number of shares on Facebook as well as share-rate. Based on more than a billion data points across hundreds of publishers worldwide and two time periods, it is shown that the correlation between these signals is generally low. Potential reasons for the low correlation are discussed, and the notion of private-social dissonance is defined. A more in-depth analysis shows that the dissonance between private engagement and social engagement consistently depends on content category. Categories such as Sex, Crime and Celebrities have higher private engagement than social engagement. On the other hand, categories such as Books, Careers and Music have higher social engagement than private engagement. In addition to the offline analysis, a model which utilizes the different signals was trained and deployed on a live recommendation system. The resulting weights ranked the social signal lower than click-through rate. The results are relevant for publishers, content marketers, architects of recommendation systems and researchers who wish to use social signals in order to measure and predict user engagement.	For Your Eyes Only: Consuming vs. Sharing Content	NA:NA	2016
Pieter-Jan Ombelet:Aleksandra Kuczerawy:Peggy Valcke	Algorithmic processes that convert data into narrative news texts allow news rooms to publish stories with limited to no human intervention (Carlson, 2015, p. 416). The new trend creates many opportunities, but also raises significant legal questions. Aside from financial benefits, further refinement could make the smart algorithms capable of writing less standard, maybe even opinion, pieces. The responsible human merely needs to define clear questions about what the algorithm needs to discuss in the article and in what manner. But how does it square with the traditional rules of publishing, editorial control and the privacy and data protection framework? This paper analyses the legal implications when employing robot journalists. More specifically, the question of authorship for algorithmic output and the liability issues that could arise when the algorithmic output includes unlawful personal data processing as well as inaccurate, harmful or even illegal content will be assessed. The analysis is performed analyzing European legislation on copyright and data protection and applying Belgian legislation on press liability as a consistent country example to support certain legal considerations and conclusions. Furthermore, the paper answers the question as to how publishers could prevent the creation of inaccurate content by the algorithms they use.	Employing Robot Journalists: Legal Implications, Considerations and Recommendations	NA:NA:NA	2016
Georgios Rizos:Symeon Papadopoulos:Yiannis Kompatsiaris	The paper presents a framework for the prediction of several news story popularity indicators, such as comment count, number of users, vote score and a measure of controversiality. The framework employs a feature engineering approach, focusing on features from two sources of social interactions inherent in online discussions: the comment tree and the user graph. We show that the proposed graph-based features capture the complexities of both these social interaction graphs and lead to improvements on the prediction of all popularity indicators in three online news post datasets and to significant improvement on the task of identifying controversial stories. Specifically, we noted a 5% relative improvement in mean square error for controversiality prediction on a news-focused Reddit dataset compared to a method employing only rudimentary comment tree features that were used by past studies.	Predicting News Popularity by Mining Online Discussions	NA:NA:NA	2016
Stephen Schifferes	The recent transformation of news is one of a series of major revolutions in news delivery. By studying previous episodes we may find some clues as to the future of social news on the web.	Technological Transformations of News: A Long Term Perspective	NA	2016
Chengcheng Shao:Giovanni Luca Ciampaglia:Alessandro Flammini:Filippo Menczer	Massive amounts of misinformation have been observed to spread in uncontrolled fashion across social media. Examples include rumors, hoaxes, fake news, and conspiracy theories. At the same time, several journalistic organizations devote significant efforts to high-quality fact checking of online claims. The resulting information cascades contain instances of both accurate and inaccurate information, unfold over multiple time scales, and often reach audiences of considerable size. All these factors pose challenges for the study of the social dynamics of online news sharing. Here we introduce Hoaxy, a platform for the collection, detection, and analysis of online misinformation and its related fact-checking efforts. We discuss the design of the platform and present a preliminary analysis of a sample of public tweets containing both fake news and fact checking. We find that, in the aggregate, the sharing of fact-checking content typically lags that of misinformation by 10-20 hours. Moreover, fake news are dominated by very active users, while fact checking is a more grass-roots activity. With the increasing risks connected to massive online misinformation, social news observatories have the potential to help researchers, journalists, and the general public understand the dynamics of real and fake news sharing.	Hoaxy: A Platform for Tracking Online Misinformation	NA:NA:NA:NA	2016
Stefanie Wiegand:Stuart E. Middleton	Social media sources are becoming increasingly important in journalism. Under breaking news deadlines semi-automated support for identification and verification of content is critical. We describe a large scale content-level analysis of over 6 million Twitter, You Tube and Instagram records covering the first 6 hours of the November 2015 Paris shootings. We ground our analysis by tracing how 5 ground truth images used in actual news reports went viral. We look at velocity of newsworthy content and its veracity with regards trusted source attribution. We also examine temporal segmentation combined with statistical frequency counters to identify likely eyewitness content for input to real-time breaking content feeds. Our results suggest attribution to trusted sources might be a good indicator of content veracity, and that temporal segmentation coupled with frequency statistical metrics could be used to highlight in real-time eyewitness content if applied with some additional text filters.	Veracity and Velocity of Social Media Content during Breaking News: Analysis of November 2015 Paris Shootings	NA:NA	2016
Sir Nigel Shadbolt:Elena Simperl:Thanassis Tiropanis	It is our great pleasure to welcome you to the 4th International Workshop on the Theory and Practice of Social Machines, associated with WWW 2016. The workshop is a continuation of 3rd International Workshop on the Theory and Practice of Social Machines (SOCM2015), held at WWW2015. Continuing from previous years' "Theory and Practice of Social Machines" workshops at WWW2013, 2014, and 2015, the 2016 edition of the SOCM workshop will look deeply at social machines that have, or may yet soon have, a profound impact on the lives of individuals, businesses, governments, and the society as a whole. Our goal is to discuss issues pertinent to the observation of both extant and yet unrealized social machines building on work of the Web Observatory Workshops of the last two years (WOW2013 and WOW2014). SOCM2016 aims to identify factors that govern the growth or impede these systems to develop, and to identify unmet observation needs or the kinds of loosely-coordinated distributed social systems the Web enables. We also intend to discuss methods to analyze and explore social machines, as essential mechanisms for deriving the guidelines and best practices that will inform the design of social machine observatories. The workshop fosters multidisciplinary discussion on the following areas: Analyzing social machines: analytics and visualisations that provide insights about social machines and their impact. Designing social machine observatories: analyses of the design of effective (extant and future) social machines. Methodology and methods: papers describing approaches and methods for observing social machines. Philosophical Theories and Framework: describing and analysing the philosophical implications of social machines. The call for papers attracted submissions from United States and Europe. The program committee reviewed and accepted the following: Paper Submissions Reviewed - 8 Accepted - 8.	Session details: SOCM'16	NA:NA:NA	2016
Dirk Ahlers:Patrick Driscoll:Erica Löfström:John Krogstie:Annemie Wyckmans	Smart Cities denote a stronger integration of information technology into the organisation of a city and the interaction and participation of its citizens. In developing the concept further, we propose to understand Smart Cities through the lens of Social Machines and thus stronger focus on the city as a socio-technical construct. We draw from an interdisciplinary background of computer science and urban planning to reexamine and combine existing theories and find a common understanding. We substantiate our claim to the validity of the concept of Smart-City-as-a-Social-Machine with a thorough literature study and comparison. We discuss the resulting system complexity issues and ways to address them. We further propose areas where this understanding can be useful in furthering research on both the Smart City and the Social Machine topics.	Understanding Smart Cities as Social Machines	NA:NA:NA:NA:NA	2016
Sally A. Applin:Michael D. Fischer	As humans become more and more immersed in a networked world of connected and mobile devices, cooperation and sociability to achieve valued outcomes within geographic locales appears to be waning in favour of extended personal networks and interaction using semi-automated agents to support communications, transportation and other services. From a messaging structure that is complex, multiplexed and much of the time asynchronous, conditions emerge that disrupt symmetry of information exchange. People thus encounter circumstances that seem unpredictable given the information available to them, resulting in limited or failed cooperation and consequent quality of outcomes. We explore the role of Social Machines to support, change, and enhance human cooperation within a blended reality context.	Exploring Cooperation with Social Machines	NA:NA	2016
Caroline A. Halcrow:Leslie Carr:Susan Halford	Online/offline community (O/OC), the integrated performance of community in a blend of online/offline activities is increasingly prevalent as online systems organise, mediate and broadcast forms of communal engagement. O/OCs are social machines where the focus is on the social achievement, rather than the computational outcomes, of the combined human-technical infrastructure. An O/OC model SPENCE is proposed as an analytical tool for describing social machines from the perspective of sociality. Twitter is a technical infrastructure and social network of shared online/offline community phenomena that is also a social machine combining social participation with conventional forms of machine-based computation. Drawing from the extensive Twitter research literature, a sample of papers are analysed against SPENCE, demonstrating the clarity of the organisation of inter-relating themes of a range of perspectives in current Twitter research. It is concluded that SPENCE provides a lens of synthesis for the sociality dimension of a social machine and can be used in taxonomic activities (such as the social machines observatory) to differentiate social machines.	Using the SPENCE Model of Online/Offline Community to Analyse Sociality of Social Machines	NA:NA:NA	2016
Aastha Madaan:Thanassis Tiropanis:Srinath Srinivasa:Wendy Hall	The Web observatory is proposed as a global catalogue for sharing data-sets and analytic applications to support researchers from a variety of disciplines for analysing huge amount of research data for Web Science research. However, often these users fail to understand various transformations and consequences of complex data processing involved in a data analytic application. Therefore, there is a need to enable these users develop and re-use analytic applications on web observatory. In this study, we propose formal design patterns called "Observlets" for analytic applications to "observe" various web phenomena. The observlets provide abstract definitions for intermediate analysis required for a data analytic application. The users can share observlets across distributed web observatory nodes. The observlets are aimed to enhance end-users' awareness and engagement on web observatory and support programmers for innovating various data analytic applications.	Observlets: Empowering Analytical Observations on Web Observatory	NA:NA:NA:NA	2016
Paul Matthews	This paper presents research into the way norms are developed, expressed and enforced on sites that are part of the Stack Exchange (SE) social question-answering network. This network has a number of topical knowledge exchange communities using similar underlying software, enabling a focus on variation in social design. SE also separates community-related discussion from topic-specific content through the use of its "Meta" sub-sites. These were analysed together with their main sites for variation in the development and enforcement of norms. Norms expressed through explicit community policies seem rather less important than those embodied in busy discussion threads on the Meta sites. While Meta participation was fairly uniform across communities, different emphasis on scope and quality led to variation in Meta discussion and the way that norms were enacted through question closures. The social distribution of moderation work was also uneven between sites, with some sites having a few highly active moderators involved in question closure. The level of closures across the sites studied did not seem to significantly discourage participation. Indeed, modelling the effect of closures on quality and engagement indicated that low levels of closure enable "legitimate peripheral participation", the process by which newcomers can become inducted and make contributions of increasing quality over time.	Going Meta: Norm Formation and Enactment on the Stack Exchange Network	NA	2016
Arpit Merchant:Tushant Jha:Navjyoti Singh	Trust plays an important role in the effective working of Social Machines by allowing for cooperative behaviour amongst human and digital components of the system. A detailed study of trust helps in gaining insights into the working of social machines, and allows designers to create better systems which are able to engage more people and allow for efficient operations. In this paper, we undertake a discussion on the variety of ways in which trust can be observed in Social Machines by outlining a three class taxonomy (personal, social and functional). We build upon earlier observations in past literature while seeking a broader definition. We discuss the problem of trust, that of promoting trust amongst the trustworthy in social machines, and present the various insights, challenges and frontiers that arise in response. This includes the role of institutions, communication processes and value aligned technologies in social, personal and functional trust respectively.	The Use of Trust in Social Machines	NA:NA:NA	2016
Shivani Poddar:Sindhu Kiranmai Ernala:Navjyoti Singh:Ashin Samvara	It is the activities of individuals that lead to formation and changes within any social system. Hence, the "social" component in a social machine (socio-technical machine) can be understood constructively from the conception of an individual as a machine. In this work, we present a stochastic finite state machine model of an individual based on Abhidhamma tradition of Buddhism. The machine models moment to moment states of consciousness of an individual in terms of the Buddhist formal ontology that constitutes an individual. Thus, the key contribution of our research is a ubiquitous framework of an individual which unifies the idea of a human agent across all possible social machines. It is shown that from web data of a particular individual this machine can be populated. We expound how our model solves issues pertaining to varied temporal granules and sparsity of data. We further illustrate through an example as to how our approach can unify the conceptualizations of an individual from the numerous ideologies and definitions of a social machine. As a part of our future work, we hope to align this proposed stochastic machine with social observatories on internet.	Towards a Ubiquitous Model of an Individual in Social Machines	NA:NA:NA:NA	2016
Jun Zhao:Reuben Binns:Max Van Kleek:Nigel Shadbolt	Privacy protection is one of the most prominent concerns for web users. Despite numerous efforts, users remain powerless in controlling how their personal information should be used and by whom, and find limited options to actually opt-out of dominating service providers, who often process users information with limited transparency or respect for their privacy preferences. Privacy languages are designed to express the privacy-related preferences of users and the practices of organisations, in order to establish a privacy-preserved data handling protocol. However, in practice there has been limited adoption of these languages, by either users or data controllers. This survey paper attempts to understand the strengths and limitations of existing policy languages, focusing on their capacity of enabling users to express their privacy preferences. Our preliminary results show a lack of focus on normal web users, in both language design and their tooling design. This systematic survey lays the ground work for future privacy protection designs that aim to be centred around web users for empowering their control of data privacy.	Privacy Languages: Are we there yet to enable user controls?	NA:NA:NA:NA	2016
Kristine Maria Gloria:Stéphane B. Bazan:Su White	It is our great pleasure to welcome you to the 1st Workshop on "Web Education": Teaching Digital Literacies associated with WWW 2016. The dynamics of Web Education and Digital Literacies are among today's most important issues surrounding the development of the Web as an efficient, safe and universal information system. A wide range of disciplines including sociology, economics, political studies, health and management science have integrated courses and specializations to teach about the Web, its nature, its realities, its impact its evolution and its integration into every dimension of human activity. The workshop will gather a very broad community of participants: professors involved in digital literacy programs or courses, consultants empowering employees in a company, students or faculty in an interdisciplinary program or activists in an NGO teaching the Web to kids. The call for short papers attracted submissions from Asia, Europe and Canada. The program committee has reviewed and accepted 7 submissions. The workshop will also be preceded and followed by online activities on the Bookwitty.com platform. These activities aim to not only strengthen links within the Web Education Community, but also to gather and present the outcomes of the workshop.	Session details: TeachWeb'16	NA:NA:NA	2016
Elisabeth Coskun:Su White	This text describes a project which aims to explore the scope of the discipline Web Science; an emerging subject which is fundamentally inter-disciplinary. There are very few definitive subject definitions currently available for Web Science. Additionally, the nature of the subject is constantly evolving as an increasing number of different disciplines begin to practice what might identifiably be called Web Science. This potentially provides educators and students with a problem; how do you teach or learn about Web Science when there is no clear definition? This text provides a brief overview of a PhD project, the final aim of which involves the emergence of a framework for a working definition of Web Science. This will be achieved by an examination and overview of current existing Web Science curricula, as well as available Web Science literature.	Emerging a Web Science Curriculum	NA:NA	2016
Lisa J. Harris:Nicholas S.R. Fair:Sarah Hewitt	This paper introduces the theoretical framework and design rationale for an innovative undergraduate module entitled "Living and Working on the Web" at the University of Southampton. The module design is based on the principles of collaborative social learning and the co-construction of knowledge. At the workshop a model of best practice will be presented, featuring a "blog-comment-reflect-feedback" cycle, which has derived from the synthesis of relevant literature and which will be reflected upon through an informal content analysis of the students' blogs. One of the problems facing this type of curriculum innovation is the difficulties faced when scaling up such modules to very large student groups, particularly in relation to feedback and assessment. To date, the single largest cohort has been 45 students. It is therefore also the intention of the presenters to engage the attendees in a discussion of how a module such as this could be feasibly extended into far larger cohort groups.	Collaborative Social Learning: Rewards and Challenges in Mainstream Higher Education	NA:NA:NA	2016
Ricardo Hoar:Randy Connolly	Web development is widely considered to be a difficult topic to teach successfully within post-secondary computing programs. One reason for this difficulty is the large number of shifting technologies that need to be taught along with the conceptual complexity that needs to be mastered by both student and professor. Another challenge is helping students see the scope of web development, and their role in an era where the web is a part of everyday human affairs. This paper describes our 2014 textbook [2], its reception, and our plans for a second edition revision (which will be published in early 2017). Our hope is that a discussion of current teaching materials will provide opportunities to spark a dialogue not only about current technological topics, but perspectives on web development from other disciplines.	The Garden of Earthly Delights: Constructing and Revising a Web Development Textbook	NA:NA	2016
Andreja Istenič Starčič:Žiga Turk	In this paper, we discuss digital literacy and preservice teacher education and reflect on the current state of web-based education and development of digital literacy in teacher education in Slovenia. The literacy context is discussed in the context of the Educational Technology course which is delivered in teacher education. The aim of this course for preservice preprimary and primary classroom teachers is to develop student teachers' digital literacy and prepare them for the efficient integration of ICT into their teaching. This will in turn influence learners' digital literacy and competence for active engagement in the emerging culture of participation. The paper discusses web-based teaching methodology with a focus on instructional design, learning resources and high-order learning outcomes. The affordance of mobile technology fosters ubiquitous learning which has been integrated into the teacher education curriculum. The notion of tools in learning and literacies is discussed in the context of the transition from traditional written culture to digital culture. In integrating mobile learning, three important dimensions converge, underlining the development of digital literacy: the technology dimension of wireless mobile providing instant access, the social dimension and the learning behaviours dimension. A survey was conducted to examine undergraduate student-teachers' attitudes on the application of ubiquitous education and the development of digital literacy through the integration of mobile learning. The results indicate that student-teachers have developed competences in a variety of mobile learning and teaching activities They believe that mobile technology increases connection between learner and teacher but are neutral about the integration of children's social practices from their free time to school environment.	Ubiquitous Learning and Digital Literacy Practices Connecting Teacher and Learner	NA:NA	2016
Brigitte Jellinek	In this position paper, we describe our experience in designing and delivering a bachelor and a master program in web development at Salzburg University of Applied Sciences, Austria. While aiming to archive similar learning objectives as other programs in web development or web engineering, our historical roots in an arts program and our decision to focus on dynamic programming languages have led us to a unique program.	Experiences with Curricula for a BSc and MSc in Web Development: 2008-2016	NA	2016
Kate Mori:Lucy Ractliffe	This paper evaluates the effectiveness of a massive open online course (MOOC) as a professional development tool in higher education. The transition from the MOOC's initial intended use as a low cost way for students to access education and aid their studies has evolved to facilitate continuing professional development (CPD), particularly within the commercial sector [1]. Findings from this study indicate there is an increase in participation and satisfaction amongst higher education staff who undertook a MOOC compared to attending traditional staff development days. Recommendations from this study?s findings highlight that staff were keen to engage with the MOOC format, but felt they needed face-to-face meetings as well to reinforce, contextualize and discuss the key messages of the MOOC. In addition to this, time allocation within workloads should be considered for any future inclusion of MOOCs for staff development.	Evaluating the use of a MOOC within Higher Education Professional Development Training	NA:NA	2016
Xiaoxuan Wang:Jiale Gao	People who design and develop web product need interdisciplinary knowledge and skills from various fields. Thus in most Chinese vocational colleges, students are lack of a completed experience to fulfill employers' needs before they leave colleges. In this paper, we propose a project-based approach to design the course, in which students unify business, product design and development theories into one project practice to build the web product. Over different stages and increasingly complexity, students learn how to interact with customers, build project management skills, design product with business mind instead of focusing on design aesthetics and developing programming skills only.	The Practice of Web Product Design and Development Course Design	NA:NA	2016
Marc Spaniol:Ricardo Baeza-Yates:Julien Masanàs	Time is a key dimension to understand the Web. It is fair to say that it has not received yet all the attention it deserves and TempWeb is an attempt to help remedy this situation by putting time as the center of its reflection. Studying time in this context actually covers a large spectrum, from the extraction of temporal information and knowledge, to diachronic studies for the design of infrastructural and experimental settings enabling a proper observation of this dimension. For its sixth edition, TempWeb accepted six out of eleven submissions for oral presentation. We interpret the high quality of the submissions and the frequent contributors to TempWeb, as indicators of an evolving community. It shows a clear sign of a positive dynamic in the study of time in the scope of the Web and evidence of the relevance of this effort. The workshop proceedings are published by ACM DL as part of the WWW 2016 Companion Publication. We hope you will find in these papers as well as the keynotes of Wolfgang Nejdl (L3S Hanover, Germany) and Omar Alonso (Microsoft, USA), and the discussion and exchanges of this edition of TempWeb, some motivations to look more into this important aspect of the Web. TempWeb 2016 was jointly organized by Caen University (Caen, France), Yahoo Labs (Sunnyvale, USA) and Internet Memory Foundation (Paris, France).	Session details: TempWeb'16	NA:NA:NA	2016
Omar Alonso	There is new and exciting research work in analyzing and exploiting information from corpora with temporal information such as news, email, or social media. We review some of the current activities around extracting temporal information from social media and present a number of examples from real-world systems. We also outline a number of open problems and potential new research areas.	Time to ship: Some Examples from the Real-World	NA	2016
Erdal Kuzey:Jannik Strötgen:Vinay Setty:Gerhard Weikum	For many NLP and IR applications, anchored temporal information extracted from textual documents is of utmost importance. Thus, temporal tagging -- the extraction and normalization of temporal expressions -- has gained a lot of attention in recent years and several tools such as HeidelTime and SUTime are proposed. However, such tools do not address textual phrases with temporal scopes like "Clinton's time as First Lady". While such phrases (so-called temponyms) are not temporal expressions per se, information about their temporal scopes can be helpful in many scenarios, e.g., in the context of temporal information retrieval. In this paper, we describe the integration of a wide range of temponyms to the publicly available temporal tagger HeidelTime to include temponym tagging.	Temponym Tagging: Temporal Scopes for Textual Phrases	NA:NA:NA:NA	2016
Vera Zaychik Moffitt:Julia Stoyanovich	Graphs are used to represent a plethora of phenomena, from the Web and social networks, to biological pathways, to semantic knowledge bases. Arguably the most interesting and important questions one can ask about graphs have to do with their evolution. Which Web pages are showing an increasing popularity trend? How does influence propagate in social networks? How does knowledge evolve? In this paper we present our ongoing work on the Portal system, an open-source distributed framework for evolving graphs. Portal streamlines exploratory analysis of evolving graphs, making it efficient and usable, and providing critical tools to computational and data scientists. Our system implements a declarative query language by the same name, which we briefly describe in this paper. Our basic abstraction is a TGraph, which logically represents a series of adjacent snapshots. We present different physical representations of TGraphs and show results of a preliminary experimental evaluation of these physical representations for an important class of evolving graph analytics.	Towards a Distributed Infrastructure for Evolving Graph Analytics	NA:NA	2016
Aécio Santos:Bruno Pasini:Juliana Freire	While much work has been devoted to understanding Web dynamics and using this knowledge to efficiently maintain the freshness of the indexes of generic search engines, the same is not true for domain-specific indexes constructed by focused crawlers. For the latter, the problem is compounded by the fact that it is important not only to maintain already-crawled pages fresh, but also to identify new relevant content and expand the collection. In this paper, we discuss the challenges involved in this problem and describe our preliminary efforts in building a testbed to better understand the dynamics of specific topics and characterize how they evolve over time. We propose a data collection methodology and a set of experiments to answer important questions about temporal dynamics and evolution of topics. We also present the results of the experimental analysis we carried out using data collected over a period of four weeks using two distinct topics. These results suggest that topic-specific refreshing strategies can be beneficial for focused crawlers.	A First Study on Temporal Dynamics of Topics on the Web	NA:NA:NA	2016
Mohsen Shahriari:Stephen Gunashekar:Marven von Domarus:Ralf Klamma	Digital media has some observable traces named communities. Several events such as split, merge, dissolve and survive happen to communities in social media. But what are significant features to predict these events? And to which extent a feature is relevant in a social media? To answer these questions, we perform a study on community evolution analysis and prediction. We employ three overlapping community detection (OCD) algorithms from literature to the case of time-evolving networks including social, email communication and co-authorship networks. Group evolution discovery (GED) technique is applied to track the identified communities. We compare structural properties of OCD algorithms and investigate most persistent communities over time. Furthermore, static and temporal features of a community are applied to build a logistic classifier for community evolution prediction (CEP). Results reveal important features to predict events happening to a community.	Predictive Analysis of Temporal and Overlapping Community Structures in Social Media	NA:NA:NA:NA	2016
Matthias Steinbauer:Gabriele Anderst-Kotsis	Graph models have a long standing history as models for real world structures and processes. In recent research two important dimensions of graphs are described of particular importance. (1) Temporal aspects of graphs cannot be neglected for many current application scenarios such as social network analysis or the analysis of the global web graph. (2) The mentioned graph structures have grown to very large sizes such that traditional methodologies no longer hold. In this work a distributed computing framework designed for storing and processing of large-scale temporal graphs is presented. For this system a reference implementation in Java was created. In this paper first insight on the implementation and observations in using the system are discussed.	DynamoGraph: A Distributed System for Large-scale, Temporal Graph Processing, its Implementation and First Observations	NA:NA	2016
Staffan Truvé	Recorded Future has developed its Temporal Analytics Engine as a general purpose platform for harvesting and analyzing unstructured text from the open, deep, and dark web, and for transforming that content into a structured representation suitable for different analyses. In this paper we present some of the key components of our system, and show how it has been adapted to the increasingly important domain of cyber threat intelligence. We also describe how our data can be used for predictive analytics, e.g. to predict the likelihood of a product vulnerability being exploited or to assess the maliciousness of an IP address.	Temporal Analytics for Predictive Cyber Threat Intelligence	NA	2016
Bettina Berendt:Laura Hollink:Markus Luczak-Roesch	It is our great pleasure to welcome you to the 6th International Workshop on Usage Analysis and the Web of Data (USEWOD), associated with WWW 2016. The workshop is dedicated to the diverse ecosystem of Web of Data access mechanisms. From academic to government data, from complex SPARQL queries to Linked Data Fragments, from DBpedia to Wikidata: mthe data sources on the Web of Data and the ways in which these sources can be created and consumed vary greatly and raise fundamental questions. The call for papers attracted submissions from United States, Canada, Asia, and Europe. The program committee reviewed and accepted the following: Venue or Track Reviewed -4 Accepted - 2. Additionally, we decided to publish an invited paper that presents a particularly interesting crossdisciplinary perspective on the Web of Data and outline current research directions and challenges in an extended 'Message from the USEWOD Chairs'. The USEWOD 2016 Research Dataset As in previous years, a standard research dataset of usage data from well-recognized Web-of-Data datasets has been published to promote reproducible research on the workshop themes. A particular highlight of this year's dataset is overlapping usage data from the official DBpedia servers as well as the Linked Data Fragments interface to DBpedia and Wikidata. This dataset allows researchers to study alternative Web of Data usage mechanisms in an unprecedented way and could therefore become a unique resource of great importance for the field. For more information on the datasets released in previous years, please see http://usewod.org/data-sets.html. Special thanks got to Open Link Software for providing us with DBpedia logs as well as Ruben Verborgh for access to Linked Data Fragments usage data.	Session details: USEWOD'16	NA:NA:NA	2016
Nathalie Casemajor	This paper analyses the specificities of metadata embedded in photographic images. It investigates how embedded metadata can help studying the usage patterns and conditions of circulation of images on digital networks.	Embedded Metadata and the Digital Lifecycle of Images: Methodological Challenges	NA	2016
Pieter Colpaert:Alvin Chua:Ruben Verborgh:Erik Mannens:Rik Van de Walle:Andrew Vande Moere	In the field of smart cities, researchers need an indication of how people move in and between cities. Yet, getting statistics of travel flows within public transit systems has proven to be troublesome. In order to get an indication of public transit travel flows in Belgium, we analyzed the query logs of the iRail API, a highly expressive route planning API for the Belgian railways. We were able to study 100k to 500k requests for each month between October 2012 and November 2015, which is between 0.56% and 1.66% of the amount of monthly passengers. Using data visualizations, we illustrate the commuting patterns in Belgium and confirm that Brussels, the capital, acts as a central hub. The Flemish region appears to be polycentric, while in the Walloon region, everything converges on Brussels. The findings correspond to the real travel demand, according to experts of the passenger federation Trein Tram Bus. We conclude that query logs of route planners are of high importance in getting an indication of travel flows. However, better travel intentions would be acquirable using dedicated HTTP POST requests.	What Public Transit API Logs Tell Us about Travel Flows	NA:NA:NA:NA:NA:NA	2016
Choudur Lakshminarayan:Ram Kosuru:Meichun Hsu	As the website is a primary customer touch-point, millions are spent to gather web data about customer visits. Sadly, the trove of data and corresponding analytics have not lived up to the promise. Current marketing practice relies on ambiguous summary statistics or small-sample usability studies. Idiosyncratic browsing and low conversion (browser-to-buyer) make modeling hard. In this paper, we model browsing patterns (sequence of clicks) via Markov chain theory to predict users' propensity to buy within a session. We focus on model complexity, imputing missing values, data augmentation, and other attendant issues that impact performance. The paper addresses the following aspects; (1) Determine appropriate order of the Markov chain (assess the influence of prior history in prediction), (2) Impute missing transitions by exploiting the inherent link structure in the page sequences, (3) predict the likelihood of a purchase based on variable-length page sequences, and (4) Augment the training set of buyers (which is typically very small: 2% by viewing the page transitions as a graph and exploiting its link structure to improve performance. The cocktail of solutions address important issues in practical digital marketing. Extensive analysis of data applied to a large commercial web-site shows that Markov chain based classifiers are useful predictors of user intent.	Modeling Complex Clickstream Data by Stochastic Models: Theory and Methods	NA:NA:NA	2016
Markus Luczak-Roesch:Laura Hollink:Bettina Berendt	Usage mining always was and still is a key topic for research in the context of the Web [16]. This is evidenced by the series of papers that appear in the scientific tracks of the WWW conference year by year. Web usage is being studied to create economic value by placing targeted ads or delivering personalized content, but also in order to better understand how people behave online in mass movements and collective action.	Current Directions for Usage Analysis and the Web of Data: The Diverse Ecosystem of Web of Data Access Mechanisms	NA:NA:NA	2016
Jacqueline Bourdeau:Bebo White:Irwin King	It is our great pleasure to welcome you to the ACM WWW2016 Workshop on Web Science and Technology for Education (WebED2016), co-located with the 2016 International WWW Conference. This workshop series began as The Workshop on Web-based Education Technologies (WebET) at WWW2014 in Seoul, Korea. However, this year's workshop has expanded its scope to explore the influence the growing field of Web Science. By doing so it is our goal to bring together educational technologists, Web researchers, and members of social science communities seeking to investigate the impact of Web technology on teaching and learning. The mission of the workshop is for attendees to share novel solutions that fulfill the needs of heterogeneous applications and environments and identify new directions for future research and development. It is also our hope that WebED2016 attendees might identify others with similar interests possibly leading to new collaborations and joint efforts. We encourage workshop attendees to attend the keynote speaker presentation, the accepted paper presentations, and the expert panel discussion. Keynote: "Web Science, Social Media and Education," Dame Wendy Hall, University of Southampton, Panel: "Evaluating Educational Software in the Web Era," Jutta Treviranus (Ontario College of Art and Design University), Jean-Philippe Bradette (Ellicom), Irwin King (The Chinese University of Hong Kong), Beverly Woolf (University of Massachusetts Amherst), and Irina Muhina (iecarus, moderator)	Session details: WEBED'16	NA:NA:NA	2016
Wendy Hall	Over the last 25 years the Web has evolved into a critical global infrastructure. Since its emergence in the 1990s, it has exploded into hundreds of billions of pages that touch almost all aspects of modern life. Little appreciated, however, is the fact that the Web is more than the sum of its pages and it is more than its technical protocols. Vast emergent properties have arisen that are transforming society. Web Science is the study of the Web as a socio-technical system. As the Web becomes increasingly significant in all our lives, studying it from an interdisciplinary perspective becomes even more important. We are now rapidly moving into a world of data on and about the Web, which gives rise to even more opportunities and challenges. In this talk we will explore the role of observatories and data analytics for the development of new methodologies for longitudinal research in Web Science that could help us understand more about how the Web evolves as a social-technical network. After many years of speculation about the potential of on-line learning, web technology has finally developed to a point where on-line learning is today a reality -- from MOOCS to complete on-line degree courses. All rely on the use of social media to enhance the learning environment for the students. We will discuss the application of the web observatory approach to the study of on-line learning and the insights such an approach can reveal that would not be possible in a traditional learning environment -- potentially giving rise to world-wide studies in this area.	Web Science, Social Media and Education	NA	2016
Rubiela Carrillo:Elise Lavoué:Yannick Prié	Learning Sciences argue that student engagement is composed of behavioral, motivational and cognitive dimensions. Many proposals in Learning Analytics have provided teachers with quantitative indicators focusing only on students' behaviors, such as the number and the duration of their actions with the learning environment. In this paper, we propose visual representations of cognitive indicators to add explanatory elements to behavioral indicators. We describe our general architecture for collecting and aggregating data used to build the proposed visualizations. We illustrate the use of these indicators in various pedagogical scenarios oriented towards supporting teachers in students' actions and performances understanding.	Towards Qualitative Insights for Visualizing Student Engagement in Web-based Learning Environments	NA:NA:NA	2016
Hou Pong Chan:Tong Zhao:Irwin King	Massive Open Online Coursers (MOOCs) offer a convenient way for people to access quality courses via the internet. However, the problem of grading open-ended assignments at such a large scale still remains challenging. Although peer assessment have been proposed to handle the large-scale grading problem in MOOCs, existing methods still suffer several limitations: (1) most current peer assessment research ignore the importance of how to allocate the assessment tasks among peers, (2) existing approaches for peer grading learn the complete ranking in an offline manner, (3) theoretical analysis for trust-aware peer grading is missing. In this work, we consider the case that we have prior knowledge about all students' reliability. We formulate the problem of peer assessment as a sequential noisy ranking aggregation problem. We derive a trust-aware allocation scheme for peer assessment to maximize the probability of constructing a correct ranking of assignments with a budget constraint.Moreover, we also derive an upper bound for the probability of prediction error on the inferred ranking of assignments. Furthermore, we propose the Trust-aware Ranking-based Multi-armed Bandit Algorithms to sequentially allocate the assessment tasks to the students based on the derived allocation scheme and learn an accurate peer grading result by taking students' reliability into consideration.	Trust-aware Peer Assessment using Multi-armed Bandit Algorithms	NA:NA:NA	2016
Mariheida Cordova-Sanchez:Pinar Yanardag	The use of micro-blogging in classrooms is a recently trending concept in computer-aided education. Micro-blogs offer an effective way of communication in large classrooms, and engage students in meaningful discussions. However, existing micro-blogging systems in education setting suffer from a few drawbacks. First, relevant content might be overwhelmed by irrelevant posts to the lecture which could jeopardize effective learning. Second, students might generate redundant content by posting similar questions to each other and create substantial information overload. Third, posts covering different aspects of the class might be left undiscovered due to real-time characteristics of micro-blogs. To address these issues, we present a principled approach for picking a set of posts that promotes relevant and diverse content while effectively turning down the noise created by redundant posts. We formulate this task as a submodular optimization problem for which we provide an efficient and near-optimal solution. We evaluate our framework on real micro-blog based classroom datasets and our empirical results demonstrate that our framework is effectively able to cover the most important and diverse content that is being discussed in classrooms.	Turning Down the Noise in Classrooms	NA:NA	2016
Sivaldo J. de Santana:Hugo A. Souza:Victor A.F. Florentin:Ranilson Paiva:Ig Ibert Bittencourt:Seiji Isotani	In the last decade, many researchers have studied the use of game elements in education. The term "gamification" refers to the application of elements used in the development of video games, such as mechanics and dynamics in other contexts unrelated to games, in order to generate more enjoyable and positive attitudes from the students. The gamication process involves using several elements present in video games, like: points, levels, rankings, rewards (badges/achievements) and missions. In this study, we assess whether or not, gamification elements can help and motivate students enrolled in a gamified ontology-based adaptive online learning environment called MeuTutor. In this context, we followed the Pedagogical Recommendation Process to discover which gamification elements were relevant to promote learning, in order to recommend improvements to the environment. To do that, this study shows a quantitative analysis(correlation analysis) of the gamification elements from MeuTutor.	A Quantitative Analysis of the Most Relevant Gamification Elements in an Online Learning Environment	NA:NA:NA:NA:NA:NA	2016
Sergio Gutierrez-Santos:Stefano Capuzzi:Ken Kahn:Sokratis Karkalas:Alexandra Poulovassilis	We present and evaluate a web-based architecture for monitoring student-system interaction indicators in Exploratory Learning Environments (ELEs),using as our case study a microworld for secondary school algebra. We discuss the challenging role of teachers in exploratory learning settings and motivate the need for visualisation and notification tools that can assist teachers in focusing their attention across the class and inform teachers' interventions. We present an architecture that can support such Teacher Assistance tools and demonstrate its scalability to allow concurrent usage by thousands of users (students and teachers).	Scalable Monitoring of Student Interaction Indicators in Exploratory Learning Environments	NA:NA:NA:NA:NA	2016
Alexandra Luccioni:Roger Nkambou:Jean Massardi:Jacqueline Bourdeau:Claude Coulombe	In this paper, we describe an innovative project where Web technologies are exploited to develop an Intelligent Tutoring System (ITS) that uses a Learning Management System (LMS) as its learning interface. The resulting ITS has been instantiated into a specific system called STI-DICO which aims at helping future French primary school teachers to acquire the knowledge and skills needed to use the French dictionary. The learning process in the ITS takes place via a number of authentic learning scenarios that represent situations that the future teachers will face in the classroom. By using a LMS as the learning interface component of the system, we enable it to be directly deployable on the Web to a large population of students, all the while retaining the adaptive components of an ITS to deliver a personalized learning experience to its users.	STI-DICO: A Web-Based System for Intelligent Tutoring of Dictionary Skills	NA:NA:NA:NA:NA	2016
Bart Pursel:Chen Liang:Shuting Wang:Zhaohui Wu:Kyle Williams:Benjamin Brautigam:Sherwyn Saul:Hannah Williams:Kyle Bowen:C. Lee Giles	We describe BBookX, a web-based tool that uses a human-computing approach to facilitate the creation of open source textbooks. The goal of BBookX is to create a system that can search various Open Educational Resource (OER) repositories such as Wikipedia, based on a set of user-generated criteria, and return various resources that can be combined, remixed, and re-used to support specific learning goals. As BBookX is a work-in-progress, we are in the midst of a design-based research study, where user testing guided multiple rounds of iteration in the design of the user interface (UI) as well as the query engine. From an interface perspective, the challenges we present are the matching of the UI to users' mental models from similar systems, as well as educating users how to best work with the algorithms in an iterative manner to find and refine content for inclusion into open textbooks.	BBookX: Design of an Automated Web-based Recommender System for the Creation of Open Learning Content	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2016
Rajendra Akerkar:Pierre Maret:Laurent Vercouter	It is with great pleasure, and on behalf of the organizing committee, we would like to welcome you to the 8th International Workshop on Web Intelligence & Communities (WI&C 2016) taking place on April 11th in Montreal, Canada and collocated with the WWW 2016 conference. This workshop, the eighth in a series of workshops, is intended to stimulate discussions on the forefront of research concerned with web intelligence applied to collaborative networks. Web Intelligence consists of a multidisciplinary area dealing with exploiting data and services over the Web, to create new data and services using both Information and Communication Technologies (ICT) and Artificial Intelligence (AI) techniques. Communities appear as a first-class object in the areas of web intelligence and agent technologies, as well as a crucial crossroads of several sub-domains (i.e. user modelling, protocols, data management, data mining, content modelling, etc.). These sub-domains impact the nature of the communities and the applications which are related to them. These applications are numerous, and the success of well-known Social Network Sites for entertainment should not be allowed to over-shadow the other application domains, for instance in education, health, design, knowledge management, and so forth. The workshop will provide presentation and discussion opportunities for researchers working on web intelligence applied to collaborative networks, such as virtual communities. The possibilities and consequences of the web usage for collaborative networks are tremendous and new tools are required to satisfy users and service providers.	Session details: WI&C'16	NA:NA:NA	2016
Babak Esfandiari:Alan Davoust	Social networks can play an important role in the process of decentralizing authority in distributed systems. We will focus on distributed wiki systems, and we show how, in the special case of a peer-to-peer wiki, there is a rational incentive for users to self-organize and form a meaningful social network. We discuss to that effect the basic metrics that can be derived from the topology of the social network to help assess the subjective quality of wiki entries. Demos and experimental results will illustrate and support our discussion. We finally speculate as to how these results may also translate to discussion forums or recommender systems.	Distributed Wikis and Social Networks: A Good Fit	NA:NA	2016
Alexandr Chernov:Nikolaos Lagos:Matthias Gallé:Ágnes Sándor	The World Wide Web contains a large number of community created knowledge of instructional nature. Similarly, in a commercial setting, databases of instructions are used by customer-care providers to guide clients in the resolution of issues. Most of these instructions are expressed in natural language. Knowledge Bases including such information are valuable through the sum of their single entries. However, as each entry is created mostly independently, users (e.g. other community members) cannot take advantage of the accumulated knowledge that can be developed via the aggregation of related entries. In this paper we consider the problem of inter-linking Knowledge Base entries, in order to get relevant information from other parts of the Knowledge Base. To achieve this, we propose to detect \textit{actionable phrases} -- text fragments that describe how to perform a certain action -- and link them to other entries. The extraction method that we implement achieves an F-score of 67.35\%. We also show that using actionable phrases results in better linking quality than using coarser-grained spans of text, as proposed in the literature. Besides the evaluation of both steps, we also include a detailed error analysis and release our annotation to the community.	Enriching How-to Guides by Linking Actionable Phrases	NA:NA:NA:NA	2016
Pierre Maret:Rajendra Akerkar:Laurent Vercouter	Web-based community is a self-defined web-based network of interactive communication organized around a shared interest or purpose. It provides the means of interactions among people in which they create, share, and exchange information and ideas in virtual space and networks. Working with big data often requires querying and reasoning that data to isolate information of interest and manipulate it in various ways. This editorial paper explores recent big data research topics -- stream querying and reasoning -- over data from web based communities. It combines aspects from some well-studied research domains, such as, social network analysis, graph databases, and data streams. We provide a brief synopsis of some research issues in supporting reasoning and querying tasks. This editorial also presents the WI&C-16 workshop's goal and programme.	Web Communities in Big Data Era. Editorial	NA:NA:NA	2016
Lucas Fonseca Navarro:Estevam Rafael Hruschka, Jr.:Ana Paula Appel	With the exponentially growing amount of available data on the Web over the last years, several projects have been created to automatically extract knowledge from this information set. As the data domains on the Web are too wide, most of these projects store the acquired knowledge in ontological knowledge bases (OKBs). Mapping it into graph-based representation makes possible to apply graph-mining techniques to extract implicit information. However most of these projects treat the mapping process using different adjustments in several ways, thus, there is not a standard mapping process or a formal way defifined to do this task. In this paper we formally describe a graph structure called Ontological Network and how it can be used to map an Ontological Knowledge Base. We also show some graph-mining based algorithms to add new facts and to extend the ontology of an OKB while mapped into an Ontological Network as example.	Ontological Networks: Mapping Ontological Knowledge Bases into Graphs	NA:NA:NA	2016
Zaher Yamak:Julien Saunier:Laurent Vercouter	Various techniques are used to manipulate users in OSN environments such as social spam, identity theft, spear phishing and Sybil attacks... In this article, we are interested in analyzing the behavior of multiple fake accounts that try to bypass the OSN regulation. In the context of social media manipulation detection, we focus on the special case of multiple Identity accounts (Sockpuppet) created on English Wikipedia (EnWiki). We set up a complete methodology spanning from the data extraction from EnWiki to the training and testing of our selected data using several machine learning algorithms. In our methodology we propose a set of features that grows on previous literature to use in automatic data analysis in order to detect the Sockpuppets accounts created on EnWiki. We apply them on a database of 10.000 user accounts. The results compare several machine learning algorithms to show that our new features and training data enable to detect 99\% of fake accounts, improving previous results from the literature.	Detection of Multiple Identity Manipulation in Collaborative Projects	NA:NA:NA	2016
Qi Ye:Feng Wang:Bo Li	Query intent mining is a critical problem in various real-world search applications. In the past few years we have witnessed dramatic advances in the field of query intent mining area. In this paper, we present a practical system---StarrySky for identifying and inferring millions of query intents in daily sponsored search with high precision and acceptable coverage. We have already achieved great advantages by deploying this system in Sogou sponsored search engine\footnote {http://www.sogou.com}. The general architecture of StarrySky consists of three stages. First, we detect millions of fine-grained query clusters from two years of click logs which can represent different query intents. Second, we refine the qualities of query clusters with a series of well-designed operations, and call the final refined clusters as concepts. Third and foremost, we build a flexible real-time inference algorithm for assigning query intents to the detected concepts with high precision. Beyond the description of the system, we employ several experiments to evaluate its performance and flexibility. Our inference algorithm achieves up to 96% precision and 68% coverage on daily search requests. We believe StarrySky is a practical and valuable system for tracking query intents.	StarrySky: A Practical System to Track Millions of High-Precision Query Intents	NA:NA:NA	2016
Robert West:Leila Zia:Dario Taraborelli:Jure Leskovec	It is our great pleasure to welcome you to Wiki Workshop, a forum for bringing together researchers exploring all aspects of Wikipedia and other Wikimedia sites. Like the editing aspect of Wikimedia sites, research on Wikimedia projects relies heavily on the community of researchers who explore the projects to improve our understanding of the current state and future directions of such projects. This workshop aims to bring this community together on an annual basis and to welcome new community members. As part of the workshop, we will share the latest research on Wikimedia projects, explore and share new directions for research, learn about the new data-sets that have been released publicly, and initiate or continue on new research initiatives. The target audience will include researchers in: natural language processing web mining and data mining artificial intelligence and machine learning graph and network theory social computing user generated content industry social sciences linguistics The workshop is the second workshop in these series, proceeding the workshop held in ICWSM 2015. The list of speakers and program can be found at the workshop's website.	Session details: Wiki Workshop'16	NA:NA:NA:NA	2016
Paolo Boldi:Corrado Monti	We propose a novel general technique aimed at pruning and cleansing the Wikipedia category hierarchy, with a tunable level of aggregation. Our approach is endogenous, since it does not use any information coming from Wikipedia articles, but it is based solely on the user-generated (noisy) Wikipedia category folksonomy itself. We show how the proposed techniques can help reduce the level of noise in the hierarchy and discuss how alternative centrality measures can differently impact on the result.	Cleansing Wikipedia Categories using Centrality	NA:NA	2016
Freddy Brasileiro:João Paulo A. Almeida:Victorio A. Carvalho:Giancarlo Guizzardi	Wikidata captures structured data on a number of subject domains, managing, among others, the information underlying Wikipedia and other Wikimedia projects. Wikidata serves as a repository of structured data, whose purpose is to support the consistent sharing and linking of data on the Web. To support these purposes, it is key that Wikidata is built on consistent data models and representation schemas, which are constructed and managed in a collaborative platform. In this paper, we address the quality of taxonomic hierarchies in Wikidata. We focus on taxonomic hierarchies with entities at different classification levels (particular individuals, types of individuals, types of types of individuals, etc.). We use an axiomatic theory for multi-level modeling to analyze current Wikidata content, and identify a significant number of problematic classification and taxonomic statements. The problems seem to arise from an inadequate use of instantiation and subclassing in certain Wikidata hierarchies.	Applying a Multi-Level Modeling Theory to Assess Taxonomic Hierarchies in Wikidata	NA:NA:NA:NA	2016
T. Chattopadhyay:Santa Maiti:Arindam Pal:Avik Ghose:Arpan Pal:Shanky Viswanathan:Narendran Sivakumar	A business problem for the telecommunication companies is to provide an appropriate promotional coupon to suitable customers. This problem leads to the challenge of identifying behavioral patterns of customers and deliver the right customer engagement at the right time. So there is a need for a system that can enable the telecommunication companies to go for the best marketing strategy by leveraging customer intelligence to drive offer acceptance based on personas. Technically it is possible for the telecommunication companies to recommend suitable advertisements if they can classify the web sites browsed by their customers into classes like sports, e-commerce, social networking, streaming media etc. Another problem is to classify a new website when it doesn't belong to any of the existing clusters. In this paper, the authors are going to propose a method to automatically classify the websites and synthesize the cluster names in case it doesn't belong to any of the predefined clusters. We have experimented on a small set of data set and the classification results are quite convincing. Moreover, the phrases used to describe a website if it doesn't belong to existing classes are compliant to the phrases obtained from manual annotation. This proposed system uses the Wikipedia data to construct the document for the websites browsed by the customers.	Automatic Discovery of Emerging Trends using Cluster Name Synthesis on User Consumption Data: Extended Abstract	NA:NA:NA:NA:NA:NA:NA	2016
Johanna Geiß:Michael Gertz	Driven by the popularity of social networks, there has been an increasing interest in employing such networks in the context of named entity linking. In this paper, we present a novel approach to person name disambiguation and linking that uses a large-scale social network extracted from the English Wikipedia. First, possible candidate matches for an ambiguous person name are determined. With each candidate match, a network substructure is associated. Based on the similarity between these network substructures and the latent network of an ambiguous person name in a document, we propose an efficient ranking method to resolve the ambiguity. We demonstrate the effectiveness of our approach, resulting in an overall precision of over 96% for disambiguating person names and linking them to real world entities.	With a Little Help from my Neighbors: Person Name Linking Using the Wikipedia Social Network	NA:NA	2016
Haggai Roitman:Shay Hummel:Ella Rabinovich:Benjamin Sznajder:Noam Slonim:Ehud Aharoni	This work presents a novel claim-oriented document retrieval task. For a given controversial topic, relevant articles containing claims that support or contest the topic are retrieved from a Wikipedia corpus. For that, a two-step retrieval approach is proposed. At the first step, an initial pool of articles that are relevant to the topic are retrieved using state-of-the-art retrieval methods. At the second step, articles in the initial pool are re-ranked according to their potential to contain as many relevant claims as possible using several claim discovery features. Hence, the second step aims at maximizing the overall claim recall of the retrieval system. Using a recently published claims benchmark, the proposed retrieval approach is demonstrated to provide more relevant claims compared to several other retrieval alternatives.	On the Retrieval of Wikipedia Articles Containing Claims on Controversial Topics	NA:NA:NA:NA:NA:NA	2016
Thomas Steiner	In this paper, we introduce the Wikipedia Tools for Google Spreadsheets. Google Spreadsheets is part of a free, Web-based software office suite offered by Google within its Google Docs service. It allows users to create and edit spreadsheets online, while collaborating with other users in realtime. Wikipedia is a free-access, free-content Internet encyclopedia, whose content and data is available, among other means, through an API. With the Wikipedia Tools for Google Spreadsheets, we have created a toolkit that facilitates working with Wikipedia data from within a spreadsheet context. We make these tools available as open-source on GitHub [https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets], released under the permissive Apache 2.0 license.	Wikipedia Tools for Google Spreadsheets	NA	2016
Yu Suzuki:Satoshi Nakamura	In this paper, we propose a method for assessing the quality of Wikipedia editors. By effectively determining whether the text meaning persists over time, we can determine the actual contribution by editors. This is used in this paper to detect vandal. However, the meaning of text does not always change if a term in the text is added or removed. Therefore, we cannot capture the changes of text meaning automatically, so we cannot detect whether the meaning of text survives or not. To solve this problem, we use crowdsourcing to manually detect changes of text meaning. In our experiment, we confirmed that our proposed method improves the accuracy of detecting vandals by about 5%.	Assessing the Quality of Wikipedia Editors through Crowdsourcing	NA:NA	2016
Ramine Tinati:Markus Luczak-Roesch:Wendy Hall	This paper documents a study of the real-time Wikipedia edit stream containing over 6 million edits on 1.5 million English Wikipedia articles, during 2015. We focus on answering questions related to identification and use of information cascades between Wikipedia articles, based on author editing activity. Our findings show that by constructing information cascades between Wikipedia articles using editing activity, we are able to construct an alternative linking structure in comparison to the embedded links within a Wikipedia page. This alternative article hyperlink structure was found to be relevant in topic, and timely in relation to external global events (e.g., political activity). Based on our analysis, we contextualise the findings against areas of interest such as events detection, vandalism, edit wars, and editing behaviour.	Finding Structure in Wikipedia Edit Activity: An Information Cascade Approach	NA:NA:NA	2016
Vikrant Yadav:Sandeep Kumar	In this paper, we present a novel method to obtain a set of most appropriate queries for retrieval of relevant information about an entity from the Web. Using the body text of existing articles in a Wikipedia category, we generate a set of queries capable of fetching the most relevant content for any entity belonging to that category. We find the common topics discussed in the articles of a category using Latent Semantic Analysis (LSA) and use them to formulate the queries. Using Long Short-Term Memory (LSTM) neural network, we reduce the number of queries by removing the less sensible ones and then select the best ones out of them. The experimental results show that the proposed method outperforms the baselines. Existing approaches are performing better in generation of the relevant section title queries by extraction from the headings of the Wikipedia articles as compared to the generation of queries by extraction from the body text of the articles. Whereas, the experimental results show that the proposed approach can perform equally well and even better in extraction of the relevant queries from the body text of the Wikipedia articles.	Learning Web Queries for Retrieval of Relevant Information about an Entity in a Wikipedia Category	NA:NA	2016
Thanassis Tiropanis:Matthew Weber	It is our great pleasure to welcome you to the WWW 2016 Tutorials. We received 21 proposals from all around the world covering a broad range of topics. We evaluated them regarding relevance, quality, and novelty, selecting 5 half-day tutorials and 2 full-day tutorials. We also took in account the coverage of the different areas related to WWW as well as the potential audience, to schedule them in two consecutive days with the minimal audience interest overlap. The morning of the first day includes the following four tutorials: Computational Social Science for the World Wide Web Centrality Measures on Big Graphs The afternoon of the first day includes the following four tutorials: Computational Social Science for the World Wide Web (continued) Cryptographic Currencies Crash Course The second day starts with three tutorials: Building Decentralized Applications for the Social Web Automatic Entity Recognition and Typing in Massive Text Corpora Mining Big Time-series Data on the Web The final afternoon includes the last three tutorials: Building Decentralized Applications for the Social Web (continued) Analyzing sequential User Behavior on the Web The call for tutorials attracted submissions from United States, Europe, Asia, Africa and South America. Review and acceptance statistics are as follows: WWW 2016 Tutorials Reviewed -21 Accepted - 7. We believe that the program provides a good balance between several trending topics such as deep learning, social media analysis, graph mining, crowdsourcing, knowledge databases, mobile data, etc. Hence we hope that you will find the tutorial program interesting, providing you with a valuable opportunity to learn and share ideas with other researchers and practitioners from institutions around the world.	Session details: Tutorials	NA:NA	2016
Francesco Bonchi:Gianmarco De Francisci Morales:Matteo Riondato	Centrality measures allow to measure the relative importance of a node or an edge in a graph w.r.t.~other nodes or edges. Several measures of centrality have been developed in the literature to capture different aspects of the informal concept of importance, and algorithms for these different measures have been proposed. In this tutorial, we survey the different definitions of centrality measures and the algorithms to compute them. We start from the most common measures, such as closeness centrality and betweenness centrality, and move to more complex ones such as spanning-edge centrality. In our presentation, we begin from exact algorithms and then progress to approximation algorithms, including sampling-based ones, and to highly-scalable MapReduce algorithms for huge graphs, both for exact computation and for keeping the measures up-to-date on dynamic graphs where edges are inserted or removed over time. Our goal is to show how advanced algorithmic techniques and scalable systems can be used to obtain efficient algorithms for an important graph mining task, and to encourage research in the area by highlighting open problems and possible directions.	Centrality Measures on Big Graphs: Exact, Approximated, and Distributed Algorithms	NA:NA:NA	2016
Aljosha Judmayer:Edgar Weippl	"Bitcoin is a rare case where practice seems to be ahead of theory." Joseph Bonneau et al.[15] This tutorial aims to further close the gap between IT security research and the area of cryptographic currencies and block chains. We will describe and refer to Bitcoin as an example throughout the tutorial, as it is the most prominent representative of a such a system. It also is a good reference to discuss the underlying block chain mechanics which are the foundation of various altcoins (e.g. Namecoin) and other derived systems. In this tutorial, the topic of cryptographic currencies is solely addressed from a technical IT security point-of-view. Therefore we do not cover any legal, sociological, financial and economical aspects. The tutorial is designed for participants with a solid IT security background but will not assume any prior knowledge on cryptographic currencies. Thus, we will quickly advance our discussion into core aspects of this field.	Cryptographic Currencies Crash Course (C4): Tutorial	NA:NA	2016
Xiang Ren:Ahmed El-Kishky:Chi Wang:Jiawei Han	In today's computerized and information-based society, we are soaked with vast amounts of natural language text data, ranging from news articles, product reviews, advertisements, to a wide range of user-generated content from social media. To turn such massive unstructured text data into actionable knowledge, one of the grand challenges is to gain an understanding of entities and the relationships between them. In this tutorial, we introduce data-driven methods to recognize typed entities of interest in different kinds of text corpora (especially in massive, domain-specific text corpora). These methods can automatically identify token spans as entity mentions in text and label their types (e.g., people, product, food) in a scalable way. We demonstrate on real datasets including news articles and yelp reviews how these typed entities aid in knowledge discovery and management.	Automatic Entity Recognition and Typing in Massive Text Corpora	NA:NA:NA:NA	2016
Yasushi Sakurai:Yasuko Matsubara:Christos Faloutsos	Online news, blogs, SNS and many other Web-based services has been attracting considerable interest for business and marketing purposes. Given a large collection of time series, such as web-click logs, online search queries, blog and review entries, how can we efficiently and effectively find typical time-series patterns? What are the major tools for mining, forecasting and outlier detection? Time-series data analysis is becoming of increasingly high importance, thanks to the decreasing cost of hardware and the increasing on-line processing capability. The objective of this tutorial is to provide a concise and intuitive overview of the most important tools that can help us find meaningful patterns in large-scale time-series data. Specifically we review the state of the art in three related fields: (1) similarity search, pattern discovery and summarization, (2) non-linear modeling and forecasting, and (3) the extension of time-series mining and tensor analysis. We also introduce case studies that illustrate their practical use for social media and Web-based services.	Mining Big Time-series Data on the Web	NA:NA:NA	2016
Andrei Sambra:Amy Guy:Sarven Capadisli:Nicola Greco	Recent advancements in technologies and protocols mean that it is easier than ever to integrate social features into diverse web applications, and increased awareness of privacy concerns means that it is pertinent to consider empowerment of application users when doing so. Many developers are already familiar with the notion of personal data stores; this tutorial will demonstrate how to access or provide such stores for users, and build simple web applications which read and write to the storage whilst remaining completely decoupled from it. This advantages developers in two ways: by removing the burden of storing and maintaining a canonical copy of user data; and by enabling access to and ease of integration with data created through other applications, creating richer, seamless experiences. From the application users' perspective, they need no longer commit and become bound to particular services, but can mix, match and move between those that best meet their needs. We will introduce Solid, a set of protocols based on existing W3C recommendations, for reading, writing and access control of the contents of a personal data store, which can be layered up in order to integrate various social features into new or existing web applications. Attendees will leave with an understanding of Solid and how different parts of the protocols can work together, and having written some code to implement the parts that interest them most. They will also have hands on experience with existing libraries and tooling to facilitate working with the Solid protocols. Those who stay for the full day will have an opportunity to build a small but complete web application with decentralized social features, and to collaborate with others to see the advantages of sharing data between multiple applications.	Building Decentralized Applications for the Social Web	NA:NA:NA:NA	2016
Philipp Singer:Florian Lemmerich	This tutorial aims at outlining fundamental methods for studying categorical sequences on the Web. Categorical sequences can refer to any kind of transitional data between a set of states, for example human navigation (transitions) between Web sites (states). Presented methods focus on sequential pattern mining, modeling and inference aiming at better understanding the production of sequences. A core model utilized in this tutorial is the Markov chain model. We hope that this tutorial raises interest and awareness of the field at hand and provides participants with basic tools for analyzing sequential user behavior on the Web.	Analyzing Sequential User Behavior on the Web	NA:NA	2016
Ingmar Weber:Claudia Wagner:Markus Strohmaier:Luca Maria Aiello	This tutorial aims at outlining fundamental methods for studying typical social science research questions with organic data (i.e., data that has not been designed for a specific research purpose but can be found on the Web). Further, social theories, statistical methods and models that help to understand the processes that generated the data will be discussed. Participants will learn (1) how to turn theoretical assumptions into models and test them, (2) how to validate measurements and (3) how to approximate causality when working with organic data.	Computational Social Science for the World Wide Web (CSSW3)	NA:NA:NA:NA	2016
Sören Auer:Tom Heath:Christian Bizer:Tim Berners-Lee	The ninth workshop on Linked Data (LDOW2016) on the Web is held in Montreal, Quebec, Canada on April 12, 2016 and co-located with the 25rd International World Wide Web Conference (WWW2016). The Web is developing from a medium for publishing textual documents into a medium for sharing structured data. This trend is fueled on the one hand by the adoption of the Linked Data principles by a growing number of data providers. On the other hand, large numbers of websites have started to semantically mark up the content of their HTML pages and thus also contribute to the wealth of structured data available on the Web. The 9th Workshop on Linked Data on the Web aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data from the Web as well as mining knowledge from the global Web of Data.	LDOW2016: 9th Workshop on Linked Data on the Web	NA:NA:NA:NA	2016
Amparo E. Cano:Daniel Preotiuc-Pietro:Danica Radovanović:Katrin Weller:Aba-Sah Dadzie	NA	#Microposts2016: 6th Workshop on Making Sense of Microposts: Big things come in small packages	NA:NA:NA:NA:NA	2016
Filip Radlinski:Serena Villata	NA	Session details: Posters	NA:NA	2018
Dominik Kowald:Paul Seitlinger:Tobias Ley:Elisabeth Lex	In this paper, we present the results of an online study with the aim to shed light on the impact that semantic context cues have on the user acceptance of tag recommendations. Therefore, we conducted a work-integrated social bookmarking scenario with 17 university employees in order to compare the user acceptance of a context-aware tag recommendation algorithm called 3Layers with the user acceptance of a simple popularity-based baseline. In this scenario, we validated and verified the hypothesis that semantic context cues have a higher impact on the user acceptance of tag recommendations in a collaborative tagging setting than in an individual tagging setting. With this paper, we contribute to the sparse line of research presenting online recommendation studies.	The Impact of Semantic Context Cues on the User Acceptance of Tag Recommendations: An Online Study	NA:NA:NA:NA	2018
Ryan A. Rossi:Nesreen K. Ahmed:Eunyee Koh	This paper describes a general framework for learning Higher-Order Network Embeddings (HONE) from graph data based on network motifs. The HONE framework is highly expressive and flexible with many interchangeable components. The experimental results demonstrate the effectiveness of learning higher-order network representations. In all cases, HONE outperforms recent embedding methods that are unable to capture higher-order structures with a mean relative gain in AUC of 19% (and up to 75% gain) across a wide variety of networks and embedding methods.	Higher-order Network Representation Learning	NA:NA:NA	2018
Peng Bao:Jiahui Wang	With the rapid development of scientific impact quantification in the field of science of success, the ability to identify the representative work of a researcher has important implications in a wide range of areas, including hiring, funding, and promotion systems. In this paper, we propose a two-step credit allocation algorithm (TSCA) for identifying the representative work of a researcher. This algorithm explicitly captures the importance of a paper, its relevance to other papers, and the unequally distributed contribution of each citation. We validate TSCA by applying it on the citation data from American Physical Society (APS) in the scenario of identifying the Nobel prize winning papers of the Nobel laureates. Experiments demonstrate that the proposed algorithm can significantly outperform the existing methods.	Identifying Your Representative Work Based on Credit Allocation	NA:NA	2018
Hongru Liang:Qian Li:Haozheng Wang:Hang Li:Jin-Mao Wei:Zhenglu Yang	Learning rap lyrics is an important area of music information retrieval because it is the basis of many applications, such as recommendation systems, automatic classification. In this paper, we tackle the issue pertaining to the lack of an effective approach to aggregate various features of lyrics by proposing an attention-based autoencoder for rap lyrics representation learning (AttAE-RL²). The proposed method appropriately integrates the semantic and prosodic features of rap lyrics. The preliminary experimental results demonstrate that our approach outperforms the state-of-the-art ones.	AttAE-RL²: Attention based Autoencoder for Rap Lyrics Representation Learning	NA:NA:NA:NA:NA:NA	2018
Yuta Takata:Mitsuaki Akiyama:Takeshi Yagi:Kunio Hato:Shigeki Goto	Threats of abusing websites that webmasters have stopped updating have increased. In this poster, we propose a method of predicting potentially abusable websites by retrospectively analyzing updates of software that composes websites. The method captures webmaster behaviors from archived snapshots of a website and analyzes the changes of web servers and web applications used in the past as update histories. A classifier that predicts website abuses is finally built by using update histories from snapshots of known malicious websites before the detections. Evaluation results showed that the classifier could predict various website abuses, such as drive-by downloads, phishes, and defacements, with accuracy: a 76% true positive rate and a 26% false positive rate.	POSTER: Predicting Website Abuse Using Update Histories	NA:NA:NA:NA:NA	2018
Yufei Wen:Lei Guo:Zhumin Chen:Jun Ma	With the advent of online social networks, the use of information hidden in social networks for recommendation has been extensively studied. Unlike previous work regarded social influence as regularization terms, we take advantage of network embedding techniques and propose an embedding based recommendation method. Specifically, we first pre-train a network embedding model on the users' social network to map each user into a low dimensional space, and then incorporate them into a matrix factorization model, which combines both latent and pre-learned features for recommendation. The experimental results on two real-world datasets indicate that our proposed model is more effective and can reach better performance than other related methods.	Network Embedding Based Recommendation Method in Social Networks	NA:NA:NA:NA	2018
Jingtao Ding:Fuli Feng:Xiangnan He:Guanghui Yu:Yong Li:Depeng Jin	Bayesian Personalized Ranking (BPR) is a representative pairwise learning method for optimizing recommendation models. It is widely known that the performance of BPR depends largely on the quality of the negative sampler. In this short paper, we make two contributions with respect to BPR. First, we find that sampling negative items from the whole space is unnecessary and may even degrade the performance. Second, focusing on the purchase feedback of the E-commerce domain, we propose a simple yet effective sampler for BPR by leveraging the additional view data. Compared to the vanilla BPR that applies a uniform sampler on all candidates, our view-aware sampler enhances BPR with a relative improvement of 27.36% and 69.54% on two real-world datasets respectively.	An Improved Sampler for Bayesian Personalized Ranking by Leveraging View Data	NA:NA:NA:NA:NA:NA	2018
Ting-Yu Yen:Yang-Yin Lee:Hen-Hsen Huang:Hsin-Hsi Chen	While recent word embedding models demonstrate their abilities to capture syntactic and semantic information, the demand for sense level embedding is getting higher. In this study, we propose a novel joint sense embedding learning model that retrofits the word representation into sense representation from contextual and ontological information. The experiment shows the effectiveness and robustness of our model that outperforms previous approaches in four public available benchmark datasets.	That Makes Sense: Joint Sense Retrofitting from Contextual and Ontological Information	NA:NA:NA:NA	2018
Qiang Xu:Xin Wang:Yueqi Xin:Zhiyong Feng:Renhai Chen	This paper presents a novel Pregel-based Distributed Subgraph Matching method PDSM to answer subgraph matching queries on big RDF graphs. In our method, the query graph is transformed to a spanning tree based on the breadth-first search (BFS). Two optimization techniques are proposed to filter out part of the unpromising intermediate results and postpone the Cartesian product operations in the Pregel iterative computation. The extensive experiments on both synthetic and real-world datasets show that PDSM outperforms the state-of-the-art methods by an order of magnitude.	PDSM: Pregel-Based Distributed Subgraph Matching on Large Scale RDF Graphs	NA:NA:NA:NA:NA	2018
Yueqi Xin:Bingyi Zhang:Xin Wang:Qiang Xu:Zhiyong Feng	This paper proposes a novel method for answering Pregel-based Parallel Provenance-aware Regular Path Queries (P3RPQ) on large RDF graphs. Our method is developed using the Pregel framework, which utilizes Glushkov automata to keep track of the matching process of RPQs in parallel. Meanwhile, four optimization strategies are devised, which can reduce the response time of the basic algorithm dramatically and overcome the counting paths problem to some extent. The experiments are conducted to verify the performance of our algorithms on both synthetic and real-world datasets.	P3RPQ: Pregel-Based Parallel Provenance-Aware Regular Path Query Processing on Large RDF Graphs	NA:NA:NA:NA:NA	2018
An-Zi Yen:Hen-Hsen Huang:Hsin-Hsi Chen	People are used to log their life on the social media platform. Life event can be expressed explicitly or implicitly in a text description. However, a description does not always contain life events related to a specific individual. To tell if there exist any life events and further know their categories is indispensable for event retrieval. This paper explores various LSTM models to detect and classify life events in tweets. Experiments show that the proposed Multi-Task LSTM model with attention achieves the best performance.	Detecting Personal Life Events from Twitter by Multi-Task LSTM	NA:NA:NA	2018
Reut Apel:Elad Yom-Tov:Moshe Tennenholtz	Users of social networks often focus on specific areas of that network, leading to the well-known "filter bubble" effect. Connecting people to a new area of the network in a way that will cause them to become active in that area could help alleviate this effect and improve social welfare. Here we present preliminary analysis of network referrals, that is, attempts by users to connect peers to other areas of the network. We classify these referrals by their efficiency, i.e., the likelihood that a referral will result in a user becoming active in the new area of the network. We show that by using features describing past experience of the referring author and the content of their messages we are able to predict whether referral will be effective, reaching an AUC of 0.87 for those users most experienced in writing efficient referrals. Our results represent a first step towards algorithmically constructing efficient referrals with the goal of mitigating the "filter bubble" effect pervasive in on line social networks.	Characterizing Efficient Referrals in Social Networks	NA:NA:NA	2018
Moumita Basu:Anurag Shandilya:Kripabandhu Ghosh:Saptarshi Ghosh	During a disaster event, it is essential to know about needs and availabilities of different types of resources, for coordinating relief operations. Microblogging sites are frequently used for aiding post-disaster relief operations, and there have been prior attempts to identify tweets that inform about resource needs and availabilities (termed as need-tweets and availability-tweets respectively). However, there has not been much attempt to effectively utilise such tweets. We introduce the problem of automatically matching need-tweets with appropriate availability-tweets, which is practically important for coordination of post-disaster relief operations. We also experiment with several methodologies for automatically matching need-tweets and availability-tweets.	Automatic Matching of Resource Needs and Availabilities in Microblogs for Post-Disaster Relief	NA:NA:NA:NA	2018
Gaoyang Guo:Chaokun Wang:Xiang Ying	A myriad of community detection methods have been designed to discover communities based on specific network features in different disciplines, such as sociology, physics, and computer science. Consequentially, we have to face the problem of Algorithm Selection for Community Detection (ASCD): Given a specific network, which algorithm should we select to reveal its latent community structures In this study, we propose a model called CYDES to address the ASCD problem. CYDES consists of two parts, namely feature matrix generation and algorithm classification. We combine three effective feature extraction methods with the idea of BOW model to construct a fixed-size feature matrix. After a nonlinear transformation to the feature matrix, a softmax regression model is utilized to generate a classification label representing the best community detection algorithm we select. Extensive experimental results demonstrate that CYDES has high algorithm selection quality for community detection in networks.	Which Algorithm Performs Best: Algorithm Selection for Community Detection	NA:NA:NA	2018
Longtao Huang:Shangwen Lv:Liangjun Zang:Yipeng Su:Jizhong Han:Songlin Hu	This paper proposes a novel approach to retrieve news articles related to a specific event and generate a storyline to help people understand the event evolution. First, a similarity calculation method is proposed to retrieve news articles related to the specific event, which combines textual similarity, temporal similarity and entity similarity. Then a multi-view attribute graph is constructed to represent the relationship between retrieved articles. Finally, a community detection algorithm is developed to segment and chain subevents in the graph. Experimental results on real-world datasets demonstrate that the proposed approach achieve better results than existing methods.0F0F	A Fresh Look at Understanding News Events Evolution	NA:NA:NA:NA:NA:NA	2018
Yu Fu:Dongliang Zhang:Hao Jiang	Besides usability, visual appearance also plays an important role in influencing users' attitudes towards the mobile shopping apps. This article presents a pilot study that explores users' and designers' preferences for mobile shopping app user interfaces (UIs). The study consisted of two phases. (1) Eliciting participants' perception of UIs similarity by sorting, using DISTATIS and cluster analysis, the UIs similarity perceptual space was identified. (2) Eliciting participants' overall preference by rating. The results identified three typical UIs and the distribution of ideal preference UIs for users and designers. Last, users and designers' differences in the UI preference were discussed.	Comparison of Users' and Designers' Differences in Mobile Shopping App Interface Preferences and Designs	NA:NA:NA	2018
Saurabh Sohoney:Nikita Prabhu:Vineet Chaoji	Inverse Propensity Score estimator (IPS) is a basic, unbiased, off-policy evaluation technique to measure the impact of a user-interactive system without serving live traffic. We present our work on applying IPS to real-world settings by addressing some practical challenges, thereby enabling successful policy evaluation. In particular, we show that off-policy evaluation can be impossible in the absence of a complete context and we describe a systematic way of defining the context.	Handling Confounding for Realistic Off-Policy Evaluation	NA:NA:NA	2018
Yunqi Qiu:Manling Li:Yuanzhuo Wang:Yantao Jia:Xiaolong Jin	Topic entity detection is to find out the main entity asked in a question, which is significant in question answering. Traditional methods ignore the information of entities, especially entity types and their hierarchical structures, restricting the performance. To take full advantage of Knowledge Base(KB) and detect topic entities correctly, we propose a deep neural model to leverage type hierarchy and relations of entities in KB. Experimental results demonstrate the effectiveness of the proposed method.	Hierarchical Type Constrained Topic Entity Detection for Knowledge Base Question Answering	NA:NA:NA:NA:NA	2018
Dhruv Gupta:Klaus Berberich	Knowledge graphs capture very little temporal information associated with facts. In this work, we address the problem of identifying time intervals of knowledge graph facts from large document collections annotated with temporal expressions. Prior approaches in this direction have leveraged limited metadata associated with documents in large collections (e.g., publication dates) or have limited techniques to model the uncertainty and dynamics of temporal expressions. Our approach to identify time intervals for time-sensitive facts in knowledge graphs leverages a time model that incorporates uncertainty and models them at different levels of granularity (i.e., day, month, and year). Evaluation on a temporal fact benchmark using two large news archives amounting to more than eleven million documents show the quality of our results.	Identifying Time Intervals for Knowledge Graph Facts	NA:NA	2018
Fei Wu:Yanen Li:Ning Xu	The affinity of a user to a type of items (e.g., stories from the same publisher, and movies of the same genre) is an important signal reflecting the user's interests. Accurately estimating of the user type affinity has various applications in ranking and recommendation systems. For frequent users, simply dividing the number of interactions with content (e.g., clicks) by the number of impressions (e.g., the number of times the content is presented to each user) would be a good estimate. However, such estimates are erroneous for users who have sparse interaction history, (e.g., new users). To alleviate the problem, feature-based approaches aim to learn functions predicting the affinity score using only none-click features, such as user demographics, locations, and interests. Likewise, such approaches do not take full advantage of the interaction history of frequent users. Motivated by the limitations of the two approaches, we propose a Gamma-Poisson model that aims at utilizing the interaction history of frequent users, as well as leveraging a feature-based model for infrequent users. Our intuition is that we should rely more on the interaction history when estimating affinity for frequent users, and weigh more on feature-based model for infrequent users. We present experimental results on large-scale real-world data in a publisher content clicks prediction task to demonstrate the effectiveness of the proposed method in estimating user type affinity scores.	User Type Affinity Estimation Using Gamma-Poisson Model	NA:NA:NA	2018
Qiang Zhang:Emine Yilmaz:Shangsong Liang	A valuable step towards news veracity assessment is to understand stance from different information sources, and the process is known as the stance detection. Specifically, the stance detection is to detect four kinds of stances ("agree'', "disagree'', "discuss'' and "unrelated'') of the news towards a claim. Existing methods tried to tackle the stance detection problem by classification-based algorithms. However, classification-based algorithms make a strong assumption that there is clear distinction between any two stances, which may not be held in the context of stance detection. Accordingly, we frame the detection problem as a ranking problem and propose a ranking-based method to improve detection performance. Compared with the classification-based methods, the ranking-based method compare the true stance and false stances and maximize the difference between them. Experimental results demonstrate the effectiveness of our proposed method.	Ranking-based Method for News Stance Detection	NA:NA:NA	2018
Anurag Roy:Kripabandhu Ghosh:Moumita Basu:Parth Gupta:Saptarshi Ghosh	The Web has several information sources on which an ongoing event is discussed. To get a complete picture of the event, it is important to retrieve information from multiple sources. We propose a novel neural network based model which integrates the embeddings from multiple sources, and thus retrieves information from them jointly, %all the sources together, as opposed to combining multiple retrieval results. The importance of the proposed model is that no document-aligned comparable data is needed. Experiments on posts related to a particular event from three different sources - Facebook, Twitter and WhatsApp - exhibit the efficacy of the proposed model.	Retrieving Information from Multiple Sources	NA:NA:NA:NA:NA	2018
Tanmoy Chakraborty:Zhe Cui:Noseong Park	A community detection (CD) method is usually evaluated by what extent it is able to discover the 'ground-truth' community structure of a network. A certain 'node-centric metadata' is used to define the ground-truth partition. However, nodes in real networks often have multiple metadata types (e.g., occupation, location); each can potentially form a ground-truth partition. Our experiment with 10 CD methods on 5 datasets (having multiple metadata-based ground-truth partitions) show that the metadata-based evaluation is misleading because there is no single CD method that can outperform others by detecting all types of metadata-based partitions. We further show that the community structure obtained from the CD methods is usually topologically stronger than any metadata-based partitions. Finally, we suggest a new task-based evaluation framework for CD methods and show that a certain type of CD methods is useful for a certain type of task.	Metadata vs. Ground-truth: A Myth behind the Evolution of Community Detection Methods	NA:NA:NA	2018
Zeyang Lei:Yujiu Yang:Yi Liu	Sentiment analysis of social media and comment data is an important issue in opinion monitoring. In this work, we propose a Linguistic-Aware Attention Network (LANN) to enhance the performance of convolution neural network (CNN). LANN adopts a two-stage strategy to model the sentiment-specific sentence representation. First, an interactive attention mechanism is designed to model word-level semantics. Second, to capture phrase-level linguistic structure, a dynamic semantic attention is adopted to select the crucial phrase chunks in the sentence. The experiments demonstrate that LANN has robust superiority over competitors and has reached the state-of-the-art performance.	LAAN: A Linguistic-Aware Attention Network for Sentiment Analysis	NA:NA:NA	2018
Daniel Alexandrov:Viktor Karepin:Ilya Musabirov:Daria Chuprina	We use social media and WWW data to analyse international educational migration from Russia. We find substantial regional differences in migration patterns for three contrast directions: the Nordic countries, China and the Middle East. We built a model of migration flows with geographic distances to destination countries, various socio-demographic data and institutional characteristics of educational organisations.	Educational Migration from Russia to the Nordic Countries, China and the Middle East. Social Media Data	NA:NA:NA:NA	2018
Mengjing Chen:Weiran Shen:Pingzhong Tang:Song Zuo	Over the past few years, ride-sharing has been proven to be an effective way to relieve urban traffic congestion, as evidenced by several emerging ride-sharing platforms such as Uber and Didi. A key economic problem for these platforms is to design a revenue-optimal (or welfare-optimal) pricing scheme and a corresponding vehicle dispatching policy that incorporates geographic information, and more importantly, dynamic supply and demand. In this paper, we aim to solve this problem by introducing a unified model that takes into account both travel time and driver redirection. We tackle the non-convexity problem using the "ironing" technique and formulate the optimization problem as a Markov decision process (MDP), where the states are the driver distributions and the decision variables are the prices. Our main finding is to give an efficient algorithm that computes the exact revenue (or welfare) optimal randomized pricing schemes. We characterize the optimal solutions of the MDP by primal-dual analysis of a convex program. We also conduct empirical analysis of our solution with real data of a major ride-sharing platform and show its significant advantages over fixed pricing schemes as well as those prevalent surge-based pricing schemes.	Optimal Vehicle Dispatching for Ride-sharing Platforms via Dynamic Pricing	NA:NA:NA:NA	2018
Deguang Kong:Xiannian Fan:Konstantin Shmakov:Jian Yang	Bid optimization, which aims to find the competitive bid to achieve the best performance for the advertiser, is an important problem in online advertising. The optimal bid recommendation enables the advertisers to make informed decisions without actually spending the budget. In this paper, we consider a bid optimization scenario that the advertiser's budget can be split across multiple campaigns. To achieve the optimal performance, we formalize the bid optimization problem as a constraint combinational optimization problem, and derive an effective method to solve it. Experiment studies on real- world ad campaigns demonstrate the effectiveness of our method.	A Combinational Optimization Approach for Advertising Budget Allocation	NA:NA:NA:NA	2018
Zhile Jiang:Shuai Yu:Qiang Qu:Min Yang:Junyu Luo:Juncheng Liu	Author profiling is an important but challenging task. In this paper, we propose a novel Multi-Task learning framework for Author Profiling (MTAP), in which a document modeling module is shared across three different author profiling tasks (i.e., age, gender and job classification tasks). To further boost author profiling, we integrate hierarchical features learned by different models. Concretely, we employ CNN, LSTM and topic model to learn the character-level, word-level and topic-level features, respectively. MTAP thus leverages the benefits of supervised deep neural neural networks as well as an unsupervised probabilistic generative model to enhance the document representation learning. Experimental results on a real-life blog dataset show that MTAP has robust superiority over competitors and sets state-of-the-art for all the three author profiling tasks	Multi-task Learning for Author Profiling with Hierarchical Features	NA:NA:NA:NA:NA:NA	2018
Qi Zhu:Xiang Ren:Jingbo Shang:Yu Zhang:Frank F. Xu:Jiawei Han	Extracting entities and their relations from text is an important task for understanding massive text corpora. Open information extraction (IE) systems mine relation tuples (i.e., entity arguments and a predicate string to describe their relation) from sentences. However, current open IE systems ignore the fact that global statistics in a large corpus can be collectively leveraged to identify high-quality sentence-level extractions. In this paper, we propose a novel open IE system, called ReMine, which integrates local context signal and global structural signal in a unified framework with distant supervision. The new system can be efficiently applied to different domains as it uses facts from external knowledge bases as supervision; and can effectively score sentence-level tuple extractions based on corpus-level statistics. Specifically, we design a joint optimization problem to unify (1) segmenting entity/relation phrases in individual sentences based on local context; and (2) measuring the quality of sentence-level extractions with a translating-based objective. Experiments on real-world corpora from different domains demonstrate the effectiveness and robustness of ReMine when compared to other open IE systems.	Open Information Extraction with Global Structure Constraints	NA:NA:NA:NA:NA:NA	2018
Wenyu Du:Shuai Yu:Min Yang:Qiang Qu:Jia Zhu	In this paper, we propose GPSP, a novel Graph Partition and Space Projection based approach, to learn the representation of a heterogeneous network that consists of multiple types of nodes and links. Concretely, we first partition the heterogeneous network into homogeneous and bipartite subnetworks. Then, the projective relations hidden in bipartite subnetworks are extracted by learning the projective embedding vectors. Finally, we concatenate the projective vectors from bipartite subnetworks with the ones learned from homogeneous subnetworks to form the final representation of the heterogeneous network. Extensive experiments are conducted on a real-life dataset. The results demonstrate that GPSP outperforms the state-of-the-art baselines in two key network mining tasks: node classification and clustering.	GPSP: Graph Partition and Space Projection based Approach for Heterogeneous Network Embedding	NA:NA:NA:NA:NA	2018
Yong-Yeon Jo:Myung-Hwan Jang:Hyungsoo Jung:Sang-Wook Kim	Existing single-machine based graph engines do not leverage the characteristic of social networks following the power-law degree distribution. We propose a new graph engine tailored for processing and analyzing large-scale social networks efficiently by exploiting the power-law degree property	A High-Performance Graph Engine for Efficient Social Network Analysis	NA:NA:NA:NA	2018
Suman Kalyan Maity:Santosh K. C.:Arjun Mukherjee	In this paper, we propose a semi-supervised framework Spam2Vec to identify spammers in Twitter. This algorithmic framework learns the spam representations of the node in the network by leveraging biased random walks. Our spammer detection method yields an AUC of 0.54 with [email protected] as 0.12 and performs significantly better with 7.77% increase in AUC and a 2.4 times improvement on precision over the best performing baseline.	Spam2Vec: Learning Biased Embeddings for Spam Detection in Twitter	NA:NA:NA	2018
Zhitao Wang:Chengyao Chen:Wenjie Li	In this paper, we propose an attention network for diffusion prediction problem. The developed diffusion attention module can effectively explore the implicit user-to-user diffusion dependency among information cascade users. Besides, the user-to-cascade importance and the time-decay effect are captured and utilized by the model. The superiority of the proposed model over state-of-the-art methods is demonstrated by experiments on real diffusion data.	Attention Network for Information Diffusion Prediction	NA:NA:NA	2018
Po-Cheng Huang:Hen-Hsen Huang:Hsin-Hsi Chen	Knowledge base completion (KBC) involves in discovering missing facts. However, knowledge changes over time. Some facts need to be removed from knowledge base (KB) to keep knowledge base integrity (KBI) while new facts are inserted or old facts are deleted. This paper proposes a path-based learning model to learn the dependency of dynamic relations automatically. In this way, we can eliminate the conflicting facts and keep KB clean. That would be a significant benefit for KBC and other tasks using KB.	Path Ranking with Path Difference Sets for Maintaining Knowledge Base Integrity	NA:NA:NA	2018
Chen Ling:Lei Wang:Jun Lang:Qiufen Xia:Guoxuan Chang:Kun Wang:Peng Zhao	Internet access restriction in various areas unceasingly poses inconvenience for users. Unblocked websites or web pages always accompany couple fate-to-fail links from blocked servers due to such restrictions, while users wait for a long time to see contents of these blocked/invalid links. Therefore, it is better to directly show timeout links to users for better experiences rather than making users excessively wait. In this paper, we present LinCa (Links Catcher), a novel approach that fully considers the Internet access restriction and reduces page loading time on client-side by parsing all HTTP requests, and intercepting all invalid links when a web navigation starts. Thus, we first create and maintain a Rule Base to store invalid links under given access restriction rules. We then update the Rule Base periodically to cover as many invalid links as possible and remove links that become valid. We finally demonstrate the effectiveness of LinCa through experiments by building and deploying a Chrome extension. Experimental results show that LinCa can reduce page loading time with average 28.12% of original page loading time for our data sets.	LinCa: A Page Loading Time Optimization Approach for Users Subject to Internet Access Restriction	NA:NA:NA:NA:NA:NA:NA	2018
Wouter Lightenberg:Yulong Pei:George Fletcher:Mykola Pechenizkiy	We introduce the Tink library for distributed temporal graph analytics. Increasingly, reasoning about temporal aspects of graph-structured data collections is an important aspect of analytics. For example, in a communication network, time plays a fundamental role in the propagation of information within the network. Whereas existing tools for temporal graph analysis are built stand alone, Tink is a library in the Apache Flink ecosystem, thereby leveraging its advanced mature features such as distributed processing and query optimization. Furthermore, Flink requires little effort to process and clean the data without having to use different tools before analyzing the data. Tink focuses on interval graphs in which every edge is associated with a starting time and an ending time. The library provides facilities for temporal graph creation and maintenance, as well as standard temporal graph measures and algorithms. Furthermore, the library is designed for ease of use and extensibility.	Tink: A Temporal Graph Analytics Library for Apache Flink	NA:NA:NA:NA	2018
Debasis Ganguly:Kripabandhu Ghosh	Effective clustering of short documents, such as tweets, is difficult because of the lack of sufficient semantic context. Word embedding is a technique that is effective in addressing this lack of semantic context. However, the process of word vector embedding, in turn, relies on the availability of sufficient contexts to learn the word associations. To get around this problem, we propose a novel word vector training approach that leverages topically similar tweets to better learn the word associations. We test our proposed word embedding approach by clustering a collection of tweets on disasters. We observe that the proposed method improves clustering effectiveness by up to 14%.	Contextual Word Embedding: A Case Study in Clustering Tweets about Emergency Situations	NA:NA	2018
Markus Schedl:Eelco Wiechert:Christine Bauer	We approach the research question whether real-world events, such as sport events or product launches, influence music consumption behavior. To this end, we consider events of different categories from Google Trends and model listening events as time series using Last.fm data. Performing an auto-regressive integrated moving average analysis to decompose the signal and subsequently an intervention time series analysis, we find significant signal discontinuities, in particular for the Google news category. We found that news and events are likely to increase the number of songs listened to per person per day by about 2%, while tech events commonly cause 1% less music being consumed.	The Effects of Real-world Events on Music Listening Behavior: An Intervention Time Series Analysis	NA:NA:NA	2018
Abhijnan Chakraborty:Mohammad Luqman:Sidhartha Satapathy:Niloy Ganguly	With a large number of stories emerging from the newsrooms, media websites need to curate interesting news for their readers. Although traditionally news was curated solely by human editors, increasing news volume has led media outlets to adopt editorial algorithms. However, such algorithms are often proprietary, and smaller outlets do not have the resources to build them from scratch. In this paper, we present a novel framework 'Samar' to automatically curate news by optimizing recency, relevance and diversity of the selected stories. Evaluations over two real-world news datasets show that Samar outperforms several state-of-the-art baselines in matching the news curation performed by human editors.	Editorial Algorithms: Optimizing Recency, Relevance and Diversity for Automated News Curation	NA:NA:NA:NA	2018
Hoang-Long Nguyen:Claudia-Lavinia Ignat:Olivier Perrin	Public key server is a simple yet effective way of key management in secure end-to-end communication. To ensure the trustworthiness of a public key server, transparent log systems such as CONIKS employ a tamper-evident data structure on the server and a gossiping protocol among clients in order to detect compromised servers. However, due to lack of incentive and vulnerability to malicious clients, a gossiping protocol is hard to implement in practice. Meanwhile, alternative solutions such as EthIKS are not scalable. This paper presents Trusternity, an auditing scheme relying on Ethereum blockchain that is easy to implement, scalable and inexpensive to operate.	Trusternity: Auditing Transparent Log Server with Blockchain	NA:NA:NA	2018
Jiaming Song:Xiaowang Zhang:Peng Peng:Zhiyong Feng:Lei Zou	In this paper, we present a plugin-based framework (MapSQ) with three parts for SPARQL queries utilizing high-performance of GPU to accelerate answering in a convenient way. Selector chooses suitable join order according to characteristics of data and queries. Executor answers subqueries and returns intermediate solutions and GPU Computing obtains the join result of intermediate solutions through MapReduce. Finally, we evaluate MapSQ bulit on gStore and RDF-3X on the LUBM benchmark and YAGO datasets (over 200 million triples). The experimental results show that MapSQ significantly improves the performance of SPARQL query engines with speedup up to 33.	MapSQ: A Plugin-based MapReduce Framework for SPARQL Queries on GPU	NA:NA:NA:NA:NA	2018
Deguang Kong:Konstantin Shmakov:Jian Yang	In cost-per-click (CPC) or cost-per-impression (CPM) advertising campaigns, advertisers always run the risk of spending the bud- get without getting enough conversions. Moreover, the bidding on advertising inventory has few connections with propensity that can reach to cost-per-acquisition (CPA) goals. To address this problem, this paper presents a bid optimization scenario to achieve the desired CPA goals for advertisers. In particular, we build the optimization engine to make a decision by solving the constrained optimization problem. The proposed model can naturally recommend the bid that meets the advertisers' expectations by making inference over history auction behaviors. The bid optimization model outperforms the baseline methods on real-world campaigns, and can be applied into a wide range of scenarios for performance improvement and revenue liftup.	Demystifying Advertising Campaign for CPA Goal Optimization	NA:NA:NA	2018
Ugo Tanielian:Anne-Marie Tousch:Flavian Vasile	Over the last decade, the number of devices per person has increased substantially. This poses a challenge for cookie-based personalization applications, such as online search and advertising, as it narrows the personalization signal to a single device environment. A key task is to find which cookies belong to the same person to recover a complete cross-device user journey. Recent work on the topic has shown the benefits of using unsupervised embeddings learned on user event sequences. In this paper, we extend this approach to a supervised setting and introduce the Siamese Cookie Embedding Network (SCEmNet), a siamese convolutional architecture that leverages the multi-modal aspect of sequences, and show significant improvement over the state-of-the-art.	Siamese Cookie Embedding Networks for Cross-Device User Matching	NA:NA:NA	2018
Saket Maheshwary:Hemant Misra	In this paper we investigate the important and challenging task of recommending appropriate jobs for job seeking candidates by matching semi structured resumes of candidates to job descriptions. To perform this task, we propose to use a siamese adaptation of convolutional neural network. The proposed approach effectively captures the underlying semantics thus enabling to project similar resumes and job descriptions closer to each other, and make dissimilar resumes and job descriptions distant from each other in the semantic space. Our experimental results on a set of 1314 resumes and a set of 3809 job descriptions (5,005,026 resume-job description pairs) demonstrate that our approach is better than the current state-of-the-art approaches.	Matching Resumes to Jobs via Deep Siamese Network	NA:NA	2018
Baoxu Shi:Tim Weninger	Understanding and visualizing human discourse has long being a challenging task. Although recent work on argument mining have shown success in classifying the role of various sentences, the task of recognizing concepts and understanding the ways in which they are discussed remains challenging. Given an email thread or a transcript of a group discussion, our task is to extract the relevant concepts and understand how they are referenced and re-referenced throughout the discussion. In the present work, we present a preliminary approach for extracting and visualizing group discourse by adapting Wikipedia's category hierarchy to be an external concept ontology. From a user study, we found that our method achieved better results than 4 strong alternative approaches, and we illustrate our visualization method based on the extracted discourse flows.	Visualizing the Flow of Discourse with a Concept Ontology	NA:NA	2018
Deguang Kong:Konstantin Shmakov:Jian Yang	In online advertising, a common objective for advertisers is to get the maximum returns on investment given the budget. On one hand, if the bid is too high, the advertiser pays more money than he should pay for the same number of clicks. On the other hand, it the bid is too low, the advertiser cannot win in auctions and therefore it loses the opportunity. A challenging problem is how to recommend the bid to achieve the maximum values for advertisers. In this paper, we present an inflection point approach for bid recommendation from discovering the bid price of click(bid)1 function at which the function changes from significant increase (i.e. concave downward) to slow increase (convex upward). We derive the optimal solution using history sparse and noisy observations given the budget limit. In real word advertising campaign evaluations, the proposed bid recommendation scenario brings in 15.37% bid increase and 30.24% click increase over the baselines.	An Inflection Point Approach for Advertising Bid Optimization	NA:NA:NA	2018
Helen Spiers:Alexandra Swanson:Lucy Fortson:Brooke D. Simmons:Laura Trouille:Samantha Blickhan:Chris Lintott	Human-computer systems are increasingly applied to data reduction problems; citizen science platforms (e.g. the Zooniverse) are one type of such a system. These platforms function as social machines, combining volunteer efforts with automated processes to enable distributed data analysis. The rapid growth of this approach is increasing the need to understand how we can improve volunteer interaction and engagement. Here, we utilize the most comprehensive collection of online citizen science data gathered to date to examine multiple variables across 63 Zooniverse projects. Our analyses reveal how subtle design changes can influence many facets of volunteer interaction, generating insights that have implications for the design and study of citizen science projects, and future research.	Patterns of Volunteer Behaviour Across Online Citizen Science	NA:NA:NA:NA:NA:NA:NA	2018
Evgeny Krivosheev:Bahareh Harandizadeh:Fabio Casati:Boualem Benatallah	In this paper we describe how crowd and machine classifier can be efficiently combined to screen items that satisfy a set of predicates. We show that this is a recurring problem in many domains, present machine-human (hybrid) algorithms that screen items efficiently and estimate the gain over human-only or machine-only screening in terms of performance and cost.	Crowd-Machine Collaboration for Item Screening	NA:NA:NA:NA	2018
Anurag Shandilya:Kripabandhu Ghosh:Saptarshi Ghosh	We propose to evaluate extractive summarization algorithms from a completely new perspective. Considering that an extractive summarization algorithm selects a subset of the textual units in the input data for inclusion in the summary, we investigate whether this selection is fair. We use several summarization algorithms over datasets that have a sensitive attribute (e.g., gender, political leaning) associated with the textual units, and find that the generated summaries often have very different distributions of the said attribute. Specifically, some classes of the textual units are under-represented in the summaries according to the fairness notion of adverse impact. To our knowledge, this is the first work on fairness of summarization, and is likely to open up interesting research problems.	Fairness of Extractive Text Summarization	NA:NA:NA	2018
Elias Moons:Tinne Tuytelaars:Marie-Francine Moens	Images have a prominent role in the communication of news on the Web. We propose a novel method for image classification with subject categories when limited annotated images are available for training the classifier. A neural network based encoder learns image representations from paired news images and their texts. Once trained, this encoder transforms any image to a text-enriched representation of the image, which is then used as input for the classifier that categorizes an image according to its subject category. We have trained classifiers with different amounts of annotated images and found that the image classifier that uses the text-enriched image representations outperforms a baseline model that only uses image features especially in cases with limited training examples.	Text-Enriched Representations for News Image Classification	NA:NA:NA	2018
Jurek Leonhardt:Avishek Anand:Megha Khosla	Recent works in recommendation systems have focused on diversity in recommendations as an important aspect of recommendation quality. In this work we argue that the post-processing algorithms aimed at only improving diversity among recommendations lead to discrimination among the users. We introduce the notion of user fairness which has been overlooked in literature so far and propose measures to quantify it. Our experiments on two diversification algorithms show that an increase in aggregate diversity results in increased disparity among the users.	User Fairness in Recommender Systems	NA:NA:NA	2018
Tomoki Sato:Hiroaki Shiokawa:Yuto Yamaguchi:Hiroyuki Kitagawa	ObjectRank is one of the popular graph mining methods that enables us to evaluate the importance of each vertex on heterogeneous graphs. However, it is computationally expensive to apply it to large graphs since ObjectRank needs to compute the importance of all vertices iteratively. In this work, we present a fast ObjectRank algorithm,FORank, that accurately approximates the keyword search results. FORank iteratively prunes vertices whose convergence score likely has less impact on the results during iterative computation. The experiments showed that FORank runs 7 times faster than ObjectRank computation with over 90% accuracy approximation.	FORank: Fast ObjectRank for Large Heterogeneous Graphs	NA:NA:NA:NA	2018
Amit Sarkar:G. Srinivasaraghavan	We investigate the task of reading context-biased web summarization, where the goal is to extract information relevant to the current reading context from a cited web article. In certain kind of linked document sets such as Wikipedia articles, scientific papers as well as news and blogs, such contextual summaries can be useful in providing additional related information to the user helping in the reading task. In this work, we focus on web articles only and try to find out the set of key components that contribute to building up the reading context. We build a supervised model for ranking sentences from the cited document according to their contextual salience. Initial evaluation based on annotated data-set of web articles show that our ranking model performs better than the generic summaries as well as baseline context-biased summaries.	Contextual Web Summarization: A Supervised Ranking Approach	NA:NA	2018
Madian Khabsa:Ahmed El Kholy:Ahmed Hassan Awadallah:Imed Zitouni:Milad Shokouhi	Digital assistants are emerging to become more prevalent in our daily lives. In interacting with these assistants, users may engage in multiple tasks within a short period of time. Identifying task boundaries and isolating them within a session is critical for measuring the performance of the system on each individual task. In this paper we aim to automatically identify sequences of interactions that together form a task. To this end, we sample interactions from a real world digital assistant and use crowd judges to segment a session into multiple tasks. After that, we use a machine learned model to identify task boundaries. Our learned model with its features significantly outperform the baselines. To the best of our knowledge, this is the first work that aims to identify tasks within digital assistant sessions.	Identifying Task Boundaries in Digital Assistants	NA:NA:NA:NA:NA	2018
Martin Atzmueller:Florian Lemmerich	Academic conferences are a backbone for the exchange of ideas in scientific communities. However, so far little is known about the communication networks emerging at those venues. Besides personal knowledge, network homophily has been identified as a driving factor for establishing contacts and followerships in social networks, i.e., people are more likely to engage with others if they are similar with respect to certain attributes. In this paper, we describe work in progress on investigating homophily at four academic conferences based on face-to-face (F2F) contact data collected using wearable sensors between conference participants. In particular, we study which personal attributes are predictive for face-to-face contacts. For that purpose, we obtained diverse personal attributes from online sources in order to elicit a variety of hypotheses, which can then be compared using descriptive statistics and a Bayesian method for comparing hypotheses in networks. Our results suggest that personal knowledge (as derived from DBLP and ResearchGate networks) and homophilic behavior with respect to several attributes, e.g., gender or country of origin, are important factors for contacts at academic conferences.	Homophily at Academic Conferences	NA:NA	2018
William Brendel:Fangqiu Han:Luis Marujo:Luo Jie:Aleksandra Korolova	Making friend recommendations is an important task for social networks, as having more friends typically leads to a better user experience. Most current friend recommendations systems grow the existing network at the cost of privacy. In particular, any given user's friend graph may be directly or indirectly leaked as a result of such recommendations. In many situations this is not desirable, as the friend list may reveal much about the user--from their identity to their sexual orientation and interests. In this work, we focus on the "cold start" problem of making friend recommendations for new users while raising the bar on protecting the privacy of the friend list of all users. We propose a practical friend recommendation framework, tested on the Snapchat social network, that preserves the privacy of users' friends lists with respect to brute-force attacks and scales to millions of users.	Practical Privacy-Preserving Friend Recommendations on Social Networks	NA:NA:NA:NA:NA	2018
Yingying Wu:Yiqun Liu:Ke Zhou:Xiaochuan Wang:Min Zhang:Shaoping Ma	Diversifying search results to satisfy as many users' intentions as possible is NP-hard. Some research employs a pruned exhaustive search, and some uses a greedy approach. However, the objective function of the result diversification problem adopts the cascade assumption which assumes users' information needs will drop once their subtopic search intents are satisfied. As a result, the intent distribution of diversified results deviates from the actual distribution of user intentions, and each subtopic tends to be chosen equally. This phenomenon is unreasonable, especially when the original distribution of user intent is unbalanced. In this paper, we present empirical evidence of the diversification equilibrium by showing that the standard deviations of subtopic distribution approach zero.	Treating Each Intent Equally: The Equilibrium of IA-Select	NA:NA:NA:NA:NA:NA	2018
Peizhi Wu:Yi Tu:Zhenglu Yang:Adam Jatowt:Masato Odagaki	Modeling the evolution of user preferences and item attributes in a dynamic social network is important because it is the basis for many applications, including recommendation systems and user behavior analysis. This study introduces a comprehensive general neural framework with several optimal strategies to jointly model the evolution of user preferences and item attributes in dynamic social networks. Preliminary experimental results conducted on real-world datasets demonstrate that our model performs better than the state-of-the-art methods.	Deep Modeling of the Evolution of User Preferences and Item Attributes in Dynamic Social Networks	NA:NA:NA:NA:NA	2018
Yingru Lin:Soyeon Caren Han:Byeong Ho Kang	The peer assessment approach is considered to be one of the best solutions for scaling both assessment and peer learning to global classrooms, such as MOOCs. However, some academic staff hesitate to use a peer assessment approach for their classes due to concerns about its credibility and reliability. The focus of our research is to detect the credibility level of each assessment performed by students during peer assessment. We found three major scopes in assessing the credibility level of evaluations, 1) Informativity, 2) Accuracy, and 3) Consistency. We collect assessments, including comments and grades provided by students during the peer assessment process and then each feedback-and-grade pair is labeled with its credibility level by Mechanical Turk evaluators. We extract relevant features from each labeled assessment and use them to build a classifier that attempts to automatically assess its level of credibility in C5.0 Decision Tree classifier. The evaluation results show that the model can be used to automatically classify peer assessments as credible or non-credible, with accuracy in the range of 88%.	Machine Learning for the Peer Assessment Credibility	NA:NA:NA	2018
Qian Li:Ziwei Li:Jin-Mao Wei:Zhenglu Yang:Yanhui Gu:R. Uday Kiran	Predicting the ending of a story is an interesting issue that has attracted considerable attention, as in case of the ROC Story Cloze Task (SCT). Although several studies have addressed this issue, the performance remains unsatisfactory due to ineffectiveness of story comprehension. In this paper, we propose to construct a story coherence based neural network model (SCNN) with well-designed optimizations. The preliminary evaluation demonstrates the effectiveness of our model which is superior to that of state-of-the-art approaches.	A Story Coherence based Neural Network Model for Predicting Story Ending	NA:NA:NA:NA:NA:NA	2018
Min Gui:Zhengkun Zhang:Zhenglu Yang:Yanhui Gu:Guandong Xu	Document summarization is an important research issue and has attracted much attention from the academe. The approaches for document summarization can be classified as extractive and abstractive. In this work, we introduce an effective joint framework that integrates extractive and abstractive summarization models, which is much closer to the way human write summaries (first underlining important information). Preliminary experiments on real benchmark dataset demonstrate that our model is competitive with the state-of-the-art methods.	An Effective Joint Framework for Document Summarization	NA:NA:NA:NA:NA	2018
Paul Groth:Amélie Gyrard	The Demo Track is one of the most exciting parts of any Web Conference. It allows researchers and practitioners to demonstrate new systems in an engaging and hands-on manner to the community. The Web has been driven forward by building systems and technology. The demo track is a venue that encourages this sort of important type of result This year the track received 71 submissions of those 30 were accepted for a 42% accept rate. We had a comprehensive review procedure that looked at a number of dimensions including the novelty of the demo, its fit with the conference, its research content, and its potential for audience engagement. We were pleased by the number of submissions that included links to online demonstrations and/or videos. This gave reviewers additional information about how the demo would be presented. Overall, we had 232 reviews across all submissions. Many of the reviews provided not only a their expert judgement but ways in which the submissions could be improved. It is often difficult judging demonstrations as there are multiple factors to be taken into account. We want to thank the entire committee for taking the time to support the track. The resulting set of selected demos reflects the wide-variety of technology and research interests impacting the wide. Demonstrations cover topics such as using data on the web, the integration of the web and the physical world, knowledge graphs, search engines, security and privacy, and dealing with multimedia data. We believe that these demos provide an exciting taste of the future of the Web.	Demo Track Chairs' Welcome & Organization	NA:NA	2018
Ruben Taelman:Miel Vander Sande:Ruben Verborgh	The Linked Open Data cloud is evergrowing and many datasets are frequently being updated. In order to fully exploit the potential of the information that is available in and over historical dataset versions, such as discovering evolution of taxonomies or diseases in biomedical datasets, we need to be able to store and query the different versions of Linked Datasets efficiently. In this demonstration, we introduce OSTRICH, which is an efficient triple store with supported for versioned query evaluation. We demonstrate the capabilities of OSTRICH using a Web-based graphical user interface in which a store can be opened or created. Using this interface, the user is able to query in, between, and over different versions, ingest new versions, and retrieve summarizing statistics.	OSTRICH: Versioned Random-Access Triple Store	NA:NA:NA	2018
Hong-Han Shuai:Yueh-Hsue Li:Chun-Chieh Feng:Wen-Chih Peng	Nowadays, people get used to buying a variety of commodities from e-commerce platforms because of the convenience and easy access to Internet. As a result, the sales on e-commerce platforms have grown exponentially. It is promising to customize the online shops for different users under VR environments since users do not need to waste time of going upstairs or downstairs for finding interesting commodities. Therefore, in this demo paper, we propose a novel four-dimensional shopping mall that offers online group shopping services with the customized shop recommendation, where a group of friends in the proposed four-dimensional shopping mall can teleport to the next shop by one click. We formulate a Sequential group wIllingness OptimizatioN (SION) problem, prove SION is NP-hard, and provide an efficient algorithm called ζ-GSS. The experimental results show that the solution quality of the proposed ζ-GSS is close to the optimal solution, while the execution time only requires 3.3%. Finally, we build the prototype of the four-dimensional shopping mall, which can be demonstrated for users to experience the next-generation online shopping.	Four-Dimensional Shopping Mall: Sequential Group Willingness Optimization under VR Environments	NA:NA:NA:NA	2018
Benjamin Klotz:Raphaël Troncy:Daniel Wilms:Christian Bonnet	In this paper, we use semantic technologies for enriching trajectory data in the automotive industry for offline analysis. We proposed to re-use a combination of existing ontologies and we designed a Vehicle Signal Specification ontology to provide an environment in which we developed an application that analyzes the variations of signal values and enables to infer the "driving smoothness'' that we represent as additional annotations of semantic trajectories.	Generating Semantic Trajectories Using a Car Signal Ontology	NA:NA:NA:NA	2018
Johannes Doleschal:Nico Höllerich:Wim Martens:Frank Neven	Chisel is a tool for flexible manipulation of CSV-like data, motivated by the recent effort of the World Wide Web Consortium (W3C) towards a recommendation for tabular data and metadata on the Web. In brief, Chisel supports an expressive built-in schema language for CSV-like data, that can handle both tabular and non-tabular data. Furthermore, it supports a simple programming language for transforming tabular and non-tabular CSV-like data. In the demo, we showcase the system for specifying and validating schemas, building transformations, and setting up a pipeline for automatic conversion of "wild" CSV-like data into structured tabular data. We present use cases for Chisel specifically targeted at exemplifying the ease of specifying, modifying, and understanding Sculpt schemas as well as extracting and transforming data.	Chisel: Sculpting Tabular and Non-Tabular Data on the Web	NA:NA:NA:NA	2018
Ram G. Athreya:Axel-Cyrille Ngonga Ngomo:Ricardo Usbeck	In this demo, we introduce the DBpedia chatbot, a knowledge-graph-driven chatbot designed to optimize community interaction. The bot was designed for integration into community software to facilitate the answering of recurrent questions. Four main challenges were addressed when building the chatbot, namely (1) understanding user queries, (2) fetching relevant information based on the queries, (3) tailoring the responses based on the standards of each output platform (i.e. Web, Slack, Facebook) as well as (4) developing subsequent user interactions with the DBpedia chatbot. With this demo, we will showcase our solutions to these four challenges.	Enhancing Community Interactions with Data-Driven Chatbots--The DBpedia Chatbot	NA:NA:NA	2018
Mayank Kejriwal:Daniel Gilley:Pedro Szekely:Jill Crisman	In this demonstration, we present the Text-enabled Humanitarian Operations in Real-time (THOR) framework, which is being prototyped to provide visual and analytical situational awareness to humanitarian and disaster relief (HADR) planners. THOR is a collaborative effort between industrial and university research laboratories, designed with an intent to support both military and civilian HADR operations. At its core, THOR is powered by a domain-specific knowledge graph, which is derived from natural language outputs and is amenable to real-time analytics. THOR is designed to operate in low-resource linguistic environments, process heterogeneous data, including news and social media, reason about arbitrary disasters not knowable in advance, and provide advanced graphical interaction capabilities. We will demo the latest prototype of THOR using an interactive case study situation.	THOR: Text-enabled Analytics for Humanitarian Operations	NA:NA:NA:NA	2018
Mehdi Bahrami:Junhee Park:Lei Liu:Wei-Peng Chen	Application Programming Interface (API) exposes data and functions of a software application to third-party users. In digital business, API economy is one of the key component for determining the value of provided services. With the rise in number of publicly available APIs, understanding each API endpoint manually is not only labor intensive but it is also an error prone task for software engineers. Due to the complexity of understanding the sheer number of APIs, it is difficult for software developers to find the best possible API combinations (i.e. API Mashups). In this demonstration, we introduce API Learning platform which employs machine-learning based technologies to efficiently search APIs, validate APIs, and generate API mashups. These technologies enable a machine to automatically generate machine-readable API specification from API documentations, understand variety of APIs, validate extracted information through automated API validation, and finally recommend API mashups for a specific purpose. As of now, API Learning platform collected over 14,000 API documentations and generates a machine readable format for REST APIs with an accuracy of 84%. The proposed demo prototype shows how it enables users to quickly find relevant APIs, automatically verify API availability, and get the best possible API mashup recommendations.	API Learning: Applying Machine Learning to Manage the Rise of API Economy	NA:NA:NA:NA	2018
Kashyap Popat:Subhabrata Mukherjee:Jannik Strötgen:Gerhard Weikum	Rapid increase of misinformation online has emerged as one of the biggest challenges in this post-truth era. This has given rise to many fact-checking websites that manually assess doubtful claims. However, the speed and scale at which misinformation spreads in online media inherently limits manual verification. Hence, the problem of automatic credibility assessment has attracted great attention. In this work, we present CredEye, a system for automatic credibility assessment. It takes a natural language claim as input from the user and automatically analyzes its credibility by considering relevant articles from the Web. Our system captures joint interaction between language style of articles, their stance towards a claim and the trustworthiness of the sources. In addition, extraction of supporting evidence in the form of enriched snippets makes the verdicts of CredEye transparent and interpretable.	CredEye: A Credibility Lens for Analyzing and Explaining Misinformation	NA:NA:NA:NA	2018
Arseny Kurnikov:Klaudia Krawiecka:Andrew Paverd:Mohammad Mannan:N. Asokan	Although passwords are by far the most widely-used user authentication mechanism on the web, their security is threatened by password phishing and password database breaches. SafeKeeper is a system for protecting web passwords against very strong adversaries, including sophisticated phishers and compromised servers. Compared to other approaches, one of the key differentiating aspects of SafeKeeper is that it provides web users with verifiable assurance that their passwords are being protected. In this paper, we demonstrate precisely how SafeKeeper can be used to protect web passwords in real-world systems. We first explain two important deployability aspects: i) how SafeKeeper can be integrated into the popular WordPress platform, and ii) how ordinary web users can use Intel SGX remote attestation to verify that SafeKeeper is running on a particular server. We then describe three demonstrations to illustrate the use of SafeKeeper: i) showing the user experience when visiting a legitimate website; ii) showing the encryption of the password in transit via live packet-capture; and iii) showing how SafeKeeper performs in the presence of phishing.	Using SafeKeeper to Protect Web Passwords	NA:NA:NA:NA:NA	2018
Welderufael B. Tesfay:Peter Hofmann:Toru Nakamura:Shinsaku Kiyomoto:Jetzabel Serna	With the continuing growth of the Internet landscape, users share large amount of personal, sometimes, privacy sensitive data. When doing so, often, users have little or no clear knowledge about what service providers do with the trails of personal data they leave on the Internet. While regulations impose rather strict requirements that service providers should abide by, the defacto approach seems to be communicating data processing practices through privacy policies. However, privacy policies are long and complex for users to read and understand, thus failing their mere objective of informing users about the promised data processing behaviors of service providers. To address this pertinent issue, we propose a machine learning based approach to summarize the rather long privacy policy into short and condensed notes following a risk-based approach and using the European Union (EU) General Data Protection Regulation (GDPR) aspects as assessment criteria. The results are promising and indicate that our tool can summarize lengthy privacy policies in a short period of time, thus supporting users to take informed decisions regarding their information disclosure behaviors.	I Read but Don't Agree: Privacy Policy Benchmarking using Machine Learning and the EU GDPR	NA:NA:NA:NA:NA	2018
Alo Allik:Florian Thalmann:Mark Sandler	MusicLynx is a web application for music discovery that enables users to explore an artist similarity graph constructed by linking together various open public data sources. It provides a multifaceted browsing platform that strives for an alternative, graph-based representation of artist connections to the grid-like conventions of traditional recommendation systems. Bipartite graph filtering of the Linked Data cloud, content-based music information retrieval, machine learning on crowd-sourced information and Semantic Web technologies are combined to analyze existing and create new categories of music artists through which they are connected. The categories can uncover similarities between artists who otherwise may not be immediately associated: for example, they may share ethnic background or nationality, common musical style or be signed to the same record label, come from the same geographic origin, share a fate or an affliction, or have made similar lifestyle choices. They may also prefer similar musical keys, instrumentation, rhythmic attributes, or even moods their music evokes. This demonstration is primarily meant to showcase the graph-based artist discovery interface of MusicLynx: how artists are connected through various categories, how the different graph filtering methods affect the topology and geometry of linked artists graphs, and ways in which users can connect to external services for additional content and information about objects of their interest.	MusicLynx: Exploring Music Through Artist Similarity Graphs	NA:NA:NA	2018
Yihong Zhang:Panote Siriaraya:Yuanyuan Wang:Shoko Wakamiya:Yukiko Kawai:Adam Jatowt	For a traveler to enjoy a trip in a city, one important factor is the diversity of sceneries and facilities along the route. Current navigation systems can provide the shortest route between two points, as well as scenic or safe routes. However, diversity is largely ignored in existing works. In this paper, we present a system that provides diversity-based route recommendation. It measures visual-based diversity and facility-based diversity with information extracted from publicly available data such as Google Street View images and FourSquare venues. As we will show, the current prototype system is able to provide diversity-based route recommendation for city areas in San Fransisco and Kyoto.	Walking down a Different Path: Route Recommendation based on Visual and Facility based Diversity	NA:NA:NA:NA:NA:NA	2018
Quyu Kong:Marian-Andrei Rizoiu:Siqi Wu:Lexing Xie	What makes content go viral Which videos become popular and why others don't Such questions have elicited significant attention from both researchers and industry, particularly in the context of online media. A range of models have been recently proposed to explain and predict popularity; however, there is a short supply of practical tools, accessible for regular users, that leverage these theoretical results. HIPie--an interactive visualization system--is created to fill this gap, by enabling users to reason about the virality and the popularity of online videos. It retrieves the metadata and the past popularity series of Youtube videos, it employs the Hawkes Intensity Process, a state-of-the-art online popularity model for explaining and predicting video popularity, and it presents videos comparatively in a series of interactive plots. This system will help both content consumers and content producers in a range of data-driven inquiries, such as to comparatively analyze videos and channels, to explain and to predict future popularity, to identify viral videos, and to estimate responses to online promotion.	Will This Video Go Viral: Explaining and Predicting the Popularity of Youtube Videos	NA:NA:NA:NA	2018
Michel Buffa:Jerome Lebrun	The ANR project WASABI will last 42 months and consists in developing a 2 million songs database with interactive WebAudio enhanced client applications. Client applications target composers, music schools, sound engineering schools, musicologists, music streaming services and journalists. In this paper, we present a virtual pedal board (a set of chainable audio effects on the form of "pedals"), and a guitar tube amplifier simulation for guitarists, that will be associated with songs from the WASABI database. Music schools and music engineering schools are interested in such tools that can be run in a Web page, without the need to install any further software. Take a classic rock song: isolate the guitar solo, study it, then mute it and play guitar real-time along the other tracks using an online guitar amplifier that reproduces the real guitar amp model used in the song, with its signature sound, proper dynamic and frequency response. Add some audio effects such as a reverberation, a delay, a flanger, etc. in order to reproduce Pink Floyd's guitar sound or Eddie Van Halen famous "Brown Sound". Learn interactively, guitar in hands, how to fine tune a compressor effect, or how to shape the sound of a tube guitar amp, how to get a "modern metal" or a "Jimi Hendrix" sound, using only your Web browser.	Real-Time Emulation of a Marshall JCM 800 Guitar Tube Amplifier, Audio FX Pedals, in a Virtual Pedal Board	NA:NA	2018
Giulio Rossetti:Letizia Milli:Salvatore Rinzivillo	Nowadays the analysis of dynamics of and on networks represents a hot topic in the Social Network Analysis playground. To support students, teachers, developers and researchers we introduced a novel framework, named NDlib, an environment designed to describe diffusion simulations. NDlib is designed to be a multi-level ecosystem that can be fruitfully used by different user segments. Upon NDlib, we designed a simulation server that allows remote execution of experiments as well as an online visualization tool that abstracts its programmatic interface and makes available the simulation platform to non-technicians.	NDlib: A Python Library to Model and Analyze Diffusion Processes over Complex Networks	NA:NA:NA	2018
Angela Bonifati:Wim Martens:Thomas Timm	In this demonstration, we showcase DARQL, the first tool for deep, large-scale analysis of SPARQL queries. We have harvested a large corpus of query logs with different lineage and sizes, from DBPedia to BioPortal and Wikidata, whose total number of queries amounts to 180M. We ran a wide range of analyses on the corpus, spanning from simple tasks (keyword counts, triple counts, operator distributions), moderately deep tasks (projection test, query classification), and deep analysis (shape analysis, well-designedness, weakly well-designedness, hypertreewidth, and fractional edge cover). The key goal of our demonstration is to let the users dive into the SPARQL query logs of our corpus and let them discover the inherent characteristics of the queries. The entire corpus of SPARQL queries is stored in a DBMS. The tool has a GUI that allows users to ask sophisticated analytical queries on the SPARQL logs. These analytical queries can both be directly written in SQL or composed by a visual query builder tool. The results of the analytical queries are represented both textually (as SPARQL queries) and visually. The DBMS performs the searches within the corpus quite efficiently. To the best of our knowledge, this is the first demonstration of this kind on such a large corpus and with such a number of varied tests.	DARQL: Deep Analysis of SPARQL Queries	NA:NA:NA	2018
Sepideh Mesbah:Alessandro Bozzon:Christoph Lofi:Geert-Jan Houben	This demo presents SmartPub, a novel web-based platform that supports the exploration and visualization of shallow meta-data (e.g., author list, keywords) and deep meta-data--long tail named entities which are rare, and often relevant only in specific knowledge domain--from scientific publications. The platform collects documents from different sources (e.g. DBLP and Arxiv), and extracts the domain-specific named entities from the text of the publications using Named Entity Recognizers (NERs) which we can train with minimal human supervision even for rare entity types. The platform further enables the interaction with the Crowd for filtering purposes or training data generation, and provides extended visualization and exploration capabilities. SmartPub will be demonstrated using sample collection of scientific publications focusing on the computer science domain and will address the entity types Dataset (i.e. dataset presented or used in a publication), and Methods (i.e. algorithms used to create/enrich/analyse a data set)	SmartPub: A Platform for Long-Tail Entity Extraction from Scientific Publications	NA:NA:NA:NA	2018
Andrea Mauri:Achilleas Psyllidis:Alessandro Bozzon	Having a thorough understanding of energy consumption behavior is an important element of sustainability studies. Traditional sources of information about energy consumption, such as smart meter devices and surveys, can be costly to deploy, may lack contextual information or have infrequent updates. In this paper, we examine the possibility of extracting energy consumption-related information from user-generated content. More specifically, we develop a pipeline that helps identify energy-related content in Twitter posts and classify it into four categories (dwelling, food, leisure, and mobility), according to the type of activity performed. We further demonstrate a web-based application--called Social Smart Meter--that implements the proposed pipeline and enables different stakeholders to gain an insight into daily energy consumption behavior, as well as showcase it in case studies involving several world cities.	Social Smart Meter: Identifying Energy Consumption Behavior in User-Generated Content	NA:NA:NA	2018
Freya Behrens:Sebastian Bischoff:Pius Ladenburger:Julius Rückin:Laurenz Seidel:Fabian Stolp:Michael Vaichenker:Adrian Ziegler:Davide Mottin:Fatemeh Aghaei:Emmanuel Müller:Martin Preusse:Nikola Müller:Michael Hunger	We present MetaExp, a system that assists the user during the exploration of large knowledge graphs, given two sets of initial nodes. At its core, MetaExp presents a small set of meta-paths to the user, which are sequences of relationships among node types. Such meta-paths do not overwhelm the user with complex structures, yet they preserve semantically-rich relationships in a graph. MetaExp engages the user in an interactive procedure, which involves simple meta-paths evaluations to infer a user-specific similarity measure. This similarity measure incorporates the domain knowledge and the preferences of the user, overcoming the fundamental limitations of previous methods based on local node neighborhoods or statically determined similarity scores. Our system provides a user-friendly interface for searching initial nodes and guides the user towards progressive refinements of the meta-paths. The system is demonstrated on three datasets, Freebase, a movie database, and a biological network.	MetaExp: Interactive Explanation and Exploration of Large Knowledge Graphs	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Rudolf Schneider:Sebastian Arnold:Tom Oberhauser:Tobias Klatt:Thomas Steffek:Alexander Löser	We demonstrate Smart-MD, an information retrieval system for medical professionals. The system supports topical queries in the form [disease topic], such as ["lyme disease", treatments]. In contrast to document-oriented retrieval systems, Smart-MD retrieves relevant paragraphs and reduces the reading load of a medical doctor drastically. We recognize diseases and topical aspects with a novel paragraph retrieval method based on bidirectional LSTM neural networks. We demonstrate Smart-MD on a dataset that contains 3,469 diseases from the English language part of Wikipedia and 6,876 distinct medical aspects extracted from Wikipedia headlines.	Smart-MD: Neural Paragraph Retrieval of Medical Topics	NA:NA:NA:NA:NA:NA	2018
Yanyan Wang:Qun Chen:Xin Liu:Murtadha Ahmed:Zhanhuai Li:Wei Pan:Hailong Liu	The state-of-the-art techniques for aspect-level sentiment analysis focus on feature modeling using a variety of deep neural networks (DNN). Unfortunately, their practical performance may fall short of expectations due to semantic complexity of natural languages. Motivated by the observation that linguistic hints (e.g. explicit sentiment words and shift words) can be strong indicators of sentiment, we present a joint framework, SenHint, which integrates the output of deep neural networks and the implication of linguistic hints into a coherent reasoning model based on Markov Logic Network (MLN). In SenHint, linguistic hints are used in two ways: (1) to identify easy instances, whose sentiment can be automatically determined by machine with high accuracy; (2) to capture implicit relations between aspect polarities. We also empirically evaluate the performance of SenHint on both English and Chinese benchmark datasets. Our experimental results show that SenHint can effectively improve accuracy compared with the state-of-the-art alternatives.	SenHint: A Joint Framework for Aspect-level Sentiment Analysis by Deep Neural Networks and Linguistic Hints	NA:NA:NA:NA:NA:NA:NA	2018
Michele Ruta:Floriano Scioscia:Giuseppe Loseto:Filippo Gramegna:Saverio Ieva:Agnese Pinto:Eugenio Di Sciascio	ThePhysical Semantic Web (PSW) is a novel paradigm built upon the Google Physical Web (PW) approach and devoted to improve the quality of interactions in the Web of Things. Beacons expose semantic annotations instead of basic identifiers, ıe\ machine-understandable descriptions of physical resources. This enables novel ontology-based object advertisement and discovery and --in turn-- advanced user-to-thing and autonomous thing-to-thing interactions. The demo shows the evolution from the PW to the PSW in a discovery scenario set in a winery, where bottles are equipped with Bluetooth Low Energy beacons and a customer can discover them using her smartphone. The final goal is to prove benefits of PSW over basic PW, including: rich semantic-based object annotation; dynamic annotations exploiting on-board sensors; enhanced discovery and ranking of nearby objects through semantic matchmaking; availability of interactions even without working Internet infrastructure, by means of point-to-point data exchanges.	A journey from the Physical Web to the Physical Semantic Web	NA:NA:NA:NA:NA:NA:NA	2018
Mohamed Abdel Maksoud:Gaurav Pandey:Shuaiqiang Wang	This paper addresses a novel tour discovery problem in the domain of travel search. We create a ranking of tours for a set of travel interests, where a tour is a group of city documents and a travel interest is a query. While generating and ranking tours, it is aimed that each interest (from the interest set) is satisfied by at least one city in a tour and the distance traveled to cover the tour is not too large. Firstly, we generate tours for the interest set, by utilizing the available ranking of cities for the individual interests and the distances between the cities. Then, in absence of existing methods directly related to our problem, we devise our novel techniques to calculate ranking scores for the tours and present a comparison of these techniques in our results. We demonstrate our web application Travición, that utilizes the best tour scoring technique.	Finding Tours for a Set of Interests	NA:NA:NA	2018
Mirko Marras:Matteo Manca:Ludovico Boratto:Gianni Fenu:David Laniado	The massive amount and variety of city-related data raise equally big challenges to enable citizens to make sense of such data for improving their daily life and fostering collective decision making. The existing dashboards include limited pre-defined use cases which can only address the most common needs of citizens, but do not allow for personalization. In this work, we propose an open source environment that enables citizens to easily explore city-related data. A backend continuously collects heterogeneous data, turns them into a unified format and exposes them through an API; a front-end allows users to create interactive visualizations combining different data sources and visual models, and to share them to engage others. In this way, citizens can build up a data-driven public awareness supporting an open, transparent, and collaborative city.	BarcelonaNow: Empowering Citizens with Interactive Dashboards for Urban Data Exploration	NA:NA:NA:NA:NA	2018
Giorgos Argyriou:George Papadakis:George Stamoulis:Efi Karra Taniskidou:Nikiforos Pittaras:George Giannakopoulos:Sergio Albani:Michele Lazzarini:Emanuele Angiuli:Anca Popescu:Argyros Argyridis:Manolis Koubarakis	GeoSensor is a novel system that enriches change detection over satellite images with event detection over news items and social media content. GeoSensor faces the major challenges of Big Data: volume (a single satellite image may be a few GBs), variety (its data sources include two different types of satellite images and various types of user-generated content) and veracity, as the accuracy of the end result is crucial for the usefulness of our system. To overcome these three challenges, while offering on-line functionality, GeoSensor comprises a complex architecture that is based on the open-source platform developed in the H2020 project Big Data Europe. Through the presented demonstration, both the effectiveness and the efficiency of GeoSensor's functionalities are highlighted.	GeoSensor: On-line Scalable Change and Event Detection over Big Data	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Weijian Zhang:Jonathan Deakin:Nicholas J. Higham:Shuaiqiang Wang	We present Etymo (https://etymo.io), a discovery engine to facilitate artificial intelligence (AI) research and development. It aims to help readers navigate a large number of AI-related papers published every week by using a novel form of search that finds relevant papers and displays related papers in a graphical interface. Etymo constructs and maintains an adaptive similarity-based network of research papers as an all-purpose knowledge graph for ranking, recommendation, and visualisation. The network is constantly evolving and can learn from user feedback to adjust itself. A screencast is available at: https://youtu.be/T4FDPk_TmN0	Etymo: A New Discovery Engine for AI Research	NA:NA:NA:NA	2018
Tanya Chowdhury:Aashay Mittal:Tanmoy Chakraborty	In this demo, we present VIZ-Wiki, a browser extension which generates an overview of summarizable threads in Question Answering forums. It reduces a user's effort to go through lengthy text-based, sarcastic and highly critiqued answers. Our tool can be used to collect community opinion from popular discussion sites like Quora, Yahoo! Answers, Reddit etc. as well as topic-centric ones such as Askubuntu, Stackoverflow. We rely on textual information of these forums to extract insightful summaries for a reader. VIZ-Wiki provides users a pie-graph view marking popular choices when such a question link is raised. A button guides them to detailed statistics and relevant list of answers. It further highlights sentences relevant to an answer choice in the text. VIZ-Wiki deals with answers contradicted by other users, prioritizes highly-recommended ones and avoids sarcasm. We test our model on the factoid questions dataset of Yahoo! Answers and obtain a macro precision of 0.6 on displayed answers and a macro recall of 0.69, beating the baseline significantly. To the best of our knowledge, VIZ-Wiki is the first attempt to visualize answers for questions in community question answering services. In the spirit of reproducibility, we have released the code and a demonstration video public at \urlhttp://goo.gl/cyx3EF and \urlhttps://youtu.be/XNmRa_jtmC8 respectively	VIZ-Wiki: Generating Visual Summaries to Factoid Threads in Community Question Answering Services	NA:NA:NA	2018
Benjamin D. Horne:William Dron:Sara Khedr:Sibel Adali	Today, journalist, information analyst, and everyday news consumers are tasked with discerning and fact-checking the news. This task has became complex due to the ever-growing number of news sources and the mixed tactics of maliciously false sources. To mitigate these problems, we introduce the The News Landscape (NELA) Toolkit: an open source toolkit for the systematic exploration of the news landscape. NELA allows users to explore the credibility of news articles using well-studied content-based markers of reliability and bias, as well as, filter and sort through article predictions based on the user's own needs. In addition, NELA allows users to visualize the media landscape at different time slices using a variety of features computed at the source level. NELA is built with a modular, pipeline design, to allow researchers to add new tools to the toolkit with ease. Our demo is an early transition of automated news credibility research to assist human fact-checking efforts and increase the understanding of the news ecosystem as a whole.	Assessing the News Landscape: A Multi-Module Toolkit for Evaluating the Credibility of News	NA:NA:NA:NA	2018
Debabrata Mahapatra:Ragunathan Mariappan:Vaibhav Rajan:Kuldeep Yadav:Seby A.:Sudeshna Roy	The number of high quality online videos is increasing rapidly. Online courses as well as universities do not fully leverage the content due to several open challenges in video search, indexing, summarization and customization requirements for specific courses, instructors or learners. We present a new web-based social learning platform called Videoken. Using novel video summarization algorithms, Videoken automatically creates Table of Contents for videos. This allows a textbook-like facility for non-linear search and navigation through the video, enables extraction of semantically coherent clips from within a video and improves video search through better semantic indexing. The platform also allows new ways of course creation and sharing of learning modules; and can be both integrated with existing Learning Management Systems and used independently.	VideoKen: Automatic Video Summarization and Course Curation to Support Learning	NA:NA:NA:NA:NA:NA	2018
Harshita Jhavar:Paramita Mirza	We present EMOFIEL, a system that identifies characters and scenes in a story from a fictional narrative summary, generates appropriate scene descriptions, identifies the emotion flow between a given directed pair of story characters in each interaction, and organizes them along the story timeline. These emotions are identified using two emotion modelling approaches: categorical and dimensional emotion models. The generated plots show that in a particular scene, two characters can share multiple emotions together with different intensity. Furthermore, the directionality of the emotion can be captured as well, depending on which character is more dominant in each interaction. EMOFIEL provides a web-based GUI that allows users to query the annotated stories to explore the emotion mapping of a given character pair throughout a given story, and to explore scenes for which a certain emotion peaks.	EMOFIEL: Mapping Emotions of Relationships in a Story	NA:NA	2018
Miltiadis Lytras:Naif Radi Aljohani:Amir Hussain:Jiebo Luo:Jacky Xi Zhang	Cognitive Computing has received increasing attention from academia and industries as it brings cognitive science and computing together for the development of new computational platforms, infrastructures, systems and algorithms. Artificial intelligence and computational intelligence are key elements to succeed in cognitive computing. The Cognitive Computing track was successfully organized at WWW2017. This year, WWW2018 continues this track with a focus on all applications. Particularly, applications in healthcare, environment, education, sustainability, smart cities and food science are among the important global challenges in 21st Century.	Cognitive Computing Track Chairs' Welcome & Organization	NA:NA:NA:NA:NA	2018
Iqra Safder:Saeed-Ul Hassan:Naif Radi Aljohani	Although, over the years, information retrieval systems have shown tremendous improvements in searching for relevant scientific literature, human cognition is still required to search for specific document elements in full text publications. For instance, pseudocodes pertaining to algorithms published in scientific publications cannot be correctly matched against user queries, hence the process requires human involvement. AlgorithmSeer, a state-of-the-art technique, claims to replace humans in this task, but one of the limitations of such an algorithm search engine is that the metadata is simply a textual description of each pseudocode, without any algorithm-specific information. Hence, the search is performed merely by matching the user query to the textual metadata and ranking the results using conventional textual similarity techniques. The ability to automatically identify algorithm-specific metadata such as precision, recall, or f-measure would be useful when searching for algorithms. In this article, we propose a set of algorithms to extract further information pertaining to the performance of each algorithm. Specifically, sentences in an article that convey information about the efficiency of the corresponding algorithm are identified and extracted using a recurrent convolutional neural network (RCNN). Furthermore, we propose improving the efficacy of the pseudocode detection task by using a multi-layer perceptron (MLP) classification trained with 15 features, which improves the classification performance of the state-of-the-art pseudocode detection methods used in AlgorithmSeer by 27%. Finally, we show the advantages of the AI-enabled search engine (based on RCNN and MLP models) over conventional text-retrieval models.	AI Cognition in Searching for Relevant Knowledge from Scholarly Big Data, Using a Multi-layer Perceptron and Recurrent Convolutional Neural Network Model	NA:NA:NA	2018
Ali Daud:Akbar Hussain:Rabeeh Ayaz Abbasi:Naif Radi Aljohani:Tehmina Amjad:Hassan Dawood	Players are ranked in various sports to show their importance over other players. Existing methods only consider intra-type links (e.g., player to player and team to team), but ignore inter-type links (e.g., one type of player to another type of player, such as batsman to bowler and player to team) based on cognitive aspects. They also ignore the spatiality of the players. There is a strong relationship among players and their teams, which can be represented as a network consisting of multi-type interrelated objects. In this paper, we propose a players' ranking method, called Region-wise Players Link Fusion (RPLF) which is applied to the sport of cricket. RPLF considers players' region-wise intra-type and inter-type relation-based features to rank the players. Considering multi-type interrelated objects is based on the intuition that a batsman scoring high against top bowlers of a strong team or a bowler taking wickets against top batsmen of a strong team is considered as a good player. The experimental results show that RPLF provides promising insights of players' rankings. RLFP is a generic method and can be applied to different sports for ranking players.	Region-wise Ranking of Sports Players based on Link Fusion	NA:NA:NA:NA:NA:NA	2018
Debabrata Mahapatra:Ragunathan Mariappan:Vaibhav Rajan	The number of freely available online educational videos from universities and other organizations is growing rapidly. Accurate indexing and summarization are essential for efficient search, recommendation and effective consumption of videos. In this paper, we describe a new method of automatically creating a hierarchical table of contents for a video. It provides a summary of the video content along with a textbook--like facility for nonlinear navigation and search through the video. Our multimodal approach combines new methods for shot level video segmentation and for hierarchical summarization. Empirical results demonstrate the efficacy of our approach on many educational videos.	Automatic Hierarchical Table of Contents Generation for Educational Videos	NA:NA:NA	2018
Kwan Hui Lim:Kate E. Lee:Dave Kendal:Lida Rashidi:Elham Naghizade:Stephan Winter:Maria Vasardani	Green spaces are believed to improve the well-being of users in urban areas. While there are urban research exploring the emotional benefits of green spaces, these works are based on user surveys and case studies, which are typically small in scale, intrusive, time-intensive and costly. In contrast to earlier works, we utilize a non-intrusive methodology to understand green space effects at large-scale and in greater detail, via digital traces left by Twitter users. Using this methodology, we perform an empirical study on the effects of green spaces on user sentiments and emotions in Melbourne, Australia and our main findings are: (i) tweets in green spaces evoke more positive and less negative emotions, compared to those in urban areas; (ii) each season affects various emotion types differently; (iii) there are interesting changes in sentiments based on the hour, day and month that a tweet was posted; and (iv) negative sentiments are typically associated with large transport infrastructures such as train interchanges, major road junctions and railway tracks. The novelty of our study is the combination of psychological theory, alongside data collection and analysis techniques on a large-scale Twitter dataset, which overcomes the limitations of traditional methods in urban research.	The Grass is Greener on the Other Side: Understanding the Effects of Green Spaces on Twitter User Sentiments	NA:NA:NA:NA:NA:NA:NA	2018
Lin Mu:Peiquan Jin:Lizhou Zheng:En-Hong Chen:Lihua Yue	Microblog like Twitter and Sina Weibo has been an important information source for event detection and monitoring. In many decision-making scenarios, it is not enough to only provide a structural tuple for an event, e.g., a 5W1H record like <who, where, when, what, whom, how>. However, in addition to event structural tuples, people need to know the evolution lifecycle of an event. The lifecycle description of an event is more helpful for decision making because people can focus on the progress and trend of events. In this paper, we propose a novel method for efficiently detecting and tracking event evolution on microblogging platforms. The major features of our study are: (1) It provides a novel event-type-driven method to extract event tuples, which forms the foundation for event evolution analysis. (2) It describes the lifecycle of an event by a staged model, and provides effective algorithms for detecting the stages of an event. (3) It offers emotional analysis over the stages of an event, through which people are able to know the public emotional tendency over a specific event at different time periods. We build a prototype system and present its architecture and implemental details in the paper. In addition, we conduct experiments on real microblog datasets and the results in terms of precision, recall, and F-measure suggest the effectiveness and efficiency of our proposal.	Lifecycle-Based Event Detection from Microblogs	NA:NA:NA:NA:NA	2018
Jiongqian Liang:Peter Jacobs:Srinivasan Parthasarathy	Hurricane-induced flooding can lead to substantial loss of life and huge damage to infrastructure. Mapping flood extent from satellite or aerial imagery is essential for prioritizing relief efforts and for assessing future flood risk. Identification of water extent in such images can be challenging considering the heterogeneity in water body size and shape, cloud cover, and natural variations in land cover. In this effort, we introduce a novel cognitive framework based on a semi-supervised learning algorithm, called HUman-Guided Flood Mapping (HUG-FM), specifically designed to tackle the flood mapping problem. Our framework first divides the satellite or aerial image into patches leveraging a graph-based clustering approach. A domain expert is then asked to provide labels for a few patches (as opposed to pixels which are harder to discern). Subsequently, we learn a classifier based on the provided labels to map flood extent. We test the efficacy and efficiency of our framework on imagery from several recent flood-induced emergencies and results show that our algorithm can robustly and correctly detect water areas compared to the state-of-the-art. We then evaluate whether expert guidance can be replaced by the wisdom of a crowd (e.g., crisis volunteers). We design an online crowdsourcing platform based on HUG-FM and propose a novel ensemble method to leverage crowdsourcing efforts. We conduct an experiment with over $50$ participants and show that crowdsourced HUG-FM (CHUG-FM) can approach or even exceed the performance of a single expert providing guidance (HUG-FM).	Human-Guided Flood Mapping: From Experts to the Crowd	NA:NA:NA	2018
Patrick Watson:TengFei Ma:Ravi Tejwani:Maria Chang:JaeWook Ahn:Sharad Sundararajan	The availability of open educational resources (OER) has enabled educators and researchers to access a variety of learning assessments online. OER communities are particularly useful for gathering multiple choice questions (MCQs), which are easy to grade, but difficult to design well. To account for this, OERs often rely on crowd-sourced data to validate the quality of MCQs. However, because crowds contain many non-experts, and are susceptible to question framing effects, they may produce ratings driven by guessing on the basis of surface-level linguistic features, rather than deep topic knowledge. Consumers of OER multiple choice questions (and authors of original multiple choice questions) would benefit from a tool that automatically provided feedback on assessment quality, and assessed the degree to which OER MCQs are susceptible to framing effects. This paper describes a model that is trained to use domain-naive strategies to guess which multiple choice answer is correct. The extent to which this model can predict the correct answer to an MCQ is an indicator that the MCQ is a poor measure of domain-specific knowledge. We describe an integration of this model with a front-end visualizer and MCQ authoring tool.	Human-level Multiple Choice Question Guessing Without Domain Knowledge: Machine-Learning of Framing Effects	NA:NA:NA:NA:NA:NA	2018
Rui Yan:Dongyan Zhao	To establish an automatic conversation system between human and computer is regarded as one of the most hardcore problems in computer science. It requires interdisciplinary techniques of information retrieval, natural language processing, data management as well as artificial intelligence. The arrival of big data era reveals the feasibility to create a conversation system empowered by data-driven approaches. Now we are able to collect extremely large conversational data on Web, and organize them to launch a human-computer conversation system. Owing to the diversity of Web resources available, a retrieval-based conversation system will be able to find at least some responses from the massive data repository for any user inputs. Given a human issued utterance, i.e., a query, a retrieval-based conversation system will search for appropriate replies, conduct a relevance ranking, and then output the highly relevant one as the response. In this paper, we propose a novel retrieval model named NeuRetrieval for short text understanding, representation and semantic matching. The proposed model is general and unified for both single-turn and multi-turn conversation scenarios in open domain. In the experiments, we investigate the effectiveness of the proposed deep neural network model for human-computer conversations. We demonstrate performance improvement against a series of baseline methods in several evaluation metrics. In contrast with previously proposed methods, NeuRetrieval is tailored for conversation scenarios and demonstrated to be more effective.	A NeuRetrieval Model for Human-Computer Conversations	NA:NA	2018
Yihang Cheng:Xi Zhang:Hao Wang:Shang Jiang	Community Question Answering (CQA) has emerged recently and it becomes popular among people. During the process of the communication, different knowledge can be merged. Recently, several vendors use the business model of paying for knowledge to make these knowledge to the monetary benefits, then the Pay-for-Knowledge Communities (PKC) have been applied. Even PKC has interesting business model, there are several problems to be solved, and one of the most salient problem is that questioners may takes too long time to choose the most valuable answers, leading to questioners not able to pay for suitable answerers and many problems about platforms' operation There are several previous research has focused on this problem but still have not found satisfactory solutions as questions and answers are more and more complex in PKC platforms. With the development of cognitive computing techniques, applying an intelligent QA system in PKC to improve the answering effectiveness may be possible. In this paper, we tried to investigate how to apply the intelligent QA system into PKC platform to improve the answering effectiveness. For solving the problems of matching complex questions and answers, we present a Four Module QA Model based on the normal intelligent QA System. Compared to normal intelligent QA System, our model uses categories to classify the questions with traditional machine learning methods. We use answers in each category of corresponding questions as one dataset, answers in each entity of corresponding question as the other dataset, finally, these two datasets make up the document database. Then we got the best answer among past answers through comparing the TF-IDF weighted bag-of word vectors of two datasets or the new answer including key words through Long Short-Term Memory (LSTM) algorithm with PKC's features composed of centrality and money. Experiments were developed on a dataset with 1222 users' QA sites collected from a QA community. The model we proposed is expected to increase QA's effectiveness and improve the business model of Pay-for-Knowledge Communities.	How to Improve the Answering Effectiveness in Pay-for-Knowledge Community: An Exploratory Application of Intelligent QA System	NA:NA:NA:NA	2018
Sylvio Barbon Junior:Gabriel Marques Tavares:Victor G. Turrisi da Costa:Paolo Ceravolo:Ernesto Damiani	One of the main challenges of Cognitive Computing (CC) is reacting to evolving environments in near-real time. Therefore, it is expected that CC models provide solutions by examining a summary of past history, rather than using full historical data. This strategy has significant benefits in terms of response time and space complexity but poses new challenges in term of concept-drift detection, where both long term and short terms dynamics should be taken into account. In this paper, we introduce the Concept-Drift in Event Stream Framework (CDESF) that addresses some of these challenges for data streams recording the execution of a Web-based business process. Thanks to CDESF support for feature transformation, we perform density clustering in the transformed feature space of the process event stream, observe track concept-drift over time and identify anomalous cases in the form of outliers. We validate our approach using logs of an e-healthcare process.	A Framework for Human-in-the-loop Monitoring of Concept-drift Detection in Event Log Stream	NA:NA:NA:NA:NA	2018
Peijun Zhao:Jia Jia:Yongsheng An:Jie Liang:Lexing Xie:Jiebo Luo	Emojis can be regarded as a language for graphical expression of emotions, and have been widely used in social media. They can express more delicate feelings beyond textual information and improve the effectiveness of computer-mediated communication. Recent advances in machine learning make it possible to automatic compose text messages with emojis. However, the usages of emojis can be complicated and subtle so that analyzing and predicting emojis is a challenging problem. In this paper, we first construct a benchmark dataset of emojis with tweets and systematically investigate emoji usages in terms of tweet content, tweet structure and user demographics. Inspired by the investigation results, we further propose a multitask multimodality gated recurrent unit (mmGRU) model to predict the categories and positions of emojis. The model leverages not only multimodality information such as text, image and user demographics, but also the strong correlations between emoji categories and their positions. Our experimental results show that the proposed method can significantly improve the accuracy for predicting emojis for tweets (+9.0% in F1-value for category and +4.6% in F1-value for position). Based on the experimental results, we further conduct a series of case studies to unveil how emojis are used in social media.	Analyzing and Predicting Emoji Usages in Social Media	NA:NA:NA:NA:NA:NA	2018
Vitobha Munigala:Abhijit Mishra:Srikanth G. Tamilselvam:Shreya Khare:Riddhiman Dasgupta:Anush Sankaran	Persuasiveness is a creative art which aims at inducing certain set of beliefs in the target audience. In an e-commerce setting, for a newly launched product, persuasive descriptions are often composed to motivate an online buyer towards a successful purchase. Such descriptions can be catchy taglines, product-summaries, style-tipsetc.. In this paper, we present PersuAIDE! - a persuasive system based on linguistic creativity to generate various forms of persuasive sentences from the input product specification. To demonstrate the effectiveness of the proposed system, we have applied the technology to fashion domain, where, for a given fashion product like"red collar shirt" we were able to generate descriptive sentences that not only explain the item but also garner positive attention, making it persuasive. PersuAIDE! identifies fashion related keywords from input specifications and intelligently expands the keywords to creative phrases. Once such compatible phrases are obtained, persuasive descriptions are synthesized from the set of phrases and input keywords with the help of a neural language model trained on a large domain-specific fashion corpus. We evaluate the system on a large fashion corpus collected from different sources using (a) automatic text generation metrics used for Machine Translation and Automatic Summarization evaluation and Readability measurement, and (b) human judgment scores evaluating the persuasiveness and fluency of the generated text. Experimental results and qualitative analysis show that an unsupervised system like ours can produce more creative and better constructed persuasive output than supervised generative counterparts based on neural sequence-to-sequence models and statistical machine translation.	PersuAIDE ! An Adaptive Persuasive Text Generation System for Fashion Domain	NA:NA:NA:NA:NA:NA	2018
Tianlang Chen:Yuxiao Chen:Han Guo:Jiebo Luo	WeChat Business, developed on WeChat, the most extensively used instant messaging platform in China, is a new business model that bursts into people's lives in the e-commerce era. As one of the most typical WeChat Business behaviors, WeChat users can advertise products, advocate companies and share customer feedback to their WeChat friends by posting a WeChat Moment--a public status that contains images and a text. Given its popularity and significance, in this paper, we propose a novel Bilateral-Attention LSTM network (BiATT-LSTM) to identify WeChat Business Moments based on their texts and images. In particular, different from previous schemes that equally consider visual and textual modalities for a joint visual-textual classification task, we start our work with a text classification task based on an LSTM network, then we incorporate a bilateral-attention mechanism that can automatically learn two kinds of explicit attention weights for each word, namely 1) a global weight that is insensitive to the images in the same Moment with the word, and 2) a local weight that is sensitive to the images in the same Moment. In this process, we utilize visual information as a guidance to figure out the local weight of a word in a specific Moment. Two-level experiments demonstrate the effectiveness of our framework. It outperforms other schemes that jointly model visual and textual modalities. We also visualize the bilateral-attention mechanism to illustrate how this mechanism helps joint visual-textual classification.	When E-commerce Meets Social Media: Identifying Business on WeChat Moment Using Bilateral-Attention LSTM	NA:NA:NA:NA	2018
Hogun Park:Hamid Reza Motahari Nezhad	A lot of knowledge about procedures and how-tos are described in text. Recently, extracting semantic relations from the procedural text has been actively explored. Prior work mostly has focused on finding relationships among verb-noun pairs or clustering of extracted pairs. In this paper, we investigate the problem of learning individual procedure-specific relationships (e.g. is method of, is alternative of, or is subtask of) among sentences. To identify the relationships, we propose an end-to-end neural network architecture, which can selectively learn important procedure-specific relationships. Using this approach, we could construct a how-to knowledge base from the largest procedure sharing-community, wiki-how.com. The evaluation of our approach shows that it outperforms the existing entity relationship extraction algorithms.	Learning Procedures from Text: Codifying How-to Procedures in Deep Neural Networks	NA:NA	2018
Zara Mansoor:Mustansar Ali Ghazanfar:Syed Muhammad Anwar:Ahmed S. Alfakeeh:Khaled H. Alyoubi	This research article focuses on the analysis of electroencephalography (EEG) signals of the brain during pain perception. The proposed system is based on the hypothesis that a noticeable change occurs in mental conditions while experiencing pain. When the human body is injured, sensory receptors in the brain enter a stimulated state. The injury may be the result of attention or an accident. Pain warnings are natural in humans and protect the body from further negative effects. In this article, an innovative and robust system based on prominent features extracted from the brain activity recorded using EEG, is proposed to predict the state of pain perception. The brain signals of subjects are observed using two low-cost EEG headsets including neurosky mindwave mobile and emotiv insight. Time and frequency domain features are selected to represent the observed signals. The results show that a combination of time and frequency domain features is the most informative approach for pain prediction using the observed brain activity.	Pain Prediction in Humans using Human Brain Activity Data	NA:NA:NA:NA:NA	2018
Yunhao Zheng:Xi Zhang:Yuting Xiao	Recommending proper experts to knowledge buyers is a significant problem in online paid Q&A community (OPQC). Existing approaches for online expert recommendation have been mainly focused on exploiting semantic similarities and social network influence, while personalizing recommendation according to individuals' motivations has not received much attention. In this paper, we propose a personalized expert recommender system, which integrates buyer's motivation for knowledge, social influence, and money in a unified framework. As an innovative application of cognitive computing, our recommender system is capable of providing users with the best matching experts so as to help them make the most cost-effective choice in OPQC. To this end, Paragraph Vector technique is implemented to construct domain knowledge base (KB) in a multilayer information retrieval (IR) framework. Then we perform knowledge pricing based on buyer's query and bid in the context of bilateral monopoly knowledge market. After that, a Markov Chain based method with user motivation learning is introduced to find the best matching experts. Finally, we evaluate the proposed approach using datasets collected from two OPQC. The experimental results show encouraging success as effectively offering reasonable personalization options. As an innovative approach to solve the expert matching problem in OPQC, this research provides flexibility in customizing the recommendation heuristics based on user motivation, and demonstrate its contribution to a higher rate of optimal knowledge seller-buyer matching.	Making the Most Cost-effective Decision in Online Paid Q&A Community: An Expert Recommender System with Motivation Modeling and Knowledge Pricing	NA:NA:NA	2018
Tehmina Amjad:Ali Daud:Min Song	With the increase in collaboration among researchers of various disciplines, changing the research topic or working on multiple topics is not an unusual behavior. Several comprehensive efforts have been made for predicting, quantifying, and studying the researcher's impact. The question, that how the change in the field of interest over time or working in more than one topics can influence the scientific impact, remains unanswered. In this research, we study the effect of topic drift on the scientific impact of an author. We apply Author Conference Topic (ACT) model to extract topic distribution of individual authors who are working on multiple topics to compare and analyze with authors who work on a single topic. We analyze the productivity of the authors on the basis of publication count, citation count and h-index. We find that authors who stick to one topic, produce a higher impact and gain more attention. To further strengthen our results we gather the h-index of top-ranked authors working on one topic and top-ranked authors working on multiple topics and examine whether there are similar trends in their progress. The results show an evidence of significant impact of topic drift on career choices of researchers.	Measuring the Impact of Topic Drift in Scholarly Networks	NA:NA:NA	2018
Aditya Mogadala:Bhargav Kanuparthi:Achim Rettinger:York Sure-Vetter	Growth of multimodal content on the web and social media has generated abundant weakly aligned image-sentence pairs. However, it is hard to interpret them directly due to intrinsic intension. In this paper, we aim to annotate such image-sentence pairs with connotations as labels to capture the intrinsic intension. We achieve it with a connotation multimodal embedding model (CMEM) using a novel loss function. It's unique characteristics over previous models include: (i) the exploitation of multimodal data as opposed to only visual information, (ii) robustness to outlier labels in a multi-label scenario and (iii) works effectively with large-scale weakly supervised data. With extensive quantitative evaluation, we exhibit the effectiveness of CMEM for detection of multiple labels over other state-of-the-art approaches. Also, we show that in addition to annotation of image-sentence pairs with connotation labels, byproduct of our model inherently supports cross-modal retrieval i.e. image query - sentence retrieval.	Discovering Connotations as Labels for Weakly Supervised Image-Sentence Data	NA:NA:NA:NA	2018
Zhiyuan He:Su Yang:Weishan Zhang:Jiulong Zhang	Different urban regions usually have different commercial hotness due to the different social contexts inside. As satellite imagery promises high-resolution, low-cost, real-time, and ubiquitous data acquisition, this study aims to solve commercial hotness prediction as well as the correlated social contexts mining problem via visual pattern analysis on satellite images. The goal is to reveal the underlying law correlating visual patterns of satellite images with commercial hotness so as to infer the commercial hotness map of a whole city for government regulation and business planning. We propose a novel deep learning-based model, which learns semantic information from raw satellite images to enable predicting regional commercial hotness. First, we collect satellite images from Google Map and label such images with POI categories according to the annotations from OpenStreetMap. Then, we train a model of deep convolutional networks that leverage raw images to infer the social attributes of the region of interest. Finally, we use three classical regression methods to predict regional commercial hotness from the corresponding social contexts reflected in satellite images in Shanghai, where the applied deep features are learned from the examples of Beijing to guarantee the generality. The result shows that the proposed model is robust enough to reach 82% precision at average. To the best of our knowledge, it is the first work focused on discovering relations between commercial hotness and satellite images. A web service is developed to demonstrate how business planning can be done in reference to the predicted commercial hotness of a given region.	Perceiving Commerial Activeness Over Satellite Images	NA:NA:NA:NA	2018
Wen Hua Lin:Kuan-Ting Chen:Hung Yueh Chiang:Winston Hsu	Recently, deep neural network models have achieved promising results in image captioning task. Yet, "vanilla'' sentences, only describing shallow appearances (e.g., types, colors), generated by current works are not satisfied netizen style resulting in lacking engagements, contexts, and user intentions. To tackle this problem, we propose Netizen Style Commenting (NSC), to automatically generate characteristic comments to a user-contributed fashion photo. We are devoted to modulating the comments in a vivid "netizen'' style which reflects the culture in a designated social community and hopes to facilitate more engagement with users. In this work, we design a novel framework that consists of three major components: (1) We construct a large-scale clothing dataset named NetiLook, which contains 300K posts (photos) with 5M comments to discover netizen-style comments. (2) We propose three unique measures to estimate the diversity of comments. (3) We bring diversity by marrying topic models with neural networks to make up the insufficiency of conventional image captioning works. Experimenting over Flickr30k and our NetiLook datasets, we demonstrate our proposed approaches benefit fashion photo commenting and improve image captioning tasks both in accuracy and diversity.	Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures	NA:NA:NA:NA	2018
Mu-Yen Chen:Tien-Chi Huang:Yu Shu:Chia-Chen Chen:Tsung-Che Hsieh:Neil Y. Yen	This study retains the meanings of the original text using Autoencoder (AE) in this regard. This study uses the different loss (includes three types) to train the neural network model, hopes that after compressing sentence features, it can still decompress the original input sentences and classify the correct targets, such as positive or negative sentiment. In this way, it supposed to get the more relative features (compressing sentence features) in the sentences to classify the targets, rather than using the classification loss that may classify by the meaningless features (words). In the result, this study discovers that adding additional features for correction of errors does not interfere with the learning. Also, not all words are needed to be restored without distortion after applying the AE method.	Learning the Chinese Sentence Representation with LSTM Autoencoder	NA:NA:NA:NA:NA:NA	2018
Shreya Ghosh:Soumya K. Ghosh:Rahul Deb Das:Stephan Winter	Several studies have shown that the spatio-temporal mobility traces of human movements can be used to identify an individual. However, this work presents a novel framework for activity-based mobility profiling of individuals using only the temporal information. The proposed framework is conducive to model individuals' activity patterns in temporal scale, and quantifies the uniqueness measures based on certain temporal features of the activity sequence.	Activity-based Mobility Profiling: A Purely Temporal Modeling Approach	NA:NA:NA:NA	2018
Muhammad Usman Ilyas:Jalal Suliman Alowibdi	Several prior studies have demonstrated the possibility of tracking the outbreak and spread of diseases using public tweets and other social media platforms. However, almost all such prior studies were restricted to geographically filtered English language tweets only. This study is the first to attempt a similar approach for Arabic language tweets originating from the Gulf Cooperation Council (GCC) countries. We obtained a list of commonly occurring diseases in the region from the Saudi Ministry of Health. We used both the English disease names as well as their Arabic translations to filter the stream of tweets. We acquired old tweets for a period spanning 29 months. All tweets were geographically filtered for the Middle East and the list of disease names in both English and Arabic languages. We observed that only a small fraction of tweets were in English, demonstrating that prior approaches to disease tracking relying on English language features are less effective for this region. We also demonstrate how Arabic language tweets can be used rather effectively to track the spread of some infectious diseases in the region. We verified our approach by demonstrating that a high degree of correlation between the occurrence of MERS-Coronavirus cases and Arabic language tweets on the disease. We also show that infectious diseases generating fewer tweets and non-infectious diseases do not exhibit the same high correlation. We also verify the usefulness of tracking cases using Twitter mentions by comparing against a ground truth data set of MERS-CoV cases obtained from the Saudi Ministry of Health.	Disease Tracking in GCC Region Using Arabic Language Tweets	NA:NA	2018
Manuel Tomas Carrasco Benitez:Pascal Hitzler:Irwin King:Moussa Lo:Erik Mannens:Daniel Schwabe	NA	Session details: International Project	NA:NA:NA:NA:NA:NA	2018
Martin Serrano:Amelie Gyrard:Elias Tragos:Hung Nguyen	FIESTA-IoT project provides a blueprint experimental infrastructure, software tools, semantic techniques, certification processes and best practices enabling IoT testbed/platforms to interconnect their facility's resources in an interoperable semantic way. FIESTA-IoT project enables the integration of IoT platform's resources, testbeds infrastructure and their associated applications. FIESTA-IoT opens up new opportunities in the development and deployment of experiments using data from IoT testbeds. The FIESTA-IoT infrastructure enables experimenters to use a single EaaS API (i.e. the FIESTA-IoT EaaS API) for executing experiments over multiple IoT federated testbeds in a testbed agnostic way i.e. like accessing a single large scale virtualized testbed. The main goal of the FIESTA-IoT project is to open new horizons in the development and deployment of IoT applications and experiments at a EU (and global) scale, based on the interconnection and interoperability of diverse IoT platforms and testbeds. FIESTA-IoT project's experimental infrastructure provides to the European experimenters in the IoT domain with the unique capability for accessing and sharing IoT semantically annotated datasets in a testbed-agnostic way. FIESTA-IoT enables execution of experiments across multiple IoT testbeds, based on a single API for submitting the experiment and a single set of credentials for the researcher and the portability of IoT experiments across different testbeds and the provision of interoperable standards-based IoT/cloud interfaces over diverse IoT experimental facilities.	FIESTAIoT Project: Federated Interoperable Semantic IoT/cloud Testbeds and Applications	NA:NA:NA:NA	2018
Mark A. Musen:Susanna-Assunta Sansone:Kei-Hoi Cheung:Steven H. Kleinstein:Morgan Crafts:Stephan C. Schürer:John Graybeal	There is an expectation that scientists will archive their experimental data online in public repositories to enable other investigators to verify their work and to re-explore their data in search of new discoveries. When left to their own devices, however, scientists do a poor job creating the metadata that describe their datasets. A lack of standardization makes it difficult for other investigators to find relevant datasets and to perform secondary analyses. The Center for Expanded Data Annotation and Retrieval (CEDAR) was founded with the goal of enhancing the authoring of experimental metadata to make online datasets more useful to the scientific community. CEDAR technology includes Web-based methods for creating and managing libraries of templates for representing metadata. CEDAR's templates interoperate with a repository of scientific ontologies to standardize the way in which the templates may be filled out. Collaborations with several major research projects are allowing us to explore how CEDAR may ease access to scientific data sets stored in public repositories.	CEDAR: Semantic Web Technology to Support Open Science	NA:NA:NA:NA:NA:NA:NA	2018
Philip E. Schreur	Linked Data for Production (LD4P) is a collaboration between six institutions (Columbia, Cornell, Harvard, Library of Congress, Princeton, and Stanford) to begin the transition of technical services production workflows from a series of library-centric data formats (MARC) to ones based in Linked Open Data (LOD). This first phase of the transition focuses on the development of the ability to produce metadata as LOD communally, the enhancement of the BIBFRAME ontology to encompass the multiple resource formats that academic libraries must process, and the engagement of the broader academic library community to ensure a sustainable and extensible environment. As its name implies, LD4P focuses on the immediate needs of metadata production such as ontology coverage and workflow transition. The LD4P partners' work will be based, in part, on a collection of tools that currently exist, such as those developed by the Library of Congress. The cyclical feedback of use and enhancement request to the developers of these tools will allow for their enhancement based on use in an actual production environment. The six institutions involved will focus on materials ranging from art to rare books, from cartographic materials to music, from annotations to workflows. Tool development and enhancement will also be a key aspect of the project. By the end of the first phase of this project (Spring 2018), the partners will have the minimal tooling, workflows, and standards developed to begin the transformation from MARC to LOD in Phase 2 of the project.	Linked Data for Production (LD4P): a Multi-Institutional Approach to Technical Services Transformation	NA	2018
Hsin-Hsi Chen:Manabu Okumura	Memory loss, common seen in elderly people, affects their social interaction in the daily life very much. This 3-year international project jointly funded by Taiwan Ministry of Science and Technology (MOST) and Japan Science and Technology Agency (JST) investigates together the crucial issues behind the hyper aged societies. We aim at developing technologies and systems to provide information recall support for elderly people at the right time and at the right place.	Information Recall Support for Elderly People in Hyper Aged Societies	NA:NA	2018
Pedro Szekely:Mayank Kejriwal	The DARPA Memex program was established with the goal of funding research into building domain-specific search systems that integrated state-of-the-art focused crawling (domain discovery) information extraction and semantic search, and that could be used by users and domain experts with no programming or technical experience. Domain-specific Insight Graphs (DIG) was proposed and funded under Memex and has led to an end-to-end search system currently being used by over 200 law enforcement for combating human trafficking, by investigators from the Securities and Exchange Commission (SEC) in the US for investigating securities fraud, and for numerous other domains of a difficult, socially consequential (e.g., investigative) and unusual nature.	Domain-specific Insight Graphs (DIG)	NA:NA	2018
Manolis Koubarakis:Herve Caumont:Ulrike Daniels:Erwin Goor:Lara König:Valentijn Venus	Copernicus App Lab is a two year project (November 2016 to October 2018) funded by the European Commission under the H2020 program. The consortium consists of AZO (project coordinator), National and Kapodistrian University of Athens, Terradue, RAMANI and VITO. The main objective of Copernicus App Lab is to make Earth observation data produced by the Copernicus program of the European Union available on the Web as linked data to aid their use by mobile developers.	Copernicus App Lab: A Platform for Easy Data Access Connecting the Scientific Earth Observation Community with Mobile Developers	NA:NA:NA:NA:NA:NA	2018
Fosca Giannotti:Roberto Trasarti:Kalina Bontcheva:Valerio Grossi	One of the most pressing and fascinating challenges scientists face today, is understanding the complexity of our globally interconnected society. The big data arising from the digital breadcrumbs of human activities has the potential of providing a powerful social microscope, which can help us understand many complex and hidden socio-economic phenomena. Such challenge requires high-level analytics, modeling and reasoning across all the social dimensions above. There is a need to harness these opportunities for scientific advancement and for the social good, compared to the currently prevalent exploitation of big data for commercial purposes or, worse, social control and surveillance. The main obstacle to this accomplishment, besides the scarcity of data scientists, is the lack of a large-scale open ecosystem where big data and social mining research can be carried out. The SoBigData Research Infrastructure (RI) provides an integrated ecosystem for ethic-sensitive scientific discoveries and advanced applications of social data mining on the various dimensions of social life as recorded by "big data". The research community uses the SoBigData facilities as a "secure digital wind-tunnel" for large-scale social data analysis and simulation experiments. SoBigData promotes repeatable and open science and supports data science research projects by providing: (i) an ever-growing, distributed data ecosystem for procurement, access and curation and management of big social data, to underpin social data mining research within an ethic-sensitive context; (ii) an ever-growing, distributed platform of interoperable, social data mining methods and associated skills: tools, methodologies and services for mining, analysing, and visualising complex and massive datasets, harnessing the techno-legal barriers to the ethically safe deployment of big data for social mining; (iii) an ecosystem where protection of personal information and the respect for fundamental human rights can coexist with a safe use of the same information for scientific purposes of broad and central societal interest. SoBigData has a dedicated ethical and legal board, which is implementing a legal and ethical framework.	SoBigData: Social Mining & Big Data Ecosystem	NA:NA:NA:NA	2018
Mathieu d'Aquin:Dominik Kowald:Angela Fessl:Elisabeth Lex:Stefan Thalmann	The goal of AFEL is to develop, pilot and evaluate methods and applications, which advance informal/collective learning as it surfaces implicitly in online social environments. The project is following a multi-disciplinary, industry-driven approach to the analysis and understanding of learner data in order to personalize, accelerate and improve informal learning processes. Learning Analytics and Educational Data Mining traditionally relate to the analysis and exploration of data coming from learning environments, especially to understand learners' behaviours. However, studies have for a long time demonstrated that learning activities happen outside of formal educational platforms, also. This includes informal and collective learning usually associated, as a side effect, with other (social) environments and activities. Relying on real data from a commercially available platform, the aim of AFEL is to provide and validate the technological grounding and tools for exploiting learning analytics on such learning activities. This will be achieved in relation to cognitive models of learning and collaboration, which are necessary to the understanding of loosely defined learning processes in online social environments. Applying the skills available in the consortium to a concrete set of live, industrial online social environments, AFEL will tackle the main challenges of informal learning analytics through 1) developing the tools and techniques necessary to capture information about learning activities from (not necessarily educational) online social environments; 2) creating methods for the analysis of such informal learning data, based on combining feature engineering and visual analytics with cognitive models of learning and collaboration; and 3) demonstrating the potential of the approach in improving the understanding of informal learning, and the way it is better supported; 4) evaluate all the former items in real world large scale applications and platforms.	AFEL - Analytics for Everyday Learning	NA:NA:NA:NA:NA	2018
Valerio Basile:Roberto Navigli	The exponential growth of the Web is resulting in vast amounts of online content. However, the information expressed therein is not at easy reach: what we typically browse is only an infinitesimal part of the Web. And even if we had time to read all the Web we could not understand it, as most of it is written in languages we do not speak. Rather than time, a key problem for a machine is language comprehension, that is, enabling a machine to transform sentences, i.e., sequences of characters, into machine-readable semantic representations linked to existing meaning inventories such as computational lexicons and knowledge bases.	From MultiJEDI to MOUSSE: Two ERC Projects for Innovating Multilingual Disambiguation and Semantic Parsing of Text	NA:NA	2018
Claudia d'Amato:Francesco Marcelloni:Rudi Studer	It is our great pleasure to welcome you to the first edition of WWW 2018 Journal Track. The track is new track within WWW conference series and it is intended as a forum for presentations of significant Web-related research results that have been published recently in well-known and established journals, and have never been presented at any Web-related conference. The goal is to give visibility of these results to a conference audience as well as to promote discussions concerning such results. The call for papers of the journal track was open for two categories of papers: 1) self-nominations from authors promoting their own journal publication(s) and matching the prerequisites reported above; 2) invited papers, selected on the basis of interest, appropriateness and attractiveness for the WWW 2018 audience, from articles published since January 1st 2015 (even only in the electronic version) in the following journals: Journal of Network and Computer Applications, Journal of Web Semantics, Semantic Web Journal, IEEE Transactions on Fuzzy Systems, IEEE Transactions on Neural Networks and Learning Systems, Journal of Machine Learning Research, Data Mining and Knowledge Discovery, ACM Transactions on the Web, ACM Computing Surveys, Knowledge-based systems, Artificial Intelligence. Exceptions have been also considered for papers judged as potentially very influential but published before January 2015 or in a journal not included in the list. We received 61 submissions and accepted 12 of them for presentation to the WWW 2018 Journal Track. The papers have been evaluated according to the following criteria: novelty, relevance to the conference, quality of the extended abstract, metrics values (e.g. number of citations) computed for the original journal paper, representativeness and impact factor of the journals in which the papers have been published (secondarily). We also took in account the coverage of the different areas related to WWW.	Journal Track Chairs' Welcome & Organization	NA:NA:NA	2018
Ulle Endriss:Umberto Grandi	Graph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs, each provided by a different source. One needs to perform graph aggregation in a wide variety of situations, e.g., when applying a voting rule (graphs as preference orders), when consolidating conflicting views regarding the relationships between arguments in a debate (graphs as abstract argumentation frameworks), or when computing a consensus between several alternative clusterings of a given dataset (graphs as equivalence relations). Other potential applications include belief merging, data integration, and social network analysis. In this short paper, we review a recently introduced formal framework for graph aggregation that is grounded in social choice theory. Our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule. Our main result is a powerful impossibility theorem that generalises Arrow's seminal result regarding the aggregation of preference orders to a large collection of different types of graphs. We also provide a discussion of existing and potential applications of graph aggregation.	Graph Aggregation	NA:NA	2018
Stefano Calzavara:Riccardo Focardi:Marco Squarcina:Mauro Tempesta	We survey the most common attacks against web sessions, i.e., attacks which target honest web browser users establishing an authenticated session with a trusted web application. We then review existing security solutions which prevent or mitigate the different attacks, by evaluating them along four different axes: protection, usability, compatibility and ease of deployment. Based on this survey, we identify five guidelines that, to different extents, have been taken into account by the designers of the different proposals we reviewed. We believe that these guidelines can be helpful for the development of innovative solutions approaching web security in a more systematic and comprehensive way.	Surviving the Web: A Journey into Web Session Security	NA:NA:NA:NA	2018
Cataldo Musto:Pasquale Lops:Marco de Gemmis:Giovanni Semeraro	In this contribution we propose a hybrid recommendation framework based on classification algorithms such as Random Forests and Naive Bayes, which are fed with several heterogeneous groups of features. We split our features into two classes: classic features, as popularity-based, collaborative and content-based ones, and extended features gathered from the Linked Open Data (LOD) cloud, as basic ones (i.e. genre of a movie or the writer of a book) and graph-based features calculated on the ground of the different topological characteristics of the tripartite representation connecting users, items and properties in the LOD cloud. In the experimental session we evaluate the effectiveness of our framework on varying of different groups of features, and results show that both LOD-based and graph-based features positively affect the overall performance of the algorithm, especially in highly sparse recommendation scenarios. Our approach also outperforms several state-of-the-art recommendation techniques, thus confirming the insights behind this research. This extended abstract summarizes the content of the journal paper published on Knowledge-based Systems.	Semantics-aware Recommender Systems Exploiting Linked Open Data and Graph-based Features	NA:NA:NA:NA	2018
Rathachai Chawuthai:Hideaki Takeda:Vilas Wuwongse:Utsugi Jinbo	Linked Open Data (LOD) technology enables web of data and exchangeable knowledge graphs through the Internet. However, the change in knowledge is happened everywhere and every time, and it becomes a challenging issue of linking data precisely because the misinterpretation and misunderstanding of some terms and concepts may be dissimilar under different context of time and different community knowledge. To solve this issue, we introduce an approach to the preservation of knowledge graph, and we select the biodiversity domain to be our case studies because knowledge of this domain is commonly changed and all changes are clearly documented. Our work produces an ontology, transformation rules, and an application to demonstrate that it is feasible to present and preserve knowledge graphs and provides open and accurate access to linked data. It covers changes in names and their relationships from different time and communities as can be seen in the cases of taxonomic knowledge.	Presenting and Preserving the Change in Taxonomic Knowledge for Linked Data (Extended Abstract)	NA:NA:NA:NA	2018
Lorenz Bühmann:Jens Lehmann:Patrick Westphal:Simon Bin	The following paper is an extended summary of the journal paper "DL-Learner A framework for inductive learning on the Semantic Web". In this system paper, we describe the DL-Learner framework. It is beneficial in various data and schema analytic tasks with applications in different standard machine learning scenarios, e.g. life sciences, as well as Semantic Web specific applications such as ontology learning and enrichment. Since its creation in 2007, it has become the main OWL and RDF-based software framework for supervised structured machine learning and includes several algorithm implementations, usage examples and has applications building on top of the framework.	DL-Learner Structured Machine Learning on Semantic Web Data	NA:NA:NA:NA	2018
Mehrdad Farajtabar:Manuel Gomez-Rodriguez:Yichen Wang:Shuang Li:Hongyuan Zha:Le Song	Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when they are exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes---information diffusion and network evolution---have been typically studied separately, ignoring their co-evolutionary dynamics. In this work, we propose a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. The model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Moreover, we develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. Experiments in both synthetic data and real data gathered from Twitter show that our model provides a good fit to the data as well as more accurate predictions than alternatives.	COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution	NA:NA:NA:NA:NA:NA	2018
Valeria Fionda:Giuseppe Pirro:Claudio Gutierrez	We research the problem of building knowledge maps of graph-like information. We live in the digital era and similarly to the Earth, the Web is simply too large and its interrelations too complex for anyone to grasp much of it through direct observation. Thus, the problem of applying cartographic principles also to digital landscapes is intriguing. We introduce a mathematical formalism that captures the general notion of map of a graph and enables its development and manipulation in a semi-automated way. We describe an implementation of our formalism on the Web of Linked Data graph and discuss algorithms that efficiently generate and combine (via an algebra) regions and maps. Finally, we discuss examples of knowledge maps built with a tool implementing our framework.	Building Knowledge Maps of Web Graphs	NA:NA:NA	2018
Deepayan Chakrabarti:Stanislav Funiak:Jonathan Chang:Sofus A. Macskassy	We consider the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Existing approaches such as Label Propagation fail to consider interactions between the label types. Our proposed method, called EdgeExplain, explicitly models these interactions, while still allowing scalable inference under a distributed message-passing architecture. On a large subset of the Facebook social network, collected in a previous study, EdgeExplain outperforms label propagation for several label types, with lifts of up to $120%$ for [email protected] and 60% for [email protected]	Joint Label Inference in Networks	NA:NA:NA:NA	2018
Elaheh Momeni:Claire Cardie:Nicholas Diakopoulos	User-generated content (UGC) on the Web, especially on social media platforms, facilitates the association of additional information with digital resources and online social topics and it can provide valuable supplementary content. However, UGC varies in quality and, consequently, raises the challenge of how to maximize its utility for a variety of end-users, in particular in the age of misinformation. This study aims to provide researchers and Web data curators with answers to the following questions: (1) What are the existing approaches and methods for assessing and ranking UGC (2) What features and metrics have been used successfully to assess and predict UGC value across a range of application domains This survey is composed of a systematic review of approaches for assessing and ranking UGC: results obtained by identifying and comparing methodologies within the context of short text-based UGC on the Web. This survey categorizes existing assessment and ranking approaches into four framework types and discusses the main contributions and considerations of each type. Furthermore, it suggests a need for further experimentation and encourages the development of new approaches for the assessment and ranking.	How to Assess and Rank User-Generated Content on Web	NA:NA:NA	2018
Jianguo Lu:Hao Wang:Dingding Li	We show that uniform random sampling is not as effective as PPS (probability proportional to size) sampling in many estimation tasks. In the setting of (graph) size estimation, this paper demonstrates that random edge sampling outperforms random node sampling, with a performance ratio proportional to the normalized graph degree variance. This result is particularly important in the era of big data, when data are typically large and scale-free, resulting in large degree variance. We derive the result by first giving the variances of random node and random edge estimators. A simpler and more intuitive result is obtained by assuming that the data is large and degree distribution follows a power law.	Uniform Random Sampling Not Recommended	NA:NA:NA	2018
Maribel Acosta:Elena Simperl:Fabian Flöck:Maria-Esther Vidal	We propose HARE, a SPARQL query engine that encompasses human-machine query processing to augment the completeness of query answers. We empirically assessed the effectiveness of HARE on 50 SPARQL queries over DBpedia. Experimental results clearly show that our solution accurately enhances answer completeness.	HARE: An Engine for Enhancing Answer Completeness of SPARQL Queries via Crowdsourcing	NA:NA:NA:NA	2018
Muhammad Imran:Carlos Castillo:Fernando Diaz:Sarah Vieweg	Millions of people use social media to share information during disasters and mass emergencies. Information available on social media, particularly in the early hours of an event when few other sources are available, can be extremely valuable for emergency responders and decision makers, helping them gain situational awareness and plan relief efforts. Processing social media content to obtain such information involves solving multiple challenges, including parsing brief and informal messages, handling information overload, and prioritizing different types of information. These challenges can be mapped to information processing operations such as filtering, classifying, ranking, aggregating, extracting, and summarizing. This work highlights these challenges and presents state of the art computational techniques to deal with social media messages, focusing on their application to crisis scenarios.	Processing Social Media Messages in Mass Emergency: Survey Summary	NA:NA:NA:NA	2018
Giovanni Luca Ciampaglia:Kristina Lerman:Panagiotis Metaxas	It is our pleasure to welcome you to the WWW 2018 Journalism, Misinformation and Fact Checking Alternate Track. Although the problem of misinformation and deceptive information is as old as Web itself, the topic has gained a lot of attention recently. Phenomena, such as misinformation propagation, fabricated news reports (also known as "fake news",) computational propaganda, astroturf, and ideological polarization have become more common on the Web and the social Web, calling for a cross-cutting approach to better understand the topic. One approach that has gained some traction is that of the establishment of fact-checking organizations. This track solicited contributions that explore the range of computational, social, cognitive, economic, and communication topics related to the above phenomena. We received submissions covering a broad range of topics, including computational approaches for detecting misinformation and propaganda on the Web and social media, as well as proposals to improve fact checking, critical thinking, information and media literacy, crowdsourcing, and societal decision-making processes.	Journalism, Misinformation and Fact Checking Chairs' Welcome & Organization	NA:NA:NA	2018
Sebastian Tschiatschek:Adish Singla:Manuel Gomez Rodriguez:Arpit Merchant:Andreas Krause	Our work considers leveraging crowd signals for detecting fake news and is motivated by tools recently introduced by Facebook that enable users to flag fake news. By aggregating users' flags, our goal is to select a small subset of news every day, send them to an expert (e.g., via a third-party fact-checking organization), and stop the spread of news identified as fake by an expert. The main objective of our work is to minimize the spread of misinformation by stopping the propagation of fake news in the network. It is especially challenging to achieve this objective as it requires detecting fake news with high-confidence as quickly as possible. We show that in order to leverage users' flags efficiently, it is crucial to learn about users' flagging accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian inference for detecting fake news and jointly learns about users' flagging accuracy over time. Our algorithm employs posterior sampling to actively trade off exploitation (selecting news that maximize the objective value at a given epoch) and exploration (selecting news that maximize the value of information towards learning about users' flagging accuracy). We demonstrate the effectiveness of our approach via extensive experiments and show the power of leveraging community signals for fake news detection.	Fake News Detection in Social Networks via Crowd Signals	NA:NA:NA:NA:NA	2018
Xuezhi Wang:Cong Yu:Simon Baumgartner:Flip Korn	With the support of major search platforms such as Google and Bing, fact-checking articles, which can be identified by their adoption of the schema.org ClaimReview structured markup, have gained widespread recognition for their role in the fight against digital misinformation. A claim-relevant document is an online document that addresses, and potentially expresses a stance towards, some claim. The claim-relevance discovery problem, then, is to find claim-relevant documents. Depending on the verdict from the fact check, claim-relevance discovery can help identify online misinformation. In this paper, we provide an initial approach to the claim-relevance discovery problem by leveraging various information retrieval and machine learning techniques. The system consists of three phases. First, we retrieve candidate documents based on various features in the fact-checking article. Second, we apply a relevance classifier to filter away documents that do not address the claim. Third, we apply a language feature based classifier to distinguish documents with different stances towards the claim. We experimentally demonstrate that our solution achieves solid results on a large-scale dataset and beats state-of-the-art baselines. Finally, we highlight a rich set of case studies to demonstrate the myriad of remaining challenges and that this problem is far from being solved.	Relevant Document Discovery for Fact-Checking Articles	NA:NA:NA:NA	2018
Dylan Bourgeois:Jérémie Rappaz:Karl Aberer	News entities must select and filter the coverage they broadcast through their respective channels since the set of world events is too large to be treated exhaustively. The subjective nature of this filtering induces biases due to, among other things, resource constraints, editorial guidelines, ideological affinities, or even the fragmented nature of the information at a journalist's disposal. The magnitude and direction of these biases are, however, widely unknown. The absence of ground truth, the sheer size of the event space, or the lack of an exhaustive set of absolute features to measure make it difficult to observe the bias directly, to characterize the leaning's nature and to factor it out to ensure a neutral coverage of the news. In this work, we introduce a methodology to capture the latent structure of media's decision process on a large scale. Our contribution is multi-fold. First, we show media coverage to be predictable using personalization techniques, and evaluate our approach on a large set of events collected from the GDELT database. We then show that a personalized and parametrized approach not only exhibits higher accuracy in coverage prediction, but also provides an interpretable representation of the selection bias. Last, we propose a method able to select a set of sources by leveraging the latent representation. These selected sources provide a more diverse and egalitarian coverage, all while retaining the most actively covered events.	Selection Bias in News Coverage: Learning it, Fighting it	NA:NA:NA	2018
Shweta Bhatt:Sagar Joglekar:Shehar Bano:Nishanth Sastry	This paper aims to shed light on alternative news media ecosystems that are believed to have influenced opinions and beliefs by false and/or biased news reporting during the 2016 US Presidential Elections. We examine a large, professionally curated list of 668 hyper-partisan websites and their corresponding Facebook pages, and identify key characteristics that mediate the traffic flow within this ecosystem. We uncover a pattern of new websites being established in the run up to the elections, and abandoned after. Such websites form an ecosystem, creating links from one website to another, and by 'liking' each others' Facebook pages. These practices are highly effective in directing user traffic internally within the ecosystem in a highly partisan manner, with right-leaning sites linking to and liking other right-leaning sites and similarly left-leaning sites linking to other sites on the left, thus forming a filter bubble amongst news producers similar to the filter bubble which has been widely observed among consumers of partisan news. Whereas there is activity along both left- and right-leaning sites, right-leaning sites are more evolved, accounting for a disproportionate number of abandoned websites and partisan internal links. We also examine demographic characteristics of consumers of hyper partisan news and find that some of the more populous demographic groups in the US tend to be consumers of more right-leaning sites.	Illuminating an Ecosystem of Partisan Websites	NA:NA:NA:NA	2018
Andreas Spitz:Michael Gertz	The increasing number of news outlets and the frequency of the news cycle have made it all but impossible to obtain the full picture from online news. Consolidating news from different sources has thus become a necessity in online news processing. Despite the amount of research that has been devoted to different aspects of new event detection and tracking in news streams, solid solutions for such entangled streams of full news articles are still lacking. Many existing works focus on streams of microblogs since the analysis of news articles raises the additional problem of summarizing or extracting the relevant sections of articles. For the consolidation of identified news snippets, schemes along numerous different dimensions have been proposed, including publication time, temporal expressions, geo-spatial references, named entities, and topics. The granularity of aggregated news snippets then includes such diverse aspects as events, incidents, threads, or topics for various subdivisions of news articles. To support this variety of granularity levels, we propose a comprehensive network model for the representation of multiple entangled streams of news documents. Unlike previous methods, the model is geared towards entity-centric explorations and enables the consolidation of news along all dimensions, including the context of entity mentions. Since the model also serves as a reverse index, it supports explorations along the dimensions of sentences or documents for an encompassing view on news events. We evaluate the performance of our model on a large collection of entangled news streams from major news outlets of English speaking countries and a ground truth that we generate from event summaries in the Wikipedia Current Events portal.	Exploring Entity-centric Networks in Entangled News Streams	NA:NA	2018
Sylvie Cazalens:Philippe Lamarre:Julien Leblay:Ioana Manolescu:Xavier Tannier	Fact checking has captured the attention of the media and the public alike; it has also recently received strong attention from the computer science community, in particular from data and knowledge management, natural language processing and information retrieval; we denote these together under the term "content management". In this paper, we identify the fact checking tasks which can be performed with the help of content management technologies, and survey the recent research works in this area, before laying out some perspectives for the future. We hope our work will provide interested researchers, journalists and fact checkers with an entry point in the existing literature as well as help develop a roadmap for future research and development work.	A Content Management Perspective on Fact-Checking	NA:NA:NA:NA:NA	2018
Svitlana Volkova:Jin Yea Jang	Deceptive information in online news and social media has had dramatic effect on our society in recent years. This study is the first to gain deeper insights into writers' intent behind digital misinformation by analyzing psycholinguistic signals: moral foundations and connotations extracted from different types of deceptive news ranging from strategic disinformation to propaganda and hoaxes. To ensure consistency of our findings and generalizability across domains, we experiment with data from: (1) confirmed cases of disinformation in news summaries, (2) propaganda, hoax, and disinformation news pages, and (3) social media news. We first contrast lexical markers of biased language, syntactic and stylistic signals, and connotations across deceptive news types including disinformation, propaganda, and hoaxes, and deceptive strategies including misleading or falsification. We then incorporate these insights to build machine learning and deep learning predictive models to infer deception strategies and deceptive news types. Our experimental results demonstrate that unlike earlier work on deception detection, content combined with biased language markers, moral foundations, and connotations leads to better predictive performance of deception strategies compared to syntactic and stylistic signals (as reported in earlier work on deceptive reviews). Falsification strategy is easier to identify than misleading strategy. Disinformation is more difficult to predict than to propaganda or hoaxes. Deceptive news types (disinformation, propaganda, and hoaxes), unlike deceptive strategies (falsification and misleading), are more salient, and thus easier to identify in tweets than in news reports. Finally, our novel connotation analysis across deception types provides deeper understanding of writers' perspectives and therefore reveals the intentions behind digital misinformation.	Misleading or Falsification: Inferring Deceptive Strategies and Types in Online News and Social Media	NA:NA	2018
Jing Ma:Wei Gao:Kam-Fai Wong	In recent years, an unhealthy phenomenon characterized as the massive spread of fake news or unverified information (i.e., rumors) has become increasingly a daunting issue in human society. The rumors commonly originate from social media outlets, primarily microblogging platforms, being viral afterwards by the wild, willful propagation via a large number of participants. It is observed that rumorous posts often trigger versatile, mostly controversial stances among participating users. Thus, determining the stances on the posts in question can be pertinent to the successful detection of rumors, and vice versa. Existing studies, however, mainly regard rumor detection and stance classification as separate tasks. In this paper, we argue that they should be treated as a joint, collaborative effort, considering the strong connections between the veracity of claim and the stances expressed in responsive posts. Enlightened by the multi-task learning scheme, we propose a joint framework that unifies the two highly pertinent tasks, i.e., rumor detection and stance classification. Based on deep neural networks, we train both tasks jointly using weight sharing to extract the common and task-invariant features while each task can still learn its task-specific features. Extensive experiments on real-world datasets gathered from Twitter and news portals demonstrate that our proposed framework improves both rumor detection and stance classification tasks consistently with the help of the strong inter-task connections, achieving much better performance than state-of-the-art methods.	Detect Rumor and Stance Jointly by Neural Multi-task Learning	NA:NA:NA	2018
Miriam Fernandez:Harith Alani	Misinformation has become a common part of our digital media environments and it is compromising the ability of our societies to form informed opinions. It generates misperceptions, which have affected the decision making processes in many domains, including economy, health, environment, and elections, among others. Misinformation and its generation, propagation, impact, and management is being studied through a variety of lenses (computer science, social science, journalism, psychology, etc.) since it widely affects multiple aspects of society. In this paper we analyse the phenomenon of misinformation from a technological point of view. We study the current socio-technical advancements towards addressing the problem, identify some of the key limitations of current technologies, and propose some ideas to target such limitations. The goal of this position paper is to reflect on the current state of the art and to stimulate discussions on the future design and development of algorithms, methodologies, and applications.	Online Misinformation: Challenges and Future Directions	NA:NA	2018
Amy X. Zhang:Aditya Ranganathan:Sarah Emlen Metz:Scott Appling:Connie Moon Sehat:Norman Gilmore:Nick B. Adams:Emmanuel Vincent:Jennifer Lee:Martin Robbins:Ed Bice:Sandro Hawke:David Karger:An Xiao Mina	The proliferation of misinformation in online news and its amplification by platforms are a growing concern, leading to numerous efforts to improve the detection of and response to misinformation. Given the variety of approaches, collective agreement on the indicators that signify credible content could allow for greater collaboration and data-sharing across initiatives. In this paper, we present an initial set of indicators for article credibility defined by a diverse coalition of experts. These indicators originate from both within an article's text as well as from external sources or article metadata. As a proof-of-concept, we present a dataset of 40 articles of varying credibility annotated with our indicators by 6 trained annotators using specialized platforms. We discuss future steps including expanding annotation, broadening the set of indicators, and considering their use by platforms and the public, towards the development of interoperable standards for content credibility.	A Structured Response to Misinformation: Defining and Annotating Credibility Indicators in News Articles	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Michele Bedard:Chianna Schoenthaler	Ever since the surprising results from the 2016 U.S. presidential race, the subject of Fake News in our worldwide media consumption has grown steadily. On a smaller scale, mainstream media have taken a closer look at the relatively narrow genre of satirical news content. Ed Koltonski of Kent State, defines satirical news as designed specifically to entertain the reader, usually with irony or wit, to critique society or a social figure and invoke change or reform. Using field experiment, survey and focus group methods we sought to determine if media consumers' ability to differentiate between satirical news and fake news is tied to socio-demographic factors. We found that age, education, sex, and political affiliation predict understanding of "fake news" and satire. Furthermore, the ability to identify different types of misinformation when presented with screen shots from social media posts appears to be related to these variables. Focus group comments were also analyzed to gain a richer perspective on how participants interpreted the SMS screen shots. Using our primary research, we seek to determine if there is a correlation between social media consumers understanding of the difference between satirical news versus fake news and their varying socio-demographic factors	Satire or Fake News: Social Media Consumers' Socio-Demographics Decide	NA:NA	2018
Fred Morstatter:Yunqiu Shao:Aram Galstyan:Shanika Karunasekera	In the 2017 German Federal elections, the "Alternative for Deutschland'', or AfD, party was able to take control of many seats in German parliament. Their success was credited, in part, to their large online presence. Like other "alt-right'' organizations worldwide, this party is tech savvy, generating a large social media footprint, especially on Twitter, which provides an ample opportunity to understand their online behavior. In this work we present an analysis of Twitter data related to the aforementioned election. We show how users self-organize into communities, and identify the themes that define those communities. Next we analyze the content generated by those communities, and the extent to which these communities interact. Despite these elections being held in Germany, we note a substantial impact from the English-speaking Twittersphere. Specifically, we note that many of these accounts appear to be from the American alt-right movement, and support the German alt-right movement.	From Alt-Right to Alt-Rechts: Twitter Analysis of the 2017 German Federal Election	NA:NA:NA:NA	2018
Evgeniy Gabrilovich:Kira Radinsky:Kuansan Wang	It is our great pleasure to welcome you to the BIG Web Track of the Web Conference 2018. Many of today's most successful enterprises in business and in science are built on the collection and analysis of data. The sheer volume and richness of these data sets has stimulated a massive wave of innovation. In addition, this revolution has also sparked important debate on data privacy policies, ethics, and governance. This track started as a co-located event called BigData Innovators Gathering (BIG) with a vision to bring together academic and industry leaders in the Big Data space to share the state of the art and its successful applications in business. This event will be co-located with the Web conference for the fifth time, but now as a fully fledged alternate track named The BIG Web in The Web Conference 2018 in Lyon. This year's track consists of two keynotes, a panel on machine learning in the field of medicine, and 11 invited talks. In addition, we have accepted 6 papers from 35 submissions (with an acceptance ratio of 17%).	The BIG Web Track Chairs' Welcome & Organization	NA:NA:NA	2018
Kunwoo Park:Meeyoung Cha:Eunhee Rhim	Customer ratings are valuable sources to understand their satisfaction and are critical for designing better customer experiences and recommendations. The majority of customers, however, do not respond to rating surveys, which makes the result less representative. To understand overall satisfaction, this paper aims to investigate how likely customers without responses had satisfactory experiences compared to those respondents. To infer customer satisfaction of such unlabeled sessions, we propose models using recurrent neural networks (RNNs) that learn continuous representations of unstructured text conversation. By analyzing online chat logs of over 170,000 sessions from Samsung's customer service department, we make a novel finding that while labeled sessions contributed by a small fraction of customers received overwhelmingly positive reviews, the majority of unlabeled sessions would have received lower ratings by customers. The data analytics presented in this paper not only have practical implications for helping detect dissatisfied customers on live chat services but also make theoretical contributions on discovering the level of biases in online rating platforms.	Positivity Bias in Customer Satisfaction Ratings	NA:NA:NA	2018
Ya-Lin Zhang:Longfei Li:Jun Zhou:Xiaolong Li:Zhi-Hua Zhou	In this paper, we consider the problem of anomaly detection. Previous studies mostly deal with this task in either supervised or unsupervised manner according to whether label information is available. However, there always exists settings which are different from the two standard manners. In this paper, we address the scenario when anomalies are partially observed, i.e., we are given a large amount of unlabeled instances as well as a handful labeled anomalies. We refer to this problem as anomaly detection with partially observed anomalies, and proposed a two-stage method ADOA to solve it. Firstly, by addressing the difference between the anomalies, the observed anomalies are clustered, while the unlabeled instances are filtered to get potential anomalies and reliable normal instances. Then, with the above instances, a weight is attached to each instance according to the confidence of its label, and a weighted multi-class model is built, which will be further used to distinguish different anomalies to the normal instances. Experimental results show that in the aforementioned setting, existing methods behave unsatisfactorily and the proposed method performs significantly better than all these methods, which validates the effectiveness of the proposed approach.	Anomaly Detection with Partially Observed Anomalies	NA:NA:NA:NA:NA	2018
Wenshan Wang:Su Yang:Zhiyuan He:Minjie Wang:Jiulong Zhang:Weishan Zhang	People can percept social attributes from streetscapes such as safety, richness, and happiness by means of visual perception, which inspires the research in terms of urban perception. To the best of our knowledge, this is the first work focused on revealing the relationship between visual patterns of satellite images as well as streetscapes and commercial activeness. We propose to make use of bag of features (BoF) in the context of computer vision and sparse representation in the sense of machine learning to predict commercial activeness of urban commercial districts. After obtaining the urban commercial districts via clustering, we predict the commercial activeness degrees of them using four image features, namely, Histogram of Oriented Gradients (HOG), Autoencoder, GIST, and multifractal spectra for satellite images and street view images, respectively. The performance evaluation with four large-scale datasets demonstrates that the presented computational framework can not only predict the commercial activeness with satisfactory precision compared with that based on Point of Interest (POI) data but also discover the visual patterns related.	Urban Perception of Commercial Activeness from Satellite Images and Streetscapes	NA:NA:NA:NA:NA:NA	2018
Dotan Di Castro:Iftah Gamzu:Irena Grabovitch-Zuyev:Liane Lewin-Eytan:Abhinav Pundir:Nil Ratan Sahoo:Michael Viderman	Mail extraction is a critical task whose objective is to extract valuable data from the content of mail messages. This task is key for many types of applications including re-targeting, mail search, and mail summarization, which utilize the important personal data pieces in mail messages to achieve their objectives. We focus on machine generated traffic, which comprises most of the Web mail traffic today, and use its structured and large-scale repetitive nature to devise a fully automated extraction method. Our solution builds on an advanced structural clustering technique previously presented by some of the authors of this work. The heart of our solution is an offline process that leverages the structural mail-specific characteristics of the clustering, and automatically creates extraction rules that are later applied online for each new arriving message. We provide of a full description of our process, which has been productized in Yahoo mail backend. We complete our work with large-scale experiments carried over real Yahoo mail traffic, and evaluate the performance of our automatic extraction method.	Automated Extractions for Machine Generated Mail	NA:NA:NA:NA:NA:NA:NA	2018
Qianyun Zhang:Shawndra Hill:David Rothschild	Although consumer behavior in response to search engine marketing has been studied extensively, few efforts have been made to understand how consumers search and respond to ads post purchase. Advertising to existing customers the same way as to prospective customers inevitably leads to wasteful and inefficient marketing. Employing a unique dataset that combines both search query and purchase data, we examine consumers' searching behavior and response to search engine marketing after purchase. We study large advertising campaigns for two popular technology products. We find that over half of the branded keyword searches come from consumers who already purchased the products, and that advertising response varies based on whether searchers are pre- or post-purchase. In general, post-purchase searchers are less likely to click on focal brand ads (i.e., they are less responsive to ads for products they already own). However, post-purchase searchers are still responsive to advertising and much more likely to click on ads for complementary products (i.e., they are more responsive to ads for relevant products other than the focal product).	Post Purchase Search Engine Marketing	NA:NA:NA	2018
Xinpeng Chen:Jingyuan Chen:Lin Ma:Jian Yao:Wei Liu:Jiebo Luo:Tong Zhang	Nowadays, billions of videos are online ready to be viewed and shared. Among an enormous volume of videos, some popular ones are widely viewed by online users while the majority attract little attention. Furthermore, within each video, different segments may attract significantly different numbers of views. This phenomenon leads to a challenging yet important problem, namely fine-grained video attractiveness prediction, which only relies on video contents to forecast video attractiveness at fine-grained levels, specifically video segments of several second length in this paper. However, one major obstacle for such a challenging problem is that no suitable benchmark dataset currently exists. To this end, we construct the first fine-grained video attractiveness dataset (FVAD), which is collected from one of the most popular video websites in the world. In total, the constructed FVAD consists of 1,019 drama episodes with 780.6 hours covering different categories and a wide variety of video contents. Apart from the large amount of videos, hundreds of millions of user behaviors during watching videos are also included, such as view counts, "fast-forward, "fast-rewind, and so on, where "view counts" reflects the video attractiveness while other engagements capture the interactions between the viewers and videos. First, we demonstrate that video attractiveness and different engagements present different relationships. Second, FVAD provides us an opportunity to study the fine-grained video attractiveness prediction problem. We design different sequential models to perform video attractiveness prediction by relying solely on video contents. The sequential models exploit the multimodal relationships between visual and audio components of the video contents at different levels. Experimental results demonstrate the effectiveness of our proposed sequential models with different visual and audio representations, the necessity of incorporating the two modalities, and the complementary behaviors of the sequential prediction models at different levels.	Fine-grained Video Attractiveness Prediction Using Multimodal Deep Learning on a Large Real-world Dataset	NA:NA:NA:NA:NA:NA:NA	2018
Manuel Serrano:Sukyoung Ryu	It is our great pleasure to welcome you to the WWW 2018 Web Programming alternate track. We received 14 proposals from all around the world covering a broad range of topics. We evaluated them regarding relevance, quality, and novelty, selecting 1 full-day talks.	Web Programming Chairs' Welcome & Organization	NA:NA	2018
Gabriel Radanne:Jérôme Vouillon	Tierless Web programming languages allow combining client-side and server-side programming in a single program. This allows defining expressions with both client and server parts, and at the same time provides good static guarantees regarding client-server communication. However, these nice properties come at a cost: most tierless languages offer very poor support for modularity and separate compilation. To regain this modularity and offer a larger-scale notion of composition, we propose to leverage a well-known tool: ML-style modules. In modern ML languages, the module system is a layer separate from the expression language. Eliom is an extension of OCaml for tierless Web programming which provides type-safe communication and an efficient execution model. In this article, we present how the Eliom module system combines the flexibility of tierless Web programming languages with a powerful module system, thus providing good support for abstraction, modularity and separate compilation. We also show that we can provide all these advantages while providing seamless integration with OCaml and its ecosystem.	Tierless Web Programming in the Large	NA:NA	2018
Arthur Charguéraud:Alan Schmitt:Thomas Wood	We present JSExplain, a reference interpreter for JavaScript that closely follows the specification and that produces execution traces. These traces may be interactively investigated in a browser, with an interface that displays not only the code and the state of the interpreter, but also the code and the state of the interpreted program. Conditional breakpoints may be expressed with respect to both the interpreter and the interpreted program. In that respect, JSExplain is a double-debugger for the specification of JavaScript.	JSExplain: A Double Debugger for JavaScript	NA:NA:NA	2018
Stéphane Letz:Yann Orlarey:Dominique Fober	\beginabstract This paper demonstrates how FAUST, a functional programming language for sound synthesis and audio processing, can be used to develop efficient audio code for the Web. After a brief overview of the language, its compiler and the architecture system allowing to deploy the same program as a variety of targets, the generation of WebAssembly code and the deployment of specialized WebAudio nodes will be explained. Several use cases will be presented. Extensive benchmarks to compare the performance of native and WebAssembly versions of the same set of DSP have be done and will be commented. \endabstract	FAUST Domain Specific Audio DSP Language Compiled to WebAssembly	NA:NA:NA	2018
Véronique Benzaken:Giuseppe Castagna:Laurent Daynès:Julien Lopez:Kim Nguyen:Romain Vernoux	We present BOLDR, a modular framework that enables the evaluation in databases of queries containing application logic and, in particular, user-defined functions. BOLDR also allows the nesting of queries for different databases of possibly different data models. The framework detects the boundaries of queries present in an application, translates them into an intermediate representation together with the relevant language environment, rewrites them in order to avoid query avalanches and to make the most out of database optimizations, and converts the results back to the application. Our experiments show that the techniques we implemented are applicable to real-world database applications, successfully handling a variety of language-integrated queries with good performances.	Language-Integrated Queries: a BOLDR Approach	NA:NA:NA:NA:NA:NA	2018
Nick ten Veen:Daco C. Harkes:Eelco Visser	Modern web applications are interactive. Reactive programming languages and libraries are the state-of-the-art approach for declara- tively specifying these interactive applications. However, programs written with these approaches contain error-prone boilerplate code for e ciency reasons. In this paper we present PixieDust, a declarative user-interface language for browser-based applications. PixieDust uses static de- pendency analysis to incrementally update a browser-DOM at run- time, without boilerplate code. We demonstrate that applications in PixieDust contain less boilerplate code than state-of-the-art ap- proaches, while achieving on-par performance.	PixieDust: Declarative Incremental User Interface Rendering Through Static Dependency Tracking	NA:NA:NA	2018
Minh Ngo:Nataliia Bielova:Cormac Flanagan:Tamara Rezk:Alejandro Russo:Thomas Schmitz	Multiple Facets (MF) is a dynamic enforcement mechanism which has proved to be a good fit for implementing information flow security for JavaScript. It relies on multi executing the program, once per each security level or view, to achieve soundness. By looking inside programs, MF encodes the views to reduce the number of needed multi-executions. In this work, we extend Multiple Facets in three directions. First, we propose a new version of MF for arbitrary lattices, called Generalised Multiple Facets, or GMF. GMF strictly generalizes MF, which was originally proposed for a specific lattice of principals. Second, we propose a new optimization on top of GMF that further reduces the number of executions. Third, we strengthen the security guarantees provided by Multiple Facets by proposing a termination sensitive version that eliminates covert channels due to termination.	A Better Facet of Dynamic Information Flow Control	NA:NA:NA:NA:NA:NA	2018
Achim D. Brucker:Michael Herzberg	At its core, the Document Object Model (DOM) defines a tree-like data structure for representing documents in general and HTML documents in particular. It is the heart of any modern web browser. Formalizing the key concepts of the DOM is a prerequisite for the formal reasoning over client-side JavaScript programs and for the analysis of security concepts in modern web browsers. We present a formalization of the core DOM, with focus on the node-tree and the operations defined on node-trees, in Isabelle/HOL. We use the formalization to verify the functional correctness of the most important functions defined in the DOM standard. Moreover, our formalization is extensible, i.e., can be extended without the need of re-proving already proven properties and executable, i.e., we can generate executable code from our specification.	A Formal Semantics of the Core DOM in Isabelle/HOL	NA:NA	2018
Amy Guy:Thomas Steiner	It is our great pleasure to welcome you to the WWW 2018 Developers' Track. We had 12 submissions in total, out of which 7 were papers for the proceedings (5 long, 2 short), and 5 were free-form formats that did not go in the proceedings. A lot of great publications are accompanied by great implementations that sometimes risk going almost unnoticed in favor of the more glamorous research results they helped produce. Likewisethe Web being a moving and ever-developing targeta lot of sometimes tedious and oftentimes less obvious work happens on standardization of future Web APIs and programming languages. The Developers' Track aims to put this implementation and standardization work front and center. It highlights research submissions that describe technically challenging Web applications of all sorts. Apart from classic papers (that we do understand are a fixed requirement for some people in order be allowed to the conference), the Developers' Track was not limited to formats that can be printed, and authors were encouraged to be creative in finding the most effective way to communicate their work, and we have included dynamic or interactive contributions.	Developers' Track Chairs' Welcome & Organization	NA:NA	2018
Ksenia Peguero:Nan Zhang:Xiuzhen Cheng	\textitBackground: JavaScript frameworks are widely used to create client-side and server-side parts of contemporary web applications. Vulnerabilities like cross-site scripting introduce significant risks in web applications.\\ \textitAim: The goal of our study is to understand how the security features of a framework impact the security of the applications written using that framework.\\ \textitMethod: In this paper, we present four locations in an application, relative to the framework being used, where a mitigation can be applied. We perform an empirical study of JavaScript applications that use the three most common template engines: Jade/Pug, EJS, and Angular. Using automated and manual analysis of each group of applications, we identify the number of projects vulnerable to cross-site scripting, and the number of vulnerabilities in each project, based on the framework used.\\ \textitResults: We analyze the results to compare the number of vulnerable projects to the mitigation locations used in each framework and perform statistical analysis of confounding variables.\\ \textitConclusions: The location of the mitigation impacts the application's security posture, with mitigations placed within the framework resulting in more secure applications.	An Empirical Study of the Framework Impact on the Security of JavaScript Web Applications	NA:NA:NA	2018
Michel Buffa:Jérôme Lebrun:Jari Kleimola:Oliver larkin:Stéphane Letz	Web Audio is a recent W3C API that brings the world of computer music applications to the browser. Although developers have been actively using it since the first beta implementations in 2012, the number of web apps built using Web Audio API cannot yet compare to the number of commercial and open source audio software tools available on native platforms. Many of the sites using this new technology are of an experimental nature or are very limited in their scope. While JavaScript and Web standards are increasingly flexible and powerful, C and C++ are the languages most often used for real-time audio applications and domain specific languages such as FAUST facilitate rapid development with high performance. Our work aims to create a continuum between native and browser based audio app development and to appeal to programmers from both worlds. This paper presents our proposal including guidelines and proof of concept implementations for an open Web Audio plug-in standard - essentially the infrastructure to support high level audio plug-ins for the browser.	Towards an open Web Audio plugin standard	NA:NA:NA:NA:NA	2018
Andrea Gallidabino:Cesare Pautasso	In the past years the average number of Web-enabled devices owned by each user has significantly increased. Liquid Web applications enable users to take advantage of all their devices sequentially to migrate their running applications across them or simultaneously when running different views of the same application at the same time on each device. Developers of liquid Web application need to control how to expose the liquid behavior of their cross-device Web applications to the users. To do so, they can use the API of Liquid.js we describe in this paper. Liquid.js is a framework for building component-based rich Web applications which run across multiple Web-enabled devices. The framework is based on technologies such as Polymer, WebRTC, WebWorkers, PouchDB and Yjs. Liquid.js helps to build decentralized Web applications whose components can seamlessly flow directly between Web browsers carrying along their execution state. The Liquid.js API gives developers fine-grained control over the liquid user experience primitives, device discovery, and the lifecycle of liquid Web components.	The Liquid User Experience API	NA:NA	2018
Pasquale Lisena:Raphaël Troncy	SPARQL endpoints are one possible access method to linked data. The results of SPARQL queries serialized in JSON are, however, not suitable to be directly used by web developers in end-user applications who often need to merge the values resulting from variable bindings. In this work, we propose a generic approach implemented in a JavaScript module that takes as input a JSON file describing both the SPARQL query and the shape of the expected output at the same time.	Transforming the JSON Output of SPARQL Queries for Linked Data Clients	NA:NA	2018
István Koren:Ralf Klamma	New Internet-enabled devices and Web services are introduced on a daily basis. Documentation formats are available that describe their functionalities in terms of API endpoints and parameters. In particular, the OpenAPI specification has gained considerable influence over the last years. Web-based solutions exist that generate interactive OpenAPI documentation with HTML5 & JavaScript. They allow developers to quickly get an understanding what the services and devices do and how they work. However, the generated user interfaces are far from real-world practices of designers and end users. We present an approach to overcome this gap, by using a model-driven methodology resulting in state-of-the-art responsive Web user interfaces. To this end, we use the Interaction Flow Modeling Language (IFML) as intermediary model specification to bring together APIs and frontends. Our implementation is based on open standards like Web Components and SVG. A screencast of our tool is available at https://youtu.be/KFOPmPShak4	The Exploitation of OpenAPI Documentation for the Generation of Web Frontends	NA:NA	2018
Thomas Steiner	Progressive Web Apps (PWA) are a new class of Web applications, enabled for the most part by the Service Workers APIs. Service Workers allow apps to work offline by intercepting network requests to deliver programmatic or cached responses, Service Workers can receive push notifications and synchronize data in the background even when the app is not running, andtogether with Web App Manifestsallow users to install PWAs to their devices' home screens. Service Workers being a Web standard, support has landed in several stand-alone Android Web browsersamong them (but not limited to) Chrome and its open-source foundation Chromium, Firefox, Edge, Opera, UC Browser, Samsung Internet, andeagerly awaitediOS Safari. In this paper, we examine the PWA feature support situation in Web Views, that is, in-app Web experiences that are explicitly not stand-alone browsers. Such in-app browsers can commonly be encountered in chat applications like WeChat or WhatsApp, online social networks like Facebook or Twitter, but also email clients like Gmail, or simply anywhere where Web content is displayed inside native apps. We have developed an open-source application called PWA Feature Detector that allows for easily testing in-app browsers (and naturally stand-alone browsers), and have evaluated the level of support for PWA features on different devices and Web Views. On the one hand, our results show that there are big differences between the various Web View technologies and the browser engines they are based upon, but on the other hand, that for Android the results are independent from the devices' operating systems, which is good news given the problematic update policy of many device manufacturers. These findings help developers make educated choices when it comes to determining whether a PWA is the right approach given their target users' means of Web access.	What is in a Web View: An Analysis of Progressive Web App Features When the Means of Web Access is not a Web Browser	NA	2018
Erik Wilde	The Web is based on numerous standards that together make up the surface of the Web: By knowing and supporting those standards, problems can be solved in well-known ways. This general design pattern on the Web applies to APIs in the very same way as it does to the human Web: By using an (evolving) set of standards, API developers benefit by not having to reinvent the wheel, and developers benefit by the same problem being solved in the same way across a variety of APIs. The evolving set of standards for Web APIs can be regarded as a set of building blocks or vocabularies for API design. Web Concepts is a site (webconcepts.info) and a repository (github.com/dret/webconcepts) that can be used to manage how within organizations these building blocks are used, thus helping to establish a Web API design culture. The main idea of Web Concepts is to promote reuse of existing standards and technologies, and to therefore make it easier for teams to understand which options are available generally speaking, and maybe which ones are popular choices within their organization.	Surfing the API Web: Web Concepts	NA	2018
Sylvie Calabretto:Lalana Kagal:Maria Maleshkova	It is our great pleasure to welcome you to the WWW 2018 PhD Symposium. The goal of the PhD Symposium is to provide a supportive atmosphere for PhD students to present and receive feedback on their ongoing work. Students at different stages in their research get an opportunity to present and discuss their problem statements, goals, methods and results. The symposium aims to provide students with useful guidance on various aspects of their research from established researchers and other PhD students, working in areas related to the World Wide Web. Finally, the symposium also aims to enable PhD students to interact with other participants and potential collaborators in order to stimulate an exchange of ideas, suggestions and experiences. We received 25 PhD proposal submissions from all around the world covering a broad range of topics. We evaluated them with respect to methodology, approach, relevance and novelty, selecting 8 papers for long presentation and 4 papers for short presentation. We also took in account the coverage of the different areas related to WWW as well as the potential audience and the possibility to receive feedback from seniors and peers.	PhD Symposium Chairs' Welcome	NA:NA:NA	2018
Lucas Azevedo	In the actual scenario of ever-growing data consumption speed and quantity, factors like news source decentralization, citizen journalism and democratization of media, make the task of manually checking and correcting disinformation across the internet impractical or infeasible . Here, there is an imperative need for a fast and reliable way to account for the veracity of what is produced and spread as information: Automatic fact-checking. In this work we present the problem of fact-checking in the era of big data and post-truth. Some existing approaches for this task are presented and their main features discussed and compared. Concluding, a new approach inspired on the best components of the existing ones is presented.	Truth or Lie: Automatically Fact Checking News	NA	2018
Tobias Grubenmann	Inspired by the World Wide Web, the Web of Data is a network of interlinked data fragments. One of the main advantages of the Web of Data is that all of its content is processable by machines. However, this also has its drawbacks when it comes to monetization of the content: advertisements and donationstwo important financial motors in the World Wide Webdo not translate into the Web of Data as they rely on exposing the user to advertisement/call for donations. The remedy this situation, we propose two different monetization strategies for the Web of Data. The first strategy involves a marketplace where users can buy data in an integrated way. The second strategy allows third parties to promote certain data. In return, the sponsors pay money whenever a user follows a link contained in the sponsored data. We identified two different kind of datacommercial and sponsored datawhich can benefit from the two respective monetization strategies. With our work, we propose solutions to the problem of financing the creation and maintenance of content in the Web of Data.	Monetization Strategies for the Web of Data	NA	2018
Tobias Weller	The number of users of the world wide web is constantly increasing. However, this also increases the risks. There is the possibility that other users illegally gain access to a users' account of social networks, web shops or other web services. Previous work use graph-based methods to identify hijacked or compromised accounts. Most often posts are used in social networks to detect fraudulences. However, not every compromised account is used to spread propaganda information or phishing attacks. Therefore, we restrict ourselves to the clickstreams from the accounts. In order to identify compromised accounts by means of clickstreams, we will also consider a temporal aspect, since the preferences of a user change over time. We choose a hybrid approach consisting of methods from subsymbolic and symbolic AI to detect fraudulences in clickstreams. We will also take into account the experience of domain experts. Our approach can also be used to identify not only compromised accounts but also shared accounts on instance streaming sites.	Compromised Account Detection Based on Clickstream Data	NA	2018
Robin Marx	Web performance is important for the user experience and can heavily influence web page revenues. While there are many established Web Performance Optimization (WPO) methods, our work so far has clearly shown that new network protocols, optimized browsers and cutting-edge web standards can have a significant impact on known best practices. Additionally, there is still low-hanging fruit to be exploited, in the form of personalizing performance based on user context (i.e., current device, network, browser) and user preferences (e.g., text reading vs multimedia experience). In our PhD project, we strive to integrate this user-specific metadata into dynamic configurations for both existing and new automated WPO techniques. An intermediate server can (pre)generate optimized versions of a web page, which are then selected based on user context and preferences. Additional metadata is also passed along to the browser, enabling improvements on that side, and used to steer new network protocols to speed up the incremental delivery of page resources. We use the Speeder platform to perform and evaluate full-factorial objective measurements and use subjective user studies across a range of groups to assess the applicability of our methods to end users. Our aim is to provide insights in how WPO can be tweaked for specific users, in the hopes of leading to new web standards that enable this behavior.	Web Performance Automation for the People	NA	2018
Dakshi Tharanga Kapugama Geeganage	Text contents are overloaded with the digitization of the data and new contents are transmitted through many sources by generating a large volume of information, which spreads all over the world through different communication media. Therefore, text data is available everywhere and reading, understanding and analysing the text data has become a main activity in daily routine. With the increment of the volume and the variety of information, organizing and searching, the required information has become vital. Topic modelling is the state of the art for information organization, understanding and extracting the content. Most of the prevailing topic models use the probabilistic approaches and consider the frequency and the co-occurrence to discover the topics from collections of documents. The proposed research aims to address the existing problems of topic modeling by introducing a concept embedded topic model which generates the most relevant and meaningful topics by understanding the content. The research includes approaches to understand the semantic elements from the content, domain identification of concepts and provide most suitable topics without getting the number of topics from the user beforehand. Capturing the semantics of document collections and generating the most related set of topics according to the actual meaning will be the significance of this research.	Concept Embedded Topic Modeling Technique	NA	2018
Reshmi Gopalakrishna Pillai	The ability to detect human stress and relaxation is central for timely diagnosing stress-related diseases, ensuring customer satisfaction in services and managing human-centric applications such as traffic management. Traditional methods employ stress measuring scales or physiological monitoring which may be intrusive and inconvenient. Instead, the ubiquitous nature of social media can be leveraged to identify stress and relaxation. In this PhD research, we introduce an improved method to detect expressions of stress and relaxation in social media content. It uses word sense vectors for word sense disambiguation to improve the performance of the first ever lexicon-based stress/relaxation detection algorithm TensiStrength. Experimental results show that TensiStrength with word sense disambiguation performs better than the original TensiStrength and state-of-the-art machine learning methods in terms of Pearson's correlation and accuracy. We also suggest a novel, word-vector based approach for detecting causes of stress and relaxation in social media content.	Detection of Strength and Causal Agents of Stress and Relaxation for Tweets	NA	2018
Xuanxing Yang	Recently with dynamic information being ubiquitous on the Web, there have been efforts to extend RDF and SPARQL for representing streaming information and continuous querying functionalities, respectively. While existing works focusing on formalization and implementation of continuous querying process over RDF streams, little attention has deserved the problem of querying the complex temporal correlations among RDF stream tuples and the effective, scalable implementation of RDF stream processing system. To fill this gap, in this paper we propose CT-SPARQL and AIMRS, a language specifying for the compositional stream patterns and an architecture for adaptive incremental maintenance of RDF stream tuples defined in CT-SPARQL. We believe that this work will benefit a wide range of real-time analyzing and future predicting applications.	Query for Streaming Information: Dynamic Processing and Adaptive Incremental Maintenance of RDF Stream	NA	2018
Laura Koesten	Structured data is becoming critical in every domain and its availability on the web is increasing rapidly. Despite its abundance and variety of applications, we know very little about how people find data, understand it, and put it to use. This work aims to inform the design of data discovery tools and technologies from a user centred perspective. We aim to better understand what type of information supports people in finding and selecting data relevant for their respective tasks. We conducted a mixed-methods study looking at the workflow of data practitioners when searching for data. From that we identified textual summaries as a key element that supports the decision making process in information seeking activities for data. Based on these results we performed a mixed-methods study to identify attributes people consider important when summarising a dataset. We found text summaries are laid out according to common structures, contain four main information types, and cover a set of dataset features. We describe follow-up studies that are planned to validate these findings and to evaluate their applicability in a dataset search scenario.	A User Centred Perspective on Structured Data Discovery	NA	2018
Dawid Wisniewski	The process of ontology authoring is inseparably connected with the quality assurance phase. One can verify the maturity and correctness of a given ontology by evaluating how many competency questions give correct answers. Competency questions are defined as a set of questions expressed in natural language that the finished ontology should be able to answer to correctly. Although this method can easily indicate what is the development status of an ontology, one has to translate competency questions from natural language into an ontology query language. This task is very hard and time consuming. To overcome this problem, my PhD thesis focuses on methods for automatically checking answerability of competency questions for a given ontology and proposing SPARQL-OWL query (OWL-aware SPARQL query) for each question where it is possible to create the query. Because the task of automatic translation from competency questions to SPARQL-OWL queries is a novel one, besides a method, we have proposed a new benchmark to evaluate such translation.	Automatic Translation of Competency Questions into SPARQL-OWL Queries	NA	2018
Maulik R. Kamdar	The vision of the Semantic Web has stimulated the development of Web-scale architectures for discovering implicit associations from multiple heterogeneous data and knowledge sources. In biomedicine, using W3C-established standards and Linked Data principles, data publishers have transformed and linked several datasets to create a huge web of Life Sciences Linked Open Data (LSLOD). However, mining the LSLOD cloud is still very difficult and often impossible for biomedical researchers due to several challenges: structural heterogeneity, lack of vocabulary reuse, inconsistencies, and incompleteness. To discover drug-adverse reaction associations and their mechanistic explanations, I have developed a novel architecture that combines information retrieval and association discovery. The architecture demonstrates favorable AUROC statistics against baseline methods in pharmacovigilance, and provides confidence values on underlying biological mechanisms. I quantify the several challenges associated with mining the LSLOD cloud for biomedical applications through an empirical analysis of more than 40 different sources. Ideally, the architecture can be extended in other domains to realize the goal of implicit association discovery.	Mining the Web of Life Sciences Linked Open Data for Mechanism-Based Pharmacovigilance	NA	2018
Isa Inuwa-Dutse	Contemporary social media networks can be viewed as a break to the early two-step flow model in which influential individuals act as intermediaries between the media and the public for information diffusion. Today's social media platforms enable users to both generate and consume online contents. Users continuously engage and disengage in discussions with varying degrees of interaction leading to formation of distinct online communities. Such communities are often formed at high-level either based on metadata, such as hashtags on Twitter, or popular content triggered by few influential users. These online communities often do not reflect true connectivity and lack the cohesiveness of traditional communities. In this study, we investigate real-time formation of temporal communities on Twitter. We aim at defining both high and low levels connections and to reveal the magnitude of clustering cohesion on temporal basis. Inspired by a real-life event center sitting arrangement scenario, the proposed method aims to cluster users into distinct and cohesive online temporal communities. Membership to a community relies on intrinsic tweet properties to define similarity as the basis for interaction networks. The proposed method can be useful for local event monitoring and clique-based marketing among other applications.	Modelling Formation of Online Temporal Communities	NA	2018
Varsha Bhat Kukkala	Social networks have been a popular choice of study, given the surge of online data on friendship networks, communication networks, collaboration networks etc. This popularity, however, is not true for all types of social networks. In the current work, we draw the reader's attention to a class of social networks which are investigated to a limited extent, classified as distributed sensitive social networks. It constitutes of networks where the presence or absence of edges in the network is distributedly known to a set of parties, who regard this information as their private data. Supply chain networks, informal networks such as trust network, advice network, enmity network, etc. are a few examples of the same. A major reason for the lack of any substantial study on these networks has been the unavailability of data. As a solution, we propose a privacy preserving approach to investigating these networks. We show the feasibility of using secure multiparty computation techniques to perform the required analysis, while preserving the privacy of every individual's data. The possible approaches that can be considered to ensure the design of efficient secure protocols are discussed such as efficient circuit design, ORAM based secure computation, use of oblivious data structures, etc. The results obtained in the direction of secure network analysis algorithms are also presented.	Privacy Preserving Distributed Analysis of Social Networks	NA	2018
Angela Bonifati:Romain Wuillemot	It is our great pleasure to welcome you to the WWW 2018 Panels Track. We have selected three panel proposals among the ones received. We also took in account the coverage of the different areas related to WWW, inputs from the community with a shared document, as well as the suggestions of the authors of the research tracks in deciding the subjects of the three panels.	Panels Track Chairs' Welcome	NA:NA	2018
Davood Rafiei:Eugene Agichtein:Ricardo Baeza-Yates:Jon Kleinberg:Jure Leskovec	The Web's content has been going through major changes, triggered by multiple factors including changes in user demographic and authoring behaviour, a shift in device types that access the Web, and changes in common use cases of the Web. More specifically, the number of mobile internet users has surpassed the desktop users according to different statistics; a considerable portion of web use cases are in the form of social interactions rather than information seeking; and the authoring behaviour has transformed from compiling a page and linking resources to sharing content with like-minded followers and leaving likes and comments on posts. Those changes have influenced and are expected to shape the way the content is organized, searched, ranked and analyzed. This panel brings together researchers who have been working in different established areas related to web search and mining, web content and social network analysis, and semantics and knowledge management. The panel will draw from the experience of the panellists, dealing with changes in their respective fields. In the first (role-playing) round, each panellist will strongly take a side on where the changes are heading, arguing that one form of content will dominate in the near future. In the second round, the panellists will counter each other and will share their vision on what future holds in terms of research problems and directions. The members of the audience will participate, in a QA session with the panellists, bringing their own perspectives to the discussion.	The Shifting Landscape of Web Search and Mining: Past, Present, and Future	NA:NA:NA:NA:NA	2018
Boualem Benatallah:Fabio Casati	Cognitive services and conversational digital assistants are emerging as the engine that powers natural interactions between humans, software services, devices and "things" - supported by advances in AI and human computations. Not surprisingly, many large and small tech companies are rushing to occupy this space by providing platforms for building cognitive services and conversational bots. Digital assistants interact in a natural way (through text or voice) with both software and humans to get information and perform actions, from checking the weather to booking a restaurants and a cab ride, managing cloud resources, answering simple scientific questions, and preparing a decaf latte using IoT enabled coffee machines. User requests or tasks are often expressed in natural language, an interaction ensues to clarify the intent and the details, and the answer is sought - or the appropriate service or device is invoked - based on the cognitive service understanding. While the potential of this new wave of services is exciting, it also brings significant challenges: we are far away from the comfort of developing deterministic software that responds to API calls by invoking other APIs. Now we have to understand, guess, explore options, take decisions based on probabilistic models over a large set of possible intents and services, all while engaging with users. Doing so brings a large set of engineering challenges related to the development, training, tuning and evolution of such services. This panel will discuss such challenges and identify interesting opportunities for research as well as promising trends.	Panel on Cognitive Service Engineering	NA:NA	2018
Steffen Staab:Jens Lehmann:Ruben Verborgh	Structured Knowledge on the Web had an intriguing history before it has become successful. We briefly revisit this history, before we go into the longer discussion about how structured knowledge on the Web should be devised such that it benefits even more applications. Core to this discussion will be issues like trust, information infrastructure usability and resilience, promising realms of structured knowledge and principles and practices of data sharing.	Structured Knowledge on the Web 7.0	NA:NA:NA	2018
Eyhab Al-Masri:Marie-Christine Rousset	It is our great pleasure to welcome you to the WWW 2018 Workshops. This year's workshops of WWW 2018 feature a number of co-located workshops that are intended to provide a forum for researchers and practitioners in Web technologies to discuss and exchange positions on current and emergent Web topics. We received forty proposals from all around the world covering a broad range of topics. We evaluated them regarding relevance, quality, and novelty selecting eighteen full-day workshops and ten half-day workshops. We also took into account the coverage of the different areas related to WWW as well as the potential audience, to schedule them in two consecutive days with the minimal audience interest overlap.	Workshop Chairs' Welcome	NA:NA	2018
Leonidas Anthopoulos:Marijn Janssen:Vishanth Weerakkody	Following up the success of the past events at WWW2015, WWW2016 and WWW2017, the 4th AW4City 2018 aims to keep on attracting a significant international attention with regard to web applications for smart cities. More specifically, the aim of this workshop is to focus on the applications smart city component and more specifically on the design and implementation of web-based applications and Apps that deliver smart services or address smart city challenges. This year, the proposed workshop will emphasize on the contribution of web applications and Apps to citizen centricity. In the era of cities, municipal leaders, service and utility providers are making an important shift regarding thinking of people as customers and of customer experience. This shift is not a simple task since it demands a continuous service monitoring, assessment and improvement [1;2;3], which normally is based on accurate data analysis and appears as a thinking that makes government and providers more personal and responsive.	AW4City 2018 Chairs' Welcome & Organization	NA:NA:NA	2018
Zeenat Rehena:Marijn Janssen	In the last few years, the smart city concept resulted in the development and deployment of platforms for providing innovative services to improve sustainability and the living standards. These platforms integrate data collected from devices and citizen-generated data and thereafter employ big data analytics to create insights from the data. These platform enable the creation of context-aware Intelligent traffic management systems (ITMS), however the involvement of various actors at different stages hampers development. In this paper, we propose a framework to support sustainable traffic management system for providing better commute, safety and security during travel based on real-time information. The framework should help to integrate the activities performed by the various actors. The main key elements of this framework are Datasets, Traffic Management Analytics, Actors and Actions which are taken by these users. The framework helps to create an overall overview of the activities needed. In this way it can be used to improve the quality of the traffic flow, increase efficient use of resources, smooth and safe commute of the citizens.	Towards a Framework for Context-Aware Intelligent Traffic Management System in Smart Cities	NA:NA	2018
Auriol Degbelo:Tomi Kauppinen	Recent years have witnessed progress of public institutions in making their datasets available online, free of charge, for re-use. This notwithstanding, there is still a long way to go to put the power of data in the hands of citizens. This article suggests that transparency in the context of open government can be increased through web maps featuring: i) Application Programming Interfaces (APIs) which support app and data usage tracking; and (ii) 'transparency badges' which inform the users about the presence/absence of extra, useful contextual information. Eight examples of web maps are introduced as proof of concept for the idea. Designing and implementing these web maps has reminded of the need of interactive guidelines to help non-experts select vocabularies, and datasets to link to. The ideas presented are relevant to making existing open data more user friendly (and ultimately more usable).	Increasing Transparency through Web Maps	NA:NA	2018
Vaia Moustaka:Zenonas Theodosiou:Athena Vakali:Anastasis Kounoudes	As smart cities infrastructures mature, data becomes a valuable asset which can radically improve city services and tools. Registration, acquisition and utilization of data, which will be transformed into smart services, are becoming more necessary than ever. Online social networks with their enormous momentum are one of the main sources of urban data offering heterogeneous real-time data at a minimal cost. However, various types of attacks often appear on them, which risk users' privacy and affect their online trust. The purpose of this article is to investigate how risks on online social networks affect smart cities and study the differences between privacy and security threats with regard to smart people and smart living dimensions.	Smart Cities at Risk!: Privacy and Security Borderlines from Social Networking in Cities	NA:NA:NA:NA	2018
Gabriela Viale Pereira:Gregor Eibl:Peter Parycek	This paper presents the analysis of the SmartGov project as a case of smart technologies application, such as expert-based Fuzzy Cognitive Maps, social media applications and open data, to promote citizen engagement and support decision-making. The objective of this paper is to analyze the role of digital technologies as inputs to achieve smart city governance. The main results are illustrated in a framework that combines the smart city governance elements in a real case description.	The Role of Digital Technologies in Promoting Smart City Governance	NA:NA:NA	2018
Agnes Mainka:Tobias Siebenlist:Lisa Beutelspacher	Participatory smartphone apps empower citizens to interact with the city's administration. The purpose of this case study is to investigate the current state of participatory apps in Germany. The 29 apps that have been found can be categorized into four topics: Information Awareness, City Service, Transparency and Public Safety. Most citizen apps can be assigned to the category Travel & Local. None of the identified apps is based on open-source code, and the citizens' reports are not publicly visible, e.g., for other citizens. It is unclear to whom the data generated by citizens belong.	Citizen Participation: Case Study on Participatory Apps in Germany	NA:NA:NA	2018
Thivya Kandappu:Archan Misra:Desmond Koh:Randy Daratan Tandriansyah:Nikita Jaiman	Active citizenry, whereby citizens actively participate in reporting and addressing challenges in urban service delivery is a strategic goal of smart cities such as Singapore. In spite of the promise, we believe that the success of such large-scale nation-wide crowdsourcing deployments depend on the real-word user preferences and behavioral characteristics of citizens. In this paper, we first present our findings on behavioral preferences and key concerns of citizens regarding smart-city services via an opinion survey conducted with 1300 participants. We then propose a "citizen-controlled" urban services reporting platform where citizens actively report on the status of various municipal resources. We advocate the importance of matching user mobility patterns against task locations to make the platform more efficient (i.e., higher task completion rate and lower detour overhead).	A Feasibility Study on Crowdsourcing to Monitor Municipal Resources in Smart Cities	NA:NA:NA:NA:NA	2018
Leonidas Anthopoulos:Amel Attour	An increasing amount of applications can be located in several cities that attempt to deal with mobility issues like traffic management, transportation safety, congestion control, taxi booking, car sharing, carpooling etc. The aim of this work in progress article is to collect information with regard to carpooling applications and attempt to recognize the underlying business models.	Smart Transportation Applications' Business Models: A Comparison	NA:NA	2018
Pernilla Bergmark	This paper looks into the use of Information and Communication Technology (ICT) for Smart Sustainable Cities (SSC). It specifically points towards ICT's potential to help cities mitigate climate change and to support a 2ºC or lower trajectory and to involve citizens in city planning and when implementing solutions. The paper also focuses on the modelling, assessment methodologies and indicators for ICT and sustainability aspects of cities, not least with regards to climate change mitigation. Especially, it highlights the city GG emissions assessment standard by ITU, and the indicator sets from ITU and ISO and their enhancement. The paper emphasizes a sustainability and citizen-centric perspective, seeing ICT as instrumental in this respect. For assessments, the above mentioned ITU-T standard is considered reusable for impacts beyond global warming. For indicators, further research on outcome and impact indicators is suggested, and also strengthening of the socio-economic and cultural aspects.	Reflections Regarding ICT and a Citizen-centric Future Path of Smart Sustainable Cities: AW4City 2018 Keynote	NA	2018
Vinay Kandpal	Nagpur has emerged as the topmost smart city in India. In just five months, Nagpur has beaten other cities chosen before it to get the best implementation of smart city plan. A recent stock-taking exercise conducted by urban development ministry has revealed that Nagpur, though chosen as a smart city in September 2016 much after 33 smart cities in two previous rounds has achieved the best investment conversion ratio. India's smart city program hopes to revolutionize city life and improve the quality of life for India's urban population. In the absence of a zonal plan, many parts of Dehradun have witnessed haphazard development over the years, which has already caused much damage to the vision of a planned smart city. Smart City would require smart economy, bright people, smart organization, smart communication, smart engineering, smart transit, fresh environment and bright living. Nevertheless, with mass migration leading to basic problems, like water shortages and overcrowding, the rate at which these cities will be developed will be the key. Several initiatives are being led by the Government of India to convert 100 Cities into Smart Cities. Government to Actively Use PPP Route and Encourage FDI for Effective Implementation of Smart Cities Project in India.	A Case Study on Smart City Projects in India: An Analysis of Nagpur, Allahabad and Dehradun	NA	2018
Jie Tang:Michalis Vazirgiannis:Yuxiao Dong:Fragkiskos D. Malliaros:Michael Cochez:Mayank Kejriwal:Achim Rettinger	It is our great pleasure to welcome you to the 2018 International Workshop on Learning Representations for Big Networks ([email protected]). This is the third edition of the BigNet workshop series, following its inauguration at the 25th ACM International Conference on Information and Knowledge Management (CIKM 2016) and the second edition at the 26th International World Wide Web Conference (WWW 2017). Recent years have witnessed the emergence of network representation learning research. Different from the feature engineering process in conventional analysis, network representation learning, also know as network embedding, aims to learn the latent low-dimensional representations for objects in networks, such as nodes, links, and groups. Its ultimate objective is to encode networks' structural properties into the latent representations, benefiting all existing network mining tasks, such as node classification, link prediction, community detection, etc. In the BigNet 2018 workshop, we aim to provide a forum for presenting the most recent advances in network representation learning to unearth rich knowledge.	BigNet 2018 Chairs' Welcome & Organization	NA:NA:NA:NA:NA:NA:NA	2018
Ayushi Dalmia:Ganesh J:Manish Gupta	Recently there have been a large number of studies on embedding large-scale information networks using low-dimensional, neighborhood and community aware node representations. Though the performance of these embedding models have been better than traditional methods for graph mining applications, little is known about what these representations encode, or why a particular node representation works better for certain tasks. Our work presented here constitutes the first step in decoding the black-box of vector embeddings of nodes by evaluating their effectiveness in encoding elementary properties of a node such as page rank, degree, closeness centrality, clustering coefficient, etc. We believe that a node representation is effective for an application only if it encodes the application-specific elementary properties of nodes. To unpack the elementary properties encoded in a node representation, we evaluate the representations on the accuracy with which they can model each of these properties. Our extensive study of three state-of-the-art node representation models (DeepWalk, node2vec and LINE) on four different tasks and six diverse graphs reveal that node2vec and LINE best encode the network properties of sparse and dense graphs respectively. We correlate the model performance obtained for elementary property prediction tasks with the high-level downstream applications such as link prediction and node classification, and visualize the task performance vector of each model to understand the semantic similarity between the embeddings learned by various models. Our first study of the node embedding models for outlier detection reveals that node2vec and DeepWalk identify outliers well for sparse and dense graphs respectively. Our analysis highlights that the proposed elementary property prediction tasks help in unearthing the important features responsible for the given node embedding model to perform well for a given downstream task. This understanding would facilitate in picking the right model for a given downstream task.	Towards Interpretation of Node Embeddings	NA:NA:NA	2018
Ryan A. Rossi:Rong Zhou:Nesreen K. Ahmed	This paper presents a general inductive graph representation learning framework called DeepGL for learning deep node and edge features that generalize across-networks. In particular, DeepGL begins by deriving a set of base features from the graph (e.g., graphlet features) and automatically learns a multi-layered hierarchical graph representation where each successive layer leverages the output from the previous layer to learn features of a higher-order. Contrary to previous work, DeepGL learns relational functions (each representing a feature) that naturally generalize across-networks and are therefore useful for graph-based transfer learning tasks. Moreover, DeepGL naturally supports attributed graphs, learns interpretable inductive graph representations, and is space-efficient (by learning sparse feature vectors). In addition, DeepGL is expressive, flexible with many interchangeable components, efficient with a time complexity of O(|E|), and scalable for large networks via an efficient parallel implementation. Compared with recent methods, DeepGL is (1) effective for across-network transfer learning tasks and large (attributed) graphs, (2) space-efficient requiring up to 6x less memory, (3) fast with up to 182x speedup in runtime performance, and (4) accurate with an average improvement in AUC of 20% or more on many learning tasks and across a wide variety of networks.	Deep Inductive Network Representation Learning	NA:NA:NA	2018
Ivan Brugere:Tanya Y. Berger-Wolf	Networks are fundamental models for data used in practically every application domain. In most instances, several implicit or explicit choices about the network definition impact the translation of underlying data to a network representation, and the subsequent question(s) about the underlying system being represented. Users of downstream network data may not even be aware of these choices or their impacts. We propose a task-focused network model selection methodology which addresses several key challenges. Our approach constructs network models from underlying data and uses minimum description length (MDL) criteria for selection. Our methodology measures efficiency, a general and comparable measure of the network's performance of a local (i.e. node-level) predictive task of interest. Selection on efficiency favors parsimonious (e.g. sparse) models to avoid overfitting and can be applied across arbitrary tasks and representations. We show stability, sensitivity, and significance testing in our methodology.	Network Model Selection Using Task-Focused Minimum Description Length	NA:NA	2018
Giang Hoang Nguyen:John Boaz Lee:Ryan A. Rossi:Nesreen K. Ahmed:Eunyee Koh:Sungchul Kim	Networks evolve continuously over time with the addition, deletion, and changing of links and nodes. Although many networks contain this type of temporal information, the majority of research in network representation learning has focused on static snapshots of the graph and has largely ignored the temporal dynamics of the network. In this work, we describe a general framework for incorporating temporal information into network embedding methods. The framework gives rise to methods for learning time-respecting embeddings from continuous-time dynamic networks. Overall, the experiments demonstrate the effectiveness of the proposed framework and dynamic network embedding approach as it achieves an average gain of 11.9% across all methods and graphs. The results indicate that modeling temporal dependencies in graphs is important for learning appropriate and meaningful network representations.	Continuous-Time Dynamic Network Embeddings	NA:NA:NA:NA:NA:NA	2018
Andriy Nikolov:Peter Haase:Daniel M. Herzig:Johannes Trame:Artem Kozlov	Vector embedding models have recently become popular for encoding both structured and unstructured data. In the context of knowledge graphs such models often serve as additional evidence supporting various tasks related to the knowledge base population: e.g., information extraction or link prediction to expand the original dataset. However, the embedding models themselves are often not used directly alongside structured data: they merely serve as additional evidence for structured knowledge extraction. In the metaphactory knowledge graph management platform, we use federated hybrid SPARQL queries for combining explicit information stated in the graph, implicit information from the associated embedding models, and information extracted using vector embeddings in a transparent way for the end user. In this paper we show how we integrated RDF data with vector space models to construct an augmented knowledge graph to be used in customer applications.	Combining RDF Graph Data and Embedding Models for an Augmented Knowledge Graph	NA:NA:NA:NA:NA	2018
Richard Han:Jeremy Blackburn:Homa Hosseinmardi:Qin Lv:Bert Huang:Shivakant Mishra	It is our great pleasure to welcome you to the WWW 2018 Workshop on Computational Methods in Cybersafety, Online Harassment, and Misinformation. The theme of cybersafety is an important emerging research topic on the Internet that manifests itself daily as users navigate the Web and networked applications. After two successful workshops on cybersafety, the main goal of this third edition of this workshop on cybersafety is to build and grow the cybersafety research community by bringing together the leading researchers and practitioners from academia, industry, government, and research labs working in the general area of cybersafety to discuss the unique challenges in addressing various cybersafety issues and to share experiences, solutions, tools, and techniques. The focus is on the detection, prevention, and mitigation of various cybersafety issues, as well as education and promoting safe practices. The focus of this workshop is on computational methods in cybersafety, including new algorithms, tools, data mining techniques, analysis, systems, and applications for the detection, prevention and mitigation of various cybersafety issues, as well as education and promoting safe practices. Our program features two invited keynote speakers. We will have Dr. April Edwards, Vice President for Academic Affairs and Dean of the Faculty at Elmhurst College, speak about Racial and Gender Differences in Cyberbullying Behavior. And we will have Dr. Neil Shah, Research Scientist at Snap Inc., speak about Anomaly Detection on Large Social Graphs. We will also feature four contributed presentations and publications selected from papers submitted to our workshop.	International Workshop on Cybersafety Chairs' Welcome & Organization	NA:NA:NA:NA:NA:NA	2018
Anna Sapienza:Sindhu Kiranmai Ernala:Alessandro Bessi:Kristina Lerman:Emilio Ferrara	Widespread adoption of networking technologies has brought about tremendous economic and social growth, but also exposed individuals and organization to new threats from malicious cyber actors. Recent attacks by WannaCry and NotPetya ransomware crypto-worms, infected hundreds of thousands of computer systems world wide, compromising data and critical infrastructure. In order to limit their impact, it is, therefore, critical to detect---and even predict---cyber attacks before they spread. Here, we introduce DISCOVER, an early cyber threat warning system, that mines online chatter from cyber actors on social media, security blogs, and darkweb forums, to identify words that signal potential cyber attacks. We evaluate DISCOVER and find that it can identify terms related to emerging cyber threats with precision above $80%$. DISCOVER also generates a time line of related online discussions on different Web sources that can be useful for analyzing emerging cyber threats.	DISCOVER: Mining Online Chatter for Emerging Cyber Threats	NA:NA:NA:NA:NA	2018
Emeric Bernard-Jones:Jeremiah Onaolapo:Gianluca Stringhini	We set out to understand the effects of differing language on the ability of cybercriminals to navigate webmail accounts and locate sensitive information in them. To this end, we configured thirty Gmail honeypot accounts with English, Romanian, and Greek language settings. We populated the accounts with email messages in those languages by subscribing them to selected online newsletters. We also hid email messages about fake bank accounts in fifteen of the accounts to mimic real-world webmail users that sometimes store sensitive information in their accounts. We then leaked credentials to the honey accounts via paste sites on the Surface Web and the Dark Web, and collected data for fifteen days. Our statistical analyses on the data show that cybercriminals are more likely to discover sensitive information (bank account information) in the Greek accounts than the remaining accounts, contrary to the expectation that Greek ought to constitute a barrier to the understanding of non-Greek visitors to the Greek accounts. We also extracted the important words among the emails that cybercriminals accessed (as an approximation of the keywords that they possibly searched for within the honey accounts), and found that financial terms featured among the top words. In summary, we show that language plays a significant role in the ability of cybercriminals to access sensitive information hidden in compromised webmail accounts.	BABELTOWER: How Language Affects Criminal Activity in Stolen Webmail Accounts	NA:NA:NA	2018
Arpita Chakraborty:Yue Zhang:Arti Ramesh	The possibility of anonymity and lack of effective ways to identify inappropriate messages have resulted in a significant amount of online interaction data that attempt to harass, bully, or offend the recipient. In this work, we perform a preliminary linguistic study on messages exchanged using one such popular web/smartphone application---Sarahah, that allows friends to exchange messages anonymously. Since messages exchanged via Sarahah are private, we collect them when the recipient shares it on Twitter. We then perform an analysis of the different kinds of messages exchanged through this application. Our linguistic analysis reveals that a significant number of these messages (~20%) include inappropriate, hurtful, or profane language intended to embarrass, offend, or bully the recipient. Our analysis helps in understanding the different ways in which anonymous message exchange platforms are used and the different types of bullying present in such exchanges.	Understanding Types of Cyberbullying in an Anonymous Messaging Application	NA:NA:NA	2018
Savvas Zannettou:Barry Bradlyn:Emiliano De Cristofaro:Haewoon Kwak:Michael Sirivianos:Gianluca Stringini:Jeremy Blackburn	Over the past few years, a number of new "fringe" communities, like 4chan or certain subreddits, have gained traction on the Web at a rapid pace. However, more often than not, little is known about how they evolve or what kind of activities they attract, despite recent research has shown that they influence how false information reaches mainstream communities. This motivates the need to monitor these communities and analyze their impact on the Web's information ecosystem. In August 2016, a new social network called Gab was created as an alternative to Twitter. It positions itself as putting "people and free speech first", welcoming users banned or suspended from other social networks. In this paper, we provide, to the best of our knowledge, the first characterization of Gab. We collect and analyze 22M posts produced by 336K users between August 2016 and January 2018, finding that Gab is predominantly used for the dissemination and discussion of news and world events, and that it attracts alt-right users, conspiracy theorists, and other trolls. We also measure the prevalence of hate speech on the platform, finding it to be much higher than Twitter, but lower than 4chan's Politically Incorrect board.	What is Gab: A Bastion of Free Speech or an Alt-Right Echo Chamber	NA:NA:NA:NA:NA:NA:NA	2018
Inaya Lahoud:Elsa Cardoso:Nada Matta	Welcome to the 3rd Educational Knowledge Management (EKM) workshop, which takes place at the WWW18 conference, in Lyon, France. The first edition was in 2014 in conjunction with the International Conference on Knowledge Engineering and Knowledge Management (EKAW), which was held in Linköping, Sweden, and the second one with EKAW 2016, in Bologna, Italy. We received 6 papers from all around the world covering a broad range of topics. Each paper was reviewed by three members of the program committee. After the reviewing process, three papers were accepted for inclusion into the WWW proceedings volume, two of them as full papers and one as a short paper. The EKM2018 workshop will run as a half-day event, including the papers' presentation and two invited speaker sessions. A Best Paper Award will be assigned, and authors will be invited to submit an extended version of their work to a special issue that will be published as part of the "International Journal of Continuing Engineering Education and Lifelong Learning." The first paper Construction and Applications of TeKnowbase A Knowledge Base of Computer Science concepts' by Rajna Upadhyay, Ashutosh Bindal, Manjeet Kumar, and Maya Ramanath describes the development and evaluation of TeKnowbase, and how to use it in a variety of applications for learning a new topic, classification of technical text and querying and ranking computer science articles. The second one untitled "Ontology-based recommender system in higher education" by Charbel Obeid, Inaya Lahoud, Hicham El Khoury, and Pierre-Antoine Champin, is a position paper discussing an ontology-based recommender system to support a student's choice of major and university. The third paper "Automatic Generation of Quizzes from DBpedia According to Educational Standards" by Oscar Rodríguez Rocha and Catherine Faron Zucker focuses on educational quizzes. The authors present an approach to generate quizzes automatically from existing knowledge bases available on the Web of Linked Open Data (LOD), according to the official French educational standards.	3rd EKM Workshop Chairs' Welcome & Organization	NA:NA:NA	2018
Serge Garlatti:Jean Marie Gilliot:Sacha Kieffer:Jérôme Eneau:Genevieve Lameul:Partricia Serrano-Alvarado:Hala Skaf-Molli:Emmanuel Desmontils	NA	Open Learner Models, Trust and Knowledge Management for Life Long Learning	NA:NA:NA:NA:NA:NA:NA:NA	2018
Elsa Cardoso	Learning Analytics (LA) is a recent research field, in which Business Intelligence and Analytics techniques are applied to learners and their contexts, with the purpose of acquiring a greater insight about the entire learning process (including outcomes). In this talk, we explore the LA landscape, delving into the definitions, techniques, challenges, and lessons learned.	The Past, Present, and Future of Learning Analytics	NA	2018
Prajna Upadhyay:Ashutosh Bindal:Manjeet Kumar:Maya Ramanath	In this paper, we make two main contributions. First, we describe the construction and evaluation of TeKnowbase, a knowledge-base of technical concepts in computer science. And second, we show how to use TeKnowbase in a variety of applications, including, generation of pre-requisite concepts for learning a new topic, classification of technical text and querying and ranking computer science articles.	Construction and Applications of TeKnowbase: A Knowledge Base of Computer Science Concepts	NA:NA:NA:NA	2018
Charbel Obeid:Inaya Lahoud:Hicham El Khoury:Pierre-Antoine Champin	Academic advising is limited in its ability to assist students in identifying academic pathways. Selecting a major and a university is a challenging process rife with anxiety. Students at high school are not sure how to match their interests with their working future or major. Therefore, high school students need guidance and support. Moreover, students need to filter, prioritize and efficiently get appropriate information from the web in order to solve the problem of information overload. This paper represents an approach for developing ontology-based recommender system improved with machine learning techniques to orient students in higher education. The proposed recommender system is an assessment tool for students' vocational strengths and weaknesses, interests and capabilities. The main objective of our ontology-based recommender system is to identify the student requirements, interests, preferences and capabilities to recommend the appropriate major and university for each one.	Ontology-based Recommender System in Higher Education	NA:NA:NA:NA	2018
Oscar Rodríguez Rocha:Catherine Faron Zucker	Educational quizzes are a powerful and popular tool to test the knowledge acquired by a learner and also to deepen her/his knowledge about a specific subject in an informal and entertaining way. Their production is a time-consuming task that can be automated by taking advantage of existing knowledge bases available on the Web of Linked Open Data (LOD). For these quizzes to be useful to learners, they must be generated according to the knowledge and skills defined by official educational standards for each subject and school year. This paper shows an approach to generate quizzes automatically according to the official French educational standards, from two different knowledge bases. Likewise, we show an evaluation of both knowledge bases.	Automatic Generation of Quizzes from DBpedia According to Educational Standards	NA:NA	2018
Franz Baader:Brigitte Grau:Yue Ma	NA	HQA18 Workshop Chairs' Welcome & Organization	NA:NA:NA	2018
Eric Gaussier	Semantic annotation in the biomedical domain raises the problem of classifying texts with large-scale taxonomies, a problem sometimes referred to as extreme classification. In this presentation, we will give an overview of this problem and the main solutions proposed, with a focus on textual collections and the BioASQ challenge.	Semantic Annotation in the Biomedical Domain: Large-scale Classification and BioASQ	NA	2018
Andreas Both	In the past, the research, as well as industry, brought Question Answering (QA) into daily use. However, there is still an obvious gap between the claim of providing access to any--structured or unstructured--knowledge stored in the world using an interface fitting the demands of regular users. On the one hand side, implementing Question Answering Systems is still hard and time-consuming, on the other hand side, the QA community is still struggling on defining a common ground for collaboration across research fields regarding, for example, realistic benchmarks, maintainability, and broad coverage of knowledge sources. In the talk, challenges of Question Answering will be highlight w.r.t. hybrid QA, domain-specific QA, cross knowledge base QA, etc. Particularly the industry perspective is also presented while aiming at a Question Answering platform which can be assembled from industry components as well as components of the research community. First steps towards this long-term vision are provided by the Qanary framework and similar frameworks aiming at a collaborative approach for the development QA systems which should lead to effective implementations, improved research results as well as a platform economy for QA. Such a platform would industry lead to a tighter collaboration while academics would have the opportunity of accessing precious data for further improvements.	Towards Component-based, Domain-specific, Efficient Question Answering Systems	NA	2018
Phuong Le-Hong:Duc-Thien Bui	In this paper, we describe the development of an end-to-end factoid question answering system for the Vietnamese language. This system combines both statistical models and ontology-based methods in a chain of processing modules to provide high-quality mappings from natural language text to entities. We present the challenges in the development of such an intelligent user interface for an isolating language like Vietnamese and show that techniques developed for inflectional languages cannot be applied as is. Our question answering system can answer a wide range of general knowledge questions with promising accuracy on a test set.	A Factoid Question Answering System for Vietnamese	NA:NA	2018
Zhen Jia:Abdalghani Abujabal:Rishiraj Saha Roy:Jannik Strötgen:Gerhard Weikum	Answering complex questions is one of the challenges that question-answering (QA) systems face today. While complexity has several facets, question dimensions like temporal and spatial intents necessitate specialized treatment. Methods geared towards such questions need benchmarks that reflect the desired aspects and challenges. Here, we take a key step in this direction, and release a new benchmark, TempQuestions, containing 1,271 questions, that are all temporal in nature, paired with their answers. As a key contribution that enabled the creation of this resource, we provide a crisp definition for temporal questions. Most questions require decomposing them into sub-questions, and the questions are of a kind that they would be best evaluated on a combination of structured data and unstructured text sources. Experiments with two QA systems demonstrate the need for further research on complex questions.	TempQuestions: A Benchmark for Temporal Question Answering	NA:NA:NA:NA:NA	2018
Atsushi Otsuka:Kyosuke Nishida:Katsuji Bessho:Hisako Asano:Junji Tomita	We propose a novel Frequently Asked Question (FAQ) retrieval technique with a neural query expansion model. With the growth in Question Answering systems and mobile communications, FAQ retrieval systems have become widely used in site searches and call center support. However, FAQ retrieval often has lexical gaps between queries and answer documents. To bridge these gaps, we design a query expansion model on the basis of an Encoder-Decoder model as a type of deep neural network. The model learns the words that appear in answers for questions using Q&A pair documents and generates the expanded queries from inputted queries to retrieve answer documents. We evaluate our proposed technique in a multi-domain FAQ retrieval task. Experimental results show that our technique retrieves FAQs more accurately than the previous methods.	Query Expansion with Neural Question-to-Answer Translation for FAQ-based Question Answering	NA:NA:NA:NA:NA	2018
Franz Baader:Stefan Borgwardt:Walter Forkel	Finding suitable candidates for clinical trials is a labor-intensive task that requires expert medical knowledge. Our goal is to design (semi-)automated techniques that can support clinical researchers in this task. We investigate the issues involved in designing formal query languages for selecting patients that are eligible for a given clinical trial, leveraging existing ontology-based query answering techniques. In particular, we propose to use a temporal extension of existing approaches for accessing data through ontologies written in Description Logics. We sketch how such a query answering system could work and show that eligibility criteria and patient data can be adequately modeled in our formalism.	Patient Selection for Clinical Trials Using Temporalized Ontology-Mediated Query Answering	NA:NA:NA	2018
Martino Mensio:Giuseppe Rizzo:Maurizio Morisio	QA systems offer a human friendly interface to navigate through knowledge, which can range from encyclopedic to domain-specific. Generally, a QA system is designed to provide an answer to a specific question once (so-called single turn) and state-of-the-art systems reach nowadays robust performance in such a scenario. However, most of the interactions with QA systems are based on multiple handshakes of question/answer pairs, where the human being refines the questions further, while the system can collect the necessary information and generate a compelling final answer through multiple turns. In this paper, we investigate and experiment a multi-turn QA system that is suited to work given a particular domain of knowledge and configurable goals. Our approach models the entire dialogue as a sequence of turns, i.e. questions and answers, using a Recurrent Neural Network which is firstly trained to understand natural language, classifying entities and intents using prior knowledge of domain-specific interactions, and provide answers according to the domain used as background knowledge. We have compared our approach with state-of-the-art sequence-based intent classification using a well-known and standardized gold standard observing an increase of 17.16% of F1. Results show the robustness of the approach and the competitive results motivate the adoption in multi-turn QA scenarios.	Multi-turn QA: A RNN Contextual Approach to Intent Classification for Goal-oriented Systems	NA:NA:NA	2018
Brigitte Grau:Anne-Laure Ligozat	Question answering has been the focus of a lot of researches and evaluation campaigns, either for text-based systems (TREC and CLEF evaluation campaigns for example), or for knowledge-based systems (QALD, BioASQ). Few systems have effectively combined both types of resources and methods in order to exploit the fruitfulness of merging the two kinds of information repositories. The only evaluation QA track that focuses on hybrid QA is QALD since 2014. As it is a recent task, few annotated data are available (around 150 questions). In this paper, we present a question answering dataset that was constructed to develop and evaluate hybrid question answering systems. In order to create this corpus, we collected several textual corpora and augmented them with entities and relations of a knowledge base by retrieving paths in the knowledge base which allow to answer the questions. The resulting corpus contains 4300 question-answer pairs and 1600 have a true link with DBpedia.	A Corpus for Hybrid Question Answering Systems	NA:NA	2018
Dennis Diefenbach:Kamal Singh:Pierre Maret	In the last two decades a new part of the web grew significantly, namely the Semantic Web. It contains many Knowledge Bases (KB) about different areas like music, books, publications, live science and many more. Question Answering (QA) over KBs is seen as the most promising approach to bring this data to end-users. We describe WDAqua-core1, a QA service for querying RDF knowledge-bases. It is multilingual, it supports different RDF knowledge bases and it understands both full natural language questions and keyword questions.	WDAqua-core1: A Question Answering service for RDF Knowledge Bases	NA:NA:NA	2018
Sanjay Kamath:Brigitte Grau:Yue Ma	Extractive Question Answering (QA) focuses on extracting precise answers from a given paragraph to questions posed in natural language. Deep learning models are widely used to address this problem and can fetch good results, provided there exists enough data for learning. Such large datasets have been released in open domain, but not in specific domains, such as the medical domain. However, the medical domain has a great amount of resources such as UMLS thesaurus, ontologies such as SNOMED CT, and tools such as Metamap etc that could be useful. In this paper, we apply transfer learning for getting a DNN baseline system on biomedical questions and we study if structured resources can help in selecting the answers based on the recognition of the Expected Answer Type (EAT), which has been proved useful in open domain QA systems. This study relies on different representations for LAT and we study if gold standard answers and answers of our model have some positive impact from the LAT.	Verification of the Expected Answer Type for Biomedical Question Answering	NA:NA:NA	2018
Lora Aroyo:Gianluca Demartini:Anna Lisa Gentile:Chris Welty	It is our great pleasure to welcome you to the WWW 2018 Augmenting Intelligence with Humans-in-the-loop ([email protected]WWW2018), http://w3id.org/huml/HumL-WWW2018/ The workshop program includes two invited talks. Praveen Paritosh (Google Research) explores the right incentives to motivate human contribution to create knowledge resources. Elena Simperl (University of Southampton) surveys how humans and bots contribute together to the development of the Wikidata knowledge graph. Seven full papers and one short paper were accepted, covering a wide range of topics related to the efficient and effective combination of the strong sides of both machine and crowd computation. Empirical results were provided and discussed with respect to (1) methods for data quality ensurance and labeling task efficiency, (2) the role of gamification elements for improving crowd performance as well as the role of quantum mathematics to simulate human behavior.	Augmenting Intelligence with Humans-in-the-Loop ([email protected]) Chairs' Welcome & Organization	NA:NA:NA:NA	2018
Lora Aroyo:Chris Welty	AI and collective intelligence systems universally suffer from a deficiency of context. There are innumerable possible contexts that may possibly change the interpretation of some signal, that may change the proper response to some stimulus. For example, an image understanding system that does not recognize an arrest event in a zoomed image of a person's face. How is it possible to know there is more information, outside of what the system can access, that affects the interpretation of data The solution to the context problem in practice today is a pragmatic, engineering one: analyze errors (in recommendations, question answers, image recognition, etc.), classify the kinds of contextual information that caused the wrong behavior, find the most common type of context that causes errors, and add information about that kind of context to the system. Clearly this approach is neither general nor scalable, and ignores the infamous long tail of possible contextual information that may affect a system's understanding and its behavior. In this paper we outline a new, more general, approach to recognizing context. The approach is grounded in a fairly simple intuition: the mathematics underlying quantum mechanics is far more appropriate for modeling, and therefore simulating, human cognitive behavior than the standard toolset from classical statistics. Notions such as Heisenberg's uncertainty principle, superpositions of states, and entanglement have direct and measurable analogs in collective intelligence.	The Quantum Collective	NA:NA	2018
Praveen Paritosh	Dictionaries, encyclopedias, knowledge graphs, annotated corpora, library classification systems and world maps are all examples of human-curated knowledge resources that have been highly valuable to science as well as amortized across multiple large-scale systems in practice. Many of these were started and built even before a crowdsourcing research community existed. While the last decade has seen unprecedented growth in research and practice in building crowdsourcing systems to do increasingly complex tasks at scale, many of these resources are still woefully incompletelacking coverage in languages and subject matter domains. Moreover, many knowledge resources needed to fill other semantic gaps for artificial intelligence systems simply don't exist or arent being built. Why I argue that we don't have the right incentives, and that in order to improve the incentives, we have some fundamental scientific questions to answer. While building a large knowledge resource, we have little more than intuitions when it comes to estimating the reusability, maintainability, and long-term value of the effort. These make it difficult to fund or manage such projects, often requiring herculean personalities or fortunate businesses. Building or expanding a resource is often not seen as "sexy," which results in lack of resources to answer those questions in any principled manner. These problems begin to outline a new science of curation, making progress on which could help improve the discussion around and funding for building sorely needed knowledge resources.	The Missing Science of Knowledge Curation: Improving Incentives for Large-scale Knowledge Curation	NA	2018
Elena Simperl	Wikidata is one of most successful knowledge graphs ever created. It expresses knowledge in the form of subject-property-value statements accompanied by provenance information. A project of the Wikimedia Foundation, Wikidata is supported by a community of currently 19 thousand active users and 234 bots, who together are responsible for editing more than 45 million entities since the start of the project in 2012. This makes Wikidata a prime example for what human-in-the-loop technology can achieve. In this talk, we are going to present several studies that aim to understand the links between its socio-technical fabric and its success.	Loops of Humans and Bots in Wikidata	NA	2018
Amrapali Zaveri:Pedro Hernandez Serrano:Manisha Desai:Michel Dumontier	Crowdsourcing involves the creating of HITs (Human Intelligent Tasks), submitting them to a crowdsourcing platform and providing a monetary reward for each HIT. One of the advantages of using crowdsourcing is that the tasks can be highly parallelized, that is, the work is performed by a high number of workers in a decentralized setting. The design also offers a means to cross-check the accuracy of the answers by assigning each task to more than one person and thus relying on majority consensus as well as reward the workers according to their performance and productivity. Since each worker is paid per task, the costs can significantly increase, irrespective of the overall accuracy of the results. Thus, one important question when designing such crowdsourcing tasks that arise is how many workers to employ and how many tasks to assign to each worker when dealing with large amounts of tasks. That is, the main research questions we aim to answer is: 'Can we a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks'. Thus, we introduce a two-staged statistical guideline, CrowdED, for optimal crowdsourcing experimental design in order to a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks. We describe the algorithm and present preliminary results and discussions. We implement the algorithm in Python and make it openly available on Github, provide a Jupyter Notebook and a R Shiny app for users to re-use, interact and apply in their own crowdsourcing experiments.	CrowdED: Guideline for Optimal Crowdsourcing Experimental Design	NA:NA:NA:NA	2018
Alexandros Chortaras:Anna Christaki:Nasos Drosopoulos:Eirini Kaldeli:Maria Ralli:Anastasia Sofou:Arne Stabenau:Giorgos Stamou:Vassilis Tzouvaras	The transformation that has been accomplished in Cultural Heritage (CH) during the last decades has resulted in the production of vast amounts of content from many different cultural institutions, such as museums, libraries and archives. A large part of this rich content has been aggregated in digital platforms that serve as cross-domain hubs, which however offer limited usability and accessibility of content due to insufficient data and metadata quality. In our effort to make CH more accessible and reusable, we introduce WITH, an aggregation platform that provides enhanced services and enables human-computer collaboration for data annotations and enrichment. WITH excels existing cultural content aggregation platforms by advancing digital cultural data through the combination of artificial intelligence automation and creative user engagement, thus facilitating its accessibility, visibility, and re-use. In particular, by using image and free text analysis methodologies for automatic metadata enrichment, in accordance to the human expertise for enrichment and validation through crowdsourcing approaches with gamification elements, WITH combines the intelligence of humans and computers to improve the quality of digital cultural content and its presentation, establishing new ways of collaboration between cultural organizations and their audiences.	WITH: Human-Computer Collaboration for Data Annotation and Enrichment	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Rafael Zequeira Jiménez:Laura Fernández Gallardo:Sebastian Möller	Crowdsourcing provides an exceptional opportunity for the rapid collection of human input for data acquisition and labelling. This approach have been adopted in multiple domains and researchers are now able to reach a demographically diverse audience at low cost. However, it remains the question of whether the results are still valid and reliable. Previous work have introduced different mechanisms to ensure data reliability in crowdsourcing. This work examines to which extend, "trapping question" or "outliers detection" assure reliable results to the detriment of, overloading task content with stimuli that are not of interest for the researcher, or by discarding data points that might be the true opinion of a worker. To this end, a speech quality assessment study have been conducted in a web crowdsourcing platform, following the ITU-T Rec. P.800. Workers assessed the speech stimuli of the database 501 from the ITU-T Rec. P.863. We examine results' validity in terms of correlations to previous ratings collected in laboratory. Our outcomes shows that neither of the techniques under investigation improve results accuracy by itself, but a combination of both. Our goal is to provide empirical guidance for designing experiments in crowdsourcing while ensuring data reliability.	Outliers Detection vs. Control Questions to Ensure Reliable Results in Crowdsourcing.: A Speech Quality Assessment Case Study	NA:NA:NA	2018
Ismini Lourentzou:Daniel Gruhl:Steve Welch	Domain-specific relation extraction requires training data for supervised learning models, and thus, significant labeling effort. Distant supervision is often leveraged for creating large annotated corpora however these methods require handling the inherent noise. On the other hand, active learning approaches can reduce the annotation cost by selecting the most beneficial examples to label in order to learn a good model. The choice of examples can be performed sequentially, i.e. select one example in each iteration, or in batches, i.e. select a set of examples in each iteration. The optimization of the batch size is a practical problem faced in every real-world application of active learning, however it is often treated as a parameter decided in advance. In this work, we study the trade-off between model performance, the number of requested labels in a batch and the time spent in each round for real-time, domain specific relation extraction. Our results show that the use of an appropriate batch size produces competitive performance, even compared to a fully sequential strategy, while reducing the training time dramatically.	Exploring the Efficiency of Batch Active Learning for Human-in-the-Loop Relation Extraction	NA:NA:NA	2018
Giorgio Maria Di Nunzio:Maria Maistro:Federica Vezzani	Supervised machine learning algorithms require a set of labelled examples to be trained; however, the labelling process is a costly and time consuming task which is carried out by experts of the domain who label the dataset by means of an iterative process to filter out non-relevant objects of the dataset. In this paper, we describe a set of experiments that use gamification techniques to transform this labelling task into an interactive learning process where users can cooperate in order to achieve a common goal. To this end, first we use a geometrical interpretation of Naïve Bayes (NB) classifiers in order to create an intuitive visualization of the current state of the system and let the user change some of the parameters directly as part of a game. We apply this visualization technique to the classification of newswire and we report the results of the experiments conducted with different groups of people: PhD students, Master Degree students and general public. Then, we present a preliminary experiment of query rewriting for systematic reviews in a medical scenario, which makes use of gamification techniques to collect different formulation of the same query. Both the experiments show how the exploitation of gamification approaches help to engage the users in abstract tasks that might be hard to understand and/or boring to perform.	A Gamified Approach to Naïve Bayes Classification: A Case Study for Newswires and Systematic Medical Reviews	NA:NA:NA	2018
Roberto Enea:Maria Teresa Pazienza:Andrea Turbati:Alessandro Colantonio	Creating ontologies is an essential while challenging task to be performed by either a human or a system: on one hand it is excessively burdensome for a human operator, on the other it is very complex also for a machine due to the not negligible amount of "uncertainty" that it must be able to manage. In the last years, some attempts have been made to automate this process, but at present, due to the large number of aspects to be covered in the automatic creation of an ontology (such as Domain terminology extraction, Concept discovery, Concept hierarchy derivation, ") satisfactory solutions have not been reached yet. In order to produce efficient tools for both creation and enrichment of ontologies, the participation of the human in such a process still seems necessary. Our approach, that foresees a broader framework for ontology learning, is based by first on the automatic extraction of triples from heterogeneous sources, then on the presentation of the most reliable triples to the human operator for validation purposes. The system provides the user with a series of graphical representations that can give him an overview of the level of uncertainty of the automatically generated ontology. Then provides the user with the possibility to perform SPARQL what-if queries, (i.e. assuming as true the triples filtered according to the level of confidence, the source and the structure of the triples).	How to Support Human Operator in "Uncertainty" Managing during the Ontology Learning Process	NA:NA:NA:NA	2018
Wei Sun:Ying Li:Anshul Sheopuri:Thales Teixeira	Making successful video advertisements has long been considered a combination of art and business acumen. In this work, we propose a system to assist human designers to produce more effective advertisements with predictable outcomes. We formalize this concept with a dynamic Bayesian network (DBN), where we represent the knowledge base with data collected from large-scale field experiments in a novel setting. Face and eye tracking which continuously measures viewers emotional responses and viewing interest on 169 television advertisements for 2334 participants, along with moment-to-moment branding activities in the advertisements are used to estimate the model. The resulting DBN represents relationships across advertisement content, viewers emotional responses, as well as effectiveness metrics such as ad avoidance, sharing and influence on purchase. Conditioned on the specified requirement on the ad, a human designer can draw high scoring samples from the DBN, which represent the optimized sequences of branding activities and entertainment content.	Computational Creative Advertisements	NA:NA:NA:NA	2018
Luis-Daniel Ibáñez:John Domingue:Pascal Molli	It is our great pleasure to welcome you to the WWW 2018 3rd International Workshop on Linked Data and Distributed Ledgers (LD-DL). We envision the workshop as a forum for researchers and practitioners from Distributed Ledgers and Linked Data to come together to discuss common challenges; propose solutions to shortcomings of existing architectures; and identify synergies for joint initiatives. The ultimate goal is the creation of a Web of Interoperable Ledgers. We received 6 submissions from all around the world. We evaluated them regarding relevance, quality, and novelty, selecting 3 short papers and 1 long paper (66% acceptance rate) --ScienceMiles - Digital currency for researchers--Can Blockchains and Linked Data Advance Taxation? --A distributed database with explicit semantics and chained RDF graphs--When trust saves energy: A Reference Framework for Proof of Trust (PoT) Blockchains. We hope that you will find the tutorial program interesting, providing you with a valuable opportunity to learn and share ideas with other researchers and practitioners from institutions around the world.	3rd International Workshop on Linked Data and Distributed Ledgers Chairs' Welcome & Organization	NA:NA:NA	2018
Leila Bahri:Sarunas Girdzijauskas	Blockchains are attracting the attention of many technical, financial, and industrial parties, as a promising infrastructure for achieving secure peer-to-peer (P2P) transactional systems. At the heart of blockchains is proof-of-work (PoW), a trustless leader election mechanism based on demonstration of computational power. PoW provides blockchain security in trusless P2P environments, but comes at the expense of wasting huge amounts of energy. In this research work, we question this energy expenditure of PoW under blockchain use cases where some form of trust exists between the peers. We propose a Proof-of-Trust (PoT) blockchain where peer trust is valuated in the network based on a trust graph that emerges in a decentralized fashion and that is encoded in and managed by the blockchain itself. This trust is then used as a waiver for the difficulty of PoW; that is, the more trust you prove in the network, the less work you do.	When Trust Saves Energy: A Reference Framework for Proof of Trust (PoT) Blockchains	NA:NA	2018
Mirek Sopek:Przemyslaw Gradzki:Witold Kosowski:Dominik Kuziski:Rafa Trójczak:Robert Trypuz	In this paper we present a new idea of creating a Blockchain compliant distributed database which exposes its data with explicit semantics, is easily and natively accessible, and which applies Blockchain securitization mechanisms to the RDF graph data model directly, without additional packaging or specific serialisation. Essentially, the resulting database forms the linked chain of named RDF graphs and is given a name: GraphChain. Such graphs can then be published with the help of any standard mechanisms using triplestores or as linked data objects accessible via standard web mechanisms using the HTTP protocol to make them available on the web. They can also be easily queried using techniques like SPARQL or methods typical to available RDF graphs frameworks (like rdflib, Apache Jena, RDF4J, OWL API, RDF HDT, dotnetRDF and others). The GraphChain concept comes with its own, OWL-compliant ontology that defines all the structural, invariant elements of the GraphChain and defines their basic semantics. The paper describes also a few simple, prototypical GraphChain implementations with examples created using Java, .NET/C# and JavaScript/Node.js frameworks.	GraphChain: A Distributed Database with Explicit Semantics and Chained RDF Graphs	NA:NA:NA:NA:NA:NA	2018
Michał R. Hoffman	Permissioned distributed ledgers (permissioned blockchains) supporting smart contracts that automatically adjust accounts and coordinate records among multiple parties, present a valid platform opportunity for establishing a fully digital tax regime. We propose a permissioned blockchain-based system aimed at eliminating some of the losses that tax authorities globally are currently struggling with. These multi-billion flaws manifest themselves as the tax gap, or the inability to collect the full amount that is owed by a given entity to a particular authority. Illegitimate or inefficient tax operations could be prevented with a global suite of smart contracts deployed on top of a consortium distributed ledger with on-chain governance. We also introduce the vision for a VAT Invoice 2.0 modelled as a Linked Data document. A tax reference generated by a smart contract would allow anyone with the right permissions to immediately investigate the entire commercial chain for any taxable item on an ontology-based tax document.	Can Blockchains and Linked Data Advance Taxation	NA	2018
Zeeshan Jan:Allan Third:Luis-Daniel Ibanez:Michelle Bachler:Elena Simperl:John Domingue	Peer-reviewing is a community-driven activity where volunteer researchers assess the work of other researchers. Peer-reviewing is an important and time-consuming activity that has very little recognition. This lack of incentive may lead to poor-quality reviews and frustration from researchers. In this paper, we envision ScienceMiles, a Blockchain-based platform to manage the incentivization of peer-reviewers through a crypto-currency.	ScienceMiles: Digital Currency for Researchers	NA:NA:NA:NA:NA:NA	2018
Dirk Ahlers:Erik Wilde:Rossano Schifanella:Jalal S. Alowibdi:Muhammad Zubair Shafiq	It is our great pleasure to welcome you to the 8th International Workshop on Location and the Web (LocWeb2018) at WWW 2018. LocWeb 2018 will continue a successful workshop series at the intersection of location-based services and Web architecture. It focuses on Web-scale services and systems facilitating location-aware information access as well as on Spatial Social Behavior Analytics on the Web as part of social computing. The location topic is seen as a cross-cutting issue equally concerning information access, semantics and standards, social analysis and mining, and Web-scale systems and services. The workshop is an integrated venue where location and spatio-social aspects can be discussed in depth with an interested community. New application areas for Web architecture, such as the Internet of Things (IoT) and the Web of Things (WoT), will lead to increasingly rich and large sets of applications for which location is highly relevant as the connection to the physical world. Location has high importance in Web-based designs, and it continues to provide challenging research questions.	LocWeb2018 Chairs' Welcome & Organization	NA:NA:NA:NA:NA	2018
Luca Rossi:Eric Boscaro:Andrea Torsello	The last decade has seen a huge expansion in the use of social media to extract data about human behaviour. While metadata and textual information have taken the lion's share as data sources for social media analysis, geotagged image-based platforms represent an unprecedented and as yet almost untapped source of data to analyse human behaviour and characterise the physical space we live in. In this paper we investigate the use of Instagram photos to analyse tourism consumption. We take the city of Venice (Italy) as a case study and we collect a dataset of about 90k photos taken between January 2014 and December 2015. Using computer vision techniques, we build a supervised classifier which assigns each photo to one of six different categories. We then observe how the frequency and spatial distribution of these categories varies with time. This in turn allows us to confirm the existence of a number of touristic hotspots associated with different events, such as Venice Carnival and Biennale. Our analysis also uncovers the existence of touristic flows associated with these events, such as the Folklore Line that marks the path of tourists from "Santa Lucia" railway station to "San Marco" square during the Carnival period. Overall, our findings confirm the effectiveness of the proposed framework to investigate tourism consumption using Instagram data.	Venice through the Lens of Instagram: A Visual Narrative of Tourism in Venice	NA:NA:NA	2018
Kendall Taylor:Kwan Hui Lim:Jeffrey Chan	Travelling and touring are popular leisure activities enjoyed by millions of tourists around the world. However, the task of travel itinerary recommendation and planning is tedious and challenging for tourists, who are often unfamiliar with the various Points-of-Interest (POIs) in a city. Apart from identifying popular POIs, the tourist needs to construct a travel itinerary comprising a subset of these POIs, and to order these POIs as a sequence of visits that can be completed within his/her available touring time. For a more realistic itinerary, the tourist also has to account for travelling time between POIs and visiting times at individual POIs. Furthermore, this itinerary should incorporate tourist preferences such as desired starting and ending POIs (e.g., POIs that are near the tourist's hotel) and a subset of must-see POIs (e.g., popular POIs that a tourist must visit). We term this the TourMustSee problem, which is based on a variant of the Orienteering problem. Following which, we propose the LP+M algorithm for solving the TourMustSee problem as an Integer Linear Program (ILP). Using a Flickr dataset of POI visits in seven touristic cities, we compare LP+M against various ILP-based baselines, and the results show that LP+M recommends better travel itineraries in terms of POI popularity, total POIs visited, total touring time utilized and must-visit POI(s) inclusion.	Travel Itinerary Recommendations with Must-see Points-of-Interest	NA:NA:NA	2018
Antonio La Salandra:Piero Fraternali:Darian Frajberg	Location-based mobile outdoor applications are powerful tools that can engage users in social and environmental tasks and support the emerging paradigm of citizen science. In this paper we present PeakLensVR, a virtual reality location-based mobile app that enables users to capture with their mobile phone panoramic mountain images and later visualize such images, enriched with metadata about the peaks visible from the capture point, with a low-end VR device. The goal of PeakLensVR is to harness the emerging trend of geo-located augmented and virtual reality applications to foster a community of environmentally conscious users who volunteer in the collection of mountain images for environment monitoring purposes.	A Location-Based Virtual Reality Application for Mountain Peak Detection	NA:NA:NA	2018
Guido Boella:Louise Francis:Elena Grassi:Axel Kistner:Andreas Nitsche:Alexey Noskov:Luigi Sanasi:Adriano Savoca:Claudio Schifanella:Ioannis Tsampoulatidis	In this paper we describe the advancement of WeGovNow, an Horizon 2020 European Union project involving twelve partners from Germany, Sweden, Greece, Italy and United Kingdom, aimed at using state-of-the-art digital technologies in community engagement platforms to involve citizens in decision making processes within their local neighbourhood. Different software components, both previously existing and developed specially for the project and covering separate aspects of community engagement, were integrated in a single web platform offering an homogeneous experience to the users. One of the main common threads beyond this integration process is the ability to collect crowd mapped information and show them back to the users in an engaging way on maps, harmonizing data coming from the different components and making the mapped space easily explorable.	WeGovNow: A Map Based Platform to Engage the Local Civic Society	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Eduardo Graells-Garrido:Diego Caro:Omar Miranda:Rossano Schifanella:Oscar F. Peredo	People fulfill their informational needs through smartphones, however, little is known regarding how the urban fabric and the activities that take place in it affect the usage of mobile applications. In this regard, starting from an anonymized dataset of Deep Packet Inspection (DPI) data from the largest telecommunications operator in Chile, we focus on the following questions: What are the most popular applications used in the city Where are they spatially clustered When does an application is more frequently used And How does the urban context and the mobility patterns relate to application usage As a result, we observed that specific applications present high spatial clustering, while the most popular services are geographically dispersed throughout the entire city. Clusters appear in places of high floating population; however, hotspots vary in space depending on the application. Interestingly, we found that commuting plays an important role, both in terms of rush hours and transportation infrastructure. We present a discussion on these results, focusing on how the physical space and the daily commuting routine affect the pattern of data consumption and represent an important aspect in mobile users behavioral studies.	The WWW (and an H) of Mobile Application Usage in the City: The What, Where, When, and How	NA:NA:NA:NA:NA	2018
Martin Atzmueller:Sabrina Gaito:Roberto Interdonato:Rushed Kanawati:Christine Largeron:Matteo Magnani:Alessandra Sala	Attributed network models have seen an increasing success in recent years, thanks to their informative power and to their ability to model complex networked relations that characterize most real-world phenomena. Their use has been attractive to communities in different disciplines such as computer science, physics, social science, as well as in interdisciplinary research environments. The use of such models has been also supported by the increasing easiness in collecting multirelational data from the Web, e.g., from online social media platforms, crowdsourced data, online knowledge bases; within this view, the World Wide Web is an inestimable source of information, which can be conveniently represented with feature-rich network models, e.g., enclosing temporal aspects of the data, quantitative and/or qualitative properties of nodes, different relations between a common set of entities, different existence probabilities, or modeling connection between different entity types.	International Workshop on Mining Attributed Networks (MATNET 2018) Chairs' Welcome	NA:NA:NA:NA:NA:NA:NA	2018
Amani H. B. Eissa:Mohamed E. El-Sharkawi:Hoda M. O. Mokhtar	Social networks can be modeled as attributed networks whose nodes represent users, edges represent relationships among users (e.g. friendship/follow) and attribute vectors hold properties of nodes and/or edges. In this paper, we consider friends' recommendation based on interest-based communities generated from topic based attributed social networks (TbASN). In our model, an attribute vector is not just a container for explicit users' profile data that is stored in social network's dataset, but rather holds topic vectors that are derived from analyzing the implicit interest of users' that are aggregated from his/her posts on the social network (e.g. tweets in Twitter, posts in Facebook). In our framework, topics of interest are represented as a hierarchy of topics (Topics/Subtopics) forming hierarchical interest-based communities. Users within each interest-based community are clustered according to their profile features (age, location, education etc.). Those clusters are later used in recommendations where recommendations target members of the same cluster to guarantee the quality and coherence of recommendations. In addition, we propose a recommendation selection approach to handle the large number of recommended candidates. The main advantage of the proposed approach is that it considers multiple criteria for candidate selection including the number of common communities, the resemblance in basic features, as well as network proximity. In addition to recommending friends of similar interests, frequent pattern mining is used to discover frequently occurring interests in order to be used in recommending communities for users to join. Although our approach is generic and can be applied to most of the existing social networks, we used Twitter as our target social network.	Towards Recommendation Using Interest-Based Communities in Attributed Social Networks	NA:NA:NA	2018
Jihwan Lee:Sunil Prabhakar	Network embedding aims to learn low-dimensional vector representations for nodes in a network that preserve structural characteristics. It has been shown that such representations are helpful in several graph mining tasks such as node classification, link prediction, and community detection. Some recent works have attempted to extend the approach to attributed networks in which each node is associated with a set of attribute values. They have focused on homophily relationships by forcing nodes with similar attribute values to obtain similar vector representations. This is unnecessarily restrictive and misses the opportunity to harness other types of relationships revealed by patterns in attribute values of connected nodes for learning insightful relationships. In this paper, we propose a new network attributed embedding framework called A3embed that is aware of attribute associations. A3embed favors significant attribute associations, not merely homophily relationships, which contributes to its robustness to diverse attribute vectors and noisy links. The experimental results on real-world datasets demonstrate that the proposed framework achieves better performance on different graph mining tasks compared to existing models.	A3embed: Attribute Association Aware Network Embedding	NA:NA	2018
Rajesh Sharma:Danilo Montesi	One of the important problems in the domain of network science is the community detection. In the past, various topological based community detection algorithms have been proposed. Recently, researchers have taken into account at- tributes of the nodes while proposing community detection algorithms. In this work, we investigate if the nodes in a community, identified through topology based algorithms al- so exhibit attribute similarity. Using four different kinds of similarity metrics, we analyse the attribute similarity of the nodes within the communities derived using five different types of topological based community detection algorithms. Based on our analysis of three real social network datasets, we found on an average of 50% attribute similarity among the nodes in the communities.	Investigating Similarity of Nodes' Attributes in Topological Based Communities.	NA:NA	2018
Ryuta Matsuno:Tsuyoshi Murata	Network embedding is a method for converting nodes in a network into low dimensional vectors, preserving its structure and the similarities among the nodes. Embedding is widely used in many applications, e.g., social network analysis and knowledge discovery. Because of its wide usage, many studies have been proposed, such as DeepWalk, LINE and node2vec. These works are designed for single-layer networks, however, real world networks often possess not just one, but multiple types of connections. Hence it is more appropriate to represent them as multiplex networks, which consist of multiple layers each of which represents one type of relationship. Embedding multiplex networks is difficult because all layer structures have to be taken into consideration. In this paper, we propose MELL, a novel embedding method for multiplex networks, which incorporates an idea of layer vector that captures and characterizes each layer's connectivity. This method exploits the overall structure effectively, and embeds both directed and undirected multiplex networks, whether their layer structures are similar or complementary. We focus on link prediction tasks and test our method and other baseline methods using five data sets from different domains. The results show that our method outperforms all of the baseline methods for all of the data sets.	MELL: Effective Embedding Method for Multiplex Networks	NA:NA	2018
Manish Kumar:Anurag Singh:Hocine Cherifi	When an epidemic occurs, it is often impossible to vaccinate the entire population due to limited amount of resources. Therefore, it is of prime interest to identify the set of influential spreaders to immunize, in order to minimize both the cost of vaccine resource and the disease spreading. While various strategies based on the network topology have been introduced, few works consider the influence of the community structure in the epidemic spreading process. Nowadays, it is clear that many real-world networks exhibit an overlapping community structure, in which nodes are allowed to belong to more than one community. Previous work shows that the numbers of communities to which a node belongs is a good measure of its epidemic influence. In this work, we address the effect of nodes in the neighborhood of the overlapping nodes on epidemics spreading. The proposed immunization strategy provides highly connected neighbors of overlapping nodes in the network to immunize. The whole process requires information only at the node level and is well suited to large-scale networks. Extensive experiments on four real-world networks of diverse nature have been performed. Comparisons with alternative local immunization strategies using the fraction of the Largest Connected Component (LCC) after immunization,show that the proposed method is much more efficient. Additionally, it compares favorably to global measures such as degree and betweenness centrality.	An Efficient Immunization Strategy Using Overlapping Nodes and Its Neighborhoods	NA:NA:NA	2018
Lisette Espín-Noboa:Claudia Wagner:Fariba Karimi:Kristina Lerman	Relational inference leverages relationships between entities and links in a network to infer information about the network from a small sample. This method is often used when global information about the network is not available or difficult to obtain. However, how reliable is inference from a small labeled sample How should the network be sampled, and what effect does it have on inference error How does the structure of the network impact the sampling strategy We address these questions by systematically examining how network sampling strategy and sample size affect accuracy of relational inference in networks. To this end, we generate a family of synthetic networks where nodes have a binary attribute and a tunable level of homophily. As expected, we find that in heterophilic networks, we can obtain good accuracy when only small samples of the network are initially labeled, regardless of the sampling strategy. Surprisingly, this is not the case for homophilic networks, and sampling strategies that work well in heterophilic networks lead to large inference errors. This finding suggests that the impact of network structure on relational classification is more complex than previously thought.	Towards Quantifying Sampling Bias in Network Inference	NA:NA:NA:NA	2018
Henry Soldano:Guillaume Santini:Dominique Bouthinon:Sophie Bary:Emmanuel Lazega	In two-mode networks there are two kinds of vertices, i.e objects, each being possibly described with a proper attribute set. This means that to select a subnetwork according to vertex descriptions we have to consider a pair of vertex subsets. A common technique is to extract from a network an essential subnetwork, the core subgraph of the network. Formal Concept Analysis and closed pattern mining were previously applied to networks with the purpose of reducing extensions of patterns to be core subgraphs. To apply this methodology to two-mode networks, we need to consider the two vertex subsets of two-mode cores and define accordingly abstract closed bi-patterns. Each component of a bi-pattern is then associated to one mode. We also show that the same methodology applies to hub-authority cores of directed networks in which each vertex subset is associated to a role (in or out). We illustrate the methodology both on a two-mode network of epistemological data and on a directed advice network of lawyers.	Bi-Pattern Mining of Two Mode and Directed Networks	NA:NA:NA:NA:NA	2018
Didier Henry:Erick Stattner:Martine Collard	Today, social media are one of the fastest ways to have access to information related to several topics. Indeed, a diffused information on these supports can travel thousands of kilometres in only few seconds contrary to an article posted on a news site. Despite the fact that a large variety of studies have been conducted to understand how fast and how scale information spreads in social media, we observe that they have not yet been interested in the geographical aspect. In this paper, we perform a geographical and temporal analysis of Twitter trends spread between May and June 2017. We introduce interesting patterns which deal with the paths taken by information between countries. In addition, we observe relevant results by taking into account the topic. Finally, we conclude and give perspectives of research of this work.	Information Propagation Routes between Countries in Social Media	NA:NA:NA	2018
Issam Falih:Nistor Grozavu:Rushed Kanawati:Younès Bennani	Graph clustering techniques are very useful for detecting densely connected groups in large graphs. Many existing graph clustering methods mainly focus on the topological structure, but ignore the vertex properties. Existing graph clustering methods have been recently extended to deal with nodes attribute. First we motivate the interest in the study of this issue. Then we review the main approaches proposed to deal with this problem. We propose a comparative study of some existing attributed network community detection algorithm on both synthetic data and on real world data.	Community detection in Attributed Network	NA:NA:NA:NA	2018
Martin Atzmueller:Alvin Chin:Christoph Trattner	Bienvenue! It is our great pleasure to welcome you to the WWW 2018 International Workshop on Modeling Social Media (MSM'2018) - Applying Machine Learning and AI for Modeling Social Media. This is our 9th edition of our workshop. Social networks such as Facebook, Twitter, and LinkedIn have paved the way for generating huge amount of diverse, streaming bit data in a short period of time. Such social media data require the application of big data analytics to produce meaningful information to both information consumers and data generators. Machine learning and AI techniques are particularly effective in situations where deep and predictive insights need to be uncovered from such social media data sets that are large, diverse and fast changing. The workshop aims to address machine learning and AI methods, frameworks, algorithms, and the applications and evaluation of these approaches on social media, big data and the web. We received 11 papers from all around the world covering a broad range of topics, and we accepted 8 papers resulting in a 72% acceptance rate. We evaluated them regarding relevance, quality, and novelty, selecting 4 full papers and 4 short papers. Each paper was reviewed by 3 reviewers and then decisions were made from the reviews and the workshop chairs.	International Workshop on Modeling Social Media (MSM 2018) Chairs' Welcome & Organization	NA:NA:NA	2018
Hong Wei:Hao Zhou:Jangan Sankaranarayanan:Sudipta Sengupta:Hanan Samet	The tweet count prediction of a local spatial region is to forecast the number of tweets that are likely to be posted from that area over a relatively short period of time. It has many applications such as human mobility analysis, traffic planning, and abnormal event detection. In this paper, we formulate tweet count prediction as a spatiotemporal sequence forecasting problem and design an end-to-end convolutional LSTM based network with skip connection for this problem. Such a model enables us to exploit the unique properties of spatiotemporal data, consisting of not only the temporal characteristics such as temporal closeness, period and trend properties but also spatial dependencies. Our experiments on the city of Seattle, WA as well as a larger city of New York City show that the proposed method consistently outperforms the competitive baseline approaches.	Residual Convolutional LSTM for Tweet Count Prediction	NA:NA:NA:NA:NA	2018
Ashwini Tonge:Cornelia Caragea	Online image sharing in social networking sites such as Facebook, Flickr, and Instagram can lead to unwanted disclosure and privacy violations, when privacy settings are used inappropriately. Despite that social networking sites allow users to set their privacy preferences, this can be cumbersome for the vast majority of users. In this paper, we explore privacy prediction models for social media that can automatically identify private (or sensitive) content from images, before they are shared online, in order to help protect users' privacy in social media. More precisely, we study "deep" visual features that are extracted from various layers of a pre-trained deep Convolutional Neural Network (CNN) as well as "deep" image tags generated from the CNN. Experimental results on a Flickr dataset of thousands of images show that the deep visual features and deep image tags can successfully identify images' private content and substantially outperform previous models for this task.	On the Use of "Deep" Features for Online Image Sharing	NA:NA	2018
Yutaro Miura:Fujio Toriumi:Toshiharu Sugawara	We propose a model of a social networking service (SNS) with diminishing marginal utility in the framework of evolutionary computing and present our investigation on the effect of diminishing marginal utility on the dominant structure of strategies in all agents. SNSs such as Twitter and Facebook have been growing rapidly, but why they are prospering is unknown. SNSs have the characteristics of a public goods game because they are maintained by users posting many articles that incur some cost and because users can also be free riders, who just read articles. Thus, a number of studies aimed at understanding the conditions or mechanisms that keep social media thriving theoretically by introducing the meta-rewards game, which is a variation of a public goods game. The meta-rewards games assume constant marginal utility, meaning that the rewards by receiving comments increase linearly according to the number of comments, but describing the psychological rewards of humans is often inappropriate. In this paper, we present our modification of the model using the diminishing marginal utility and our comparison of the experimental results with those of the original meta-rewards game. We demonstrate that the structure of dominant strategies of all agents in our game is quite different from that in the original meta-rewards game and is more reasonable to explain the users' behavior in SNSs because their efforts in SNSs are limited even if they have many friends.	Evolutionary Learning Model of Social Networking Services with Diminishing Marginal Utility	NA:NA:NA	2018
Mehwish Nasim:Andrew Nguyen:Nick Lothian:Robert Cope:Lewis Mitchell	Content polluters, or bots that hijack a conversation for political or advertising purposes are a known problem for event prediction, election forecasting and when distinguishing real news from fake news in social media data. Identifying this type of bot is particularly challenging, with state-of-the-art methods utilising large volumes of network data as features for machine learning models. Such datasets are generally not readily available in typical applications which stream social media data for real-time event prediction. In this work we develop a methodology to detect content polluters in social media datasets that are streamed in real-time. Applying our method to the problem of civil unrest event prediction in Australia, we identify content polluters from individual tweets, without collecting social network or historical data from individual accounts. We identify some peculiar characteristics of these bots in our dataset and propose metrics for identification of such accounts. We then pose some research questions around this type of bot detection, including: how good Twitter is at detecting content polluters and how well state-of-the-art methods perform in detecting bots in our dataset.	Real-time Detection of Content Polluters in Partially Observable Twitter Networks	NA:NA:NA:NA:NA	2018
Vedant Nanda:Hemank Lamba:Divyansh Agarwal:Megha Arora:Niharika Sachdeva:Ponnurangam Kumaraguru	Selfies have become a prominent medium for self-portrayal on social media. Unfortunately, certain social media users go to extreme lengths to click selfies, which puts their lives at risk. Two hundred and sixteen individuals have died since March 2014 until January 2018 while trying to click selfies. It is imperative to be able to identify dangerous selfies posted on social media platforms to be able to build an intervention for users going to extreme lengths for clicking such selfies. In this work, we propose a convolutional neural network based classifier to identify dangerous selfies posted on social media using only the image (no metadata). We show that our proposed approach gives an accuracy of $98%$ and performs better than previous methods.	Stop the KillFies! Using Deep Learning Models to Identify Dangerous Selfies	NA:NA:NA:NA:NA:NA	2018
Javier Sanz-Cruzado:Sofía M. Pepa:Pablo Castells	Link prediction has mainly been addressed as an accuracy-targeting problem in the social networks field. We discuss different perspectives on the problem considering other dimensions and effects that the link prediction methods may have on the social network where they are applied. Specifically, we consider the structural effects the prediction can have if the predicted links are added to the network. We consider further utility dimensions beyond prediction accuracy, namely novelty and diversity. We discuss the adaptation, for this purpose, of specific network, novelty and diversity metrics from social network analysis, recommender systems, and information retrieval.	Structural Novelty and Diversity in Link Prediction	NA:NA:NA	2018
Gaurav Bhatt:Aman Sharma:Shivam Sharma:Ankush Nagpal:Balasubramanian Raman:Ankush Mittal	Identifying the veracity of a news article is an interesting problem while automating this process can be a challenging task. Detection of a news article as fake is still an open question as it is contingent on many factors which the current state-of-the-art models fail to incorporate. In this paper, we explore a subtask to fake news identification, and that is stance detection. Given a news article, the task is to determine the relevance of the body and its claim. We present a novel idea that combines the neural, statistical and external features to provide an efficient solution to this problem. We compute the neural embedding from the deep recurrent model, statistical features from the weighted n-gram bag-of-words model and handcrafted external features with the help of feature engineering heuristics. Finally, using deep neural layer all the features are combined, thereby classifying the headline-body news pair as agree, disagree, discuss, or unrelated. Through extensive experiments, we find that the proposed model outperforms all the state-of-the-art techniques including the submissions to the fake news challenge.	Combining Neural, Statistical and External Features for Fake News Stance Identification	NA:NA:NA:NA:NA:NA	2018
Marco Brambilla:Stefano Ceri:Florian Daniel:Marco Di Giovanni:Andrea Mauri:Giorgia Ramponi	Knowledge in the world continuously evolves, and ontologies are largely incomplete, especially regarding data belonging to the so-called long tail. We propose a method for discovering emerging knowledge by extracting it from social content. Once initialized by domain experts, the method is capable of finding relevant entities by means of a mixed syntactic-semantic method. The method uses seeds, i.e. prototypes of emerging entities provided by experts, for generating candidates; then, it associates candidates to feature vectors built by using terms occurring in their social content and ranks the candidates by using their distance from the centroid of seeds, returning the top candidates. Our method can run iteratively, using the results as new seeds. as new seeds. In this paper we address the following research questions: (1) How does the reconstructed domain knowledge evolve if the candidates of one extraction are recursively used as seeds (2) How does the reconstructed domain knowledge spread geographically (3) Can the method be used to inspect the past, present, and future of knowledge (4) Can the method be used to find emerging knowledge	Iterative Knowledge Extraction from Social Networks	NA:NA:NA:NA:NA:NA	2018
Alípio Jorge:João Vinagre:Pawel Matuszyk:Myra Spiliopoulou	It is our great pleasure to welcome you to the WWW 2018 Workshop on Online Recommender Systems and User Modeling (ORSUM). We have received eleven submissions covering highly relevant topics in the research related to recommender systems and user modeling. During this workshop its participants will have the opportunity to see eight presentations corresponding to the accepted papers and to discuss the recent advances on these topics both with authors and other researchers in the audience. The topics of the talks include, among others, location and news recommendation, local models for online recommendations, recommendations' diversity, page optimization, context-aware recommender systems, crowdsourcing and incremental matrix factorization methods.	ORSUM Chairs' Welcome & Organization	NA:NA:NA:NA	2018
Julien Subercaze:Christophe Gravier:Frederique Laforest	Real-time recommendation of Twitter users based on the content of their profiles is a very challenging task. Traditional IR methods such as TF-IDF fail to handle efficiently large datasets. In this paper we present a scalable approach that allows real time recommendation of users based on their tweets. Our model builds a graph of terms, driven by the fact that users sharing similar interests will share similar terms. We show how this model can be encoded as a compact binary footprint, that allows very fast comparison and ranking, taking full advantage of modern CPU architectures. We validate our approach through an empirical evaluation against the Apache Lucene's implementation of TF-IDF. We show that our approach is in average two hundred times faster than standard optimised implementation of TF-IDF with a precision of 58%. The work presented here has been published in The Web Intelligence Journal.	Real-time, Scalable, Content-based Twitter Users Recommendation	NA:NA:NA	2018
George Karypis	Recommender systems are designed to identify the items that a user will like or find useful based on the user's prior preferences and activities. These systems have become ubiquitous and are an essential tool for information filtering and (e-)commerce. Over the years, collaborative filtering, which derive these recommendations by leveraging past activities of groups of users, has emerged as the most prominent approach for solving this problem. This talk will present some of our recent work towards improving the performance of collaborative filtering-based recommender systems and understanding some of their fundamental limitations and characteristics. It will start by analyzing how the ratings that users provide to a set of items relate to their ratings of the set's individual items and, using these insights, will present rating prediction approaches that utilize distant supervision. It will then discuss extensions to approaches based on sparse linear and latent factor models that postulate that users' preferences are a combination of global and local preferences, which are shown to lead to better user modeling and as such improved prediction performance. Finally, the talk will conclude by discussing what can be accurately predicted by latent factor approaches and by analyzing the estimation error of sparse linear and latent factor models and how its characteristics impacts the performance of top N recommendation algorithms.	Recent Advances in Recommender Systems: Sets, Local Models, Coverage, and Errors	NA	2018
Eiman Aldahari:Vivek Shandilya:Sajjan Shiva	Crowdsourcing is an approach whereby employers call for workers online with different capabilities to process a task for monetary reward. With a vast amount of tasks posted every day, satisfying the workers, employers, and service providers who are the stakeholders of any crowdsourcing system is critical to its success. To achieve this, the system should address three objectives: (1) match the worker with suitable tasks that fit the worker's interests and skills and raise the worker's rewards and rating, (2) give the employer more acceptable solutions with lower cost and time and raise the employer's rating, and (3) raise the rate of accepted tasks, which will raise the aggregated commissions to the service provider and improve the average rating of the registered users (employers and workers) accordingly. For these objectives, we present a mechanism design that is capable of reaching holistic satisfaction using a multi-objective recommendation system. In contrast, all previous crowdsourcing recommendation systems are designed to address one stakeholder who could be either the worker or the employer. Moreover, our unique contribution is to consider each stakeholder to be self serving. Considering selfish behavior from every stakeholder, we provide a more qualified recommendation for each stakeholder.	Crowdsourcing Multi-Objective Recommendation System*	NA:NA:NA	2018
Michele Zanitti:Sokol Kosta:Jannick Sørensen	Recommender systems (RS) have seen widespread adoption across the Internet. However, by emphasizing personalization through the optimization of accuracy-focused metrics, over-personalization may emerge, with negative effects on the user experience. A countermeasure to the problem is to diversify recommendations. In this paper, we present a solution that addresses the problem in the context of a movie application domain. The solution enhances diversity on four related dimensions, namely global coverage, local coverage, novelty, and redundancy. The proposed solution is designed to diversify users profiles, modeled on categorical preferences, within the same group in the recommendation filtering. We evaluate our approach on the Movielens dataset and show that our algorithm yields better results compared to random selection distant neighbors and performs comparably to one of the current state of the art solutions.	A User-Centric Diversity by Design Recommender System for the Movie Application Domain	NA:NA:NA	2018
Leonardo Cella	Traditional collaborative filtering, and content-based approaches attempt to learn a static recommendation model in a batch fashion. These approaches are not suitable in highly dynamic recommendation scenarios, like news recommendation and computational advertisement. Due to this well-known limitation, in the last decade a lot of efforts have been spent over the study of online learning techniques. Currently, a lot of attention has been devoted to improvements on the theoretical guarantees, without caring too much about computational cost and memory footprint. However, in the era of big-data content features tend to be high-dimensional, which leads to a direct challenge for traditional on-line learning algorithms (e.g., multi-armed bandits) since these are mostly designed for low-dimensional feature spaces. In this work we face the aforementioned problem, investigating an approximated context-aware bandit learner. Our model takes into account the problem of finding the actual low-dimensional manifold spanned by data content-features. In particular, we propose to store the covariance matrix of the previously seen contexts in a compressed space, without losing too much in terms of recommendation quality. With this work we provide an overview over the main properties, describe the adopted techniques, and report on preliminary experimental results on a synthetic dataset. We also discuss a drawback of the proposed method that may appear in typical scenarios and suggest future research avenues.	Efficient Context-Aware Sequential Recommender System	NA	2018
Gabriele Sottocornola:Panagiotis Symeonidis:Markus Zanker	In the context of news recommendations, many time-aware approaches were proposed. These approaches have tried to capture the recency of news with respect to their short life span, by using either decaying weights on past articles or even forgetting them. However, most of these approaches have missed to consider sessions, which encapsulate inside them the articles that a user has interacted with in a short time period. In this paper, we provide news recommendations based on user sessions to reveal their short-term intentions. We also combine content-based with collaborative filtering to deal with the severe data sparsity problem that exists in our real-life data set. We have experimentally seen that the users' interests evolve over time and that our strategies can adapt fast to these changes.	Session-based News Recommendations	NA:NA:NA	2018
Jia Wang:Yungang Feng:Elham Naghizade:Lida Rashidi:Kwan Hui Lim:Kate Lee	Studying large, widely spread Twitter data has laid the foundation for many novel applications from predicting natural disasters and epidemics to understanding urban dynamics. Recent studies have focused on exploring people's emotional response to their urban environment, e.g., green spaces versus built up areas, through analysing the sentiment of tweets within that area. Since green spaces have the capacity to improve citizen's well-being, we developed a system that is capable of recommending green spaces to users. Our system is unique in the sense that the recommendations are tailored with regard to users' preferred activity as well as the degree of positive sentiments in each green space. We show that the incoming flow of tweets can be used to refine the recommendations over time. Furthermore, We implemented a web-based, user-friendly interface to solicit user inputs and display recommendation results.	Happiness is a Choice: Sentiment and Activity-Aware Location Recommendation	NA:NA:NA:NA:NA:NA	2018
Weiru Zhang:Chao Wei:Xiaonan Meng:Yi Hu:Hao Wang	Modern search engines present result pages composed of two most prominent types of information: sponsored and organic search results. The whole-page results must satisfy user's information inquiry while sponsored ad alongside the search results has become a key monetization strategy for the platform. Against the backdrop of this situation, a basic question has received comparatively little attention: how many ads are good enough to get higher user satisfaction and better monetization Most search engines always display a fixed number of ads or use heuristic rules to determine the number of ads. In this paper, we formulate the task of finding the best number of ads into a linear programming optimization problem, for which we propose a novel online algorithm to solve. We have conducted several offline experiments and tested our approach in Alibaba E-commerce platform. The experimental results show that the platform could achieve higher revenue and more clicks simultaneously by the proposed algorithm.	The Whole-Page Optimization via Dynamic Ad Allocation	NA:NA:NA:NA:NA	2018
Susan C. Anyosa:João Vinagre:Alípio M. Jorge	Recommender systems try to predict which items a user will prefer. Traditional models for recommendation only take into account the user-item interaction, usually expressed by explicit ratings. However, in these days, web services continuously generate auxiliary data from users and items that can be incorporated into the recommendation model to improve recommendations. In this work, we propose an incremental Matrix Co-factorization model with implicit user feedback, considering a real-world data-stream scenario. This model can be seen as an extension of the conventional Matrix Factorization that includes additional dimensions to be decomposed in the common latent factor space. We test our proposal against a baseline algorithm that relies exclusively on interaction data, using prequential evaluation. Our experimental results show a significant improvement in the accuracy of recommendations, after incorporating an additional dimension in three music domain datasets.	Incremental Matrix Co-factorization for Recommender Systems with Implicit Feedback	NA:NA:NA	2018
Marie Al-Ghossein:Talel Abdessalem:Anthony Barré	With the explosion of the volume of user-generated data, designing online recommender systems that learn from data streams has become essential. These systems rely on incremental learning that continuously update models as new observations arrive and they should be able to adapt to drifts in real-time. User preferences evolve over time and tracking their evolution is not an easy task. In addition to the low number of observations available per user, the preferences change at different moments and in different ways for each individual. In this paper, we propose a novel approach based on local models to address this problem. Local models are known for their ability to capture diverse preferences among user subsets. Our approach automatically detects the drift of preferences that leads a user to adopt a behavior closer to the users of another subset, and adjusts the models accordingly. Our experiments on real world datasets show promising results and prove the effectiveness of using local models to adapt to changes in user preferences.	Dynamic Local Models for Online Recommendation	NA:NA:NA	2018
Rémy Cazabet:Andrea Passarella:Giulio Rossetti:Fabrizio Silvestri	NA	OSNED 2018 Chairs' Welcome & Organization	NA:NA:NA:NA	2018
Gaku Morio:Katsuhide Fujita	Large-scale online civic engagements (OCEs) with more than 100 participants have become possible due to recent developments in online social media technology. OCEs have the potential to achieve consensus building and collective decision-making with a large number of citizens, which is difficult to achieve in face-to-face contexts. However, most users in a large-scale OCE are rarely constantly active. Therefore, an important problem for the activation of a large-scale OCE is to facilitate the discussion by predicting which citizens will have significant influence in the discussion. This paper examines the activation prediction problem in a large-scale OCE. We propose a novel influence model based on the impulse response of activity histories and argumentative pressures, as well as an effective testing algorithm. The experimental results demonstrate that the proposed models with impulse response and the lexical pressures show better accuracy compared with baselines. In addition, the testing time required by the proposed method can be reduced significantly by employing a node-cutting algorithm.	Predicting Argumentative Influence Probabilities in Large-Scale Online Civic Engagement	NA:NA	2018
Caitlin Gray:Lewis Mitchell:Matthew Roughan	Modelling information cascades over online social networks is important in fields from marketing to civil unrest prediction, however the underlying network structure strongly affects the probability and nature of such cascades. Even with simple cascade dynamics the probability of large cascades are almost entirely dictated by network properties, with well-known networks such as Erdos-Renyi and Barabasi-Albert producing wildly different cascades from the same model. Indeed, the notion of 'superspreaders' has arisen to describe highly influential nodes promoting global cascades in a social network. Here we use a simple model of global cascades to show that the presence of locality in the network increases the probability of a global cascade due to the increased vulnerability of connecting nodes. Rather than 'super-spreaders', we find that the presence of these highly connected 'super-blockers' in heavy-tailed networks in fact reduces the probability of global cascades, while promoting information spread when targeted as the initial spreader.	Super-blockers and the Effect of Network Structure on Information Cascades	NA:NA:NA	2018
Clemens Deusser:Nora Jansen:Jan Reubold:Benjamin Schiller:Oliver Hinz:Thorsten Strufe	Social media interaction happens in a broad variety of context and magnitude. The vast majority of posts cause little to no discussion, while some start trends and become viral. We study the virality, explicitly of "Buzzes" - posts that evoke intense interaction over a short period of time, as they have been observed frequently, some- times with severe consequences for individuals and companies in the physical world. Early detection of a Buzz may help mitigate or prevent negative consequences of large scale social media outrage against companies or persons, by giving them a chance to react at an early stage. Collecting a labeled set of over 100,000 posts on Facebook pages, we first explore properties that define a Buzz using logistic regres- sion. This method helps us to interpret the results and derive prac- tical recommendations. We subsequently train classifiers and apply machine learning based classification techniques to demonstrate the potential capabilities of automated prediction. We achieve high recall with moderate precision, where feature boosting on broad feature sets yields the most promising results. Our study reveals that Buzzes are well described by a high num- ber of comments from previously passive users, a high number of likes given to comments, and a prolonged discussion period - properties that can be used to distinguish inconsequential posts from potentially volatile ones.	Buzz in Social Media: Detection of Short-lived Viral Phenomena	NA:NA:NA:NA:NA:NA	2018
Qiu Fang Ying:Dah Ming Chiu:Srinivasan Venkatramanan:Xiaopeng Zhang	In this paper, we study the posting behavior of OSN users, in particular the posting frequency and temporal patterns, and consider possible interpretations of how users use the platform. At the aggregate (macro) level, we find two distinct peaks, one during morning working hours, and one in the evening. The morning peak is more pronounced for frequent posters, while the evening peak is pronounced in the remaining users. We postulate that this difference results from qualitatively different usage of the OSN platform (e.g. for work, with customers, etc.) than purely social interactions (e.g., friends, family, etc.). We also study user posting behavior at an individual (micro) level and apply LDA to cluster user temporal patterns, interpret our results. Our study provides possibly new insights into user activity in today's OSNs, and suggests a framework for profiling users based on their posting activities. In the process, we provide a novel application of LDA, to temporal user posting behavior by equating the time epochs of posts to words in documents. We believe our approach will complement other methods of user profiling based on static demographic information and friendship network information.	Profiling OSN Users Based on Temporal Posting Patterns	NA:NA:NA:NA	2018
Sebastian Schams:Jan Hauffa:Georg Groh	To improve the quality of communication in Online Social Networks and Media (OSNEM), we envision a system that models a person's contributive social capital (CSC), which encompasses their competence, trustworthiness, and social responsibility. Having the CSC score available may inspire social behavior and mutual support. The system is based on three pillars: the analysis of OSNEM activity, interactions in virtual social capital market systems, and personal endorsements. In this paper we present our investigations regarding the first pillar. To obtain a dataset, we ran an experiment where 165 participants interacted on a custom social networking platform and assessed each other. Ground truth data was derived from these assessments. The dataset shows characteristics that are similar to larger OSNs. With different machine learning algorithms we investigated the hypothesis that contributive social capital can be extracted from network properties and networking activity, which were assessed with features such as the number of contributions of each participant. The prediction of contributive social capital showed an improvement over the baseline. A ranking of the participants following their predicted CSC scores showed a moderate correlation with the ranking according to the ground truth assessment. We also investigated the relative importance of the features for the analysis, and the effect of excluding inactive users to better understand network dynamics on a micro level. The selected features are also available in most other OSNEM platforms, like Facebook and Twitter. This allows a large-scale application of our investigations.	Analyzing a User's Contributive Social Capital Based on Acitivities in Online Social Networks and Media	NA:NA:NA	2018
Mauro Coletto:Claudio Lucchese:Salvatore Orlando	The popularity of online social platforms has also determined the emergence of violent and abusive behaviors reflecting real life issues into the digital arena. Cyberbullying, Internet banging, pedopornography, sexting are examples of these behaviors, as witnessed in the social media environments. Several studies have shown how to approximately detect those behaviors by analyzing the social interactions and in particular the content of the exchanged messages. The features considered in the models basically include detection of o ensive language through NLP techniques and vocabularies, social network structural measures and, if available, user context information. Our goal is to investigate those users who adopt offensive language and hate speech in Twitter by analyzing their profile pictures. Results show that violent people smile less and they are dominating by anger, fear and sadness.	Do Violent People Smile: Social Media Analysis of their Profile Pictures	NA:NA:NA	2018
Stefano Cresci:Marinella Petrocchi:Angelo Spognardi:Stefano Tognazzi	We envisage a revolutionary change in the approach to spambot detection: instead of taking countermeasures only after having collected evidence of new spambot mischiefs, in a near future techniques will be able to anticipate the ever-evolving spammers.	From Reaction to Proaction: Unexplored Ways to the Detection of Evolving Spambots	NA:NA:NA:NA	2018
Chiara Boldrini:Mustafa Toprak:Marco Conti:Andrea Passarella	Ego networks have proved to be a valuable tool for understanding the relationships that individuals establish with their peers, both in offline and online social networks. Particularly interesting are the cognitive constraints associated with the interactions between the ego and the members of their ego network, whereby individuals cannot maintain meaningful interactions with more than 150 people, on average. In this work, we focus on the ego networks of journalists on Twitter, and we investigate whether they feature the same characteristics observed for other relevant classes of Twitter users, like politicians and generic users. Our findings are that journalists are generally more active and interact with more people than generic users. Their ego network structure is very aligned with reference models derived from the social brain hypothesis and observed in general human ego networks. Remarkably, the similarity is even higher than the one of politicians and generic users ego networks. This may imply a greater cognitive involvement with Twitter than with other social interaction means. Moreover, the ego networks of journalists are much stabler than those of politicians and generic users, and the ego-alter ties are often information-driven.	Twitter and the Press: an Ego-Centred Analysis	NA:NA:NA:NA	2018
Laura Koesten:Elena Demidova:Vadim Savenkov:John Breslin:Oscar Corcho:Stefan Dietze:Elena Simperl	The web of data has seen tremendous growth recently. New forms of structured data have emerged in the form of web markup, such as schema.org and web tables. Exploiting these rich, heterogeneous and evolving data sources has become increasingly important for many different types of applications, including (federated) search, question answering and fact verification. The objective of the PROFILES & DATA:SEARCH Workshop is to bring together researchers and practitioners interested in the development of data search techniques, data profiling, and dataset retrieval on the web. This includes looking at the specifics of data-centric information seeking behaviours, understanding interaction challenges in data search on the web, and analysing the cognitive processes involved in the consumption of structured data by users. At the same time, we aim to discuss technologies addressing data search including semantics, information retrieval for web data (ranking algorithms and indexing), in particular in the context of decentralised and distributed systems, such as the web. We are interested in approaches to analyse, characterise and discover data sources. We want to facilitate a discussion around data search across formats and domain-specific applications. The PROFILES & DATA:SEARCH Workshop includes papers on a variety of topics such as profiling and data search, including querying and searching for structured data, profiling applications for cultural heritage, as well as data quality improvements through schema inference, content analysis and communities.	PROFILES & DATA: SEARCH International Workshop on Profiling and Searching Data on the Web Chairs' Welcome & Organization	NA:NA:NA:NA:NA:NA:NA	2018
Aidan Hogan	Graphs are being increasingly adopted as a flexible data model in scenarios (e.g., Google's Knowledge Graph, Facebook's Graph API, Wikidata, etc.) where multiple editors are involved in content creation, where the schema is ever changing, where data are incomplete, where the connectivity of resources plays a key rolescenarios where relational models traditionally struggle. But with this flexibility comes a conceptual cost: it can be difficult to summarise and understand, at a high level, the content that a given graph contains. Hence profiling graphs becomes of increasing importance to extract order, a posteriori, from the chaotic processes by which such graphs are generated. This talk will motivate the use of graphs as a data model, abstract recent trends in graph data management, and then turn to the issue of profiling and summarising graphs: what are the goals of such profiling, the principles by which graphs can be summarised, the main techniques by which this can/could be achieved The talk will emphasise the importance of profiling graphs while highlighting a variety of open research questions yet to be tackled.	Profiling Graphs: Order from Chaos	NA	2018
Maarten de Rijke	Over the years, search engines have developed to return a broad range of retrievable items, from documents to answers, people, locations, and products. Research datasets are increasingly being turned in retrievable items too. This raises a number of interesting challenges. Starting from the user end (What do users want from datasets) to increasing the retrievability of datasets (What kind of contextual information is available to enrich datasets so as to make the more easily retrieval) to optimizing rankers for datasets in the absence of large volumes of interaction data (How can we train learning to rank datasets algorithms in weakly supervised ways).	Learning to Search for Datasets	NA	2018
Emilia Kacprzak:Laura Koesten:Jeni Tennison:Elena Simperl	The amount of data generated and published on the web is increasing rapidly, but search for structured data on the web still presents challenges. In this paper we explore dataset search by analysing queries specifically generated for this work through a crowdsourcing experiment and comparing them to a search log analysis of queries on data portals. The change in search environment together with the task we gave people altered the generated queries. We found that queries issued in our experiment were much longer than search queries for datasets on data portals. They further contained seven times more mentions of geospatial and of temporal information and are more likely to be structured as questions. These insights can be used to tailor search functionalities to the particular information needs and characteristics of dataset search.	Characterising Dataset Search Queries	NA:NA:NA:NA	2018
Mohamed Ben Ellefi:Odile Papini:Djamal Merad:Jean-Marc Boi:Jean-Philip Royer:Jérôme Pasquet:Jean-Christophe Sourisseau:Filipe Castro:Mohammad Motasem Nawaf:Pierre Drap	Cultural heritage (CH) resources are very heterogeneous since the information was collected from vast diversity of cultural sites and digitally recorded in different formats. With the progress of 3D technologies, photogrammetry techniques become the adopted solution for representing CH artifacts by turning photos from small finds, to entire landscapes, into accurate 3D models. To meet knowledge representation with cultural heritage photogrammetry, this paper proposes an ontology-profiling method for modeling a real case of archaeological amphorae. The ontological profile consists of all needed information to represent a CH resource including typology attributes, geo-spatial information and photogrammetry process. An example illustrating the applicability of this profiling method to the problem of CH resources conceptualization is presented. We also outline our perspectives for using ontologies in data-driven science, in particular on modeling a complete pipeline that manages both the photogrammetric process and the archaeological knowledge.	Cultural Heritage Resources Profiling: Ontology-based Approach	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Semih Yumusak:Andreas Kamilaris:Erdogan Dogdu:Halife Kodaz:Elif Uysal:Riza Emre Aras	The Semantic Web promotes common data formats and exchange protocols on the web towards better interoperability among systems and machines. Although Semantic Web technologies are being used to semantically annotate data and resources for easier reuse, the ad hoc discovery of these data sources remains an open issue. Popular Semantic Web endpoint repositories such as SPARQLES, Linking Open Data Project (LOD Cloud), and LODStats do not include recently published datasets and are not updated frequently by the publishers. Hence, there is a need for a web-based dynamic search engine that discovers these endpoints and datasets at frequent intervals. To address this need, a novel web meta-crawling method is proposed for discovering Linked Data sources on the Web. We implemented the method in a prototype system named SPARQL Endpoints Discovery (SpEnD). In this paper, we describe the design and implementation of SpEnD, together with an analysis and evaluation of its operation, in comparison to the aforementioned static endpoint repositories in terms of time performance, availability, and size. Findings indicate that SpEnD outperforms existing Linked Data resource discovery methods.	A Discovery and Analysis Engine for Semantic Web	NA:NA:NA:NA:NA:NA	2018
Sean Soderman:Anusha Kola:Maksim Podkorytov:Michael Geyer:Michael Gubanov	Variety of Big data is a significant impediment for anyone who wants to search inside a large-scale structured dataset. For example, there are millions of tables available on the Web, but the most relevant search result does not necessarily match the keyword-query exactly due to a variety of ways to represent the same information. Here we describe Hybrid.AI, a learning search engine for large-scale structured data that uses automatically generated machine learning classifiers and Unified Famous Objects (UFOs) to return the most relevant search results from a large-scale Web tables corpora. We evaluate it over this corpora, collecting 99 queries and their results from users, and observe significant relevance gain.	Hybrid.AI: A Learning Search Engine for Large-scale Structured Data	NA:NA:NA:NA:NA	2018
Zhiyu Chen:Haiyan Jia:Jeff Heflin:Brian D. Davison	Impoverished descriptions and convoluted schema labels are common challenges in data-centric tasks such as schema matching and data linking, especially when datasets can span domains. To address these issues, we consider the task of schema label generation. Typically, schema labels are created by dataset providers and are useful for users to understand a dataset. The motivation behind the task is that a lot of data linking systems require overlapping information between two datasets and rely on unique identifiers of schema labels. Moreover, it is common for schema labels in different datasets to have different identifiers even when they refer to the same concept. With no naming standard for schema labels, unintelligible labels are widely found in real-world datasets. For example, many schema labels contain abbreviations and compound nouns that hinder automated matching of attributes in corresponding datasets. Through schema label generation, more common (and thus understandable) schema labels can be provided to allow for broader schema matches in contexts such as dataset search and data linking. We develop a variety of features based on analysis of dataset content to enable machine learning methods to recommend useful labels. We test our approach on two real-world data collections and demonstrate that our method is able to outperform the alternative approach.	Generating Schema Labels through Dataset Content Analysis	NA:NA:NA:NA	2018
Sebastian Neumaier:Lörinc Thurnay:Thomas J. Lampoltshammer:Tomá Knap	The present work describes the ADEQUATe platform: a framework to monitor the quality of (Governmental) Open Data catalogs, to re-publish improved and linked versions of the datasets and their respective metadata descriptions, and to include the community in the quality improvement process. The information acquired by the linking and (meta)data improvement steps is then integrated in a semantic search engine. In the paper, we first describe the requirements of the platform, which are based on focus group interviews and a web-based survey. Second, we use these requirements to formulate the goals and show the architecture of the overall platform, and third, we showcase the potential and relevance of the platform to resolve the requirements by describing exemplary user journeys exploring the system. The platform is available at: https://www.adequate.at/	Search, Filter, Fork, and Link Open Data: The ADEQUATe platform: data- and community-driven quality improvements	NA:NA:NA:NA	2018
Pinelopi Troullinou:Mathieu d'Aquin:Ilaria Tiddi	This volume of proceedings presents the papers from the 2nd edition of the interdisciplinary workshop Re-coding Black Mirror, held on April 24, 2018 in Lyon, France and co-located with The WEB Conference (WWW2018). Participating to the topical debate of data ethics and algorithmic governance, Re-coding Black Mirror offers the research community tools to reflect on its role in the construction of the technological future and the potential societal implications. The workshop becomes a venue for computer scientists, data scientists and social scientists to create bridges of knowledge. The complexity of the societal phenomena emerging from the development in web technologies urge for interdisciplinary collaboration. Following the slightly futuristic approach to technology of the British-made sci-fi series Black Mirror, we called scientists to create their dystopic scenarios developed from their own existing technologies. Through this thought experiment, researchers considered potential ethical and social risks of technological advancements offering in some cases possible solutions.	Re-coding Black Mirror Chairs' Welcome & Organization	NA:NA:NA	2018
Sven Helmer	We analyze the scenario depicted in the "Black Mirror" episode "Fifteen Million Merits" from an economic point of view, focusing on treating the attention of a user or consumer as a commodity. We continue by sketching the technological requirements for building such an economic framework, looking at advertisement platforms, payment schemes, and surveillance technology. As we show, a lot of the technology already exists and we expect the gaps to be filled in the very near future. Additionally, we briefly discuss the impact on social and work environments. While we believe that a scenario as extreme as shown in the episode is unlikely, we think that certain facets of it could find their way into our society.	May I Have Your Attention, Please: - Building a Dystopian Attention Economy	NA	2018
Tabea Tietz:Francesca Pichierri:Maria Koutraki:Dara Hallinan:Franziska Boehm:Harald Sack	What happens to our social media profiles when we die The episode "Be Right Back" as part of Netflix's series "Black Mirror" provides a possible scenario. A digital avatar is created to communicate with close relatives which learns from past social media activities of the deceased user. While the users entrust their social media content to one or more companies, even after their death, it may be reasonable to ask: What will the company really do with a deceased user's data: sell it to manipulate users or create advertisements In this paper we tackle the issues of ownership, ethics, and transparency of post mortem user data.	Digital Zombies - the Reanimation of our Digital Selves	NA:NA:NA:NA:NA:NA	2018
Martino Mensio:Giuseppe Rizzo:Maurizio Morisio	A future where the conversation with machines can potentially involve mutual emotions between the parties may be not so far in time. Inspired by the episode of Black Mirror "Be Right Back'' and Replika, a futuristic app that promises to be "your best friend'', in this work we are considering the positive and negative points of including an automated learning conversational agent inside the personal world of feelings and emotions. These systems can impact both single individuals and society, worsening an already critical situation. Our conclusion is that a regulation on the artificial emotional content should be considered before actually going beyond some one-way-only limits.	The Rise of Emotion-aware Conversational Agents: Threats in Digital Emotions	NA:NA:NA	2018
Kevin Koidl	Social Media has transformed modern day society. It can be argued that one of the main drivers behind this transformation are novel ways to effectively distribute content in a highly targeted fashion and at scale. Recently, this effectiveness has come under attack based on new phenomena known as Fake News, Filter Bubble and Echo Chambers. The public debate about the impact of these phenomena on modern day society ranges from demanding a complete social media shutdown to government intervention and censorship. Furthermore, it appears that Social Media Platform providers are not sure what countermeasures are needed to address these new challenges. The main concern is that Black Mirror like scenarios will emerge simply by allowing privately held companies decide what content is conforming to public norms leading to a distortion of values. This paper presents an alternative solution by focusing on empowering meaningful relationships and not content engagement. The main motivation behind the proposed solution is to create social networks that follow a Trust by Design paradigm. This paper introduces and discusses the above-­mentioned challenges and presents a novel new social media concept seeking to overcome current challenges.	Towards Trust­-based Decentralized Ad-Hoc Social Networks	NA	2018
Linda Anticoli:Marco Basaldella	In this paper we explore possible negative drawbacks in the use of wearable sensors, i.e., wearable devices used to detect different kinds of activity, e.g., from step and calories counting to heart rate and sleep monitoring. These technologies, which in the latter years witnessed a rapid development in terms of accuracy and diffusion, are now available on different platforms at reasonable prices and can lead to an healthier behavior in people using them. Nevertheless, we will try to investigate possibly harming behaviors related to these devices. We will provide different scenarios in which wearable sensors, in connection with social media, data mining, or other technologies, could prove harmful for their users.	Shut Up and Run: the Never-ending Quest for Social Fitness	NA:NA	2018
Patrick Wang:Rafael Angarita:Ilaria Renna	Social media is an amazing platform for enhancing public exposure. Anyone, even social bots, can reach out to a vast community and expose one's opinion. But what happens when fake news is (un)intentionally spread within a social media This paper reviews techniques that can be used to fabricate fake news and depicts a scenario where social bots evolve in a fully semantic Web to infest social media with automatically generated deceptive information.	Is this the Era of Misinformation yet: Combining Social Bots and Fake News to Deceive the Masses	NA:NA:NA	2018
Martin Cooney:Sepideh Pashami:Anita Sant'Anna:Yuantao Fan:Slawomir Nowaczyk	What would happen in a world where people could "see'' others' hidden emotions directly through some visualizing technology Would lies become uncommon and would we understand each other better Or to the contrary, would such forced honesty make it impossible for a society to exist The science fiction television show Black Mirror has exposed a number of darker scenarios in which such futuristic technologies, by blurring the lines of what is private and what is not, could also catalyze suffering. Thus, the current paper first turns an eye towards identifying some potential pitfalls in emotion visualization which could lead to psychological or physical harm, miscommunication, and disempowerment. Then, some countermeasures are proposed and discussed--including some level of control over what is visualized and provision of suitably rich emotional information comprising intentions--toward facilitating a future in which emotion visualization could contribute toward people's well-being. The scenarios presented here are not limited to web technologies, since one typically thinks about emotion recognition primarily in the context of direct contact. However, as interfaces develop beyond today's keyboard and monitor, more information becomes available also at a distance--for example, speech-to-text software could evolve to annotate any dictated text with a speaker's emotional state.	Pitfalls of Affective Computing: How can the automatic visual communication of emotions lead to harm, and what can be done to mitigate such risks	NA:NA:NA:NA:NA	2018
