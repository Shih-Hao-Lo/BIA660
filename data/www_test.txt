Luca Viganò:Diego Sempreboni	What if we delegated so much to autonomous AI and intelligent machines that They passed a law that forbids humans to carry out a number of professions We conceive the plot of a new episode of Black Mirror to reflect on what might await us and how we can deal with such a future.	Gnirut: The Trouble With Being Born Human In An Autonomous World	NA:NA	2018
Diego Sempreboni:Luca Viganò	Consider the following set-up for the plot of a possible future episode of the TV series Black Mirror: human brains can be connected directly to the net and MiningMind Inc. has developed a technology that merges a reward system with a cryptojacking engine that uses the human brain to mine cryptocurrency (or to carry out some other mining activity). Part of our brain will be committed to cryptographic calculations (mining), leaving the remaining part untouched for everyday operations, i.e., for our brain's normal daily activity. In this short paper, we briefly argue why this set-up might not be so far fetched after all, and explore the impact that such a technology could have on our lives and our society.	MMM: May I Mine Your Mind	NA:NA	2018
Harshvardhan J. Pandit:Dave Lewis	The use of personal data is a double-edged sword that on one side provides benefits through personalisation and user profiling, while the other raises several ethical and moral implications that impede technological progress. Laws often try to reflect the shifting values of social perception, such as the General Data Protection Regulation (GDPR) catering to explicit consent over use of personal data, though actions may still be legal without being perceived as acceptable. Black Mirror is a TV series that serves to imagine scenarios that test the boundary of such perceptions, and is often described as being futuristic. In this paper, we discuss how existing technologies have already coalesced towards calculating a probability metric or rating as presented by the episode 'Nosedive'. We present real-world instances of such technologies and their applications, and how they can be easily expanded using the interminable web. The dilemma posed by the ethics of such technological applications is discussed using the 'Ethics Canvas', our methodology and tool for encouraging discussions on ethical implications in responsible innovation.	Ease and Ethics of User Profiling in Black Mirror	NA:NA	2018
Marie-Laure Mugnier:Catherine Roussey:Pierre Senellart	It is our great pleasure to welcome you to the WWW 2018 Reasoning on Data Workshop. This workshop will gather people on a timely issue at the crossroad on knowledge representation and reasoning, data management, and the Semantic Web: How to use knowledge to make better use of data The workshop will more precisely focus on reasoning techniques that allow us to exploit domain knowledge in data access. An emblematic task is query answering, but knowledge can be exploited within the whole data lifecycle.	Reasoning on Data Workshop Chairs' Welcome and Organization	NA:NA:NA	2018
Mehdi Terdjimi:Lionel Médini:Michael Mrissa	Today's Web applications tend to reason about cyclic data (i.e. facts that re-occur periodically) on the client side. Although they can benefit from efficient incremental maintenance algorithms capable of handling frequent data updates, existing rule-based algorithms cause successive re-derivations of previously inferred information. In this paper, we propose an incremental maintenance approach for rule-based reasoning that prevents successive re-computations of fact derivations. We tag (i.e. annotate) facts to keep trace of their provenance and validity. We compare our solution with the DRed-based incremental reasoning algorithm and show that it significantly outperforms this algorithm for fact updates in re-occurring situations, to the cost of tagging facts at their first insertion. Our experiments show that this cost can be recovered within a small number of cycles of deletions and reinsertions of explicit facts. We discuss the utility and limitations of our approach on Web clients and provide implementation packages of this reasoner that can be directly integrated in Web applications, on both server and client sides.	Web Reasoning Using Fact Tagging	NA:NA:NA	2018
Viet-Phi Huynh:Paolo Papotti	Fact checking is the task of determining if a given claim holds. Several algorithms have been developed to check facts with reference information in the form of knowledge bases. While individual algorithms have been experimentally evaluated, we provide a first publicly available benchmark evaluating fact checking implementations across a range of assumptions about the properties of the facts and the reference data. We used our benchmark to compare algorithms designed on different principles and assumptions, as well as algorithms that can solve similar tasks developed in closely related communities. Our evaluation provided us with a number of new insights concerning the factors that impact the performance of the different methods.	Towards a Benchmark for Fact Checking with Knowledge Bases	NA:NA	2018
Jérôme David:Jérôme Euzenat:Pierre Genevès:Nabil Layaïda	Query transformations are ubiquitous in semantic web query processing. For any situation in which transformations are not proved correct by construction, the quality of these transformations has to be evaluated. Usual evaluation measures are either overly syntactic and not very informative the result being: correct or incorrect or dependent from the evaluation sources. Moreover, both approaches do not necessarily yield the same result. We suggest that grounding the evaluation on query containment allows for a data-independent evaluation that is more informative than the usual syntactic evaluation. In addition, such evaluation modalities may take into account ontologies, alignments or different query languages as soon as they are relevant to query evaluation.	Evaluation of Query Transformations without Data: Short paper	NA:NA:NA:NA	2018
Franz Baader:Pavlos Marantidis:Maximilian Pensel	Ontology-mediated query answering can be used to access large data sets through a mediating ontology. It has drawn considerable attention in the Description Logic (DL) community where both the complexity of query answering and practical query answering approaches based on rewriting were investigated in detail. Surprisingly, there is still a gap in what is known about the data complexity of query answering w.r.t. ontologies formulated in the inexpressive DL FL0. While it is known that the data complexity of answering conjunctive queries w.r.t. FL0 ontologies is coNP-complete, the exact complexity of answering instance queries was open until now. In the present paper, we show that answering instance queries w.r.t. FL0 ontologies is in P for data complexity. Together with the known lower bound of P-completeness for a fragment of FL0, this closes the gap mentioned above.	The Data Complexity of Answering Instance Queries in FL0	NA:NA:NA	2018
Marie-Francine Moens:Gareth J. F. Jones:Saptarshi Ghosh:Debasis Ganguly:Tanmoy Chakraborty:Kripabandhu Ghosh	User-generated content on online social media (OSM) platforms has become an important source of real-time information during emergency events. The SMERP workshop series aims to provide a forum for researchers working on utilizing OSM for emergency preparedness and aiding post-emergency relief operations. The workshop aims to bring together researchers from diverse fields - Information Retrieval, Data Mining and Machine Learning, Natural Language Processing, Social Network Analysis, Computational Social Science, Human Computer Interaction - who can potentially contribute to utilizing social media for emergency relief and preparedness. The first SMERP workshop was held in April 2017 in conjunction with the ECIR 2017 conference. This 2nd SMERP Workshop with The Web Conference 2018 includes two keynote talks, a peer-reviewed research paper track, and a panel discussion.	WWW'18 Workshop on Exploitation of Social Media for Emergency Relief and Preparedness: Chairs' Welcome & Organization	NA:NA:NA:NA:NA:NA	2018
Dheeraj Kumar:Satish V. Ukkusuri	Hurricane evacuation is a complex process and a better understanding of the evacuation behavior of the coastal residents could be helpful in planning better evacuation policy. Traditionally, various aspects of the household evacuation decisions have been determined by post-evacuation questionnaire surveys, which are usually time-consuming and expensive. Increased activity of users on social media, especially during emergencies, along with the geo-tagging of the posts, provides an opportunity to gain insights into user's decision-making process, as well as to gauge public opinion and activities using the social media data as a supplement to the traditional survey data. This paper leverages the geo-tagged Tweets posted in the New York City (NYC) in wake of Hurricane Sandy to understand the evacuation behavior of the residents. Based on the geo-tagged Tweet locations, we classify the NYC Twitter users into one of the three categories: outside evacuation zone, evacuees, and non-evacuees and examine the types of Tweets posted by each group during different phases of the hurricane. We establish a strong link between the social connectivity with the decision of the users to evacuate or stay. We analyze the geo-tagged Tweets to understand evacuation and return time and evacuation location patterns of evacuees. The analysis presented in this paper could be useful for authorities to plan a better evacuation campaign to minimize the risk to the life of the residents of the emergency hit areas.	Utilizing Geo-tagged Tweets to Understand Evacuation Dynamics during Emergencies: A case study of Hurricane Sandy	NA:NA	2018
Anastasia Moumtzidou:Stelios Andreadis:Ilias Gialampoukidis:Anastasios Karakostas:Stefanos Vrochidis:Ioannis Kompatsiaris	Disaster monitoring based on social media posts has raised a lot of interest in the domain of computer science the last decade, mainly due to the wide area of applications in public safety and security and due to the pervasiveness not solely on daily communication but also in life-threating situations. Social media can be used as a valuable source for producing early warnings of eminent disasters. This paper presents a framework to analyse social media multimodal content, in order to decide if the content is relevant to flooding. This is very important since it enhances the crisis situational awareness and supports various crisis management procedures such as preparedness. Evaluation on a benchmark dataset shows very good performance in both text and image classification modules.	Flood Relevance Estimation from Visual and Textual Content in Social Media Streams	NA:NA:NA:NA:NA:NA	2018
Samujjwal Ghosh:Maunendra Sankar Desarkar	Proper formulation of features plays an important role in short-text classification tasks as the amount of text available is very little. In literature, Term Frequency - Inverse Document Frequency (TF-IDF) is commonly used to create feature vectors for such tasks. However, TF-IDF formulation does not utilize the class information available in supervised learning. For classification problems, if it is possible to identify terms that can strongly distinguish among classes, then more weight can be given to those terms during feature construction phase. This may result in improved classifier performance with the incorporation of extra class label related information. We propose a supervised feature construction method to classify tweets, based on the actionable information that might be present, posted during different disaster scenarios. Improved classifier performance for such classification tasks can be helpful in the rescue and relief operations. We used three benchmark datasets containing tweets posted during Nepal and Italy earthquakes in 2015 and 2016 respectively. Experimental results show that the proposed method obtains better classification performance on these benchmark datasets.	Class Specific TF-IDF Boosting for Short-text Classification: Application to Short-texts Generated During Disasters	NA:NA	2018
Ribhav Soni:Sukomal Pal	Microblogging sites like Twitter, Facebook, etc., are important sources of first-hand accounts during disaster situations, and have the potential to significantly aid disaster relief efforts. The IRMiDis track at FIRE 2017 focused on developing and comparing IR approaches to automatically identify and match tweets that indicate the need or availability of a resource, leading to the creation of a benchmark dataset for future improvements in this task. However, based on our experiments, we argue that the gold standard data obtained in the track is substantially incomplete. We also discuss some reasons why it may have been so, and provide some suggestions for making more robust ground truth data in such tasks.	Gold Standard Creation for Microblog Retrieval: Challenges of Completeness in IRMiDis 2017	NA:NA	2018
Ritam Dutt:Kaustubh Hiware:Avijit Ghosh:Rameshwar Bhaskaran	We present SAVITR, a system that leverages the information posted on the Twitter microblogging site to monitor and analyse emergency situations. Given that only a very small percentage of microblogs are geo-tagged, it is essential for such a system to extract locations from the text of the microblogs. We employ natural language processing techniques to infer the locations mentioned in the microblog text, in an unsupervised fashion and display it on a map-based interface. The system is designed for efficient performance, achieving an F-score of 0.81, and is approximately two orders of magnitude faster than other available tools for location extraction.	SAVITR: A System for Real-time Location Extraction from Microblogs during Emergencies	NA:NA:NA:NA	2018
Cheng-Te Li:Lun-Wei Ku	With the rapid growing of social networking services (e.g., Facebook and Twitter), being able to process data come from such platforms has gained much attention in recent years. SocialNLP is a new inter-disciplinary area of natural language processing (NLP) and social computing. There are three plausible directions of SocialNLP: (1) addressing issues in social computing using NLP techniques; (2) solving NLP problems using information from social media; and (3) handling new problems related to both social computing and natural language processing. Several challenges are foreseeable in SocialNLP. First, the message lengths on social media are usu-ally short, and thus it is difficult to apply traditional NLP approaches directly. Second, social media contains heterogeneous information (e.g. tags, friends, followers, likes, and retweets) that should be considered together with the contents for better quality of analysis. Finally, social media contents always involve multiple persons with slangs and jargons, and usually require special techniques to process. We organize SocialNLP in WWW 2018 with three goals. First, social media data is essentially generated and collected from online social services that are functioned based on Web techniques. One can leverage Web techniques to investigate various user behaviors and investigate the interactions between users. Second, user-generated data in social media is mainly in the form of text. Theories and techniques on Web information retrieval and natural language processing are desired for semantic understanding, accurate search, and efficient processing of big social media data. Third, from the perspective of application, if social media data can be effectively processed to distill the collective knowledge of users, novel Web applications, such as emergency management, social recommendation, and future prediction, can be developed with higher accuracy and better user experience. We expect SocialNLP workshop in WWW community can provide mutually-reinforced benefits for researchers in areas of Web techniques, information retrieval and social media analytics.	[email protected] 2018 Chairs' Welcome & Organization	NA:NA	2018
Xuetong Chen:Martin D. Sykora:Thomas W. Jackson:Suzanne Elayan	Depression is among the most commonly diagnosed mental disorders around the world. With the increasing popularity of online social network platforms and the advances in data science, more research efforts have been spent on understanding mental disorders through social media by analysing linguistic style, sentiment, online social networks and other activity traces. However, the role of basic emotions and their changes over time, have not yet been fully explored in extant work. In this paper, we proposed a novel approach for identifying users with or at risk of depression by incorporating measures of eight basic emotions as features from Twitter posts over time, including a temporal analysis of these features. The results showed that emotion-related expressions can reveal insights of individuals' psychological states and emotions measured from such expressions show predictive power of identifying depression on Twitter. We also demonstrated that the changes in an individual's emotions as measured over time bear additional information and can further improve the effectiveness of emotions as features, hence, improve the performance of our proposed model in this task.	What about Mood Swings: Identifying Depression on Twitter with Temporal Measures of Emotions	NA:NA:NA:NA	2018
Sinya Peng:Vincent S. Tseng:Che-Wei Liang:Man-Kwan Shan	Social media provides a vast continuous supply of dynamic and diverse information contents from the crowd, which serves as useful resources for predictive analytical applications. Although there exist already a number of studies on emerging topics detection, they focused on modelling of textual contents and emerging detection mechanism over topic popularity. To meet the real-life demands, prediction of emerging product topic, rather than detection, in the early stage is required. Besides, despite that some relevant studies considered social structure information, they suffer from the assumption that the complete network is available and the diffusion process only depends on social influence among members of networks. Moreover, not all social media sites provide the functionality to facilitate the development of online social networks. In this paper, we tackle the problem of emerging product topics prediction in social network with implicit networks. Two tasks, one for long-term forecast in pre-production stage and the other for short-term forecast in post-release stage, are investigated. We present a novel framework named Emerging Topics Predictor (ETP). Two novel features, namely author diversity and competition features, are also proposed to accommodate the diffusion process with implicit networks based on the rationale of product marketing. Through empirical evaluation on movie reviews from two real social media sites, ETP is shown to provide effective and efficient performance in predicting the emerging topics as early as possible. In particular, the experiment results show the promising effect of author diversity in emerging prediction. To the best of our knowledge, this work is among the very first studies on emerging product topic prediction in social media with considerations of implicit networks.	Emerging Product Topics Prediction in Social Media without Social Structure Information	NA:NA:NA:NA	2018
Saratchandra Indrakanti:Gyanit Singh:Justin House	Product reviews on modern e-commerce websites have evolved into repositories of valuable firsthand opinions on products. Showcasing the opinions that reviewers express on a product in a succinct way can not only promote the product, but also provide an engaging experience that simplifies the shopping journey for online shoppers. In the case of traditional media such as movies and books, employingblurbs or excerpts from critic reviews for promotional purposes is an established practice among movie publicists and book editors that has proven to be an effective way of capturing attention of customers. Such excerpts can be discovered from e-commerce product reviews to highlight interesting reviewer opinions and add emotive elements to otherwise bland e-commerce product pages. While traditional movie or book blurbs are manually extracted, they must be automatically extracted from e-commerce product reviews owing to the scale of catalogues. Further, traditional blurbs are generally phrased to be very positive in tone and sometimes may take some words out of context. However, excerpts for e-commerce products must represent the true opinions of the reviewers and must capture the context in which the words were used to retain trust of users. To that end, we introduce the problem of extracting engaging excerpts from e-commerce product reviews in this paper. We present methods to automatically discover such excerpts from reviews at scale by leveraging natural language properties such as syntactic structure of sentences and sentiment, and discuss some of the underlying challenges. We further present an evaluation of the effectiveness of the proposed methods in terms of the quality of the blurbs generated and their ranking orders produced.	Blurb Mining: Discovering Interesting Excerpts from E-commerce Product Reviews	NA:NA:NA	2018
Reshmi Gopalakrishna Pillai:Mike Thelwall:Constantin Orasan	The ability to automatically detect human stress and relaxation is crucial for timely diagnosing stress-related diseases, ensuring customer satisfaction in services and managing human-centric applications such as traffic management. Traditional methods employ stress-measuring scales or physiological monitoring which may be intrusive and inconvenient. Instead, the ubiquitous nature of the social media can be leveraged to identify stress and relaxation, since many people habitually share their recent life experiences through social networking sites. This paper introduces an improved method to detect expressions of stress and relaxation in social media content. It uses word sense disambiguation by word sense vectors to improve the performance of the first and only lexicon-based stress/relaxation detection algorithm TensiStrength. Experimental results show that incorporating word sense disambiguation substantially improves the performance of the original TensiStrength. It performs better than state-of-the-art machine learning methods too in terms of Pearson correlation and percentage of exact matches. We also propose a novel framework for identifying the causal agents of stress and relaxation in tweets as future work.	Detection of Stress and Relaxation Magnitudes for Tweets	NA:NA:NA	2018
Lipika Dey:Tirthanker Dasgupta:Priyanka Sinha	Breakthroughs in Artificial Intelligence (AI) and World Wide Web technologies have opened a new direction in Enterprise Intelligence, that is gradually transforming the way enterprises perform business and interact with their customers. This change is largely driven by the widespread consumer adoption of sophisticated AI technologies and web based social media. Consequently, almost all business enterprises face a number of challenges such as adoption of new business paradigms; customer centric business processes; issues with large, multi-modal, multilingual, and multicultural data, analyzing behavioral signals from social media; agility, security and many more. Therefore, the primary goal of this workshop is to bring together industry professionals and researchers working in the area of AI, Natural Language Processing (NLP), Machine-Learning, linguistics, social science, HCI, design and computer vision and those whose work concerns the intersection of these areas, together to provide a venue for the multidisciplinary discussion of how ubiquitous AI technologies can help in extracting social and enterprise intelligence for smart enterprise transformation while addressing the aforementioned challenges.	Social Sensing and Enterprise Intelligence: Towards a Smart Enterprise Transformation (SSEI 2018) Chairs' Welcome and Organization	NA:NA:NA	2018
Dominik Slezak	The AI methods are regaining a lot of attention in the areas of data analytics and decision support. Given the increasing amount of information and computational resources available, it is now possible for intelligent algorithms to learn from the data and assist humans more efficiently. Still, there is a question about the goals of learning and a form of the resulting data-driven knowledge. It is evident that humans do not operate with precise information in decision-making and, thus, it might be unnecessary to provide them with complete outcomes of analytical processes. Consequently, the next question arises whether approximate results of computations or results derived from the approximate data could be delivered more efficiently than their standard counterparts. Such questions are analogous to the ones about precision of calculations conducted by machine learning and KDD methods, whereby heuristic algorithms could be boosted by letting them rely on approximate computations. This leads us toward discussion of the importance of approximations in the areas of machine intelligence and business intelligence and, more broadly, the meaning of approximate derivations for various aspects of AI. In this talk, this discussion is supported by four industry-related case studies\footnoteThe first case study refers entirely to the author's work for Security On-Demand (\urlhttps://www.securityondemand.com/ ). The work on the second case study was co-financed by the EU Smart Growth Operational Programme 2014-2020 under the Innovation Voucher project POIR.02.03.02-14-0009/15-00. The work on the third case study is co-financed by the EU Smart Growth Operational Programme 2014-2020 under the project POIR.01.01.01-00-0831/17-00. The work on the fourth case study is co-financed by the EU Smart Growth Operational Programme 2014-2020 under the GameINN project POIR.01.02.00-00-0184/17-00 : \beginenumerate ıtem The approximate database engine based on the paradigms of rough-granular computing applied in the area of cyber-security analytics \citeslezak:queryengine,\citeslezak:scalablefeature, \citeslezak:cyberengine ıtem The similarity-based feature engineering methodology embedded into an HR support system working with heterogeneous information sources \citeslezak:jobs ; ıtem The ensemble-based attribute approximation approach that will be used in the area of online health support \citeOvu ; ıtem The process of approximate data generation that will be used for tuning an online gaming coaching platform \citeEsensei.\endenumerate	Toward Approximate Intelligence: Approximate Query Engines & Approximate Data Exploration	NA	2018
Galit B. Yom-Tov:Shelly Ashtar:Daniel Altman:Michael Natapov:Neta Barkay:Monika Westphal:Anat Rafaeli	We adjust sentiment analysis techniques to automatically detect customer emotion in on-line service interactions of multiple business domains. Then we use the adjusted sentiment analysis tool to report insights about the dynamics of emotion in on-line service chats, using a large data set of Telecommunication customer service interactions. Our analyses show customer emotions starting out negative and evolving into positive as the interaction ends. Also, we identify a close relationship between customer emotion dynamicsduring the service interaction and the concepts of service failure and recovery. This connection manifests in customer service quality evaluationsafter the interaction ends. Our study shows the connection between customer emotion and service quality as service interactions unfold, and suggests the use of sentiment analysis tools for real-time monitoring and control of web-based service quality.	Customer Sentiment in Web-Based Service Interactions: Automated Analyses and New Insights	NA:NA:NA:NA:NA:NA:NA	2018
Changzhou Li:Yao Lu:Junfeng Wu:Yongrui Zhang:Zhongzhou Xia:Tianchen Wang:Dantian Yu:Xurui Chen:Peidong Liu:Junyu Guo	Clustering narrow-domain short texts, such as academic abstracts, is an extremely difficult clustering problem. Firstly, short texts lead to low frequency and sparseness of words, making clustering results highly unstable and inaccurate; Secondly, narrow domain leads to great overlapping of insignificant words and makes it hard to distinguish between sub-domains, or fine-grained clusters. The vocabulary size is also too small to construct a good word bag needed by traditional clustering algorithms like LDA to give a meaningful topic distribution. A novel clustering model, Partitioned Word2Vec-LDA (PW-LDA), is proposed in this paper to tackle the described problems. Since the purpose sentences of an abstract contain crucial information about the topic of the paper, we firstly implement a novel algorithm to extract them from the abstracts according to its structural features. Then high-frequency words are removed from those purpose sentences to get a purified-purpose corpus and LDA and Word2Vec models are trained. After combining the results of both models, we can cluster the abstracts more precisely. Our model uses abstract text instead of keywords to cluster because keywords may be ambiguous and cause unsatisfied clustering results shown by previous work. Experimental results show that the clustering results of PW-LDA are much more accurate and stable than state-of-the-art techniques.	LDA Meets Word2Vec: A Novel Model for Academic Abstract Clustering	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Pankaj Trivedi:Arvind Singh	Payment transaction engine at PayU processes multimillion trans- actions every day through multiple payment gateways. Routing a transaction through an appropriate payment gateway is crucial to the engine for optimizing the availability and cost. The problem is that every transaction needs to choose one of K available payment gateways characterized by an unknown probability reward distri- bution. The reward for a gateway is a combination of its health and cost factors. The reward for a gateway is only realized when transaction is processed by the gateway i.e. by its success or failure. The objective of dynamic routing is to maximize the cumulative expected rewards over some given horizon of transactions' life. To do this, the dynamic switching system needs to acquire informa- tion about gateways (exploration) while simultaneously optimizing immediate rewards by selecting the best gateway at the moment (exploitation); the price paid due to this trade o is referred to as the regret. The main objective is to minimize the regret and maximize the rewards. The basic idea is to choose a gateway according to its probability of being the best gateway. The routing problem is a direct formulation of reinforcement learning (RL) problem. In an RL problem, an agent interacts with a dynamic, stochastic, and incompletely known environment, with the goal of finding an action-selection strategy, or policy, that optimizes some long-term performance measure. Thompson Sampling algorithm has experimentally been shown to be close to optimal.	Stochastic Multi-path Routing Problem with Non-stationary Rewards: Building PayU's Dynamic Routing	NA:NA	2018
Nabeel Albishry:Tom Crick:Theo Tryfonas:Tesleem Fagade	With an increasing number of consumers using social media platforms to share both their satisfaction and displeasure about the products and services they use every day, organisations with a customer service focus are recognising the importance of rapid--and genuine--online engagement with their customers. In turn, consumers increasingly judge organisations on the quality of customer service and degree of responsiveness to online queries. This paper presents an extensible framework for evaluating direct engagements of customer service teams with customers on Twitter. Furthermore, this framework provides the capability to measure and analyse indirect engagement with industry sector rivals, especially their patterns, frequency and intensity. By applying graph analysis to these Twitter interactions, our framework generates various analytical measures and visual representations, exemplified through a case study based on seven major UK telecoms companies. With a dataset consisting of 15,000 tweets and 3,500 user profiles, the results provide sustained evidence for indirect engagements between business rivals, with customer queries acting as a trigger for intense competition between companies based in the same industry sub-domain.	An Evaluation of Performance and Competition in Customer Services on Twitter: A UK Telecoms Case Study	NA:NA:NA:NA	2018
Manish Puri:Xu Du:Aparna S. Varde:Gerard de Melo	This research focuses on mining ordinances (local laws) and public reactions to them expressed on social media. We place particular emphasis on ordinances and tweets relating to Smart City Characteristics (SCCs), since an important aim of our work is to assess how well a given region heads towards a Smart City. We rely on SCCs as a nexus between a seemingly infinite number of ordinances and tweets to be able to map them, and also to facilitate SCC-based opinion mining later for providing feedback to urban agencies based on public reactions. Common sense knowledge is harnessed in our approach to reflect human judgment in mapping. This paper presents our research in ordinance and tweet mapping with SCCs, including the proposed mapping approach, our initial experiments, related discussion, and future work emerging therein. To the best of our knowledge, ours is among the first works to conduct mining on ordinances and tweets for Smart Cities. This work has a broader impact with a vision to enhance Smart City growth.	Mapping Ordinances and Tweets using Smart City Characteristics to Aid Opinion Mining	NA:NA:NA:NA	2018
Marc Spaniol:Ricardo Baeza-Yates:Julien Masanès	Time is a key dimension to understand the Web. It is fair to say that it has not received yet all the attention it deserves and TempWeb is an attempt to help remedy this situation by putting time as the center of its reflection. Studying time in this context actually covers a large spectrum, from the extraction of temporal information and knowledge, to diachronic studies for the design of infrastructural and experimental settings enabling a proper observation of this dimension.	TempWeb 2018 Chairs' Welcome and Organization	NA:NA:NA	2018
Andreas Spitz:Jannik Strötgen:Michael Gertz	For the temporal analysis of news articles or the extraction of temporal expressions from such documents, accurate document creation times are indispensable. While document creation times are available as time stamps or HTML metadata in many cases, depending on the document collection in question, this data can be inaccurate or incomplete in others. Especially in digitally published online news articles, publication times are often missing from the article or inaccurate due to (partial) updates of the content at a later time. In this paper, we investigate the prediction of document creation times for articles in citation networks of digitally published news articles, which provide a network structure of knowledge flows between individual articles in addition to the contained temporal expressions. We explore the evolution of such networks to motivate the extraction of suitable features, which we utilize in a subsequent prediction of document creation times, framed as a regression task. Based on our evaluation of several established machine learning regressors on a large network of English news articles, we show that the combination of temporal and local structural features allows for the estimation of document creation times from the network.	Predicting Document Creation Times in News Citation Networks	NA:NA:NA	2018
Bowen Zhang:Wing Cheong Lau	Predicting the popularity of a discussion topic in an online social network (OSN) or the responses to an online fund-raising campaign is a practical challenge of immense value. Previous work tries to predict the popularity of an online campaign by modeling information diffusion as a homogeneous temporal point process within a network of a single-type of actors. However, real-world information propagation often involved multiple types of actors. In particular, there are the so-called opinion leaders, e.g. online celebrities or influential OSN users with a huge number of followers, who can create a great impact on the visibility and thus the final popularity of an event by simply mentioning it in their tweets or postings. In this paper, we propose MASEP, a Multi-actor Self-exciting Process, to model and predict the popularity of different online campaigns involving multiple types of actors. MASEP combines a self-exciting branching process with a periodical decay process to capture the dynamics and interdependent relationship between opinion leaders and ordinary users during an online campaign. A closed-form expression is derived for the temporal campaign popularity under the MASEP model. Based on this closed-form expression, we can efficiently perform regression against the empirical activity measurements of an online campaign during its early stage to estimate the parameters of the corresponding MASEP model. The final popularity of the campaign can then be predicted. To demonstrate the efficacy of the MASEP-based approach, we apply it to predict the popularity of three types of online campaigns from different large-scale real-world datasets, namely, the total number of posts in retweeting cascades, the overall count of individual hashtags in posting streams, and the final number of sponsors for crowd-funding campaigns. In particular, using the initial 30% of each campaign data trace for training, our approach can achieve absolute prediction error (APE) of 13.25%, 15.7%, and 36.9% respectively for datasets of 3 different types of campaigns. This corresponds to a 26.1% to 63.2% reduction in prediction error when comparing to state-of-the-art approaches including SEISMIC, SpikeM, and STRM.	Temporal Modeling of Information Diffusion using MASEP: Multi-Actor Self-Exciting Processes	NA:NA	2018
Behrooz Mansouri:Mohammad Sadegh Zahedi:Ricardo Campos:Mojgan Farhoodi:Alireza Yari	The development of information retrieval algorithms and temporal information retrieval ones has been extensively carried out over the last few years. While several studies have been conducted, most of these researches relate to English, leading to a lack of knowledge in several other important languages. This includes the Persian one. In this work, we aim to shorten this gap by contributing, disseminating and enlarging the knowledge we have on temporal information retrieval aspects in Persian, which is one of the dominant languages in the Middle East, widely spoken in several countries. To achieve this objective, we propose to understand the use of temporal expressions on a large-scale Persian search engine query log consisting of 27M queries. In particular, we focus on explicit (e.g., June 2017) and relative temporal expressions (e.g., tomorrow) and try to understand (1) how often temporal expressions are used in web queries; (2) which type of temporal expressions (Date, Time, Duration and Set) are commonly used; (3) to which time (past, current or future) do temporal expressions mostly refer to; (4) to which category they often belong; (5) how often do user's reformulate their queries by adding temporal expressions; and (6) how using temporal expressions affects user's satisfaction. We believe that answering these questions may be beneficial for a large number of tasks including, user's behavior understanding and search engines' improvement effectiveness.	Understanding the use of Temporal Expressions on Persian Web Search	NA:NA:NA:NA:NA	2018
Henry S. Thompson:Jian Tong	We report here on the results of two studies using two and four monthly web crawls respectively from the Common Crawl (CC) initiative between 2014 and 2017, whose initial goal was to provide empirical evidence for the changing patterns of use of so-called persistent identifiers. This paper focusses on the tooling needed for dealing with CC data, and the problems we found with it. The first study is based on over 10^12 URIs from over 5 x 10^9 pages crawled in April 2014 and April 2017, the second study adds a further 3 x 10^9 pages from the April 2015 and April 2016 crawls. We conclude with suggestions on specific actions needed to enable studies based on CC to give reliable longitudinal information.	Can Common Crawl Reliably Track Persistent Identifier (PID) Use Over Time	NA:NA	2018
Melisachew Wudage Chekol:Heiner Stuckenschmidt	The emergence of open information extraction as a tool for constructing and expanding knowledge graphs has aided the growth of temporal data, for instance, YAGO, NELL and Wikidata. While YAGO and Wikidata maintain the valid time of facts, NELL records the time point at which a fact is retrieved from some Web corpora. Collectively, these knowledge graphs (KGs) store facts extracted from Wikipedia and other sources. Due to the imprecise nature of the extraction tools that are used to build and expand KGs, such as NELL, the facts in the KGs are weighted (a confidence value representing the correctness of a fact). Additionally, NELL can be considered as a transaction time KG because every fact is associated with extraction date. On the other hand, YAGO and Wikidata use the valid time model because they only maintain facts together with their validity time (temporal scope). In this paper, we propose a bitemporal model (that combines transaction and valid time models) for maintaining and querying probabilistic temporal knowledge graphs. We report our evaluation results of the proposed approach.	Towards Probabilistic Bitemporal Knowledge Graphs	NA:NA	2018
Behrooz Mansouri:Mohammad Sadegh Zahedi:Ricardo Campos:Mojgan Farhoodi:Maseud Rahgozar	Web searches are done by users every day on a million-daily basis. Many of these web searches are related to events, social occasions that attracts society's attention. Events may happen multiple times on cyclic or non-periodic occasions. These are known as spiky events. When these events occur, multiple spikes can be observed in query logs triggered by a change in the user's behaviour and an increase in the frequency of the user's queries. In this paper, we aim to understand the user's search behaviour towards this kind of events. To this regard, we propose a new taxonomy of spiky events which categorizes queries into two groups: periodic (ongoing, historical, traditional) and aperiodic (predictable and unpredictable), and study how various features concerning the query and the clicked web pages describe the user's behaviour, before, during, and after the event. To conduct this research, we consider 100 spiky events and rely on a two-year Persian search engine query log to analyse their related queries and associated information. The results obtained show that users have a different behaviour regarding the query frequency, length and temporality, depending on the category of the spiky event and that query formulation and clicked pages are also different for each category before, during and after the event. Understanding these user's behaviours and their relationship with the different categories may play an important role for any search engine looking to provide better services for their users.	Understanding User's Search Behavior towards Spiky Events	NA:NA:NA:NA:NA	2018
Julien Leblay:Melisachew Wudage Chekol	Knowledge Graphs (KGs) are a popular means to represent knowledge on the Web, typically in the form of node/edge labelled directed graphs. We consider temporal KGs, in which edges are further annotated with time intervals, reflecting when the relationship between entities held in time. In this paper, we focus on the task of predicting time validity for unannotated edges. We introduce the problem as a variation of relational embedding. We adapt existing approaches, and explore the importance example selection and the incorporation of side information in the learning process. We present our experimental evaluation in details.	Deriving Validity Time in Knowledge Graph	NA:NA	2018
Robert West:Leila Zia:Dario Taraborelli:Jure Leskovec	It is our great pleasure to welcome you to the Wiki Workshop at the Web Conference 2018. The goal of this workshop is to bring together researchers exploring all aspects of Wikimedia projects such as Wikipedia, Wikidata, and Wikimedia Commons. With members of the Wikimedia Foundation's Research team on the organizing committee and with the experience of successful workshops in 2015, 2016, and 2017, we aim to continue facilitating a direct pathway for exchanging ideas between the organization that coordinates Wikimedia projects and the researchers interested in studying them. We received 17 paper submissions (and counting) from all around the world covering a broad range of topics. Our program committee evaluated them regarding relevance, quality, and novelty, selecting 8 to be presented as posters and published in the workshop proceedings, and numerous others for poster presentation only.	Welcome on Behalf of the Wiki Workshop Chairs & Organization	NA:NA:NA:NA	2018
Christoph Hube:Besnik Fetahu	Quality in Wikipedia is enforced through a set of editing policies and guidelines recommended for Wikipedia editors. Neutral point of view (NPOV) is one of the main principles in Wikipedia, which ensures that for controversial information all possible points of view are represented proportionally. Furthermore, language used in Wikipedia should be neutral and not opinionated. However, due to the large number of Wikipedia articles and its operating principle based on a voluntary basis of Wikipedia editors; quality assurances and Wikipedia guidelines cannot always be enforced. Currently, there are more than 40,000 articles, which are flagged with NPOV or similar quality tags. Furthermore, these represent only the portion of articles for which such quality issues are explicitly flagged by the Wikipedia editors, however, the real number may be higher considering that only a small percentage of articles are of good quality or featured as categorized by Wikipedia. In this work, we focus on the case of language bias at the sentence level in Wikipedia. Language bias is a hard problem, as it represents a subjective task and usually the linguistic cues are subtle and can be determined only through its context. We propose a supervised classification approach, which relies on an automatically created lexicon of bias words, and other syntactical and semantic characteristics of biased statements. We experimentally evaluate our approach on a dataset consisting of biased and unbiased statements, and show that we are able to detect biased statements with an accuracy of 74%. Furthermore, we show that competitors that determine bias words are not suitable for detecting biased statements, which we outperform with a relative improvement of over 20%.	Detecting Biased Statements in Wikipedia	NA:NA	2018
Vevake Balaraman:Simon Razniewski:Werner Nutt	The collaborative knowledge base Wikidata is the central storage of Wikimedia projects, containing over 45 million data items. It acts as the hub for interlinking Wikipedia pages about a specific item in different languages, automates features such as infoboxes in Wikipedia, and is increasingly used for other applications such as data enrichment and question answering. Tracking the quality of Wikidata is an important issue for this project. In this paper we focus particularly on the completeness aspect. Several automated techniques have been adopted by Wikis to track and manage completeness, yet these techniques are generally subjective and do not provide a clear quality estimate at the level of entities. In this paper, we present an approach towards measuring Relative Completeness in Wikidata by comparison with data present for similar entities. This relative completeness approach is easily scalable with the introduction of new classes in the knowledge base, and has been implemented for all available entities in Wikidata. The results provide an intuition on the completeness of an entity comparing it with other similar entities. Here, we present our implementation approach along with a discussion on strategies and open challenges.	Recoin: Relative Completeness in Wikidata	NA:NA:NA	2018
Lijun Lyu:Besnik Fetahu	Wikipedia is one of the top visited resources on the Web, furthermore, it is used extensively as the main source of information for applications like Web search, question & answering etc. This is mostly attributed to Wikipedia's coverage in terms of topics and real-world entities and the fact that Wikipedia articles are constantly updated with new and emerging facts. However, only a small fraction of articles are considered to be of good quality. The large majority of articles are incomplete and have other quality issues. A strong quality indicator is the presence of external references from third-party sources (e.g. news sources) as suggested by the verifiability principle in Wikipedia. Even for the existing references in Wikipedia there is an inherent lag in terms of the publication time of cited resources and the time they are cited in Wikipedia articles. We propose a near real-time suggestion of news references for Wikipedia from a daily news stream. We model daily news into specific events, spanning from a day up to year. Thus, we construct an event-chain from which we determine when the information in an event has converged and consequentially based on a learning-to-rank approach suggest the most authoritative and complete news article to Wikipedia articles involved in a specific event. We evaluate our news suggestion approach on a set of 41 events extracted from Wikipedia currents event portal, and on new corpus consisting of daily news between the period of 2016-2017 with more than 14 million news articles. We are able to suggest news articles to Wikipedia pages with an overall accuracy of MAP=0.77 and with a minimal lag w.r.t the publication time of the news article.	Real-time Event-based News Suggestion for Wikipedia Pages from News Streams	NA:NA	2018
Thomas Pellissier Tanon:Lucie-Aimée Kaffee	Stability in Wikidata's schema is essential for the reuse of its data. In this paper, we analyze the stability of the data based on the changes in labels of properties in six languages. We find that the schema is overall stable, making it a reliable resource for external usage.	Property Label Stability in Wikidata: Evolution and Convergence of Schemas in Collaborative Knowledge Bases	NA:NA	2018
Laxmi Amulya Gundala:Francesca Spezzano	In this paper, we describe our on-going research on the problem of predicting needed hyperlinks between pairs of Wikipedia pages (u,v) that are not connected, yet show readers' search navigation from u to v. We propose a solution that first estimates how long will these searches last and then predicts new hyperlinks according to descending order of duration. Our initial experimental results show that our best solution achieves an AUROC of 0.77 on the Wikipedia Clickstream dataset and a [email protected]% of 1.0 and significantly beats the baselines.	Readers' Demanded Hyperlink Prediction in Wikipedia	NA:NA	2018
Finn Årup Nielsen	The linkage of ImageNet WordNet synsets to Wikidata items will leverage deep learning algorithm with access to a rich multilingual knowledge graph. Here I will describe our on-going efforts in linking the two resources and issues faced in matching the Wikidata and WordNet knowledge graphs. I show an example on how the linkage can be used in a deep learning setting with real-time image classification and labeling in a non-English language and discuss what opportunities lies ahead.	Linking ImageNet WordNet Synsets with Wikidata	NA	2018
Sebastián Ferrada:Nicolás Bravo:Benjamin Bustos:Aidan Hogan	Despite its importance to the Web, multimedia content is often neglected when building and designing knowledge-bases: though descriptive metadata and links are often provided for images, video, etc., the multimedia content itself is often treated as opaque and is rarely analysed. IMGpedia is an effort to bring together the images of Wikimedia Commons (including visual information), and relevant knowledge-bases such as Wikidata and DBpedia. The result is a knowledge-base that incorporates similarity relations between the images based on visual descriptors, as well as links to the resources of Wikidata and DBpedia that relate to the image. Using the IMGpedia SPARQL endpoint, it is then possible to perform visuo-semantic queries, combining the semantic facts extracted from the external resources and the similarity relations of the images. This paper presents a new web interface to browse and explore the dataset of IMGpedia in a more friendly manner, as well as new visuo-semantic queries that can be answered using 6 million recently added links from IMGpedia to Wikidata. We also discuss future directions we foresee for the IMGpedia project.	Querying Wikimedia Images using Wikidata Facts	NA:NA:NA:NA	2018
Tomás Sáez:Aidan Hogan	Info-boxes provide a summary of the most important meta-data relating to a particular entity described by a Wikipedia article. However, many articles have no info-box or have info-boxes with only minimal information; furthermore, there is a huge disparity between the level of detail available for info-boxes in English articles and those for other languages. Wikidata has been proposed as a central repository of facts to try to address such disparities, and has been used as a source of information to generate info-boxes. However, current processes still rely on human intervention either to create generic templates for entities of a given type or to create a specific info-box for a specific article in a specific language. As such, there are still many articles of Wikipedia without info-boxes but where relevant data are provided by Wikidata. In this paper, we investigate fully automatic methods to generate info-boxes for Wikipedia from the Wikidata knowledge graph. The primary challenge is to create ranking mechanisms that provide an intuitive prioritisation of the facts associated with an entity. We discuss this challenge, propose several straightforward metrics to prioritise information in info-boxes, and present an initial user evaluation to compare the quality of info-boxes generated by various metrics.	Automatically Generating Wikipedia Info-boxes from Wikidata	NA:NA	2018
Payam Barnaghi:Jean-Paul Calbimonte:Daniele Dell'Aglio	Applications in different domains require reactive processing of massive, dynamically generated streams of data. This trend is increasingly visible also on the Web, where more and more streaming sources are becoming available. These originate from social networks, sensor networks, the Internet of Things (IoT) and many other technologies that use the Web as a platform for sharing data. This has resulted in new Web-centric efforts such as the Web of Things (WoT), which focuses on exposing and describing the IoT resources on the Web; or the Social Web which provides protocols, vocabularies, and APIs to facilitate access to social communications and interactions on the Web.	Web Stream Processing Workshop Chairs' Welcome & Organization	NA:NA:NA	2018
Mathias De Brouwer:Femke Ongenae:Glenn Daneels:Esteban Municio:Jeroen Famaey:Steven Latré:Filip De Turck	Enabling real-time collection and analysis of cyclist sensor data could allow amateur cyclists to continuously monitor themselves, receive personalized feedback on their performance, and communicate with each other during cycling events. Semantic Web technologies enable intelligent consolidation of all available context and sensor data. Stream reasoning techniques allow to perform advanced processing tasks by correlating the consolidated data to enable personalized and context-aware real-time feedback. In this paper, these technologies are leveraged and evaluated to design a Proof-of-Concept application of a personalized real-time feedback platform for amateur cyclists. Real-time feedback about the user's heart rate and heart rate training zones is given through a web application. The performance and scalability of the platform is evaluated on a Raspberry Pi. This shows the potential of the framework to be used in real-life cycling by small groups of amateur cyclists, who can only access low-end devices during events and training.	Personalized Real-Time Monitoring of Amateur Cyclists on Low-End Devices: Proof-of-Concept & Performance Evaluation	NA:NA:NA:NA:NA:NA:NA	2018
Maria Bermudez-Edo:Payam Barnaghi	The data gathered from smart cities can help citizens and city manager planners know where and when they should be aware of the repercussions regarding events happening in different parts of the city. Most of the smart city data analysis solutions are focused on the events and occurrences of the city as a whole, making it difficult to discern the exact place and time of the consequences of a particular event. We propose a novel method to model the events in a city in space and time. We apply our methodology for vehicular traffic data basing our models in (convolutional) neuronal networks.	Spatio-Temporal Analysis for Smart City Data	NA:NA	2018
Julián Andrés Rojas Meléndez:Brecht Van de Vyvere:Arne Gevaert:Ruben Taelman:Pieter Colpaert:Ruben Verborgh	For smart decision making, user agents need live and historic access to open data from sensors installed in the public domain. In contrast to a closed environment, for Open Data and federated query processing algorithms, the data publisher cannot anticipate in advance on specific questions, nor can it deal with a bad cost-efficiency of the server interface when data consumers increase. When publishing observations from sensors, different fragmentation strategies can be thought of depending on how the historic data needs to be queried. Furthermore, both publish/subscribe and polling strategies exist to publish live updates. Each of these strategies come with their own trade-offs regarding cost-efficiency of the server-interface, user-perceived performance and cpu use. A polling strategy where multiple observations are published in a paged collection was tested in a proof of concept for parking spaces availability. In order to understand the different resource trade-offs presented by publish/subscribe and polling publication strategies, we devised an experiment on two machines, for a scalability test. The preliminary results were inconclusive and suggest more large scale tests are needed in order to see a trend. While the large-scale tests will be performed in future work, the proof of concept helped to identify the technical Open Data principles for the 13 biggest cities in Flanders.	A Preliminary Open Data Publishing Strategy for Live Data in Flanders	NA:NA:NA:NA:NA:NA	2018
Gustavo Gonçalves:Flávio Martins:João Magalhães	The rise of large data streams introduces new challenges regarding the delivery of relevant content towards an information need. This need can be seen as a broad topic of information. By identifying sub-streams within a broader data stream, we can retrieve relevant content that matches the multiple facets of the topic; thus summarizing information, and matching the initial need. In this paper, we propose to study the generation of sub-streams over time and compare various aggregation methods to summarize information. Our experiments were made using the standard TREC Real-Time Summarization (RTS) 2017 dataset.	Analysis of Subtopic Discovery Algorithms for Real-time Information Summarization	NA:NA:NA	2018
Danh Le-Phuoc	The join operator is a core component of an RDF Stream Processing engine. The join operations usually dominate the processing load of a query execution plan. Due to the constantly updating nature of continuous queries, the query optimiser has to frequently change the optimal execution plan for a query. However, optimising the join executing plan for every execution step might be prohibitively expensive, hence, dynamic optimisation of continuous join operations is still a challenging problem so far. Therefore, this paper proposes the first adaptive optimisation approach towards this problem in the context of RDF Stream Processing. The approach comes with two dynamic cost-based optimisation algorithms which use a light-weight process to search for the best execution plan for every execution step. The experiments show the encouraging results towards this direction.	Adaptive Optimisation For Continuous Multi-Way Joins Over RDF Streams	NA	2018
Erik Wilde:Mike Amundsen:Mehdi Medjaoui	Welcome to the 9th International Workshop on Web APIs and Service Architecture (WS-REST). First held in 2010 at WWW in Raleigh, North Carolina, USA, this 2018 edition of the WS-REST Workshop Series is proud to be a part of the renowned WWW conference series in Lyon, France. WS-REST 2018 brings together a community of researcher and practitioners interested in Web APIs and service architecture. Bringing Research and Industry Together In keeping with the history of WS-REST events, the 2018 edition strives to bring together vital content from both the Web Services and REST communities. This year we are hosting two individual tracks (Research and Industry) as a way to continue and strengthen this collaboration between academic and applied experience. The research track submissions received careful peer-review and will be published in the Web Conference proceedings. Industry track submissions focus on field-tested examples and use-cases in the form of extended abstracts or position papers and were selected by the organizers with advice from select program committee members. APIs, Services, and REST APIs have become the connective fabric of the Web and any application area that uses Internet or Web technologies. The goal of WS-REST 2018 is to provide a forum for researchers and practitioners where they can openly and freely exchange ideas about how they are using Web technologies in their APIs, what works and what does not work for them, and what challenges they see in the current landscape of standards and technologies. Our goal is to capture both the state of the art when it comes to Web APIs and service architecture, but to also provide a forum that identifies some of the most pressing issues in that space, and can help solving them.	Workshop on Web APIs and Service Architecture (WS-REST) Chairs' Welcome & Organization	NA:NA:NA	2018
Anastasios Dimanidis:Kyriakos C. Chatzidimitriou:Andreas L. Symeonidis	Speeding up the development process of Web Services, while adhering to high quality software standards is a typical requirement in the software industry. This is why industry specialists usually suggest "driven by" development approaches to tackle this problem. In this paper, we propose such a methodology that employs Specification Driven Development and Behavior Driven Development in order to facilitate the phases of Web Service requirements elicitation and specification. Furthermore, we introduce gherkin2OAS, a software tool that aspires to bridge the aforementioned development approaches. Through the suggested methodology and tool, one may design and build RESTful services fast, while ensuring proper functionality.	A Natural Language Driven Approach for Automated Web API Development: Gherkin2OAS	NA:NA:NA	2018
Tobias Fertig:Peter Braun	Representational State Transfer (REST) is an efficient and by now established architectural style for distributed hypermedia systems. However, REST has not been designed for offline operations, yet many applications must also keep functioning when going offline for more than a few seconds. Burdening the programmer with knowledge about offline status is undesirable. RESTful applications can be described by a formal model. Therefore, we define a function to derive a formal model of the proxy for handling offline support on the client-side. We then extend existing caching approaches so that a client-side proxy can transparently hide the offline status from the application. We validate our solution with a proxy layer that covers all test cases derived from the model. Using our model and proxy, clients do not have to know and worry about whether they are online or offline.	Towards Offline Support for RESTful Systems	NA:NA	2018
Henry Vu:Tobias Fertig:Peter Braun	Being an architectural style rather than a specification or a standard, the proper design of REpresentational State Transfer (REST) APIs is not trivial, since developers have to deal with a flood of recommendations and best practices, especially the proper application of the hypermedia constraint requires some decent experience. Furthermore, testing RESTful APIs is a missing topic within literature and especially, hypermedia testing is not mentioned at all. To deal with this state of affairs, we have elaborated a Model-Driven Software Development (MDSD) approach for creating RESTful APIs. As this project matured, we also explored the possibility of Model-Driven Testing (MDT). This work addresses the challenges of hypermedia testing and proposes approaches to overcome them with MDT techniques. We present the results of hypermedia testing for RESTful APIs using a model verification approach that were discovered within our research. MDT enables the verification of the underlying model of a RESTful API and ensuring its correctness before initiating any code generation. Therefore, we can prevent a poorly designed model from being transformed into a poorly designed RESTful API.	Verification of Hypermedia Characteristic of RESTful Finite-State Machines	NA:NA:NA	2018
Sebastian R. Bader:Maria Maleshkova	A central vision of the Internet of Things is the representation of the physical world in a consistent virtual environment. Especially in the context of smart factories the connection of the different, heterogeneous production modules through a digital shop floor promises faster conversion rates, data-driven maintenance or automated machine configurations for use cases which haven't been recognized at design time. Nevertheless, these scenarios demand IoT representations of all participating machines and components, which requires high installation efforts and hardware adjustments. We propose an incremental process for bringing the shop floor closer to the IoT vision. Currently the majority of systems, components or parts are not yet connected with the internet and might not even provide the possibility to be technically equipped with sensors. However, those could be essential parts for a realistic digital shop floor representation. We therefore propose Virtual Representations, which are capable of independently calculating a physical object's condition by dynamically collecting and interpreting already available data through RESTful Web APIs. The internal logic of such Virtual Representations are further adjustable at runtime since changes to its respective physical object, its environment or updates to the resource itself should not cause any downtime.	Virtual Representations for an Iterative IoT Deployment	NA:NA	2018
Akshay Soni:Aasish Pappu:Robert Busa-Fekete:Krzysztof Dembczynski	Extreme Multilabel Classification (XMLC) is a very active and rapidly growing research area that deals with the problem of labeling an item with a small set of tags out of an extremely large number of potential tags. Applications include content understanding, document tagging, image tagging, biological sequence tagging, recommendation, etc. While the difficulty and the potential applications of XMLC are well understood in the core machine learning community, to the best of our knowledge, XMLC has not made inroads in the field of Information Retrieval (IR) and related areas. The aim of this workshop is to bring researchers from academia and industry in order to further advance this very exciting field and come up with potential applications of XMLC in new areas.	Extreme Multilabel Classification for Social Media Chairs' Welcome and Organization	NA:NA:NA:NA	2018
Anshumali Shrivastava	In this talk, I will present Merged-Averaged Classifiers via Hashing (MACH) for K-classification with ultra-large values of K. Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(dłogK)$ (d is dimensionality) memory while only requiring $O(KłogK + dłogK )$ operation for inference. MACH is a generic K-classification algorithm, with provably theoretical guarantees, without any assumption on the relationship between classes. MACH uses universal hashing to reduce classification with a large number of classes to few (logarithmic many) independent classification tasks with small (constant) number of classes. I will show the first quantification of discriminability-memory tradeoff in multi-class classification. Using the simple idea of hashing, we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU, with the classification accuracy of 19.28%, which is the best-reported accuracy on this dataset. Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (160 GB model size) and achieves 9% accuracy. In contrast, MACH can achieve 9% accuracy with 480x reduction in the model size (of mere 0.3GB). With MACH, we also demonstrate complete training of feature extracted fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU. To the best of our knowledge, this is the first work to demonstrate complete training of these extreme-class datasets on a single Titan X. Furthermore, the algorithm is trivially parallelizable. Our experiments show that we can train ODP datasets in 7 hours on a single GPU or in 15 minutes with 25 GPUs. Similarly, we can train classifiers over the fine-grained imagenet dataset in 24 hours on a single GPU which can be reduced to little over 1 hour with 20 GPUs.	Training 100,000 Classes on a Single Titan X in 7 Hours or 15 Minutes with 25 Titan Xs	NA	2018
Manik Varma	I will introduce extreme classi cation which is a new area of machine learning research focusing on multi-class & multi-label problems involving millions of categories. Extreme classification has opened up a new paradigm for thinking about key applications such as tagging, ranking and recommendation. I will discuss algorithms for some of these applications and present results on tagging on Wikipedia, product recommendation on Amazon and search and advertising on the Bing search engine. More details can be found on The Extreme Classification Repository webpage at http://manikvarma.org/downloads/XC/XMLRepository.html	Extreme Classification: Tagging on Wikipedia, Recommendation on Amazon & Advertising on Bing	NA	2018
Marius Kloft	Training of multi-class or multi-label classification machines are embarrassingly parallelizable via the one-vs.-rest approach. However, training of all-in-one multi-class learning machines such as multinomial logistic regression or all-in-one multi-class SVMs (MC-SVMs) is not parallelizable out of the box. In my talk, I present optimization strategies to distribute the training of some all-in-one multi-class SVMs over the classes, which makes them appealing for the use in extreme classification.	Distributed Optimization of All-in-one SVMs for Extreme Classfication	NA	2018
Lu Tang:Sougata Chaudhuri:Abraham Bagherjeiran:Ling Zhou	Structured prediction, where outcomes have a precedence order, lies at the heart of machine learning for information retrieval, movie recommendation, product review prediction, and digital advertising. Ordinal ranking, in particular, assumes that the structured response has a linear ranked order. Due to the extensive applicability of these models, substantial research has been devoted to understanding them, as well as developing efficient training techniques. One popular and widely cited technique of training ordinal ranking models is to exploit the linear precedence order and systematically reduce it to a binary classification problem. This facilitates the usage of readily available, powerful binary classifiers, but necessitates an expansion of the original training data, where the training data increases by $K-1$ times of its original size, with K being the number of ordinal classes. Due to prevalent nature of problems with large number of ordered classes, the reduction leads to datasets which are too large to train on single machines. While approximation methods like stochastic gradient descent are typically applied here, we investigate exact optimization solutions that can scale. In this paper, we present a divide-and-conquer (DC) algorithm, which divides large scale binary classification data into a cluster of machines and trains logistic models in parallel, and combines them at the end of the training phase to create a single binary classifier, which can then be used as an ordinal ranker. It requires no synchronization between the parallel learning algorithms during the training period, which makes training on large datasets feasible and efficient. We prove consistency and asymptotic normality property of the learned models using our proposed algorithm. We provide empirical evidence, on various ordinal datasets, of improved estimation and prediction performance of the model learnt using our algorithm, over several standard divide-and-conquer algorithms.	Learning Large Scale Ordinal Ranking Model via Divide-and-Conquer Technique	NA:NA:NA:NA	2018
Kishaloy Halder:Lahari Poddar:Min-Yen Kan	In public online discussion forums, the large user base and frequent posts can create challenges for recommending threads to users. Importantly, traditional recommender systems, based on collaborative filtering, are not capable of handlingnever-seen-before items (threads). We can view this task as a form of Extreme Multi-label Classification (XMLC), where for a newly-posted thread, we predict the set of users (labels) who will want to respond to it. Selecting a subset of users from the set of all users in the community poses significant challenges due to scalability, and sparsity. We propose a neural network architecture to solve thisnew thread recommendation task. Our architecture uses stacked bi-directional Gated Recurrent Units (GRU) for text encoding along with cluster sensitive attention for exploiting correlations among the large label space. Experimental evaluation with four datasets from different domains show that our model outperforms both the state-of-the-art recommendation systems as well as other XMLC approaches for this task in terms of MRR, Recall, and NDCG.	Cold Start Thread Recommendation as Extreme Multi-label Classification	NA:NA:NA	2018
Mathieu d'Aquin:Elena Cabrio	It is our great pleasure to welcome you to the WWW 2018 Challenges Track. It is the first time that the WWW conference includes such a track, which aim was to showcase the maturity of the state of the art on tasks common to the Web community and adjacent academic communities, in a controlled setting of rigorous evaluation. Through our call for challenge organisation, we also wanted to see which open questions might be seen as most relevant in this community today, and how groups of researchers might come together around shared resources (e.g. datasets) to address those questions in a hands-on, practical way. We received five proposals for challenges, and selected four of them, which were proposed by well established researchers in their respective domains. The topics addressed varied from being purely focused on a domain specific task, without constraints on the technical approach to take (e.g. music genre recognition) to fundamentally technical tasks, which can be seen as useful across domains (question answering), with two of the challenges representing a mix of both.	Challenges Track Chairs' Welcome	NA:NA	2018
Michaël Defferrard:Sharada P. Mohanty:Sean F. Carroll:Marcel Salathé	We here summarize our experience running a challenge with open data for musical genre recognition. Those notes motivate the task and the challenge design, show some statistics about the submissions, and present the results.	Learning to Recognize Musical Genre from Audio: Challenge Overview	NA:NA:NA:NA	2018
Benjamin Murauer:Günther Specht	This paper summarizes our contribution to the CrowdAI music genre classification challenge "Learning to Recognise Musical Genre from Audio on the Web'' as part of the WebConference 2018. We utilize different approaches from the field of music analysis to predict the music genre of given mp3 music files, including a convolutional neural network for spectrogram classification, deep neural networks and ensemble methods using various numerical audio features. Our best results were obtained by an extreme gradient boosting classifier.	Detecting Music Genre Using Extreme Gradient Boosting	NA:NA	2018
Jaehun Kim:Minz Won:Xavier Serra:Cynthia C. S. Liem	The automated recognition of music genres from audio information is a challenging problem, as genre labels are subjective and noisy. Artist labels are less subjective and less noisy, while certain artists may relate more strongly to certain genres. At the same time, at prediction time, it is not guaranteed that artist labels are available for a given audio segment. Therefore, in this work, we propose to apply the transfer learning framework, learning artist-related information which will be used at inference time for genre classification. We consider different types of artist-related information, expressed through artist group factors, which will allow for more efficient learning and stronger robustness to potential label noise. Furthermore, we investigate how to achieve the highest validation accuracy on the given FMA dataset, by experimenting with various kinds of transfer methods, including single-task transfer, multi-task transfer and finally multi-task learning.	Transfer Learning of Artist Group Factors to Musical Genre Classification	NA:NA:NA:NA	2018
Amelie Gyrard:Manas Gaur:Swati Padhee:Amit Sheth:Mihaela Juganaru-Mathieu	The Web of Things (WoT) is an extension of the Internet of Things (IoT) to ease the access to data generated by things/devices using the benefits of Web technologies. Data is exploited by WoT applications to monitor healthcare or even control home automation devices. The purpose of the Knowledge Extraction for the Web of Things (KE4WoT) challenge is to automatically extract the relevant knowledge from already designed smart WoT applications in various applicative domains. Those applications design and release Knowledge Bases (e.g., datasets and/or models) on the web.	Knowledge Extraction for the Web of Things (KE4WoT): WWW 2018 Challenge Summary	NA:NA:NA:NA:NA	2018
Fabricio F. de Faria:Ricardo Usbeck:Alessio Sarullo:Tingting Mu:Andre Freitas	This challenge focuses on the use of semantic representation methods to support Visual Question Answering: given a large image collection, find a set of images matching natural language queries. The task supports advancing the state-of-the-art in Visual Question Answering by focusing on methods which explore the interplay between contemporary machine learning techniques, semantic representation and reasoning mechanisms.	Question Answering Mediated by Visual Clues and Knowledge Graphs	NA:NA:NA:NA:NA	2018
Macedo Maia:Siegfried Handschuh:André Freitas:Brian Davis:Ross McDermott:Manel Zarrouk:Alexandra Balahur	The growing maturity of Natural Language Processing (NLP) techniques and resources is dramatically changing the landscape of many application domains which are dependent on the analysis of unstructured data at scale. The finance domain, with its reliance on the interpretation of multiple unstructured and structured data sources and its demand for fast and comprehensive decision making is already emerging as a primary ground for the experimentation of NLP, Web Mining and Information Retrieval (IR) techniques for the automatic analysis of financial news and opinions online. This challenge focuses on advancing the state-of-the-art of aspect-based sentiment analysis and opinion-based Question Answering for the financial domain.	WWW'18 Open Challenge: Financial Opinion Mining and Question Answering	NA:NA:NA:NA:NA:NA:NA	2018
Chung-Chi Chen:Hen-Hsen Huang:Hsin-Hsi Chen	This paper decribes our experimental methods and results in FiQA 2018 Task 1. There are two subtasks : (1) to predict continuous sentiment score between -1 to 1, and (2) to determine which aspect(s) are related to the content of financial tweets. First, we propose a preprocessing procedure for decomposing financial tweets. Second, we collect over 334K labeled financial tweets to enlarge the scale of the experiments. Third, the sentiment prediction task is separated into two steps in this paper, i.e., (1) bullish/bearish and (2) sentiment degree. We compare the results of the CNN, CRNN and Bi-LSTM models. Besides, we further combine the results of the best models in both steps as the model of subtask 1. Finally, we make an investigation of aspects in depth, and propose some clues for dealing with the 14 aspects.	Fine-Grained Analysis of Financial Tweets	NA:NA:NA	2018
Shijia E.:Li Yang:Mohan Zhang:Yang Xiang	Aspect-based financial sentiment analysis, which aims to classify the text instance into a pre-defined aspect class and predict the sentiment score for the mentioned target. In this paper, we propose a neural network model, Attention-based LSTM model with the Aspect information (ALA), to solve the financial opinion mining problem introduced by the WWW 2018 shared task. The proposed neural network model can adapt to the financial dataset so that the neural network can effectively understand the semantic information of the short text. We evaluate our model with the 10-fold cross-validation, and we compare our model with a variety of related deep neural network models.	Aspect-based Financial Sentiment Analysis with Deep Neural Networks	NA:NA:NA:NA	2018
Shijia E.:Shiyao Xu:Yang Xiang	The goal of question answering with financial data is selecting sentences as answers from the given documents for a question. The core of the task is computing the similarity score between the question and answer pairs. In this paper, we incorporate statistical features such as the term frequency-inverse document frequency (TF-IDF) and the word overlap in convolutional neural networks to learn optimal vector representations of question-answering pairs. The proposed model does not depend on any external resources and can be easily extended to other domains. Our experiments show that the TF-IDF and the word overlap features can improve the performance of basic neural network models. Also, with our experimental results, we can prove that models based on the margin loss training achieve better performance than the traditional classification models. When the number of candidate answers for each question is 500, our proposed model can achieve 0.622 in Top-1 accuracy (Top-1), 0.654 in mean average precision (MAP), 0.767 in normalized discounted cumulative gain (NDCG), and 0.701 in bilingual evaluation understudy (BLEU). If the number of candidate answers is 30, all the values of the evaluation metrics can reach more than 90%.	Incorporating Statistical Features in Convolutional Neural Networks for Question Answering with Financial Data	NA:NA:NA	2018
Hitkul Jangid:Shivangi Singhal:Rajiv Ratn Shah:Roger Zimmermann	Aspect based sentiment analysis aims to detect an aspect (i.e. features) in a given text and then perform sentiment analysis of the text with respect to that aspect. This paper aims to give a solution for the FiQA 2018 challenge subtask 1. We perform aspect-based sentiment analysis on the microblogs and headlines of financial domain. We use a multi-channel convolutional neural network for sentiment analysis and a recurrent neural network with bidirectional long short-term memory units to extract aspect from a given headline or microblog. Our proposed model produces a weighted average F1 score of 0.69 for the aspect extraction task and predicts sentiment intensity scores with a mean squared error of 0.112 on 10-fold cross validation. We believe that the developed system has direct applications in the financial domain.	Aspect-Based Financial Sentiment Analysis using Deep Learning	NA:NA:NA:NA	2018
Dayan de França Costa:Nadia Felix Felipe da Silva	This paper describes our system which participate in Task 1 of FiQA 2018. The task's focuses was to predict sentiment and aspects of financial microblog posts and headlines. The sentiment analysis for a specific company had to be predicted using a scale between -1 and 1, while the aspect prediction had to be predicted using a set of aspects which was given in train data. We had used Support Vector Regression (SVR) to predict the sentiments in both cases (microblog posts and headlines).	INF-UFG at FiQA 2018 Task 1: Predicting Sentiments and Aspects on Financial Tweets and News Headlines	NA:NA	2018
Guangyuan Piao:John G. Breslin	In this paper, we describe our ensemble approach for sentiment and aspect predictions in the financial domain for a given text. This ensemble approach uses Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) with a ridge regression and a voting strategy for sentiment and aspect predictions, and therefore, does not rely on any handcrafted feature. Based on 5-cross validation on the released training set, the results show that CNNs overall perform better than RNNs on both tasks, and the ensemble approach can boost the performance further by leveraging different types of deep learning approaches.	Financial Aspect and Sentiment Predictions with Deep Neural Networks: An Ensemble Approach	NA:NA	2018
