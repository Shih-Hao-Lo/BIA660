Yoomi Choi	With the advent of media, artists have expanded the domains of visual arts through various new attempts to integrate digital technology with arts. Along with the changes in working environment, nowadays, there have been new attempts to reinterpret old masterpiece using digital media. The initial studies regarding 3D image of oriental painting were mainly focused on non-photorealistic rendering techniques. However, the technique has limitations in expressing or reproducing the unique elements of expression used in oriental paintings. The traditional oriental paintings were not expressed with perspective view and shadow; and in a water ink painting, color expression is also limited because of the unique properties of the ingredient, 'Muk (Ink Stick).' Moreover, the oriental paintings have multiple viewpoints in one scene. Therefore, there are difficulties in using tools such as Maya or 3DMax to express the traditional artworks of the East. In this context, the purpose of this study is to provide alternative solutions to resolve the problems that arise in the process of recreating oriental paintings into 3D image, based on the theoretical framework of spatial arrangement and composition principles. Also, the study reinterpreted the classical works by applying storytelling techniques, which utilize the expandability of time, a main characteristic of visual media. <Inwang Jesaekdo> and <Geumgang Jeondo>, the two major artwork of Gyeomjae Jeong Seon, who is one of the most representative literary artists of the Joseon Dynasty, were reinterpreted with 3D digital image, and new design methods and techniques were applied.	A study on 3D digital image applying oriental painting techniques	NA	2016
Shinichi Higashino:Sakiko Nishi:Ryuuki Sakamoto	In this paper, we present ARTTag, an aesthetic fiducial marker system, of which the design development can be performed with any color, texture, shape, or other features as long as circle pairs are integrated. By utilizing the projective properties of circular features, ARTTag is appropriate for detection, identification, and camera-based registration in augmented reality (AR) applications.	ARTTag: aesthetic fiducial markers based on circle pairs	NA:NA:NA	2016
Suzi Kim:Sunghee Choi	Three-dimensional typography (3D typography) refers to the arrangement of text in three-dimensional space. It injects vitality into the letters, thereby giving the viewer a strong impression that is hard to forget. These days, 3D typography plays an important role in daily life beyond the artistic design. It is easy to observe the 3D typography used in the 3D virtual space such as movie or games. Also it is used frequently in signboard or furniture design. Despite its noticeable strength, most of the 3D typography is generated by just a simple extrusion of flat 2D typography. Comparing with 2D typography, 3D typography is more difficult to generate in short time due to its high complexity.	Automatic generation of 3D typography	NA:NA	2016
Asako Soga:Yuho Yazaki:Bin Umino:Motoko Hirayama	We developed a body-part motion synthesis system (BMSS) that allows users to create short choreographies by synthesizing body-part motions and to simulate them in 3D animation. This system automatically provides various short choreographies. First, users select a base motion and body-part categories. Then the system automatically selects and synthesizes body-part motions to the base motion. The system randomly determined the synthesis timings of the selected motions. Users can use the composed sequences as references for dance creation, learning, and training. We experimentally evaluated our system's effectiveness for supporting dance creation with four professional choreographers of contemporary dance. From our experiment results, we basically verified the usability of BMSS for choreographic creation.	Body-part motion synthesis system for contemporary dance creation	NA:NA:NA:NA	2016
Yuxiao Du:Ergun Akleman	In this work, we have developed an approach to include global illumination effects into charcoal drawing (see Figure 1). Our charcoal shader provides a robust computation to obtain charcoal effect for a wide variety of diffuse and specular materials. Our contributions can be summarized as follows: (1) A Barrycentric shader that is based on degree zero B-spline basis functions; (2) A set of hand-drawn charcoal control texture images that naturally provide desired charcoal look-and-feel; and (3) A painter's hierarchy for handling a high number of shading parameters consistent with charcoal drawing.	Charcoal rendering and shading with reflections	NA:NA	2016
Yaozhun Huang:Sze-Chun Tsang:Miu-Ling Lam	Light painting is a photography technique in which light sources are moved in specific patterns while being captured by long exposure. The movements of lights will result in bright strokes or selectively illuminated and colored areas in the scene being captured, thus decorating the real scene with special visual effects without the need for post-production. Light painting is not only a popular activity for hobbyists to express creativities, but also a practice for professional media artists and photographers to produce aesthetic visual arts and commercial photography. In conventional light paintings, the light sources are usually flashlights or other simple handheld lights made by attaching one or multiple LEDs to a stick or a ring. The patterns created are limited to abstract shapes or freehand strokes.	Computational swept volume light painting via robotic non-linear motion	NA:NA:NA	2016
Takeshi Oozu:Aki Yamada:Yuki Enzaki:Hiroo Iwata	Furniture-device is the device having furniture appearance and physical input and output function. The Escaping Chair is a furniture-device having physical and dynamic interaction with a user to let them perceive the intent of their action and personify the furniture. The Escaping Chair interacts with the bystanders by trying to move away from nearby people. By doing this, the device tries to make the person fail to sit on it, and stimulates their perception about sitting. The idea of a furniture-shaped device was extended from one of my previous artworks, which used furniture as input mechanisms. I exhibited the chair and observed the interaction sit produced with exhibition visitors. It succeeded in making people wonder during the interaction, as I planned, and making them further chase the chair, which indicates a new capability of the device. There were some challenges regarding load tolerance, detection latency and failure, which I have proposed improvements for.	Escaping chair: furniture-shaped device art	NA:NA:NA:NA	2016
Felipe Caputo:Victoria Mc Gowen:Joe Geigel:Steven Cerqueira:Quincey Williams:Marla Schweppe:Zhongyuan Fa:Anastasia Pembrook:Heather Roffe	Farewell to Dawn is a mixed reality dance performance which explores two dancers' voyage from a physical space to a virtual stage and back, as the day passes before them.	Farewell to dawn: a mixed reality dance performance in a virtual space	NA:NA:NA:NA:NA:NA:NA:NA:NA	2016
E. Entem:L. Barthe:M.-P. Cani:M. van de Panne	We present an automatic method to build a layered vector graphics structure ready for animation from a clean-line vector drawing of an organic, smooth shape. Inspiring from 3D segmentation methods, we introduce a new metric computed on the medial axis of a region to identify and quantify the visual salience of a sub-region relative to the rest. This enables us to recursively separate each region into two closed sub-regions at the location of the most salient junction. The resulting structure, layered in depth, can be used to pose and animate the drawing using a regular 2D skeleton.	From drawing to animation-ready vector graphics	NA:NA:NA:NA	2016
Hiroko Iwasaki:Momoko Kondo:Rei Ito:Saya Sugiura:Yuka Oba:Shinji Mizuno	In this paper, we propose a method to interact with virtual shadows through real shadows various physical objects by using two projectors. In our method, the system scans physical objects in front of a projector, generates virtual shadows with CG according to the scan data, and superimposes the virtual shadows to real shadows of the physical objects with the projector. Another projector is used to superimpose virtual light sources inside real shadows. Our method enables us to experience novel interaction with various shadows such as shadows of flower arrangements.	Interaction with virtual shadow through real shadow using two projectors	NA:NA:NA:NA:NA:NA	2016
Yuki Igarashi:Tsubasa Hiyama:Kaoru Arakawa	We propose an interactive system to assist novices in the design and construction of original necklaces. The system consists of two design tools, an interactive drag-and-drop design tool using images of pearls and a design selection tool with an interactive evolutionary computation (IEC) system. The system includes a virtual modeling simulation which allows users to superimpose a necklace design over their own photograph, taken with a web camera. The system also provides a customized construction guide to assist the user with the construction process. We conduct a field trial to demonstrate that non-professional users can design original necklaces using our system.	An interactive system for original necklace design	NA:NA:NA	2016
Ryan Lustig:Balu Adsumilli:David Newman	Image composition for GoPro videos captured in the presence of significant camera motion is a manual and time consuming process. Existing techniques typically fail to automate this process due to the wide-capture field of view and high camera motion of such videos. The proposed method seeks to solve these problems by developing an image registration algorithm for fisheye images without expensive pixel warping or loss of field of view. Background subtraction is performed to extract moving foreground objects, which are noise corrected and then layered on a reference image to build the final composite. The results show marked improvements in accuracy and efficiency for automating image composition.	Motion compensated automatic image compositing for GoPro videos	NA:NA:NA	2016
Caio Brito:Gutenberg Barros:Walter Correia:Veronica Teichrieb:João Marcelo Teixeira	Accessible tactile pictures (ATPs) consist of tactile representations that convey different kinds of messages and present information through the sense of touch. Traditional approaches use contours and patterns, which create a distinct and recognizable shape and enables separate objects to be identified. The success rate for recognizing pictures by touch is much lower than it would be for vision. Besides that, some pictures are more frequently recognized than others. Finally, there is also some variation from individual to individual: while some blind people recognize many images, others recognize few. Auditory support can improve the points listed before, even eliminating the need for sighted assistance.	Multimodal augmentation of surfaces using conductive 3D printing	NA:NA:NA:NA:NA	2016
Junho Choi:Jong Hun Lee:Yong Yi Lee:Yong Hwi Kim:Bilal Ahmed:Moon Gu Son:Min Ho Joo:Kwan H. Lee	Projection mapping has been widely used to efficiently visualize real world objects in various areas such as exhibitions, advertisements, and theatrical performances. To represent the projected content in a realistic manner, the appearance of an object should be taken into consideration. Although there have been various attempts to realistically represent the appearance through digital modeling of appearance materials in computer graphics, it is difficult to combine it with the projection mapping because it takes huge amount of time and requires large space for the measurement. To counteract these challenges of time and space, [Malzbender et al. 2001] present polynomial texture maps (PTM) that can represent the reflectance properties of the surface such as diffuse and shadow artifacts by relighting of the 3D objects according to varying light direction around the object. PTM does not have temporal or spatial constraints requiring only several tens of images of different light directions so that it makes it possible to easily produce an appealing appearance.	Realistic 3D projection mapping using polynomial texture maps	NA:NA:NA:NA:NA:NA:NA:NA	2016
Laura Ferrarello:Kevin Walker	We describe a digital design process that interfaces real-time data with 3D modeling and 3D printing techniques. Digital Impressionism (DI) is a platform that explores new material possibilities, by 3D modeling physical and digital objects, as affected by invisible forces that act upon them in real time. Using a 3D pointcloud as a medium, we describe an experimental project run with students in our programme, incorporating realtime audio data to manipulate 3D physical forms, resulting in new static and dynamic shapes with what we call a hybrid materiality. The modeling platform of DI treats materials as composites, through which substance becomes physical via the digital interference the environment applies to digital forms. We describe dynamic modeling processes through which data enables a new hybrid tectonic made of composite shapes and materials. This abstract introduces the project and describes our methodology and results so far.	The form of sound through hybrid materials	NA:NA	2016
Adam Schuster:Anas Salah Eddin	Hundreds of thousands of stereoscopic view cards (stereoviews) produced in the late 1800s and early 1900s are being digitized in collections and museums worldwide. However, viewing this important part of media technology history requires dedicated stereoscopes or difficult eye exercises. With the recent spread of Consumer VR, a fresh way to view, study, and learn from this vast store of knowledge becomes available. In this project, we present an automated method to process digitized stereoviews and make them suitable for VR viewing.	Vintage VR: a method of processing 19th century stereoviews for display on 21st century VR systems	NA:NA	2016
Pei-Hsuan Tsai:Yu-Hsuan Huang:Yu-Ju Tsai:Hao-Yu Chang:Masatoshi Chang-Ogimoto:Ming Ouhyoung	Users always got bad experiences while using the general virtual reality head-mounted displays (HMDs) because of the low pixel density through optical lenses. For this reason, the narrow field-of-view (FoV) and high pixel density are the main goals we are going to pursue in the near-field video see-through augmented reality (AR) applications with sophisticated operations, such as the biological observation with AR microscope (e.g. Scope+ [Huang et al. 2015]), the AR surgery simulation, and telescope applications. Therefore with high resolution to see tiny objects clearly is the most important concern in this paper.	A modified wheatstone-style head-mounted display prototype for narrow field-of-view video see-through augmented reality	NA:NA:NA:NA:NA:NA	2016
Shinji Mizuno	In this paper, I improve a tabletop stereoscopic 3DCG system with motion parallax so as to use it with two users and share a stereoscopic 3DCG scene together. I develop a method to calculate two users' viewpoints simultaneously by using depth images. I use a 3D-enabled projector to superimpose two 3DCG images for each user, and use active shutter glasses to separate them into individual images for each user. The improved system would be useable for cooperative works and match type games.	A tabletop stereoscopic 3DCG system with motion parallax for two users	NA	2016
Naoki Hashimoto:Daisuke Kobayashi	We propose a dynamic spatial augmented reality (SAR) system with effective machine learning techniques and edge-based object tracking. Real-time 3D pose estimation is the significant problem of projecting images on moving objects. However, camera-based feature detection is difficult, because most targets have a texture-less surface. Image projection and projected images also interfere with detection. Obtaining 3D shape information with stereo-paired cameras [Resch et al. 2016] is still a time-consuming process, and using a depth sensor with IR [Koizumi et al. 2015] is still unstable and have a fatal time-delay for the dynamic SAR. Therefore, we quickly and robustly estimate the 3D pose of the target objects by using effective machine learning with IR images. And by the combined use of high-speed edge-based object tracking, we realize a stable and low-delay SAR for moving objects.	Dynamic spatial augmented reality with a single IR camera	NA:NA	2016
Yun Suen Pai:Benjamin Tag:Benjamin Outram:Noriyasu Vontin:Kazunori Sugiura:Kai Kunze	We present a novel technique of implementing customized hardware that uses eye gaze focus depth as an input modality for virtual reality applications. By utilizing eye tracking technology, our system can detect the point in depth the viewer focusses on, and therefore promises more natural responses of the eye to stimuli, which will help overcoming VR sickness and nausea. The obtained information for the depth focus of the eye allows the utilization of foveated rendering to keep the computing workload low and create a more natural image that is clear in the focused field, but blurred outside that field.	GazeSim: simulating foveated rendering using depth in eye gaze for VR	NA:NA:NA:NA:NA:NA	2016
Akira Ishii:Ippei Suzuki:Shinji Sakamoto:Keita Kanai:Kazuki Takazawa:Hiraku Doi:Yoichi Ochiai	Conventional research on pedestrian navigation systems has explored the possibilities of presenting information to users both visually and aurally. Existing navigation systems require users to recognize information, and then to follow directions as separate, conscious processes, which inevitably require attention to the system. This study proposes a novel method that enables pedestrians to be guided without conscious interaction with a navigational system.	Graphical manipulation of human's walking direction with visual illusion	NA:NA:NA:NA:NA:NA:NA	2016
Mike Lambeta:Matt Dridger:Paul White:Jesslyn Janssen:Ahmad Byagowi	Virtual reality aims to provide an immersive experience to a user, with the help of a virtual environment. This immersive experience requires two key components; one for capturing inputs from the real world, and the other for synthesizing real world outputs based on interactions with the virtual environment. However, a user in a real world environment experiences a greater set of feedback from real world inputs which relate directly to auditory, visual, and force feedback. As such, in a virtual environment, a dissociation is introduced between the user's inputs and the feedback from the virtual environment. This dissociation relates to the discomfort the user experiences with real world interaction. Our team has introduced a novel way of receiving synthesized feedback from the virtual environment through the use of a haptic wheelchair.	Haptic wheelchair	NA:NA:NA:NA:NA	2016
Valentina Feldman	This project is a synthesis of digital paleoart reconstruction, prototype VR pipeline design, and the remediation of structural narrative principles for immersive media. We approach common issues associated with the accurate portrayal of dinosaurs in media, Cinematic Virtual Reality (CVR) production, and the direction of viewer attention in immersive digital environments. After developing and testing a stable CVR workflow, we designed and produced a piece of scientific VR Paleoart content intended for educational outreach. Our production methods include a state-of-the-art CGI dinosaur reconstruction informed by comparative anatomy and biomechanical simulation, stereoscopic spherical rendering, and photographic CVR film production. Our approach is validated through the completion of a CVR documentary about the titanosaur Dreadnoughtus schrani, one of the largest dinosaurs yet discovered. This documentary, starring paleontologist Dr. Ken Lacovara, will be made publicly available for all common VR distribution platforms. Our goal is to make scientific CVR content accessible to an audience of mobile device owners, taking advantage of the VR media disruption to establish new design guidelines for educational media.	Immersive paleoart: reconstructing dreadnoughtus schrani and remediating the science documentary for cinematic virtual reality	NA	2016
MHD Yamen Saraiji:Shota Sugimoto:Charith Lasantha Fernando:Kouta Minamizawa:Susumu Tachi	We propose "Layered Telepresence", a novel method of experiencing simultaneous multi-presence. Users eye gaze and perceptual awareness are blended with real-time audio-visual information received from multiple telepresence robots. The system arranges audio-visual information received through multiple robots into a priority-driven layered stack. A weighted feature map was created based on the objects recognized for each layer, using image-processing techniques, and pushes the most weighted layer around the users gaze in to the foreground. All other layers are pushed back to the background providing an artificial depth-of-field effect. The proposed method not only works with robots, but also each layer could represent any audio-visual content, such as video see-through HMD, television screen or even your PC screen enabling true multitasking.	Layered telepresence: simultaneous multi presence experience using eye gaze based perceptual awareness blending	NA:NA:NA:NA:NA	2016
Naoki Kawai:Cédric Audras:Sou Tabata:Takahiro Matsubara	We propose a method to generate new views of a scene by capturing a few panorama images in real space and interpolating captured images. We describe a procedure for interpolating panoramas captured at four corners of a rectangle area without geometry, and present experimental results including walkthrough in real time. Our image-based method enables walking through space much more easily than using 3D modeling and rendering.	Panorama image interpolation for real-time walkthrough	NA:NA:NA:NA	2016
Fuko Takano:Takafumi Koike	We propose a walk through imaging method for head-mounted display (HMD) named 'PlenoGap'. The method always displays a refocused image on a HMD. The refocused image is generated from a trimmed panorama light field image which is 360° cylindrical and always focused on center of HMD. In addition, we realized walkthrough experience by making some intermediate images between three panorama light field images. User can roam around small area using controller.	PlenoGap: panorama light field viewing for HMD with focusing on gazing point	NA:NA	2016
Mie Sato:Sota Suzuki:Daiki Ebihara:Sho Kato:Sato Ishigaki	Bare hand interaction with a virtual object reduces uncomfortableness with devices mounted on a user's hand. There are some studies on the bare hand interaction[Benko et al. 2012], however a virtual object is supposed to be a hard object or a user touches a physical object during the bare hand interaction. We focus on grasping a virtual object without using any physical object. Grasping is one of the basic movements in manipulating an object and is more difficult than simple movements like touching an object. Because of the bare hand interaction with no physical object, there is no haptic device on a user's hand and so there is no physical feedback to the user. Our challenge is to provide a user with pseudo-softness while grasping a virtual object with a bare hand. We have been developing an AR system that makes it possible for a user to grasp a virtual object with a bare hand[Suzuki et al. 2014]. Using this AR system, we propose visual stimuli that correspond with the user's hand movements, to manipulate the pseudo-softness of a virtual object. Evaluation results show that with the visual stimuli a user feels pseudo-softness while grasping a virtual object with a bare hand.	Pseudo-softness evaluation in grasping a virtual object with a bare hand	NA:NA:NA:NA:NA	2016
Andrew Jacobson:Jinsil Hwaryoung Seo	This paper presents PulmonaReality, an interactive virtual reality game aimed at young patients to help immerse them into a world that makes pulmonary function tests more enjoyable for the user while providing more reliable results for the examiner. Computer games designed to work with medical tests have been shown to have potential. While there are existing games out there, they are beginning to show their age in comparison to many games played by modern-day patients. The design of our project focuses on usability and enjoyment for young children. In our preliminary user studies, children reported that the system was easy to use with minimal instruction and evoked a sense of wonder when they experienced our different interactive 3D environments.	PulmonaReality: transforming pediatric pulmonary function experience using virtual reality	NA:NA	2016
Ari Rapkin Blenkhorn:Yu Wang:Marc Olano	We have created a suite of automated tools to calibrate and configure a projection virtual reality system. Test subjects (rats) explore an interactive computer-graphics environment presented on a large curved screen using multiple projectors. The locations and characteristics of the projectors can vary and the shape of the screen may be complex. We place several cameras around the workspace for redundant coverage. We locate each projector's hotspot as seen by each camera, and produce a brightness profile which tells the projector how much to dim each pixel of its output to achieve uniform output. We reconstruct the 3D geometry of the screen and the location of each projector using shape-from-motion and structured-light multi-camera computer vision techniques. We determine which projected pixel corresponds to a given view direction for the rat. From these, we create a warping profile for each projector, which tells it how to pre-distort its output image to appear undistorted to the rat's viewpoint. We apply both pre-distortion and hotspot correction before displaying to the screen.	RatCAVE: calibration of a projection virtual reality system	NA:NA:NA	2016
Rodrigo M. A. Silva:Bruno Feijó:Pablo B. Gomes:Thiago Frensh:Daniel Monteiro	In this paper we propose a real time 360° video stitching and streaming processing methodology focused on GPU. The solution creates a scalable solution for large resolutions, such as 4K and 8K per camera, and supports broadcasting solutions with cloud architectures. The methodology uses a group of deformable meshes, processed using OpenGL (GLSL) and the final image combine the inputs using a robust pixel shader. Moreover, the result can be streamed to a cloud service using h.264 encoding with nVEnc GPU encoding. Finally, we present some results.	Real time 360° video stitching and streaming	NA:NA:NA:NA:NA	2016
Ayaka Nishi:Keisuke Hoshino:Hiroyuki Kajimoto	Virtually infinite space is a holy grail of immersive virtual environment (IVE), and numerous approaches have been proposed, yet there still is a hardware and spatial cost. We propose a novel low-cost locomotion interface that combines a 1-DoF treadmill and a head mounted display (HMD), in which displayed image is rotated to induce straightened trajectory for the treadmill, similar to the technique known as Redirected Walking. We conducted an experiment using the proposed method, and showed that by using PD control algorithm, the walking path became straightened.	Straightening walking path using redirected walking technique	NA:NA:NA	2016
Yukari Konishi:Nobuhisa Hanamitsu:Kouta Minamizawa:Ayahiko Sato:Tetsuya Mizuguchi	The Synesthesia Suit provides immersive embodied experience in Virtual Reality environment with vibro-tactile sensations on the entire body. Each vibro-tactile actuator provides not a simple vibration such as traditional game controller, but we designed the haptic sensation based on the haptic design method we have developed in the TECHTILE[Minamizawa et al. 2012] technology. In haptics research using multi-channel vibro-tactile feedback, Surround Haptics [Israr et al. 2012] proposed moving tactile strokes using multiple vibrators spaced on a gaming chair. And then they also proposed Po2[Israr et al. 2015], which shows illusion of tactile sensation for gesture based games by providing vibrations on the hand based on psycho-physical study.	Synesthesia suit: the full body immersive experience	NA:NA:NA:NA:NA	2016
Yu-Xiang Wang:Yu-Ju Tsai:Yu-Hsuan Huang:Wan-Ling Yang:Tzu-Chieh Yu:Yu-Kai Chiu:Ming Ouhyoung	For stereoscopic augmented reality (AR) system, continuous feature tracking of the observing target is required to generate a virtual object in the real world coordinate. Besides, dual cameras have to be placed with proper distance to obtain correct stereo images for video see-through applications. Both higher resolution and frame rate per second (FPS) can improve the user experience. However, feature tracking could be the bottleneck with high resolution images and the latency would increase if image processing was done before tracking.	ThirdEye: a coaxial feature tracking system for stereoscopic video see-through augmented reality	NA:NA:NA:NA:NA:NA:NA	2016
Chun-Jui Lai:Ping-Hsuan Han:Yi-Ping Hung	By using the head-mounted display (HMD), we can have an immersive virtual reality experience. But the user cannot see any information from the real world. To solve the problem, video seethrough HMD can acquire images from real environment, and present into the HMD, then, we could build a mixed reality (MR) or augmented reality (AR) system. However, how to append and calibrate cameras on HMD for recovering real environment is still a research issue. HTC VIVE has a single camera in front of its device. [Steptoe et al. 2014] and OVRVISION Pro proposed to append dual cameras to capture left and right images. Due to the difference of viewpoint, images captured by cameras are different to what human eyes see (figure 2). Although we could recover true 3D information with a depth map, there are still some occlusion areas that we cannot recover by single camera. Therefore, multiple cameras with different positions could complement each other for reducing occlusion areas. In this work, four configurations are simulated with a synthesized scene.	View interpolation for video see-through head-mounted display	NA:NA:NA	2016
Alexandre Cardoso:Edgard Lamounier, Jr.:Gerson Flavio Mendes de Lima:Paulo Roberto do Prado:José Newton Ferreira	In this work, we propose a Virtual Reality based solution to provide a more natural and intuitive environment for controlling electrical operation centers. The research is being carried out with the collaboration of one electric company called Cemig. The novelty of this approach is the ability operators will have to manage the electric system and its electric components by being immersed within a 3D world, reflecting the very true arrangement found in the real electrical substation. Besides, the solution has been designed in a way to provide the operator with all supervisory data in the same virtual environment. We have conducted experiments with the electric company operators Mental efforts to understand the reality of the field have been reduced, according to Cemig's employees. They also claim that a unique environment with all data integrated is very important for taking engineering decisions.	VRCEMIG: a novel approach to power substation control	NA:NA:NA:NA:NA	2016
Henry Fernandez:Koji Mikami:Kunio Kondo	For high skilled players, an easy game might become boring and for low skilled players, a difficult game might become frustrating. This research's goal is to offer players a personalized experience adapted according to their performance and levels of attention. We created a simple side-scrolling 2D platform game using Procedural Content Generation, Dynamic Difficulty Adjustment techniques and brain computer data obtained from players in real time using an Electroencephalography device. We conducted a series of experiments with different players and got results that confirm that our method is adjusting each level according to performance and attention.	Adaptable game experience through procedural content generation and brain computer interface	NA:NA:NA	2016
Jhe-Wei Lin:Po-Wen Cheng:Chien-hung Lin:I-Chen Lin	Object movement in virtual 3D space involves 3D rotations and translations. Conventional desktop interfaces are difficult for a user to simultaneously control the variations of six degrees of freedom. In this paper, we present a vision-based user-friendly method for this purpose. A user can simply grasp one or more objects and fly the objects intuitively. The proposed system using an efficient method to detect and track the objects in the scene. The estimated object motions are used to drive the corresponding targets on the screen or to trigger predefined actions. This interface can track more than one objects at an interactive rate and is applied to a 3D flight shooting game.	Intuitive 3D flight gaming with tangible objects	NA:NA:NA:NA	2016
Pedro Rossa:Nicolas Hoffman:João Ricardo Bittencourt:Fernando Marson:Vinicius J. Cassol	In this work we explore the use of games and VR in order to collaborate with History teaching in Brazil. We develop a game and a VR experience based on local technology. In or approach the player is considered as an Indian who lived in the Jesuitical Reductions in the South of Brazil and was requested to practice bow and arrow shooting.	Living the past: the use of VR to provide a historical experience	NA:NA:NA:NA:NA	2016
Jae-Ho Nah:Sunho Ki:Yeongkyu Lim:Jinhong Park:Chulho Shin	Post-processing anti-aliasing algorithms are widely used now for real-time rendering because of their simplicity, performance, and suitability for deferred shading. Fast approximate anti-aliasing (FXAA) [Lottes 2009] is the fastest method among them, so many games support FXAA to get anti-aliased images. However, FXAA can easily lose texture details and text sharpness due to its excessive blurring. To alleviate those problems of FXAA, we present adaptive approximate anti-aliasing (AXAA). Our approach adds three contributions to FXAA in order to avoid unnecessary filtering. First, we stop further anti-aliasing processes if the current pixel or its neighbors are judged as pixels on already filtered textures or fonts. Second, we try to maintain thin lines as much as possible in order to avoid blurring fonts and lines. Third, for higher performance, we adaptively set the search range of each pixel according to luma contrast. Our experiments show that AXAA provides significantly better image quality than FXAA, in terms of texture, text, and geometry details. Nevertheless, processing overhead of AXAA is still similar to that of FXAA.	AXAA: adaptive approximate anti-aliasing	NA:NA:NA:NA:NA	2016
Amir Semmo:Matthias Trapp:Tobias Dürschmid:Jürgen Döllner:Sebastian Pasewaldt	This work presents an interactive mobile implementation of a filter that transforms images into an oil paint look. At this, a multi-scale approach that processes image pyramids is introduced that uses flow-based joint bilateral upsampling to achieve deliberate levels of abstraction at multiple scales and interactive frame rates. The approach facilitates the implementation of interactive tools that adjust the appearance of filtering effects at run-time, which is demonstrated by an on-screen painting interface for per-pixel parameterization that fosters the casual creativity of non-artists.	Interactive multi-scale oil paint filtering on mobile devices	NA:NA:NA:NA:NA	2016
Shintaro Murakami:Tomoyuki Mukasa:Tony Tung	We present a new feature for AR/VR applications for consumer mobile devices equipped with video camera (e.g., smartphone). Direct or indirect scale estimation of scene or objects is necessary for realistic rendering of virtual objects in real-world environment. Standard approaches usually rely on 3D vision with sensor fusion (e.g., visual SLAM), or pattern recognition (e.g., using AR markers, reference object learning), and suffer from various limitations. Here, we argue that combining inertial measurements and visual cues, the problem reduces to a 1D parameter estimation representing distance from device to floor. In particular, we discuss robust solutions to solve absolute scale estimation problem for indoor environments.	Mobile virtual interior stylization from scale estimation	NA:NA:NA	2016
Roberto Lopez Mendez:Sylwester Bala	Local cubemaps (LC) were introduced for the first time more than ten years ago for rendering reflections [Bjorke 2004]. Nevertheless it is only in recent years that major game engines have incorporated this technique. In this paper we introduce a generalized concept of LC and present two new LC applications for rendering shadows and refractions. We show that limitations associated with the static nature of LC can be overcome by combining this technique with other well-known runtime techniques for reflections and shadows. Rendering techniques based on LC allow high quality shadows, reflections and refractions to be rendered very efficiently which makes them ideally suited to mobile devices where runtime resources must be carefully balanced [Ice Cave Demo 2015].	Optimized mobile rendering techniques based on local cubemaps	NA:NA	2016
Victoria McGowen:Joe Geigel	Blend shape animation is one of the most common methods for facial animation used by animators. Creation of effective blend shapes is an investment as they can take a very long time to create, but once finished, help with consistency in animation. As this is a prime animation method, its extensive process can be off-putting to newcomers. This project is focused on creating a system that will automate the blend shape creation process. The resulting blend shapes could be used in a blend shape based facial motion capture system (eg. [Weise et al. 2011]). The goal of this application is to produce a comparable result to that of blend shapes done by hand for student projects.	Automatic blend shape creation for facial motion capture	NA:NA	2016
Wakana Asahina:Naoya Iwamoto:Hubert. P. H. Shum:Shigeo Morishima	In recent years, thanks to the development of 3DCG animation editing tools (e.g. MikuMikuDance), a lot of 3D character dance animation movies are created by amateur users. However it is very difficult to create choreography from scratch without any technical knowledge. Shiratori et al. [2006] produced the dance automatic generation system considering rhythm and intensity of dance motions. However each segment is selected randomly from database, so the generated dance motion has no linguistic or emotional meanings. Takano et al. [2010] produced a human motion generation system considering motion labels. However they use simple motion labels like "running" or "jump", so they cannot generate motions that express emotions. In reality, professional dancers make choreography based on music features or lyrics in music, and express emotion or how they feel in music. In our work, we aim at generating more emotional dance motion easily. Therefore, we use linguistic information in lyrics, and generate dance motion.	Automatic dance generation system considering sign language information	NA:NA:NA:NA	2016
Shota Ekuni:Koichi Murata:Yasunari Asakura:Akira Uehara	Visual extension has been an essential issue because the visual information accounts for a large part of sensory information which human processes. There are some instruments which are used to watch distant, objects or people, such as a monocle, a binocular, and a telescope. When we use these instruments, we firstly take a general view without them and adjust magnification and focus of them. These operations are complicated and occupy the user's hands. Therefore, a visual extension device that is capable of being used easily without hands is extremely useful. A system developed in the previous work recognizes the movement of the user's eyelid and operating devices by using it [Hideaki et al. 2013]. However, a camera is placed in front of the eye, and that obstructs the field of view. In addition, image recognition needs much calculation cost and it is difficult to be processed in a small computer. When human intends to move his/her muscles, bioelectrical signal (BES) leaks out on the surface of skin. The BES can be measured by small and thin electrodes attached to the surface of the skin. By using the BES, user's operational intentions can be detected promptly without obstructing the user's field of view. Moreover, using BES sensors can reduce electrical power, and contribute to downsizing systems.	Bionic scope: wearable system for visual extension triggered by bioelectrical signal	NA:NA:NA:NA	2016
Kai-Lin Chuang	Dynamic Frame Rate (DFR) is the change in frame rate of a movie sequence in real time as the sequence is playing. Throughout the majority of the past century and after the introduction of sound in films, frame rates used in films have been kept at a standardization of 24 frame per second despite technological advancement [Salmon et. Al 2011]. In the past decade, spatial resolution has been increasing in display systems while the temporal resolution, the frame rate, has not been changed. Because of this, researchers and filmmakers stress that motion judders and blurriness are much more apparent and they propose that high frame rates will solve the issue [Emoto et. Al 2014] [Turnock 2013]. Some industry experts and critics, however, oppose the use of high frame rates [Wilcox 2015]. Despite all the research and attempts in using high frame rate, the idea of using dynamic frame rate in digital cinema has not been explored in depth. As such, there is very limited information on how people perceive DFR and how it actually works. By understanding DFR and how viewers perceive the changes in frame rate, it will help us adapt new techniques in the creation of cinema. We can utilize high frame rate in sequences that could benefit from high frame rate while keeping the rest of the sequences at standard frame rate. This thesis aims to understand the basics of DFR, how different implementations of DFR changes viewer perception and how people perceive a change of frame rate in an animated movie sequence displayed.	Dynamic frame rate: a study on viewer perception of changes in frame rate within an animated movie sequence	NA	2016
Jose A. S. Fonseca:Denis Kravtsov:Anargyros Sarafopoulos:Jian J. Zhang	Effective communication through character animation depends on the recognition of the performed body expressions. The creation of the right body postures is crucial for character animation in the context of animated films and games, as it allows for conveying the right set of emotions to the viewer. Audience needs to be able to identify familiar features mainly based on their own experiences, which allows the viewer to relate and feel empathy to observed characters. It is, therefore, crucial for the animator to accurately create the right posture and expressive body motion, during the posing phase of the animation process.	Enhancement of 3D character animations through the use of automatically generated guidelines inspired by traditional art concepts	NA:NA:NA:NA	2016
Simone Barbieri:Nicola Garau:Wenyu Hu:Zhidong Xiao:Xiaosong Yang	Sketch as the most intuitive and powerful 2D design method has been used by artists for decades. However it is not fully integrated into current 3D animation pipeline as the difficulties of interpreting 2D line drawing into 3D. Several successful research for character posing from sketch has been presented in the past few years, such as the Line of Action [Guay et al. 2013] and Sketch Abstractions [Hahn et al. 2015]. However both of the methods require animators to manually give some initial setup to solve the corresponding problems. In this paper, we propose a new sketch based character posing system which is more flexible and efficient. It requires less input from the user than the system from [Hahn et al. 2015]. The character can be easily posed no matter the sketch represents a skeleton structure or shape contours.	Enhancing character posing by a sketch-based interaction	NA:NA:NA:NA:NA	2016
Chun-Kai Huang:Tsung-Hung Wu:Yi-Ling Chen:Bing-Yu Chen	In recent years, personalized fabrication has attracted much attention due to the greatly improved accessibility of consumer-level 3D printers. However, 3D printers still suffer from the relatively long production time and limited output size, which are undesirable for large-scale rapid-prototyping. Zometool, which is a popular building block system widely used for education and entertainment, is potentially suitable for providing an alternative solution to the aforementioned scenarios. However, even for 3D models of moderate complexity, novice users may still have difficulty in building visually plausible results by themselves. Therefore, the goal of this work is to develop an automatic system to assist users to realize Zometool rapid prototyping with a specified 3D shape. Compared with the previous work [Zimmer and Kobbelt 2014], our method may achieve the ease of assembly and economic usage of building units since we focus on generating the Zometool structures through a higher level of shape abstraction.	Large-scale rapid-prototyping with zometool	NA:NA:NA:NA	2016
Gustavo E. Boehs:Milton L. H. Vieira	We propose a framework for using human acting as input for the animation of non-humanoid creatures; captured motion is classified using machine learning techniques, and a combination of preexisting clips and motion retargeting are used to synthetize new motions. This should lead to a broader use of motion capture.	Non-humanoid creature performance from human acting	NA:NA	2016
Cyril Corvazier:Benjamin Legros:Rachid Chikh	We present a new storage scheme for computer graphic images based on OpenEXR 2. Using such EXR/Id files, the compositing artist can isolate an object selection (by picking them or using a regular expression to match their names) and color corrects them with no edge artefact, which was not possible to achieve without rendering the object selection on its own layer. Using this file format avoids going back and forth between the rendering and the compositing departments because no mask image or layering are needed anymore. The technique is demonstrated in an open source software suite, including a library to read and write the EXR/Id files and an OpenFX plug-in which generates the images in any compositing software.	OpenEXR/Id isolate any object with a perfect antialiasing	NA:NA:NA	2016
Fabio Turchet:Marco Romeo:Oleg Fryazinov	Recent developments in character rigging and animation shape the computer graphics industry in general and visual effects in particular. Advances in deformation techniques, which include linear blend skinning, dual quaternion skinning and shape interpolation, meet with sophisticated muscle and skin simulations to produce more realistic results. Effects such as skin sliding, wrinkling and contact of subcutaneous fat and muscles become possible when simulating the anatomy of human-like characters as well as creatures in feature films. One of the main techniques adopted nowadays in the industry is the Finite Element Method (FEM) for deformable objects. Despite the life-like results, the setup cost to generate and tweak volumetric anatomical models for a FEM solver is not only very high, but it cannot easily guarantee the quality of the models either, in terms of simulation requirements. In a production environment in fact (see Fig. 1), models often require additional processing in order to be ready for FEM simulations. For example, self-intersections or interpenetrations in rest pose may result in unwanted forces from the collision detection and response algorithms that affect negatively the simulation at its start.	Physics-aided editing of simulation-ready muscles for visual effects	NA:NA:NA	2016
Deschanel Li	It is currently possible to reliably motion-track humans and some animals, but not possible to track insects using standard motion tracking techniques. By programming a virtual prototype rig/skeleton for the insects small scale creatures will be able to be tracked in real time. Possible applications include behavioural research of animals and entertainment industry, e.g., when realistic insect motion simulation is needed and insects cannot be outfitted with sensors like humans for animation in movies or games.	Towards real-time insect motion capture	NA	2016
Tsukasa Nozawa:Takuya Kato:Pavel A. Savkin:Naoki Nozawa:Shigeo Morishima	3D facial shape reconstruction in the wild environments is an important research task in the field of CG and CV. This is because it can be applied to a lot of products, such as 3DCG video games and face recognition. One of the most popular 3D facial shape reconstruction techniques is 3D Model-based approach. This approach approximates a facial shape by using 3D face model, which is calculated by principal component analysis. [Blanz and Vetter 1999] performed a 3D facial reconstruction by fitting points from facial feature points of an input of single facial image to vertex of template 3D facial model named 3D Morphable Model. This method can reconstruct a facial shape from a variety of images which include different lighting and face orientation, as long as facial feature points can be detected. However, representation quality of the result depends on the number of 3D model resolution.	3D facial geometry reconstruction using patch database	NA:NA:NA:NA:NA	2016
Murat Kurt:Greg Ward:Nicolas Bonneel	We present a data-driven Bidirectional Scattering Distribution Function (BSDF) representation and a model-free technique that preserves the integrity of the original data and interpolates reflection as well as transmission functions for arbitrary materials. Our interpolation technique employs Radial Basis Functions (RBFs), Radial Basis Systems (RBSs) and displacement techniques to track peaks in the distribution. The proposed data-driven BSDF representation can be used to render arbitrary BSDFs and includes an efficient Monte Carlo importance sampling scheme. We show that our data-driven BSDF framework can be used to represent measured BSDFs that are visually plausible and demonstrably accurate.	A data-driven BSDF framework	NA:NA:NA	2016
Carlos Aliaga:Carlos Castillo:Diego Gutierrez:Miguel A. Otaduy:Jorge Lopez-Moreno:Adrian Jarabo	Rendering realistic fabrics is an active research area with many applications in computer graphics and other fields like textile design. Reproducing the appearance of cloth remains challenging due to the micro-structures found in textiles, and the complex light scattering patterns exhibited at such scales. Recent approaches have reached very realistic results, either by directly modeling the arrangement of the fibers [Schröder et al. 2011], or capturing the structure of small pieces of cloth using Computed Tomography scanners (CT) [Zhao et al. 2011]. However, there is still a need for predictive modeling of cloth appearance; existing methods either rely on manually-set parameter values, or use photographs of real pieces of cloth to guide appearance matching algorithms, often assuming certain simplifications such as considering circular or elliptical cross sections, or assuming an homogeneous volume density, that lead to very different appearances.	A fiber-level model for predictive cloth rendering	NA:NA:NA:NA:NA:NA	2016
Bilal Ahmed:Jong Hun Lee:Yong Yi Lee:Junho Choi:Yong Hwi Kim:Moon Gu Son:Min Ho Joo:Kwan H. Lee	Recently researchers have shown much interest in 3D projection mapping systems but relatively less work has been done to make the contents look realistic. Much work has been done for multi-projector blending, 3D projection mapping and multi-projector based large displays but existing color compensation based systems still suffer from contrast compression, color inconsistencies and inappropriate luminance over the three dimensional projection surface giving rise to an un-appealing appearance. Until now having a realistic result with projection mapping on 3D objects when compared with a similar original object still remains a challenge. In this paper, we present a framework that optimizes projected images using multiple projectors in order to achieve an appearance that looks close to a real object whose appearance is being regenerated by projection mapping.	A method for realistic 3D projection mapping using multiple projectors	NA:NA:NA:NA:NA:NA:NA:NA	2016
Masato Ishimuroya:Takashi Kanai	We propose a method for adding visual details to fluid animation while reducing noisy appearances. In grid-based fluid simulations, an issue is that while highly detailed fluids with small eddies can be obtained by increasing the number of grid cells, it costs much more computational time. To address this, various methods for adding details (or up-scaling resolutions) have been proposed. Those methods can generate fine animations quickly by adding high-frequency noises or external forces to coarse simulation results. However, those methods typically generate tiny eddies on a whole surface of fluid and the result appears too noisy. In this paper, we consider the distribution of kinetic energy in the spatial frequency domain and then apply it to two existing methods for adding details. By using our method, noises or external forces can be added to the appropriate positions of fluids and consequently natural-looking details can be achieved.	Adding visual details based on low-resolution energy cascade ratios for smoke simulation	NA:NA	2016
Tatsuya Matsumoto:Kensuke Tobitani:Yusuke Tani:Hiroki Fujii:Noriko Nagata	The visual expression of a surface quality of human skin is required in a wide range of fields, such as in cosmetics industry. It is much harder, however, to generate intuitively and accurately a computer graphics (CG) image of human skin that has a desired impression without professional knowledge and skills because of its complex physical properties. It is necessary to model the linkage between the visual impressions and physical properties of human skin in order to effectively determine parameters for the generation of CG images.	An evaluation of the relationship between impression and the physical properties of human skin	NA:NA:NA:NA:NA	2016
Qian Chen:Haiyuan Wu:Shinichi Higashino:Ryuuki Sakamoto	In this paper, we present a convenient method for camera calibration with arbitrary co-planar circle-pairs from one image. This method is based on the accurate recovery of the projected centers of the circle pairs using a closed-form algorithm.	Camera calibration by recovering projected centers of circle pairs	NA:NA:NA:NA	2016
Miyu Iwafune:Taisuke Ohshima:Yoichi Ochiai	We propose novel design method to fabricate user interfaces with mechanical metamaterial called Coded Skeleton. The Coded Skeleton is combination of shape memory alloys (SMA) and 3-D printed bodies, and it has computationally designed structure that is flexible in one deformation mode but is stiff in the other modes. This property helps to realize materials that automatically deform by a small and lightweight actuator such as SMA. Also it enables to sense user inputs with the resistance value of SMA. In this paper, we propose shape-changing user interfaces by integrating sensors and actuators as Coded Skeleton. The deformation and stiffness of this structure is computationally designed and also controllable. Further, we propose interactions and applications with user interfaces fabricated using our design method.	Coded skeleton: programmable bodies for shape changing user interfaces	NA:NA:NA	2016
Satoshi Hashizume:Kazuki Takazawa:Amy Koike:Yoichi Ochiai	The representation of texture is a major concern during fabrication and manufacturing in many industries. Thus, the approach for fabricating everyday objects and the digital expression of their textures before fabrication process has become a popular research area. Although it is easy to change the texture of objects in the digital world (i.e. just setting texture parameters), it is difficult to achieve this in the real world.	Cross-field haptics: push-pull haptics combined with magnetic and electrostatic fields	NA:NA:NA:NA	2016
Kaimo Hu:Dong-Ming Yan:Bedrich Benes	Surface remeshing is a key component in many geometry processing applications. However, existing high quality remeshing methods usually introduce approximation errors that are difficult to control, while error-driven approaches pay little attention to the meshing quality. Moreover, neither of those approaches can guarantee the minimal angle bound in resulting meshes. We propose a novel error-bounded surface remeshing approach that is based on minimal angle elimination. Our method employs a dynamic priority queue that first parameterize triangles who contain angles smaller than a user-specified threshold. Then, those small angles are eliminated by applying several local operators ingeniously. To control the geometric fidelity where local operators are applied, an efficient local error measure scheme is proposed and integrated in our remeshing framework. The initial results show that the proposed approach is able to bound the geometric fidelity strictly, while the minimal angles of the results can be eliminated to be up to 40 degrees.	Error-bounded surface remeshing with minimal angle elimination	NA:NA:NA	2016
Masashi Baba:Kesuke Haruta:Shinsaku Hiura	To create realistic CG images, the information about the lighting is very important. There are two ways to estimate the information of the light source. One is a direct measurement method using images captured with a fish-eye lens or a spherical mirror[Debevec 1998], and the other is an indirect measurement method to estimate positions and intensities of the light sources from the shadow information of objects[Sato et al. 2003]. In the direct measurement method, by concerning pixels of the captured image as light sources having corresponding intensities, it is possible to estimate the lighting environment densely. However, for a high-intensity light source like the sun, the dynamic range of the camera is insufficient, and the radiant intensity of the light source cannot be accurately estimated. So, we propose a method that combines a direct measurement technique and an indirect measurement method. In our proposed method, the light source information of the high-intensity area in the captured image is estimated by indirect measurement method. In the experiments using real images, even for outdoor scenes that contain the high-intensity light source like the sun, the measurement of the light source environment could be performed by the proposed method. Also, it was confirmed that images including realistic shadows equivalent to real images could be created.	Estimating lighting environments based on shadow area in an omni-directional image	NA:NA:NA	2016
Seungbae Bang:Meekyoung Kim:Doekkyeong Jang:Sung-Hee Lee	Knee brace is a sports product or medical equipment that increases the stability in the dynamics of the knee. The proper design of a subject-specific knee brace should take her anatomical characteristics into account since they are influential to the knee dynamics. However, anatomical information is hidden under the skin, and obtaining such information is restricted to expensive equipments such as Magnetic Resonance Imaging (MRI) device or Computed Tomography (CT) scan device.	Estimating skeleton from skin data for designing subject-specific knee braces	NA:NA:NA:NA	2016
Sara C. Schvartzman:Marco Romeo	Digital characters are common in modern films visual effects and the demand for digital actors has increased during the past few years. The success of digitally created actors is related to their believability and, in particular, the realism of the animation and simulation of their faces. Facial expressions in computer graphics are commonly obtained through linear vertex interpolation techniques such as blend shapes. These enable high artistic control and fast interaction, but cannot properly reproduce collisions or other physical phenomena such as gravity and inertia. These effects can be achieved by applying simulation techniques over the animated facial geometry (e.g. muscle simulation), but could potentially alter the look of the desired facial expression and produce inconsistencies with the work approved in animation. Moreover, animating such muscle rigs can be very cumbersome.	Example-based data optimization for facial simulation	NA:NA	2016
K. Edum-Fotwe:P. Shepherd:M. Brown:D. Harper:R. Dinnis	This simple paper describes an intuitive data-driven approach to reconstructing architectural facade models from unstructured point-clouds. The algorithm presented yields sparse semantically-rich models that are better suited to interactive simulation than the equivalent dense-reconstructions, yet executes significantly faster than the prevalent sparse-operators. The key advantages include accuracy, efficiency and the ability to model irregular windows.	Fast, accurate and sparse, automatic facade reconstruction from unstructured ground laser-scans	NA:NA:NA:NA:NA	2016
Karan Sharma:Arun C. S. Kumar:Suchendra Bhandarkar	Large scale object classification has seen commendable progress owing, in large part, to recent advances in deep learning. However, generating annotated training datasets is still a significant challenge, especially when training classifiers for large number of object categories. In these situations, generating training datasets is expensive coupled with the fact that training data may not be available for all categories and situations. Such situations are generally resolved using zero-shot learning. However, training zero-shot classifiers entails serious programming effort and is not scalable to very large number of object categories. We propose a novel simple framework that can guess objects in an image. The proposed framework has the advantages of scalability and ease of use with minimal loss in accuracy. The proposed framework answers the following question: How does one guess objects in an image from very few object detections?	Guessing objects in context	NA:NA:NA	2016
Martin Šik:Jaroslav Křivánek	Markov Chain Monte Carlo (MCMC) has recently received a lot of attention in light transport simulation research [Hanika et al. 2015; Hachisuka et al. 2014]. While these methods aim at high quality sampling of local extremes of the path space (so called local exploration), the other issue - discovering these extremes - has been so far neglected. Poor global exploration results in oversampling some parts of the paths space, while undersampling or completely missing other parts (see Fig. 1). Such behavior of MCMC-based light transport algorithms limits their use in practice, since we can never tell for sure whether the image has already converged.	Improving global exploration of MCMC light transport simulation	NA:NA	2016
Ana Serrano:Diego Gutierrez:Karol Myszkowski:Hans-Peter Seidel:Belen Masia	Many different techniques for measuring material appearance have been proposed in the last few years. These have produced large public datasets, which have been used for accurate, data-driven appearance modeling. However, although these datasets have allowed us to reach an unprecedented level of realism in visual appearance, editing the captured data remains a challenge. In this work, we develop a novel methodology for intuitive and predictable editing of captured BRDF data, which allows for artistic creation of plausible material appearances, bypassing the difficulty of acquiring novel samples. We synthesize novel materials, and extend the existing MERL dataset [Matusik et al. 2003] up to 400 mathematically valid BRDFs. We design a large-scale experiment with 400 participants, gathering 56000 ratings about the perceptual attributes that best describe our extended dataset of materials. Using these ratings, we build and train networks of radial basis functions to act as functionals that map the high-level perceptual attributes to an underlying PCA-based representation of BRDFs. We show how our approach allows for intuitive edits of a wide range of visual properties, and demonstrate through a user study that our functionals are excellent predictors of the perceived attributes of appearance, enabling predictable editing with our framework.	Intuitive editing of material appearance	NA:NA:NA:NA:NA	2016
Terence Broad:Mick Grierson	Both light field photography and focal stack photography are rapidly becoming more accessible with Lytro's commercial light field cameras and the ever increasing processing power of mobile devices. Light field photography offers the ability of post capturing perspective changes and digital refocusing, but little is available in the way of post-production editing of light field images. We present a first approach for interactive content aware completion of light fields and focal stacks, allowing for the removal of foreground or background elements from a scene.	Light field completion using focal stack propagation	NA:NA	2016
Yoshinori Dobashi:Takashi Ijiri:Hideki Todo:Kei Iwasaki:Makoto Okabe:Satoshi Nishimura	Realistic image synthesis is an important research goal in computer graphics. One important factor to achieve this goal is a bidirectional reflectance distribution function (BRDF) that mainly governs an appearance of an object. Many BRDF models have therefore been developed. A physically-based BRDF based on microfacet theory [Cook and Torrance 1982] is widely used in many applications since it can produce highly realistic images. The microfacetbased BRDF consists of three terms; a Fresnel, a normal distribution, and a geometric functions. There are many analytical and approximate models for each of these terms.	Measuring microstructures using confocal laser scanning microscopy for estimating surface roughness	NA:NA:NA:NA:NA:NA	2016
Tuur Stuyck:Philip Dutré	Physics-based animation has become an important tool in computer graphics and is essential in recreating realistic looking natural phenomena. Researchers have been looking for tools to control passive simulations that allow artists to easily modify the simulation to best suit the artistic requirements. However, fluid motion is very hard to predict and it is very difficult, if not impossible, to achieve specific behavior just by altering the global variables. Active control of the simulation will be necessary to achieve this goal.	Model predictive control for robust art-directable fluids	NA:NA	2016
Lode Jorissen:Patrik Goorts:Gauthier Lafruit:Philippe Bekaert	In recent years there is a growing interest in the generation of virtual views from a limited set of input cameras. This is especially useful for applications such as Free Viewpoint Navigation and light field displays [Tanimoto 2015]. The latter often requires tens to hundreds of input views, while it is often not feasible to record with as many cameras. View interpolation algorithms often traverse a set of depths to find correspondences between the input images [Stankiewicz et al. 2013; Goorts et al. 2013]. Most algorithms choose a uniform set of depths to traverse (as shown in Figure 2(a)), but this often leads to an excessive amount of unnecessary calculations in regions where no objects are located. It also results in an increased amount of mismatches, and thus, inaccuracies in the generated views. These problems also occur when a too large depth range is selected. Hence, typically a depth range that encloses the scene tightly is manually selected to mitigate these errors. A depth distribution that organizes the depth layers around the objects in the scene, as shown in Figure 2(b), would reduce these errors and decrease the number of computations by reducing the number of depths to search through. [Goorts et al. 2013] determine a nonuniform global depth distribution by reusing the generated depth information from the previous time stamp. This makes the algorithm dependent on previous results.	Nonuniform depth distribution selection with discrete Fourier transform	NA:NA:NA:NA	2016
Chloe LeGendre:Xueming Yu:Paul Debevec	We demonstrate the sufficiency of using as few as five LEDs of distinct spectra for multispectral lighting reproduction and solve for the optimal set of five from 11 such commercially available LEDs. We leverage published spectral reflectance, illuminant, and camera spectral sensitivity datasets to show that two approaches of lighting reproduction, matching illuminant spectra directly and matching material color appearance observed by one or more cameras or a human observer, yield the same LED selections. Our proposed optimal set of five LEDs includes red, green, and blue with narrow emission spectra, along with white and amber with broader spectra.	Optimal LED selection for multispectral lighting reproduction	NA:NA:NA	2016
Kuo-Wei Chen:Chih-Yuan Yao:Yu-Chi Lai:You-En Lin	Although 3D printing is becoming more popular, but there are two major problem. The first is the slowness of the process because of requirement of processing information of an extra axis comparing to tradition 2D printers. The second is the printable dimension of 3D printers. Generally, the larger the model is printed, the larger a 3D printer has to be and the more expensive it is. Furthermore, it would also require a large amount of extra inflation materials. With the entrance of cheap 3D printers, such as OLO 3D printers [Inc. 2016], parallel printing with multiple cheap printers can possibly be the solution. In order to parallel print a 3D model, we must decompose a 3D model into smaller components. After printing out all the components, we assemble them together by attaching them to the skeleton through supporters and joints to form the final result. As shown in our results, our designed shell-and-bone-based model printing can not only save the printing time but also use lesser material than the original whole model printing.	Parallel 3D printing based on skeletal remeshing	NA:NA:NA:NA	2016
K. Edum-Fotwe:P. Shepherd:M. Brown:D. Harper:R. Dinnis	This simple paper describes an intuitive data-driven approach to reconstructing architectural building-footprints from structured or unstructured 2D pointsets. The function is fast, accurate and unconstrained. Further unlike the prevalent L-Shape detectors predicated on a shape's skeletal descriptor [Szeliski 2010], the method is robust to sensing noise at the boundary of a 2D pointset.	Quick, unconstrained, approximate l-shape method	NA:NA:NA:NA:NA	2016
Henrik Lieng	Diffusion curves [Orzan et al. 2008] (DCs) has risen as an attractive vector primitive for representing complex colour gradients. Its flexible mathematical definition, taking curves with colour values as input, can be easily adopted by artists and designers because curves represent an intuitive approach to 2D drawing and design. However, the (Laplacian) diffusion process is computationally expensive and naive DCs (solving the large sparse PDE naively) are consequently unattractive as a practical vector graphics primitive. Lots of work has therefore been undertaken to identify a practical framework for defining and rendering DCs.	Ray-traced diffusion points	NA	2016
Patrik Huber:William Christmas:Adrian Hilton:Josef Kittler:Matthias Rätsch	We present a fully automatic approach to real-time 3D face reconstruction from monocular in-the-wild videos. We use a 3D Morphable Face Model to obtain a semi-dense shape and combine it with a fast median-based super-resolution technique to obtain a high-fidelity textured 3D face model. Our system does not need prior training and is designed to work in uncontrolled scenarios.	Real-time 3D face super-resolution from monocular in-the-wild videos	NA:NA:NA:NA:NA	2016
Chih-Fan Chen:Mark Bolas:Evan Suma	With the recent proliferation of high-fidelity head-mounted displays (HMDs), there is increasing demand for realistic 3D content that can be integrated into virtual reality environments. However, creating photorealistic models is not only difficult but also time consuming. A simpler alternative involves scanning objects in the real world and rendering their digitized counterpart in the virtual world. Capturing objects can be achieved by performing a 3D scan using widely available consumer-grade RGB-D cameras. This process involves reconstructing the geometric model from depth images generated using a structured light or time-of-flight sensor. The colormap is determined by fusing data from multiple color images captured during the scan. Existing methods compute the color of each vertex by averaging the colors from all these images. Blending colors in this manner creates low-fidelity models that appear blurry. (Figure 1 right). Furthermore, this approach also yields textures with fixed lighting that is baked on the model. This limitation becomes more apparent when viewed in head-tracked virtual reality, as the illumination (e.g. specular reflections) does not change appropriately based on the user's viewpoint.	Real-time 3D rendering using depth-based geometry reconstruction and view-dependent texture mapping	NA:NA:NA	2016
Serguei A. Mokhov:Miao Song:Jonathan Llewellyn:Jie Zhang:Alexander Charette:Ruofan Wu:Shuiying Ge	It was not possible to do reliable 3D skeletal tracking with the currently publicly available inexpensive consumer grade hardware/software tools, such as depth cameras and their SDKs using multiple of such sensors in a single application (e.g., a game, motion recording for animation, or 3D scanning). We successfully attached 3 Kinect v2 sensors to a single application to track skeletal data without using Microsoft's Kinect 2 SDK. We created a new toolkit -- MultiCamTk++ for 3 or more Kinects v2 with skeleton support in C++. It is a successor of our previous version, MultiCamTk, done in Processing/Java that had no skeletal tracking. We achieve high resiliency and good frame rate even if 1--2 Kinects are disconnected at runtime. We are able to receive the skeleton data from the multiple sources to correlate the coordinates for spatial 3D user tracking.	Real-time collection and analysis of 3-Kinect v2 skeleton data in a single application	NA:NA:NA:NA:NA:NA:NA	2016
Daniel Limberger:Jürgen Döllner	In a rendering environment of comparatively sparse interaction, e.g., digital production tools, image synthesis and its quality do not have to be constrained to single frames. This work analyzes strategies for highly economically rendering of state-of-the-art rendering effects using progressive multi-frame sampling in real-time. By distributing and accumulating samples of sampling-based rendering techniques (e.g., anti-aliasing, order-independent transparency, physically-based depth-of-field and shadowing, ambient occlusion, reflections) over multiple frames, images of very high quality can be synthesized with unequaled resource-efficiency.	Real-time rendering of high-quality effects using multi-frame sampling	NA:NA	2016
Kurt Leimer:Michael Wimmer:Przemyslaw Musialski	With online repositories for 3D models like 3D Warehouse becoming more prevalent and growing ever larger, new possibilities have opened up for both experienced and inexperienced users alike. These large collections of shapes can provide inspiration for designers or make it possible to synthesize new shapes by combining different parts from already existing shapes, which can be both easy to learn and a fast way of creating new shapes.	Relation-based parametrization and exploration of shape collections	NA:NA:NA	2016
Tomokazu Ishikawa:Kousaku Kamata:Yuriko Takeshima:Masanori Kakimoto	In recent years, expressions close to realities have become possible thanks to the technologically advanced computer graphics. Secular change and weathering are important factors to create realistic computer graphics images. Metal rust is an important secular change and there are much research work on rust [Kanazawa et al. 2015]. Although the rust forming processes vary according to coating rain-water and seawater, dissolved oxygen contents of them and flowing water effects, no rust forming methods which have examined the object geometry of models and chemical reaction processes exist as far as we know. Our proposed method calculates water flowing on 3D models to reproduce the process of corrosion which advances from the surface region coated with water. Our corrosion simulation model takes into account the quantity of coating water and the chemical reaction processes. As a result, we confirm that the images close to the rust formed in reality can be obtained.	Rusting and corroding simulation taking into account chemical reaction processes	NA:NA:NA:NA	2016
Tuur Stuyck:Philip Dutré	Fluid simulations are very useful for creating physically based water effects in computer graphics but are notoriously hard to control. In this talk we propose a novel and intuitive animation technique for fluid animations using interactive direct manipulation of the simulated fluid inspired by clay sculpting. Artists can simply shape the fluid directly into the desired visual effect whilst the fluid still adheres to its physical properties such as surface tension and volume preservation. Our approach is faster and much more intuitive compared to previous work which relies on indirect approaches such as providing reference geometry or density fields. It makes it very easy, even for novice users, to modify simulations ranging from enlarging splashes or altering droplet shapes to adjusting the flow of a large fluid body. The sculpted fluid shapes are incorporated into the simulation using guided re-simulation using control theory instead of simply using geometric deformations resulting in natural-looking animations.	Sculpting fluids: a new and intuitive approach to art-directable fluids	NA:NA	2016
Shuhei Kodama:Tokiichiro Takahashi	In non-photorealistic rendering (NPR), an evaluated value of the rendered image is not absolute. It is different for each user. Therefore, in NPR, it is important to render an image that satisfies user preferences. Many methods of painterly style image generation have been proposed/developed. However, these methods focus on image generation that imitates painting material or a painterly style. Therefore, to the best of our knowledge, the study of how to generate a painterly style image that satisfies user preferences dose not yet exist. When gazing at an image, the user is more effective at quickly identifying regions that the user dislikes than in finding preferred regions. To address this point, we propose a mechanism of painterly style rendering that satisfies user preferences.	Suggestive painterly style image generation system to satisfy user preferences	NA:NA	2016
Syuhei Sato:Yoshinori Dobashi:Tomoyuki Nishita	In this paper, we develop a method for synthesizing desired flow fields by combining existing multiple flow fields. Our system allows the user to specify arbitrary regions of the precomputed flow fields and combine them to synthesize a new flow field. In order to maintain plausible physical behavior, we ensure the incompressibility for the combined flow field. To address this, we use stream functions for representing the flow fields. However, there exist discontinuities at the boundaries between the combined flow fields, resulting in unnatural animation of fluids. In order to remove the discontinuities, we apply Poisson image editing to the stream functions.	Combining multiple flow fields for editing existing fluid animations	NA:NA:NA	2016
Junho Jeon:Yeongyu Jung:Haejoon Kim:Seungyong Lee	Nowadays RGB-D cameras, such as Microsoft Kinect, have become widely available. Various researches on 3D reconstruction based on RGB-D images have enabled 3D navigation of a scene by rendering the reconstructed 3D model from desirable viewpoints. However, these reconstructed 3D models are not yet popularly used in applications due to the lack of accurate color information.	Texture map generation for large-scale 3D reconstructed scenes	NA:NA:NA:NA	2016
William J. Joel	In 2009, the ACM/SIGGRAPH Education Committee established an Undergraduate Research Alliance [Undergraduate Research Alliance] to foster the development of undergraduate research, in computer graphics and interactive techniques, across all related disciplines. Since its inception, the Alliance has hosted sessions at the annual SIGGRAPH conferences to allow educators and other the chance to discuss what they have accomplished and what still needs to be done. If we in the SIGGRAPH community wish to continue to expand the envelope of knowledge, it is necessary that we engage students in the exploration of new ideas as early as possible in their education. The purpose of this poster, therefore, is to present a case study for undergraduate research with the hopes that it spurs others to join in this endeavor.	The need for interdisciplinary undergraduate research	NA	2016
Shoichi Furukawa:Takuya Kato:Pavel Savkin:Shigeo Morishima	Numerous video have been translated using "dubbing," spurred by the recent growth of video market. However, it is very difficult to achieve the visual-audio synchronization. That is to say in general a new audio does not synchronize with actor's mouth motion. This discrepancy can disturb comprehension of video contents. There-fore many methods have been researched so far to solve this problem.	Video reshuffling: automatic video dubbing without prior knowledge	NA:NA:NA:NA	2016
