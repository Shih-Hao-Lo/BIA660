Column1,Column2,Column3,Column4,Column5
Yoomi Choi,"With the advent of media, artists have expanded the domains of visual arts through various new attempts to integrate digital technology with arts. Along with the changes in working environment, nowadays, there have been new attempts to reinterpret old masterpiece using digital media. The initial studies regarding 3D image of oriental painting were mainly focused on non-photorealistic rendering techniques. However, the technique has limitations in expressing or reproducing the unique elements of expression used in oriental paintings. The traditional oriental paintings were not expressed with perspective view and shadow; and in a water ink painting, color expression is also limited because of the unique properties of the ingredient, 'Muk (Ink Stick).' Moreover, the oriental paintings have multiple viewpoints in one scene. Therefore, there are difficulties in using tools such as Maya or 3DMax to express the traditional artworks of the East. In this context, the purpose of this study is to provide alternative solutions to resolve the problems that arise in the process of recreating oriental paintings into 3D image, based on the theoretical framework of spatial arrangement and composition principles. Also, the study reinterpreted the classical works by applying storytelling techniques, which utilize the expandability of time, a main characteristic of visual media. <Inwang Jesaekdo> and <Geumgang Jeondo>, the two major artwork of Gyeomjae Jeong Seon, who is one of the most representative literary artists of the Joseon Dynasty, were reinterpreted with 3D digital image, and new design methods and techniques were applied.",A study on 3D digital image applying oriental painting techniques,NA,2016
Shinichi Higashino:Sakiko Nishi:Ryuuki Sakamoto,"In this paper, we present ARTTag, an aesthetic fiducial marker system, of which the design development can be performed with any color, texture, shape, or other features as long as circle pairs are integrated. By utilizing the projective properties of circular features, ARTTag is appropriate for detection, identification, and camera-based registration in augmented reality (AR) applications.",ARTTag: aesthetic fiducial markers based on circle pairs,NA:NA:NA,2016
Suzi Kim:Sunghee Choi,"Three-dimensional typography (3D typography) refers to the arrangement of text in three-dimensional space. It injects vitality into the letters, thereby giving the viewer a strong impression that is hard to forget. These days, 3D typography plays an important role in daily life beyond the artistic design. It is easy to observe the 3D typography used in the 3D virtual space such as movie or games. Also it is used frequently in signboard or furniture design. Despite its noticeable strength, most of the 3D typography is generated by just a simple extrusion of flat 2D typography. Comparing with 2D typography, 3D typography is more difficult to generate in short time due to its high complexity.",Automatic generation of 3D typography,NA:NA,2016
Asako Soga:Yuho Yazaki:Bin Umino:Motoko Hirayama,"We developed a body-part motion synthesis system (BMSS) that allows users to create short choreographies by synthesizing body-part motions and to simulate them in 3D animation. This system automatically provides various short choreographies. First, users select a base motion and body-part categories. Then the system automatically selects and synthesizes body-part motions to the base motion. The system randomly determined the synthesis timings of the selected motions. Users can use the composed sequences as references for dance creation, learning, and training. We experimentally evaluated our system's effectiveness for supporting dance creation with four professional choreographers of contemporary dance. From our experiment results, we basically verified the usability of BMSS for choreographic creation.",Body-part motion synthesis system for contemporary dance creation,NA:NA:NA:NA,2016
Yuxiao Du:Ergun Akleman,"In this work, we have developed an approach to include global illumination effects into charcoal drawing (see Figure 1). Our charcoal shader provides a robust computation to obtain charcoal effect for a wide variety of diffuse and specular materials. Our contributions can be summarized as follows: (1) A Barrycentric shader that is based on degree zero B-spline basis functions; (2) A set of hand-drawn charcoal control texture images that naturally provide desired charcoal look-and-feel; and (3) A painter's hierarchy for handling a high number of shading parameters consistent with charcoal drawing.",Charcoal rendering and shading with reflections,NA:NA,2016
Yaozhun Huang:Sze-Chun Tsang:Miu-Ling Lam,"Light painting is a photography technique in which light sources are moved in specific patterns while being captured by long exposure. The movements of lights will result in bright strokes or selectively illuminated and colored areas in the scene being captured, thus decorating the real scene with special visual effects without the need for post-production. Light painting is not only a popular activity for hobbyists to express creativities, but also a practice for professional media artists and photographers to produce aesthetic visual arts and commercial photography. In conventional light paintings, the light sources are usually flashlights or other simple handheld lights made by attaching one or multiple LEDs to a stick or a ring. The patterns created are limited to abstract shapes or freehand strokes.",Computational swept volume light painting via robotic non-linear motion,NA:NA:NA,2016
Takeshi Oozu:Aki Yamada:Yuki Enzaki:Hiroo Iwata,"Furniture-device is the device having furniture appearance and physical input and output function. The Escaping Chair is a furniture-device having physical and dynamic interaction with a user to let them perceive the intent of their action and personify the furniture. The Escaping Chair interacts with the bystanders by trying to move away from nearby people. By doing this, the device tries to make the person fail to sit on it, and stimulates their perception about sitting. The idea of a furniture-shaped device was extended from one of my previous artworks, which used furniture as input mechanisms. I exhibited the chair and observed the interaction sit produced with exhibition visitors. It succeeded in making people wonder during the interaction, as I planned, and making them further chase the chair, which indicates a new capability of the device. There were some challenges regarding load tolerance, detection latency and failure, which I have proposed improvements for.",Escaping chair: furniture-shaped device art,NA:NA:NA:NA,2016
Felipe Caputo:Victoria Mc Gowen:Joe Geigel:Steven Cerqueira:Quincey Williams:Marla Schweppe:Zhongyuan Fa:Anastasia Pembrook:Heather Roffe,"Farewell to Dawn is a mixed reality dance performance which explores two dancers' voyage from a physical space to a virtual stage and back, as the day passes before them.",Farewell to dawn: a mixed reality dance performance in a virtual space,NA:NA:NA:NA:NA:NA:NA:NA:NA,2016
E. Entem:L. Barthe:M.-P. Cani:M. van de Panne,"We present an automatic method to build a layered vector graphics structure ready for animation from a clean-line vector drawing of an organic, smooth shape. Inspiring from 3D segmentation methods, we introduce a new metric computed on the medial axis of a region to identify and quantify the visual salience of a sub-region relative to the rest. This enables us to recursively separate each region into two closed sub-regions at the location of the most salient junction. The resulting structure, layered in depth, can be used to pose and animate the drawing using a regular 2D skeleton.",From drawing to animation-ready vector graphics,NA:NA:NA:NA,2016
Hiroko Iwasaki:Momoko Kondo:Rei Ito:Saya Sugiura:Yuka Oba:Shinji Mizuno,"In this paper, we propose a method to interact with virtual shadows through real shadows various physical objects by using two projectors. In our method, the system scans physical objects in front of a projector, generates virtual shadows with CG according to the scan data, and superimposes the virtual shadows to real shadows of the physical objects with the projector. Another projector is used to superimpose virtual light sources inside real shadows. Our method enables us to experience novel interaction with various shadows such as shadows of flower arrangements.",Interaction with virtual shadow through real shadow using two projectors,NA:NA:NA:NA:NA:NA,2016
Yuki Igarashi:Tsubasa Hiyama:Kaoru Arakawa,"We propose an interactive system to assist novices in the design and construction of original necklaces. The system consists of two design tools, an interactive drag-and-drop design tool using images of pearls and a design selection tool with an interactive evolutionary computation (IEC) system. The system includes a virtual modeling simulation which allows users to superimpose a necklace design over their own photograph, taken with a web camera. The system also provides a customized construction guide to assist the user with the construction process. We conduct a field trial to demonstrate that non-professional users can design original necklaces using our system.",An interactive system for original necklace design,NA:NA:NA,2016
Ryan Lustig:Balu Adsumilli:David Newman,"Image composition for GoPro videos captured in the presence of significant camera motion is a manual and time consuming process. Existing techniques typically fail to automate this process due to the wide-capture field of view and high camera motion of such videos. The proposed method seeks to solve these problems by developing an image registration algorithm for fisheye images without expensive pixel warping or loss of field of view. Background subtraction is performed to extract moving foreground objects, which are noise corrected and then layered on a reference image to build the final composite. The results show marked improvements in accuracy and efficiency for automating image composition.",Motion compensated automatic image compositing for GoPro videos,NA:NA:NA,2016
Caio Brito:Gutenberg Barros:Walter Correia:Veronica Teichrieb:João Marcelo Teixeira,"Accessible tactile pictures (ATPs) consist of tactile representations that convey different kinds of messages and present information through the sense of touch. Traditional approaches use contours and patterns, which create a distinct and recognizable shape and enables separate objects to be identified. The success rate for recognizing pictures by touch is much lower than it would be for vision. Besides that, some pictures are more frequently recognized than others. Finally, there is also some variation from individual to individual: while some blind people recognize many images, others recognize few. Auditory support can improve the points listed before, even eliminating the need for sighted assistance.",Multimodal augmentation of surfaces using conductive 3D printing,NA:NA:NA:NA:NA,2016
Junho Choi:Jong Hun Lee:Yong Yi Lee:Yong Hwi Kim:Bilal Ahmed:Moon Gu Son:Min Ho Joo:Kwan H. Lee,"Projection mapping has been widely used to efficiently visualize real world objects in various areas such as exhibitions, advertisements, and theatrical performances. To represent the projected content in a realistic manner, the appearance of an object should be taken into consideration. Although there have been various attempts to realistically represent the appearance through digital modeling of appearance materials in computer graphics, it is difficult to combine it with the projection mapping because it takes huge amount of time and requires large space for the measurement. To counteract these challenges of time and space, [Malzbender et al. 2001] present polynomial texture maps (PTM) that can represent the reflectance properties of the surface such as diffuse and shadow artifacts by relighting of the 3D objects according to varying light direction around the object. PTM does not have temporal or spatial constraints requiring only several tens of images of different light directions so that it makes it possible to easily produce an appealing appearance.",Realistic 3D projection mapping using polynomial texture maps,NA:NA:NA:NA:NA:NA:NA:NA,2016
Laura Ferrarello:Kevin Walker,"We describe a digital design process that interfaces real-time data with 3D modeling and 3D printing techniques. Digital Impressionism (DI) is a platform that explores new material possibilities, by 3D modeling physical and digital objects, as affected by invisible forces that act upon them in real time. Using a 3D pointcloud as a medium, we describe an experimental project run with students in our programme, incorporating realtime audio data to manipulate 3D physical forms, resulting in new static and dynamic shapes with what we call a hybrid materiality. The modeling platform of DI treats materials as composites, through which substance becomes physical via the digital interference the environment applies to digital forms. We describe dynamic modeling processes through which data enables a new hybrid tectonic made of composite shapes and materials. This abstract introduces the project and describes our methodology and results so far.",The form of sound through hybrid materials,NA:NA,2016
Adam Schuster:Anas Salah Eddin,"Hundreds of thousands of stereoscopic view cards (stereoviews) produced in the late 1800s and early 1900s are being digitized in collections and museums worldwide. However, viewing this important part of media technology history requires dedicated stereoscopes or difficult eye exercises. With the recent spread of Consumer VR, a fresh way to view, study, and learn from this vast store of knowledge becomes available. In this project, we present an automated method to process digitized stereoviews and make them suitable for VR viewing.",Vintage VR: a method of processing 19th century stereoviews for display on 21st century VR systems,NA:NA,2016
Pei-Hsuan Tsai:Yu-Hsuan Huang:Yu-Ju Tsai:Hao-Yu Chang:Masatoshi Chang-Ogimoto:Ming Ouhyoung,"Users always got bad experiences while using the general virtual reality head-mounted displays (HMDs) because of the low pixel density through optical lenses. For this reason, the narrow field-of-view (FoV) and high pixel density are the main goals we are going to pursue in the near-field video see-through augmented reality (AR) applications with sophisticated operations, such as the biological observation with AR microscope (e.g. Scope+ [Huang et al. 2015]), the AR surgery simulation, and telescope applications. Therefore with high resolution to see tiny objects clearly is the most important concern in this paper.",A modified wheatstone-style head-mounted display prototype for narrow field-of-view video see-through augmented reality,NA:NA:NA:NA:NA:NA,2016
Shinji Mizuno,"In this paper, I improve a tabletop stereoscopic 3DCG system with motion parallax so as to use it with two users and share a stereoscopic 3DCG scene together. I develop a method to calculate two users' viewpoints simultaneously by using depth images. I use a 3D-enabled projector to superimpose two 3DCG images for each user, and use active shutter glasses to separate them into individual images for each user. The improved system would be useable for cooperative works and match type games.",A tabletop stereoscopic 3DCG system with motion parallax for two users,NA,2016
Naoki Hashimoto:Daisuke Kobayashi,"We propose a dynamic spatial augmented reality (SAR) system with effective machine learning techniques and edge-based object tracking. Real-time 3D pose estimation is the significant problem of projecting images on moving objects. However, camera-based feature detection is difficult, because most targets have a texture-less surface. Image projection and projected images also interfere with detection. Obtaining 3D shape information with stereo-paired cameras [Resch et al. 2016] is still a time-consuming process, and using a depth sensor with IR [Koizumi et al. 2015] is still unstable and have a fatal time-delay for the dynamic SAR. Therefore, we quickly and robustly estimate the 3D pose of the target objects by using effective machine learning with IR images. And by the combined use of high-speed edge-based object tracking, we realize a stable and low-delay SAR for moving objects.",Dynamic spatial augmented reality with a single IR camera,NA:NA,2016
Yun Suen Pai:Benjamin Tag:Benjamin Outram:Noriyasu Vontin:Kazunori Sugiura:Kai Kunze,"We present a novel technique of implementing customized hardware that uses eye gaze focus depth as an input modality for virtual reality applications. By utilizing eye tracking technology, our system can detect the point in depth the viewer focusses on, and therefore promises more natural responses of the eye to stimuli, which will help overcoming VR sickness and nausea. The obtained information for the depth focus of the eye allows the utilization of foveated rendering to keep the computing workload low and create a more natural image that is clear in the focused field, but blurred outside that field.",GazeSim: simulating foveated rendering using depth in eye gaze for VR,NA:NA:NA:NA:NA:NA,2016
Akira Ishii:Ippei Suzuki:Shinji Sakamoto:Keita Kanai:Kazuki Takazawa:Hiraku Doi:Yoichi Ochiai,"Conventional research on pedestrian navigation systems has explored the possibilities of presenting information to users both visually and aurally. Existing navigation systems require users to recognize information, and then to follow directions as separate, conscious processes, which inevitably require attention to the system. This study proposes a novel method that enables pedestrians to be guided without conscious interaction with a navigational system.",Graphical manipulation of human's walking direction with visual illusion,NA:NA:NA:NA:NA:NA:NA,2016
Mike Lambeta:Matt Dridger:Paul White:Jesslyn Janssen:Ahmad Byagowi,"Virtual reality aims to provide an immersive experience to a user, with the help of a virtual environment. This immersive experience requires two key components; one for capturing inputs from the real world, and the other for synthesizing real world outputs based on interactions with the virtual environment. However, a user in a real world environment experiences a greater set of feedback from real world inputs which relate directly to auditory, visual, and force feedback. As such, in a virtual environment, a dissociation is introduced between the user's inputs and the feedback from the virtual environment. This dissociation relates to the discomfort the user experiences with real world interaction. Our team has introduced a novel way of receiving synthesized feedback from the virtual environment through the use of a haptic wheelchair.",Haptic wheelchair,NA:NA:NA:NA:NA,2016
Valentina Feldman,"This project is a synthesis of digital paleoart reconstruction, prototype VR pipeline design, and the remediation of structural narrative principles for immersive media. We approach common issues associated with the accurate portrayal of dinosaurs in media, Cinematic Virtual Reality (CVR) production, and the direction of viewer attention in immersive digital environments. After developing and testing a stable CVR workflow, we designed and produced a piece of scientific VR Paleoart content intended for educational outreach. Our production methods include a state-of-the-art CGI dinosaur reconstruction informed by comparative anatomy and biomechanical simulation, stereoscopic spherical rendering, and photographic CVR film production. Our approach is validated through the completion of a CVR documentary about the titanosaur Dreadnoughtus schrani, one of the largest dinosaurs yet discovered. This documentary, starring paleontologist Dr. Ken Lacovara, will be made publicly available for all common VR distribution platforms. Our goal is to make scientific CVR content accessible to an audience of mobile device owners, taking advantage of the VR media disruption to establish new design guidelines for educational media.",Immersive paleoart: reconstructing dreadnoughtus schrani and remediating the science documentary for cinematic virtual reality,NA,2016
MHD Yamen Saraiji:Shota Sugimoto:Charith Lasantha Fernando:Kouta Minamizawa:Susumu Tachi,"We propose ""Layered Telepresence"", a novel method of experiencing simultaneous multi-presence. Users eye gaze and perceptual awareness are blended with real-time audio-visual information received from multiple telepresence robots. The system arranges audio-visual information received through multiple robots into a priority-driven layered stack. A weighted feature map was created based on the objects recognized for each layer, using image-processing techniques, and pushes the most weighted layer around the users gaze in to the foreground. All other layers are pushed back to the background providing an artificial depth-of-field effect. The proposed method not only works with robots, but also each layer could represent any audio-visual content, such as video see-through HMD, television screen or even your PC screen enabling true multitasking.",Layered telepresence: simultaneous multi presence experience using eye gaze based perceptual awareness blending,NA:NA:NA:NA:NA,2016
Naoki Kawai:Cédric Audras:Sou Tabata:Takahiro Matsubara,"We propose a method to generate new views of a scene by capturing a few panorama images in real space and interpolating captured images. We describe a procedure for interpolating panoramas captured at four corners of a rectangle area without geometry, and present experimental results including walkthrough in real time. Our image-based method enables walking through space much more easily than using 3D modeling and rendering.",Panorama image interpolation for real-time walkthrough,NA:NA:NA:NA,2016
Fuko Takano:Takafumi Koike,"We propose a walk through imaging method for head-mounted display (HMD) named 'PlenoGap'. The method always displays a refocused image on a HMD. The refocused image is generated from a trimmed panorama light field image which is 360° cylindrical and always focused on center of HMD. In addition, we realized walkthrough experience by making some intermediate images between three panorama light field images. User can roam around small area using controller.",PlenoGap: panorama light field viewing for HMD with focusing on gazing point,NA:NA,2016
Mie Sato:Sota Suzuki:Daiki Ebihara:Sho Kato:Sato Ishigaki,"Bare hand interaction with a virtual object reduces uncomfortableness with devices mounted on a user's hand. There are some studies on the bare hand interaction[Benko et al. 2012], however a virtual object is supposed to be a hard object or a user touches a physical object during the bare hand interaction. We focus on grasping a virtual object without using any physical object. Grasping is one of the basic movements in manipulating an object and is more difficult than simple movements like touching an object. Because of the bare hand interaction with no physical object, there is no haptic device on a user's hand and so there is no physical feedback to the user. Our challenge is to provide a user with pseudo-softness while grasping a virtual object with a bare hand. We have been developing an AR system that makes it possible for a user to grasp a virtual object with a bare hand[Suzuki et al. 2014]. Using this AR system, we propose visual stimuli that correspond with the user's hand movements, to manipulate the pseudo-softness of a virtual object. Evaluation results show that with the visual stimuli a user feels pseudo-softness while grasping a virtual object with a bare hand.",Pseudo-softness evaluation in grasping a virtual object with a bare hand,NA:NA:NA:NA:NA,2016
Andrew Jacobson:Jinsil Hwaryoung Seo,"This paper presents PulmonaReality, an interactive virtual reality game aimed at young patients to help immerse them into a world that makes pulmonary function tests more enjoyable for the user while providing more reliable results for the examiner. Computer games designed to work with medical tests have been shown to have potential. While there are existing games out there, they are beginning to show their age in comparison to many games played by modern-day patients. The design of our project focuses on usability and enjoyment for young children. In our preliminary user studies, children reported that the system was easy to use with minimal instruction and evoked a sense of wonder when they experienced our different interactive 3D environments.",PulmonaReality: transforming pediatric pulmonary function experience using virtual reality,NA:NA,2016
Ari Rapkin Blenkhorn:Yu Wang:Marc Olano,"We have created a suite of automated tools to calibrate and configure a projection virtual reality system. Test subjects (rats) explore an interactive computer-graphics environment presented on a large curved screen using multiple projectors. The locations and characteristics of the projectors can vary and the shape of the screen may be complex. We place several cameras around the workspace for redundant coverage. We locate each projector's hotspot as seen by each camera, and produce a brightness profile which tells the projector how much to dim each pixel of its output to achieve uniform output. We reconstruct the 3D geometry of the screen and the location of each projector using shape-from-motion and structured-light multi-camera computer vision techniques. We determine which projected pixel corresponds to a given view direction for the rat. From these, we create a warping profile for each projector, which tells it how to pre-distort its output image to appear undistorted to the rat's viewpoint. We apply both pre-distortion and hotspot correction before displaying to the screen.",RatCAVE: calibration of a projection virtual reality system,NA:NA:NA,2016
Rodrigo M. A. Silva:Bruno Feijó:Pablo B. Gomes:Thiago Frensh:Daniel Monteiro,"In this paper we propose a real time 360° video stitching and streaming processing methodology focused on GPU. The solution creates a scalable solution for large resolutions, such as 4K and 8K per camera, and supports broadcasting solutions with cloud architectures. The methodology uses a group of deformable meshes, processed using OpenGL (GLSL) and the final image combine the inputs using a robust pixel shader. Moreover, the result can be streamed to a cloud service using h.264 encoding with nVEnc GPU encoding. Finally, we present some results.",Real time 360° video stitching and streaming,NA:NA:NA:NA:NA,2016
Ayaka Nishi:Keisuke Hoshino:Hiroyuki Kajimoto,"Virtually infinite space is a holy grail of immersive virtual environment (IVE), and numerous approaches have been proposed, yet there still is a hardware and spatial cost. We propose a novel low-cost locomotion interface that combines a 1-DoF treadmill and a head mounted display (HMD), in which displayed image is rotated to induce straightened trajectory for the treadmill, similar to the technique known as Redirected Walking. We conducted an experiment using the proposed method, and showed that by using PD control algorithm, the walking path became straightened.",Straightening walking path using redirected walking technique,NA:NA:NA,2016
Yukari Konishi:Nobuhisa Hanamitsu:Kouta Minamizawa:Ayahiko Sato:Tetsuya Mizuguchi,"The Synesthesia Suit provides immersive embodied experience in Virtual Reality environment with vibro-tactile sensations on the entire body. Each vibro-tactile actuator provides not a simple vibration such as traditional game controller, but we designed the haptic sensation based on the haptic design method we have developed in the TECHTILE[Minamizawa et al. 2012] technology. In haptics research using multi-channel vibro-tactile feedback, Surround Haptics [Israr et al. 2012] proposed moving tactile strokes using multiple vibrators spaced on a gaming chair. And then they also proposed Po2[Israr et al. 2015], which shows illusion of tactile sensation for gesture based games by providing vibrations on the hand based on psycho-physical study.",Synesthesia suit: the full body immersive experience,NA:NA:NA:NA:NA,2016
Yu-Xiang Wang:Yu-Ju Tsai:Yu-Hsuan Huang:Wan-Ling Yang:Tzu-Chieh Yu:Yu-Kai Chiu:Ming Ouhyoung,"For stereoscopic augmented reality (AR) system, continuous feature tracking of the observing target is required to generate a virtual object in the real world coordinate. Besides, dual cameras have to be placed with proper distance to obtain correct stereo images for video see-through applications. Both higher resolution and frame rate per second (FPS) can improve the user experience. However, feature tracking could be the bottleneck with high resolution images and the latency would increase if image processing was done before tracking.",ThirdEye: a coaxial feature tracking system for stereoscopic video see-through augmented reality,NA:NA:NA:NA:NA:NA:NA,2016
Chun-Jui Lai:Ping-Hsuan Han:Yi-Ping Hung,"By using the head-mounted display (HMD), we can have an immersive virtual reality experience. But the user cannot see any information from the real world. To solve the problem, video seethrough HMD can acquire images from real environment, and present into the HMD, then, we could build a mixed reality (MR) or augmented reality (AR) system. However, how to append and calibrate cameras on HMD for recovering real environment is still a research issue. HTC VIVE has a single camera in front of its device. [Steptoe et al. 2014] and OVRVISION Pro proposed to append dual cameras to capture left and right images. Due to the difference of viewpoint, images captured by cameras are different to what human eyes see (figure 2). Although we could recover true 3D information with a depth map, there are still some occlusion areas that we cannot recover by single camera. Therefore, multiple cameras with different positions could complement each other for reducing occlusion areas. In this work, four configurations are simulated with a synthesized scene.",View interpolation for video see-through head-mounted display,NA:NA:NA,2016
"Alexandre Cardoso:Edgard Lamounier, Jr.:Gerson Flavio Mendes de Lima:Paulo Roberto do Prado:José Newton Ferreira","In this work, we propose a Virtual Reality based solution to provide a more natural and intuitive environment for controlling electrical operation centers. The research is being carried out with the collaboration of one electric company called Cemig. The novelty of this approach is the ability operators will have to manage the electric system and its electric components by being immersed within a 3D world, reflecting the very true arrangement found in the real electrical substation. Besides, the solution has been designed in a way to provide the operator with all supervisory data in the same virtual environment. We have conducted experiments with the electric company operators Mental efforts to understand the reality of the field have been reduced, according to Cemig's employees. They also claim that a unique environment with all data integrated is very important for taking engineering decisions.",VRCEMIG: a novel approach to power substation control,NA:NA:NA:NA:NA,2016
Henry Fernandez:Koji Mikami:Kunio Kondo,"For high skilled players, an easy game might become boring and for low skilled players, a difficult game might become frustrating. This research's goal is to offer players a personalized experience adapted according to their performance and levels of attention. We created a simple side-scrolling 2D platform game using Procedural Content Generation, Dynamic Difficulty Adjustment techniques and brain computer data obtained from players in real time using an Electroencephalography device. We conducted a series of experiments with different players and got results that confirm that our method is adjusting each level according to performance and attention.",Adaptable game experience through procedural content generation and brain computer interface,NA:NA:NA,2016
Jhe-Wei Lin:Po-Wen Cheng:Chien-hung Lin:I-Chen Lin,"Object movement in virtual 3D space involves 3D rotations and translations. Conventional desktop interfaces are difficult for a user to simultaneously control the variations of six degrees of freedom. In this paper, we present a vision-based user-friendly method for this purpose. A user can simply grasp one or more objects and fly the objects intuitively. The proposed system using an efficient method to detect and track the objects in the scene. The estimated object motions are used to drive the corresponding targets on the screen or to trigger predefined actions. This interface can track more than one objects at an interactive rate and is applied to a 3D flight shooting game.",Intuitive 3D flight gaming with tangible objects,NA:NA:NA:NA,2016
Pedro Rossa:Nicolas Hoffman:João Ricardo Bittencourt:Fernando Marson:Vinicius J. Cassol,In this work we explore the use of games and VR in order to collaborate with History teaching in Brazil. We develop a game and a VR experience based on local technology. In or approach the player is considered as an Indian who lived in the Jesuitical Reductions in the South of Brazil and was requested to practice bow and arrow shooting.,Living the past: the use of VR to provide a historical experience,NA:NA:NA:NA:NA,2016
Jae-Ho Nah:Sunho Ki:Yeongkyu Lim:Jinhong Park:Chulho Shin,"Post-processing anti-aliasing algorithms are widely used now for real-time rendering because of their simplicity, performance, and suitability for deferred shading. Fast approximate anti-aliasing (FXAA) [Lottes 2009] is the fastest method among them, so many games support FXAA to get anti-aliased images. However, FXAA can easily lose texture details and text sharpness due to its excessive blurring. To alleviate those problems of FXAA, we present adaptive approximate anti-aliasing (AXAA). Our approach adds three contributions to FXAA in order to avoid unnecessary filtering. First, we stop further anti-aliasing processes if the current pixel or its neighbors are judged as pixels on already filtered textures or fonts. Second, we try to maintain thin lines as much as possible in order to avoid blurring fonts and lines. Third, for higher performance, we adaptively set the search range of each pixel according to luma contrast. Our experiments show that AXAA provides significantly better image quality than FXAA, in terms of texture, text, and geometry details. Nevertheless, processing overhead of AXAA is still similar to that of FXAA.",AXAA: adaptive approximate anti-aliasing,NA:NA:NA:NA:NA,2016
Amir Semmo:Matthias Trapp:Tobias Dürschmid:Jürgen Döllner:Sebastian Pasewaldt,"This work presents an interactive mobile implementation of a filter that transforms images into an oil paint look. At this, a multi-scale approach that processes image pyramids is introduced that uses flow-based joint bilateral upsampling to achieve deliberate levels of abstraction at multiple scales and interactive frame rates. The approach facilitates the implementation of interactive tools that adjust the appearance of filtering effects at run-time, which is demonstrated by an on-screen painting interface for per-pixel parameterization that fosters the casual creativity of non-artists.",Interactive multi-scale oil paint filtering on mobile devices,NA:NA:NA:NA:NA,2016
Shintaro Murakami:Tomoyuki Mukasa:Tony Tung,"We present a new feature for AR/VR applications for consumer mobile devices equipped with video camera (e.g., smartphone). Direct or indirect scale estimation of scene or objects is necessary for realistic rendering of virtual objects in real-world environment. Standard approaches usually rely on 3D vision with sensor fusion (e.g., visual SLAM), or pattern recognition (e.g., using AR markers, reference object learning), and suffer from various limitations. Here, we argue that combining inertial measurements and visual cues, the problem reduces to a 1D parameter estimation representing distance from device to floor. In particular, we discuss robust solutions to solve absolute scale estimation problem for indoor environments.",Mobile virtual interior stylization from scale estimation,NA:NA:NA,2016
Roberto Lopez Mendez:Sylwester Bala,"Local cubemaps (LC) were introduced for the first time more than ten years ago for rendering reflections [Bjorke 2004]. Nevertheless it is only in recent years that major game engines have incorporated this technique. In this paper we introduce a generalized concept of LC and present two new LC applications for rendering shadows and refractions. We show that limitations associated with the static nature of LC can be overcome by combining this technique with other well-known runtime techniques for reflections and shadows. Rendering techniques based on LC allow high quality shadows, reflections and refractions to be rendered very efficiently which makes them ideally suited to mobile devices where runtime resources must be carefully balanced [Ice Cave Demo 2015].",Optimized mobile rendering techniques based on local cubemaps,NA:NA,2016
Victoria McGowen:Joe Geigel,"Blend shape animation is one of the most common methods for facial animation used by animators. Creation of effective blend shapes is an investment as they can take a very long time to create, but once finished, help with consistency in animation. As this is a prime animation method, its extensive process can be off-putting to newcomers. This project is focused on creating a system that will automate the blend shape creation process. The resulting blend shapes could be used in a blend shape based facial motion capture system (eg. [Weise et al. 2011]). The goal of this application is to produce a comparable result to that of blend shapes done by hand for student projects.",Automatic blend shape creation for facial motion capture,NA:NA,2016
Wakana Asahina:Naoya Iwamoto:Hubert. P. H. Shum:Shigeo Morishima,"In recent years, thanks to the development of 3DCG animation editing tools (e.g. MikuMikuDance), a lot of 3D character dance animation movies are created by amateur users. However it is very difficult to create choreography from scratch without any technical knowledge. Shiratori et al. [2006] produced the dance automatic generation system considering rhythm and intensity of dance motions. However each segment is selected randomly from database, so the generated dance motion has no linguistic or emotional meanings. Takano et al. [2010] produced a human motion generation system considering motion labels. However they use simple motion labels like ""running"" or ""jump"", so they cannot generate motions that express emotions. In reality, professional dancers make choreography based on music features or lyrics in music, and express emotion or how they feel in music. In our work, we aim at generating more emotional dance motion easily. Therefore, we use linguistic information in lyrics, and generate dance motion.",Automatic dance generation system considering sign language information,NA:NA:NA:NA,2016
Shota Ekuni:Koichi Murata:Yasunari Asakura:Akira Uehara,"Visual extension has been an essential issue because the visual information accounts for a large part of sensory information which human processes. There are some instruments which are used to watch distant, objects or people, such as a monocle, a binocular, and a telescope. When we use these instruments, we firstly take a general view without them and adjust magnification and focus of them. These operations are complicated and occupy the user's hands. Therefore, a visual extension device that is capable of being used easily without hands is extremely useful. A system developed in the previous work recognizes the movement of the user's eyelid and operating devices by using it [Hideaki et al. 2013]. However, a camera is placed in front of the eye, and that obstructs the field of view. In addition, image recognition needs much calculation cost and it is difficult to be processed in a small computer. When human intends to move his/her muscles, bioelectrical signal (BES) leaks out on the surface of skin. The BES can be measured by small and thin electrodes attached to the surface of the skin. By using the BES, user's operational intentions can be detected promptly without obstructing the user's field of view. Moreover, using BES sensors can reduce electrical power, and contribute to downsizing systems.",Bionic scope: wearable system for visual extension triggered by bioelectrical signal,NA:NA:NA:NA,2016
Kai-Lin Chuang,"Dynamic Frame Rate (DFR) is the change in frame rate of a movie sequence in real time as the sequence is playing. Throughout the majority of the past century and after the introduction of sound in films, frame rates used in films have been kept at a standardization of 24 frame per second despite technological advancement [Salmon et. Al 2011]. In the past decade, spatial resolution has been increasing in display systems while the temporal resolution, the frame rate, has not been changed. Because of this, researchers and filmmakers stress that motion judders and blurriness are much more apparent and they propose that high frame rates will solve the issue [Emoto et. Al 2014] [Turnock 2013]. Some industry experts and critics, however, oppose the use of high frame rates [Wilcox 2015]. Despite all the research and attempts in using high frame rate, the idea of using dynamic frame rate in digital cinema has not been explored in depth. As such, there is very limited information on how people perceive DFR and how it actually works. By understanding DFR and how viewers perceive the changes in frame rate, it will help us adapt new techniques in the creation of cinema. We can utilize high frame rate in sequences that could benefit from high frame rate while keeping the rest of the sequences at standard frame rate. This thesis aims to understand the basics of DFR, how different implementations of DFR changes viewer perception and how people perceive a change of frame rate in an animated movie sequence displayed.",Dynamic frame rate: a study on viewer perception of changes in frame rate within an animated movie sequence,NA,2016
Jose A. S. Fonseca:Denis Kravtsov:Anargyros Sarafopoulos:Jian J. Zhang,"Effective communication through character animation depends on the recognition of the performed body expressions. The creation of the right body postures is crucial for character animation in the context of animated films and games, as it allows for conveying the right set of emotions to the viewer. Audience needs to be able to identify familiar features mainly based on their own experiences, which allows the viewer to relate and feel empathy to observed characters. It is, therefore, crucial for the animator to accurately create the right posture and expressive body motion, during the posing phase of the animation process.",Enhancement of 3D character animations through the use of automatically generated guidelines inspired by traditional art concepts,NA:NA:NA:NA,2016
Simone Barbieri:Nicola Garau:Wenyu Hu:Zhidong Xiao:Xiaosong Yang,"Sketch as the most intuitive and powerful 2D design method has been used by artists for decades. However it is not fully integrated into current 3D animation pipeline as the difficulties of interpreting 2D line drawing into 3D. Several successful research for character posing from sketch has been presented in the past few years, such as the Line of Action [Guay et al. 2013] and Sketch Abstractions [Hahn et al. 2015]. However both of the methods require animators to manually give some initial setup to solve the corresponding problems. In this paper, we propose a new sketch based character posing system which is more flexible and efficient. It requires less input from the user than the system from [Hahn et al. 2015]. The character can be easily posed no matter the sketch represents a skeleton structure or shape contours.",Enhancing character posing by a sketch-based interaction,NA:NA:NA:NA:NA,2016
Chun-Kai Huang:Tsung-Hung Wu:Yi-Ling Chen:Bing-Yu Chen,"In recent years, personalized fabrication has attracted much attention due to the greatly improved accessibility of consumer-level 3D printers. However, 3D printers still suffer from the relatively long production time and limited output size, which are undesirable for large-scale rapid-prototyping. Zometool, which is a popular building block system widely used for education and entertainment, is potentially suitable for providing an alternative solution to the aforementioned scenarios. However, even for 3D models of moderate complexity, novice users may still have difficulty in building visually plausible results by themselves. Therefore, the goal of this work is to develop an automatic system to assist users to realize Zometool rapid prototyping with a specified 3D shape. Compared with the previous work [Zimmer and Kobbelt 2014], our method may achieve the ease of assembly and economic usage of building units since we focus on generating the Zometool structures through a higher level of shape abstraction.",Large-scale rapid-prototyping with zometool,NA:NA:NA:NA,2016
Gustavo E. Boehs:Milton L. H. Vieira,"We propose a framework for using human acting as input for the animation of non-humanoid creatures; captured motion is classified using machine learning techniques, and a combination of preexisting clips and motion retargeting are used to synthetize new motions. This should lead to a broader use of motion capture.",Non-humanoid creature performance from human acting,NA:NA,2016
Cyril Corvazier:Benjamin Legros:Rachid Chikh,"We present a new storage scheme for computer graphic images based on OpenEXR 2. Using such EXR/Id files, the compositing artist can isolate an object selection (by picking them or using a regular expression to match their names) and color corrects them with no edge artefact, which was not possible to achieve without rendering the object selection on its own layer. Using this file format avoids going back and forth between the rendering and the compositing departments because no mask image or layering are needed anymore. The technique is demonstrated in an open source software suite, including a library to read and write the EXR/Id files and an OpenFX plug-in which generates the images in any compositing software.",OpenEXR/Id isolate any object with a perfect antialiasing,NA:NA:NA,2016
Fabio Turchet:Marco Romeo:Oleg Fryazinov,"Recent developments in character rigging and animation shape the computer graphics industry in general and visual effects in particular. Advances in deformation techniques, which include linear blend skinning, dual quaternion skinning and shape interpolation, meet with sophisticated muscle and skin simulations to produce more realistic results. Effects such as skin sliding, wrinkling and contact of subcutaneous fat and muscles become possible when simulating the anatomy of human-like characters as well as creatures in feature films. One of the main techniques adopted nowadays in the industry is the Finite Element Method (FEM) for deformable objects. Despite the life-like results, the setup cost to generate and tweak volumetric anatomical models for a FEM solver is not only very high, but it cannot easily guarantee the quality of the models either, in terms of simulation requirements. In a production environment in fact (see Fig. 1), models often require additional processing in order to be ready for FEM simulations. For example, self-intersections or interpenetrations in rest pose may result in unwanted forces from the collision detection and response algorithms that affect negatively the simulation at its start.",Physics-aided editing of simulation-ready muscles for visual effects,NA:NA:NA,2016
Deschanel Li,"It is currently possible to reliably motion-track humans and some animals, but not possible to track insects using standard motion tracking techniques. By programming a virtual prototype rig/skeleton for the insects small scale creatures will be able to be tracked in real time. Possible applications include behavioural research of animals and entertainment industry, e.g., when realistic insect motion simulation is needed and insects cannot be outfitted with sensors like humans for animation in movies or games.",Towards real-time insect motion capture,NA,2016
Tsukasa Nozawa:Takuya Kato:Pavel A. Savkin:Naoki Nozawa:Shigeo Morishima,"3D facial shape reconstruction in the wild environments is an important research task in the field of CG and CV. This is because it can be applied to a lot of products, such as 3DCG video games and face recognition. One of the most popular 3D facial shape reconstruction techniques is 3D Model-based approach. This approach approximates a facial shape by using 3D face model, which is calculated by principal component analysis. [Blanz and Vetter 1999] performed a 3D facial reconstruction by fitting points from facial feature points of an input of single facial image to vertex of template 3D facial model named 3D Morphable Model. This method can reconstruct a facial shape from a variety of images which include different lighting and face orientation, as long as facial feature points can be detected. However, representation quality of the result depends on the number of 3D model resolution.",3D facial geometry reconstruction using patch database,NA:NA:NA:NA:NA,2016
Murat Kurt:Greg Ward:Nicolas Bonneel,"We present a data-driven Bidirectional Scattering Distribution Function (BSDF) representation and a model-free technique that preserves the integrity of the original data and interpolates reflection as well as transmission functions for arbitrary materials. Our interpolation technique employs Radial Basis Functions (RBFs), Radial Basis Systems (RBSs) and displacement techniques to track peaks in the distribution. The proposed data-driven BSDF representation can be used to render arbitrary BSDFs and includes an efficient Monte Carlo importance sampling scheme. We show that our data-driven BSDF framework can be used to represent measured BSDFs that are visually plausible and demonstrably accurate.",A data-driven BSDF framework,NA:NA:NA,2016
Carlos Aliaga:Carlos Castillo:Diego Gutierrez:Miguel A. Otaduy:Jorge Lopez-Moreno:Adrian Jarabo,"Rendering realistic fabrics is an active research area with many applications in computer graphics and other fields like textile design. Reproducing the appearance of cloth remains challenging due to the micro-structures found in textiles, and the complex light scattering patterns exhibited at such scales. Recent approaches have reached very realistic results, either by directly modeling the arrangement of the fibers [Schröder et al. 2011], or capturing the structure of small pieces of cloth using Computed Tomography scanners (CT) [Zhao et al. 2011]. However, there is still a need for predictive modeling of cloth appearance; existing methods either rely on manually-set parameter values, or use photographs of real pieces of cloth to guide appearance matching algorithms, often assuming certain simplifications such as considering circular or elliptical cross sections, or assuming an homogeneous volume density, that lead to very different appearances.",A fiber-level model for predictive cloth rendering,NA:NA:NA:NA:NA:NA,2016
Bilal Ahmed:Jong Hun Lee:Yong Yi Lee:Junho Choi:Yong Hwi Kim:Moon Gu Son:Min Ho Joo:Kwan H. Lee,"Recently researchers have shown much interest in 3D projection mapping systems but relatively less work has been done to make the contents look realistic. Much work has been done for multi-projector blending, 3D projection mapping and multi-projector based large displays but existing color compensation based systems still suffer from contrast compression, color inconsistencies and inappropriate luminance over the three dimensional projection surface giving rise to an un-appealing appearance. Until now having a realistic result with projection mapping on 3D objects when compared with a similar original object still remains a challenge. In this paper, we present a framework that optimizes projected images using multiple projectors in order to achieve an appearance that looks close to a real object whose appearance is being regenerated by projection mapping.",A method for realistic 3D projection mapping using multiple projectors,NA:NA:NA:NA:NA:NA:NA:NA,2016
Masato Ishimuroya:Takashi Kanai,"We propose a method for adding visual details to fluid animation while reducing noisy appearances. In grid-based fluid simulations, an issue is that while highly detailed fluids with small eddies can be obtained by increasing the number of grid cells, it costs much more computational time. To address this, various methods for adding details (or up-scaling resolutions) have been proposed. Those methods can generate fine animations quickly by adding high-frequency noises or external forces to coarse simulation results. However, those methods typically generate tiny eddies on a whole surface of fluid and the result appears too noisy. In this paper, we consider the distribution of kinetic energy in the spatial frequency domain and then apply it to two existing methods for adding details. By using our method, noises or external forces can be added to the appropriate positions of fluids and consequently natural-looking details can be achieved.",Adding visual details based on low-resolution energy cascade ratios for smoke simulation,NA:NA,2016
Tatsuya Matsumoto:Kensuke Tobitani:Yusuke Tani:Hiroki Fujii:Noriko Nagata,"The visual expression of a surface quality of human skin is required in a wide range of fields, such as in cosmetics industry. It is much harder, however, to generate intuitively and accurately a computer graphics (CG) image of human skin that has a desired impression without professional knowledge and skills because of its complex physical properties. It is necessary to model the linkage between the visual impressions and physical properties of human skin in order to effectively determine parameters for the generation of CG images.",An evaluation of the relationship between impression and the physical properties of human skin,NA:NA:NA:NA:NA,2016
Qian Chen:Haiyuan Wu:Shinichi Higashino:Ryuuki Sakamoto,"In this paper, we present a convenient method for camera calibration with arbitrary co-planar circle-pairs from one image. This method is based on the accurate recovery of the projected centers of the circle pairs using a closed-form algorithm.",Camera calibration by recovering projected centers of circle pairs,NA:NA:NA:NA,2016
Miyu Iwafune:Taisuke Ohshima:Yoichi Ochiai,"We propose novel design method to fabricate user interfaces with mechanical metamaterial called Coded Skeleton. The Coded Skeleton is combination of shape memory alloys (SMA) and 3-D printed bodies, and it has computationally designed structure that is flexible in one deformation mode but is stiff in the other modes. This property helps to realize materials that automatically deform by a small and lightweight actuator such as SMA. Also it enables to sense user inputs with the resistance value of SMA. In this paper, we propose shape-changing user interfaces by integrating sensors and actuators as Coded Skeleton. The deformation and stiffness of this structure is computationally designed and also controllable. Further, we propose interactions and applications with user interfaces fabricated using our design method.",Coded skeleton: programmable bodies for shape changing user interfaces,NA:NA:NA,2016
Satoshi Hashizume:Kazuki Takazawa:Amy Koike:Yoichi Ochiai,"The representation of texture is a major concern during fabrication and manufacturing in many industries. Thus, the approach for fabricating everyday objects and the digital expression of their textures before fabrication process has become a popular research area. Although it is easy to change the texture of objects in the digital world (i.e. just setting texture parameters), it is difficult to achieve this in the real world.",Cross-field haptics: push-pull haptics combined with magnetic and electrostatic fields,NA:NA:NA:NA,2016
Kaimo Hu:Dong-Ming Yan:Bedrich Benes,"Surface remeshing is a key component in many geometry processing applications. However, existing high quality remeshing methods usually introduce approximation errors that are difficult to control, while error-driven approaches pay little attention to the meshing quality. Moreover, neither of those approaches can guarantee the minimal angle bound in resulting meshes. We propose a novel error-bounded surface remeshing approach that is based on minimal angle elimination. Our method employs a dynamic priority queue that first parameterize triangles who contain angles smaller than a user-specified threshold. Then, those small angles are eliminated by applying several local operators ingeniously. To control the geometric fidelity where local operators are applied, an efficient local error measure scheme is proposed and integrated in our remeshing framework. The initial results show that the proposed approach is able to bound the geometric fidelity strictly, while the minimal angles of the results can be eliminated to be up to 40 degrees.",Error-bounded surface remeshing with minimal angle elimination,NA:NA:NA,2016
Masashi Baba:Kesuke Haruta:Shinsaku Hiura,"To create realistic CG images, the information about the lighting is very important. There are two ways to estimate the information of the light source. One is a direct measurement method using images captured with a fish-eye lens or a spherical mirror[Debevec 1998], and the other is an indirect measurement method to estimate positions and intensities of the light sources from the shadow information of objects[Sato et al. 2003]. In the direct measurement method, by concerning pixels of the captured image as light sources having corresponding intensities, it is possible to estimate the lighting environment densely. However, for a high-intensity light source like the sun, the dynamic range of the camera is insufficient, and the radiant intensity of the light source cannot be accurately estimated. So, we propose a method that combines a direct measurement technique and an indirect measurement method. In our proposed method, the light source information of the high-intensity area in the captured image is estimated by indirect measurement method. In the experiments using real images, even for outdoor scenes that contain the high-intensity light source like the sun, the measurement of the light source environment could be performed by the proposed method. Also, it was confirmed that images including realistic shadows equivalent to real images could be created.",Estimating lighting environments based on shadow area in an omni-directional image,NA:NA:NA,2016
Seungbae Bang:Meekyoung Kim:Doekkyeong Jang:Sung-Hee Lee,"Knee brace is a sports product or medical equipment that increases the stability in the dynamics of the knee. The proper design of a subject-specific knee brace should take her anatomical characteristics into account since they are influential to the knee dynamics. However, anatomical information is hidden under the skin, and obtaining such information is restricted to expensive equipments such as Magnetic Resonance Imaging (MRI) device or Computed Tomography (CT) scan device.",Estimating skeleton from skin data for designing subject-specific knee braces,NA:NA:NA:NA,2016
Sara C. Schvartzman:Marco Romeo,"Digital characters are common in modern films visual effects and the demand for digital actors has increased during the past few years. The success of digitally created actors is related to their believability and, in particular, the realism of the animation and simulation of their faces. Facial expressions in computer graphics are commonly obtained through linear vertex interpolation techniques such as blend shapes. These enable high artistic control and fast interaction, but cannot properly reproduce collisions or other physical phenomena such as gravity and inertia. These effects can be achieved by applying simulation techniques over the animated facial geometry (e.g. muscle simulation), but could potentially alter the look of the desired facial expression and produce inconsistencies with the work approved in animation. Moreover, animating such muscle rigs can be very cumbersome.",Example-based data optimization for facial simulation,NA:NA,2016
K. Edum-Fotwe:P. Shepherd:M. Brown:D. Harper:R. Dinnis,"This simple paper describes an intuitive data-driven approach to reconstructing architectural facade models from unstructured point-clouds. The algorithm presented yields sparse semantically-rich models that are better suited to interactive simulation than the equivalent dense-reconstructions, yet executes significantly faster than the prevalent sparse-operators. The key advantages include accuracy, efficiency and the ability to model irregular windows.","Fast, accurate and sparse, automatic facade reconstruction from unstructured ground laser-scans",NA:NA:NA:NA:NA,2016
Karan Sharma:Arun C. S. Kumar:Suchendra Bhandarkar,"Large scale object classification has seen commendable progress owing, in large part, to recent advances in deep learning. However, generating annotated training datasets is still a significant challenge, especially when training classifiers for large number of object categories. In these situations, generating training datasets is expensive coupled with the fact that training data may not be available for all categories and situations. Such situations are generally resolved using zero-shot learning. However, training zero-shot classifiers entails serious programming effort and is not scalable to very large number of object categories. We propose a novel simple framework that can guess objects in an image. The proposed framework has the advantages of scalability and ease of use with minimal loss in accuracy. The proposed framework answers the following question: How does one guess objects in an image from very few object detections?",Guessing objects in context,NA:NA:NA,2016
Martin Šik:Jaroslav Křivánek,"Markov Chain Monte Carlo (MCMC) has recently received a lot of attention in light transport simulation research [Hanika et al. 2015; Hachisuka et al. 2014]. While these methods aim at high quality sampling of local extremes of the path space (so called local exploration), the other issue - discovering these extremes - has been so far neglected. Poor global exploration results in oversampling some parts of the paths space, while undersampling or completely missing other parts (see Fig. 1). Such behavior of MCMC-based light transport algorithms limits their use in practice, since we can never tell for sure whether the image has already converged.",Improving global exploration of MCMC light transport simulation,NA:NA,2016
Ana Serrano:Diego Gutierrez:Karol Myszkowski:Hans-Peter Seidel:Belen Masia,"Many different techniques for measuring material appearance have been proposed in the last few years. These have produced large public datasets, which have been used for accurate, data-driven appearance modeling. However, although these datasets have allowed us to reach an unprecedented level of realism in visual appearance, editing the captured data remains a challenge. In this work, we develop a novel methodology for intuitive and predictable editing of captured BRDF data, which allows for artistic creation of plausible material appearances, bypassing the difficulty of acquiring novel samples. We synthesize novel materials, and extend the existing MERL dataset [Matusik et al. 2003] up to 400 mathematically valid BRDFs. We design a large-scale experiment with 400 participants, gathering 56000 ratings about the perceptual attributes that best describe our extended dataset of materials. Using these ratings, we build and train networks of radial basis functions to act as functionals that map the high-level perceptual attributes to an underlying PCA-based representation of BRDFs. We show how our approach allows for intuitive edits of a wide range of visual properties, and demonstrate through a user study that our functionals are excellent predictors of the perceived attributes of appearance, enabling predictable editing with our framework.",Intuitive editing of material appearance,NA:NA:NA:NA:NA,2016
Terence Broad:Mick Grierson,"Both light field photography and focal stack photography are rapidly becoming more accessible with Lytro's commercial light field cameras and the ever increasing processing power of mobile devices. Light field photography offers the ability of post capturing perspective changes and digital refocusing, but little is available in the way of post-production editing of light field images. We present a first approach for interactive content aware completion of light fields and focal stacks, allowing for the removal of foreground or background elements from a scene.",Light field completion using focal stack propagation,NA:NA,2016
Yoshinori Dobashi:Takashi Ijiri:Hideki Todo:Kei Iwasaki:Makoto Okabe:Satoshi Nishimura,"Realistic image synthesis is an important research goal in computer graphics. One important factor to achieve this goal is a bidirectional reflectance distribution function (BRDF) that mainly governs an appearance of an object. Many BRDF models have therefore been developed. A physically-based BRDF based on microfacet theory [Cook and Torrance 1982] is widely used in many applications since it can produce highly realistic images. The microfacetbased BRDF consists of three terms; a Fresnel, a normal distribution, and a geometric functions. There are many analytical and approximate models for each of these terms.",Measuring microstructures using confocal laser scanning microscopy for estimating surface roughness,NA:NA:NA:NA:NA:NA,2016
Tuur Stuyck:Philip Dutré,"Physics-based animation has become an important tool in computer graphics and is essential in recreating realistic looking natural phenomena. Researchers have been looking for tools to control passive simulations that allow artists to easily modify the simulation to best suit the artistic requirements. However, fluid motion is very hard to predict and it is very difficult, if not impossible, to achieve specific behavior just by altering the global variables. Active control of the simulation will be necessary to achieve this goal.",Model predictive control for robust art-directable fluids,NA:NA,2016
Lode Jorissen:Patrik Goorts:Gauthier Lafruit:Philippe Bekaert,"In recent years there is a growing interest in the generation of virtual views from a limited set of input cameras. This is especially useful for applications such as Free Viewpoint Navigation and light field displays [Tanimoto 2015]. The latter often requires tens to hundreds of input views, while it is often not feasible to record with as many cameras. View interpolation algorithms often traverse a set of depths to find correspondences between the input images [Stankiewicz et al. 2013; Goorts et al. 2013]. Most algorithms choose a uniform set of depths to traverse (as shown in Figure 2(a)), but this often leads to an excessive amount of unnecessary calculations in regions where no objects are located. It also results in an increased amount of mismatches, and thus, inaccuracies in the generated views. These problems also occur when a too large depth range is selected. Hence, typically a depth range that encloses the scene tightly is manually selected to mitigate these errors. A depth distribution that organizes the depth layers around the objects in the scene, as shown in Figure 2(b), would reduce these errors and decrease the number of computations by reducing the number of depths to search through. [Goorts et al. 2013] determine a nonuniform global depth distribution by reusing the generated depth information from the previous time stamp. This makes the algorithm dependent on previous results.",Nonuniform depth distribution selection with discrete Fourier transform,NA:NA:NA:NA,2016
Chloe LeGendre:Xueming Yu:Paul Debevec,"We demonstrate the sufficiency of using as few as five LEDs of distinct spectra for multispectral lighting reproduction and solve for the optimal set of five from 11 such commercially available LEDs. We leverage published spectral reflectance, illuminant, and camera spectral sensitivity datasets to show that two approaches of lighting reproduction, matching illuminant spectra directly and matching material color appearance observed by one or more cameras or a human observer, yield the same LED selections. Our proposed optimal set of five LEDs includes red, green, and blue with narrow emission spectra, along with white and amber with broader spectra.",Optimal LED selection for multispectral lighting reproduction,NA:NA:NA,2016
Kuo-Wei Chen:Chih-Yuan Yao:Yu-Chi Lai:You-En Lin,"Although 3D printing is becoming more popular, but there are two major problem. The first is the slowness of the process because of requirement of processing information of an extra axis comparing to tradition 2D printers. The second is the printable dimension of 3D printers. Generally, the larger the model is printed, the larger a 3D printer has to be and the more expensive it is. Furthermore, it would also require a large amount of extra inflation materials. With the entrance of cheap 3D printers, such as OLO 3D printers [Inc. 2016], parallel printing with multiple cheap printers can possibly be the solution. In order to parallel print a 3D model, we must decompose a 3D model into smaller components. After printing out all the components, we assemble them together by attaching them to the skeleton through supporters and joints to form the final result. As shown in our results, our designed shell-and-bone-based model printing can not only save the printing time but also use lesser material than the original whole model printing.",Parallel 3D printing based on skeletal remeshing,NA:NA:NA:NA,2016
K. Edum-Fotwe:P. Shepherd:M. Brown:D. Harper:R. Dinnis,"This simple paper describes an intuitive data-driven approach to reconstructing architectural building-footprints from structured or unstructured 2D pointsets. The function is fast, accurate and unconstrained. Further unlike the prevalent L-Shape detectors predicated on a shape's skeletal descriptor [Szeliski 2010], the method is robust to sensing noise at the boundary of a 2D pointset.","Quick, unconstrained, approximate l-shape method",NA:NA:NA:NA:NA,2016
Henrik Lieng,"Diffusion curves [Orzan et al. 2008] (DCs) has risen as an attractive vector primitive for representing complex colour gradients. Its flexible mathematical definition, taking curves with colour values as input, can be easily adopted by artists and designers because curves represent an intuitive approach to 2D drawing and design. However, the (Laplacian) diffusion process is computationally expensive and naive DCs (solving the large sparse PDE naively) are consequently unattractive as a practical vector graphics primitive. Lots of work has therefore been undertaken to identify a practical framework for defining and rendering DCs.",Ray-traced diffusion points,NA,2016
Patrik Huber:William Christmas:Adrian Hilton:Josef Kittler:Matthias Rätsch,We present a fully automatic approach to real-time 3D face reconstruction from monocular in-the-wild videos. We use a 3D Morphable Face Model to obtain a semi-dense shape and combine it with a fast median-based super-resolution technique to obtain a high-fidelity textured 3D face model. Our system does not need prior training and is designed to work in uncontrolled scenarios.,Real-time 3D face super-resolution from monocular in-the-wild videos,NA:NA:NA:NA:NA,2016
Chih-Fan Chen:Mark Bolas:Evan Suma,"With the recent proliferation of high-fidelity head-mounted displays (HMDs), there is increasing demand for realistic 3D content that can be integrated into virtual reality environments. However, creating photorealistic models is not only difficult but also time consuming. A simpler alternative involves scanning objects in the real world and rendering their digitized counterpart in the virtual world. Capturing objects can be achieved by performing a 3D scan using widely available consumer-grade RGB-D cameras. This process involves reconstructing the geometric model from depth images generated using a structured light or time-of-flight sensor. The colormap is determined by fusing data from multiple color images captured during the scan. Existing methods compute the color of each vertex by averaging the colors from all these images. Blending colors in this manner creates low-fidelity models that appear blurry. (Figure 1 right). Furthermore, this approach also yields textures with fixed lighting that is baked on the model. This limitation becomes more apparent when viewed in head-tracked virtual reality, as the illumination (e.g. specular reflections) does not change appropriately based on the user's viewpoint.",Real-time 3D rendering using depth-based geometry reconstruction and view-dependent texture mapping,NA:NA:NA,2016
Serguei A. Mokhov:Miao Song:Jonathan Llewellyn:Jie Zhang:Alexander Charette:Ruofan Wu:Shuiying Ge,"It was not possible to do reliable 3D skeletal tracking with the currently publicly available inexpensive consumer grade hardware/software tools, such as depth cameras and their SDKs using multiple of such sensors in a single application (e.g., a game, motion recording for animation, or 3D scanning). We successfully attached 3 Kinect v2 sensors to a single application to track skeletal data without using Microsoft's Kinect 2 SDK. We created a new toolkit -- MultiCamTk++ for 3 or more Kinects v2 with skeleton support in C++. It is a successor of our previous version, MultiCamTk, done in Processing/Java that had no skeletal tracking. We achieve high resiliency and good frame rate even if 1--2 Kinects are disconnected at runtime. We are able to receive the skeleton data from the multiple sources to correlate the coordinates for spatial 3D user tracking.",Real-time collection and analysis of 3-Kinect v2 skeleton data in a single application,NA:NA:NA:NA:NA:NA:NA,2016
Daniel Limberger:Jürgen Döllner,"In a rendering environment of comparatively sparse interaction, e.g., digital production tools, image synthesis and its quality do not have to be constrained to single frames. This work analyzes strategies for highly economically rendering of state-of-the-art rendering effects using progressive multi-frame sampling in real-time. By distributing and accumulating samples of sampling-based rendering techniques (e.g., anti-aliasing, order-independent transparency, physically-based depth-of-field and shadowing, ambient occlusion, reflections) over multiple frames, images of very high quality can be synthesized with unequaled resource-efficiency.",Real-time rendering of high-quality effects using multi-frame sampling,NA:NA,2016
Kurt Leimer:Michael Wimmer:Przemyslaw Musialski,"With online repositories for 3D models like 3D Warehouse becoming more prevalent and growing ever larger, new possibilities have opened up for both experienced and inexperienced users alike. These large collections of shapes can provide inspiration for designers or make it possible to synthesize new shapes by combining different parts from already existing shapes, which can be both easy to learn and a fast way of creating new shapes.",Relation-based parametrization and exploration of shape collections,NA:NA:NA,2016
Tomokazu Ishikawa:Kousaku Kamata:Yuriko Takeshima:Masanori Kakimoto,"In recent years, expressions close to realities have become possible thanks to the technologically advanced computer graphics. Secular change and weathering are important factors to create realistic computer graphics images. Metal rust is an important secular change and there are much research work on rust [Kanazawa et al. 2015]. Although the rust forming processes vary according to coating rain-water and seawater, dissolved oxygen contents of them and flowing water effects, no rust forming methods which have examined the object geometry of models and chemical reaction processes exist as far as we know. Our proposed method calculates water flowing on 3D models to reproduce the process of corrosion which advances from the surface region coated with water. Our corrosion simulation model takes into account the quantity of coating water and the chemical reaction processes. As a result, we confirm that the images close to the rust formed in reality can be obtained.",Rusting and corroding simulation taking into account chemical reaction processes,NA:NA:NA:NA,2016
Tuur Stuyck:Philip Dutré,"Fluid simulations are very useful for creating physically based water effects in computer graphics but are notoriously hard to control. In this talk we propose a novel and intuitive animation technique for fluid animations using interactive direct manipulation of the simulated fluid inspired by clay sculpting. Artists can simply shape the fluid directly into the desired visual effect whilst the fluid still adheres to its physical properties such as surface tension and volume preservation. Our approach is faster and much more intuitive compared to previous work which relies on indirect approaches such as providing reference geometry or density fields. It makes it very easy, even for novice users, to modify simulations ranging from enlarging splashes or altering droplet shapes to adjusting the flow of a large fluid body. The sculpted fluid shapes are incorporated into the simulation using guided re-simulation using control theory instead of simply using geometric deformations resulting in natural-looking animations.",Sculpting fluids: a new and intuitive approach to art-directable fluids,NA:NA,2016
Shuhei Kodama:Tokiichiro Takahashi,"In non-photorealistic rendering (NPR), an evaluated value of the rendered image is not absolute. It is different for each user. Therefore, in NPR, it is important to render an image that satisfies user preferences. Many methods of painterly style image generation have been proposed/developed. However, these methods focus on image generation that imitates painting material or a painterly style. Therefore, to the best of our knowledge, the study of how to generate a painterly style image that satisfies user preferences dose not yet exist. When gazing at an image, the user is more effective at quickly identifying regions that the user dislikes than in finding preferred regions. To address this point, we propose a mechanism of painterly style rendering that satisfies user preferences.",Suggestive painterly style image generation system to satisfy user preferences,NA:NA,2016
Syuhei Sato:Yoshinori Dobashi:Tomoyuki Nishita,"In this paper, we develop a method for synthesizing desired flow fields by combining existing multiple flow fields. Our system allows the user to specify arbitrary regions of the precomputed flow fields and combine them to synthesize a new flow field. In order to maintain plausible physical behavior, we ensure the incompressibility for the combined flow field. To address this, we use stream functions for representing the flow fields. However, there exist discontinuities at the boundaries between the combined flow fields, resulting in unnatural animation of fluids. In order to remove the discontinuities, we apply Poisson image editing to the stream functions.",Combining multiple flow fields for editing existing fluid animations,NA:NA:NA,2016
Junho Jeon:Yeongyu Jung:Haejoon Kim:Seungyong Lee,"Nowadays RGB-D cameras, such as Microsoft Kinect, have become widely available. Various researches on 3D reconstruction based on RGB-D images have enabled 3D navigation of a scene by rendering the reconstructed 3D model from desirable viewpoints. However, these reconstructed 3D models are not yet popularly used in applications due to the lack of accurate color information.",Texture map generation for large-scale 3D reconstructed scenes,NA:NA:NA:NA,2016
William J. Joel,"In 2009, the ACM/SIGGRAPH Education Committee established an Undergraduate Research Alliance [Undergraduate Research Alliance] to foster the development of undergraduate research, in computer graphics and interactive techniques, across all related disciplines. Since its inception, the Alliance has hosted sessions at the annual SIGGRAPH conferences to allow educators and other the chance to discuss what they have accomplished and what still needs to be done. If we in the SIGGRAPH community wish to continue to expand the envelope of knowledge, it is necessary that we engage students in the exploration of new ideas as early as possible in their education. The purpose of this poster, therefore, is to present a case study for undergraduate research with the hopes that it spurs others to join in this endeavor.",The need for interdisciplinary undergraduate research,NA,2016
Shoichi Furukawa:Takuya Kato:Pavel Savkin:Shigeo Morishima,"Numerous video have been translated using ""dubbing,"" spurred by the recent growth of video market. However, it is very difficult to achieve the visual-audio synchronization. That is to say in general a new audio does not synchronize with actor's mouth motion. This discrepancy can disturb comprehension of video contents. There-fore many methods have been researched so far to solve this problem.",Video reshuffling: automatic video dubbing without prior knowledge,NA:NA:NA:NA,2016
Bill Freeman,NA,Session details: Computational cameras,NA,2016
Yifan Peng:Qiang Fu:Felix Heide:Wolfgang Heidrich,"Diffractive optical elements (DOEs) have recently drawn great attention in computational imaging because they can drastically reduce the size and weight of imaging devices compared to their refractive counterparts. However, the inherent strong dispersion is a tremendous obstacle that limits the use of DOEs in full spectrum imaging, causing unacceptable loss of color fidelity in the images. In particular, metamerism introduces a data dependency in the image blur, which has been neglected in computational imaging methods so far. We introduce both a diffractive achromat based on computational optimization, as well as a corresponding algorithm for correction of residual aberrations. Using this approach, we demonstrate high fidelity color diffractive-only imaging over the full visible spectrum. In the optical design, the height profile of a diffractive lens is optimized to balance the focusing contributions of different wavelengths for a specific focal length. The spectral point spread functions (PSFs) become nearly identical to each other, creating approximately spectrally invariant blur kernels. This property guarantees good color preservation in the captured image and facilitates the correction of residual aberrations in our fast two-step deconvolution without additional color priors. We demonstrate our design of diffractive achromat on a 0.5mm ultrathin substrate by photolithography techniques. Experimental results show that our achromatic diffractive lens produces high color fidelity and better image quality in the full visible spectrum.",The diffractive achromat full spectrum computational imaging with diffractive optics,NA:NA:NA:NA,2016
Chloe LeGendre:Xueming Yu:Dai Liu:Jay Busch:Andrew Jones:Sumanta Pattanaik:Paul Debevec,"We present a practical framework for reproducing omnidirectional incident illumination conditions with complex spectra using a light stage with multispectral LED lights. For lighting acquisition, we augment standard RGB panoramic photography with one or more observations of a color chart with numerous reflectance spectra. We then solve for how to drive the multispectral light sources so that they best reproduce the appearance of the color charts in the original lighting. Even when solving for non-negative intensities, we show that accurate lighting reproduction is achievable using just four or six distinct LED spectra for a wide range of incident illumination spectra. A significant benefit of our approach is that it does not require the use of specialized equipment (other than the light stage) such as monochromators, spectroradiometers, or explicit knowledge of the LED power spectra, camera spectral response functions, or color chart reflectance spectra. We describe two simple devices for multispectral lighting capture, one for slow measurements of detailed angular spectral detail, and one for fast measurements with coarse angular detail. We validate the approach by realistically compositing real subjects into acquired lighting environments, showing accurate matches to how the subject would actually look within the environments, even for those including complex multispectral illumination. We also demonstrate dynamic lighting capture and playback using the technique.",Practical multispectral lighting reproduction,NA:NA:NA:NA:NA:NA:NA,2016
Shikhar Shrestha:Felix Heide:Wolfgang Heidrich:Gordon Wetzstein,"Depth cameras are a ubiquitous technology used in a wide range of applications, including robotic and machine vision, human-computer interaction, autonomous vehicles as well as augmented and virtual reality. In this paper, we explore the design and applications of phased multi-camera time-of-flight (ToF) systems. We develop a reproducible hardware system that allows for the exposure times and waveforms of up to three cameras to be synchronized. Using this system, we analyze waveform interference between multiple light sources in ToF applications and propose simple solutions to this problem. Building on the concept of orthogonal frequency design, we demonstrate state-of-the-art results for instantaneous radial velocity capture via Doppler time-of-flight imaging and we explore new directions for optically probing global illumination, for example by de-scattering dynamic scenes and by non-line-of-sight motion detection via frequency gating.",Computational imaging with multi-camera time-of-flight systems,NA:NA:NA:NA,2016
Paul Kry,NA,Session details: Rigging & skinning,NA,2016
Ben Jones:Nils Thuerey:Tamar Shinar:Adam W. Bargteil,"Physics-based animation is often used to animate scenes containing destruction of near-rigid, man-made materials. For these applications, the most important visual features are plastic deformation and fracture. Methods based on continuum mechanics model these materials as elastoplastic, and must perform expensive elasticity computations even though elastic deformations are imperceptibly small for rigid materials. We introduce an example-based plasticity model based on linear blend skinning that allows artists to author simulation objects using familiar tools. Dynamics are computed using an unmodified rigid body simulator, making our method computationally efficient and easy to integrate into existing pipelines. We introduce a flexible technique for mapping impulses computed by the rigid body solver to local, example-based deformations. For completeness, our method also supports prescoring based fracture. We demonstrate the practicality of our method by animating a variety of destructive scenes.",Example-based plastic deformation of rigid bodies,NA:NA:NA:NA,2016
Hongyi Xu:Jernej Barbič,"We enrich character animations with secondary soft-tissue Finite Element Method (FEM) dynamics computed under arbitrary rigged or skeletal motion. Our method optionally incorporates pose-space deformation (PSD). It runs at milliseconds per frame for complex characters, and fits directly into standard character animation pipelines. Our simulation method does not require any skin data capture; hence, it can be applied to humans, animals, and arbitrary (real-world or fictional) characters. In standard model reduction of three-dimensional nonlinear solid elastic models, one builds a reduced model around a single pose, typically the rest configuration. We demonstrate how to perform multi-model reduction of Finite Element Method (FEM) nonlinear elasticity, where separate reduced models are precomputed around a representative set of object poses, and then combined at runtime into a single fast dynamic system, using subspace interpolation. While time-varying reduction has been demonstrated before for offline applications, our method is fast and suitable for hard real-time applications in games and virtual reality. Our method supports self-contact, which we achieve by computing linear modes and derivatives under contact constraints.",Pose-space subspace dynamics,NA:NA,2016
Tomohiko Mukai:Shigeru Kuriyama,"Dynamic skin deformation is vital for creating life-like characters, and its real-time computation is in great demand in interactive applications. We propose a practical method to synthesize plausible and dynamic skin deformation based on a helper bone rig. This method builds helper bone controllers for the deformations caused not only by skeleton poses but also secondary dynamics effects. We introduce a state-space model for a discrete time linear time-invariant system that efficiently maps the skeleton motion to the dynamic movement of the helper bones. Optimal transfer of nonlinear, complicated deformations, including the effect of soft-tissue dynamics, is obtained by learning the training sequence consisting of skeleton motions and corresponding skin deformations. Our approximation method for a dynamics model is highly accurate and efficient owing to its low-rank property obtained by a sparsity-oriented nuclear norm optimization. The resulting linear model is simple enough to easily implement in the existing workflows and graphics pipelines. We demonstrate the superior performance of our method compared to conventional dynamic skinning in terms of computational efficiency including LOD controls, stability in interactive controls, and flexible expression in deformations.",Efficient dynamic skinning with low-rank helper bone controllers,NA:NA,2016
Binh Huy Le:Jessica K. Hodgins,"Skinning algorithms that work across a broad range of character designs and poses are crucial to creating compelling animations. Currently, linear blend skinning (LBS) and dual quaternion skinning (DQS) are the most widely used, especially for real-time applications. Both techniques are efficient to compute and are effective for many purposes. However, they also have many well-known artifacts, such as collapsing elbows, candy wrapper twists, and bulging around the joints. Due to the popularity of LBS and DQS, it would be of great benefit to reduce these artifacts without changing the animation pipeline or increasing the computational cost significantly. In this paper, we introduce a new direct skinning method that addresses this problem. Our key idea is to pre-compute the optimized center of rotation for each vertex from the rest pose and skinning weights. At runtime, these centers of rotation are used to interpolate the rigid transformation for each vertex. Compared to other direct skinning methods, our method significantly reduces the artifacts of LBS and DQS while maintaining real-time performance and backwards compatibility with the animation pipeline.",Real-time skeletal skinning with optimized centers of rotation,NA:NA,2016
Pierre Alliez,NA,Session details: Geometry,NA,2016
Yajie Yan:Kyle Sykes:Erin Chambers:David Letscher:Tao Ju,"While playing a fundamental role in shape understanding, the medial axis is known to be sensitive to small boundary perturbations. Methods for pruning the medial axis are usually guided by some measure of significance. The majority of significance measures over the medial axes of 3D shapes are locally defined and hence unable to capture the scale of features. We introduce a global significance measure that generalizes in 3D the classical Erosion Thickness (ET) measure over the medial axes of 2D shapes. We give precise definition of ET in 3D, analyze its properties, and present an efficient approximation algorithm with bounded error on a piece-wise linear medial axis. Experiments showed that ET outperforms local measures in differentiating small boundary noise from prominent shape features, and it is significantly faster to compute than existing global measures. We demonstrate the utility of ET in extracting clean, shape-revealing and topology-preserving skeletons of 3D shapes.",Erosion thickness on medial axes of 3D shapes,NA:NA:NA:NA:NA,2016
Qingnan Zhou:Eitan Grinspun:Denis Zorin:Alec Jacobson,"Many high-level geometry processing tasks rely on low-level constructive solid geometry operations. Though trivial for implicit representations, boolean operations are notoriously difficult to execute robustly for explicit boundary representations. Existing methods for 3D triangle meshes fall short in one way or another. Some methods are fast but fail to produce closed, self-intersection free output. Other methods are robust but place prohibitively strict assumptions on the input, e.g., no hollow cavities, non-manifold edges or self-intersections. We propose a systematic recipe for conducting a family of exact constructive solid geometry operations. The two-stage method makes no general position assumptions and does not resort to numerical perturbation. The method is variadic, operating on any number of input meshes. This generalizes unary mesh-repair operations, classic binary boolean differencing, and n-ary operations such as finding all regions inside at least k out of n inputs. We demonstrate the superior effectiveness and robustness of our method on a dataset of 10,000 ""real-world"" meshes from a popular online repository. To encourage development, validation, and comparison, we release both our code and dataset to the public.",Mesh arrangements for solid geometry,NA:NA:NA:NA,2016
Jaakko Lehtinen,NA,Session details: Efficient sampling & rendering,NA,2016
Bochang Moon:Steven McDonagh:Kenny Mitchell:Markus Gross,"In this paper, we propose a new adaptive rendering method to improve the performance of Monte Carlo ray tracing, by reducing noise contained in rendered images while preserving high-frequency edges. Our method locally approximates an image with polynomial functions and the optimal order of each polynomial function is estimated so that our reconstruction error can be minimized. To robustly estimate the optimal order, we propose a multi-stage error estimation process that iteratively estimates our reconstruction error. In addition, we present an energy-preserving outlier removal technique to remove spike noise without causing noticeable energy loss in our reconstruction result. Also, we adaptively allocate additional ray samples to high error regions guided by our error estimation. We demonstrate that our approach outperforms state-of-the-art methods by controlling the tradeoff between reconstruction bias and variance through locally defining our polynomial order, even without need for filtering bandwidth optimization, the common approach of other recent methods.",Adaptive polynomial rendering,NA:NA:NA:NA,2016
Eric Heitz:Jonathan Dupuy:Stephen Hill:David Neubelt,"In this paper, we show that applying a linear transformation---represented by a 3 x 3 matrix---to the direction vectors of a spherical distribution yields another spherical distribution, for which we derive a closed-form expression. With this idea, we can use any spherical distribution as a base shape to create a new family of spherical distributions with parametric roughness, elliptic anisotropy and skewness. If the original distribution has an analytic expression, normalization, integration over spherical polygons, and importance sampling, then these properties are inherited by the linearly transformed distributions. By choosing a clamped cosine for the original distribution we obtain a family of distributions, which we call Linearly Transformed Cosines (LTCs), that provide a good approximation to physically based BRDFs and that can be analytically integrated over arbitrary spherical polygons. We show how to use these properties in a realtime polygonal-light shading application. Our technique is robust, fast, accurate and simple to implement.",Real-time polygonal-light shading with linearly transformed cosines,NA:NA:NA:NA,2016
Jiří Vorba:Jaroslav Křivánek,"While Russian roulette (RR) and splitting are considered fundamental importance sampling techniques in neutron transport simulations, they have so far received relatively little attention in light transport. In computer graphics, RR and splitting are most often based solely on local reflectance properties. However, this strategy can be far from optimal in common scenes with non-uniform light distribution as it does not accurately predict the actual path contribution. In our approach, like in neutron transport, we estimate the expected contribution of a path as the product of the path weight and a pre-computed estimate of the adjoint transport solution. We use this estimate to generate so-called weight window which keeps the path contribution roughly constant through RR and splitting. As a result, paths in unimportant regions tend to be terminated early while in the more important regions they are spawned by splitting. This results in substantial variance reduction in both path tracing and photon tracing-based simulations. Furthermore, unlike the standard computer graphics RR, our approach does not interfere with importance-driven sampling of scattering directions, which results in superior convergence when such a technique is combined with our approach. We provide a justification of this behavior by relating our approach to the zero-variance random walk theory.",Adjoint-driven Russian roulette and splitting in light transport simulation,NA:NA,2016
Wojciech Matusik,NA,Session details: Fabricating structure & appearance,NA,2016
Christian Schüller:Daniele Panozzo:Anselm Grundhöfer:Henning Zimmer:Evgeni Sorkine:Olga Sorkine-Hornung,"We propose a method to fabricate textured 3D models using thermoforming. Differently from industrial techniques, which target mass production of a specific shape, we propose a combined hardware and software solution to manufacture customized, unique objects. Our method simulates the forming process and converts the texture of a given digital 3D model into a pre-distorted image that we transfer onto a plastic sheet. During thermoforming, the sheet deforms to create a faithful physical replica of the digital model. Our hardware setup uses off-the-shelf components and can be calibrated with an automatic algorithm that extracts the simulation parameters from a single calibration object produced by the same process.",Computational thermoforming,NA:NA:NA:NA:NA:NA,2016
Jonàs Martínez:Jérémie Dumas:Sylvain Lefebvre,"Microstructures at the scale of tens of microns change the physical properties of objects, making them lighter or more flexible. While traditionally difficult to produce, additive manufacturing now lets us physically realize such microstructures at low cost. In this paper we propose to study procedural, aperiodic microstructures inspired by Voronoi open-cell foams. The absence of regularity affords for a simple approach to grade the foam geometry --- and thus its mechanical properties --- within a target object and its surface. Rather than requiring a global optimization process, the microstructures are directly generated to exhibit a specified elastic behavior. The implicit evaluation is akin to procedural textures in computer graphics, and locally adapts to follow the elasticity field. This allows very detailed structures to be generated in large objects without having to explicitly produce a full representation --- mesh or voxels --- of the complete object: the structures are added on the fly, just before each object slice is manufactured. We study the elastic behavior of the microstructures and provide a complete description of the procedure generating them. We explain how to determine the geometric parameters of the microstructures from a target elasticity, and evaluate the result on printed samples. Finally, we apply our approach to the fabrication of objects with spatially varying elasticity, including the implicit modeling of a frame following the object surface and seamlessly connecting to the microstructures.",Procedural voronoi foams for additive manufacturing,NA:NA:NA,2016
Peng Song:Bailin Deng:Ziqi Wang:Zhichao Dong:Wei Li:Chi-Wing Fu:Ligang Liu,"This paper presents CofiFab, a coarse-to-fine 3D fabrication solution, combining 3D printing and 2D laser cutting for cost-effective fabrication of large objects at lower cost and higher speed. Our key approach is to first build coarse internal base structures within the given 3D object using laser cutting, and then attach thin 3D-printed parts, as an external shell, onto the base to recover the fine surface details. CofiFab achieves this with three novel algorithmic components. First, we formulate an optimization model to compute fabricatable polyhedrons of maximized volume, as the geometry of the internal base. Second, we devise a new interlocking scheme to tightly connect the laser-cut parts into a strong internal base, by iteratively building a network of nonorthogonal joints and interlocking parts around polyhedral corners. Lastly, we optimize the partitioning of the external object shell into 3D-printable parts, while saving support material and avoiding overhangs. Besides cost saving, these components also consider aesthetics, stability and balancing. Hence, CofiFab can efficiently produce large objects by assembly. To evaluate CofiFab, we fabricate objects of varying shapes and sizes, and show that CofiFab can significantly outperform previous methods.",CofiFab: coarse-to-fine fabrication of large 3D objects,NA:NA:NA:NA:NA:NA:NA,2016
Misha Kazhdan,NA,Session details: Shape analysis,NA,2016
Paul Guerrero:Niloy J. Mitra:Peter Wonka,"As humans, we regularly interpret scenes based on how objects are related, rather than based on the objects themselves. For example, we see a person riding an object X or a plank bridging two objects. Current methods provide limited support to search for content based on such relations. We present raid, a relation-augmented image descriptor that supports queries based on inter-region relations. The key idea of our descriptor is to encode region-to-region relations as the spatial distribution of point-to-region relationships between two image regions. raid allows sketch-based retrieval and requires minimal training data, thus making it suited even for querying uncommon relations. We evaluate the proposed descriptor by querying into large image databases and successfully extract non-trivial images demonstrating complex inter-region relations, which are easily missed or erroneously classified by existing methods. We assess the robustness of raid on multiple datasets even when the region segmentation is computed automatically or very noisy.",RAID: a relation-augmented image descriptor,NA:NA:NA,2016
Ruizhen Hu:Oliver van Kaick:Bojian Wu:Hui Huang:Ariel Shamir:Hao Zhang,"We introduce a co-analysis method which learns a functionality model for an object category, e.g., strollers or backpacks. Like previous works on functionality, we analyze object-to-object interactions and intra-object properties and relations. Differently from previous works, our model goes beyond providing a functionality-oriented descriptor for a single object; it prototypes the functionality of a category of 3D objects by co-analyzing typical interactions involving objects from the category. Furthermore, our co-analysis localizes the studied properties to the specific locations, or surface patches, that support specific functionalities, and then integrates the patch-level properties into a category functionality model. Thus our model focuses on the how, via common interactions, and where, via patch localization, of functionality analysis. Given a collection of 3D objects belonging to the same category, with each object provided within a scene context, our co-analysis yields a set of proto-patches, each of which is a patch prototype supporting a specific type of interaction, e.g., stroller handle held by hand. The learned category functionality model is composed of proto-patches, along with their pairwise relations, which together summarize the functional properties of all the patches that appear in the input object category. With the learned functionality models for various object categories serving as a knowledge base, we are able to form a functional understanding of an individual 3D object, without a scene context. With patch localization in the model, functionality-aware modeling, e.g, functional object enhancement and the creation of functional object hybrids, is made possible.",Learning how objects function via co-analysis of interactions,NA:NA:NA:NA:NA:NA,2016
Paul Guerrero:Gilbert Bernstein:Wilmot Li:Niloy J. Mitra,"Patterns play a central role in 2D graphic design. A critical step in the design of patterns is evaluating multiple design alternatives. Exploring these alternatives with existing tools is challenging because most tools force users to work with a single fixed representation of the pattern that encodes a specific set of geometric relationships between pattern elements. However, for most patterns, there are many different interpretations of its regularity that correspond to different design variations. The exponential nature of this variation space makes the problem of finding all variations intractable. We present a method called PATEX to characterize and efficiently identify distinct and valid pattern variations, allowing users to directly navigate the variation space. Technically, we propose a novel linear approximation to handle the complexity of the problem and efficiently enumerate suitable pattern variations under proposed element movements. We also present two pattern editing interfaces that expose the detected pattern variations as suggested edits to the user. We show a diverse collection of pattern edits and variations created with PATEX. The results from our user study indicate that our suggested variations can be useful and inspirational for typical pattern editing tasks.",PATEX: exploring pattern variations,NA:NA:NA:NA,2016
Derek Nowrouzezahrai,NA,Session details: Cloth,NA,2016
James McCann:Lea Albaugh:Vidya Narayanan:April Grow:Wojciech Matusik:Jennifer Mankoff:Jessica Hodgins,"Industrial knitting machines can produce finely detailed, seamless, 3D surfaces quickly and without human intervention. However, the tools used to program them require detailed manipulation and understanding of low-level knitting operations. We present a compiler that can automatically turn assemblies of high-level shape primitives (tubes, sheets) into low-level machine instructions. These high-level shape primitives allow knit objects to be scheduled, scaled, and otherwise shaped in ways that require thousands of edits to low-level instructions. At the core of our compiler is a heuristic transfer planning algorithm for knit cycles, which we prove is both sound and complete. This algorithm enables the translation of high-level shaping and scheduling operations into needle-level operations. We show a wide range of examples produced with our compiler and demonstrate a basic visual design interface that uses our compiler as a backend.",A compiler for 3D machine knitting,NA:NA:NA:NA:NA:NA:NA,2016
Aric Bartle:Alla Sheffer:Vladimir G. Kim:Danny M. Kaufman:Nicholas Vining:Floraine Berthouzoz,"Designers frequently reuse existing designs as a starting point for creating new garments. In order to apply garment modifications, which the designer envisions in 3D, existing tools require meticulous manual editing of 2D patterns. These 2D edits need to account both for the envisioned geometric changes in the 3D shape, as well as for various physical factors that affect the look of the draped garment. We propose a new framework that allows designers to directly apply the changes they envision in 3D space; and creates the 2D patterns that replicate this envisioned target geometry when lifted into 3D via a physical draping simulation. Our framework removes the need for laborious and knowledge-intensive manual 2D edits and allows users to effortlessly mix existing garment designs as well as adjust for garment length and fit. Following each user specified editing operation we first compute a target 3D garment shape, one that maximally preserves the input garment's style-its proportions, fit and shape-subject to the modifications specified by the user. We then automatically compute 2D patterns that recreate the target garment shape when draped around the input mannequin within a user-selected simulation environment. To generate these patterns, we propose a fixed-point optimization scheme that compensates for the deformation due to the physical forces affecting the drape and is independent of the underlying simulation tool used. Our experiments show that this method quickly and reliably converges to patterns that, under simulation, form the desired target look, and works well with different black-box physical simulators. We demonstrate a range of edited and resimulated garments, and further validate our approach via expert and amateur critique, and comparisons to alternative solutions.",Physics-driven pattern adjustment for direct 3D garment editing,NA:NA:NA:NA:NA:NA,2016
Shuang Zhao:Fujun Luan:Kavita Bala,"Fabrics play a significant role in many applications in design, prototyping, and entertainment. Recent fiber-based models capture the rich visual appearance of fabrics, but are too onerous to design and edit. Yarn-based procedural models are powerful and convenient, but too regular and not realistic enough in appearance. In this paper, we introduce an automatic fitting approach to create high-quality procedural yarn models of fabrics with fiber-level details. We fit CT data to procedural models to automatically recover a full range of parameters, and augment the models with a measurement-based model of flyaway fibers. We validate our fabric models against CT measurements and photographs, and demonstrate the utility of this approach for fabric modeling and editing.",Fitting procedural yarn models for realistic cloth rendering,NA:NA:NA,2016
Jessica Hodgins,NA,Session details: Perception of shapes and people,NA,2016
Manfred Lau:Kapil Dev:Weiqi Shi:Julie Dorsey:Holly Rushmeier,"While the concept of visual saliency has been previously explored in the areas of mesh and image processing, saliency detection also applies to other sensory stimuli. In this paper, we explore the problem of tactile mesh saliency, where we define salient points on a virtual mesh as those that a human is more likely to grasp, press, or touch if the mesh were a real-world object. We solve the problem of taking as input a 3D mesh and computing the relative tactile saliency of every mesh vertex. Since it is difficult to manually define a tactile saliency measure, we introduce a crowdsourcing and learning framework. It is typically easy for humans to provide relative rankings of saliency between vertices rather than absolute values. We thereby collect crowdsourced data of such relative rankings and take a learning-to-rank approach. We develop a new formulation to combine deep learning and learning-to-rank methods to compute a tactile saliency measure. We demonstrate our framework with a variety of 3D meshes and various applications including material suggestion for rendering and fabrication.",Tactile mesh saliency,NA:NA:NA:NA:NA,2016
Ludovic Hoyet:Anne-Helene Olivier:Richard Kulpa:Julien Pettré,"A typical crowd engine pipeline animates numerous moving characters according to a two-step process: global trajectories are generated by a crowd simulator, whereas full body motions are generated by animation engines. Because interactions are only considered at the first stage, animations sometimes lead to residual collisions and/or characters walking as if they were alone, showing no sign to the influence of others. In this paper, we investigate the value of adding shoulder motions to characters passing at close distances on the perceived visual quality of crowd animations (i.e., perceived residual collisions and animation naturalness). We present two successive perceptual experiments exploring this question where we investigate first, local interactions between two isolated characters, and second, crowd scenarios. The first experiment shows that shoulder motions have a strong positive effect on both perceived residual collisions and animation naturalness. The second experiment demonstrates that the effect of shoulder motions on animation naturalness is preserved in the context of crowd scenarios, even though the complexity of the scene is largely increased. Our general conclusion is that adding secondary motions in character interactions has a significant impact on the visual quality of crowd animations, with a very light impact on the computational cost of the whole animation pipeline. Our results advance crowd animation techniques by enhancing the simulation of complex interactions between crowd characters with simple secondary motion triggering techniques.",Perceptual effect of shoulder motions on crowd animations,NA:NA:NA:NA,2016
Stephan Streuber:M. Alejandra Quiros-Ramirez:Matthew Q. Hill:Carina A. Hahn:Silvia Zuffi:Alice O'Toole:Michael J. Black,"Realistic, metrically accurate, 3D human avatars are useful for games, shopping, virtual reality, and health applications. Such avatars are not in wide use because solutions for creating them from high-end scanners, low-cost range cameras, and tailoring measurements all have limitations. Here we propose a simple solution and show that it is surprisingly accurate. We use crowdsourcing to generate attribute ratings of 3D body shapes corresponding to standard linguistic descriptions of 3D shape. We then learn a linear function relating these ratings to 3D human shape parameters. Given an image of a new body, we again turn to the crowd for ratings of the body shape. The collection of linguistic ratings of a photograph provides remarkably strong constraints on the metric 3D shape. We call the process crowdshaping and show that our Body Talk system produces shapes that are perceptually indistinguishable from bodies created from high-resolution scans and that the metric accuracy is sufficient for many tasks. This makes body ""scanning"" practical without a scanner, opening up new applications including database search, visualization, and extracting avatars from books.",Body talk: crowdshaping realistic 3D avatars with words,NA:NA:NA:NA:NA:NA:NA,2016
Michal Piovarči:David I. W. Levin:Jason Rebello:Desai Chen:Roman Ďurikovič:Hanspeter Pfister:Wojciech Matusik:Piotr Didyk,"Everyone, from a shopper buying shoes to a doctor palpating a growth, uses their sense of touch to learn about the world. 3D printing is a powerful technology because it gives us the ability to control the haptic impression an object creates. This is critical for both replicating existing, real-world constructs and designing novel ones. However, each 3D printer has different capabilities and supports different materials, leaving us to ask: How can we best replicate a given haptic result on a particular output device? In this work, we address the problem of mapping a real-world material to its nearest 3D printable counterpart by constructing a perceptual model for the compliance of nonlinearly elastic objects. We begin by building a perceptual space from experimentally obtained user comparisons of twelve 3D-printed metamaterials. By comparing this space to a number of hypothetical computational models, we identify those that can be used to accurately and efficiently evaluate human-perceived differences in nonlinear stiffness. Furthermore, we demonstrate how such models can be applied to complex geometries in an interaction-aware way where the compliance is influenced not only by the material properties from which the object is made but also its geometry. We demonstrate several applications of our method in the context of fabrication and evaluate them in a series of user experiments.","An interaction-aware, perceptual model for non-linear elastic objects",NA:NA:NA:NA:NA:NA:NA:NA,2016
Wenzel Jakob,NA,Session details: Rendering of complex microstructure,NA,2016
Ling-Qi Yan:Miloš Hašan:Steve Marschner:Ravi Ramamoorthi,"Specular BRDF rendering traditionally approximates surface microstructure using a smooth normal distribution, but this ignores glinty effects, easily observable in the real world. While modeling the actual surface microstructure is possible, the resulting rendering problem is prohibitively expensive. Recently, Yan et al. [2014] and Jakob et al. [2014] made progress on this problem, but their approaches are still expensive and lack full generality in their material and illumination support. We introduce an efficient and general method that can be easily integrated in a standard rendering system. We treat a specular surface as a four-dimensional position-normal distribution, and fit this distribution using millions of 4D Gaussians, which we call elements. This leads to closed-form solutions to the required BRDF evaluation and sampling queries, enabling the first practical solution to rendering specular microstructure.",Position-normal distributions for efficient rendering of specular microstructure,NA:NA:NA:NA,2016
Boris Raymond:Gaël Guennebaud:Pascal Barla,"We introduce a Spatially-Varying BRDF model tailored to the multi-scale rendering of scratched materials such as metals, plastics or finished woods. Our approach takes advantage of the regular structure of scratch distributions to achieve high performance without compromising visual quality. We provide users with controls over the profile, micro-BRDF, density and orientation of scratches, while updating our material model at interactive rates. The BRDF for a single scratch is simulated using an optimized 2D ray-tracer and compactly stored in a three-component 2D texture. In contrast to existing models, our approach takes into account all interreflections inside a scratch, including Fresnel effects. At render time, the SV-BRDF for the scratch distribution under a pixel or ray footprint is obtained by linear combination of individual scratch BRDFs. We show how to evaluate it using both importance and light sampling, in direct and global illumination settings.",Multi-scale rendering of scratched materials using a structured SV-BRDF model,NA:NA:NA,2016
Eric Heitz:Johannes Hanika:Eugene d'Eon:Carsten Dachsbacher,"Modeling multiple scattering in microfacet theory is considered an important open problem because a non-negligible portion of the energy leaving rough surfaces is due to paths that bounce multiple times. In this paper we derive the missing multiple-scattering components of the popular family of BSDFs based on the Smith microsurface model. Our derivations are based solely on the original assumptions of the Smith model. We validate our BSDFs using raytracing simulations of explicit random Beckmann surfaces. Our main insight is that the microfacet theory for surfaces with the Smith model can be derived as a special case of the microflake theory for volumes, with additional constraints to enforce the presence of a sharp interface, i.e. to transform the volume into a surface. We derive new free-path distributions and phase functions such that plane-parallel scattering from a microvolume with these distributions exactly produces the BSDF based on the Smith microsurface model, but with the addition of higher-order scattering. With this new formulation, we derive multiple-scattering micro-facet BSDFs made of either diffuse, conductive, or dielectric material. Our resulting BSDFs are reciprocal, energy conserving, and support popular anisotropic parametric normal distribution functions such as Beckmann and GGX. While we do not provide closed-form expressions for the BSDFs, they are mathematically well-defined and can be evaluated at arbitrary precision. We show how to practically use them with Monte Carlo physically based rendering algorithms by providing analytic importance sampling and unbiased stochastic evaluation. Our implementation is analytic and does not use per-BSDF precomputed data, which makes our BSDFs usable with textured albedos, roughness, and anisotropy.",Multiple-scattering microfacet BSDFs with the Smith model,NA:NA:NA:NA,2016
Ashok Veeraraghavan,NA,Session details: Computational display,NA,2016
Netalee Efrat:Piotr Didyk:Mike Foshey:Wojciech Matusik:Anat Levin,"While 3D movies are gaining popularity, viewers in a 3D cinema still need to wear cumbersome glasses in order to enjoy them. Automultiscopic displays provide a better alternative to the display of 3D content, as they present multiple angular images of the same scene without the need for special eyewear. However, automultiscopic displays cannot be directly implemented in a wide cinema setting due to variants of two main problems: (i) The range of angles at which the screen is observed in a large cinema is usually very wide, and there is an unavoidable tradeoff between the range of angular images supported by the display and its spatial or angular resolutions. (ii) Parallax is usually observed only when a viewer is positioned at a limited range of distances from the screen. This work proposes a new display concept, which supports automultiscopic content in a wide cinema setting. It builds on the typical structure of cinemas, such as the fixed seat positions and the fact that different rows are located on a slope at different heights. Rather than attempting to display many angular images spanning the full range of viewing angles in a wide cinema, our design only displays the narrow angular range observed within the limited width of a single seat. The same narrow range content is then replicated to all rows and seats in the cinema. To achieve this, it uses an optical construction based on two sets of parallax barriers, or lenslets, placed in front of a standard screen. This paper derives the geometry of such a display, analyzes its limitations, and demonstrates a proof-of-concept prototype.",Cinema 3D: large scale automultiscopic display,NA:NA:NA:NA:NA,2016
Seungjae Lee:Changwon Jang:Seokil Moon:Jaebum Cho:Byoungho Lee,"We propose a see-through additive light field display as a novel type of compressive light field display. We utilize holographic optical elements (HOEs) as transparent additive layers. The HOE layers are almost free from diffraction unlike spatial light modulator layers, which makes this additive light field display more advantageous when modifying the number of layers, thickness, and pixel density compared with conventional compressive displays. Meanwhile, the additive light field display maintains advantages of compressive light field displays. The proposed additive light field display shows bright and full-color volumetric images in high definition. In addition, users can view real-world scenes beyond the displays. Hence, we expect that our method can contribute to the realization of augmented reality. Here, we describe implementation of a prototype additive light field display with two additive layers, evaluate the performance of transparent HOE layers, describe several results of display experiments, discuss the diffraction effect of spatial light modulators, and analyze the ability of the additive light field display to express uncorrelated light fields.",Additive light field displays: realization of augmented reality with holographic optical elements,NA:NA:NA:NA:NA,2016
Hao Li,NA,Session details: Camera control & VR,NA,2016
Mike Roberts:Pat Hanrahan,"When designing trajectories for quadrotor cameras, it is important that the trajectories respect the dynamics and physical limits of quadrotor hardware. We refer to such trajectories as being feasible. In this paper, we introduce a fast and user-friendly algorithm for generating feasible quadrotor camera trajectories. Our algorithm takes as input an infeasible trajectory designed by a user, and produces as output a feasible trajectory that is as similar as possible to the user's input. By design, our algorithm does not change the spatial layout or visual contents of the input trajectory. Instead, our algorithm guarantees the feasibility of the output trajectory by re-timing the input trajectory, perturbing its timing as little as possible while remaining within velocity and control force limits. Our choice to perturb the timing of a shot, while leaving the spatial layout and visual contents of the shot intact, leads to a well-behaved non-convex optimization problem that can be solved at interactive rates. We implement our algorithm in an open-source tool for designing quadrotor camera shots, where we achieve interactive performance across a wide range of camera trajectories. We demonstrate that our algorithm is between 25x and 45x faster than a spacetime constraints approach implemented using a commercially available solver. As we scale to more finely discretized trajectories, this performance gap widens, with our algorithm outperforming spacetime constraints by between 90x and 180x. Finally, we fly 5 feasible trajectories generated by our algorithm on a real quadrotor camera, producing video footage that is faithful to Google Earth shot previews, even when the trajectories are at the quadrotor's physical limits.",Generating dynamically feasible trajectories for quadrotor cameras,NA:NA,2016
Wenbin Li:Fabio Viola:Jonathan Starck:Gabriel J. Brostow:Neill D. F. Campbell,"Rotoscoping (cutting out different characters/objects/layers in raw video footage) is a ubiquitous task in modern post-production and represents a significant investment in person-hours. In this work, we study the particular task of professional rotoscoping for high-end, live action movies and propose a new framework that works with roto-artists to accelerate the workflow and improve their productivity. Working with the existing keyframing paradigm, our first contribution is the development of a shape model that is updated as artists add successive keyframes. This model is used to improve the output of traditional interpolation and tracking techniques, reducing the number of keyframes that need to be specified by the artist. Our second contribution is to use the same shape model to provide a new interactive tool that allows an artist to reduce the time spent editing each keyframe. The more keyframes that are edited, the better the interactive tool becomes, accelerating the process and making the artist more efficient without compromising their control. Finally, we also provide a new, professionally rotoscoped dataset that enables truly representative, real-world evaluation of rotoscoping methods. We used this dataset to perform a number of experiments, including an expert study with professional roto-artists, to show, quantitatively, the advantages of our approach.",Roto++: accelerating professional rotoscoping using shape manifolds,NA:NA:NA:NA:NA,2016
Jungjin Lee:Bumki Kim:Kyehyun Kim:Younghui Kim:Junyong Noh,"This paper presents Rich360, a novel system for creating and viewing a 360° panoramic video obtained from multiple cameras placed on a structured rig. Rich360 provides an as-rich-as-possible 360° viewing experience by effectively resolving two issues that occur in the existing pipeline. First, a deformable spherical projection surface is utilized to minimize the parallax from multiple cameras. The surface is deformed spatio-temporally according to the depth constraints estimated from the overlapping video regions. This enables fast and efficient parallax-free stitching independent of the number of views. Next, a non-uniform spherical ray sampling is performed. The density of the sampling varies depending on the importance of the image region. Finally, for interactive viewing, the non-uniformly sampled video is mapped onto a uniform viewing sphere using a UV map. This approach can preserve the richness of the input videos when the resolution of the final 360° panoramic video is smaller than the overall resolution of the input videos, which is the case for most 360° panoramic videos. We show various results from Rich360 to demonstrate the richness of the output video and the advancement in the stitching results.",Rich360: optimized spherical representation from structured panoramic camera arrays,NA:NA:NA:NA:NA,2016
Qi Sun:Li-Yi Wei:Arie Kaufman,"Real walking offers higher immersive presence for virtual reality (VR) applications than alternative locomotive means such as walking-in-place and external control gadgets, but needs to take into consideration different room sizes, wall shapes, and surrounding objects in the virtual and real worlds. Despite perceptual study of impossible spaces and redirected walking, there are no general methods to match a given pair of virtual and real scenes. We propose a system to match a given pair of virtual and physical worlds for immersive VR navigation. We first compute a planar map between the virtual and physical floor plans that minimizes angular and distal distortions while conforming to the virtual environment goals and physical environment constraints. Our key idea is to design maps that are globally surjective to allow proper folding of large virtual scenes into smaller real scenes but locally injective to avoid locomotion ambiguity and intersecting virtual objects. From these maps we derive altered rendering to guide user navigation within the physical environment while retaining visual fidelity to the virtual environment. Our key idea is to properly warp the virtual world appearance into real world geometry with sufficient quality and performance. We evaluate our method through a formative user study, and demonstrate applications in gaming, architecture walkthrough, and medical imaging.",Mapping virtual and physical reality,NA:NA:NA,2016
Szymon Rusinkiewicz,NA,Session details: Materials,NA,2016
Miika Aittala:Timo Aila:Jaakko Lehtinen,"We extend parametric texture synthesis to capture rich, spatially varying parametric reflectance models from a single image. Our input is a single head-lit flash image of a mostly flat, mostly stationary (textured) surface, and the output is a tile of SVBRDF parameters that reproduce the appearance of the material. No user intervention is required. Our key insight is to make use of a recent, powerful texture descriptor based on deep convolutional neural network statistics for ""softly"" comparing the model prediction and the examplars without requiring an explicit point-to-point correspondence between them. This is in contrast to traditional reflectance capture that requires pointwise constraints between inputs and outputs under varying viewing and lighting conditions. Seen through this lens, our method is an indirect algorithm for fitting photorealistic SVBRDFs. The problem is severely ill-posed and non-convex. To guide the optimizer towards desirable solutions, we introduce a soft Fourier-domain prior for encouraging spatial stationarity of the reflectance parameters and their correlations, and a complementary preconditioning technique that enables efficient exploration of such solutions by L-BFGS, a standard non-linear numerical optimizer.",Reflectance modeling by neural texture synthesis,NA:NA:NA,2016
Leo Miyashita:Kota Ishihara:Yoshihiro Watanabe:Masatoshi Ishikawa,"Reality is the most realistic representation. We introduce a material display called ZoeMatrope that can reproduce a variety of materials with high resolution, dynamic range and light field reproducibility by using compositing and animation principles used in a zoetrope and a thaumatrope. With ZoeMatrope, the quality of the material is equivalent to that of real objects and the range of expressible materials is diversified by overlaying a set of base materials in a linear combination. ZoeMatrope is also able to express spatially-varying materials, and even augmented materials such as materials with an alpha channel. In this paper, we propose a method for selecting the optimal material set and determining the weights of the linear combination to reproduce a wide range of target materials properly. We also demonstrate the effectiveness of this approach with the developed system and show the results for various materials.",ZoeMatrope: a system for physical material design,NA:NA:NA:NA,2016
Wolfgang Heidrich,NA,Session details: Display software,NA,2016
Krzysztof Templin:Piotr Didyk:Karol Myszkowski:Hans-Peter Seidel,"The visual quality of a motion picture is significantly influenced by the choice of the presentation frame rate. Increasing the frame rate improves the clarity of the image and helps to alleviate many artifacts, such as blur, strobing, flicker, or judder. These benefits, however, come at the price of losing well-established film aesthetics, often referred to as the ""cinematic look"". Current technology leaves artists with a sparse set of choices, e.g., 24 Hz or 48 Hz, limiting the freedom in adjusting the frame rate to artistic needs, content, and display technology. In this paper, we solve this problem by proposing a novel filtering technique which enables emulating the whole spectrum of presentation frame rates on a single-frame-rate display. The key component of our technique is a set of simple yet powerful filters calibrated and evaluated in psychophysical experiments. By varying their parameters we can achieve an impression of continuously varying presentation frame rate in both the spatial and temporal dimensions. This allows artists to achieve the best balance between the aesthetics and the objective quality of the motion picture. Furthermore, we show how our technique, informed by cinematic guidelines, can adapt to the content and achieve this balance automatically.",Emulating displays with continuously varying frame rates,NA:NA:NA:NA,2016
Petr Kellnhofer:Piotr Didyk:Karol Myszkowski:Mohamed M. Hefeeda:Hans-Peter Seidel:Wojciech Matusik,"Producing a high quality stereoscopic impression on current displays is a challenging task. The content has to be carefully prepared in order to maintain visual comfort, which typically affects the quality of depth reproduction. In this work, we show that this problem can be significantly alleviated when the eye fixation regions can be roughly estimated. We propose a new method for stereoscopic depth adjustment that utilizes eye tracking or other gaze prediction information. The key idea that distinguishes our approach from the previous work is to apply gradual depth adjustments at the eye fixation stage, so that they remain unnoticeable. To this end, we measure the limits imposed on the speed of disparity changes in various depth adjustment scenarios, and formulate a new model that can guide such seamless stereoscopic content processing. Based on this model, we propose a real-time controller that applies local manipulations to stereoscopic content to find the optimum between depth reproduction and visual comfort. We show that the controller is mostly immune to the limitations of low-cost eye tracking solutions. We also demonstrate benefits of our model in off-line applications, such as stereoscopic movie production, where skillful directors can reliably guide and predict viewers' attention or where attended image regions are identified during eye tracking sessions. We validate both our model and the controller in a series of user experiments. They show significant improvements in depth perception without sacrificing the visual quality when our techniques are applied.",GazeStereo3D: seamless disparity manipulations,NA:NA:NA:NA:NA:NA,2016
Gou Koutaki,"This paper proposes multi-view display using a digital light processing (DLP) projector and new active shutter glasses. In conventional stereoscopic active shutter systems, active shutter glasses have a 0--1 (open and closed) state, and the right and left frames are temporally divided. However, this causes the display to flicker because the human eye perceives the appearance of black frames when the other shutter is closing. Furthermore, it is difficult to increase the number of views because the number of frames representing images is also divided. We solve these problems by extending the active shutter beyond the use of the 0--1 state to a continuous range of states [0, 1] instead. This relaxation leads to the formulation of a new DLP imaging model and an optimization problem. The special structure of DLP binary imaging and the continuous transmittance of the new active shutter glasses require the solution of a binary continuous image decomposition problem. Although it contains NP-hard problems, the proposed algorithm can efficiently solve the problem. The implementation of our imaging system requires the development of an active shutter device with continuous transmittance. We implemented the control of the transmittance of the liquid crystal display (LCD) shutter by using a pulse-width modulation (PWM). A simulation and the developed multi-view display system were used to show that our model can represent multi-view images more accurately than the conventional time-division 0-1 active shutter system.",Binary continuous image decomposition for multi-view display,NA,2016
Wuyao Shen:Xiangyu Mao:Xinghong Hu:Tien-Tsin Wong,"Approximately 250 million people suffer from color vision deficiency (CVD). They can hardly share the same visual content with normal-vision audiences. In this paper, we propose the first system that allows CVD and normal-vision audiences to share the same visual content simultaneously. The key that we can achieve this is because the ordinary stereoscopic display (non-autostereoscopic ones) offers users two visual experiences (with and without wearing stereoscopic glasses). By allocating one experience to CVD audiences and one to normal-vision audiences, we allow them to share. The core problem is to synthesize an image pair, that when they are presented binocularly, CVD audiences can distinguish the originally indistinguishable colors; and when it is in monocular presentation, normal-vision audiences cannot distinguish its difference from the original image. We solve the image-pair recoloring problem by optimizing an objective function that minimizes the color deviation for normal-vision audiences, and maximizes the color distinguishability and binocular fusibility for CVD audiences. Our method is extensively evaluated via multiple quantitative experiments and user studies. Convincing results are obtained in all our test cases.",Seamless visual sharing with color vision deficiencies,NA:NA:NA:NA,2016
Yaron Lipman,NA,Session details: Correspondences & mapping,NA,2016
Nicolas Bonneel:Gabriel Peyré:Marco Cuturi,"This article defines a new way to perform intuitive and geometrically faithful regressions on histogram-valued data. It leverages the theory of optimal transport, and in particular the definition of Wasserstein barycenters, to introduce for the first time the notion of barycentric coordinates for histograms. These coordinates take into account the underlying geometry of the ground space on which the histograms are defined, and are thus particularly meaningful for applications in graphics to shapes, color or material modification. Beside this abstract construction, we propose a fast numerical optimization scheme to solve this backward problem (finding the barycentric coordinates of a given histogram) with a low computational overhead with respect to the forward problem (computing the barycenter). This scheme relies on a backward algorithmic differentiation of the Sinkhorn algorithm which is used to optimize the entropic regularization of Wasserstein barycenters. We showcase an illustrative set of applications of these Wasserstein coordinates to various problems in computer graphics: shape approximation, BRDF acquisition and color editing.",Wasserstein barycentric coordinates: histogram regression using optimal transport,NA:NA:NA,2016
Justin Solomon:Gabriel Peyré:Vladimir G. Kim:Suvrit Sra,"Many shape and image processing tools rely on computation of correspondences between geometric domains. Efficient methods that stably extract ""soft"" matches in the presence of diverse geometric structures have proven to be valuable for shape retrieval and transfer of labels or semantic information. With these applications in mind, we present an algorithm for probabilistic correspondence that optimizes an entropy-regularized Gromov-Wasserstein (GW) objective. Built upon recent developments in numerical optimal transportation, our algorithm is compact, provably convergent, and applicable to any geometric domain expressible as a metric measure matrix. We provide comprehensive experiments illustrating the convergence and applicability of our algorithm to a variety of graphics tasks. Furthermore, we expand entropic GW correspondence to a framework for other matching problems, incorporating partial distance matrices, user guidance, shape exploration, symmetry detection, and joint analysis of more than two domains. These applications expand the scope of entropic GW correspondence to major shape analysis problems and are stable to distortion and noise.",Entropic metric alignment for correspondence problems,NA:NA:NA:NA,2016
Haggai Maron:Nadav Dym:Itay Kezurer:Shahar Kovalsky:Yaron Lipman,"Point cloud registration is a fundamental task in computer graphics, and more specifically, in rigid and non-rigid shape matching. The rigid shape matching problem can be formulated as the problem of simultaneously aligning and labelling two point clouds in 3D so that they are as similar as possible. We name this problem the Procrustes matching (PM) problem. The non-rigid shape matching problem can be formulated as a higher dimensional PM problem using the functional maps method. High dimensional PM problems are difficult non-convex problems which currently can only be solved locally using iterative closest point (ICP) algorithms or similar methods. Good initialization is crucial for obtaining a good solution. We introduce a novel and efficient convex SDP (semidefinite programming) relaxation for the PM problem. The algorithm is guaranteed to return a correct global solution of the problem when matching two isometric shapes which are either asymmetric or bilaterally symmetric. We show our algorithm gives state of the art results on popular shape matching datasets. We also show that our algorithm gives state of the art results for anatomical classification of shapes. Finally we demonstrate the power of our method in aligning shape collections.",Point registration via efficient convex relaxation,NA:NA:NA:NA:NA,2016
Marcel Campen:Cláudio T. Silva:Denis Zorin,"This paper presents a method for bijective parametrization of 2D and 3D objects over canonical domains. While a range of solutions for the two-dimensional case are well-known, our method guarantees bijectivity of mappings also for a large, combinatorially-defined class of tetrahedral meshes (shellable meshes). The key concept in our method is the piecewise-linear (PL) foliation, decomposing the mesh into one-dimensional submanifolds and reducing the mapping problem to parametrization of a lower-dimensional manifold (a foliation section). The maps resulting from these foliations are proved to be bijective and continuous, and shown to have provably bijective PL approximations. We describe exact, numerically robust evaluation methods and demonstrate our implementation's capabilities on a large variety of meshes.",Bijective maps from simplicial foliations,NA:NA:NA,2016
Haichao Zhu:Xueting Liu:Tien-Tsin Wong:Pheng-Ann Heng,"The ability to identify objects or region correspondences between consecutive frames of a given hand-drawn animation sequence is an indispensable tool for automating animation modification tasks such as sequence-wide recoloring or shape-editing of a specific animated character. Existing correspondence identification methods heavily rely on appearance features, but these features alone are insufficient to reliably identify region correspondences when there exist occlusions or when two or more objects share similar appearances. To resolve the above problems, manual assistance is often required. In this paper, we propose a new correspondence identification method which considers both appearance features and motions of regions in a global manner. We formulate correspondence likelihoods between temporal region pairs as a network flow graph problem which can be solved by a well-established optimization algorithm. We have evaluated our method with various animation sequences and results show that our method consistently outperforms the state-of-the-art methods without any user guidance.",Globally optimal toon tracking,NA:NA:NA:NA,2016
Craig Schroeder,NA,Session details: Fluids simulation,NA,2016
Xinxin Zhang:Minchen Li:Robert Bridson,"Most fluid scenarios in graphics have a high Reynolds number, where viscosity is dominated by inertial effects, thus most solvers drop viscosity altogether: numerical damping from coarse grids is generally stronger than physical viscosity while resembling it in character. However, viscosity remains crucial near solid boundaries, in the boundary layer, to a large extent determining the look of the flow as a function of Reynolds number. Typical graphics simulations do not resolve boundary layer dynamics, so their look is determined mostly by numerical errors with the given grid size and time step, rather than physical parameters. We introduce two complementary techniques to capture boundary layer dynamics, bringing more physical control and predictability. We extend the FLIP particle-grid method with viscous particle strength exchange[Rivoalen and Huberson 2001] to better transfer momentum at solid boundaries, dubbed VFLIP. We also introduce Weakly Higher Resolution Regional Projection (WHIRP), a cheap and simple way to increase grid resolution where important by overlaying high resolution grids on the global coarse grid.",Resolving fluid boundary layers with particle strength exchange and weak adaptivity,NA:NA:NA,2016
Albert Chern:Felix Knöppel:Ulrich Pinkall:Peter Schröder:Steffen Weißmann,"We describe a new approach for the purely Eulerian simulation of incompressible fluids. In it, the fluid state is represented by a C2-valued wave function evolving under the Schrödinger equation subject to incompressibility constraints. The underlying dynamical system is Hamiltonian and governed by the kinetic energy of the fluid together with an energy of Landau-Lifshitz type. The latter ensures that dynamics due to thin vortical structures, all important for visual simulation, are faithfully reproduced. This enables robust simulation of intricate phenomena such as vortical wakes and interacting vortex filaments, even on modestly sized grids. Our implementation uses a simple splitting method for time integration, employing the FFT for Schrödinger evolution as well as constraint projection. Using a standard penalty method we also allow arbitrary obstacles. The resulting algorithm is simple, unconditionally stable, and efficient. In particular it does not require any Lagrangian techniques for advection or to counteract the loss of vorticity. We demonstrate its use in a variety of scenarios, compare it with experiments, and evaluate it against benchmark tests. A full implementation is included in the ancillary materials.",Schrödinger's smoke,NA:NA:NA:NA:NA,2016
Fang Da:David Hahn:Christopher Batty:Chris Wojtan:Eitan Grinspun,"We propose a novel surface-only technique for simulating incompressible, inviscid and uniform-density liquids with surface tension in three dimensions. The liquid surface is captured by a triangle mesh on which a Lagrangian velocity field is stored. Because advection of the velocity field may violate the incompressibility condition, we devise an orthogonal projection technique to remove the divergence while requiring the evaluation of only two boundary integrals. The forces of surface tension, gravity, and solid contact are all treated by a boundary element solve, allowing us to perform detailed simulations of a wide range of liquid phenomena, including waterbells, droplet and jet collisions, fluid chains, and crown splashes.",Surface-only liquids,NA:NA:NA:NA:NA,2016
Xiao Yan:Yun-Tao Jiang:Chen-Feng Li:Ralph R. Martin:Shi-Min Hu,"This work extends existing multiphase-fluid SPH frameworks to cover solid phases, including deformable bodies and granular materials. In our extended multiphase SPH framework, the distribution and shapes of all phases, both fluids and solids, are uniformly represented by their volume fraction functions. The dynamics of the multiphase system is governed by conservation of mass and momentum within different phases. The behavior of individual phases and the interactions between them are represented by corresponding constitutive laws, which are functions of the volume fraction fields and the velocity fields. Our generalized multiphase SPH framework does not require separate equations for specific phases or tedious interface tracking. As the distribution, shape and motion of each phase is represented and resolved in the same way, the proposed approach is robust, efficient and easy to implement. Various simulation results are presented to demonstrate the capabilities of our new multiphase SPH framework, including deformable bodies, granular materials, interaction between multiple fluids and deformable solids, flow in porous media, and dissolution of deformable solids.",Multiphase SPH simulation for interactive fluids and solids,NA:NA:NA:NA:NA,2016
Stelian Coros,NA,Session details: Motion control,NA,2016
Daiki Satoi:Mikihiro Hagiwara:Akira Uemoto:Hisanao Nakadai:Junichi Hoshino,"We propose a unified motion planner that reproduces variations in swimming styles based on the differences in the fish skeletal structures or the variations in the swimming styles based on changes in environmental conditions. The key idea in our method, based on biology, is the following. We considered the common decision-making mechanism in fish that allows them to instantly decide ""where and how to swim."" The unified motion planner comprises two stages. In the first stage, where to swim to is decided. Using a probability distribution generated by integrating the perceptual information, the short-term target position and target speed are decided. In the second stage, how to swim is decided. A style of swimming that matches the information for transitioning from the current speed to the target speed is selected. Using the proposed method, we demonstrate 12 types of CG models with completely different sizes and skeletal structures, such as manta ray, tuna, and boxfish, as well as a scene where a school of a few thousand fish swim realistically. Our method is easy to integrate into existing graphics pipelines. In addition, in our method, the movement characteristics can easily be changed by adjusting the parameters. The method also has a feature where the expression of an entire school of fish, such as tornado or circling, can be designated top-down.",Unified motion planner for fishes with various swimming styles,NA:NA:NA:NA:NA,2016
Xue Bin Peng:Glen Berseth:Michiel van de Panne,"Reinforcement learning offers a promising methodology for developing skills for simulated characters, but typically requires working with sparse hand-crafted features. Building on recent progress in deep reinforcement learning (DeepRL), we introduce a mixture of actor-critic experts (MACE) approach that learns terrain-adaptive dynamic locomotion skills using high-dimensional state and terrain descriptions as input, and parameterized leaps or steps as output actions. MACE learns more quickly than a single actor-critic approach and results in actor-critic experts that exhibit specialization. Additional elements of our solution that contribute towards efficient learning include Boltzmann exploration and the use of initial actor biases to encourage specialization. Results are demonstrated for multiple planar characters and terrain classes.",Terrain-adaptive locomotion skills using deep reinforcement learning,NA:NA:NA,2016
Shailen Agrawal:Michiel van de Panne,"High quality locomotion is key to achieving believable character animation, but is often modeled as a generic stepping motion between two locations. In practice, locomotion often has task-specific characteristics and can exhibit a rich vocabulary of step types, including side steps, toe pivots, heel pivots, and intentional foot slides. We develop a model for such types of behaviors, based on task-specific foot-step plans that act as motion templates. The footstep plans are invoked and optimized at interactive rates and then serve as the basis for producing full body motion. We demonstrate the production of high-quality motions for three tasks: whiteboard writing, moving boxes, and sitting behaviors. The model enables retargeting to characters of varying proportions by yielding motion plans that are appropriately tailored to these proportions. We also show how the task effort or duration can be taken into account, yielding coarticulation behaviors.",Task-based locomotion,NA:NA,2016
James Hays,NA,Session details: Optimizing image processing,NA,2016
Ravi Teja Mullapudi:Andrew Adams:Dillon Sharlet:Jonathan Ragan-Kelley:Kayvon Fatahalian,"The Halide image processing language has proven to be an effective system for authoring high-performance image processing code. Halide programmers need only provide a high-level strategy for mapping an image processing pipeline to a parallel machine (a schedule), and the Halide compiler carries out the mechanical task of generating platform-specific code that implements the schedule. Unfortunately, designing high-performance schedules for complex image processing pipelines requires substantial knowledge of modern hardware architecture and code-optimization techniques. In this paper we provide an algorithm for automatically generating high-performance schedules for Halide programs. Our solution extends the function bounds analysis already present in the Halide compiler to automatically perform locality and parallelism-enhancing global program transformations typical of those employed by expert Halide developers. The algorithm does not require costly (and often impractical) auto-tuning, and, in seconds, generates schedules for a broad set of image processing benchmarks that are performance-competitive with, and often better than, schedules manually authored by expert Halide developers on server and mobile CPUs, as well as GPUs.",Automatically scheduling halide image processing pipelines,NA:NA:NA:NA:NA,2016
Felix Heide:Steven Diamond:Matthias Nießner:Jonathan Ragan-Kelley:Wolfgang Heidrich:Gordon Wetzstein,"Computational photography systems are becoming increasingly diverse, while computational resources---for example on mobile platforms---are rapidly increasing. As diverse as these camera systems may be, slightly different variants of the underlying image processing tasks, such as demosaicking, deconvolution, denoising, inpainting, image fusion, and alignment, are shared between all of these systems. Formal optimization methods have recently been demonstrated to achieve state-of-the-art quality for many of these applications. Unfortunately, different combinations of natural image priors and optimization algorithms may be optimal for different problems, and implementing and testing each combination is currently a time-consuming and error-prone process. ProxImaL is a domain-specific language and compiler for image optimization problems that makes it easy to experiment with different problem formulations and algorithm choices. The language uses proximal operators as the fundamental building blocks of a variety of linear and nonlinear image formation models and cost functions, advanced image priors, and noise models. The compiler intelligently chooses the best way to translate a problem formulation and choice of optimization algorithm into an efficient solver implementation. In applications to the image processing pipeline, deconvolution in the presence of Poisson-distributed shot noise, and burst denoising, we show that a few lines of ProxImaL code can generate highly efficient solvers that achieve state-of-the-art results. We also show applications to the nonlinear and nonconvex problem of phase retrieval.",ProxImaL: efficient image optimization using proximal algorithms,NA:NA:NA:NA:NA:NA,2016
James Hegarty:Ross Daly:Zachary DeVito:Jonathan Ragan-Kelley:Mark Horowitz:Pat Hanrahan,"Image processing algorithms implemented using custom hardware or FPGAs of can be orders-of-magnitude more energy efficient and performant than software. Unfortunately, converting an algorithm by hand to a hardware description language suitable for compilation on these platforms is frequently too time consuming to be practical. Recent work on hardware synthesis of high-level image processing languages demonstrated that a single-rate pipeline of stencil kernels can be synthesized into hardware with provably minimal buffering. Unfortunately, few advanced image processing or vision algorithms fit into this highly-restricted programming model. In this paper, we present Rigel, which takes pipelines specified in our new multi-rate architecture and lowers them to FPGA implementations. Our flexible multi-rate architecture supports pyramid image processing, sparse computations, and space-time implementation tradeoffs. We demonstrate depth from stereo, Lucas-Kanade, the SIFT descriptor, and a Gaussian pyramid running on two FPGA boards. Our system can synthesize hardware for FPGAs with up to 436 Megapixels/second throughput, and up to 297x faster runtime than a tablet-class ARM CPU.",Rigel: flexible multi-rate image processing hardware,NA:NA:NA:NA:NA:NA,2016
David I. W. Levin,NA,"Session details: Computational design of structures, shapes, and sound",NA,2016
Eder Miguel:Mathias Lepoutre:Bernd Bickel,"We present a computational method for designing wire sculptures consisting of interlocking wires. Our method allows the computation of aesthetically pleasing structures that are structurally stable, efficiently fabricatable with a 2D wire bending machine, and assemblable without the need of additional connectors. Starting from a set of planar contours provided by the user, our method automatically tests for the feasibility of a design, determines a discrete ordering of wires at intersection points, and optimizes for the rest shape of the individual wires to maximize structural stability under frictional contact. In addition to their application to art, wire sculptures present an extremely efficient and fast alternative for low-fidelity rapid prototyping because manufacturing time and required material linearly scales with the physical size of objects. We demonstrate the effectiveness of our approach on a varied set of examples, all of which we fabricated.",Computational design of stable planar-rod structures,NA:NA:NA,2016
Przemyslaw Musialski:Christian Hafner:Florian Rist:Michael Birsak:Michael Wimmer:Leif Kobbelt,"In this paper we present a novel method for non-linear shape optimization of 3d objects given by their surface representation. Our method takes advantage of the fact that various shape properties of interest give rise to underdetermined design spaces implying the existence of many good solutions. Our algorithm exploits this by performing iterative projections of the problem to local subspaces where it can be solved much more efficiently using standard numerical routines. We demonstrate how this approach can be utilized for various shape optimization tasks using different shape parameterizations. In particular, we show how to efficiently optimize natural frequencies, mass properties, as well as the structural yield strength of a solid body. Our method is flexible, easy to implement, and very fast.",Non-linear shape optimization using local subspace projections,NA:NA:NA:NA:NA:NA,2016
Dingzeyu Li:David I. W. Levin:Wojciech Matusik:Changxi Zheng,"Acoustic filters have a wide range of applications, yet customizing them with desired properties is difficult. Motivated by recent progress in additive manufacturing that allows for fast prototyping of complex shapes, we present a computational approach that automates the design of acoustic filters with complex geometries. In our approach, we construct an acoustic filter comprised of a set of parameterized shape primitives, whose transmission matrices can be precomputed. Using an efficient method of simulating the transmission matrix of an assembly built from these underlying primitives, our method is able to optimize both the arrangement and the parameters of the acoustic shape primitives in order to satisfy target acoustic properties of the filter. We validate our results against industrial laboratory measurements and high-quality off-line simulations. We demonstrate that our method enables a wide range of applications including muffler design, musical wind instrument prototyping, and encoding imperceptible acoustic information into everyday objects.",Acoustic voxels: computational optimization of modular acoustic filters,NA:NA:NA:NA,2016
Etienne Vouga,NA,Session details: Deformable surface design,NA,2016
Mina Konaković:Keenan Crane:Bailin Deng:Sofien Bouaziz:Daniel Piker:Mark Pauly,"We present a computational method for interactive 3D design and rationalization of surfaces via auxetic materials, i.e., flat flexible material that can stretch uniformly up to a certain extent. A key motivation for studying such material is that one can approximate doubly-curved surfaces (such as the sphere) using only flat pieces, making it attractive for fabrication. We physically realize surfaces by introducing cuts into approximately inextensible material such as sheet metal, plastic, or leather. The cutting pattern is modeled as a regular triangular linkage that yields hexagonal openings of spatially-varying radius when stretched. In the same way that isometry is fundamental to modeling developable surfaces, we leverage conformal geometry to understand auxetic design. In particular, we compute a global conformal map with bounded scale factor to initialize an otherwise intractable non-linear optimization. We demonstrate that this global approach can handle non-trivial topology and non-local dependencies inherent in auxetic material. Design studies and physical prototypes are used to illustrate a wide range of possible applications.",Beyond developable: computational design and fabrication with auxetic materials,NA:NA:NA:NA:NA:NA,2016
Akash Garg:Alec Jacobson:Eitan Grinspun,"A reconfigurable is an object or collection of objects whose transformation between various states defines its functionality or aesthetic appeal. For example, consider a mechanical assembly composed of interlocking pieces, a transforming folding bicycle, or a space-saving arrangement of apartment furniture. Unlike traditional computer-aided design of static objects, specialized tools are required to address problems unique to the computational design and revision of objects undergoing rigid transformations. Collisions and interpenetrations as objects transition from one configuration to another prevent the physical realization of a design. We present a software environment intended to support fluid interactive design of reconfigurables, featuring tools that identify, visualize, monitor and resolve infeasible configurations. We demonstrate the versatility of the environment on a number of examples spanning mechanical systems, urban dwelling, and interlocking puzzles, some of which we then realize via additive manufacturing. Spatial-temporal information about collisions between objects is presented to the designer according to a cascading order of precedence. A designer may quickly determine when, and then where, and then how objects are colliding. This precedence guides the design and implementation of our four-dimensional spacetime bounding volume hierarchy for interactive-rate collision detection. On screen, the designer experiences a suite of interactive visualization and monitoring tools during editing: timeline notifications of new collisions, picture-in-picture windows for tracking collisions and suggestive hints for contact resolution. Contacts too tedious to remove manually can be eliminated automatically via our proposed constrained numerical optimization and swept-volume carving.",Computational design of reconfigurables,NA:NA:NA,2016
Connelly Barnes,NA,Session details: Image & shape manipulation,NA,2016
Claudio Calabrese:Gabriele Salvati:Marco Tarini:Fabio Pellacini,"Collaborative systems are well established solutions for sharing work among people. In computer graphics these workflows are still not well established, compared to what is done for text writing or software development. Usually artists work alone and share their final models by sending files. In this paper we present a system for collaborative 3D digital sculpting. In our prototype, multiple artists concurrently sculpt a polygonal mesh on their local machines by changing its vertex properties, such as positions and material BRDFs. Our system shares the artists' edits automatically and seamlessly merges these edits even when they happen on the same region of the surface. We propose a merge algorithm that is fast-enough for seamless collaboration, respects users' edits as much as possible, can support any sculpting operation, and works for both geometry and appearance modifications. Since in sculpting artists alternatively perform fine adjustments and large scale modifications, our algorithm is based on a multiresolution edit representation that handles concurrent overlapping edits at different scales. We tested our algorithm by modeling meshes collaboratively in different sculpting sessions and found that our algorithm outperforms prior works on collaborative mesh editing in all cases.",cSculpt: a system for collaborative sculpting,NA:NA:NA:NA,2016
Jakub Fišer:Ondřej Jamriška:Michal Lukáč:Eli Shechtman:Paul Asente:Jingwan Lu:Daniel Sýkora,"We present an approach to example-based stylization of 3D renderings that better preserves the rich expressiveness of hand-created artwork. Unlike previous techniques, which are mainly guided by colors and normals, our approach is based on light propagation in the scene. This novel type of guidance can distinguish among context-dependent illumination effects, for which artists typically use different stylization techniques, and delivers a look closer to realistic artwork. In addition, we demonstrate that the current state of the art in guided texture synthesis produces artifacts that can significantly decrease the fidelity of the synthesized imagery, and propose an improved algorithm that alleviates them. Finally, we demonstrate our method's effectiveness on a variety of scenes and styles, in applications like interactive shading study or autocompletion.",StyLit: illumination-guided example-based stylization of 3D renderings,NA:NA:NA:NA:NA:NA:NA,2016
Romain Vergne:Pascal Barla:Georges-Pierre Bonneau:Roland W. Fleming,"We present an interactive method that manipulates perceived object shape from a single input color image thanks to a warping technique implemented on the GPU. The key idea is to give the illusion of shape sharpening or rounding by exaggerating orientation patterns in the image that are strongly correlated to surface curvature. We build on a growing literature in both human and computer vision showing the importance of orientation patterns in the communication of shape, which we complement with mathematical relationships and a statistical image analysis revealing that structure tensors are indeed strongly correlated to surface shape features. We then rely on these correlations to introduce a flow-guided image warping algorithm, which in effect exaggerates orientation patterns involved in shape perception. We evaluate our technique by 1) comparing it to ground truth shape deformations, and 2) performing two perceptual experiments to assess its effects. Our algorithm produces convincing shape manipulation results on synthetic images and photographs, for various materials and lighting environments.",Flow-guided warping for image-based shape manipulation,NA:NA:NA:NA,2016
Ira Kemelmacher-Shlizerman,"People may look dramatically different by changing their hair color, hair style, when they grow older, in a different era style, or a different country or occupation. Some of those may transfigure appearance and inspire creative changes, some not, but how would we know without physically trying? We present a system that enables automatic synthesis of limitless numbers of appearances. A user inputs one or more photos (as many as they like) of his or her face, text queries an appearance of interest (just like they'd search an image search engine) and gets as output the input person in the queried appearance. Rather than fixing the number of queries or a dataset our system utilizes all the relevant and searchable images on the Internet, estimates a doppelgänger set for the inputs, and utilizes it to generate composites. We present a large number of examples on photos taken with completely unconstrained imaging conditions.",Transfiguring portraits,NA,2016
Nikunj Raghuvanshi,NA,"Session details: Sound, fluids & boundaries",NA,2016
Timothy R. Langlois:Changxi Zheng:Doug L. James,"This paper explores methods for synthesizing physics-based bubble sounds directly from two-phase incompressible simulations of bubbly water flows. By tracking fluid-air interface geometry, we identify bubble geometry and topological changes due to splitting, merging and popping. A novel capacitance-based method is proposed that can estimate volume-mode bubble frequency changes due to bubble size, shape, and proximity to solid and air interfaces. Our acoustic transfer model is able to capture cavity resonance effects due to near-field geometry, and we also propose a fast precomputed bubble-plane model for cheap transfer evaluation. In addition, we consider a bubble forcing model that better accounts for bubble entrainment, splitting, and merging events, as well as a Helmholtz resonator model for bubble popping sounds. To overcome frequency bandwidth limitations associated with coarse resolution fluid grids, we simulate micro-bubbles in the audio domain using a power-law model of bubble populations. Finally, we present several detailed examples of audiovisual water simulations and physical experiments to validate our frequency model.",Toward animating water with complex acoustic bubbles,NA:NA:NA,2016
Morten Bojsen-Hansen:Chris Wojtan,"When aiming to seamlessly integrate a fluid simulation into a larger scenario (like an open ocean), careful attention must be paid to boundary conditions. In particular, one must implement special ""non-reflecting"" boundary conditions, which dissipate out-going waves as they exit the simulation. Unfortunately, the state of the art in non-reflecting boundary conditions (perfectly-matched layers, or PMLs) only permits trivially simple inflow/outflow conditions, so there is no reliable way to integrate a fluid simulation into a more complicated environment like a stormy ocean or a turbulent river. This paper introduces the first method for combining non-reflecting boundary conditions based on PMLs with inflow/outflow boundary conditions that vary arbitrarily throughout space and time. Our algorithm is a generalization of state-of-the-art mean-flow boundary conditions in the computational fluid dynamics literature, and it allows for seamless integration of a fluid simulation into much more complicated environments. Our method also opens the door for previously-unseen post-process effects like retroactively changing the location of solid obstacles, and locally increasing the visual detail of a pre-existing simulation.",Generalized non-reflecting boundaries for fluid re-simulation,NA:NA,2016
Vinicius C. Azevedo:Christopher Batty:Manuel M. Oliveira,"Fluid animation methods based on Eulerian grids have long struggled to resolve flows involving narrow gaps and thin solid features. Past approaches have artificially inflated or voxelized boundaries, although this sacrifices the correct geometry and topology of the fluid domain and prevents flow through narrow regions. We present a boundary-respecting fluid simulator that overcomes these challenges. Our solution is to intersect the solid boundary geometry with the cells of a background regular grid to generate a topologically correct, boundary-conforming cut-cell mesh. We extend both pressure projection and velocity advection to support this enhanced grid structure. For pressure projection, we introduce a general graph-based scheme that properly preserves discrete incompressibility even in thin and topologically complex flow regions, while nevertheless yielding symmetric positive definite linear systems. For advection, we exploit polyhedral interpolation to improve the degree to which the flow conforms to irregular and possibly non-convex cell boundaries, and propose a modified PIC/FLIP advection scheme to eliminate the need to inaccurately reinitialize invalid cells that are swept over by moving boundaries. The method naturally extends the standard Eulerian fluid simulation framework, and while we focus on thin boundaries, our contributions are beneficial for volumetric solids as well. Our results demonstrate successful one-way fluid-solid coupling in the presence of thin objects and narrow flow regions even on very coarse grids.",Preserving geometry and topology for fluid flows with thin obstacles and narrow gaps,NA:NA:NA,2016
Richard Hao Zhang,NA,Session details: Curve & strut networks for fabrication,NA,2016
Weikai Chen:Xiaolong Zhang:Shiqing Xin:Yang Xia:Sylvain Lefebvre:Wenping Wang,"Filigrees are thin patterns found in jewelry, ornaments and lace fabrics. They are often formed of repeated base elements manually composed into larger, delicate patterns. Digital fabrication simplifies the process of turning a virtual model of a filigree into a physical object. However, designing a virtual model of a filigree remains a time consuming and challenging task. The difficulty lies in tightly packing together the base elements while covering a target surface. In addition, the filigree has to be well connected and sufficiently robust to be fabricated. We propose a novel approach automating this task. Our technique covers a target surface with a set of input base elements, forming a filigree strong enough to be fabricated. We exploit two properties of filigrees to make this possible. First, as filigrees form delicate traceries they are well captured by their skeleton. This affords for a simpler definition of operators such as matching and deformation. Second, instead of seeking for a perfect packing of the base elements we relax the problem by allowing appearance preserving partial overlaps. We optimize a filigree by a stochastic search, further improved by a novel boosting algorithm that records and reuses good configurations discovered during the process. We illustrate our technique on a number of challenging examples reproducing filigrees on large objects, which we manufacture by 3D printing. Our technique affords for several user controls, such as the scale and orientation of the elements.",Synthesis of filigrees for digital fabrication,NA:NA:NA:NA:NA:NA,2016
Jonas Zehnder:Stelian Coros:Bernhard Thomaszewski,"We present a computational tool for designing ornamental curve networks---structurally-sound physical surfaces with user-controlled aesthetics. In contrast to approaches that leverage texture synthesis for creating decorative surface patterns, our method relies on user-defined spline curves as central design primitives. More specifically, we build on the physically-inspired metaphor of an embedded elastic curve that can move on a smooth surface, deform, and connect with other curves. We formalize this idea as a globally coupled energy-minimization problem, discretized with piece-wise linear curves that are optimized in the parametric space of a smooth surface. Building on this technical core, we propose a set of interactive design and editing tools that we demonstrate on manually-created layouts and semi-automated deformable packings. In order to prevent excessive compliance, we furthermore propose a structural analysis tool that uses eigenanalysis to identify potentially large deformations between geodesically-close curves and guide the user in strengthening the corresponding regions. We used our approach to create a variety of designs in simulation, validated with a set of 3D-printed physical prototypes.",Designing structurally-sound ornamental curve networks,NA:NA:NA,2016
Haisen Zhao:Fanglin Gu:Qi-Xing Huang:Jorge Garcia:Yong Chen:Changhe Tu:Bedrich Benes:Hao Zhang:Daniel Cohen-Or:Baoquan Chen,"We develop a new kind of ""space-filling"" curves, connected Fermat spirals, and show their compelling properties as a tool path fill pattern for layered fabrication. Unlike classical space-filling curves such as the Peano or Hilbert curves, which constantly wind and bind to preserve locality, connected Fermat spirals are formed mostly by long, low-curvature paths. This geometric property, along with continuity, influences the quality and efficiency of layered fabrication. Given a connected 2D region, we first decompose it into a set of sub-regions, each of which can be filled with a single continuous Fermat spiral. We show that it is always possible to start and end a Fermat spiral fill at approximately the same location on the outer boundary of the filled region. This special property allows the Fermat spiral fills to be joined systematically along a graph traversal of the decomposed sub-regions. The result is a globally continuous curve. We demonstrate that printing 2D layers following tool paths as connected Fermat spirals leads to efficient and quality fabrication, compared to conventional fill patterns.",Connected fermat spirals for layered fabrication,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2016
Rundong Wu:Huaishu Peng:François Guimbretière:Steve Marschner,"Traditional 3D printers fabricate objects by depositing material to build up the model layer by layer. Instead printing only wireframes can reduce printing time and the cost of material while producing effective depictions of shape. However, wireframe printing requires the printer to undergo arbitrary 3D motions, rather than slice-wise 2D motions, which can lead to collisions with already-printed parts of the model. Previous work has either limited itself to restricted meshes that are collision free by construction, or simply dropped unreachable parts of the model, but in this paper we present a method to print arbitrary meshes on a 5DOF wireframe printer. We formalize the collision avoidance problem using a directed graph, and propose an algorithm that finds a locally minimal set of constraints on the order of edges that guarantees there will be no collisions. Then a second algorithm orders the edges so that the printing progresses smoothly. Though meshes do exist that still cannot be printed, our method prints a wide range of models that previous methods cannot, and it provides a fundamental enabling algorithm for future development of wireframe printing.",Printing arbitrary meshes with a 5DOF wireframe printer,NA:NA:NA:NA,2016
Adam Bargteil,NA,Session details: Physical phenomena,NA,2016
Gilles Daviet:Florence Bertails-Descoubes,"We present a new continuum-based method for the realistic simulation of large-scale free-flowing granular materials. We derive a compact model for the rheology of the material, which accounts for the exact nonsmooth Drucker-Prager yield criterion combined with a varying volume fraction. Thanks to a semi-implicit time-stepping scheme and a careful spatial discretization of our rheology built upon the Material-Point Method, we are able to preserve at each time step the exact coupling between normal and tangential stresses, in a stable way. This contrasts with previous approaches which either regularize or linearize the yield criterion for implicit integration, leading to unrealistic behaviors or visible grid artifacts. Remarkably, our discrete problem turns out to be very similar to the discrete contact problem classically encountered in multibody dynamics, which allows us to leverage robust and efficient nonsmooth solvers from the literature. We validate our method by successfully capturing typical macroscopic features of some classical experiments, such as the discharge of a silo or the collapse of a granular column. Finally, we show that our method can be easily extended to accommodate more complex scenarios including two-way rigid body coupling as well as anisotropic materials.",A semi-implicit material point method for the continuum simulation of granular materials,NA:NA,2016
Gergely Klár:Theodore Gast:Andre Pradhana:Chuyuan Fu:Craig Schroeder:Chenfanfu Jiang:Joseph Teran,"We simulate sand dynamics using an elastoplastic, continuum assumption. We demonstrate that the Drucker-Prager plastic flow model combined with a Hencky-strain-based hyperelasticity accurately recreates a wide range of visual sand phenomena with moderate computational expense. We use the Material Point Method (MPM) to discretize the governing equations for its natural treatment of contact, topological change and history dependent constitutive relations. The Drucker-Prager model naturally represents the frictional relation between shear and normal stresses through a yield stress criterion. We develop a stress projection algorithm used for enforcing this condition with a non-associative flow rule that works naturally with both implicit and explicit time integration. We demonstrate the efficacy of our approach on examples undergoing large deformation, collisions and topological changes necessary for producing modern visual effects.",Drucker-prager elastoplasticity for sand animation,NA:NA:NA:NA:NA:NA:NA,2016
David Hahn:Chris Wojtan,"We present a boundary element based method for fast simulation of brittle fracture. By introducing simplifying assumptions that allow us to quickly estimate stress intensities and opening displacements during crack propagation, we build a fracture algorithm where the cost of each time step scales linearly with the length of the crack-front. The transition from a full boundary element method to our faster variant is possible at the beginning of any time step. This allows us to build a hybrid method, which uses the expensive but more accurate BEM while the number of degrees of freedom is low, and uses the fast method once that number exceeds a given threshold as the crack geometry becomes more complicated. Furthermore, we integrate this fracture simulation with a standard rigid-body solver. Our rigid-body coupling solves a Neumann boundary value problem by carefully separating translational, rotational and deformational components of the collision forces and then applying a Tikhonov regularizer to the resulting linear system. We show that our method produces physically reasonable results in standard test cases and is capable of dealing with complex scenes faster than previous finite- or boundary element approaches.",Fast approximations for boundary element based brittle fracture simulation,NA:NA,2016
Justin Solomon,NA,Session details: Mappings,NA,2016
Edward Chien:Renjie Chen:Ofir Weber,"Planar shape interpolation is a classic problem in computer graphics. We present a novel shape interpolation method that blends C∞ planar harmonic mappings represented in closed-form. The intermediate mappings in the blending are guaranteed to be locally injective C∞ harmonic mappings, with conformal and isometric distortion bounded by that of the input mappings. The key to the success of our method is the fact that the blended differentials of our interpolated mapping have a simple closed-form expression, so they can be evaluated with unprecedented efficiency and accuracy. Moreover, in contrast to previous approaches, these differentials are integrable, and result in an actual mapping without further modification. Our algorithm is embarrassingly parallel and is orders of magnitude faster than state-of-the-art methods due to its simplicity, yet it still produces mappings that are superior to those of existing techniques due to its guaranteed bounds on geometric distortion.",Bounded distortion harmonic shape interpolation,NA:NA:NA,2016
Zohar Levi:Ofir Weber,"Computation of mappings is a central building block in many geometry processing and graphics applications. The pursuit to compute mappings that are injective and have a controllable amount of conformal and isometric distortion is a long endeavor which has received significant attention by the scientific community in recent years. The difficulty of the problem stems from the fact that the space of bounded distortion mappings is nonconvex. In this paper, we consider the special case of harmonic mappings which have been used extensively in many graphics applications. We show that, somewhat surprisingly, the space of locally injective planar harmonic mappings with bounded conformal and isometric distortion has a convex characterization. We describe several projection operators that, given an arbitrary input mapping, are guaranteed to output a bounded distortion locally injective harmonic mapping that is closest to the input mapping in some special sense. In contrast to alternative approaches, the optimization problems that correspond to our projection operators are shown to be always feasible for any choice of distortion bounds. We use the boundary element method (BEM) to discretize the space of planar harmonic mappings and demonstrate the effectiveness of our approach through the application of planar shape deformation.",On the convexity and feasibility of the bounded distortion harmonic mapping problem,NA:NA,2016
Marco Tarini,"UV-maps are required in order to apply a 2D texture over a 3D model. Conventional UV-maps are defined by an assignment of uv positions to mesh vertices. We present an alternative representation, volume-encoded UV-maps, in which each point on the surface is mapped to a uv position which is solely a function of its 3D position. This function is tailored for a target surface: its restriction to the surface is a parametrization exhibiting high quality, e.g. in terms of angle and area preservation; and, near the surface, it is almost constant for small orthogonal displacements. The representation is applicable to a wide range of shapes and UV-maps, and unlocks several key advantages: it removes the need to duplicate vertices in the mesh to encode cuts in the map; it makes the UV-map representation independent from the meshing of the surface; the same texture, and even the same UV-map, can be shared by multiple geometrically similar models (e.g. all levels of a LoD pyramid); UV-maps can be applied to representations other than polygonal meshes, like point clouds or set of registered range-maps. Our schema is cheap on GPU computational and memory resources, requiring only a single, cache-coherent indirection to a small volumetric texture per fragment. We also provide an algorithm to construct a volume-encoded UV-map given a target surface.",Volume-encoded UV-maps,NA,2016
Fabián Prada:Misha Kazhdan:Ming Chuang:Alvaro Collet:Hugues Hoppe,"Scanned performances are commonly represented in virtual environments as sequences of textured triangle meshes. Detailed shapes deforming over time benefit from meshes with dynamically evolving connectivity. We analyze these unstructured mesh sequences to automatically synthesize motion graphs with new smooth transitions between compatible poses and actions. Such motion graphs enable natural periodic motions, stochastic playback, and user-directed animations. The main challenge of unstructured sequences is that the meshes differ not only in connectivity but also in alignment, shape, and texture. We introduce new geometry processing techniques to address these problems and demonstrate visually seamless transitions on high-quality captures.",Motion graphs for unstructured textured meshes,NA:NA:NA:NA:NA,2016
Piotr Didyk,NA,Session details: Intrinsic images,NA,2016
Abhimitra Meka:Michael Zollhöfer:Christian Richardt:Christian Theobalt,"Intrinsic video decomposition refers to the fundamentally ambiguous task of separating a video stream into its constituent layers, in particular reflectance and shading layers. Such a decomposition is the basis for a variety of video manipulation applications, such as realistic recoloring or retexturing of objects. We present a novel variational approach to tackle this underconstrained inverse problem at real-time frame rates, which enables on-line processing of live video footage. The problem of finding the intrinsic decomposition is formulated as a mixed variational ℓ2-ℓp-optimization problem based on an objective function that is specifically tailored for fast optimization. To this end, we propose a novel combination of sophisticated local spatial and global spatio-temporal priors resulting in temporally coherent decompositions at real-time frame rates without the need for explicit correspondence search. We tackle the resulting high-dimensional, non-convex optimization problem via a novel data-parallel iteratively reweighted least squares solver that runs on commodity graphics hardware. Real-time performance is obtained by combining a local-global solution strategy with hierarchical coarse-to-fine optimization. Compelling real-time augmented reality applications, such as recoloring, material editing and retexturing, are demonstrated in a live setup. Our qualitative and quantitative evaluation shows that we obtain high-quality real-time decompositions even for challenging sequences. Our method is able to outperform state-of-the-art approaches in terms of runtime and result quality -- even without user guidance such as scribbles.",Live intrinsic video,NA:NA:NA:NA,2016
Satoshi Iizuka:Edgar Simo-Serra:Hiroshi Ishikawa,"We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network features a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification database to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Furthermore, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.",Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification,NA:NA:NA,2016
Andrew Selle,NA,Session details: Rendering & simulation with GPUs,NA,2016
Rui Wang:Bowen Yu:Julio Marco:Tianlei Hu:Diego Gutierrez:Hujun Bao,"With recent advances on mobile computing, power consumption has become a significant limiting constraint for many graphics applications. As a result, rendering on a power budget arises as an emerging demand. In this paper, we present a real-time, power-optimal rendering framework to address this problem, by finding the optimal rendering settings that minimize power consumption while maximizing visual quality. We first introduce a novel power-error, multi-objective cost space, and formally formulate power saving as an optimization problem. Then, we develop a two-step algorithm to efficiently explore the vast power-error space and leverage optimal Pareto frontiers at runtime. Finally, we show that our rendering framework can be generalized across different platforms, desktop PC or mobile device, by demonstrating its performance on our own OpenGL rendering framework, as well as the commercially available Unreal Engine.",Real-time rendering on a power budget,NA:NA:NA:NA:NA:NA,2016
Yong He:Tim Foley:Kayvon Fatahalian,"We present Spire, a shading language and compiler framework that facilitates rapid exploration of shader optimization choices (such as frequency reduction and algorithmic approximation) afforded by modern real-time graphics engines. Our design combines ideas from rate-based shader programming with new language features that expand the scope of shader execution beyond traditional GPU hardware pipelines, and enable a diverse set of shader optimizations to be described by a single mechanism: overloading shader terms at various spatio-temporal computation rates provided by the pipeline. In contrast to prior work, neither the shading language's design, nor our compiler framework's implementation, is specific to the capabilities of any one rendering pipeline, thus Spire establishes architectural separation between the shading system and the implementation of modern rendering engines (allowing different rendering pipelines to utilize its services). We demonstrate use of Spire to author complex shaders that are portable across different rendering pipelines and to rapidly explore shader optimization decisions that span multiple compute and graphics passes and even offline asset preprocessing. We further demonstrate the utility of Spire by developing a shader level-of-detail library and shader auto-tuning system on top of its abstractions, and demonstrate rapid, automatic re-optimization of shaders for different target hardware platforms.",A system for rapid exploration of shader optimization choices,NA:NA:NA,2016
Wade Brainerd:Tim Foley:Manuel Kraemer:Henry Moreton:Matthias Nießner,"We present a novel method for real-time rendering of subdivision surfaces whose goal is to make subdivision faces as easy to render as triangles, points, or lines. Our approach uses standard GPU tessellation hardware and processes each face of a base mesh independently, thus allowing an entire model to be rendered in a single pass. The key idea of our method is to subdivide the u, v domain of each face ahead of time, generating a quadtree structure, and then submit one tessellated primitive per input face. By traversing the quadtree for each post-tessellation vertex, we are able to accurately and efficiently evaluate the limit surface. Our method yields a more uniform tessellation of the surface, and faster rendering, as fewer primitives are submitted. We evaluate our method on a variety of assets, and realize performance that can be three times faster than state-of-the-art approaches. In addition, our streaming formulation makes it easier to integrate subdivision surfaces into applications and shader code written for polygonal models. We illustrate integration of our technique into a full-featured video game engine.",Efficient GPU rendering of subdivision surfaces using adaptive quadtrees,NA:NA:NA:NA:NA,2016
Kun Zhou,NA,Session details: Capturing humans,NA,2016
Mingsong Dou:Sameh Khamis:Yury Degtyarev:Philip Davidson:Sean Ryan Fanello:Adarsh Kowdle:Sergio Orts Escolano:Christoph Rhemann:David Kim:Jonathan Taylor:Pushmeet Kohli:Vladimir Tankovich:Shahram Izadi,"We contribute a new pipeline for live multi-view performance capture, generating temporally coherent high-quality reconstructions in real-time. Our algorithm supports both incremental reconstruction, improving the surface estimation over time, as well as parameterizing the nonrigid scene motion. Our approach is highly robust to both large frame-to-frame motion and topology changes, allowing us to reconstruct extremely challenging scenes. We demonstrate advantages over related real-time techniques that either deform an online generated template or continually fuse depth data nonrigidly into a single reference model. Finally, we show geometric reconstruction results on par with offline methods which require orders of magnitude more processing time and many more RGBD cameras.",Fusion4D: real-time performance capture of challenging scenes,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2016
Chenglei Wu:Derek Bradley:Markus Gross:Thabo Beeler,"We present a new anatomically-constrained local face model and fitting approach for tracking 3D faces from 2D motion data in very high quality. In contrast to traditional global face models, often built from a large set of blendshapes, we propose a local deformation model composed of many small subspaces spatially distributed over the face. Our local model offers far more flexibility and expressiveness than global blendshape models, even with a much smaller model size. This flexibility would typically come at the cost of reduced robustness, in particular during the under-constrained task of monocular reconstruction. However, a key contribution of this work is that we consider the face anatomy and introduce subspace skin thickness constraints into our model, which constrain the face to only valid expressions and helps counteract depth ambiguities in monocular tracking. Given our new model, we present a novel fitting optimization that allows 3D facial performance reconstruction from a single view at extremely high quality, far beyond previous fitting approaches. Our model is flexible, and can be applied also when only sparse motion data is available, for example with marker-based motion capture or even face posing from artistic sketches. Furthermore, by incorporating anatomical constraints we can automatically estimate the rigid motion of the skull, obtaining a rigid stabilization of the performance for free. We demonstrate our model and single-view fitting method on a number of examples, including, for the first time, extreme local skin deformation caused by external forces such as wind, captured from a single high-speed camera.",An anatomically-constrained local deformation model for monocular face capture,NA:NA:NA:NA,2016
Menglei Chai:Tianjia Shao:Hongzhi Wu:Yanlin Weng:Kun Zhou,"We introduce AutoHair, the first fully automatic method for 3D hair modeling from a single portrait image, with no user interaction or parameter tuning. Our method efficiently generates complete and high-quality hair geometries, which are comparable to those generated by the state-of-the-art methods, where user interaction is required. The core components of our method are: a novel hierarchical deep neural network for automatic hair segmentation and hair growth direction estimation, trained over an annotated hair image database; and an efficient and automatic data-driven hair matching and modeling algorithm, based on a large set of 3D hair exemplars. We demonstrate the efficacy and robustness of our method on Internet photos, resulting in a database of around 50K 3D hair models and a corresponding hairstyle space that covers a wide variety of real-world hairstyles. We also show novel applications enabled by our method, including 3D hairstyle space navigation and hair-aware image retrieval.",AutoHair: fully automatic hair modeling from a single image,NA:NA:NA:NA:NA,2016
Pascal Bérard:Derek Bradley:Markus Gross:Thabo Beeler,"Facial scanning has become ubiquitous in digital media, but so far most efforts have focused on reconstructing the skin. Eye reconstruction, on the other hand, has received only little attention, and the current state-of-the-art method is cumbersome for the actor, time-consuming, and requires carefully setup and calibrated hardware. These constraints currently make eye capture impractical for general use. We present the first approach for high-quality lightweight eye capture, which leverages a database of pre-captured eyes to guide the reconstruction of new eyes from much less constrained inputs, such as traditional single-shot face scanners or even a single photo from the internet. This is accomplished with a new parametric model of the eye built from the database, and a novel image-based model fitting algorithm. Our method provides both automatic reconstructions of real eyes, as well as artistic control over the parameters to generate user-specific eyes.",Lightweight eye capture using a parametric model,NA:NA:NA:NA,2016
Congyi Wang:Fuhao Shi:Shihong Xia:Jinxiang Chai,"This paper presents the first realtime 3D eye gaze capture method that simultaneously captures the coordinated movement of 3D eye gaze, head poses and facial expression deformation using a single RGB camera. Our key idea is to complement a realtime 3D facial performance capture system with an efficient 3D eye gaze tracker. We start the process by automatically detecting important 2D facial features for each frame. The detected facial features are then used to reconstruct 3D head poses and large-scale facial deformation using multi-linear expression deformation models. Next, we introduce a novel user-independent classification method for extracting iris and pupil pixels in each frame. We formulate the 3D eye gaze tracker in the Maximum A Posterior (MAP) framework, which sequentially infers the most probable state of 3D eye gaze at each frame. The eye gaze tracker could fail when eye blinking occurs. We further introduce an efficient eye close detector to improve the robustness and accuracy of the eye gaze tracker. We have tested our system on both live video streams and the Internet videos, demonstrating its accuracy and robustness under a variety of uncontrolled lighting conditions and overcoming significant differences of races, genders, shapes, poses and expressions across individuals.",Realtime 3D eye gaze animation using a single RGB camera,NA:NA:NA:NA,2016
Karan Singh,NA,Session details: Sketching & writing,NA,2016
Patsorn Sangkloy:Nathan Burnell:Cusuh Ham:James Hays,"We present the Sketchy database, the first large-scale collection of sketch-photo pairs. We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. The Sketchy database gives us fine-grained associations between particular photos and sketches, and we use this to train cross-domain convolutional networks which embed sketches and photographs in a common feature space. We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both hand-crafted features as well as deep features trained for sketch or photo classification. Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.",The sketchy database: learning to retrieve badly drawn bunnies,NA:NA:NA:NA,2016
Jean-Dominique Favreau:Florent Lafarge:Adrien Bousseau,"Vector drawing is a popular representation in graphic design because of the precision, compactness and editability offered by parametric curves. However, prior work on line drawing vectorization focused solely on faithfully capturing input bitmaps, and largely overlooked the problem of producing a compact and editable curve network. As a result, existing algorithms tend to produce overly-complex drawings composed of many short curves and control points, especially in the presence of thick or sketchy lines that yield spurious curves at junctions. We propose the first vectorization algorithm that explicitly balances fidelity to the input bitmap with simplicity of the output, as measured by the number of curves and their degree. By casting this trade-off as a global optimization, our algorithm generates few yet accurate curves, and also disambiguates curve topology at junctions by favoring the simplest interpretations overall. We demonstrate the robustness of our algorithm on a variety of drawings, sketchy cartoons and rough design sketches.",Fidelity vs. simplicity: a global approach to line drawing vectorization,NA:NA:NA,2016
Edgar Simo-Serra:Satoshi Iizuka:Kazuma Sasaki:Hiroshi Ishikawa,"In this paper, we present a novel technique to simplify sketch drawings based on learning a series of convolution operators. In contrast to existing approaches that require vector images as input, we allow the more general and challenging input of rough raster sketches such as those obtained from scanning pencil sketches. We convert the rough sketch into a simplified version which is then amendable for vectorization. This is all done in a fully automatic way without user intervention. Our model consists of a fully convolutional neural network which, unlike most existing convolutional neural networks, is able to process images of any dimensions and aspect ratio as input, and outputs a simplified sketch which has the same dimensions as the input image. In order to teach our model to simplify, we present a new dataset of pairs of rough and simplified sketch drawings. By leveraging convolution operators in combination with efficient use of our proposed dataset, we are able to train our sketch simplification model. Our approach naturally overcomes the limitations of existing methods, e.g., vector images as input and long computation time; and we show that meaningful simplifications can be obtained for many different test cases. Finally, we validate our results with a user study in which we greatly outperform similar approaches and establish the state of the art in sketch simplification of raster images.",Learning to simplify: fully convolutional networks for rough sketch cleanup,NA:NA:NA:NA,2016
Changqing Zou:Junjie Cao:Warunika Ranaweera:Ibraheem Alhashim:Ping Tan:Alla Sheffer:Hao Zhang,"A calligram is an arrangement of words or letters that creates a visual image, and a compact calligram fits one word into a 2D shape. We introduce a fully automatic method for the generation of legible compact calligrams which provides a balance between conveying the input shape, legibility, and aesthetics. Our method has three key elements: a path generation step which computes a global layout path suitable for embedding the input word; an alignment step to place the letters so as to achieve feature alignment between letter and shape protrusions while maintaining word legibility; and a final deformation step which deforms the letters to fit the shape while balancing fit against letter legibility. As letter legibility is critical to the quality of compact calligrams, we conduct a large-scale crowd-sourced study on the impact of different letter deformations on legibility and use the results to train a letter legibility measure which guides the letter deformation. We show automatically generated calligrams on an extensive set of word-image combinations. The legibility and overall quality of the calligrams are evaluated and compared, via user studies, to those produced by human creators, including a professional artist, and existing works.",Legible compact calligrams,NA:NA:NA:NA:NA:NA:NA,2016
Daniele Panozzo,NA,Session details: Meshes,NA,2016
Max Lyon:David Bommes:Leif Kobbelt,"State-of-the-art hex meshing algorithms consist of three steps: Frame-field design, parametrization generation, and mesh extraction. However, while the first two steps are usually discussed in detail, the last step is often not well studied. In this paper, we fully concentrate on reliable mesh extraction. Parametrization methods employ computationally expensive countermeasures to avoid mapping input tetrahedra to degenerate or flipped tetrahedra in the parameter domain because such a parametrization does not define a proper hexahedral mesh. Nevertheless, there is no known technique that can guarantee the complete absence of such artifacts. We tackle this problem from the other side by developing a mesh extraction algorithm which is extremely robust against typical imperfections in the parametrization. First, a sanitization process cleans up numerical inconsistencies of the parameter values caused by limited precision solvers and floating-point number representation. On the sanitized parametrization, we extract vertices and so-called darts based on intersections of the integer grid with the parametric image of the tetrahedral mesh. The darts are reliably interconnected by tracing within the parametrization and thus define the topology of the hexahedral mesh. In a postprocessing step, we let certain pairs of darts cancel each other, counteracting the effect of flipped regions of the parametrization. With this strategy, our algorithm is able to robustly extract hexahedral meshes from imperfect parametrizations which previously would have been considered defective. The algorithm will be published as an open source library [Lyon et al. 2016].",HexEx: robust hexahedral mesh extraction,NA:NA:NA,2016
Xianzhong Fang:Weiwei Xu:Hujun Bao:Jin Huang,"The polycube-based hexahedralization methods are robust to generate all-hex meshes without internal singularities. They avoid the difficulty to control the global singularity structure for a valid hexahedralization in frame-field based methods. To thoroughly utilize this advantage, we propose to use a frame field without internal singularities to guide the polycube construction. Theoretically, our method extends the vector fields associated with the polycube from exact forms to closed forms, which are curl free everywhere but may be not globally integrable. The closed forms give additional degrees of freedom to deal with the topological structure of high-genus models, and also provide better initial axis alignment for subsequent polycube generation. We demonstrate the advantages of our method on various models, ranging from genus-zero models to high-genus ones, and from single-boundary models to multiple-boundary ones.",All-hex meshing using closed-form induced polycube,NA:NA:NA:NA,2016
Yipeng Qin:Xiaoguang Han:Hongchuan Yu:Yizhou Yu:Jianjun Zhang,"Computing discrete geodesic distance over triangle meshes is one of the fundamental problems in computational geometry and computer graphics. In this problem, an effective window pruning strategy can significantly affect the actual running time. Due to its importance, we conduct an in-depth study of window pruning operations in this paper, and produce an exhaustive list of scenarios where one window can make another window partially or completely redundant. To identify a maximal number of redundant windows using such pairwise cross checking, we propose a set of procedures to synchronize local window propagation within the same triangle by simultaneously propagating a collection of windows from one triangle edge to its two opposite edges. On the basis of such synchronized window propagation, we design a new geodesic computation algorithm based on a triangle-oriented region growing scheme. Our geodesic algorithm can remove most of the redundant windows at the earliest possible stage, thus significantly reducing computational cost and memory usage at later stages. In addition, by adopting triangles instead of windows as the primitive in propagation management, our algorithm significantly cuts down the data management overhead. As a result, it runs 4--15 times faster than MMP and ICH algorithms, 2-4 times faster than FWP-MMP and FWP-CH algorithms, and also incurs the least memory usage.",Fast and exact discrete geodesic computation based on triangle-oriented wavefront propagation,NA:NA:NA:NA:NA,2016
Andrew Nealen,NA,Session details: Faces & portraits,NA,2016
Chen Cao:Hongzhi Wu:Yanlin Weng:Tianjia Shao:Kun Zhou,"We present a novel image-based representation for dynamic 3D avatars, which allows effective handling of various hairstyles and headwear, and can generate expressive facial animations with fine-scale details in real-time. We develop algorithms for creating an image-based avatar from a set of sparsely captured images of a user, using an off-the-shelf web camera at home. An optimization method is proposed to construct a topologically consistent morphable model that approximates the dynamic hair geometry in the captured images. We also design a real-time algorithm for synthesizing novel views of an image-based avatar, so that the avatar follows the facial motions of an arbitrary actor. Compelling results from our pipeline are demonstrated on a variety of cases.",Real-time facial animation with image-based dynamic avatars,NA:NA:NA:NA:NA,2016
Pif Edwards:Chris Landreth:Eugene Fiume:Karan Singh,"The rich signals we extract from facial expressions imposes high expectations for the science and art of facial animation. While the advent of high-resolution performance capture has greatly improved realism, the utility of procedural animation warrants a prominent place in facial animation workflow. We present a system that, given an input audio soundtrack and speech transcript, automatically generates expressive lip-synchronized facial animation that is amenable to further artistic refinement, and that is comparable with both performance capture and professional animator output. Because of the diversity of ways we produce sound, the mapping from phonemes to visual depictions as visemes is many-valued. We draw from psycholinguistics to capture this variation using two visually distinct anatomical actions: Jaw and Lip, wheresound is primarily controlled by jaw articulation and lower-face muscles, respectively. We describe the construction of a transferable template jali 3D facial rig, built upon the popular facial muscle action unit representation facs. We show that acoustic properties in a speech signal map naturally to the dynamic degree of jaw and lip in visual speech. We provide an array of compelling animation clips, compare against performance capture and existing procedural animation, and report on a brief user study.",JALI: an animator-centric viseme model for expressive lip synchronization,NA:NA:NA:NA,2016
Ohad Fried:Eli Shechtman:Dan B. Goldman:Adam Finkelstein,"This paper introduces a method to modify the apparent relative pose and distance between camera and subject given a single portrait photo. Our approach fits a full perspective camera and a parametric 3D head model to the portrait, and then builds a 2D warp in the image plane to approximate the effect of a desired change in 3D. We show that this model is capable of correcting objectionable artifacts such as the large noses sometimes seen in ""selfies,"" or to deliberately bring a distant camera closer to the subject. This framework can also be used to re-pose the subject, as well as to create stereo pairs from an input portrait. We show convincing results on both an existing dataset as well as a new dataset we captured to validate our method.",Perspective-aware manipulation of portrait photos,NA:NA:NA:NA,2016
Ahmed Selim:Mohamed Elgharib:Linda Doyle,Head portraits are popular in traditional painting. Automating portrait painting is challenging as the human visual system is sensitive to the slightest irregularities in human faces. Applying generic painting techniques often deforms facial structures. On the other hand portrait painting techniques are mainly designed for the graphite style and/or are based on image analogies; an example painting as well as its original unpainted version are required. This limits their domain of applicability. We present a new technique for transferring the painting from a head portrait onto another. Unlike previous work our technique only requires the example painting and is not restricted to a specific style. We impose novel spatial constraints by locally transferring the color distributions of the example painting. This better captures the painting texture and maintains the integrity of facial structures. We generate a solution through Convolutional Neural Networks and we present an extension to video. Here motion is exploited in a way to reduce temporal inconsistencies and the shower-door effect. Our approach transfers the painting style while maintaining the input photograph identity. In addition it significantly reduces facial deformations over state of the art.,Painting style transfer for head portraits using convolutional neural networks,NA:NA:NA,2016
Takeo Igarashi,NA,Session details: Procedural modeling,NA,2016
Gen Nishida:Ignacio Garcia-Dorado:Daniel G. Aliaga:Bedrich Benes:Adrien Bousseau,"3D modeling remains a notoriously difficult task for novices despite significant research effort to provide intuitive and automated systems. We tackle this problem by combining the strengths of two popular domains: sketch-based modeling and procedural modeling. On the one hand, sketch-based modeling exploits our ability to draw but requires detailed, unambiguous drawings to achieve complex models. On the other hand, procedural modeling automates the creation of precise and detailed geometry but requires the tedious definition and parameterization of procedural models. Our system uses a collection of simple procedural grammars, called snippets, as building blocks to turn sketches into realistic 3D models. We use a machine learning approach to solve the inverse problem of finding the procedural model that best explains a user sketch. We use non-photorealistic rendering to generate artificial data for training convolutional neural networks capable of quickly recognizing the procedural rule intended by a sketch and estimating its parameters. We integrate our algorithm in a coarse-to-fine urban modeling system that allows users to create rich buildings by successively sketching the building mass, roof, facades, windows, and ornaments. A user study shows that by using our approach non-expert users can generate complex buildings in just a few minutes.",Interactive sketching of urban procedural models,NA:NA:NA:NA:NA,2016
Chi-Han Peng:Yong-Liang Yang:Fan Bao:Daniel Fink:Dong-Ming Yan:Peter Wonka:Niloy J. Mitra,"Connectivity and layout of underlying networks largely determine agent behavior and usage in many environments. For example, transportation networks determine the flow of traffic in a neighborhood, whereas building floorplans determine the flow of people in a workspace. Designing such networks from scratch is challenging as even local network changes can have large global effects. We investigate how to computationally create networks starting from only high-level functional specifications. Such specifications can be in the form of network density, travel time versus network length, traffic type, destination location, etc. We propose an integer programming-based approach that guarantees that the resultant networks are valid by fulfilling all the specified hard constraints and that they score favorably in terms of the objective function. We evaluate our algorithm in two different design settings, street layout and floorplans to demonstrate that diverse networks can emerge purely from high-level functional specifications.",Computational network design from functional specifications,NA:NA:NA:NA:NA:NA:NA,2016
Tian Feng:Lap-Fai Yu:Sai-Kit Yeung:KangKang Yin:Kun Zhou,"We propose a novel approach for designing mid-scale layouts by optimizing with respect to human crowd properties. Given an input layout domain such as the boundary of a shopping mall, our approach synthesizes the paths and sites by optimizing three metrics that measure crowd flow properties: mobility, accessibility, and coziness. While these metrics are straightforward to evaluate by a full agent-based crowd simulation, optimizing a layout usually requires hundreds of evaluations, which would require a long time to compute even using the latest crowd simulation techniques. To overcome this challenge, we propose a novel data-driven approach where nonlinear regressors are trained to capture the relationship between the agent-based metrics, and the geometrical and topological features of a layout. We demonstrate that by using the trained regressors, our approach can synthesize crowd-aware layouts and improve existing layouts with better crowd flow properties.",Crowd-driven mid-scale layout design,NA:NA:NA:NA:NA,2016
Wenping Wang,NA,Session details: Meshes & fields,NA,2016
Fernando de Goes:Mathieu Desbrun:Mark Meyer:Tony DeRose,"This paper introduces a new computational method to solve differential equations on subdivision surfaces. Our approach adapts the numerical framework of Discrete Exterior Calculus (DEC) from the polygonal to the subdivision setting by exploiting the refin-ability of subdivision basis functions. The resulting Subdivision Exterior Calculus (SEC) provides significant improvements in accuracy compared to existing polygonal techniques, while offering exact finite-dimensional analogs of continuum structural identities such as Stokes' theorem and Helmholtz-Hodge decomposition. We demonstrate the versatility and efficiency of SEC on common geometry processing tasks including parameterization, geodesic distance computation, and vector field design.",Subdivision exterior calculus for geometry processing,NA:NA:NA:NA,2016
Shahar Z. Kovalsky:Meirav Galun:Yaron Lipman,"We present the Accelerated Quadratic Proxy (AQP) - a simple first-order algorithm for the optimization of geometric energies defined over triangular and tetrahedral meshes. The main stumbling block of current optimization techniques used to minimize geometric energies over meshes is slow convergence due to ill-conditioning of the energies at their minima. We observe that this ill-conditioning is in large part due to a Laplacian-like term existing in these energies. Consequently, we suggest to locally use a quadratic polynomial proxy, whose Hessian is taken to be the Laplacian, in order to achieve a preconditioning effect. This already improves stability and convergence, but more importantly allows incorporating acceleration in an almost universal way, that is independent of mesh size and of the specific energy considered. Experiments with AQP show it is rather insensitive to mesh resolution and requires a nearly constant number of iterations to converge; this is in strong contrast to other popular optimization techniques used today such as Accelerated Gradient Descent and Quasi-Newton methods, e.g., L-BFGS. We have tested AQP for mesh deformation in 2D and 3D as well as for surface parameterization, and found it to provide a considerable speedup over common baseline techniques.",Accelerated quadratic proxy for geometric optimization,NA:NA:NA,2016
Xin Li:G. Thomas Finnigan:Thomas W. Sederberg,"This paper develops new refinement rules for non-uniform Catmull-Clark surfaces that produce G1 extraordinary points whose blending functions have a single local maximum. The method consists of designing an ""eigen polyhedron"" in R2 for each extraordinary point, and formulating refinement rules for which refinement of the eigen polyhedron reduces to a scale and translation. These refinement rules, when applied to a non-uniform Catmull-Clark control mesh in R3, yield a G1 extraordinary point.",G1 non-uniform Catmull-Clark surfaces,NA:NA:NA,2016
KangKang Yin,NA,Session details: Plants & humans,NA,2016
Andrew Owens:Mikolaj Cieslak:Jeremy Hart:Regine Classen-Bockhoff:Przemyslaw Prusinkiewicz,"Showy inflorescences - clusters of flowers - are a common feature of many plants, greatly contributing to their beauty. The large numbers of individual flowers (florets), arranged in space in a systematic manner, make inflorescences a natural target for procedural modeling. We present a suite of biologically motivated algorithms for modeling and animating the development of inflorescences with closely packed florets. These inflorescences share the following characteristics: (i) in their ensemble, the florets form a relatively smooth, often approximately planar surface; (ii) there are numerous collisions between petals of the same or adjacent florets; and (iii) the developmental stage and type of a floret may depend on its position within the inflorescence, with drastic or gradual differences. To model flat-topped branched inflorescences (corymbs and umbels), we propose a florets-first algorithm, in which the branching structure self-organizes to support florets in predetermined positions. This is an alternative to previous branching-first models, in which floret positions were determined by branch arrangement. To obtain realistic visualizations, we complement the algorithms that generate the inflorescence structure with an interactive method for modeling floret corollas (petal sets). The method supports corollas with both separate and fused petals. We illustrate our techniques with models from several plant families.",Modeling dense inflorescences,NA:NA:NA:NA:NA,2016
M. Ersin Yumer:Niloy J. Mitra,"Human motion is complex and difficult to synthesize realistically. Automatic style transfer to transform the mood or identity of a character's motion is a key technology for increasing the value of already synthesized or captured motion data. Typically, state-of-the-art methods require all independent actions observed in the input to be present in a given style database to perform realistic style transfer. We introduce a spectral style transfer method for human motion between independent actions, thereby greatly reducing the required effort and cost of creating such databases. We leverage a spectral domain representation of the human motion to formulate a spatial correspondence free approach. We extract spectral intensity representations of reference and source styles for an arbitrary action, and transfer their difference to a novel motion which may contain previously unseen actions. Building on this core method, we introduce a temporally sliding window filter to perform the same analysis locally in time for heterogeneous motion processing. This immediately allows our approach to serve as a style database enhancement technique to fill-in non-existent actions in order to increase previous style transfer method's performance. We evaluate our method both via quantitative experiments, and through administering controlled user studies with respect to previous work, where significant improvement is observed with our approach.",Spectral style transfer for human motion between independent actions,NA:NA,2016
Daniel Holden:Jun Saito:Taku Komura,"We present a framework to synthesize character movements based on high level parameters, such that the produced movements respect the manifold of human motion, trained on a large motion capture dataset. The learned motion manifold, which is represented by the hidden units of a convolutional autoencoder, represents motion data in sparse components which can be combined to produce a wide range of complex movements. To map from high level parameters to the motion manifold, we stack a deep feedforward neural network on top of the trained autoencoder. This network is trained to produce realistic motion sequences from parameters such as a curve over the terrain that the character should follow, or a target location for punching and kicking. The feedforward control network and the motion manifold are trained independently, allowing the user to easily switch between feedforward networks according to the desired interface, without re-training the motion manifold. Once motion is generated it can be edited by performing optimization in the space of the motion manifold. This allows for imposing kinematic constraints, or transforming the style of the motion, while ensuring the edited motion remains natural. As a result, the system can produce smooth, high quality motion sequences without any manual pre-processing of the training data.",A deep learning framework for character motion synthesis and editing,NA:NA:NA,2016
Manolis Savva:Angel X. Chang:Pat Hanrahan:Matthew Fisher:Matthias Nießner,"We learn a probabilistic model connecting human poses and arrangements of object geometry from real-world observations of interactions collected with commodity RGB-D sensors. This model is encoded as a set of prototypical interaction graphs (PiGraphs), a human-centric representation capturing physical contact and visual attention linkages between 3D geometry and human body parts. We use this encoding of the joint probability distribution over pose and geometry during everyday interactions to generate interaction snapshots, which are static depictions of human poses and relevant objects during human-object interactions. We demonstrate that our model enables a novel human-centric understanding of 3D content and allows for jointly generating 3D scenes and interaction poses given terse high-level specifications, natural language, or reconstructed real-world scene constraints.",PiGraphs: learning interaction snapshots from observations,NA:NA:NA:NA:NA,2016
Sylvain Lefebvre,NA,Session details: Texture,NA,2016
Yitzchak David Lockerman:Basile Sauvage:Rémi Allègre:Jean-Michel Dischler:Julie Dorsey:Holly Rushmeier,"Texture synthesis is a well-established area, with many important applications in computer graphics and vision. However, despite their success, synthesis techniques are not used widely in practice because the creation of good exemplars remains challenging and extremely tedious. In this paper, we introduce an unsupervised method for analyzing texture content across multiple scales that automatically extracts good exemplars from natural images. Unlike existing methods, which require extensive manual tuning, our method is fully automatic. This allows the user to focus on using texture palettes derived from their own images, rather than on manual interactions dictated by the needs of an underlying algorithm. Most natural textures exhibit patterns at multiple scales that may vary according to the location (non-stationarity). To handle such textures many synthesis algorithms rely on an analysis of the input and a guidance of the synthesis. Our new analysis is based on a labeling of texture patterns that is both (i) multi-scale and (ii) unsupervised -- that is, patterns are labeled at multiple scales, and the scales and the number of labeled clusters are selected automatically. Our method works in two stages. The first builds a hierarchical extension of superpixels and the second labels the superpixels based on random walk in a graph of similarity between superpixels and a nonnegative matrix factorization. Our label-maps provide descriptors for pixels and regions that benefit state-of-the-art texture synthesis algorithms. We show several applications including guidance of non-stationary synthesis, content selection and texture painting. Our method is designed to treat large inputs and can scale to many megapixels. In addition to traditional exemplar inputs, our method can also handle natural images containing different textured regions.",Multi-scale label-map extraction for texture synthesis,NA:NA:NA:NA:NA:NA,2016
Rachele Bellini:Yanir Kleiman:Daniel Cohen-Or,"We present a technique to synthesize time-varying weathered textures. Given a single texture image as input, the degree of weathering at different regions of the input texture is estimated by prevalence analysis of texture patches. This information then allows to gracefully increase or decrease the popularity of weathered patches, simulating the evolution of texture appearance both backward and forward in time. Our method can be applied to a wide variety of different textures since the reaction of the material to weathering effects is physically-oblivious and learned from the input texture itself. The weathering process evolves new structures as well as color variations, providing rich and natural results. In contrast with existing methods, our method does not require any user interaction or assistance. We demonstrate our technique on various textures, and their application to time-varying weathering of 3D scenes. We also extend our method to handle multi-layered textures, weathering transfer, and interactive weathering painting.",Time-varying weathering in texture space,NA:NA:NA,2016
Maneesh Agrawala,NA,Session details: User interfaces,NA,2016
Jaime Lien:Nicholas Gillian:M. Emre Karagozler:Patrick Amihood:Carsten Schwesig:Erik Olson:Hakim Raja:Ivan Poupyrev,"This paper presents Soli, a new, robust, high-resolution, low-power, miniature gesture sensing technology for human-computer interaction based on millimeter-wave radar. We describe a new approach to developing a radar-based sensor optimized for human-computer interaction, building the sensor architecture from the ground up with the inclusion of radar design principles, high temporal resolution gesture tracking, a hardware abstraction layer (HAL), a solid-state radar chip and system architecture, interaction models and gesture vocabularies, and gesture recognition. We demonstrate that Soli can be used for robust gesture recognition and can track gestures with sub-millimeter accuracy, running at over 10,000 frames per second on embedded hardware.",Soli: ubiquitous gesture sensing with millimeter wave radar,NA:NA:NA:NA:NA:NA:NA:NA,2016
Jonathan Taylor:Lucas Bordeaux:Thomas Cashman:Bob Corish:Cem Keskin:Toby Sharp:Eduardo Soto:David Sweeney:Julien Valentin:Benjamin Luff:Arran Topalian:Erroll Wood:Sameh Khamis:Pushmeet Kohli:Shahram Izadi:Richard Banks:Andrew Fitzgibbon:Jamie Shotton,"Fully articulated hand tracking promises to enable fundamentally new interactions with virtual and augmented worlds, but the limited accuracy and efficiency of current systems has prevented widespread adoption. Today's dominant paradigm uses machine learning for initialization and recovery followed by iterative model-fitting optimization to achieve a detailed pose fit. We follow this paradigm, but make several changes to the model-fitting, namely using: (1) a more discriminative objective function; (2) a smooth-surface model that provides gradients for non-linear optimization; and (3) joint optimization over both the model pose and the correspondences between observed data points and the model surface. While each of these changes may actually increase the cost per fitting iteration, we find a compensating decrease in the number of iterations. Further, the wide basin of convergence means that fewer starting points are needed for successful model fitting. Our system runs in real-time on CPU only, which frees up the commonly over-burdened GPU for experience designers. The hand tracker is efficient enough to run on low-power devices such as tablets. We can track up to several meters from the camera to provide a large working volume for interaction, even using the noisy data from current-generation depth cameras. Quantitative assessments on standard datasets show that the new approach exceeds the state of the art in accuracy. Qualitative results take the form of live recordings of a range of interactive experiences enabled by this new approach.","Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences",NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2016
Oliver Glauser:Wan-Chun Ma:Daniele Panozzo:Alec Jacobson:Otmar Hilliges:Olga Sorkine-Hornung,"We propose a novel approach to digital character animation, combining the benefits of tangible input devices and sophisticated rig animation algorithms. A symbiotic software and hardware approach facilitates the animation process for novice and expert users alike. We overcome limitations inherent to all previous tangible devices by allowing users to directly control complex rigs using only a small set (5-10) of physical controls. This avoids oversimplification of the pose space and excessively bulky device configurations. Our algorithm derives a small device configuration from complex character rigs, often containing hundreds of degrees of freedom, and a set of sparse sample poses. Importantly, only the most influential degrees of freedom are controlled directly, yet detailed motion is preserved based on a pose interpolation technique. We designed a modular collection of joints and splitters, which can be assembled to represent a wide variety of skeletons. Each joint piece combines a universal joint and two twisting elements, allowing to accurately sense its configuration. The mechanical design provides a smooth inverse kinematics-like user experience and is not prone to gimbal locking. We integrate our method with the professional 3D software Autodesk Maya® and discuss a variety of results created with characters available online. Comparative user experiments show significant improvements over the closest state-of-the-art in terms of accuracy and time in a keyframe posing task.",Rig animation with a tangible and modular input device,NA:NA:NA:NA:NA:NA,2016
Marie-Paule Cani,NA,Session details: Expressive animation,NA,2016
Yunfei Bai:Danny M. Kaufman:C. Karen Liu:Jovan Popović,"Animation artists enjoy the benefits of simulation but do not want to be held back by its constraints. Artist-directed dynamics seeks to resolve this need with a unified method that combines simulation with classical keyframing techniques. The combination of these approaches improves upon both extremes: simulation becomes more customizable and keyframing becomes more automatic. Examining our system in the context of the twelve fundamental animation principles reveals that it stands out for its treatment of exaggeration and appeal. Our system accommodates abrupt jumps, large plastic deformations, and makes it easy to reuse carefully crafted animations.",Artist-directed dynamics for 2D animation,NA:NA:NA:NA,2016
Byungkuk Choi:Roger Blanco i Ribera:J. P. Lewis:Yeongho Seol:Seokpyo Hong:Haegwang Eom:Sunjin Jung:Junyong Noh,"We present SketchiMo, a novel approach for the expressive editing of articulated character motion. SketchiMo solves for the motion given a set of projective constraints that relate the sketch inputs to the unknown 3D poses. We introduce the concept of sketch space, a contextual geometric representation of sketch targets---motion properties that are editable via sketch input---that enhances, right on the viewport, different aspects of the motion. The combination of the proposed sketch targets and space allows for seamless editing of a wide range of properties, from simple joint trajectories to local parent-child spatiotemporal relationships and more abstract properties such as coordinated motions. This is made possible by interpreting the user's input through a new sketch-based optimization engine in a uniform way. In addition, our view-dependent sketch space also serves the purpose of disambiguating the user inputs by visualizing their range of effect and transparently defining the necessary constraints to set the temporal boundaries for the optimization.",SketchiMo: sketch-based motion editing for articulated characters,NA:NA:NA:NA:NA:NA:NA:NA,2016
Jungdam Won:Jehee Lee,"Shadow theatre is a genre of performance art in which the actors are only visible as shadows projected on the screen. The goal of this study is to generate animated characters, the shadows of which match a sequence of target silhouettes. This poses several challenges. The motion of multiple characters are carefully coordinated to form a target silhouette on the screen, and each character's pose should be stable, balanced, and plausible. The resulting character animation should be smooth and coherent spatially and temporally. We formulate the problem as nonlinear constrained optimization with objectives, which were designed to generate plausible human motions. Our optimization algorithm was primarily inspired by the heuristic strategies of professional shadow theatre actors. Their know-how was studied and then incorporated into our optimization formulation. We demonstrate the effectiveness of our approach with a variety of target silhouettes and 3D fabrication of the results.",Shadow theatre: discovering human motion from a sequence of silhouettes,NA:NA,2016
Ping Tan,NA,Session details: Photo organization & manipulation,NA,2016
Huiwen Chang:Fisher Yu:Jue Wang:Douglas Ashley:Adam Finkelstein,"People often take a series of nearly redundant pictures to capture a moment or scene. However, selecting photos to keep or share from a large collection is a painful chore. To address this problem, we seek a relative quality measure within a series of photos taken of the same scene, which can be used for automatic photo triage. Towards this end, we gather a large dataset comprised of photo series distilled from personal photo albums. The dataset contains 15, 545 unedited photos organized in 5,953 series. By augmenting this dataset with ground truth human preferences among photos within each series, we establish a benchmark for measuring the effectiveness of algorithmic models of how people select photos. We introduce several new approaches for modeling human preference based on machine learning. We also describe applications for the dataset and predictor, including a smart album viewer, automatic photo enhancement, and providing overviews of video clips.",Automatic triage for a photo series,NA:NA:NA:NA:NA,2016
Sylvain Paris,NA,Session details: Imaginative imaging,NA,2017
Brandon M. Smith:Pratham Desai:Vishal Agarwal:Mohit Gupta,"We present CoLux, a novel system for measuring micro 3D motion of multiple independently moving objects at macroscopic standoff distances. CoLux is based on speckle imaging, where the scene is illuminated with a coherent light source and imaged with a camera. Coherent light, on interacting with optically rough surfaces, creates a high-frequency speckle pattern in the captured images. The motion of objects results in movement of speckle, which can be measured to estimate the object motion. Speckle imaging is widely used for micro-motion estimation in several applications, including industrial inspection, scientific imaging, and user interfaces (e.g., optical mice). However, current speckle imaging methods are largely limited to measuring 2D motion (parallel to the sensor image plane) of a single rigid object. We develop a novel theoretical model for speckle movement due to multi-object motion, and present a simple technique based on global scale-space speckle motion analysis for measuring small (5--50 microns) compound motion of multiple objects, along all three axes. Using these tools, we develop a method for measuring 3D micro-motion histograms of multiple independently moving objects, without tracking the individual motion trajectories. In order to demonstrate the capabilities of CoLux, we develop a hardware prototype and a proof-of-concept subtle hand gesture recognition system with a broad range of potential applications in user interfaces and interactive computer graphics.",CoLux: multi-object 3D micro-motion analysis using speckle imaging,NA:NA:NA:NA,2017
Julian Iseringhausen:Bastian Goldlücke:Nina Pesheva:Stanimir Iliev:Alexander Wender:Martin Fuchs:Matthias B. Hullin,"Light fields are a powerful concept in computational imaging and a mainstay in image-based rendering; however, so far their acquisition required either carefully designed and calibrated optical systems (micro-lens arrays), or multi-camera/multi-shot settings. Here, we show that fully calibrated light field data can be obtained from a single ordinary photograph taken through a partially wetted window. Each drop of water produces a distorted view on the scene, and the challenge of recovering the unknown mapping from pixel coordinates to refracted rays in space is a severely underconstrained problem. The key idea behind our solution is to combine ray tracing and low-level image analysis techniques (extraction of 2D drop contours and locations of scene features seen through drops) with state-of-the-art drop shape simulation and an iterative refinement scheme to enforce photo-consistency across features that are seen in multiple views. This novel approach not only recovers a dense pixel-to-ray mapping, but also the refractive geometry through which the scene is observed, to high accuracy. We therefore anticipate that our inherently self-calibrating scheme might also find applications in other fields, for instance in materials science where the wetting properties of liquids on surfaces are investigated.",4D imaging through spray-on optics,NA:NA:NA:NA:NA:NA:NA,2017
Jinhui Xiong:Ramzi Idoughi:Andres A. Aguirre-Pablo:Abdulrahman B. Aljedaani:Xiong Dun:Qiang Fu:Sigurdur T. Thoroddsen:Wolfgang Heidrich,"Despite significant recent progress, dense, time-resolved imaging of complex, non-stationary 3D flow velocities remains an elusive goal. In this work we tackle this problem by extending an established 2D method, Particle Imaging Velocimetry, to three dimensions by encoding depth into color. The encoding is achieved by illuminating the flow volume with a continuum of light planes (a ""rainbow""), such that each depth corresponds to a specific wavelength of light. A diffractive component in the camera optics ensures that all planes are in focus simultaneously. With this setup, a single color camera is sufficient for tracking 3D trajectories of particles by combining 2D spatial and 1D color information. For reconstruction, we derive an image formation model for recovering stationary 3D particle positions. 3D velocity estimation is achieved with a variant of 3D optical flow that accounts for both physical constraints as well as the rainbow image formation model. We evaluate our method with both simulations and an experimental prototype setup.",Rainbow particle imaging velocimetry for dense 3D fluid velocity imaging,NA:NA:NA:NA:NA:NA:NA:NA,2017
Supreeth Achar:Joseph R. Bartels:William L. 'Red' Whittaker:Kiriakos N. Kutulakos:Srinivasa G. Narasimhan,"Consumer time-of-flight depth cameras like Kinect and PMD are cheap, compact and produce video-rate depth maps in short-range applications. In this paper we apply energy-efficient epipolar imaging to the ToF domain to significantly expand the versatility of these sensors: we demonstrate live 3D imaging at over 15 m range outdoors in bright sunlight; robustness to global transport effects such as specular and diffuse inter-reflections---the first live demonstration for this ToF technology; interference-free 3D imaging in the presence of many ToF sensors, even when they are all operating at the same optical wavelength and modulation frequency; and blur-free, distortion-free 3D video in the presence of severe camera shake. We believe these achievements can make such cheap ToF devices broadly applicable in consumer and robotics domains.",Epipolar time-of-flight imaging,NA:NA:NA:NA:NA,2017
Ofir Weber,NA,Session details: Mappings and deformations,NA,2017
Michael Rabinovich:Roi Poranne:Daniele Panozzo:Olga Sorkine-Hornung,"We present a scalable approach for the optimization of flip-preventing energies in the general context of simplicial mappings and specifically for mesh parameterization. Our iterative minimization is based on the observation that many distortion energies can be optimized indirectly by minimizing a family of simpler proxy energies. Minimization of these proxies is a natural extension of the local/global minimization of the ARAP energy. Our algorithm is simple to implement and scales to datasets with millions of faces. We demonstrate our approach for the computation of maps that minimize a conformal or isometric distortion energy, both in two and three dimensions. In addition to mesh parameterization, we show that our algorithm can be applied to mesh deformation and mesh quality improvement.",Scalable Locally Injective Mappings,NA:NA:NA:NA,2017
Anna Shtengel:Roi Poranne:Olga Sorkine-Hornung:Shahar Z. Kovalsky:Yaron Lipman,"Many algorithms on meshes require the minimization of composite objectives, i.e., energies that are compositions of simpler parts. Canonical examples include mesh parameterization and deformation. We propose a second order optimization approach that exploits this composite structure to efficiently converge to a local minimum. Our main observation is that a convex-concave decomposition of the energy constituents is simple and readily available in many cases of practical relevance in graphics. We utilize such convex-concave decompositions to define a tight convex majorizer of the energy, which we employ as a convex second order approximation of the objective function. In contrast to existing approaches that largely use only local convexification, our method is able to take advantage of a more global view on the energy landscape. Our experiments on triangular meshes demonstrate that our approach outperforms the state of the art on standard problems in geometry processing, and potentially provide a unified framework for developing efficient geometric optimization algorithms.",Geometric optimization via composite majorization,NA:NA:NA:NA:NA,2017
Manish Mandad:David Cohen-Steiner:Leif Kobbelt:Pierre Alliez:Mathieu Desbrun,"We introduce an efficient computational method for generating dense and low distortion maps between two arbitrary surfaces of same genus. Instead of relying on semantic correspondences or surface parameterization, we directly optimize a variance-minimizing transport plan between two input surfaces that defines an as-conformal-as-possible inter-surface map satisfying a user-prescribed bound on area distortion. The transport plan is computed via two alternating convex optimizations, and is shown to minimize a generalized Dirichlet energy of both the map and its inverse. Computational efficiency is achieved through a coarse-to-fine approach in diffusion geometry, with Sinkhorn iterations modified to enforce bounded area distortion. The resulting inter-surface mapping algorithm applies to arbitrary shapes robustly, with little to no user interaction.",Variance-minimizing transport plans for inter-surface mapping,NA:NA:NA:NA:NA,2017
Fernando De Goes:Doug L. James,"We introduce a new technique for real-time physically based volume sculpting of virtual elastic materials. Our formulation is based on the elastic response to localized force distributions associated with common modeling primitives such as grab, scale, twist, and pinch. The resulting brush-like displacements correspond to the regularization of fundamental solutions of linear elasticity in infinite 2D and 3D media. These deformations thus provide the realism and plausibility of volumetric elasticity, and the interactivity of closed-form analytical solutions. To finely control our elastic deformations, we also construct compound brushes with arbitrarily fast spatial decay. Furthermore, pointwise constraints can be imposed on the displacement field and its derivatives via a single linear solve. We demonstrate the versatility and efficiency of our method with multiple examples of volume sculpting and image editing.",Regularized kelvinlets: sculpting brushes based on fundamental solutions of elasticity,NA:NA,2017
Karan Singh,NA,Session details: Learning to move,NA,2017
Xue Bin Peng:Glen Berseth:Kangkang Yin:Michiel Van De Panne,"Learning physics-based locomotion skills is a difficult problem, leading to solutions that typically exploit prior knowledge of various forms. In this paper we aim to learn a variety of environment-aware locomotion skills with a limited amount of prior knowledge. We adopt a two-level hierarchical control framework. First, low-level controllers are learned that operate at a fine timescale and which achieve robust walking gaits that satisfy stepping-target and style objectives. Second, high-level controllers are then learned which plan at the timescale of steps by invoking desired step targets for the low-level controller. The high-level controller makes decisions directly based on high-dimensional inputs, including terrain maps or other suitable representations of the surroundings. Both levels of the control policy are trained using deep reinforcement learning. Results are demonstrated on a simulated 3D biped. Low-level controllers are learned for a variety of motion styles and demonstrate robustness with respect to force-based disturbances, terrain variations, and style interpolation. High-level controllers are demonstrated that are capable of following trails through terrains, dribbling a soccer ball towards a target location, and navigating through static or dynamic obstacles.",DeepLoco: dynamic locomotion skills using hierarchical deep reinforcement learning,NA:NA:NA:NA,2017
Daniel Holden:Taku Komura:Jun Saito,"We present a real-time character control mechanism using a novel neural network architecture called a Phase-Functioned Neural Network. In this network structure, the weights are computed via a cyclic function which uses the phase as an input. Along with the phase, our system takes as input user controls, the previous state of the character, the geometry of the scene, and automatically produces high quality motions that achieve the desired user control. The entire network is trained in an end-to-end fashion on a large dataset composed of locomotion such as walking, running, jumping, and climbing movements fitted into virtual environments. Our system can therefore automatically produce motions where the character adapts to different geometric environments such as walking and running over rough terrain, climbing over large rocks, jumping over obstacles, and crouching under low ceilings. Our network architecture produces higher quality results than time-series autoregressive models such as LSTMs as it deals explicitly with the latent variable of motion relating to the phase. Once trained, our system is also extremely fast and compact, requiring only milliseconds of execution time and a few megabytes of memory, even when trained on gigabytes of motion data. Our work is most appropriate for controlling characters in interactive scenes such as computer games and virtual reality systems.",Phase-functioned neural networks for character control,NA:NA:NA,2017
Libin Liu:Jessica Hodgins,"Given a robust control system, physical simulation offers the potential for interactive human characters that move in realistic and responsive ways. In this article, we describe how to learn a scheduling scheme that reorders short control fragments as necessary at runtime to create a control system that can respond to disturbances and allows steering and other user interactions. These schedulers provide robust control of a wide range of highly dynamic behaviors, including walking on a ball, balancing on a bongo board, skateboarding, running, push-recovery, and breakdancing. We show that moderate-sized Q-networks can model the schedulers for these control tasks effectively and that those schedulers can be efficiently learned by the deep Q-learning algorithm.",Learning to Schedule Control Fragments for Physics-Based Characters Using Deep Q-Learning,NA:NA,2017
Kourosh Naderi:Joose Rajamäki:Perttu Hämäläinen,"This paper addresses the problem of offline path and movement planning for wall climbing humanoid agents. We focus on simulating bouldering, i.e. climbing short routes with diverse moves, although we also demonstrate our system on a longer wall. Our approach combines a graph-based high-level path planner with low-level sampling-based optimization of climbing moves. Although the planning problem is complex, our system produces plausible solutions to bouldering problems (short climbing routes) in less than a minute. We further utilize a k-shortest paths approach, which enables the system to discover alternative paths - in climbing, alternative strategies often exist, and what might be optimal for one climber could be impossible for others due to individual differences in strength, flexibility, and reach. We envision our system could be used, e.g. in learning a climbing strategy, or as a test and evaluation tool for climbing route designers. To the best of our knowledge, this is the first paper to solve and simulate rich humanoid wall climbing, where more than one limb can move at the same time, and limbs can also hang free for balance or use wall friction in addition to predefined holds.",Discovering and synthesizing humanoid climbing movements,NA:NA:NA,2017
George Drettakis,NA,Session details: Get more out of your photo,NA,2017
Dushyant Mehta:Srinath Sridhar:Oleksandr Sotnychenko:Helge Rhodin:Mohammad Shafiei:Hans-Peter Seidel:Weipeng Xu:Dan Casas:Christian Theobalt,"We present the first real-time method to capture the full global 3D skeletal pose of a human in a stable, temporally consistent manner using a single RGB camera. Our method combines a new convolutional neural network (CNN) based pose regressor with kinematic skeleton fitting. Our novel fully-convolutional pose formulation regresses 2D and 3D joint positions jointly in real time and does not require tightly cropped input frames. A real-time kinematic skeleton fitting method uses the CNN output to yield temporally stable 3D global pose reconstructions on the basis of a coherent kinematic skeleton. This makes our approach the first monocular RGB method usable in real-time applications such as 3D character control---thus far, the only monocular methods for such applications employed specialized RGB-D cameras. Our method's accuracy is quantitatively on par with the best offline 3D monocular RGB pose estimation methods. Our results are qualitatively comparable to, and sometimes better than, results from monocular RGB-D approaches, such as the Kinect. However, we show that our approach is more broadly applicable than RGB-D solutions, i.e., it works for outdoor scenes, community videos, and low quality commodity RGB cameras.",VNect: real-time 3D human pose estimation with a single RGB camera,NA:NA:NA:NA:NA:NA:NA:NA:NA,2017
Kaiwen Guo:Feng Xu:Tao Yu:Xiaoyang Liu:Qionghai Dai:Yebin Liu,"This article proposes a real-time method that uses a single-view RGB-D input (a depth sensor integrated with a color camera) to simultaneously reconstruct a casual scene with a detailed geometry model, surface albedo, per-frame non-rigid motion, and per-frame low-frequency lighting, without requiring any template or motion priors. The key observation is that accurate scene motion can be used to integrate temporal information to recover the precise appearance, whereas the intrinsic appearance can help to establish true correspondence in the temporal domain to recover motion. Based on this observation, we first propose a shading-based scheme to leverage appearance information for motion estimation. Then, using the reconstructed motion, a volumetric albedo fusing scheme is proposed to complete and refine the intrinsic appearance of the scene by incorporating information from multiple frames. Since the two schemes are iteratively applied during recording, the reconstructed appearance and motion become increasingly more accurate. In addition to the reconstruction results, our experiments also show that additional applications can be achieved, such as relighting, albedo editing, and free-viewpoint rendering of a dynamic scene, since geometry, appearance, and motion are all reconstructed by our technique.","Real-Time Geometry, Albedo, and Motion Reconstruction Using a Single RGB-D Camera",NA:NA:NA:NA:NA:NA,2017
Xiao Li:Yue Dong:Pieter Peers:Xin Tong,"We present a convolutional neural network (CNN) based solution for modeling physically plausible spatially varying surface reflectance functions (SVBRDF) from a single photograph of a planar material sample under unknown natural illumination. Gathering a sufficiently large set of labeled training pairs consisting of photographs of SVBRDF samples and corresponding reflectance parameters, is a difficult and arduous process. To reduce the amount of required labeled training data, we propose to leverage the appearance information embedded in unlabeled images of spatially varying materials to self-augment the training process. Starting from an initial approximative network obtained from a small set of labeled training pairs, we estimate provisional model parameters for each unlabeled training exemplar. Given this provisional reflectance estimate, we then synthesize a novel temporary labeled training pair by rendering the exact corresponding image under a new lighting condition. After refining the network using these additional training samples, we re-estimate the provisional model parameters for the unlabeled data and repeat the self-augmentation process until convergence. We demonstrate the efficacy of the proposed network structure on spatially varying wood, metals, and plastics, as well as thoroughly validate the effectiveness of the self-augmentation training process.",Modeling surface appearance from a single photograph using self-augmented convolutional neural networks,NA:NA:NA:NA,2017
Michael Waechter:Mate Beljan:Simon Fuhrmann:Nils Moehrle:Johannes Kopf:Michael Goesele,"The ultimate goal of many image-based modeling systems is to render photo-realistic novel views of a scene without visible artifacts. Existing evaluation metrics and benchmarks focus mainly on the geometric accuracy of the reconstructed model, which is, however, a poor predictor of visual accuracy. Furthermore, using only geometric accuracy by itself does not allow evaluating systems that either lack a geometric scene representation or utilize coarse proxy geometry. Examples include a light field and most image-based rendering systems. We propose a unified evaluation approach based on novel view prediction error that is able to analyze the visual quality of any method that can render novel views from input images. One key advantage of this approach is that it does not require ground truth geometry. This dramatically simplifies the creation of test datasets and benchmarks. It also allows us to evaluate the quality of an unknown scene during the acquisition and reconstruction process, which is useful for acquisition planning. We evaluate our approach on a range of methods, including standard geometry-plus-texture pipelines as well as image-based rendering techniques, compare it to existing geometry-based benchmarks, demonstrate its utility for a range of use cases, and present a new virtual rephotography-based benchmark for image-based modeling and rendering systems.",Virtual Rephotography: Novel View Prediction Error for 3D Reconstruction,NA:NA:NA:NA:NA:NA,2017
Abhishek Badki:Orazio Gallo:Jan Kautz:Pradeep Sen,"Capturing a picture that ""tells a story"" requires the ability to create the right composition. The two most important parameters controlling composition are the camera position and the focal length of the lens. The traditional paradigm is for a photographer to mentally visualize the desired picture, select the capture parameters to produce it, and finally take the photograph, thus committing to a particular composition. We propose to change this paradigm. To do this, we introduce computational zoom, a framework that allows a photographer to manipulate several aspects of composition in post-processing from a stack of pictures captured at different distances from the scene. We further define a multi-perspective camera model that can generate compositions that are not physically attainable, thus extending the photographer's control over factors such as the relative size of objects at different depths and the sense of depth of the picture. We show several applications and results of the proposed computational zoom framework.",Computational zoom: a framework for post-capture image composition,NA:NA:NA:NA,2017
Eakta Jain,NA,Session details: People power,NA,2017
Ana Serrano:Vincent Sitzmann:Jaime Ruiz-Borau:Gordon Wetzstein:Diego Gutierrez:Belen Masia,"Traditional cinematography has relied for over a century on a well-established set of editing rules, called continuity editing, to create a sense of situational continuity. Despite massive changes in visual content across cuts, viewers in general experience no trouble perceiving the discontinuous flow of information as a coherent set of events. However, Virtual Reality (VR) movies are intrinsically different from traditional movies in that the viewer controls the camera orientation at all times. As a consequence, common editing techniques that rely on camera orientations, zooms, etc., cannot be used. In this paper we investigate key relevant questions to understand how well traditional movie editing carries over to VR, such as: Does the perception of continuity hold across edit boundaries? Under which conditions? Does viewers' observational behavior change after the cuts? To do so, we rely on recent cognition studies and the event segmentation theory, which states that our brains segment continuous actions into a series of discrete, meaningful events. We first replicate one of these studies to assess whether the predictions of such theory can be applied to VR. We next gather gaze data from viewers watching VR videos containing different edits with varying parameters, and provide the first systematic analysis of viewers' behavior and the perception of continuity in VR. From this analysis we make a series of relevant findings; for instance, our data suggests that predictions from the cognitive event segmentation theory are useful guides for VR editing; that different types of edits are equally well understood in terms of continuity; and that spatial misalignments between regions of interest at the edit boundaries favor a more exploratory behavior even after viewers have fixated on a new region of interest. In addition, we propose a number of metrics to describe viewers' attentional behavior in VR. We believe the insights derived from our work can be useful as guidelines for VR content creation.",Movie editing and cognitive event segmentation in virtual reality video,NA:NA:NA:NA:NA:NA,2017
Yuki Koyama:Issei Sato:Daisuke Sakamoto:Takeo Igarashi,"Parameter tweaking is a common task in various design scenarios. For example, in color enhancement of photographs, designers tweak multiple parameters such as ""brightness"" and ""contrast"" to obtain the best visual impression. Adjusting one parameter is easy; however, if there are multiple correlated parameters, the task becomes much more complex, requiring many trials and a large cognitive load. To address this problem, we present a novel extension of Bayesian optimization techniques, where the system decomposes the entire parameter tweaking task into a sequence of one-dimensional line search queries that are easy for human to perform by manipulating a single slider. In addition, we present a novel concept called crowd-powered visual design optimizer, which queries crowd workers, and provide a working implementation of this concept. Our single-slider manipulation microtask design for crowdsourcing accelerates the convergence of the optimization relative to existing comparison-based microtask designs. We applied our framework to two different design domains: photo color enhancement and material BRDF design, and thereby showed its applicability to various design domains.",Sequential line search for efficient visual design optimization by crowds,NA:NA:NA:NA,2017
Funda Durupinar:Mubbasir Kapadia:Susan Deutsch:Michael Neff:Norman I. Badler,"A major goal of research on virtual humans is the animation of expressive characters that display distinct psychological attributes. Body motion is an effective way of portraying different personalities and differentiating characters. The purpose and contribution of this work is to describe a formal, broadly applicable, procedural, and empirically grounded association between personality and body motion and apply this association to modify a given virtual human body animation that can be represented by these formal concepts. Because the body movement of virtual characters may involve different choices of parameter sets depending on the context, situation, or application, formulating a link from personality to body motion requires an intermediate step to assist generalization. For this intermediate step, we refer to Laban Movement Analysis, which is a movement analysis technique for systematically describing and evaluating human motion. We have developed an expressive human motion generation system with the help of movement experts and conducted a user study to explore how the psychologically validated OCEAN personality factors were perceived in motions with various Laban parameters. We have then applied our findings to procedurally animate expressive characters with personality, and validated the generalizability of our approach across different models and animations via another perception study.",PERFORM: Perceptual Approach for Adding OCEAN Personality to Human Motion Using Laban Movement Analysis,NA:NA:NA:NA:NA,2017
Harrison Jesse Smith:Michael Neff,"Applications such as virtual tutors, games, and natural interfaces increasingly require animated characters to take on social roles while interacting with humans. The effectiveness of these applications depends on our ability to control the social presence of characters, including their personality. Understanding how movement impacts the perception of personality allows us to generate characters more capable of fulfilling this social role. The two studies described herein focus on gesture as a key component of social communication and examine how a set of gesture edits, similar to the types of changes that occur during motion warping, impact the perceived personality of the character. Surprisingly, when based on thin-slice gesture data, people's judgments of character personality mainly fall in a 2D subspace rather than independently impacting the full set of traits in the standard Big Five model of personality. These two dimensions are plasticity, which includes extraversion and openness, and stability, which includes emotional stability, agreeableness, and conscientiousness. A set of motion properties is experimentally determined that impacts each of these two traits. We show that when these properties are systematically edited in new gesture sequences, we can independently influence the character's perceived stability and plasticity (and the corresponding Big Five traits), to generate distinctive personalities. We identify motion adjustments salient to each judgment and, in a series of perceptual studies, repeatedly generate four distinctly perceived personalities. The effects extend to novel gesture sequences and character meshes, and even largely persist in the presence of accompanying speech. This paper furthers our understanding of how gesture can be used to control the perception of personality and suggests both the potential and possible limits of motion editing approaches.",Understanding the impact of animated gesture performance on personality perceptions,NA:NA,2017
Elena Arabadzhiyska:Okan Tarhan Tursun:Karol Myszkowski:Hans-Peter Seidel:Piotr Didyk,"Gaze-contingent rendering shows promise in improving perceived quality by providing a better match between image quality and the human visual system requirements. For example, information about fixation allows rendering quality to be reduced in peripheral vision, and the additional resources can be used to improve the quality in the foveal region. Gaze-contingent rendering can also be used to compensate for certain limitations of display devices, such as reduced dynamic range or lack of accommodation cues. Despite this potential and the recent drop in the prices of eye trackers, the adoption of such solutions is hampered by system latency which leads to a mismatch between image quality and the actual gaze location. This is especially apparent during fast saccadic movements when the information about gaze location is significantly delayed, and the quality mismatch can be noticed. To address this problem, we suggest a new way of updating images in gaze-contingent rendering during saccades. Instead of rendering according to the current gaze position, our technique predicts where the saccade is likely to end and provides an image for the new fixation location as soon as the prediction is available. While the quality mismatch during the saccade remains unnoticed due to saccadic suppression, a correct image for the new fixation is provided before the fixation is established. This paper describes the derivation of a model for predicting saccade landing positions and demonstrates how it can be used in the context of gaze-contingent rendering to reduce the influence of system latency on the perceived quality. The technique is validated in a series of experiments for various combinations of display frame rate and eye-tracker sampling rate.",Saccade landing position prediction for gaze-contingent rendering,NA:NA:NA:NA:NA,2017
Niloy Mitra,NA,Session details: Comparing 3D shapes and parts,NA,2017
Ruizhen Hu:Wenchao Li:Oliver Van Kaick:Hui Huang:Melinos Averkiou:Daniel Cohen-Or:Hao Zhang,"We introduce a method for co-locating style-defining elements over a set of 3D shapes. Our goal is to translate high-level style descriptions, such as “Ming” or “European” for furniture models, into explicit and localized regions over the geometric models that characterize each style. For each style, the set of style-defining elements is defined as the union of all the elements that are able to discriminate the style. Another property of the style-defining elements is that they are frequently occurring, reflecting shape characteristics that appear across multiple shapes of the same style. Given an input set of 3D shapes spanning multiple categories and styles, where the shapes are grouped according to their style labels, we perform a cross-category co-analysis of the shape set to learn and spatially locate a set of defining elements for each style. This is accomplished by first sampling a large number of candidate geometric elements and then iteratively applying feature selection to the candidates, to extract style-discriminating elements until no additional elements can be found. Thus, for each style label, we obtain sets of discriminative elements that together form the superset of defining elements for the style. We demonstrate that the co-location of style-defining elements allows us to solve problems such as style classification, and enables a variety of applications such as style-revealing view selection, style-aware sampling, and style-driven modeling for 3D shapes.",Co-Locating Style-Defining Elements on 3D Shapes,NA:NA:NA:NA:NA:NA:NA,2017
Chenyang Zhu:Renjiao Yi:Wallace Lira:Ibraheem Alhashim:Kai Xu:Hao Zhang,"Many approaches to shape comparison and recognition start by establishing a shape correspondence. We ""turn the table"" and show that quality shape correspondences can be obtained by performing many shape recognition tasks. What is more, the method we develop computes a fine-grained, topology-varying part correspondence between two 3D shapes where the core evaluation mechanism only recognizes shapes globally. This is made possible by casting the part correspondence problem in a deformation-driven framework and relying on a data-driven ""deformation energy"" which rates visual similarity between deformed shapes and models from a shape repository. Our basic premise is that if a correspondence between two chairs (or airplanes, bicycles, etc.) is correct, then a reasonable deformation between the two chairs anchored on the correspondence ought to produce plausible, ""chair-like"" in-between shapes. Given two 3D shapes belonging to the same category, we perform a top-down, hierarchical search for part correspondences. For a candidate correspondence at each level of the search hierarchy, we deform one input shape into the other, while respecting the correspondence, and rate the correspondence based on how well the resulting deformed shapes resemble other shapes from ShapeNet belonging to the same category as the inputs. The resemblance, i.e., plausibility, is measured by comparing multi-view depth images over category-specific features learned for the various shape categories. We demonstrate clear improvements over state-of-the-art approaches through tests covering extensive sets of man-made models with rich geometric and topological variations.",Deformation-driven shape correspondence via shape recognition,NA:NA:NA:NA:NA:NA,2017
Jun Li:Kai Xu:Siddhartha Chaudhuri:Ersin Yumer:Hao Zhang:Leonidas Guibas,"We introduce a novel neural network architecture for encoding and synthesis of 3D shapes, particularly their structures. Our key insight is that 3D shapes are effectively characterized by their hierarchical organization of parts, which reflects fundamental intra-shape relationships such as adjacency and symmetry. We develop a recursive neural net (RvNN) based autoencoder to map a flat, unlabeled, arbitrary part layout to a compact code. The code effectively captures hierarchical structures of man-made 3D objects of varying structural complexities despite being fixed-dimensional: an associated decoder maps a code back to a full hierarchy. The learned bidirectional mapping is further tuned using an adversarial setup to yield a generative model of plausible structures, from which novel structures can be sampled. Finally, our structure synthesis framework is augmented by a second trained module that produces fine-grained part geometry, conditioned on global and local structural context, leading to a full generative pipeline for 3D shapes. We demonstrate that without supervision, our network learns meaningful structural hierarchies adhering to perceptual grouping principles, produces compact codes which enable applications such as shape classification and partial matching, and supports shape synthesis and interpolation with significant variations in topology and geometry.",GRASS: generative recursive autoencoders for shape structures,NA:NA:NA:NA:NA:NA,2017
Adriana Schulz:Ariel Shamir:Ilya Baran:David I. W. Levin:Pitchaya Sitthi-Amorn:Wojciech Matusik,"While collections of parametric shapes are growing in size and use, little progress has been made on the fundamental problem of shape-based matching and retrieval for parametric shapes in a collection. The search space for such collections is both discrete (number of shapes) and continuous (parameter values). In this work, we propose representing this space using descriptors that have shown to be effective for single shape retrieval. While single shapes can be represented as points in a descriptor space, parametric shapes are mapped into larger continuous regions. For smooth descriptors, we can assume that these regions are bounded low-dimensional manifolds where the dimensionality is given by the number of shape parameters. We propose representing these manifolds with a set of primitives, namely, points and bounded tangent spaces. Our algorithm describes how to define these primitives and how to use them to construct a manifold approximation that allows accurate and fast retrieval. We perform an analysis based on curvature, boundary evaluation, and the allowed approximation error to select between primitive types. We show how to compute decision variables with no need for empirical parameter adjustments and discuss theoretical guarantees on retrieval accuracy. We validate our approach with experiments that use different types of descriptors on a collection of shapes from multiple categories.",Retrieval on Parametric Shape Collections,NA:NA:NA:NA:NA:NA,2017
Sören Pirk:Vojtech Krs:Kaimo Hu:Suren Deepak Rajasekaran:Hao Kang:Yusuke Yoshiyasu:Bedrich Benes:Leonidas J. Guibas,"Interactions play a key role in understanding objects and scenes for both virtual and real-world agents. We introduce a new general representation for proximal interactions among physical objects that is agnostic to the type of objects or interaction involved. The representation is based on tracking particles on one of the participating objects and then observing them with sensors appropriately placed in the interaction volume or on the interaction surfaces. We show how to factorize these interaction descriptors and project them into a particular participating object so as to obtain a new functional descriptor for that object, its interaction landscape, capturing its observed use in a spatiotemporal framework. Interaction landscapes are independent of the particular interaction and capture subtle dynamic effects in how objects move and behave when in functional use. Our method relates objects based on their function, establishes correspondences between shapes based on functional key points and regions, and retrieves peer and partner objects with respect to an interaction.",Understanding and Exploiting Object Interaction Landscapes,NA:NA:NA:NA:NA:NA:NA:NA,2017
Adam Bargteil,NA,Session details: Clever solids,NA,2017
Hongyi Xu:Jernej Barbič,"To date, material modeling in physically based computer animation has largely focused on mass and stiffness material properties. However, deformation dynamics is largely affected also by the damping properties. In this paper, we propose an interactive design method for nonlinear isotropic and anisotropic damping of complex three-dimensional solids simulated using the Finite Element Method (FEM). We first give a damping design method and interface whereby the user can set the damping properties so that motion aligned with each of a few chosen example deformations is damped by an independently prescribed amount, whereas the rest of the deformation space follows standard Rayleigh damping, or any viscous damping. Next, we demonstrate how to design nonlinear damping that depends on the magnitude of the deformation along each example deformation, by editing a single spline curve for each example deformation. Our user interface enables an art-directed and intuitive approach to controlling damping in solid simulations. We mathematically prove that our nonlinear anisotropic damping generalizes the frequency-dependent Caughey damping model, when starting from the Rayleigh damping. Finally, we give an inverse design method whereby the damping curve parameters can be inferred automatically from high-level user input, such as the amount of amplitude loss in one oscillation cycle along each of the chosen example deformations. To minimize numerical damping for implicit integration, we introduce an accurate and stable implicit integrator, which removes spurious high-frequency oscillations while only introducing a minimal amount of numerical damping. Our damping can generate effects not possible with previous methods, such as controllable nonlinear decaying envelopes whereby large deformations are damped faster or slower than small deformations, and damping anisotropic effects. We also fit our damping to videos of real-world objects undergoing large deformations, capturing their nonlinear and anisotropic damping dynamics.",Example-based damping design,NA:NA,2017
Meekyoung Kim:Gerard Pons-Moll:Sergi Pujades:Seungbae Bang:Jinwook Kim:Michael J. Black:Sung-Hee Lee,"Data driven models of human poses and soft-tissue deformations can produce very realistic results, but they only model the visible surface of the human body and cannot create skin deformation due to interactions with the environment. Physical simulations can generalize to external forces, but their parameters are difficult to control. In this paper, we present a layered volumetric human body model learned from data. Our model is composed of a data-driven inner layer and a physics-based external layer. The inner layer is driven with a volumetric statistical body model (VSMPL). The soft tissue layer consists of a tetrahedral mesh that is driven using the finite element method (FEM). Model parameters, namely the segmentation of the body into layers and the soft tissue elasticity, are learned directly from 4D registrations of humans exhibiting soft tissue deformations. The learned two layer model is a realistic full-body avatar that generalizes to novel motions and external forces. Experiments show that the resulting avatars produce realistic results on held out sequences and react to external forces. Moreover, the model supports the retargeting of physical properties from one avatar when they share the same topology.",Data-driven physics for human soft tissue animation,NA:NA:NA:NA:NA:NA:NA,2017
Dan Koschier:Jan Bender:Nils Thuerey,"In this paper we present a robust remeshing-free cutting algorithm on the basis of the eXtended Finite Element Method (XFEM) and fully implicit time integration. One of the most crucial points of the XFEM is that integrals over discontinuous polynomials have to be computed on subdomains of the polyhedral elements. Most existing approaches construct a cut-aligned auxiliary mesh for integration. In contrast, we propose a cutting algorithm that includes the construction of specialized quadrature rules for each dissected element without the requirement to explicitly represent the arising subdomains. Moreover, we solve the problem of ill-conditioned or even numerically singular solver matrices during time integration using a novel algorithm that constrains non-contributing degrees of freedom (DOFs) and introduce a preconditioner that efficiently reuses the constructed quadrature weights. Our method is particularly suitable for fine structural cutting as it decouples the added number of DOFs from the cut's geometry and correctly preserves geometry and physical properties by accurate integration. Due to the implicit time integration these fine features can still be simulated robustly using large time steps. As opposed to this, the vast majority of existing approaches either use remeshing or element duplication. Remeshing based methods are able to correctly preserve physical quantities but strongly couple cut geometry and mesh resolution leading to an unnecessary large number of additional DOFs. Element duplication based approaches keep the number of additional DOFs small but fail at correct conservation of mass and stiffness properties. We verify consistency and robustness of our approach on simple and reproducible academic examples while stability and applicability are demonstrated in large scenarios with complex and fine structural cutting.",Robust eXtended finite elements for complex cutting of deformables,NA:NA:NA,2017
Yun (Raymond) Fei:Henrique Teles Maia:Christopher Batty:Changxi Zheng:Eitan Grinspun,"The diverse interactions between hair and liquid are complex and span multiple length scales, yet are central to the appearance of humans and animals in many situations. We therefore propose a novel multi-component simulation framework that treats many of the key physical mechanisms governing the dynamics of wet hair. The foundations of our approach are a discrete rod model for hair and a particle-in-cell model for fluids. To treat the thin layer of liquid that clings to the hair, we augment each hair strand with a height field representation. Our contribution is to develop the necessary physical and numerical models to evolve this new system and the interactions among its components. We develop a new reduced-dimensional liquid model to solve the motion of the liquid along the length of each hair, while accounting for its moving reference frame and influence on the hair dynamics. We derive a faithful model for surface tension-induced cohesion effects between adjacent hairs, based on the geometry of the liquid bridges that connect them. We adopt an empirically-validated drag model to treat the effects of coarse-scale interactions between hair and surrounding fluid, and propose new volume-conserving dripping and absorption strategies to transfer liquid between the reduced and particle-in-cell liquid representations. The synthesis of these techniques yields an effective wet hair simulator, which we use to animate hair flipping, an animal shaking itself dry, a spinning car wash roller brush dunked in liquid, and intricate hair coalescence effects, among several additional scenarios.",A multi-scale model for simulating liquid-hair interactions,NA:NA:NA:NA:NA,2017
Alec Jacobson,NA,Session details: Being discrete about geometry processing,NA,2017
Stéphane Calderon:Tamy Boubekeur,"Many computer graphics applications use simpler yet faithful approximations of complex shapes to conduct reliably part of their computations. Some tasks, such as physical simulation, collision detection, occlusion queries or free-form deformation, require the simpler proxy to strictly enclose the input shape. While there are algorithms that can output such bounding proxies on simple input shapes, most of them fail at generating a proper coarse approximant on real-world complex shapes, which may contain multiple components and have a high genus. We advocate that, before reducing the number of primitives to describe a shape, one needs to regularize it while maintaining the strict enclosing property, to avoid any geometric aliasing that makes the decimation unreliable. Depending on the scale of the desired approximation, the topology of the shape itself may indeed have to be first simplified, to let the subsequent geometric optimization be free from topological locks. We propose a new bounding shape approximation algorithm which takes as input an arbitrary surface mesh, with potentially complex multi-component structures, and generates automatically a bounding proxy which is tightened on the input and can match even the coarsest levels of approximation. To sustain the nonlinear approximation process that may eventually abstract both geometry and topology, we propose to use an intermediate regularized representation in the form of a shape closing, computed in real time using a new fast morphological framework designed for efficient parallel execution. Once the desired level of approximation is reached in the shape closing, a coarse, tight and bounding polygonization of the proxy geometry is extracted using an adaptive meshing scheme. Our underlying representation is both geometry- and topology-adaptive and can be optionally controlled accurately by a user, through sizing and orientation fields, yielding an intuitive brush metaphor within an interactive proxy design environment. We provide extensive experiments on various kinds of input meshes and illustrate the potential applications of our method in scenarios that benefit greatly from coarse, tight bounding substitutes to the actual high resolution geometry of the original 3D model, including freeform deformation, physical simulation and level of detail generation for rendering.",Bounding proxies for shape approximation,NA:NA,2017
Fabián Prada:Misha Kazhdan:Ming Chuang:Alvaro Collet:Hugues Hoppe,"We convert a sequence of unstructured textured meshes into a mesh with incrementally changing connectivity and atlas parameterization. Like prior work on surface tracking, we seek temporally coherent mesh connectivity to enable efficient representation of surface geometry and texture. Like recent work on evolving meshes, we pursue local remeshing to permit tracking over long sequences containing significant deformations or topological changes. Our main contribution is to show that both goals are realizable within a common framework that simultaneously evolves both the set of mesh triangles and the parametric map. Sparsifying the remeshing operations allows the formation of large spatiotemporal texture charts. These charts are packed as prisms into a 3D atlas for a texture video. Reducing tracking drift using mesh-based optical flow helps improve compression of the resulting video stream.",Spatiotemporal atlas parameterization for evolving meshes,NA:NA:NA:NA:NA,2017
Giorgio Gori:Alla Sheffer:Nicholas Vining:Enrique Rosales:Nathan Carr:Tao Ju,"We present FlowRep, an algorithm for extracting descriptive compact 3D curve networks from meshes of free-form man-made shapes. We infer the desired compact curve network from complex 3D geometries by using a series of insights derived from perception, computer graphics, and design literature. These sources suggest that visually descriptive networks are cycle-descriptive, i.e their cycles unambiguously describe the geometry of the surface patches they surround. They also indicate that such networks are designed to be projectable, or easy to envision when observed from a static general viewpoint; in other words, 2D projections of the network should be strongly indicative of its 3D geometry. Research suggests that both properties are best achieved by using networks dominated by flowlines, surface curves aligned with principal curvature directions across anisotropic regions and strategically extended across sharp-features and isotropic areas. Our algorithm leverages these observation in the construction of a compact descriptive curve network. Starting with a curvature aligned quad dominant mesh we first extract sequences of mesh edges that form long, well-shaped and reliable flowlines by leveraging directional similarity between nearby meaningful flowline directions We then use a compact subset of the extracted flowlines and the model's sharp-feature, or trim, curves to form a sparse, projectable network which describes the underlying surface. We validate our method by demonstrating a range of networks computed from diverse inputs, using them for surface reconstruction, and showing extensive comparisons with prior work and artist generated networks.",FlowRep: descriptive curve networks for free-form design shapes,NA:NA:NA:NA:NA:NA,2017
Etienne Corman:Justin Solomon:Mirela Ben-Chen:Leonidas Guibas:Maks Ovsjanikov,"We propose a novel way to capture and characterize distortion between pairs of shapes by extending the recently proposed framework of shape differences built on functional maps. We modify the original definition of shape differences slightly and prove that after this change, the discrete metric is fully encoded in two shape difference operators and can be recovered by solving two linear systems of equations. Then we introduce an extension of the shape difference operators using offset surfaces to capture extrinsic or embedding-dependent distortion, complementing the purely intrinsic nature of the original shape differences. Finally, we demonstrate that a set of four operators is complete, capturing intrinsic and extrinsic structure and fully encoding a shape up to rigid motion in both discrete and continuous settings. We highlight the usefulness of our constructions by showing the complementary nature of our extrinsic shape differences in capturing distortion ignored by previous approaches. We additionally provide examples where we recover local shape structure from the shape difference operators, suggesting shape editing and analysis tools based on manipulating shape differences.",Functional Characterization of Intrinsic and Extrinsic Geometry,NA:NA:NA:NA:NA,2017
Ariel Shamir,NA,Session details: Color & compositing,NA,2017
Nicolas Mellado:David Vanderhaeghe:Charlotte Hoarau:Sidonie Christophe:Mathieu Brédif:Loic Barthe,"Color palettes are widely used by artists to define colors of artworks and explore color designs. In general, artists select the colors of a palette by following a set of rules, e.g. contrast or relative luminance. Existing interactive palette exploration tools explore palette spaces following limited constraints defined as geometric configurations in color space e.g. harmony rules on the color wheel. Palette search algorithms sample palettes from color relations learned from an input dataset, however they cannot provide interactive user edits and palette refinement. We introduce in this work a new versatile formulation enabling the creation of constraint-based interactive palette exploration systems. Our technical contribution is a graph-based palette representation, from which we define palette exploration as a minimization problem that can be solved efficiently and provide real-time feedback. Based on our formulation, we introduce two interactive palette exploration strategies: constrained palette exploration, and for the first time, constrained palette interpolation. We demonstrate the performances of our approach on various application cases and evaluate how it helps users finding trade-offs between concurrent constraints.",Constrained palette-space exploration,NA:NA:NA:NA:NA:NA,2017
Maria Shugrina:Jingwan Lu:Stephen Diverdi,"We present Playful Palette, a color picker interface for digital paint programs that derives intuition from oil paint and watercolor palettes, but extends them with digital features. A Playful Palette is a set of blobs of color that blend together to create gradients and gamuts. They can be directly manipulated to explore arrangements and harmonies. All edits are non-destructive, and an infinite history allows previous palettes to be revisited and modified, recoloring the painting. The Playful Palette design is motivated by a pilot study of how artists use paint palettes, and we evaluate the final design with a set of traditional and digital media painters to demonstrate that Playful Palette is effective both at enabling artists' color tasks, and at amplifying their creativity.",Playful palette: an interactive parametric color mixer for artists,NA:NA:NA,2017
Jianchao Tan:Jyh-Ming Lien:Yotam Gingold,"In digital image editing software, layers organize images. However, layers are often not explicitly represented in the final image, and may never have existed for a scanned physical painting or a photograph. We propose a technique to decompose an image into layers. In our decomposition, each layer represents a single-color coat of paint applied with varying opacity. Our decomposition is based on the image’s RGB-space geometry. In RGB-space, the linear nature of the standard Porter-Duff [1984] “over” pixel compositing operation implies a geometric structure. The vertices of the convex hull of image pixels in RGB-space correspond to a palette of paint colors. These colors may be “hidden” and inaccessible to algorithms based on clustering visible colors. For our layer decomposition, users choose the palette size (degree of simplification to perform on the convex hull), as well as a layer order for the paint colors (vertices). We then solve a constrained optimization problem to find translucent, spatially coherent opacity for each layer, such that the composition of the layers reproduces the original image. We demonstrate the utility of the resulting decompositions for recoloring (global and local) and object insertion. Our layers can be interpreted as generalized barycentric coordinates; we compare to these and other recoloring approaches.",Decomposing Images into Layers via RGB-Space Geometry,NA:NA:NA,2017
Yağiz Aksoy:Tunç Ozan Aydin:Marc Pollefeys:Aljoša Smolić,"Due to the widespread use of compositing in contemporary feature films, green-screen keying has become an essential part of postproduction workflows. To comply with the ever-increasing quality requirements of the industry, specialized compositing artists spend countless hours using multiple commercial software tools, while eventually having to resort to manual painting because of the many shortcomings of these tools. Due to the sheer amount of manual labor involved in the process, new green-screen keying approaches that produce better keying results with less user interaction are welcome additions to the compositing artist’s arsenal. We found that—contrary to the common belief in the research community—production-quality green-screen keying is still an unresolved problem with its unique challenges. In this article, we propose a novel green-screen keying method utilizing a new energy minimization-based color unmixing algorithm. We present comprehensive comparisons with commercial software packages and relevant methods in literature, which show that the quality of our results is superior to any other currently available green-screen keying solution. It is important to note that, using the proposed method, these high-quality results can be generated using only one-tenth of the manual editing time that a professional compositing artist requires to process the same content having all previous state-of-the-art tools at one’s disposal.",Interactive High-Quality Green-Screen Keying via Color Unmixing,NA:NA:NA:NA,2017
Yağiz Aksoy:Tunç Ozan Aydin:Aljoša Smolić:Marc Pollefeys,"We present a new method for decomposing an image into a set of soft color segments that are analogous to color layers with alpha channels that have been commonly utilized in modern image manipulation software. We show that the resulting decomposition serves as an effective intermediate image representation, which can be utilized for performing various, seemingly unrelated, image manipulation tasks. We identify a set of requirements that soft color segmentation methods have to fulfill, and present an in-depth theoretical analysis of prior work. We propose an energy formulation for producing compact layers of homogeneous colors and a color refinement procedure, as well as a method for automatically estimating a statistical color model from an image. This results in a novel framework for automatic and high-quality soft color segmentation that is efficient, parallelizable, and scalable. We show that our technique is superior in quality compared to previous methods through quantitative analysis as well as visually through an extensive set of examples. We demonstrate that our soft color segments can easily be exported to familiar image manipulation software packages and used to produce compelling results for numerous image manipulation applications without forcing the user to learn new tools and workflows.",Unmixing-Based Soft Color Segmentation for Image Manipulation,NA:NA:NA:NA,2017
Dave Levin,NA,"Session details: Fabricating curves, surfaces, & volumes",NA,2017
Jesús Pérez:Miguel A. Otaduy:Bernhard Thomaszewski,"We propose a computational tool for designing Kirchhoff-Plateau Surfaces---planar rod networks embedded in pre-stretched fabric that deploy into complex, three-dimensional shapes. While Kirchhoff-Plateau Surfaces offer an intriguing and expressive design space, navigating this space is made difficult by the highly nonlinear nature of the underlying mechanical problem. In order to tackle this challenge, we propose a user-guided but computer-assisted approach that combines an efficient forward simulation model with a dedicated optimization algorithm in order to implement a powerful set of design tools. We demonstrate our method by designing a diverse set of complex-shaped Kirchhoff-Plateau Surfaces, each validated through physically-fabricated prototypes.",Computational design and automated fabrication of kirchhoff-plateau surfaces,NA:NA:NA,2017
Lingjie Liu:Duygu Ceylan:Cheng Lin:Wenping Wang:Niloy J. Mitra,"Objects created by connecting and bending wires are common in furniture design, metal sculpting, wire jewelry, etc. Reconstructing such objects with traditional depth and image based methods is extremely difficult due to their unique characteristics such as lack of features, thin elements, and severe self-occlusions. We present a novel image-based method that reconstructs a set of continuous 3D wires used to create such an object, where each wire is composed of an ordered set of 3D curve segments. Our method exploits two main observations: simplicity - wire objects are often created using only a small number of wires, and smoothness - each wire is primarily smoothly bent with sharp features appearing only at joints or isolated points. In light of these observations, we tackle the challenging image correspondence problem across featureless wires by first generating multiple candidate 3D curve segments and then solving a global selection problem that balances between image and smoothness cues to identify the correct 3D curves. Next, we recover a decomposition of such curves into a set of distinct and continuous wires by formulating a multiple traveling salesman problem, which finds smooth paths, i.e., wires, connecting the curves. We demonstrate our method on a wide set of real examples with varying complexity and present high-fidelity results using only 3 images for each object. We provide the source code and data for our work in the project website.",Image-based reconstruction of wire art,NA:NA:NA:NA:NA,2017
Ruslan Guseinov:Eder Miguel:Bernd Bickel,"We present a computational approach for designing CurveUps, curvy shells that form from an initially flat state. They consist of small rigid tiles that are tightly held together by two pre-stretched elastic sheets attached to them. Our method allows the realization of smooth, doubly curved surfaces that can be fabricated as a flat piece. Once released, the restoring forces of the pre-stretched sheets support the object to take shape in 3D. CurveUps are structurally stable in their target configuration. The design process starts with a target surface. Our method generates a tile layout in 2D and optimizes the distribution, shape, and attachment areas of the tiles to obtain a configuration that is fabricable and in which the curved up state closely matches the target. Our approach is based on an efficient approximate model and a local optimization strategy for an otherwise intractable nonlinear optimization problem. We demonstrate the effectiveness of our approach for a wide range of shapes, all realized as physical prototypes.",CurveUps: shaping objects from flat plates with tension-actuated curvature,NA:NA:NA,2017
Martin Kilian:Aron Monszpart:Niloy J. Mitra,"Curved folded surfaces, given their ability to produce elegant freeform shapes by folding flat sheets etched with curved creases, hold a special place in computational Origami. Artists and designers have proposed a wide variety of different fold patterns to create a range of interesting surfaces. The creative process, design, as well as fabrication is usually only concerned with the static surface that emerges once folding has completed. Folding such patterns, however, is difficult as multiple creases have to be folded simultaneously to obtain a properly folded target shape. We introduce string actuated curved folded surfaces that can be shaped by pulling a network of strings, thus, vastly simplifying the process of creating such surfaces and making the folding motion an integral part of the design. Technically, we solve the problem of which surface points to string together and how to actuate them by locally expressing a desired folding path in the space of isometric shape deformations in terms of novel string actuation modes. We demonstrate the validity of our approach by computing string actuation networks for a range of well-known crease patterns and testing their effectiveness on physical prototypes. All the examples in this article can be downloaded for personal use from http://geometry.cs.ucl.ac.uk/projects/2017/string-actuated/.",String Actuated Curved Folded Surfaces,NA:NA:NA,2017
Marc Alexa:Kristian Hildebrand:Sylvain Lefebvre,"Slicing is the procedure necessary to prepare a shape for layered manufacturing. There are degrees of freedom in this process, such as the starting point of the slicing sequence and the thickness of each slice. The choice of these parameters influences the manufacturing process and its result: The number of slices significantly affects the time needed for manufacturing, while their thickness affects the error. Assuming a discrete setting, we measure the error as the number of voxels that are incorrectly assigned due to slicing. We provide an algorithm that generates, for a given set of available slice heights and a shape, a slicing that is provably optimal. By optimal, we mean that the algorithm generates sequences with minimal error for any possible number of slices. The algorithm is fast and flexible, that is, it can accommodate a user driven importance modulation of the error function and allows the interactive exploration of the desired quality/time tradeoff. We demonstrate the practical importance of our optimization on several three-dimensional-printed results.",Optimal Discrete Slicing,NA:NA:NA,2017
Matthias Hullin,NA,Session details: Reflectance & scattering,NA,2017
Antoine Toisoul:Abhijeet Ghosh,"We propose two novel contributions for measurement-based rendering of diffraction effects in surface reflectance of planar homogeneous diffractive materials. As a general solution for commonly manufactured materials, we propose a practical data-driven rendering technique and a measurement approach to efficiently render complex diffraction effects in real time. Our measurement step simply involves photographing a planar diffractive sample illuminated with an LED flash. Here, we directly record the resultant diffraction pattern on the sample surface due to a narrow-band point source illumination. Furthermore, we propose an efficient rendering method that exploits the measurement in conjunction with the Huygens-Fresnel principle to fit relevant diffraction parameters based on a first-order approximation. Our proposed data-driven rendering method requires the precomputation of a single diffraction look-up table for accurate spectral rendering of complex diffraction effects. Second, for sharp specular samples, we propose a novel method for practical measurement of the underlying diffraction grating using out-of-focus “bokeh” photography of the specular highlight. We demonstrate how the measured bokeh can be employed as a height field to drive a diffraction shader based on a first-order approximation for efficient real-time rendering. Finally, we also drive analytic solutions for a few special cases of diffraction from our measurements and demonstrate realistic rendering results under complex light sources and environments.",Practical Acquisition and Rendering of Diffraction Effects in Surface Reflectance,NA:NA,2017
Laurent Belcour:Pascal Barla,"In this work, we introduce an extension to microfacet theory for the rendering of iridescent effects caused by thin-films of varying thickness (such as oil, grease, alcohols, etc) on top of an arbitrarily rough base layer. Our material model is the first to produce a consistent appearance between tristimulus (e.g., RGB) and spectral rendering engines by analytically pre-integrating its spectral response. The proposed extension works with any microfacet-based model: not only on reflection over dielectrics or conductors, but also on transmission through dielectrics. We adapt its evaluation to work in multi-scale rendering contexts, and we expose parameters enabling artistic control over iridescent appearance. The overhead compared to using the classic Fresnel reflectance or transmittance terms remains reasonable enough for practical uses in production.",A practical extension to microfacet theory for the modeling of varying iridescence,NA:NA,2017
Nicolas Holzschuch:Romain Pacanowski,"Adequate reflectance models are essential for the production of photorealistic images. Microfacet reflectance models predict the appearance of a material at the macroscopic level based on microscopic surface details. They provide a good match with measured reflectance in some cases, but not always. This discrepancy between the behavior predicted by microfacet models and the observed behavior has puzzled researchers for a long time. In this paper, we show that diffraction effects in the micro-geometry provide a plausible explanation. We describe a two-scale reflectance model, separating between geometry details much larger than wavelength and those of size comparable to wavelength. The former model results in the standard Cook-Torrance model. The latter model is responsible for diffraction effects. Diffraction effects at the smaller scale are convolved by the micro-geometry normal distribution. The resulting two-scale model provides a very good approximation to measured reflectances.",A two-scale microfacet reflectance model combining reflection and diffraction,NA:NA,2017
Ling-Qi Yan:Henrik Wann Jensen:Ravi Ramamoorthi,"Physically-based fur rendering is difficult. Recently, structural differences between hair and fur fibers have been revealed by Yan et al. (2015), who showed that fur fibers have an inner scattering medulla, and developed a double cylinder model. However, fur rendering is still complicated due to the complex scattering paths through the medulla. We develop a number of optimizations that improve efficiency and generality without compromising accuracy, leading to a practical fur reflectance model. We also propose a key contribution to support both near and far-field rendering, and allow smooth transitions between them. Specifically, we derive a compact BCSDF model for fur reflectance with only 5 lobes. Our model unifies hair and fur rendering, making it easy to implement within standard hair rendering software, since we keep the traditional R, TT, and TRT lobes in hair, and only add two extensions to scattered lobes, TTs and TRTs. Moreover, we introduce a compression scheme using tensor decomposition to dramatically reduce the precomputed data storage for scattered lobes to only 150 KB, with minimal loss of accuracy. By exploiting piecewise analytic integration, our method further enables a multi-scale rendering scheme that transitions between near and far field rendering smoothly and efficiently for the first time, leading to 6 -- 8× speed up over previous work.",An efficient and practical near and far field fur reflectance model,NA:NA:NA,2017
Chris Wojtan,NA,Session details: Fluid control & synthesis,NA,2017
Alexey Stomakhin:Andrew Selle,"We present a novel approach to guiding physically based particle simulations using boundary conditions. Unlike commonly used ad hoc particle techniques for adding and removing the material from a simulation, our approach is principled by utilizing the concept of volumetric flux. Artists are provided with a simple yet powerful primitive called a fluxed animated boundary (FAB), allowing them to specify a control shape and a material flow field. The system takes care of enforcing the corresponding boundary conditions and necessary particle reseeding. We show how FABs can be used artistically or physically. Finally, we demonstrate production examples that show the efficacy of our method.",Fluxed animated boundary method,NA:NA,2017
Zherong Pan:Dinesh Manocha,"We present a novel algorithm to control the physically-based animation of smoke. Given a set of keyframe smoke shapes, we compute a dense sequence of control force fields that can drive the smoke shape to match several keyframes at certain time instances. Our approach formulates this control problem as a spacetime optimization constrained by partial differential equations. In order to compute the locally optimal control forces, we alternatively optimize the velocity fields and density fields using an alternating direction method of multiplier (ADMM) optimizer. In order to reduce the high complexity of multiple passes of fluid resimulation during velocity field optimization, we utilize the coherence between consecutive fluid simulation passes. We demonstrate the benefits of our approach by computing accurate solutions on 2D and 3D benchmarks. In practice, we observe up to an order of magnitude improvement over prior optimal control methods.",Efficient Solver for Spacetime Control of Smoke,NA:NA,2017
Mengyu Chu:Nils Thuerey,"We present a novel data-driven algorithm to synthesize high resolution flow simulations with reusable repositories of space-time flow data. In our work, we employ a descriptor learning approach to encode the similarity between fluid regions with differences in resolution and numerical viscosity. We use convolutional neural networks to generate the descriptors from fluid data such as smoke density and flow velocity. At the same time, we present a deformation limiting patch advection method which allows us to robustly track deformable fluid regions. With the help of this patch advection, we generate stable space-time data sets from detailed fluids for our repositories. We can then use our learned descriptors to quickly localize a suitable data set when running a new simulation. This makes our approach very efficient, and resolution independent. We will demonstrate with several examples that our method yields volumes with very high effective resolutions, and non-dissipative small scale details that naturally integrate into the motions of the underlying flow.",Data-driven synthesis of smoke flows with CNN-based feature descriptors,NA:NA,2017
Justin Solomon,NA,Session details: Learning & analysis for geometry,NA,2017
Li Yi:Leonidas Guibas:Aaron Hertzmann:Vladimir G. Kim:Hao Su:Ersin Yumer,"We propose a method for converting geometric shapes into hierarchically segmented parts with part labels. Our key idea is to train category-specific models from the scene graphs and part names that accompany 3D shapes in public repositories. These freely-available annotations represent an enormous, untapped source of information on geometry. However, because the models and corresponding scene graphs are created by a wide range of modelers with different levels of expertise, modeling tools, and objectives, these models have very inconsistent segmentations and hierarchies with sparse and noisy textual tags. Our method involves two analysis steps. First, we perform a joint optimization to simultaneously cluster and label parts in the database while also inferring a canonical tag dictionary and part hierarchy. We then use this labeled data to train a method for hierarchical segmentation and labeling of new 3D shapes. We demonstrate that our method can mine complex information, detecting hierarchies in man-made objects and their constituent parts, obtaining finer scale details than existing alternatives. We also show that, by performing domain transfer using a few supervised examples, our technique outperforms fully-supervised techniques that require hundreds of manually-labeled models.",Learning hierarchical shape segmentation and labeling from online repositories,NA:NA:NA:NA:NA:NA,2017
Haggai Maron:Meirav Galun:Noam Aigerman:Miri Trope:Nadav Dym:Ersin Yumer:Vladimir G. Kim:Yaron Lipman,"The recent success of convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to achieve similar success for geometric tasks. One of the main challenges in applying CNNs to surfaces is defining a natural convolution operator on surfaces. In this paper we present a method for applying deep learning to sphere-type shapes using a global seamless parameterization to a planar flat-torus, for which the convolution operator is well defined. As a result, the standard deep learning framework can be readily applied for learning semantic, high-level properties of the shape. An indication of our success in bridging the gap between images and surfaces is the fact that our algorithm succeeds in learning semantic information from an input of raw low-dimensional feature vectors. We demonstrate the usefulness of our approach by presenting two applications: human body segmentation, and automatic landmark detection on anatomical surfaces. We show that our algorithm compares favorably with competing geometric deep-learning algorithms for segmentation tasks, and is able to produce meaningful correspondences on anatomical surfaces where hand-crafted features are bound to fail.",Convolutional neural networks on surfaces via seamless toric covers,NA:NA:NA:NA:NA:NA:NA:NA,2017
Peng-Shuai Wang:Yang Liu:Yu-Xiao Guo:Chun-Yu Sun:Xin Tong,"We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.",O-CNN: octree-based convolutional neural networks for 3D shape analysis,NA:NA:NA:NA:NA,2017
Gerard Pons-Moll:Sergi Pujades:Sonny Hu:Michael J. Black,"Designing and simulating realistic clothing is challenging. Previous methods addressing the capture of clothing from 3D scans have been limited to single garments and simple motions, lack detail, or require specialized texture patterns. Here we address the problem of capturing regular clothing on fully dressed people in motion. People typically wear multiple pieces of clothing at a time. To estimate the shape of such clothing, track it over time, and render it believably, each garment must be segmented from the others and the body. Our ClothCap approach uses a new multi-part 3D model of clothed bodies, automatically segments each piece of clothing, estimates the minimally clothed body shape and pose under the clothing, and tracks the 3D deformations of the clothing over time. We estimate the garments and their motion from 4D scans; that is, high-resolution 3D scans of the subject in motion at 60 fps. ClothCap is able to capture a clothed person in motion, extract their clothing, and retarget the clothing to new body shapes; this provides a step towards virtual try-on.",ClothCap: seamless 4D clothing capture and retargeting,NA:NA:NA:NA,2017
Jaakko Lehtinen,NA,Session details: Rendering in path space,NA,2017
Hisanari Otsu:Anton S. Kaplanyan:Johannes Hanika:Carsten Dachsbacher:Toshiya Hachisuka,"Rendering algorithms using Markov chain Monte Carlo (MCMC) currently build upon two different state spaces. One of them is the path space, where the algorithms operate on the vertices of actual transport paths. The other state space is the primary sample space, where the algorithms operate on sequences of numbers used for generating transport paths. While the two state spaces are related by the sampling procedure of transport paths, all existing MCMC rendering algorithms are designed to work within only one of the state spaces. We propose a first framework which provides a comprehensive connection between the path space and the primary sample space. Using this framework, we can use mutation strategies designed for one space with mutation strategies in the respective other space. As a practical example, we take a combination of manifold exploration and multiplexed Metropolis light transport using our framework. Our results show that the simultaneous use of the two state spaces improves the robustness of MCMC rendering. By combining efficient local exploration in the path space with global jumps in primary sample space, our method achieves more uniform convergence as compared to using only one space.",Fusing state spaces for markov chain Monte Carlo rendering,NA:NA:NA:NA:NA,2017
Jacopo Pantaleoni,"In this manuscript, inspired by a simpler reformulation of primary sample space Metropolis light transport, we derive a novel family of general Markov chain Monte Carlo algorithms called charted Metropolis-Hastings, that introduces the notion of sampling charts to extend a given sampling domain and make it easier to sample the desired target distribution and escape from local maxima through coordinate changes. We further apply the novel algorithms to light transport simulation, obtaining a new type of algorithm called charted Metropolis light transport, that can be seen as a bridge between primary sample space and path space Metropolis light transport. The new algorithms require to provide only right inverses of the sampling functions, a property that we believe crucial to make them practical in the context of light transport simulation.",Charted metropolis light transport,NA,2017
Adrien Gruson:Mickaël Ribardière:Martin Šik:Jiří Vorba:Rémi Cozot:Kadi Bouatouch:Jaroslav Křivánek,"The human visual system is sensitive to relative differences in luminance, but light transport simulation algorithms based on Metropolis sampling often result in a highly nonuniform relative error distribution over the rendered image. Although this issue has previously been addressed in the context of the Metropolis light transport algorithm, our work focuses on Metropolis photon tracing. We present a new target function (TF) for Metropolis photon tracing that ensures good stratification of photons leading to pixel estimates with equalized relative error. We develop a hierarchical scheme for progressive construction of the TF from paths sampled during rendering. In addition to the approach taken in previous work, where the TF is defined in the image plane, ours can be associated with compact spatial regions. This allows us to take advantage of illumination coherence to more robustly estimate the TF while adapting to geometry discontinuities. To sample from this TF, we design a new replica exchange Metropolis scheme. We apply our algorithm in progressive photon mapping and show that it often outperforms alternative approaches in terms of image quality by a large margin.",A Spatial Target Function for Metropolis Photon Tracing,NA:NA:NA:NA:NA:NA:NA,2017
Laurent Belcour:Ling-Qi Yan:Ravi Ramamoorthi:Derek Nowrouzezahrai,"We present the first method to efficiently predict antialiasing footprints to pre-filter color-, normal-, and displacement-mapped appearance in the context of multi-bounce global illumination. We derive Fourier spectra for radiance and importance functions that allow us to compute spatial-angular filtering footprints at path vertices for both uni- and bi-directional path construction. We then use these footprints to antialias reflectance modulated by high-resolution maps (such as color and normal maps) encountered along a path. In doing so, we also unify the traditional path-space formulation of light transport with our frequency-space interpretation of global illumination pre-filtering. Our method is fully compatible with all existing single bounce pre-filtering appearance models, not restricted by path length, and easy to implement atop existing path-space renderers. We illustrate its effectiveness on several radiometrically complex scenarios where previous approaches either completely fail or require orders of magnitude more time to arrive at similarly high-quality results.",Antialiasing Complex Global Illumination Effects in Path-Space,NA:NA:NA:NA,2017
Alla Sheffer,NA,"Session details: Reconstructing 3D surfaces from points, lines, images & water",NA,2017
Zhiyang Huang:Ming Zou:Nathan Carr:Tao Ju,"In this work we present the first algorithm for reconstructing multi-labeled material interfaces the allows for explicit topology control. Our algorithm takes in a set of 2D cross-sectional slices (not necessarily parallel), each partitioned by a curve network into labeled regions representing different material types. For each label, the user has the option to constrain the number of connected components and genus. Our algorithm is able to not only produce a material interface that interpolates the curve networks but also simultaneously satisfy the topological requirements. Our key innovation is defining a space of topology-varying material interfaces, which extends the family of level sets in a scalar function, and developing discrete methods for sampling distinct topologies in this space. Besides specifying topological constraints, the user can steer the algorithm interactively, such as by scribbling. We demonstrate, on synthetic and biological shapes, how our algorithm opens up new opportunities for topology-aware modeling in the multi-labeled context.",Topology-controlled reconstruction of multi-labelled domains from cross-sections,NA:NA:NA:NA,2017
Angela Dai:Matthias Nießner:Michael Zollhöfer:Shahram Izadi:Christian Theobalt,"Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed reality and robotic applications. However, scalability brings challenges of drift in pose estimation, introducing significant errors in the accumulated model. Approaches often require hours of offline processing to globally correct model errors. Recent online methods demonstrate compelling results but suffer from (1) needing minutes to perform online correction, preventing true real-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation, resulting in many tracking failures; or (3) supporting only unstructured point-based representations, which limit scan quality and applicability. We systematically address these issues with a novel, real-time, end-to-end reconstruction framework. At its core is a robust pose estimation strategy, optimizing per frame for a global set of camera poses by considering the complete history of RGB-D input with an efficient hierarchical approach. We remove the heavy reliance on temporal tracking and continually localize to the globally optimized frames instead. We contribute a parallelizable optimization framework, which employs correspondences based on sparse features and dense geometric and photometric matching. Our approach estimates globally optimized (i.e., bundle adjusted) poses in real time, supports robust tracking with recovery from gross tracking failures (i.e., relocalization), and re-estimates the 3D model in real time to ensure global consistency, all within a single framework. Our approach outperforms state-of-the-art online systems with quality on par to offline methods, but with unprecedented speed and scan completeness. Our framework leads to a comprehensive online scanning solution for large indoor environments, enabling ease of use and high-quality results.1",BundleFusion: Real-Time Globally Consistent 3D Reconstruction Using On-the-Fly Surface Reintegration,NA:NA:NA:NA:NA,2017
Nico Schertler:Marco Tarini:Wenzel Jakob:Misha Kazhdan:Stefan Gumhold:Daniele Panozzo,"Today's 3D scanning pipelines can be classified into two overarching categories: offline, high accuracy methods that rely on global optimization to reconstruct complex scenes with hundreds of millions of samples, and online methods that produce real-time but low-quality output, usually from structure-from-motion or depth sensors. The method proposed in this paper is the first to combine the benefits of both approaches, supporting online reconstruction of scenes with hundreds of millions of samples from high-resolution sensing modalities such as structured light or laser scanners. The key property of our algorithm is that it sidesteps the signed-distance computation of classical reconstruction techniques in favor of direct filtering, parametrization, and mesh and texture extraction. All of these steps can be realized using only weak notions of spatial neighborhoods, which allows for an implementation that scales approximately linearly with the size of each dataset that is integrated into a partial reconstruction. Combined, these algorithmic differences enable a drastically more efficient output-driven interactive scanning and reconstruction workflow, where the user is able to see the final quality field-aligned textured mesh during the entirety of the scanning procedure. Holes or parts with registration problems are displayed in real-time to the user and can be easily resolved by adding further localized scans, or by adjusting the input point cloud using our interactive editing tools with immediate visual feedback on the output mesh. We demonstrate the effectiveness of our algorithm in conjunction with a state-of-the-art structured light scanner and optical tracking system and test it on a large variety of challenging models.",Field-aligned online surface reconstruction,NA:NA:NA:NA:NA:NA,2017
Arno Knapitsch:Jaesik Park:Qian-Yi Zhou:Vladlen Koltun,"We present a benchmark for image-based 3D reconstruction. The benchmark sequences were acquired outside the lab, in realistic conditions. Ground-truth data was captured using an industrial laser scanner. The benchmark includes both outdoor scenes and indoor environments. High-resolution video sequences are provided as input, supporting the development of novel pipelines that take advantage of video input to increase reconstruction fidelity. We report the performance of many image-based 3D reconstruction pipelines on the new benchmark. The results point to exciting challenges and opportunities for future work.",Tanks and temples: benchmarking large-scale scene reconstruction,NA:NA:NA:NA,2017
Kfir Aberman:Oren Katzir:Qiang Zhou:Zegang Luo:Andrei Sharf:Chen Greif:Baoquan Chen:Daniel Cohen-Or,"The paper presents a novel three-dimensional shape acquisition and reconstruction method based on the well-known Archimedes equality between fluid displacement and the submerged volume. By repeatedly dipping a shape in liquid in different orientations and measuring its volume displacement, we generate the dip transform: a novel volumetric shape representation that characterizes the object's surface. The key feature of our method is that it employs fluid displacements as the shape sensor. Unlike optical sensors, the liquid has no line-of-sight requirements, it penetrates cavities and hidden parts of the object, as well as transparent and glossy materials, thus bypassing all visibility and optical limitations of conventional scanning devices. Our new scanning approach is implemented using a dipping robot arm and a bath of water, via which it measures the water elevation. We show results of reconstructing complex 3D shapes and evaluate the quality of the reconstruction with respect to the number of dips.",Dip transform for 3D shape reconstruction,NA:NA:NA:NA:NA:NA:NA:NA,2017
Takeo Igarashi,NA,Session details: Fabrication animation,NA,2017
James M. Bern:Kai-Hung Chang:Stelian Coros,"We present a computational approach to creating animated plushies, soft robotic plush toys specifically-designed to reenact user-authored motions. Our design process is inspired by muscular hydrostat structures, which drive highly versatile motions in many biological systems. We begin by instrumenting simulated plush toys with a large number of small, independently-actuated, virtual muscle-fibers. Through an intuitive posing interface, users then begin animating their plushie. A novel numerical solver, reminiscent of inverse-kinematics, computes optimal contractions for each muscle-fiber such that the soft body of the plushie deforms to best match user input. By analyzing the co-activation patterns of the fibers that contribute most to the plushie's motions, our design system generates physically-realizable winch-tendon networks. Winch-tendon networks model the motorized cable-driven actuation mechanisms that drive the motions of our real-life plush toy prototypes. We demonstrate the effectiveness of our computational approach by co-designing motions and actuation systems for a variety of physically-simulated and fabricated plushies.",Interactive design of animated plushies,NA:NA:NA,2017
Ran Zhang:Thomas Auzinger:Duygu Ceylan:Wilmot Li:Bernd Bickel,"We present an interactive design system to create functional mechanical objects. Our computational approach allows novice users to retarget an existing mechanical template to a user-specified input shape. Our proposed representation for a mechanical template encodes a parameterized mechanism, mechanical constraints that ensure a physically valid configuration, spatial relationships of mechanical parts to the user-provided shape, and functional constraints that specify an intended functionality. We provide an intuitive interface and optimization-in-the-loop approach for finding a valid configuration of the mechanism and the shape to ensure that higher-level functional goals are met. Our algorithm interactively optimizes the mechanism while the user manipulates the placement of mechanical components and the shape. Our system allows users to efficiently explore various design choices and to synthesize customized mechanical objects that can be fabricated with rapid prototyping technologies. We demonstrate the efficacy of our approach by retargeting various mechanical templates to different shapes and fabricating the resulting functional mechanical objects.",Functionality-aware retargeting of mechanisms to 3D shapes,NA:NA:NA:NA:NA,2017
Vittorio Megaro:Jonas Zehnder:Moritz Bächer:Stelian Coros:Markus Gross:Bernhard Thomaszewski,"We present a computational tool for designing compliant mechanisms. Our method takes as input a conventional, rigidly-articulated mechanism defining the topology of the compliant design. This input can be both planar or spatial, and we support a number of common joint types which, whenever possible, are automatically replaced with parameterized flexures. As the technical core of our approach, we describe a number of objectives that shape the design space in a meaningful way, including trajectory matching, collision avoidance, lateral stability, resilience to failure, and minimizing motor torque. Optimal designs in this space are obtained as solutions to an equilibrium-constrained minimization problem that we solve using a variant of sensitivity analysis. We demonstrate our method on a set of examples that range from simple four-bar linkages to full-fledged animatronics, and verify the feasibility of our designs by manufacturing physical prototypes.",A computational design tool for compliant mechanisms,NA:NA:NA:NA:NA:NA,2017
Christopher Yu:Keenan Crane:Stelian Coros,"Telescoping structures are valuable for a variety of applications where mechanisms must be compact in size and yet easily deployed. So far, however, there has been no systematic study of the types of shapes that can be modeled by telescoping structures, nor practical tools for telescopic design. We present a novel geometric characterization of telescoping curves, and explore how free-form surfaces can be approximated by networks of such curves. In particular we consider piecewise helical space curves with torsional impulses, which significantly generalize the linear telescopes found in typical engineering designs. Based on this principle we develop a system for computational design and fabrication which allows users to explore the space of telescoping structures; inputs to our system include user sketches or arbitrary meshes, which are then converted to a curve skeleton. We prototype applications in animation, fabrication, and robotics, using our system to design a variety of both simulated and fabricated examples.",Computational design of telescoping structures,NA:NA:NA,2017
Desai Chen:David I. W. Levin:Wojciech Matusik:Danny M. Kaufman,"The realistic simulation of highly-dynamic elastic objects is important for a broad range of applications in computer graphics, engineering and computational fabrication. However, whether simulating flipping toys, jumping robots, prosthetics or quickly moving creatures, performing such simulations in the presence of contact, impact and friction is both time consuming and inaccurate. In this paper we present Dynamics-Aware Coarsening (DAC) and the Boundary Balanced Impact (BBI) model which allow for the accurate simulation of dynamic, elastic objects undergoing both large scale deformation and frictional contact, at rates up to 79 times faster than state-of-the-art methods. DAC and BBI produce simulations that are accurate and fast enough to be used (for the first time) for the computational design of 3D-printable compliant dynamic mechanisms. Thus we demonstrate the efficacy of DAC and BBI by designing and fabricating mechanisms which flip, throw and jump over and onto obstacles as requested.",Dynamics-aware numerical coarsening for fabrication design,NA:NA:NA:NA,2017
Elmar Eisemann,NA,Session details: Time to focus,NA,2017
Andrew Maimone:Andreas Georgiou:Joel S. Kollin,"We present novel designs for virtual and augmented reality near-eye displays based on phase-only holographic projection. Our approach is built on the principles of Fresnel holography and double phase amplitude encoding with additional hardware, phase correction factors, and spatial light modulator encodings to achieve full color, high contrast and low noise holograms with high resolution and true per-pixel focal control. We provide a GPU-accelerated implementation of all holographic computation that integrates with the standard graphics pipeline and enables real-time (≥90 Hz) calculation directly or through eye tracked approximations. A unified focus, aberration correction, and vision correction model, along with a user calibration process, accounts for any optical defects between the light source and retina. We use this optical correction ability not only to fix minor aberrations but to enable truly compact, eyeglasses-like displays with wide fields of view (80°) that would be inaccessible through conventional means. All functionality is evaluated across a series of hardware prototypes; we discuss remaining challenges to incorporate all features into a single device.",Holographic near-eye displays for virtual and augmented reality,NA:NA:NA,2017
Nathan Matsuda:Alexander Fix:Douglas Lanman,"Conventional binocular head-mounted displays (HMDs) vary the stimulus to vergence with the information in the picture, while the stimulus to accommodation remains fixed at the apparent distance of the display, as created by the viewing optics. Sustained vergence-accommodation conflict (VAC) has been associated with visual discomfort, motivating numerous proposals for delivering near-correct accommodation cues. We introduce focal surface displays to meet this challenge, augmenting conventional HMDs with a phase-only spatial light modulator (SLM) placed between the display screen and viewing optics. This SLM acts as a dynamic freeform lens, shaping synthesized focal surfaces to conform to the virtual scene geometry. We introduce a framework to decompose target focal stacks and depth maps into one or more pairs of piecewise smooth focal surfaces and underlying display images. We build on recent developments in ""optimized blending"" to implement a multifocal display that allows the accurate depiction of occluding, semi-transparent, and reflective objects. Practical benefits over prior accommodation-supporting HMDs are demonstrated using a binocular focal surface display employing a liquid crystal on silicon (LCOS) phase SLM and an organic light-emitting diode (OLED) display.",Focal surface displays,NA:NA:NA,2017
George-Alex Koulieris:Bee Bui:Martin S. Banks:George Drettakis,"Head-mounted displays (HMDs) often cause discomfort and even nausea. Improving comfort is therefore one of the most significant challenges for the design of such systems. In this paper, we evaluate the effect of different HMD display configurations on discomfort. We do this by designing a device to measure human visual behavior and evaluate viewer comfort. In particular, we focus on one known source of discomfort: the vergence-accommodation (VA) conflict. The VA conflict is the difference between accommodative and vergence response. In HMDs the eyes accommodate to a fixed screen distance while they converge to the simulated distance of the object of interest, requiring the viewer to undo the neural coupling between the two responses. Several methods have been proposed to alleviate the VA conflict, including Depth-of-Field (DoF) rendering, focus-adjustable lenses, and monovision. However, no previous work has investigated whether these solutions actually drive accommodation to the distance of the simulated object. If they did, the VA conflict would disappear, and we expect comfort to improve. We design the first device that allows us to measure accommodation in HMDs, and we use it to obtain accommodation measurements and to conduct a discomfort study. The results of the first experiment demonstrate that only the focus-adjustable-lens design drives accommodation effectively, while other solutions do not drive accommodation to the simulated distance and thus do not resolve the VA conflict. The second experiment measures discomfort. The results validate that the focus-adjustable-lens design improves comfort significantly more than the other solutions.",Accommodation and comfort in head-mounted displays,NA:NA:NA:NA,2017
Robert Konrad:Nitish Padmanaban:Keenan Molner:Emily A. Cooper:Gordon Wetzstein,"Although emerging virtual and augmented reality (VR/AR) systems can produce highly immersive experiences, they can also cause visual discomfort, eyestrain, and nausea. One of the sources of these symptoms is a mismatch between vergence and focus cues. In current VR/AR near-eye displays, a stereoscopic image pair drives the vergence state of the human visual system to arbitrary distances, but the accommodation, or focus, state of the eyes is optically driven towards a fixed distance. In this work, we introduce a new display technology, dubbed accommodation-invariant (AI) near-eye displays, to improve the consistency of depth cues in near-eye displays. Rather than producing correct focus cues, AI displays are optically engineered to produce visual stimuli that are invariant to the accommodation state of the eye. The accommodation system can then be driven by stereoscopic cues, and the mismatch between vergence and accommodation state of the eyes is significantly reduced. We validate the principle of operation of AI displays using a prototype display that allows for the accommodation state of users to be measured while they view visual stimuli using multiple different display modes.",Accommodation-invariant computational near-eye displays,NA:NA:NA:NA:NA,2017
Fernando de Goes,NA,Session details: Global parameterization,NA,2017
Alon Bright:Edward Chien:Ofir Weber,"We present a method for locally injective seamless parametrization of triangular mesh surfaces of arbitrary genus, with or without boundaries, given desired cone points and rational holonomy angles (multiples of 2π/q for some positive integer q). The basis of the method is an elegant generalization of Tutte's ""spring embedding theorem"" to this setting. The surface is cut to a disk and a harmonic system with appropriate rotation constraints is solved, resulting in a harmonic global parametrization (HGP) method. We show a remarkable result: that if the triangles adjacent to the cones and boundary are positively oriented, and the correct cone and turning angles are induced, then the resulting map is guaranteed to be locally injective. Guided by this result, we solve the linear system by convex optimization, imposing convexification frames on only the boundary and cone triangles, and minimizing a Laplacian energy to achieve harmonicity. We compare HGP to state-of-the-art methods and see that it is the most robust, and is significantly faster than methods with comparable robustness.",Harmonic global parametrization with rational holonomy,NA:NA:NA,2017
Noam Aigerman:Shahar Z. Kovalsky:Yaron Lipman,"This work presents an algorithm for injectively parameterizing surfaces into spherical target domains called spherical orbifolds. Spherical orbifolds are cone surfaces that are generated from symmetry groups of the sphere. The surface is mapped the spherical orbifold via an extension of Tutte's embedding. This embedding is proven to be bijective under mild additional assumptions, which hold in all experiments performed. This work also completes the adaptation of Tutte's embedding to orbifolds of the three classic geometries - Euclidean, hyperbolic and spherical - where the first two were recently addressed. The spherical orbifold embeddings approximate conformal maps and require relatively low computational times. The constant positive curvature of the spherical orbifolds, along with the flexibility of their cone angles, enables producing embeddings with lower isometric distortion compared to their Euclidean counterparts, a fact that makes spherical orbifolds a natural candidate for surface parameterization.",Spherical orbifold tutte embeddings,NA:NA:NA,2017
Marcel Campen:Denis Zorin,"A variety of techniques were proposed to model smooth surfaces based on tensor product splines (e.g. subdivision surfaces, free-form splines, T-splines). Conversion of an input surface into such a representation is commonly achieved by constructing a global seamless parametrization, possibly aligned to a guiding cross-field (e.g. of principal curvature directions), and using this parametrization as domain to construct the spline-based surface. One major fundamental difficulty in designing robust algorithms for this task is the fact that for common types, e.g. subdivision surfaces (requiring a conforming domain mesh) or T-spline surfaces (requiring a globally consistent knot interval assignment) reliably obtaining a suitable parametrization that has the same topological structure as the guiding field poses a major challenge. Even worse, not all fields do admit suitable parametrizations, and no concise conditions are known as to which fields do. We present a class of surface constructions (T-splines with halfedge knots) and a class of parametrizations (seamless similarity maps) that are, in a sense, a perfect match for the task: for any given guiding field structure, a compatible parametrization of this kind exists and a smooth piecewise rational surface with exactly the same structure as the input field can be constructed from it. As a byproduct, this enables full control over extraordinary points. The construction is backward compatible with classical NURBS. We present efficient algorithms for building discrete conformal similarity maps and associated T-meshes and T-spline surfaces.",Similarity maps and field-guided T-splines: a perfect couple,NA:NA,2017
Omri Azencot:Etienne Corman:Mirela Ben-Chen:Maks Ovsjanikov,"We propose a novel technique for computing consistent cross fields on a pair of triangle meshes given an input correspondence, which we use as guiding fields for approximately consistent quadrangulations. Unlike the majority of existing methods our approach does not assume that the meshes share the same connectivity or even have the same number of vertices, and furthermore does not place any restrictions on the topology (genus) of the shapes. Importantly, our method is robust with respect to small perturbations of the given correspondence, as it only relies on the transportation of real-valued functions and thus avoids the costly and error-prone estimation of the map differential. Key to this robustness is a novel formulation, which relies on the previously-proposed notion of power vectors, and we show how consistency can be enforced without pre-alignment of local basis frames, in which these power vectors are computed. We demonstrate that using the same formulation we can both compute a quadrangulation that would respect a given symmetry on the same shape or a map across a pair of shapes. We provide quantitative and qualitative comparison of our method with several baselines and show that it both provides more accurate results and allows to handle more general cases than existing techniques.",Consistent functional cross field design for mesh quadrangulation,NA:NA:NA:NA,2017
Nikunj Raghuvanshi,NA,Session details: Speech and facial animation,NA,2017
Sarah Taylor:Taehwan Kim:Yisong Yue:Moshe Mahler:James Krahe:Anastasio Garcia Rodriguez:Jessica Hodgins:Iain Matthews,"We introduce a simple and effective deep learning approach to automatically generate natural looking speech animation that synchronizes to input speech. Our approach uses a sliding window predictor that learns arbitrary nonlinear mappings from phoneme label input sequences to mouth movements in a way that accurately captures natural motion and visual coarticulation effects. Our deep learning approach enjoys several attractive properties: it runs in real-time, requires minimal parameter tuning, generalizes well to novel input speech sequences, is easily edited to create stylized and emotional speech, and is compatible with existing animation retargeting approaches. One important focus of our work is to develop an effective approach for speech animation that can be easily integrated into existing production pipelines. We provide a detailed description of our end-to-end approach, including machine learning design decisions. Generalized speech animation results are demonstrated over a wide range of animation clips on a variety of characters and voices, including singing and foreign language input. Our approach can also generate on-demand speech animation in real-time from user speech input.",A deep learning approach for generalized speech animation,NA:NA:NA:NA:NA:NA:NA:NA,2017
Tero Karras:Timo Aila:Samuli Laine:Antti Herva:Jaakko Lehtinen,"We present a machine learning technique for driving 3D facial animation by audio input in real time and with low latency. Our deep neural network learns a mapping from input waveforms to the 3D vertex coordinates of a face model, and simultaneously discovers a compact, latent code that disambiguates the variations in facial expression that cannot be explained by the audio alone. During inference, the latent code can be used as an intuitive control for the emotional state of the face puppet. We train our network with 3--5 minutes of high-quality animation data obtained using traditional, vision-based performance capture methods. Even though our primary goal is to model the speaking style of a single actor, our model yields reasonable results even when driven with audio from other speakers with different gender, accent, or language, as we demonstrate with a user study. The results are applicable to in-game dialogue, low-cost localization, virtual reality avatars, and telepresence.",Audio-driven facial animation by joint end-to-end learning of pose and emotion,NA:NA:NA:NA:NA,2017
Supasorn Suwajanakorn:Steven M. Seitz:Ira Kemelmacher-Shlizerman,"Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.",Synthesizing Obama: learning lip sync from audio,NA:NA:NA,2017
Zeyu Jin:Gautham J. Mysore:Stephen Diverdi:Jingwan Lu:Adam Finkelstein,"Editing audio narration using conventional software typically involves many painstaking low-level manipulations. Some state of the art systems allow the editor to work in a text transcript of the narration, and perform select, cut, copy and paste operations directly in the transcript; these operations are then automatically applied to the waveform in a straightforward manner. However, an obvious gap in the text-based interface is the ability to type new words not appearing in the transcript, for example inserting a new word for emphasis or replacing a misspoken word. While high-quality voice synthesizers exist today, the challenge is to synthesize the new word in a voice that matches the rest of the narration. This paper presents a system that can synthesize a new word or short phrase such that it blends seamlessly in the context of the existing narration. Our approach is to use a text to speech synthesizer to say the word in a generic voice, and then use voice conversion to convert it into a voice that matches the narration. Offering a range of degrees of control to the editor, our interface supports fully automatic synthesis, selection among a candidate set of alternative pronunciations, fine control over edit placements and pitch profiles, and even guidance by the editors own voice. The paper presents studies showing that the output of our method is preferred over baseline methods and often indistinguishable from the original voice.",VoCo: text-based insertion and replacement in audio narration,NA:NA:NA:NA:NA,2017
Toshiya Hachisuka,NA,Session details: Rendering systems,NA,2017
Steve Bako:Thijs Vogels:Brian Mcwilliams:Mark Meyer:Jan NováK:Alex Harvill:Pradeep Sen:Tony Derose:Fabrice Rousselle,"Regression-based algorithms have shown to be good at denoising Monte Carlo (MC) renderings by leveraging its inexpensive by-products (e.g., feature buffers). However, when using higher-order models to handle complex cases, these techniques often overfit to noise in the input. For this reason, supervised learning methods have been proposed that train on a large collection of reference examples, but they use explicit filters that limit their denoising ability. To address these problems, we propose a novel, supervised learning approach that allows the filtering kernel to be more complex and general by leveraging a deep convolutional neural network (CNN) architecture. In one embodiment of our framework, the CNN directly predicts the final denoised pixel value as a highly non-linear combination of the input features. In a second approach, we introduce a novel, kernel-prediction network which uses the CNN to estimate the local weighting kernels used to compute each denoised pixel from its neighbors. We train and evaluate our networks on production data and observe improvements over state-of-the-art MC denoisers, showing that our methods generalize well to a variety of scenes. We conclude by analyzing various components of our architecture and identify areas of further research in deep learning for MC denoising.",Kernel-predicting convolutional networks for denoising Monte Carlo renderings,NA:NA:NA:NA:NA:NA:NA:NA:NA,2017
Chakravarty R. Alla Chaitanya:Anton S. Kaplanyan:Christoph Schied:Marco Salvi:Aaron Lefohn:Derek Nowrouzezahrai:Timo Aila,"We describe a machine learning technique for reconstructing image sequences rendered using Monte Carlo methods. Our primary focus is on reconstruction of global illumination with extremely low sampling budgets at interactive rates. Motivated by recent advances in image restoration with deep convolutional networks, we propose a variant of these networks better suited to the class of noise present in Monte Carlo rendering. We allow for much larger pixel neighborhoods to be taken into account, while also improving execution speed by an order of magnitude. Our primary contribution is the addition of recurrent connections to the network in order to drastically improve temporal stability for sequences of sparsely sampled input images. Our method also has the desirable property of automatically modeling relationships based on auxiliary per-pixel input channels, such as depth and normals. We show significantly higher quality results compared to existing methods that run at comparable speeds, and furthermore argue a clear path for making our method run at realtime rates in the near future.",Interactive reconstruction of Monte Carlo image sequences using a recurrent denoising autoencoder,NA:NA:NA:NA:NA:NA:NA,2017
Luke Anderson:Tzu-Mao Li:Jaakko Lehtinen:Frédo Durand,"Implementing Monte Carlo integration requires significant domain expertise. While simple samplers, such as unidirectional path tracing, are relatively forgiving, more complex algorithms, such as bidirectional path tracing or Metropolis methods, are notoriously difficult to implement correctly. We propose Aether, an embedded domain specific language for Monte Carlo integration, which offers primitives for writing concise and correct-by-construction sampling and probability code. The user is tasked with writing sampling code, while our compiler automatically generates the code necessary for evaluating PDFs as well as the book keeping and combination of multiple sampling strategies. Our language focuses on ease of implementation for rapid exploration, at the cost of run time performance. We demonstrate the effectiveness of the language by implementing several challenging rendering algorithms as well as a new algorithm, which would otherwise be prohibitively difficult.",Aether: an embedded domain specific sampling language for Monte Carlo rendering,NA:NA:NA:NA,2017
Yong He:Tim Foley:Teguh Hofstee:Haomin Long:Kayvon Fatahalian,"Modern game engines seek to balance the conflicting goals of high rendering performance and productive software development. To improve CPU performance, the most recent generation of real-time graphics APIs provide new primitives for performing efficient batch updates to shader parameters. However, modern game engines featuring large shader codebases have struggled to take advantage of these benefits. The problem is that even though shader parameters can be organized into efficient modules bound to the pipeline at various frequencies, modern shading languages lack corresponding primitives to organize shader logic (requiring these parameters) into modules as well. The result is that complex shaders are typically compiled to use a monolithic block of parameters, defeating the design, and performance benefits, of the new parameter binding API. In this paper we propose to resolve this mismatch by introducing shader components, a first-class unit of modularity in a shader program that encapsulates a unit of shader logic and the parameters that must be bound when that logic is in use. We show that by building sophisticated shaders out of components, we can retain essential aspects of performance (static specialization of the shader logic in use and efficient update of parameters at component granularity) while maintaining the modular shader code structure that is desirable in today's high-end game engines.",Shader components: modular and high performance shader development,NA:NA:NA:NA:NA,2017
Kai Selgrad:Alexander Lier:Magdalena Martinek:Christoph Buchenau:Michael Guthe:Franziska Kranz:Henry Schäfer:Marc Stamminger,"Parametric surfaces are an essential modeling tool in computer aided design and movie production. Even though their use is well established in industry, generating ray-traced images adds significant cost in time and memory consumption. Ray tracing such surfaces is usually accomplished by subdividing the surfaces on the fly, or by conversion to a polygonal representation. However, on-the-fly subdivision is computationally very expensive, whereas polygonal meshes require large amounts of memory. This is a particular problem for parametric surfaces with displacement, where very fine tessellation is required to faithfully represent the shape. Hence, memory restrictions are the major challenge in production rendering. In this article, we present a novel solution to this problem. We propose a compression scheme for a priori Bounding Volume Hierarchies (BVHs) on parametric patches, that reduces the data required for the hierarchy by a factor of up to 48. We further propose an approximate evaluation method that does not require leaf geometry, yielding an overall reduction of memory consumption by a factor of 60 over regular BVHs on indexed face sets and by a factor of 16 over established state-of-the-art compression schemes. Alternatively, our compression can simply be applied to a standard BVH while keeping the leaf geometry, resulting in a compression rate of up to 2:1 over current methods. Although decompression generates additional costs during traversal, we can manage very complex scenes even on the memory restrictive GPU at competitive render times.",A Compressed Representation for Ray Tracing Parametric Surfaces,NA:NA:NA:NA:NA:NA:NA:NA,2017
Tamar Shinar,NA,Session details: Fluids II,NA,2017
Egor Larionov:Christopher Batty:Robert Bridson,"We propose a novel unsteady Stokes solver for coupled viscous and pressure forces in grid-based liquid animation which yields greater accuracy and visual realism than previously achieved. Modern fluid simulators treat viscosity and pressure in separate solver stages, which reduces accuracy and yields incorrect free surface behavior. Our proposed implicit variational formulation of the Stokes problem leads to a symmetric positive definite linear system that gives properly coupled forces, provides unconditional stability, and treats difficult boundary conditions naturally through simple volume weights. Surface tension and moving solid boundaries are also easily incorporated. Qualitatively, we show that our method recovers the characteristic rope coiling instability of viscous liquids and preserves fine surface details, while previous grid-based schemes do not. Quantitatively, we demonstrate that our method is convergent through grid refinement studies on analytical problems in two dimensions. We conclude by offering practical guidelines for choosing an appropriate viscous solver, based on the scenario to be animated and the computational costs of different methods.",Variational stokes: a unified pressure-viscosity solver for accurate viscous liquids,NA:NA:NA,2017
Rene Winchenbach:Hendrik Hochstetter:Andreas Kolb,"In this paper we introduce a novel method to adaptive incompressible SPH simulations. Instead of using a scheme with a number of fixed particle sizes or levels, our approach allows continuous particle sizes. This enables us to define optimal particle masses with respect to, e.g., the distance to the fluid's surface. A required change in mass due to the dynamics of the fluid is properly and stably handled by our scheme of mass redistribution. This includes temporally smooth changes in particle masses as well as sudden mass variations in regions of high flow dynamics. Our approach guarantees low spatial variations in particle size, which is a core property in order to achieve large adaptivity ratios for incompressible fluid simulations. Conceptually, our approach allows for infinite continuous adaptivity, practically we achieved adaptivity ratios up to 5 orders of magnitude, while still being mass preserving and numerically stable, yielding unprecedented vivid surface detail at comparably low computational cost and moderate particle counts.",Infinite continuous adaptivity for incompressible SPH,NA:NA:NA,2017
Stefan Jeschke:Chris Wojtan,"This paper presents a method for simulating water surface waves as a displacement field on a 2D domain. Our method relies on Lagrangian particles that carry packets of water wave energy; each packet carries information about an entire group of wave trains, as opposed to only a single wave crest. Our approach is unconditionally stable and can simulate high resolution geometric details. This approach also presents a straightforward interface for artistic control, because it is essentially a particle system with intuitive parameters like wavelength and amplitude. Our implementation parallelizes well and runs in real time for moderately challenging scenarios.",Water wave packets,NA:NA,2017
Alexis Angelidis,"We present a multi-scale method for simulating incompressible gases in 3-dimensions with resolution variation suitable for perspective cameras and regions of importance. The dynamics is derived from the vorticity equation. Lagrangian particles are created, modified and deleted in a manner that handles advection with buoyancy and viscosity. Boundaries and deformable object collisions are modeled with the source and doublet panel method. Our acceleration structure is based on the FMM (Fast Multipole Method), but with a varying size to account for non-uniform sampling. Because the dynamics of our method is voxel free, we can freely specify the voxel resolution of the output density and velocity while keeping the main shapes and timing.",Multi-scale vorticle fluids,NA,2017
Andre Pradhana Tampubolon:Theodore Gast:Gergely Klár:Chuyuan Fu:Joseph Teran:Chenfanfu Jiang:Ken Museth,"We present a multi-species model for the simulation of gravity driven landslides and debris flows with porous sand and water interactions. We use continuum mixture theory to describe individual phases where each species individually obeys conservation of mass and momentum and they are coupled through a momentum exchange term. Water is modeled as a weakly compressible fluid and sand is modeled with an elastoplastic law whose cohesion varies with water saturation. We use a two-grid Material Point Method to discretize the governing equations. The momentum exchange term in the mixture theory is relatively stiff and we use semi-implicit time stepping to avoid associated small time steps. Our semi-implicit treatment is explicit in plasticity and preserves symmetry of force linearizations. We develop a novel regularization of the elastic part of the sand constitutive model that better mimics plasticity during the implicit solve to prevent numerical cohesion artifacts that would otherwise have occurred. Lastly, we develop an improved return mapping for sand plasticity that prevents volume gain artifacts in the traditional Drucker-Prager model.",Multi-species simulation of porous sand and water mixtures,NA:NA:NA:NA:NA:NA:NA,2017
Maneesh Agrawala,NA,Session details: Image texture & completion,NA,2017
Hugo Loi:Thomas Hurtut:Romain Vergne:Joelle Thollot,"This article introduces a programmable method for designing stationary 2D arrangements for element textures, namely textures made of small geometric elements. These textures are ubiquitous in numerous applications of computer-aided illustration. Previous methods, whether they be example-based or layout-based, lack control and can produce a limited range of possible arrangements. Our approach targets technical artists who will design an arrangement by writing a script. These scripts are using three types of operators: partitioning operators for defining the broad-scale organization of the arrangement, mapping operators for controlling the local organization of elements, and merging operators for mixing different arrangements. These operators are designed so as to guarantee a stationary result, meaning that the produced arrangements will always be repetitive. We show that this simple set of operators is sufficient to reach a much broader variety of arrangements than previous methods. Editing the script leads to predictable changes in the synthesized arrangement, which allows an easy iterative design of complex structures. Finally, our operator set is extensible and can be adapted to application-dependent needs.",Programmable 2D Arrangements for Element Texture Design,NA:NA:NA:NA,2017
Omry Sendik:Daniel Cohen-Or,"Example-based texture synthesis has been an active research problem for over two decades. Still, synthesizing textures with nonlocal structures remains a challenge. In this article, we present a texture synthesis technique that builds upon convolutional neural networks and extracted statistics of pretrained deep features. We introduce a structural energy, based on correlations among deep features, which capture the self-similarities and regularities characterizing the texture. Specifically, we show that our technique can synthesize textures that have structures of various scales, local and nonlocal, and the combination of the two.",Deep Correlations for Texture Synthesis,NA:NA,2017
Sai Bi:Nima Khademi Kalantari:Ravi Ramamoorthi,"Image-based texture mapping is a common way of producing texture maps for geometric models of real-world objects. Although a high-quality texture map can be easily computed for accurate geometry and calibrated cameras, the quality of texture map degrades significantly in the presence of inaccuracies. In this paper, we address this problem by proposing a novel global patch-based optimization system to synthesize the aligned images. Specifically, we use patch-based synthesis to reconstruct a set of photometrically-consistent aligned images by drawing information from the source images. Our optimization system is simple, flexible, and more suitable for correcting large misalignments than other techniques such as local warping. To solve the optimization, we propose a two-step approach which involves patch search and vote, and reconstruction. Experimental results show that our approach can produce high-quality texture maps better than existing techniques for objects scanned by consumer depth cameras such as Intel RealSense. Moreover, we demonstrate that our system can be used for texture editing tasks such as hole-filling and reshuffling as well as multiview camouflage.",Patch-based optimization for image-based texture mapping,NA:NA:NA,2017
Satoshi Iizuka:Edgar Simo-Serra:Hiroshi Ishikawa,"We present a novel approach for image completion that results in images that are both locally and globally consistent. With a fully-convolutional neural network, we can complete images of arbitrary resolutions by filling-in missing regions of any shape. To train this image completion network to be consistent, we use global and local context discriminators that are trained to distinguish real images from completed ones. The global discriminator looks at the entire image to assess if it is coherent as a whole, while the local discriminator looks only at a small area centered at the completed region to ensure the local consistency of the generated patches. The image completion network is then trained to fool the both context discriminator networks, which requires it to generate images that are indistinguishable from real ones with regard to overall consistency as well as in details. We show that our approach can be used to complete a wide variety of scenes. Furthermore, in contrast with the patch-based approaches such as PatchMatch, our approach can generate fragments that do not appear elsewhere in the image, which allows us to naturally complete the images of objects with familiar and highly specific structures, such as faces.",Globally and locally consistent image completion,NA:NA:NA,2017
Michal Lukáč:Daniel Sýkora:Kalyan Sunkavalli:Eli Shechtman:Ondřej Jamriška:Nathan Carr:Tomáš Pajdla,"Natural images often exhibit symmetries that should be taken into account when editing them. In this paper we present Nautilus --- a method for automatically identifying symmetric regions in an image along with their corresponding symmetry transformations. We compute dense local similarity symmetry transformations using a novel variant of the Generalised PatchMatch algorithm that uses Metropolis-Hastings sampling. We combine and refine these local symmetries using an extended Lucas-Kanade algorithm to compute regional transformations and their spatial extents. Our approach produces dense estimates of complex symmetries that are combinations of translation, rotation, scale, and reflection under perspective distortion. This enables a number of automatic symmetry-aware image editing applications including inpainting, rectification, beautification, and segmentation, and we demonstrate state-of-the-art applications for each of them.",Nautilus: recovering regional symmetry transformations for image editing,NA:NA:NA:NA:NA:NA:NA,2017
Holly Rushmeier,NA,Session details: Rendering volumes,NA,2017
Roald Frederickx:Philip Dutré,"Rendering translucent materials with physically based Monte Carlo methods tends to be computationally expensive due to the long chains of volumetric scattering interactions. In the case of strongly forward scattering materials, the problem gets compounded since each scattering interaction becomes highly anisotropic and near-specular. Various well-known approaches try to avoid the resulting sampling problem through analytical approximations based on diffusion theory. Although these methods are computationally efficient, their assumption of diffusive, isotropic scattering can lead to considerable errors when rendering forward scattering materials, even in the optically dense limit. In this paper, we present an analytical subsurface scattering model, derived with the explicit assumption of strong forward scattering. Our model is not based on diffusion theory, but follows from a connection that we identified between the functional integral formulation of radiative transport and the partition function of a worm-like chain in polymer physics. Our resulting model does not need a separate Monte Carlo solution for unscattered or single-scattered contributions, nor does it require ad-hoc regularization procedures. It has a single singularity by design, corresponding to the initial unscattered propagation, which can be accounted for by the extensive analytical importance sampling scheme that we provide. Our model captures the full behaviour of forward scattering media, ranging from unscattered straight-line propagation to the fully diffusive limit. Moreover, we derive a novel forward scattering BRDF as limiting case of our subsurface scattering model, which can be used in a level of detail hierarchy. We show how our model can be integrated in existing Monte Carlo rendering algorithms, and make comparisons to previous approaches.",A forward scattering dipole model from a functional integral approximation,NA:NA,2017
Can Yuksel:Cem Yuksel,"Rendering explosions with self-illumination is a challenging problem. Explosions contain animated volumetric light sources immersed in animated smoke that cast volumetric shadows, which play an essential role and are expensive to compute. We propose an efficient solution that redefines this problem as rendering with many animated lights by converting the volumetric lighting data into a large number of point lights. Focusing on temporal coherency to avoid flickering in animations, we introduce lighting grid hierarchy for approximating the volumetric illumination at different resolutions. Using this structure we can efficiently approximate the lighting at any point inside or outside of the explosion volume as a mixture of lighting contributions from all levels of the hierarchy. As a result, we are able to capture high-frequency details of local illumination, as well as the potentially strong impact of distant illumination. Most importantly, this hierarchical structure allows us to efficiently precompute volumetric shadows, which substantially accelerates the lighting computation. Finally, we provide a scalable approach for computing the multiple scattering of light within the smoke volume using our lighting grid hierarchy. Temporal coherency is achieved by relying on continuous formulations at all stages of the lighting approximation. We show that our method is efficient and effective approximating the self-illumination of explosions with visually indistinguishable results, as compared to path tracing. We also show that our method can be applied to other problems involving a large number of (animated) point lights.",Lighting grid hierarchy for self-illuminating explosions,NA:NA,2017
Peter Kutz:Ralf Habel:Yining Karl Li:Jan Novák,"We present two novel unbiased techniques for sampling free paths in heterogeneous participating media. Our decomposition tracking accelerates free-path construction by splitting the medium into a control component and a residual component and sampling each of them separately. To minimize expensive evaluations of spatially varying collision coefficients, we define the control component to allow constructing free paths in closed form. The residual heterogeneous component is then homogenized by adding a fictitious medium and handled using weighted delta tracking, which removes the need for computing strict bounds of the extinction function. Our second contribution, spectral tracking, enables efficient light transport simulation in chromatic media. We modify free-path distributions to minimize the fluctuation of path throughputs and thereby reduce the estimation variance. To demonstrate the correctness of our algorithms, we derive them directly from the radiative transfer equation by extending the integral formulation of null-collision algorithms recently developed in reactor physics. This mathematical framework, which we thoroughly review, encompasses existing trackers and postulates an entire family of new estimators for solving transport problems; our algorithms are examples of such. We analyze the proposed methods in canonical settings and on production scenes, and compare to the current state of the art in simulating light transport in heterogeneous participating media.",Spectral and decomposition tracking for rendering heterogeneous volumes,NA:NA:NA:NA,2017
Benedikt Bitterli:Wojciech Jarosz,"We develop a theory of volumetric density estimation which generalizes prior photon point (0D) and beam (1D) approaches to a broader class of estimators using ""nD"" samples along photon and/or camera subpaths. Volumetric photon mapping performs density estimation by point sampling propagation distances within the medium and performing density estimation over the generated points (0D). Beam-based (1D) approaches consider the expected value of this distance sampling process along the last camera and/or light subpath segments. Our theory shows how to replace propagation distance sampling steps across multiple bounces to form higher-dimensional samples such as photon planes (2D), photon volumes (3D), their camera path equivalents, and beyond. We perform a theoretical error analysis which reveals that in scenarios where beams already outperform points, each additional dimension of nD samples compounds these benefits further. Moreover, each additional sample dimension reduces the required dimensionality of the blurring needed for density estimation, allowing us to formulate, for the first time, fully unbiased forms of volumetric photon mapping. We demonstrate practical implementations of several of the new estimators our theory predicts, including both biased and unbiased variants, and show that they outperform state-of-the-art beam-based volumetric photon mapping by a factor of 2.4--40×.",Beyond points and beams: higher-dimensional photon samples for volumetric light transport,NA:NA,2017
Mirela Ben-Chen,NA,Session details: Meshing,NA,2017
Amir Vaxman:Christian Müller:Ofir Weber,"We present a framework for designing shapes from diverse combinatorial patterns, where the vertex 1-rings and the faces are as rotationally symmetric as possible, and define such meshes as regular. Our algorithm computes the geometry that brings out the symmetries encoded in the combinatorics. We then allow designers and artists to envision and realize original meshes with great aesthetic qualities. Our method is general and applicable to meshes of arbitrary topology and connectivity, from triangle meshes to general polygonal meshes. The designer controls the result by manipulating and constraining vertex positions. We offer a novel characterization of regularity, using quaternionic ratios of mesh edges, and optimize meshes to be as regular as possible according to this characterization. Finally, we provide a mathematical analysis of these regular meshes, and show how they relate to concepts like the discrete Willmore energy and connectivity shapes.",Regular meshes from polygonal patterns,NA:NA:NA,2017
Xifeng Gao:Wenzel Jakob:Marco Tarini:Daniele Panozzo,"We propose a robust and efficient field-aligned volumetric meshing algorithm that produces hex-dominant meshes, i.e. meshes that are predominantly composed of hexahedral elements while containing a small number of irregular polyhedra. The latter are placed according to the singularities of two optimized guiding fields, which allow our method to generate meshes with an exceptionally high amount of isotropy. The field design phase of our method relies on a compact quaternionic representation of volumetric octa-fields and a corresponding optimization that explicitly models the discrete matchings between neighboring elements. This optimization naturally supports alignment constraints and scales to very large datasets. We also propose a novel extraction technique that uses field-guided mesh simplification to convert the optimized fields into a hexdominant output mesh. Each simplification operation maintains topological validity as an invariant, ensuring manifold output. These steps easily generalize to other dimensions or representations, and we show how they can be an asset in existing 2D surface meshing techniques. Our method can automatically and robustly convert any tetrahedral mesh into an isotropic hex-dominant mesh and (with minor modifications) can also convert any triangle mesh into a corresponding isotropic quad-dominant mesh, preserving its genus, number of holes, and manifoldness. We demonstrate the benefits of our algorithm on a large collection of shapes provided in the supplemental material along with all generated results.",Robust hex-dominant mesh generation using field-guided polyhedral agglomeration,NA:NA:NA:NA,2017
Dmitry Sokolov:Nicolas Ray:Lionel Untereiner:Bruno Lévy,"This article introduces a method that generates a hexahedral-dominant mesh from an input tetrahedral mesh. It follows a three-step pipeline similar to the one proposed by Carrier Baudoin et al.: (1) generate a frame field, (2) generate a pointset P that is mostly organized on a regular grid locally aligned with the frame field, and (3) generate the hexahedral-dominant mesh by recombining the tetrahedra obtained from the constrained Delaunay triangulation of P. For step (1), we use a state-of-the-art algorithm to generate a smooth frame field. For step (2), we introduce an extension of Periodic Global Parameterization to the volumetric case. As compared with other global parameterization methods (such as CubeCover), our method relaxes some global constraints to avoid creating degenerate elements, at the expense of introducing some singularities that are meshed using non-hexahedral elements. For step (3), we build on the formalism introduced by Meshkat and Talmor, fill in a gap in their proof, and provide a complete enumeration of all the possible recombinations, as well as an algorithm that efficiently detects all the matches in a tetrahedral mesh. The method is evaluated and compared with the state of the art on a database of examples with various mesh complexities, varying from academic examples to real industrial cases. Compared with the method of Carrier-Baudoin et al., the method results in better scores for classical quality criteria of hexahedral-dominant meshes (hexahedral proportion, scaled Jacobian, etc.). The method also shows better robustness than CubeCover and its derivatives when applied to complicated industrial models.",Hexahedral-Dominant Meshing,NA:NA:NA:NA,2017
Justin Solomon:Amir Vaxman:David Bommes,"The computation of smooth fields of orthogonal directions within a volume is a critical step in hexahedral mesh generation, used to guide placement of edges and singularities. While this problem shares high-level structure with surface-based frame field problems, critical aspects are lost when extending to volumes, while new structure from the flat Euclidean metric emerges. Taking these considerations into account, this article presents an algorithm for computing such “octahedral” fields. Unlike existing approaches, our formulation achieves infinite resolution in the interior of the volume via the boundary element method (BEM), continuously assigning frames to points in the interior from only a triangle mesh discretization of the boundary. The end result is an orthogonal direction field that can be sampled anywhere inside the mesh, with smooth variation and singular structure in the interior, even with a coarse boundary. We illustrate our computed frames on a number of challenging test geometries. Since the octahedral frame field problem is relatively new, we also contribute a thorough discussion of theoretical and practical challenges unique to this problem.",Boundary Element Octahedral Fields in Volumes,NA:NA:NA,2017
Ming Lin,NA,Session details: Sound & elastics,NA,2017
Carl Schissler:Dinesh Manocha,"We present an approach to generate plausible acoustic effects at interactive rates in large dynamic environments containing many sound sources. Our formulation combines listener-based backward ray tracing with sound source clustering and hybrid audio rendering to handle complex scenes. We present a new algorithm for dynamic late reverberation that performs high-order ray tracing from the listener against spherical sound sources. We achieve sublinear scaling with the number of sources by clustering distant sound sources and taking relative visibility into account. We also describe a hybrid convolution-based audio rendering technique that can process hundreds of thousands of sound paths at interactive rates. We demonstrate the performance on many indoor and outdoor scenes with up to 200 sound sources. In practice, our algorithm can compute more than 50 reflection orders at interactive rates on a multicore PC, and we observe a 5x speedup over prior geometric sound propagation algorithms.",Interactive Sound Propagation and Rendering for Large Multi-Source Scenes,NA:NA,2017
Eston Schweickart:Doug L. James:Steve Marschner,"Sound generation methods, such as linear modal synthesis, can sonify a wide range of physics-based animation of solid objects, resolving vibrations and sound radiation from various structures. However, elastic rods are an important computer animation primitive for which prior sound synthesis methods, such as modal synthesis, are ill-suited for several reasons: large displacements, nonlinear vibrations, dispersion effects, and the geometrically singular nature of rods. In this paper, we present physically based methods for simultaneous generation of animation and sound for deformable rods. We draw on Kirchhoff theory to simplify the representation of rod dynamics and introduce a generalized dipole model to calculate the spatially varying acoustic radiation. In doing so, we drastically decrease the amount of precomputation required (in some cases eliminating it completely), while being able to resolve sound radiation for arbitrary body deformations encountered in computer animation. We present several examples, including challenging scenes involving thousands of highly coupled frictional contacts.",Animating elastic rods with sound,NA:NA:NA,2017
Dominik L. Michels:Vu Thai Luan:Mayya Tokman,"We present a new integration algorithm for the accurate and efficient solution of stiff elastodynamic problems governed by the second-order ordinary differential equations of structural mechanics. Current methods have the shortcoming that their performance is highly dependent on the numerical stiffness of the underlying system that often leads to unrealistic behavior or a significant loss of efficiency. To overcome these limitations, we present a new integration method which is based on a mathematical reformulation of the underlying differential equations, an exponential treatment of the full nonlinear forcing operator as opposed to more standard partially implicit or exponential approaches, and the utilization of the concept of stiff accuracy which ensures that the efficiency of the simulations is significantly less sensitive to increased stiffness. As a consequence, we are able to tremendously accelerate the simulation of stiff systems compared to established integrators and significantly increase the overall accuracy. The advantageous behavior of this approach is demonstrated on a broad spectrum of complex examples like deformable bodies, textiles, bristles, and human hair. Our easily parallelizable integrator enables more complex and realistic models to be explored in visual computing without compromising efficiency.",A stiffly accurate integrator for elastodynamic problems,NA:NA:NA,2017
Tiantian Liu:Sofien Bouaziz:Ladislav Kavan,"We present a new method for real-time physics-based simulation supporting many different types of hyperelastic materials. Previous methods such as Position-Based or Projective Dynamics are fast but support only a limited selection of materials; even classical materials such as the Neo-Hookean elasticity are not supported. Recently, Xu et al. [2015] introduced new “spline-based materials” that can be easily controlled by artists to achieve desired animation effects. Simulation of these types of materials currently relies on Newton’s method, which is slow, even with only one iteration per timestep. In this article, we show that Projective Dynamics can be interpreted as a quasi-Newton method. This insight enables very efficient simulation of a large class of hyperelastic materials, including the Neo-Hookean, spline-based materials, and others. The quasi-Newton interpretation also allows us to leverage ideas from numerical optimization. In particular, we show that our solver can be further accelerated using L-BFGS updates (Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm). Our final method is typically more than 10 times faster than one iteration of Newton’s method without compromising quality. In fact, our result is often more accurate than the result obtained with one iteration of Newton’s method. Our method is also easier to implement, implying reduced software development costs.",Quasi-Newton Methods for Real-Time Simulation of Hyperelastic Materials,NA:NA:NA,2017
Aaron Hertzmann,NA,Session details: Deep image processing,NA,2017
Chengze Li:Xueting Liu:Tien-Tsin Wong,"Extraction of structural lines from pattern-rich manga is a crucial step for migrating legacy manga to digital domain. Unfortunately, it is very challenging to distinguish structural lines from arbitrary, highly-structured, and black-and-white screen patterns. In this paper, we present a novel data-driven approach to identify structural lines out of pattern-rich manga, with no assumption on the patterns. The method is based on convolutional neural networks. To suit our purpose, we propose a deep network model to handle the large variety of screen patterns and raise output accuracy. We also develop an efficient and effective way to generate a rich set of training data pairs. Our method suppresses arbitrary screen patterns no matter whether these patterns are regular, irregular, tone-varying, or even pictorial, and regardless of their scales. It outputs clear and smooth structural lines even if these lines are contaminated by and immersed in complex patterns. We have evaluated our method on a large number of mangas of various drawing styles. Our method substantially outperforms state-of-the-art methods in terms of visual quality. We also demonstrate its potential in various manga applications, including manga colorization, manga retargeting, and 2.5D manga generation.",Deep extraction of manga structural lines,NA:NA:NA,2017
Michaël Gharbi:Jiawen Chen:Jonathan T. Barron:Samuel W. Hasinoff:Frédo Durand,"Performance is a critical challenge in mobile image processing. Given a reference imaging pipeline, or even human-adjusted pairs of images, we seek to reproduce the enhancements and enable real-time evaluation. For this, we introduce a new neural network architecture inspired by bilateral grid processing and local affine color transforms. Using pairs of input/output images, we train a convolutional neural network to predict the coefficients of a locally-affine model in bilateral space. Our architecture learns to make local, global, and content-dependent decisions to approximate the desired image transformation. At runtime, the neural network consumes a low-resolution version of the input image, produces a set of affine transformations in bilateral space, upsamples those transformations in an edge-preserving fashion using a new slicing node, and then applies those upsampled transformations to the full-resolution image. Our algorithm processes high-resolution images on a smartphone in milliseconds, provides a real-time viewfinder at 1080p resolution, and matches the quality of state-of-the-art approximation techniques on a large class of image operators. Unlike previous work, our model is trained off-line from data and therefore does not require access to the original operator at runtime. This allows our model to learn complex, scene-dependent transformations for which no reference implementation is available, such as the photographic edits of a human retoucher.",Deep bilateral learning for real-time image enhancement,NA:NA:NA:NA:NA,2017
Richard Zhang:Jun-Yan Zhu:Phillip Isola:Xinyang Geng:Angela S. Lin:Tianhe Yu:Alexei A. Efros,"We propose a deep learning approach for user-guided image colorization. The system directly maps a grayscale image, along with sparse, local user ""hints"" to an output colorization with a Convolutional Neural Network (CNN). Rather than using hand-defined rules, the network propagates user edits by fusing low-level cues along with high-level semantic information, learned from large-scale data. We train on a million images, with simulated user inputs. To guide the user towards efficient input selection, the system recommends likely colors based on the input image and current user inputs. The colorization is performed in a single feed-forward pass, enabling real-time use. Even with randomly simulated user inputs, we show that the proposed system helps novice users quickly create realistic colorizations, and offers large improvements in colorization quality with just a minute of use. In addition, we demonstrate that the framework can incorporate other user ""hints"" to the desired colorization, showing an application to color histogram transfer.",Real-time user-guided image colorization with learned deep priors,NA:NA:NA:NA:NA:NA:NA,2017
Jing Liao:Yuan Yao:Lu Yuan:Gang Hua:Sing Bing Kang,"We propose a new technique for visual attribute transfer across images that may have very different appearance but have perceptually similar semantic structure. By visual attribute transfer, we mean transfer of visual information (such as color, tone, texture, and style) from one image to another. For example, one image could be that of a painting or a sketch while the other is a photo of a real scene, and both depict the same type of scene. Our technique finds semantically-meaningful dense correspondences between two input images. To accomplish this, it adapts the notion of ""image analogy"" [Hertzmann et al. 2001] with features extracted from a Deep Convolutional Neutral Network for matching; we call our technique deep image analogy. A coarse-to-fine strategy is used to compute the nearest-neighbor field for generating the results. We validate the effectiveness of our proposed method in a variety of cases, including style/texture transfer, color/style swap, sketch/painting to photo, and time lapse.",Visual attribute transfer through deep image analogy,NA:NA:NA:NA:NA,2017
Tom Duff,"Deep compositing is an important practical tool in creating digital imagery, but there has been little theoretical analysis of the underlying mathematical operators. Motivated by finding a simple formulation of the merging operation on OpenEXR-style deep images, we show that the Porter-Duff over function is the operator of a Lie group. In its corresponding Lie algebra, the splitting and mixing functions that OpenEXR deep merging requires have a particularly simple form. Working in the Lie algebra, we present a novel, simple proof of the uniqueness of the mixing function. The Lie group structure has many more applications, including new, correct resampling algorithms for volumetric images with alpha channels, and a deep image compression technique that outperforms that of OpenEXR.",Deep Compositing Using Lie Algebras,NA,2017
Bernhard Thomaszewski,NA,Session details: Fabricating look & feel,NA,2017
Bo Zhu:Mélina Skouras:Desai Chen:Wojciech Matusik,"In this article, we present a novel two-scale framework to optimize the structure and the material distribution of an object given its functional specifications. Our approach utilizes multi-material microstructures as low-level building blocks of the object. We start by precomputing the material property gamut—the set of bulk material properties that can be achieved with all material microstructures of a given size. We represent the boundary of this material property gamut using a level set field. Next, we propose an efficient and general topology optimization algorithm that simultaneously computes an optimal object topology and spatially varying material properties constrained by the precomputed gamut. Finally, we map the optimal spatially varying material properties onto the microstructures with the corresponding properties to generate a high-resolution printable structure. We demonstrate the efficacy of our framework by designing, optimizing, and fabricating objects in different material property spaces on the level of a trillion voxels, that is, several orders of magnitude higher than what can be achieved with current systems.",Two-Scale Topology Optimization with Microstructures,NA:NA:NA:NA,2017
Jonàs Martínez:Haichuan Song:Jérémie Dumas:Sylvain Lefebvre,"Additive manufacturing enables the fabrication of objects embedding meta-materials. By creating fine-scale structures, the object's physical properties can be graded (e.g. elasticity, porosity), even though a single base material is used for fabrication. Designing the fine and detailed geometry of a metamaterial while attempting to achieve specific properties is difficult. In addition, the structures are intended to fill comparatively large volumes, which quickly leads to large data structures and intractable simulation costs. Thus, most metamaterials are defined as periodic structures repeated in regular lattices. The periodicity simplifies modeling, simulation, and reduces memory costs - however it limits the possibility to smoothly grade properties along free directions. In this work, we propose a novel metamaterial with controllable, freely orientable, orthotropic elastic behavior - orthotropy means that elasticity is controlled independently along three orthogonal axes, which leads to materials that better adapt to uneven, directional load scenarios, and offer a more versatile material design primitive. The fine-scale structures are generated procedurally by a stochastic process, and resemble a foam. The absence of global organization and periodicity allows the free gradation of density, orientation, and stretch, leading to the controllable orthotropic behavior. The procedural nature of the synthesis process allows it to scale to arbitrarily large volumes at low memory costs. We detail the foam structure synthesis, analyze and discuss its properties through numerical and experimental verifications, and finally demonstrate the use of orthotropic materials for the design of 3D printed objects.",Orthotropic k-nearest foams for additive manufacturing,NA:NA:NA:NA,2017
Julian Panetta:Abtin Rahimian:Denis Zorin,"Additive fabrication technologies are limited by the types of material they can print: while the technologies are continuously improving, still only a relatively small discrete set of materials can be used in each printed object. At the same time, the low cost of introducing geometric complexity suggests the alternative of controlling the elastic material properties by producing microstructures, which can achieve behaviors significantly differing from the solid printing material. While promising results have been obtained in this direction, fragility is a significant problem blocking practical applications, especially for achieving soft material properties: due to stress concentrations at thin joints, deformations and repeated loadings are likely to cause fracture. We present a set of methods to minimize stress concentrations in microstructures by evolving their shapes. First, we demonstrate that the worst-case stress analysis problem (maximizing a stress measure over all possible unit loads) has an exact solution for periodic microstructures. We develop a new, accurate discretization of the shape derivative for stress objectives and introduce a low-dimensional parametric shape model for microstructures. This model supports robust minimization of maximal stress (approximated by an Lp norm with high p) and an efficient implementation of printability constraints. In addition to significantly reducing stresses (by a typical factor of 5X), the new method substantially expands the range of effective material properties covered by the collection of structures.",Worst-case stress relief for microstructures,NA:NA:NA,2017
Thiago Pereira:Carolina L. A. Paes Leme:Steve Marschner:Szymon Rusinkiewicz,"The ability to fabricate surfaces with fine control over bidirectional reflectance (BRDF) is a long-standing goal in appearance research, with applications in product design and manufacturing. We propose a technique that embeds magnetic flakes in a photo-cured resin, allowing the orientation distribution of those flakes to be controlled at printing time using a magnetic field. We show that time-varying magnetic fields allow us to control off-specular lobe direction, anisotropy, and lobe width, while using multiple spatial masks displayed by a UV projector allows for spatial variation. We demonstrate optical effects including bump maps: fat surfaces with spatially-varying specular lobe direction.",Printing anisotropic appearance with magnetic flakes,NA:NA:NA:NA,2017
Vahid Babaei:Kiril Vidimče:Michael Foshey:Alexandre Kaspar:Piotr Didyk:Wojciech Matusik,"Appearance reproduction is an important aspect of 3D printing. Current color reproduction systems use halftoning methods that create colors through a spatial combination of different inks at the object's surface. This introduces a variety of artifacts to the object, especially when viewed from a closer distance. In this work, we propose an alternative color reproduction method for 3D printing. Inspired by the inherent ability of 3D printers to layer different materials on top of each other, 3D color contoning creates colors by combining inks with various thicknesses inside the object's volume. Since inks are inside the volume, our technique results in a uniform color surface with virtually invisible spatial patterns on the surface. For color prediction, we introduce a simple and highly accurate spectral model that relies on a weighted regression of spectral absorptions. We fully characterize the proposed framework by addressing a number of problems, such as material arrangement, calculation of ink concentration, and 3D dot gain. We use a custom 3D printer to fabricate and validate our results.",Color contoning for 3D printing,NA:NA:NA:NA:NA:NA,2017
Yotam Gingold,NA,Session details: Sketching & curves,NA,2017
Changjian Li:Hao Pan:Yang Liu:Xin Tong:Alla Sheffer:Wenping Wang,"Sketch-based modeling provides a powerful paradigm for geometric modeling. Recent research had shown, sketch based modeling methods are most effective when targeting a specific family of surfaces. A large and growing arsenal of sketching tools is available for different types of geometries and different target user populations. Our work augments this arsenal with a new and powerful tool for modeling complex freeform shapes by sketching sparse 2D strokes; our method complements existing approaches in enabling the generation of surfaces with complex curvature patterns that are challenging to produce with existing methods. To model a desired surface patch with our technique, the user sketches the patch boundary as well as a small number of strokes representing the major bending directions of the shape. Our method uses this input to generate a curvature field that conforms to the user strokes and then uses this field to derive a freeform surface with the desired curvature pattern. To infer the surface from the strokes we first disambiguate the convex versus concave bending directions indicated by the strokes and estimate the surface bending magnitude along the strokes. We subsequently construct a curvature field based on these estimates, using a non-orthogonal 4-direction field coupled with a scalar magnitude field, and finally construct a surface whose curvature pattern reflects this field through an iterative sequence of simple linear optimizations. Our framework is well suited for single-view modeling, but also supports multi-view interaction, necessary to model complex shapes portions of which can be occluded in many views. It effectively combines multi-view inputs to obtain a coherent 3D shape. It runs at interactive speed allowing for immediate user feedback. We demonstrate the effectiveness of the proposed method through a large collection of complex examples created by both artists and amateurs. Our framework provides a useful complement to the existing sketch-based modeling methods.",BendSketch: modeling freeform surfaces through 2D sketching,NA:NA:NA:NA:NA:NA,2017
Xiaoguang Han:Chang Gao:Yizhou Yu,"Face modeling has been paid much attention in the field of visual computing. There exist many scenarios, including cartoon characters, avatars for social media, 3D face caricatures as well as face-related art and design, where low-cost interactive face modeling is a popular approach especially among amateur users. In this paper, we propose a deep learning based sketching system for 3D face and caricature modeling. This system has a labor-efficient sketching interface, that allows the user to draw freehand imprecise yet expressive 2D lines representing the contours of facial features. A novel CNN based deep regression network is designed for inferring 3D face models from 2D sketches. Our network fuses both CNN and shape based features of the input sketch, and has two independent branches of fully connected layers generating independent subsets of coefficients for a bilinear face representation. Our system also supports gesture based interactions for users to further manipulate initial face models. Both user studies and numerical results indicate that our sketching system can help users create face models quickly and effectively. A significantly expanded face database with diverse identities, expressions and levels of exaggeration is constructed to promote further research and evaluation of face modeling techniques.",DeepSketch2Face: a deep learning based sketching system for 3D face and caricature modeling,NA:NA:NA,2017
Marek Dvorožňák:Pierre Bénard:Pascal Barla:Oliver Wang:Daniel Sýkora,"We present a novel approach to facilitate the creation of stylized 2D rigid body animations. Our approach can handle multiple rigid objects following complex physically-simulated trajectories with collisions, while retaining a unique artistic style directly specified by the user. Starting with an existing target animation (e.g., produced by a physical simulation engine) an artist interactively draws over a sparse set of frames, and the desired appearance and motion stylization is automatically propagated to the rest of the sequence. The stylization process may also be performed in an off-line batch process from a small set of drawn sequences. To achieve these goals, we combine parametric deformation synthesis that generalizes and reuses hand-drawn exemplars, with non-parametric techniques that enhance the hand-drawn appearance of the synthesized sequence. We demonstrate the potential of our method on various complex rigid body animations which are created with an expressive hand-drawn look using notably less manual interventions as compared to traditional techniques.",Example-based expressive animation of 2D rigid bodies,NA:NA:NA:NA:NA,2017
Vojtěch Krs:Ersin Yumer:Nathan Carr:Bedrich Benes:Radomír Měch,"We introduce Skippy, a novel algorithm for 3D interactive curve modeling from a single view. While positing curves in space can be a tedious task, our rapid sketching algorithm allows users to draw curves in and around existing geometry in a controllable manner. The key insight behind our system is to automatically infer the 3D curve coordinates by enumerating a large set of potential curve trajectories. More specifically, we partition 2D strokes into continuous segments that land both on and off the geometry, duplicating segments that could be placed in front or behind, to form a directed graph. We use distance fields to estimate 3D coordinates for our curve segments and solve for an optimally smooth path that follows the curvature of the scene geometry while avoiding intersections. Using our curve design framework we present a collection of novel editing operations allowing artists to rapidly explore and refine the combinatorial space of solutions. Furthermore, we include the quick placement of transient geometry to aid in guiding the 3D curve. Finally we demonstrate our interactive design curve system on a variety of applications including geometric modeling, and camera motion path planning.",Skippy: single view 3D curve interactive modeling,NA:NA:NA:NA:NA,2017
Zhipei Yan:Stephen Schiller:Gregg Wilensky:Nathan Carr:Scott Schaefer,"We present a method for constructing almost-everywhere curvature-continuous, piecewise-quadratic curves that interpolate a list of control points and have local maxima of curvature only at the control points. Our premise is that salient features of the curve should occur only at control points to avoid the creation of features unintended by the artist. While many artists prefer to use interpolated control points, the creation of artifacts, such as loops and cusps, away from control points has limited the use of these types of curves. By enforcing the maximum curvature property, loops and cusps cannot be created unless the artist intends for them to be. To create such curves, we focus on piecewise quadratic curves, which can have only one maximum curvature point. We provide a simple, iterative optimization that creates quadratic curves, one per interior control point, that meet with G2 continuity everywhere except at inflection points of the curve where the curves are G1. Despite the nonlinear nature of curvature, our curves only obtain local maxima of the absolute value of curvature only at interpolated control points.",k-curves: interpolation at local maximum curvature,NA:NA:NA:NA:NA,2017
Kayvon Fatahalian,NA,Session details: Video,NA,2017
Mackenzie Leake:Abe Davis:Anh Truong:Maneesh Agrawala,"We present a system for efficiently editing video of dialogue-driven scenes. The input to our system is a standard film script and multiple video takes, each capturing a different camera framing or performance of the complete scene. Our system then automatically selects the most appropriate clip from one of the input takes, for each line of dialogue, based on a user-specified set of film-editing idioms. Our system starts by segmenting the input script into lines of dialogue and then splitting each input take into a sequence of clips time-aligned with each line. Next, it labels the script and the clips with high-level structural information (e.g., emotional sentiment of dialogue, camera framing of clip, etc.). After this pre-process, our interface offers a set of basic idioms that users can combine in a variety of ways to build custom editing styles. Our system encodes each basic idiom as a Hidden Markov Model that relates editing decisions to the labels extracted in the pre-process. For short scenes (< 2 minutes, 8--16 takes, 6--27 lines of dialogue) applying the user-specified combination of idioms to the pre-processed inputs generates an edited sequence in 2--3 seconds. We show that this is significantly faster than the hours of user time skilled editors typically require to produce such edits and that the quick feedback lets users iteratively explore the space of edit designs.",Computational video editing for dialogue-driven scenes,NA:NA:NA:NA,2017
Zhaopeng Cui:Oliver Wang:Ping Tan:Jue Wang,"Time slice photography is a popular effect that visualizes the passing of time by aligning and stitching multiple images capturing the same scene at different times together into a single image. Extending this effect to video is a difficult problem, and one where existing solutions have only had limited success. In this paper, we propose an easy-to-use and robust system for creating time slice videos from a wide variety of consumer videos. The main technical challenge we address is how to align videos taken at different times with substantially different appearances, in the presence of moving objects and moving cameras with slightly different trajectories. To achieve a temporally stable alignment, we perform a mixed 2D-3D alignment, where a rough 3D reconstruction is used to generate sparse constraints that are integrated into a pixelwise 2D registration. We apply our method to a number of challenging scenarios, and show that we can achieve a higher quality registration than prior work. We propose a 3D user interface that allows the user to easily specify how multiple videos should be composited in space and time. Finally, we show that our alignment method can be applied in more general video editing and compositing tasks, such as object removal.",Time slice video synthesis by robust video alignment,NA:NA:NA:NA,2017
Tobias Nägeli:Lukas Meier:Alexander Domahidi:Javier Alonso-Mora:Otmar Hilliges,"We propose a method for automated aerial videography in dynamic and cluttered environments. An online receding horizon optimization formulation facilitates the planning process for novices and experts alike. The algorithm takes high-level plans as input, which we dub virtual rails, alongside interactively defined aesthetic framing objectives and jointly solves for 3D quadcopter motion plans and associated velocities. The method generates control inputs subject to constraints of a non-linear quadrotor model and dynamic constraints imposed by actors moving in an a priori unknown way. The output plans are physically feasible, for the horizon length, and we apply the resulting control inputs directly at each time-step, without requiring a separate trajectory tracking algorithm. The online nature of the method enables incorporation of feedback into the planning and control loop, makes the algorithm robust to disturbances. Furthermore, we extend the method to include coordination between multiple drones to enable dynamic multi-view shots, typical for action sequences and live TV coverage. The algorithm runs in real-time on standard hardware and computes motion plans for several drones in the order of milliseconds. Finally, we evaluate the approach qualitatively with a number of challenging shots, involving multiple drones and actors and qualitatively characterize the computational performance experimentally.",Real-time planning for automated multi-view drone cinematography,NA:NA:NA:NA:NA,2017
Ting-Chun Wang:Jun-Yan Zhu:Nima Khademi Kalantari:Alexei A. Efros:Ravi Ramamoorthi,"Light field cameras have many advantages over traditional cameras, as they allow the user to change various camera settings after capture. However, capturing light fields requires a huge bandwidth to record the data: a modern light field camera can only take three images per second. This prevents current consumer light field cameras from capturing light field videos. Temporal interpolation at such extreme scale (10x, from 3 fps to 30 fps) is infeasible as too much information will be entirely missing between adjacent frames. Instead, we develop a hybrid imaging system, adding another standard video camera to capture the temporal information. Given a 3 fps light field sequence and a standard 30 fps 2D video, our system can then generate a full light field video at 30 fps. We adopt a learning-based approach, which can be decomposed into two steps: spatio-temporal flow estimation and appearance estimation. The flow estimation propagates the angular information from the light field sequence to the 2D video, so we can warp input images to the target view. The appearance estimation then combines these warped images to output the final pixels. The whole process is trained end-to-end using convolutional neural networks. Experimental results demonstrate that our algorithm outperforms current video interpolation methods, enabling consumer light field videography, and making applications such as refocusing and parallax view generation achievable on videos for the first time.",Light field video capture using a learning-based hybrid imaging system,NA:NA:NA:NA:NA,2017
Ken Anjyo,NA,Session details: Simulation for virtual worlds,NA,2017
Ignacio Garcia-Dorado:Daniel G. Aliaga:Saiprasanth Bhalachandran:Paul Schmid:Dev Niyogi,"We present the first realistic, physically based, fully coupled, real-time weather design tool for use in urban procedural modeling. We merge designing of a 3D urban model with a controlled long-lasting spatiotemporal interactive simulation of weather. Starting from the fundamental dynamical equations similar to those used in state-of-the-art weather models, we present a novel simplified urban weather model for interactive graphics. Control of physically based weather phenomena is accomplished via an inverse modeling methodology. In our results, we present several scenarios of forward design, inverse design with high-level and detailed-level weather control and optimization, and comparisons of our method against well-known weather simulation results and systems.",Fast Weather Simulation for Inverse Procedural Design of 3D Urban Models,NA:NA:NA:NA:NA,2017
Guillaume Cordonnier:Eric Galin:James Gain:Bedrich Benes:Eric Guérin:Adrien Peytavie:Marie-Paule Cani,"We introduce a novel framework for interactive landscape authoring that supports bi-directional feedback between erosion and vegetation simulation. Vegetation and terrain erosion have strong mutual impact and their interplay influences the overall realism of virtual scenes. Despite their importance, these complex interactions have been neglected in computer graphics. Our framework overcomes this by simulating the effect of a variety of geomorphological agents and the mutual interaction between different material and vegetation layers, including rock, sand, humus, grass, shrubs, and trees. Users are able to exploit these interactions with an authoring interface that consistently shapes the terrain and populates it with details. Our method, validated through side-by-side comparison with real terrains, can be used not only to generate realistic static landscapes, but also to follow the temporal evolution of a landscape over a few centuries.",Authoring landscapes by combining ecosystem and terrain erosion simulation,NA:NA:NA:NA:NA:NA:NA,2017
Bohan Wang:Yili Zhao:Jernej Barbič,"Botanical simulation plays an important role in many fields including visual effects, games and virtual reality. Previous plant simulation research has focused on computing physically based motion, under the assumption that the material properties are known. It is too tedious and impractical to manually set the spatially-varying material properties of complex trees. In this paper, we give a method to set the mass density, stiffness and damping properties of individual tree components (branches and leaves) using a small number of intuitive parameters. Our method is rooted in plant biomechanics literature and builds upon power laws observed in real botanical systems. We demonstrate our materials by simulating them using offline and model-reduced FEM simulators. Our parameters can be tuned directly by artists; but we also give a technique to infer the parameters from ground truth videos of real trees. Our materials produce tree animations that look much more similar to real trees than previous methods, as evidenced by our user study and experiments.",Botanical materials based on biomechanics,NA:NA:NA,2017
Ioannis Karamouzas:Nick Sohre:Rahul Narain:Stephen J. Guy,"Large multi-agent systems such as crowds involve inter-agent interactions that are typically anticipatory in nature, depending strongly on both the positions and the velocities of agents. We show how the nonlinear, anticipatory forces seen in multi-agent systems can be made compatible with recent work on energy-based formulations in physics-based animation, and propose a simple and effective optimization-based integration scheme for implicit integration of such systems. We apply this approach to crowd simulation by using a state-of-the-art model derived from a recent analysis of human crowd data, and adapting it to our framework. Our approach provides, for the first time, guaranteed collision-free motion while simultaneously maintaining high-quality collective behavior in a way that is insensitive to simulation parameters such as time step size and crowd density. These benefits are demonstrated through simulation results on various challenging scenarios and validation against real-world crowd data.",Implicit crowds: optimization integrator for robust crowd simulation,NA:NA:NA:NA,2017
Li-Yi Wei,NA,Session details: Random sampling,NA,2017
Gurprit Singh:Wojciech Jarosz,"Traditional Monte Carlo (MC) integration methods use point samples to numerically approximate the underlying integral. This approximation introduces variance in the integrated result, and this error can depend critically on the sampling patterns used during integration. Most of the well-known samplers used for MC integration in graphics---e.g. jittered, Latin-hypercube (N-rooks), multijittered---are anisotropic in nature. However, there are currently no tools available to analyze the impact of such anisotropic samplers on the variance convergence behavior of Monte Carlo integration. In this work, we develop a Fourier-domain mathematical tool to analyze the variance, and subsequently the convergence rate, of Monte Carlo integration using any arbitrary (anisotropic) sampling power spectrum. We also validate and leverage our theoretical analysis, demonstrating that judicious alignment of anisotropic sampling and integrand spectra can improve variance and convergence rates in MC rendering, and that similar improvements can apply to (anisotropic) deterministic samplers.",Convergence analysis for anisotropic monte carlo sampling spectra,NA:NA,2017
Hongxing Qin:Yi Chen:Jinlong He:Baoquan Chen,"In this article, we present a multi-class blue noise sampling algorithm by throwing samples as the constrained Wasserstein barycenter of multiple density distributions. Using an entropic regularization term, a constrained transport plan in the optimal transport problem is provided to break the partition required by the previous Capacity-Constrained Voronoi Tessellation method. The entropic regularization term cannot only control spatial regularity of blue noise sampling, but it also reduces conflicts between the desired centroids of Vornoi cells for multi-class sampling. Moreover, the adaptive blue noise property is guaranteed for each individual class, as well as their combined class. Our method can be easily extended to multi-class sampling on a point set surface. We also demonstrate applications in object distribution and color stippling.",Wasserstein Blue Noise Sampling,NA:NA:NA:NA,2017
Abdalla G. M. Ahmed:Till Niese:Hui Huang:Oliver Deussen,"We present a framework to distribute point samples with controlled spectral properties using a regular lattice of tiles with a single sample per tile. We employ a word-based identification scheme to identify individual tiles in the lattice. Our scheme is recursive, permitting tiles to be subdivided into smaller tiles that use the same set of IDs. The corresponding framework offers a very simple setup for optimization towards different spectral properties. Small lookup tables are sufficient to store all the information needed to produce different point sets. For blue noise with varying densities, we employ the bit-reversal principle to recursively traverse sub-tiles. Our framework is also capable of delivering multi-class blue noise samples. It is well-suited for different sampling scenarios in rendering, including area-light sampling (uniform and adaptive), and importance sampling. Other applications include stippling and distributing objects.",An adaptive point sampler on a regular lattice,NA:NA:NA:NA,2017
Jonathan Dupuy:Eric Heitz:Laurent Belcour,"We introduce a novel parameterization for spherical distributions that is based on a point located inside the sphere, which we call a pivot. The pivot serves as the center of a straight-line projection that maps solid angles onto the opposite side of the sphere. By transforming spherical distributions in this way, we derive novel parametric spherical distributions that can be evaluated and importance-sampled from the original distributions using simple, closed-form expressions. Moreover, we prove that if the original distribution can be sampled and/or integrated over a spherical cap, then so can the transformed distribution. We exploit the properties of our parameterization to derive efficient spherical lighting techniques for both real-time and offline rendering. Our techniques are robust, fast, easy to implement, and achieve quality that is superior to previous work.",A spherical cap preserving parameterization for spherical distributions,NA:NA:NA,2017
Rahul Narain,NA,Session details: Fluids III,NA,2017
Jieyu Chu:Nafees Bin Zafar:Xubo Yang,"We present an algorithmically efficient and parallelized domain decomposition based approach to solving Poisson’s equation on irregular domains. Our technique employs the Schur complement method, which permits a high degree of parallel efficiency on multicore systems. We create a novel Schur complement preconditioner which achieves faster convergence, and requires less computation time and memory. This domain decomposition method allows us to apply different linear solvers for different regions of the flow. Subdomains with regular boundaries can be solved with an FFT-based Fast Poisson Solver. We can solve systems with 1,0243 degrees of freedom, and demonstrate its use for the pressure projection step of incompressible liquid and gas simulations. The results demonstrate considerable speedup over preconditioned conjugate gradient methods commonly employed to solve such problems, including a multigrid preconditioned conjugate gradient method.",A Schur Complement Preconditioner for Scalable Parallel Fluid Simulation,NA:NA:NA,2017
Mridul Aanjaneya:Ming Gao:Haixiang Liu:Christopher Batty:Eftychios Sifakis,"We present an efficient and scalable octree-inspired fluid simulation framework with the flexibility to leverage adaptivity in any part of the computational domain, even when resolution transitions reach the free surface. Our methodology ensures symmetry, definiteness and second order accuracy of the discrete Poisson operator, and eliminates numerical and visual artifacts of prior octree schemes. This is achieved by adapting the operators acting on the octree's simulation variables to reflect the structure and connectivity of a power diagram, which recovers primal-dual mesh orthogonality and eliminates problematic T-junction configurations. We show how such operators can be efficiently implemented using a pyramid of sparsely populated uniform grids, enhancing the regularity of operations and facilitating parallelization. A novel scheme is proposed for encoding the topology of the power diagram in the neighborhood of each octree cell, allowing us to locally reconstruct it on the fly via a lookup table, rather than resorting to costly explicit meshing. The pressure Poisson equation is solved via a highly efficient, matrix-free multigrid preconditioner for Conjugate Gradient, adapted to the power diagram discretization. We use another sparsely populated uniform grid for high resolution interface tracking with a narrow band level set representation. Using the recently introduced SPGrid data structure, sparse uniform grids in both the power diagram discretization and our narrow band level set can be compactly stored and efficiently updated via streaming operations. Additionally, we present enhancements to adaptive level set advection, velocity extrapolation, and the fast marching method for redistancing. Our overall framework gracefully accommodates the task of dynamically adapting the octree topology during simulation. We demonstrate end-to-end simulations of complex adaptive flows in irregularly shaped domains, with tens of millions of degrees of freedom.",Power diagrams and sparse paged grids for high resolution adaptive liquids,NA:NA:NA:NA:NA,2017
Tobias Günther:Markus Gross:Holger Theisel,"In flow visualization, vortex extraction is a long-standing and unsolved problem. For decades, scientists developed numerous definitions that characterize vortex regions and their corelines in different ways, but none emerged as ultimate solution. One reason is that almost all techniques have a fundamental weakness: they are not invariant under changes of the reference frame, i.e., they are not objective. This has two severe implications: First, the result depends on the movement of the observer, and second, they cannot track vortices that are moving on arbitrary paths, which limits their reliability and usefulness in practice. Objective measures are rare, but recently gained more attention in the literature. Instead of only introducing a new objective measure, we show in this paper how all existing measures that are based on velocity and its derivatives can be made objective. We achieve this by observing the vector field in optimal local reference frames, in which the temporal derivative of the flow vanishes, i.e., reference frames in which the flow appears steady. The central contribution of our paper is to show that these optimal local reference frames can be found by a simple and elegant linear optimization. We prove that in the optimal frame, all local vortex extraction methods that are based on velocity and its derivatives become objective. We demonstrate our approach with objective counterparts to λ2, vorticity and Sujudi-Haimes.",Generic objective vortices for flow visualization,NA:NA:NA,2017
Albert Chern:Felix Knöppel:Ulrich Pinkall:Peter Schröder,"Clebsch maps encode velocity fields through functions. These functions contain valuable information about the velocity field. For example, closed integral curves of the associated vorticity field are level lines of the vorticity Clebsch map. This makes Clebsch maps useful for visualization and fluid dynamics analysis. Additionally they can be used in the context of simulations to enhance flows through the introduction of subgrid vorticity. In this paper we study spherical Clebsch maps, which are particularly attractive. Elucidating their geometric structure, we show that such maps can be found as minimizers of a non-linear Dirichlet energy. To illustrate our approach we use a number of benchmark problems and apply it to numerically given flow fields. Code and a video can be found in the ACM Digital Library.",Inside fluids: clebsch maps for visualization and processing,NA:NA:NA:NA,2017
Kiwon Um:Xiangyu Hu:Nils Thuerey,"This paper proposes a novel framework to evaluate fluid simulation methods based on crowd-sourced user studies in order to robustly gather large numbers of opinions. The key idea for a robust and reliable evaluation is to use a reference video from a carefully selected real-world setup in the user study. By conducting a series of controlled user studies and comparing their evaluation results, we observe various factors that affect the perceptual evaluation. Our data show that the availability of a reference video makes the evaluation consistent. We introduce this approach for computing scores of simulation methods as visual accuracy metric. As an application of the proposed framework, a variety of popular simulation methods are evaluated.",Perceptual evaluation of liquid simulation methods,NA:NA:NA,2017
Diego Gutierrez,NA,"Session details: Image: mapping, size & range",NA,2017
Jung-Hsuan Wu:Suguru Saito,"This article addresses the relighting of outdoor and large indoor scenes illuminated by nondistant lights, which has seldom been discussed in previous works. We propose a method for users to interactively edit the illumination of a scene by moving existing lights and inserting synthetic lights into the scene that requires only a small amount of user annotation and a single low-dynamic range (LDR) image. We achieve this by adopting a top-down approach that estimates the scene reflectance by fitting a diffuse illumination model to a photograph. This approach gains stability and robustness by estimating the camera, scene geometry, and light sources in sequence and by using a confidence map, which is a per-pixel weight map. The results of our evaluation demonstrates that the proposed method can estimate a scene accurately enough for realistic relighting of images. Moreover, the experimental results of our user studies show that the synthesized images are so realistic as to be almost indistinguishable from real photographs.",Interactive Relighting in Single Low-Dynamic Range Images,NA:NA,2017
Clemens Birklbauer:David C. Schedl:Oliver Bimber,"Light-field cameras offer new imaging possibilities compared to conventional digital cameras. However, the additional angular domain of light fields prohibits direct application of frequently used image processing algorithms, such as warping, retargeting, or stitching. We present a general and efficient framework for nonuniform light-field warping, which forms the basis for extending many of these image processing techniques to light fields. It propagates arbitrary spatial deformations defined in one light-field perspective consistently to all other perspectives by means of 4D patch matching instead of relying on explicit depth reconstruction. This allows processing light-field recordings of complex scenes with non-Lambertian properties such as transparency and refraction. We show application examples of our framework in panorama light-field imaging, light-field retargeting, and artistic manipulation of light fields.",Nonuniform Spatial Deformation of Light Fields by Locally Linear Transformations,NA:NA:NA,2017
Nima Khademi Kalantari:Ravi Ramamoorthi,"Producing a high dynamic range (HDR) image from a set of images with different exposures is a challenging process for dynamic scenes. A category of existing techniques first register the input images to a reference image and then merge the aligned images into an HDR image. However, the artifacts of the registration usually appear as ghosting and tearing in the final HDR images. In this paper, we propose a learning-based approach to address this problem for dynamic scenes. We use a convolutional neural network (CNN) as our learning model and present and compare three different system architectures to model the HDR merge process. Furthermore, we create a large dataset of input LDR images and their corresponding ground truth HDR images to train our system. We demonstrate the performance of our system by producing high-quality HDR images from a set of three LDR images. Experimental results show that our method consistently produces better results than several state-of-the-art approaches on challenging scenes.",Deep high dynamic range imaging of dynamic scenes,NA:NA,2017
Eduardo S. L. Gastal:Manuel M. Oliveira,"We present an image downscaling technique capable of appropriately representing high-frequency structured patterns. Our method breaks conventional wisdom in sampling theory---instead of discarding high-frequency information to avoid aliasing, it controls aliasing by remapping such information to the representable range of the downsampled spectrum. The resulting images provide more faithful representations of their original counterparts, retaining visually-important details that would otherwise be lost. Our technique can be used with any resampling method and works for both natural and synthetic images. We demonstrate its effectiveness on a large number of images downscaled in combination with various resampling strategies. By providing an alternative solution for a long-standing problem, our method opens up new possibilities for image processing.",Spectral remapping for image downscaling,NA:NA,2017
Zhixin Shu:Sunil Hadap:Eli Shechtman:Kalyan Sunkavalli:Sylvain Paris:Dimitris Samaras,"Lighting is a critical element of portrait photography. However, good lighting design typically requires complex equipment and significant time and expertise. Our work simplifies this task using a relighting technique that transfers the desired illumination of one portrait onto another. The novelty in our approach to this challenging problem is our formulation of relighting as a mass transport problem. We start from standard color histogram matching that only captures the overall tone of the illumination, and we show how to use the mass-transport formulation to make it dependent on facial geometry. We fit a three-dimensional (3D) morphable face model to the portrait, and for each pixel, we combine the color value with the corresponding 3D position and normal. We then solve a mass-transport problem in this augmented space to generate a color remapping that achieves localized, geometry-aware relighting. Our technique is robust to variations in facial appearance and small errors in face reconstruction. As we demonstrate, this allows our technique to handle a variety of portraits and illumination conditions, including scenarios that are challenging for previous methods.",Portrait Lighting Transfer Using a Mass Transport Approach,NA:NA:NA:NA:NA:NA,2017
Jehee Lee,NA,Session details: Human motion,NA,2017
Changgu Kang:Sung-Hee Lee,"Multi-contact locomotion that uses both the hands and feet in a complex environment remains a challenging problem in computer animation. To address this problem, we present a contact graph, which is a motion graph augmented by learned feasibility predictors, namely contact spaces and an occupancy estimator, for a motion clip in each graph node. By estimating the feasibilities of candidate contact points that can be reached by modifying a motion clip, the predictors allow us to find contact points that are likely to be valid and natural before attempting to generate the actual motion for the contact points. The contact graph thus enables the efficient generation of multi-contact motion in two steps: planning contact points to the goal and then generating the whole-body motion. We demonstrate the effectiveness of our method by creating several climbing motions in complex and cluttered environments by using only a small number of motion samples.",Multi-Contact Locomotion Using a Contact Graph with Feasibility Predictors,NA:NA,2017
Mazen Al Borno:Michiel Van De Panne:Eugene Fiume,"Determining effective control strategies and solutions for high-degree-of-freedom humanoid characters has been a difficult, ongoing problem. A controller is only valid for a subset of the states of the character, known as the domain of attraction (DOA). This article shows how many states that are initially outside the DOA can be brought inside it. Our first contribution is to show how DOA expansion can be performed for a high-dimensional simulated character. Our second contribution is to present an algorithm that efficiently increases the DOA using random trees that provide denser coverage than the trees produced by typical sampling-based motion-planning algorithms. The trees are constructed offline but can be queried fast enough for near-real-time control. We show the effect of DOA expansion on getting up, crouch-to-stand, jumping, and standing-twist controllers. We also show how DOA expansion can be used to connect controllers together.",Domain of Attraction Expansion for Physics-Based Character Control,NA:NA:NA,2017
Taesoo Kwon:Jessica K. Hodgins,"Designing a unified framework for simulating a broad variety of human behaviors has proven to be challenging. In this article, we present an approach for control system design that can generate animations of a diverse set of behaviors including walking, running, and a variety of gymnastic behaviors. We achieve this generalization with a balancing strategy that relies on a new form of inverted pendulum model (IPM), which we call the momentum-mapped IPM (MMIPM). We analyze reference motion capture data in a pre-processing step to extract the motion of the MMIPM. To compute a new motion, the controller plans a desired motion, frame by frame, based on the current pendulum state and a predicted pendulum trajectory. By tracking this time-varying trajectory, the controller creates a character that dynamically balances, changes speed, makes turns, jumps, and performs gymnastic maneuvers.",Momentum-Mapped Inverted Pendulum Models for Controlling Dynamic Human Motions,NA:NA,2017
Gordon Wetzstein,NA,Session details: Computational cameras & displays,NA,2017
Petr Kellnhofer:Piotr Didyk:Szu-Po Wang:Pitchaya Sitthi-Amorn:William Freeman:Fredo Durand:Wojciech Matusik,"Stereoscopic 3D (S3D) movies have become widely popular in the movie theaters, but the adoption of S3D at home is low even though most TV sets support S3D. It is widely believed that S3D with glasses is not the right approach for the home. A much more appealing approach is to use automulti-scopic displays that provide a glasses-free 3D experience to multiple viewers. A technical challenge is the lack of native multiview content that is required to deliver a proper view of the scene for every viewpoint. Our approach takes advantage of the abundance of stereoscopic 3D movies. We propose a real-time system that can convert stereoscopic video to a high-quality multiview video that can be directly fed to automultiscopic displays. Our algorithm uses a wavelet-based decomposition of stereoscopic images with per-wavelet disparity estimation. A key to our solution lies in combining Lagrangian and Eulerian approaches for both the disparity estimation and novel view synthesis, which leverages the complementary advantages of both techniques. The solution preserves all the features of Eulerian methods, e.g., subpixel accuracy, high performance, robustness to ambiguous depth cases, and easy integration of inter-view aliasing while maintaining the advantages of Lagrangian approaches, e.g., robustness to large disparities and possibility of performing non-trivial disparity manipulations through both view extrapolation and interpolation. The method achieves real-time performance on current GPUs. Its design also enables an easy hardware implementation that is demonstrated using a field-programmable gate array. We analyze the visual quality and robustness of our technique on a number of synthetic and real-world examples. We also perform a user experiment which demonstrates benefits of the technique when compared to existing solutions.",3DTV at home: eulerian-lagrangian stereo-to-multiview conversion,NA:NA:NA:NA:NA:NA:NA,2017
Taiki Fukiage:Takahiro Kawabe:Shin'ya Nishida,"When a conventional stereoscopic display is viewed without stereo glasses, image blurs, or 'ghosts', are visible due to the fusion of stereo image pairs. This artifact severely degrades 2D image quality, making it difficult to simultaneously present clear 2D and 3D contents. To overcome this limitation (backward incompatibility), here we propose a novel method to synthesize ghost-free stereoscopic images. Our method gives binocular disparity to a 2D image, and drives human binocular disparity detectors, by the addition of a quadrature-phase pattern that induces spatial subband phase shifts. The disparity-inducer patterns added to the left and right images are identical except for the contrast polarity. Physical fusion of the two images cancels out the disparity-inducer components and makes only the original 2D pattern visible to viewers without glasses. Unlike previous solutions, our method perfectly excludes stereo ghosts without using special hardware. A simple algorithm can transform 3D contents from the conventional stereo format into ours. Furthermore, our method can alter the depth impression of a real object without its being noticed by naked-eye viewers by means of light projection of the disparity-inducer components onto the object's surface. Psychophysical evaluations have confirmed the practical utility of our method.",Hiding of phase-based stereo disparity for ghost-free viewing without glasses,NA:NA:NA,2017
Kevin Matzen:Michael F. Cohen:Bryce Evans:Johannes Kopf:Richard Szeliski,"A number of consumer-grade spherical cameras have recently appeared, enabling affordable monoscopic VR content creation in the form of full 360° X 180° spherical panoramic photos and videos. While monoscopic content is certainly engaging, it fails to leverage a main aspect of VR HMDs, namely stereoscopic display. Recent stereoscopic capture rigs involve placing many cameras in a ring and synthesizing an omni-directional stereo panorama enabling a user to look around to explore the scene in stereo. In this work, we describe a method that takes images from two 360° spherical cameras and synthesizes an omni-directional stereo panorama with stereo in all directions. Our proposed method has a lower equipment cost than camera-ring alternatives, can be assembled with currently available off-the-shelf equipment, and is relatively small and light-weight compared to the alternatives. We validate our method by generating both stills and videos. We have conducted a user study to better understand what kinds of geometric processing are necessary for a pleasant viewing experience. We also discuss several algorithmic variations, each with their own time and quality trade-offs.",Low-cost 360 stereo photography and video capture,NA:NA:NA:NA:NA,2017
Fu-Chung Huang:Dawid Pająk:Jonghyun Kim:Jan Kautz:David Luebke,"Increasing resolution and dynamic range of digital color displays is challenging with designs confined by cost and power specifications. This necessitates modern displays to trade-off spatial and temporal resolution for color reproduction capability. In this work we explore the idea of joint hardware and algorithm design to balance such trade-offs. We introduce a system that uses content-adaptive and compressive factorizations to reproduce colors. Each target frame is factorized into two products of high-resolution monochromatic and low-resolution color images, which then get integrated through temporal or spatial multiplexing. As our framework minimizes the error in colorimetric space, the perceived color rendition is high, and thanks to GPU acceleration, the results are generated in real-time. We evaluate our system with a LCD prototype that uses LED backlight array and temporal multiplexing to reproduce color images. Our approach enables high effective resolution and dynamic range without increasing power consumption. We also demonstrate low-cost extensions to hyperspectral and light-field imaging, which are possible due to compressive nature of our system.",Mixed-primary factorization for dual-frame computational displays,NA:NA:NA:NA:NA,2017
Paul Kry,NA,Session details: Let's get in contact,NA,2017
Jui-Hsien Wang:Rajsekhar Setaluri:Doug L. James:Dinesh K. Pai,"We present a novel method to enrich standard rigid-body impact models with a spatially varying coefficient of restitution map, or Bounce Map. Even state-of-the art methods in computer graphics assume that for a single rigid body, post- and pre-impact dynamics are related with a single global, constant, namely the coefficient of restitution. We first demonstrate that this assumption is highly inaccurate, even for simple objects. We then present a technique to efficiently and automatically generate a function which maps locations on the object's surface along with impact normals, to a scalar coefficient of restitution value. Furthermore, we propose a method for two-body restitution analysis, and, based on numerical experiments, estimate a practical model for combining one-body Bounce Map values to approximate the two-body coefficient of restitution. We show that our method not only improves accuracy, but also enables visually richer rigid-body simulations.",Bounce maps: an improved restitution model for real-time rigid-body impact,NA:NA:NA:NA,2017
Etienne Vouga:Breannan Smith:Danny M. Kaufman:Rasmus Tamstorf:Eitan Grinspun,"Iterative algorithms are frequently used to resolve simultaneous impacts between rigid bodies in physical simulations. However, these algorithms lack formal guarantees of termination, which is sometimes viewed as potentially dangerous, so failsafes are used in practical codes to prevent infinite loops. We show such steps are unnecessary. In particular, we study the broad class of such algorithms that are conservative and satisfy a minimal set of physical correctness properties, and which encompasses recent methods like Generalized Reflections as well as pairwise schemes. We fully characterize finite termination of these algorithms. The only possible failure cases can be detected, and we describe a procedure for modifying the algorithms to provably ensure termination. We also describe modifications necessary to guarantee termination in the presence of numerical error due to the use of floating-point arithmetic. Finally, we discuss the challenges dissipation introduce for finite termination, and describe how dissipation models can be incorporated while retaining the termination guarantee.",All's well that ends well: guaranteed resolution of simultaneous rigid body impact,NA:NA:NA:NA:NA,2017
Mattia Montanari:Nik Petrinic:Ettore Barbieri,"This article presents a new version of the Gilbert-Johnson-Keerthi (GJK) algorithm that circumvents the shortcomings introduced by degenerate geometries. The original Johnson algorithm and Backup procedure are replaced by a distance subalgorithm that is faster and accurate to machine precision, thus guiding the GJK algorithm toward a shorter search path in less computing time. Numerical tests demonstrate that this effectively is a more robust procedure. In particular, when the objects are found in contact, the newly proposed subalgorithm runs from 15% to 30% times faster than the original one. The improved performance has a significant impact on various applications, such as real-time simulations and collision avoidance systems. Altogether, the main contributions made to the GJK algorithm are faster convergence rate and reduced computational time. These improvements may be easily added into existing implementations; furthermore, engineering applications that require solutions of distance queries to machine precision can now be tackled using the GJK algorithm.",Improving the GJK Algorithm for Faster and More Reliable Distance Queries Between Convex Objects,NA:NA:NA,2017
Chenfanfu Jiang:Theodore Gast:Joseph Teran,"The typical elastic surface or curve simulation method takes a Lagrangian approach and consists of three components: time integration, collision detection and collision response. The Lagrangian view is beneficial because it naturally allows for tracking of the codimensional manifold, however collision must then be detected and resolved separately. Eulerian methods are promising alternatives because collision processing is automatic and while this is effective for volumetric objects, advection of a codimensional manifold is too inaccurate in practice. We propose a novel hybrid Lagrangian/Eulerian approach that preserves the best aspects of both views. Similar to the Drucker-Prager and Mohr-Coulomb models for granular materials, we define our collision response with a novel elastoplastic constitutive model. To achieve this, we design an anisotropic hyperelastic constitutive model that separately characterizes the response to manifold strain as well as shearing and compression in the directions orthogonal to the manifold. We discretize the model with the Material Point Method and a novel codimensional Lagrangian/Eulerian update of the deformation gradient. Collision intensive scenarios with millions of degrees of freedom require only a few minutes per frame and examples with up to one million degrees of freedom run in less than thirty seconds per frame.","Anisotropic elastoplasticity for cloth, knit and hair frictional contact",NA:NA:NA,2017
Ira Kemelmacher-Shlizerman,NA,Session details: Faces & hair,NA,2017
Alexandru-Eugen Ichim:Petr Kadleček:Ladislav Kavan:Mark Pauly,"We present a novel physics-based approach to facial animation. Contrary to commonly used generative methods, our solution computes facial expressions by minimizing a set of non-linear potential energies that model the physical interaction of passive flesh, active muscles, and rigid bone structures. By integrating collision and contact handling into the simulation, our algorithm avoids inconsistent poses commonly observed in generative methods such as blendshape rigs. A novel muscle activation model leads to a robust optimization that faithfully reproduces complex facial articulations. We show how person-specific simulation models can be built from a few expression scans with a minimal data acquisition process and an almost entirely automated processing pipeline. Our method supports temporal dynamics due to inertia or external forces, incorporates skin sliding to avoid unnatural stretching, and offers full control of the simulation parameters, which enables a variety of advanced animation effects. For example, slimming or fattening the face is achieved by simply scaling the volume of the soft tissue elements. We show a series of application demos, including artistic editing of the animation model, simulation of corrective facial surgery, or dynamic interaction with external forces and objects.",Phace: physics-based face modeling and animation,NA:NA:NA:NA,2017
Roger Blanco i Ribera:Eduard Zell:J. P. Lewis:Junyong Noh:Mario Botsch,"While facial capturing focuses on accurate reconstruction of an actor's performance, facial animation retargeting has the goal to transfer the animation to another character, such that the semantic meaning of the animation remains. Because of the popularity of blendshape animation, this effectively means to compute suitable blendshape weights for the given target character. Current methods either require manually created examples of matching expressions of actor and target character, or are limited to characters with similar facial proportions (i.e., realistic models). In contrast, our approach can automatically retarget facial animations from a real actor to stylized characters. We formulate the problem of transferring the blendshapes of a facial rig to an actor as a special case of manifold alignment, by exploring the similarities of the motion spaces defined by the blendshapes and by an expressive training sequence of the actor. In addition, we incorporate a simple, yet elegant facial prior based on discrete differential properties to guarantee smooth mesh deformation. Our method requires only sparse correspondences between characters and is thus suitable for retargeting marker-less and marker-based motion capture as well as animation transfer between virtual characters.",Facial retargeting with automatic range of motion alignment,NA:NA:NA:NA:NA,2017
Jakub Fišer:Ondřej Jamriška:David Simons:Eli Shechtman:Jingwan Lu:Paul Asente:Michal Lukáč:Daniel Sýkora,"We introduce a novel approach to example-based stylization of portrait videos that preserves both the subject's identity and the visual richness of the input style exemplar. Unlike the current state-of-the-art based on neural style transfer [Selim et al. 2016], our method performs non-parametric texture synthesis that retains more of the local textural details of the artistic exemplar and does not suffer from image warping artifacts caused by aligning the style exemplar with the target face. Our method allows the creation of videos with less than full temporal coherence [Ruder et al. 2016]. By introducing a controllable amount of temporal dynamics, it more closely approximates the appearance of real hand-painted animation in which every frame was created independently. We demonstrate the practical utility of the proposed solution on a variety of style exemplars and target videos.",Example-based synthesis of stylized facial animations,NA:NA:NA:NA:NA:NA:NA:NA,2017
Meng Zhang:Menglei Chai:Hongzhi Wu:Hao Yang:Kun Zhou,"We introduce a novel four-view image-based hair modeling method. Given four hair images taken from the front, back, left and right views as input, we first estimate the rough 3D shape of the hair observed in the input using a predefined database of 3D hair models, then synthesize a hair texture on the surface of the shape, from which the hair growing direction information is calculated and used to construct a 3D direction field in the hair volume. Finally, we grow hair strands from the scalp, following the direction field, to produce the 3D hair model, which closely resembles the hair in all input images. Our method does not require that all input images are from the same hair, enabling an effective way to create compelling hair models from images of considerably different hairstyles at different views. We demonstrate the efficacy of our method using a wide range of examples.",A data-driven approach to four-view image-based hair modeling,NA:NA:NA:NA:NA,2017
Stelian Coros,NA,"Session details: Work it, make it better, stronger",NA,2017
Adriana Schulz:Jie Xu:Bo Zhu:Changxi Zheng:Eitan Grinspun:Wojciech Matusik,"Computer Aided Design (CAD) is a multi-billion dollar industry used by almost every mechanical engineer in the world to create practically every existing manufactured shape. CAD models are not only widely available but also extremely useful in the growing field of fabrication-oriented design because they are parametric by construction and capture the engineer's design intent, including manufacturability. Harnessing this data, however, is challenging, because generating the geometry for a given parameter value requires time-consuming computations. Furthermore, the resulting meshes have different combinatorics, making the mesh data inherently discontinuous with respect to parameter adjustments. In our work, we address these challenges and develop tools that allow interactive exploration and optimization of parametric CAD data. To achieve interactive rates, we use precomputation on an adaptively sampled grid and propose a novel scheme for interpolating in this domain where each sample is a mesh with different combinatorics. Specifically, we extract partial correspondences from CAD representations for local mesh morphing and propose a novel interpolation method for adaptive grids that is both continuous/smooth and local (i.e., the influence of each sample is constrained to the local regions where mesh morphing can be computed). We show examples of how our method can be used to interactively visualize and optimize objects with a variety of physical properties.",Interactive design space exploration and optimization for CAD models,NA:NA:NA:NA:NA:NA,2017
Jiaxian Yao:Danny M. Kaufman:Yotam Gingold:Maneesh Agrawala,"High-quality hand-made furniture often employs intrinsic joints that geometrically interlock along mating surfaces. Such joints increase the structural integrity of the furniture and add to its visual appeal. We present an interactive tool for designing such intrinsic joints. Users draw the visual appearance of the joints on the surface of an input furniture model as groups of two-dimensional (2D) regions that must belong to the same part. Our tool automatically partitions the furniture model into a set of solid 3D parts that conform to the user-specified 2D regions and assemble into the furniture. If the input does not merit assemblable solid 3D parts, then our tool reports the failure and suggests options for redesigning the 2D surface regions so that they are assemblable. Similarly, if any parts in the resulting assembly are unstable, then our tool suggests where additional 2D regions should be drawn to better interlock the parts and improve stability. To perform this stability analysis, we introduce a novel variational static analysis method that addresses shortcomings of the equilibrium method for our task. Specifically, our method correctly detects sliding instabilities and reports the locations and directions of sliding and hinging failures. We show that our tool can be used to generate over 100 joints inspired by traditional woodworking and Japanese joinery. We also design and fabricate nine complete furniture assemblies that are stable and connected using only the intrinsic joints produced by our tool.",Interactive Design and Stability Analysis of Decorative Joinery for Furniture,NA:NA:NA:NA,2017
Erva Ulu:James Mccann:Levent Burak Kara,"We introduce a lightweight structure optimization approach for problems in which there is uncertainty in the force locations. Such uncertainty may arise due to force contact locations that change during use or are simply unknown a priori. Given an input 3D model, regions on its boundary where arbitrary normal forces may make contact, and a total force-magnitude budget, our algorithm generates a minimum weight 3D structure that withstands any force configuration capped by the budget. Our approach works by repeatedly finding the most critical force configuration and altering the internal structure accordingly. A key issue, however, is that the critical force configuration changes as the structure evolves, resulting in a significant computational challenge. To address this, we propose an efficient critical instant analysis approach. Combined with a reduced order formulation, our method provides a practical solution to the structural optimization problem. We demonstrate our method on a variety of models and validate it with mechanical tests.",Lightweight structure design under force location uncertainty,NA:NA:NA,2017
Naoki Nozawa:Tsukasa Fukusato:Shigeo Morishima,"Resizing of 3D model is necessary for computer graphics animation and application such as games and movies. In general, when users deform a target model, they built on a bounding box or a closed polygon mesh (cage) to enclose a target model. Then, the resizing is done by deforming the cage with target model. However, these approaches are not good for detailed adjustment of 3D shape because they do not preserve local information. In contrast, based on a local information (e.g., edge set and weight map), Sorkine et al. [Sorkine and Alexa 2007; Sorkine et al. 2004] can generate smooth and conformal deformation results with only a few control points. While these approaches are useful for some situations, the results depend on resolution and topology of the target model. In addition, these approaches do not consider texture (UV) information.",3D model partial-resizing via normal and texture map combination,NA:NA:NA,2017
Shinji Mizuno:Kenji Funahashi,"The authors have been developing an interactive CG creation system ""Amazing Sketchbook"" in which, a user can create a 3DCG scene composed of many 3DCG objects interactively by drawing a picture in an ordinary physical sketchbook with ordinary physical color pens, and can interact with the 3DCG scene by touching the picture or shaking the sketchbook [Kondo et al. 2013]. ""Amazing Sketchbook"" does not require any drafts to color in, and a user can create 3DCG scenes from arbitrary hand-drawn pictures interactively, unlike other systems with the same concepts [Clark et al. 2011][teamLab 2013].",Amazing sketchbook advance,NA:NA,2017
Masanori Nakamura:Shugo Yamaguchi:Shigeo Morishima,We propose a novel font called Beautifying Font to assist learning techniques in writing Chinese calligraphy. Chinese calligraphy has various expressions but they are hard to acquire for beginners. Beautifying Font visualizes the speed and pressure of brush-strokes so that users can intuitively understand how to write.,Beautifying font: effective handwriting template for mastering expression of Chinese calligraphy,NA:NA:NA,2017
Claudio Mura:Renato Pajarola,"We propose a scalable strategy for the architectural modeling of large-scale interiors from 3D point clouds. We exploit the fact that buildings are structured into different rooms to cast the modeling of a large, multi-room environment as a set of simpler and independent reconstruction problems. This drastically reduces the complexity of the computation and makes the processing of large-scale datasets feasible even without using restrictive priors that affect the precision of the final output.",Exploiting the room structure of buildings for scalable architectural modeling of interiors,NA:NA,2017
Masaki Fujita:Suguru Saito,"Although 3D CG tools produce similar style animations with conventional 2D animation, conventional hand-drawn animation has advantages that cannot be substituted in animation produced by those tools. This paper introduces a new method to assist animators in creating 2D keyframe animation, which utilizes the self-shaped canvas hidden from the user. Drawing keyframes is the main input. Because the canvas has a three-dimensional shape deformed according to keyframes, inbetween animation with them mapped on it obtains depth-aware motion.",Hand-drawn animation with self-shaped canvas,NA:NA,2017
Teemu Lindborg:Philip Gifford:Oleg Fryazinov,"Modern 3D modelling tools allow to create very complex scenes, however, they usually require a certain level of knowledge from users, which is not always possible for artists, especially if one needs to define internal material properties or change the parameters during the modelling process. In this paper, we present a visual, node-based approach to heterogeneous 3D modelling by using signed distance fields and material interpolation which allows for easy parametrisation of the scene with interactive rendering for a good user experience.",Interactive parameterised heterogeneous 3D modelling with signed distance fields,NA:NA:NA,2017
Yoichi Ochiai:Tatsuya Minagawa:Takayuki Hoshi:Daitetsu Sato:Satoshi Hashizume:Kazuki Takazawa:Amy Koike:Ippei Suzuki:Atsushi Shinoda:Kazuyoshi Kubokawa,"Aerial manipulation of material objects is fascinating and is used in many performance situations. Many scientific demonstrations and magic shows employ these levitations. Acoustic, magnetic, electric, and superconductive levitation are used in many situations. Adding controllability and increasing the design space of these levitation methods are often studied for use in entertainment applications in graphics and HCI communities. In this study, we focus on superconductive levitation (Figure 1) because it has not been well explored for entertainment applications.",LeviFab: stabilization and manipulation of digitally fabricated objects for superconductive levitation,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2017
Hojin Choi:Seungkyu Lee,"We propose a framework for novel view synthesis for a cartoon face given two images of key views such as front and side views. Due to the structure inconsistency of cartoon faces in manually drawn images, matching region detection between different views is a challenging task. We propose a local complexity based segmentation that grabs facial components such as eyes, eyebrows and mouth that are not clearly separated by contour in cartoon faces. We automatically find the matching region in the two input key images and morph them to create intermediate views.",Novel view synthesis from two cartoon face drawings,NA:NA,2017
Hei-Ting Tamar Wong:Yaozhun Huang:Sze-Chun Tsang:Miu-Ling Lam,"In applications such as computational swept volume light painting, real-time contour rendering is essential to ensure shape fidelity. However, there is, at yet, no real-time and scalable solution for slicing a model in arbitrary direction. We propose a new slicing method by organizing the triangular mesh into Octree data structure. The approach can significantly reduce the computational time and improve the performance of real-time rendering. The data structure is invariant to the slicing direction, thus constructing the Octree is a one-time, offline pre-process.",Real-time model slicing in arbitrary direction using octree,NA:NA:NA:NA,2017
Austin E. MacKay:Jonathan D. Denning,"We use systematic layering of variously sized layers to quickly create large, seemingly non-repeating textures. This leads to significantly more control for artists to create expansive visual scenes without the need for large teams to create massive textures. Our method maintains the visual appeal of seamless and non-repeating design, while using little memory and rendering quickly.",rpTextures: systematic layering for large texture generation,NA:NA,2017
Quentin Corker-Marin:Valery Adzhiev:Alexander Pasko,This poster describes an original approach to producing artistic shapes in a cubist style. We propose mathematical models and algorithms for adding cubist features to (or cubification of) time-variant sculptural shapes as well as a practical technological pipeline embracing all the main phases of their production. A novel method is proposed for faceting and local distortion of the given artistic shape. A new concept of a 4D cubist camera is introduced for multiple projections from 4D space-time to 3D space and combining them using space-time blending to create animated sculptures. The proposed techniques are implemented and experimental results are presented.,Space-time cubification of artistic shapes,NA:NA:NA,2017
Xin Liu:Xuan Li:Caowei Zhang:Chuqi Tang:Xiaolian Zhang:Cheng Zheng:Ye Tao:Guanyun Wang:Wenjie Xu:Cheng Yao:Fangtian Ying,"Inspired by the composition characteristics of Escher's images, we noticed that regular mosaic patterns can be interconverted from background to foreground smoothly by gradually changing borders of mosaic shapes in a non-linear way. Based on this, we developed an interactive system to assist children in creating artworks of pattern evolution. In this paper, taking the child's graffti patterns as a source of inspiration, our system employed interactive GA (IGAs) to achieve the transformation and evolution of patterns as well as the positive and negative conjunction among any two or more patterns. Our developed system aims to not only help children complete the art creations for the transformation and conjunction of innovative positive and negative forms among patterns, but also inspire the development of their brain which is the original purpose of building the system.",The artwork generating system of escher-like positive and negative pattern evolution,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2017
Wataru Date:Yasuaki Kakehi,"In this research we propose a new method for typeface design that incorporates the chronological change of physical materials as the algorithm, and interventions by the environment and users as variables to determine the typeface shape in the design. We have focused on physical materials and phenomena with different time scales, such as vaporization of water, oxidation of copper, discoloration of leaves, and have designed and implemented a device for forming typefaces.",Typeharvesting: a typeface design utilizing time-dependent appearance change of physical materials,NA:NA,2017
David Goodhue,"Modern video game engines feature animation compression built using algorithms which offer fast runtime decompression. In comparison to other state-of-the-art industry techniques, we present new methods by which better compression ratios can be realized without significantly impacting performance. We first present a technique for reconstructing a stream of sparsely-keyed rotations from a sequence of angular velocities. Next, we encode those velocities as consecutive deltas, making it possible to use much smaller key sizes. As a final enhancement, we allow the speed component of our angular velocity to in some cases receive influence from velocity keys which do not specify an axis of rotation. Instead, the axis remains unchanged from the previous frame's velocity, yielding smaller data on those frames.",Velocity-based compression of 3D animated rotations,NA,2017
Naoki Kawai,We propose a simple method to generate new views of a scene using a sparse light field generated from a few panoramic images. We introduce a virtual sphere centered at a new viewpoint as a screen for projecting rays of the light field to get resampled images. Our method creates new views directly from a light field and thus provides a way to build virtual reality system which is more stable than using 3D modeling and rendering and less distorted than image morphing approaches.,A simple method for light field resampling,NA,2017
Hirotsugu Yamamoto:Shusei Ito:Tomoyuki Okamoto:Ryosuke Kujime:Kengo Fujii:Yoshiki Terashima:Yukihiro Takeda,"We have developed a visual, thermal and acoustic floating aerial display. Our developed display forms aerial visual images over a tabletop and locally heats a part of aerial images. Furthermore, our display forms locally aerial speaker on a part of aerial images, where sound pressure locally increases. Aerial images are formed with aerial imaging by retro-reflection (AIRR), which features a wide viewing angle, scalability, and mass-productivity. The aerial heater and the aerial speaker are realized with double-layered arrays of rectangular mirror (WARM) and crossed-mirror array (CMA), respectively. In combination of these imaging methods, we have realized a multi-modal aerial display.",Aerial display with thermal and acoustic sensation,NA:NA:NA:NA:NA:NA:NA,2017
Yu-Kai Chiu:Yi-Lung Kao:Yu-Hsuan Huang:Ming Ouhyoung,"Creating videos from compositing multiple footage requires the support of the robotic arm due to the camera motion model needs to be precise. It is extremely difficult to shoot the footage with hand-held camera. However, the cost of the robotic arm is extremely high. Thus, we introduce an augmented reality guiding system to replace it. In our system, we utilized augmented reality to guide the user for the camera motion and implemented an algorithm of stabilization and camera motion alignment for a hand-held camera. The system reduces the cost but remaining good quality of the result at the same time.",AR filming: augmented reality guide for compositing footage in filmmaking,NA:NA:NA:NA,2017
Molly Flexman:Ashish Panse:Benoit Mory:Christopher Martel:Atul Gupta,There is a growing awareness of the effects of radiation exposure to the patient and staff during minimally-invasive x-ray guided interventions. Augmented reality can provide real-time visualization of radiation dose during the procedure with relevant information displayed in the appropriate context.,Augmented reality for radiation dose awareness in the catheterization lab,NA:NA:NA:NA:NA,2017
Yu-Kai Chiu:Yu-Hsuan Huang:Ming Ouhyoung,"The traditional method of cinematography tutorials often separates the theories from the practical experience. The theory was taught first, thus the students often need to practice on their own after that. However, cinematography equipment is costly and not affordable for most students. In this poster, we introduce a virtual reality tutorial system of cinematography. The system contains hands-on drills and simulation of cinematography equipment. Therefore, the user could learn cinematography in an immersive way through our system and gain hands-on experience at the same time. Our system are and can be adapted to assisting tools for production use, and not just for a tutorial.",Cinematography tutorials in virtual reality,NA:NA:NA,2017
Yun Suen Pai:Benjamin I. Outram:Benjamin Tag:Megumi Isogai:Daisuke Ochi:Hideaki Kimata:Kai Kunze,"Layout planning is a process often used in architectural interior design, for factory production plans, and so on. We present CleaVR, a system that provides the user with an immersive virtual reality system that accurately visualizes the layout plan in three dimensions as shown in Figure 1(a). The user is able to freely orbit around the design to observe it in every angle for an accurate evaluation and assessment. The implemented gesture recognition system means that no physical buttons are required, allowing a complete immersion with intuitive controls. These controls allow the user to pan around the environment, pinch to pick and place objects, as well as swiping the view to switch into first person view, as shown in Figure 1(b). With our system, architects, engineers, designers, and even sports analysts may approach their targeted environment through a multi-user, multi-view tool with full control of the virtual environment purely by intuitive gesture controls.",CleaVR: collaborative layout evaluation and assessment in virtual reality,NA:NA:NA:NA:NA:NA:NA,2017
Mengyuan Ren:Ning Xie:Yang Yang:Heng Tao Shen,"Cosmetic medical visualization has become an important application in computer graphics, especially for facial appearance visualization[Chandawarkar et al. 2013]. Recent approaches have reached very realistic results by blend shape[Ma et al. 2012], which is the most practical tool to make the facial appearance and expression animation in application domains on the entertainment industry (VFXs and games). In many role-playing games (RPGs), players enable to edit the character's facial appearance. However, it is unrealistic since arbitrary discontinuities and position relationship violations (a selected nose might be at a higher position that the bottom of the eyes selected from a different character) caused by players' manual operation. Moreover, the validity on changing facial organs has not be considered well yet.",Cosmetic-vis: sample-based 3D facial editor for cosmetic medical visualization,NA:NA:NA:NA,2017
Sheng-Kuen Huang:Hong-Shiang Lin:Ming Ouhyoung,"We presents an alternative and effective method for omnistereo panorama video generation by using deformable spheres. Deformable spheres use vertex based spherical mesh to represent the depth of a scene. Meanwhile, the spherical mesh was substituted for the original 3D points cloud mesh in the rendering step. In our experiments, this approach reduces the time of omnistereo panorama generation and the result is smooth in both spatial and temporal domain.",Effective omnistereo panorama video generation by deformable spheres,NA:NA:NA,2017
Yun Suen Pai:Benjamin I. Outram:Benjamin Tag:Megumi Isogai:Daisuke Ochi:Kai Kunze,"Viewing 360-degree-images and videos through head-mounted displays (HMDs) currently lacks a compelling interface to transition between them. We propose GazeSphere; a navigation system that provides a seamless transition between 360-degree-video environment locations through the use of orbit-like motion, via head rotation and eye gaze tracking. The significance of this approach is threefold: 1) It allows navigation and transition through spatially continuous 360-video environments, 2) It leverages the human's proprioceptive sense of rotation for locomotion that is intuitive and negates motion sickness, and 3) it uses eye tracking for a completely seamless, hands-free, and unobtrusive interaction. The proposed method uses an orbital motion technique for navigation in virtual space, which we demonstrate in applications such as navigation and interaction in computer aided design (CAD), data visualization, as a game mechanic, and for virtual tours.",GazeSphere: navigating 360-degree-video environments in VR using head rotation and eye gaze,NA:NA:NA:NA:NA:NA,2017
Toshikazu Ohshima:Kenzo Kojima,"In this study, we propose the use of mixed reality (MR) for the purposes of biological education. Our objective is to create an interactive edutainment MR framework for users to learn about nature and human beings. MitsuDomoe, an interactive ecosystem simulator of virtual creatures in a petri dish, comprises three species of primitive artificial creatures. MitsuDomoe simulates the predation chain of the virtual creatures in the petri dish, and users can interact with this ecosystem via the petri dish interface. Users can also experience immersive observation by wearing HMD. By combining the MR petri dish and immersive virtual reality (VR) interfaces, we synergistically improve user understanding of the experience.",MitsuDomoe: ecosystem simulation of virtual creatures in mixed reality petri dish (2),NA:NA,2017
Hiroyuki Osone:Takatoshi Yoshida:Yoichi Ochiai,"Many people exercise in water. However, when they swim in the pool, they may get bored. Therefore, studies on virtural reality (VR) and augmented reality (AR) in water have been made. Aquacave[Yamashita et al. 2016] allows you to experience VR in an aquarium. The payload is low but the cost of setting up the environment is high. We cannot swim, over a wide area, and so it cannot be used by many people. Zhang, Tan, and Chen (2016) have created a head-mounted display (HMD)[Zhang et al. 2016] that can be used underwater, but in this structure, air enters the device, which greatly increases the buoyancy, making swimming uncomfortable. In Quarles (2015)[Quarles 2015], water was present in the internal structure of the HMD, but its optical impact was not discussed, the viewing angle is unknown. Therefore, we designed an optimal HMD for swimming. Because there was no air layer in the HMD, it was expected that buoyancy would not be an issue and that the HMD could easily be worn while swimming. Our study is the first to evaluate underwater VR by subject experiments.",Optimized HMD system for underwater VR experience,NA:NA:NA,2017
Yoonsik Yang:Yoonjung Park:Seungho Chae:Tack-don Han,"With the development of digital technology, many researchers increasingly study projection-based augmented reality (AR) that recognizes surrounding space, considers users' situation and provides space-oriented information rather than merely providing simple, fragmentary information. However, complicated installation issues persist, such as projection system installation and space setup to provide projection under various environments. In this study, we propose a portable, ambient projection system to address such complex installation issues. We defined pervasive projection AR system that enables projection in various spaces. The system consists of a pico-projector, a depth-sensing camera, and a pan/tilting platform that supports the projector-camera system. The portable ambient projection system can be positioned at an unknown space and can extract plane information by scanning its surrounding environment as it rotates 360-degrees in a clockwise manner. Users are provided with a pervasive projection AR environment as with a simple tablet user interface to control it.",Portable ambient projection system: build your room projectable space,NA:NA:NA:NA,2017
Shoki Miyagawa:Yoshihiro Fukuhara:Fumiya Narita:Norihiro Ogata:Shigeo Morishima,"Marker-based retexturing is to superimpose the texture on a target object by detecting and identifying markers from within the captured image. We propose a new marker that can be identified under a large deformation that involves self-occlusion, which was not taken into consideration in the following markers. Bradley et al. [Bradley et al. 2009] designed the independent markers, but it is difficult to recognize them under complicated occlusion. Scholz et al.[Scholz and Magnor 2006] created a circular marker with a single color selected from multiple colors. They created ID corresponding to the alignment of colors by one marker and the markers around it and identified by placing the marker so that the ID would be unique. However, when some markers are covered by self-occlusion, the positional relationship of the markers appears to be different from the original, so markers near the self-occlusion are failed to identify. Narita et al. [Narita et al. 2017] considered self-occlusion by improving the identification algorithm. They succeeded in improving the accuracy of identification by creating triangle meshes whose vertices are the center of gravity of markers and assuming that they are close to a right isosceles triangle. However, since outliers are removed using angles, identification of markers may fail in the case of the object that is likely to be deformed in the shear direction like a cloth. Therefore, we considered self-occlusion by designing hierarchical markers so that they can be refferred to in a global scope. We designed a color based marker for easy recognition even at low resolution.",Retexturing under self-occlusion using hierarchical markers,NA:NA:NA:NA:NA,2017
Akira Ishii:Masaya Tsuruta:Ippei Suzuki:Shuta Nakamae:Tatsuya Minagawa:Junichi Suzuki:Yoichi Ochiai,"Virtual reality (VR) with HMD is closed experience among those who are experiencing the VR, and can only be individually experienced by the specific person. We call this ""perspective gap."" These perspective gaps exist in many situations. To address these problems, we present ""ReverseCAVE"", a system for sharing the experiences of people in VR with others (observer). As another application, it is possible to visually recognize the actual appearance of the person performing the act at the motion capture studio and the superimposed character at the same time. ReverseCAVE has four translucent screens surrounding the player. VR environment that the player is experiencing is projected onto the screens. By this, the observer can see both the physical player and the VR environment experienced by the player simultaneously. Also, in the motion capture system, when viewing the actor from the observer outside of ReverseCAVE, the character is superimposed to the actor. This makes it look as if the actor is the actual character from the observer. ReverseCAVE enhances the observers' experience.",ReverseCAVE: providing reverse perspectives for sharing VR experience,NA:NA:NA:NA:NA:NA:NA,2017
Nami Ogawa:Takuji Narumi:Michitaka Hirose,"We present a multiuser, wide-angle, and naked-eye three-dimensional (3D) display technique called a ""swinging 3D lamp."" This technique creates 3D optical illusions of motion parallax by superimposing dynamic luminance patterns on a static two-dimensional (2D) image in a real environment. The basic idea involves combining ""wiggle stereoscopy,"" a method of creating 3D images by exploiting motion parallax, with ""dynamic luminance projection,"" a projection technique making static images dynamic. However, in some cases, it can be difficult to obtain sufficient depth information when combining these methods. This was overcome by adding a depth-of-field (DOF) effect on the original image. The proposed technique is useful for simple and eye-catching 3D displays in public spaces because of the fact that depth information can be presented on the RGB images of common printed media and that multiple people can perceive the depth without special glasses or equipment.",Swinging 3D lamps: a projection technique to convert a static 2D picture to 3D using wiggle stereoscopy,NA:NA:NA,2017
Ippei Suzuki:Satoshi Hashizume:Kazuki Takazawa:Ryuichiro Sasaki:Yoshikuni Hashimoto:Yoichi Ochiai,"In this paper, we propose a telepresence system that is able to provide care from a remote location by implementing functions such as object recognition on a wheelchair (Figure 1 Left). In conventional remote control robots, the operator controls the system while receiving feedback from cameras mounted on the robot [Gundersen et al. 1996]. However, this operating method cannot capture the full environment around the system, even if we use wide FOV cameras, such as omnidirectional cameras. This leaves the operator with incomplete feedback. In order to utilize the telepresence system safely, it is necessary to solve the problem of the blind spot of the user. Further, human operators are limited by their attention span. The reaction time of the computer is greater than that of humans.",Telewheelchair: The intelligent electric wheelchair system towards human-machine combined environmental supports,NA:NA:NA:NA:NA:NA,2017
Ping-Hsuan Han:Yang-Sheng Chen:Han-Lei Wang:Yu-Jie Huang:Jus-Chun Hsiao:Kuan-Wen Chen:Yi-Ping Hung,"In general, highly-skilled manipulation without instruction is difficult. Recently there are some works which apply the manipulating guidance by a Virtual Reality (VR) or Augmented Reality (AR) head-mounted display (HMD) to keep the user hands-free. Henderson et al. [Henderson and Feiner 2009] applied AR to armored vehicle turret maintenance. With a head-worn display, the mechanic could acquire steps in the form of texts, images and animations. Our previous work (My Tai-Chi Coaches) [Han et al. 2017] used optical see-through HMD for Tai-Chi Chuan (TCC) augmented learning tool. Although it can provide the user visual hints such as virtual coaches, the visual hints which the user can see are constrained to the small augmented FOV of the HMD.",The design of video see-through window for manipulating physical object with head-mounted display,NA:NA:NA:NA:NA:NA:NA,2017
Zikun Chen:Wei Peng:Roshan Peiris:Kouta Minamizawa,"With rise in the popularity of virtual reality, head mounted displays (HMDs) have become a main piece of hardware that delivers an immersive experience to the user. As one of the approaches to further enhance the user's presence in the virtual reality environment, haptic feedback has been widely used in the current VR space.",ThermoReality: thermally enriched head mounted displays for virtual reality,NA:NA:NA:NA,2017
Chika Matsusue:Kenji Funahashi:Shinji Mizuno,"In this paper, we propose a novel virtual piano system to assist composition, that corrects deviation of a position based on music theory when a user strikes a wrong key. It means that this system estimates a correct melody from deviated fingering due to the non-physical keyboard. When experienced performer comes up with a good melody, his hands usually move by themselves to play it. Our proposed VR piano does not have any physical keyboard; it estimates a correct key when user's fingering is deviated. Furthermore, if the weight of music theory is adjusted higher than the weight of the user's fingering position, this system changes to easy-playable VR piano. Even by hitting a key randomly, it will produce an appropriate melody based on music theory by taking a bit of fingering into account.",Touch-typable VR piano that corrects positional deviation of fingering based on music theory,NA:NA:NA,2017
Atsuto Inoue:Kohei Yatabe:Yasuhiro Oikawa:Yusuke Ikeda,"We propose a visualization system of three-dimensional (3D) sound information using video and optical see-through head mounted displays (ST-HMDs). The Mixed Reality (MR) displays enable intuitive understanding of 3D information of a sound field which is quite difficult to project onto an ordinary two-dimensional (2D) display in an easily understandable way. As examples of the visualization, the sound intensity (a stationary vector field representing the energy flow of sound) around a speaker and a motor engine is shown.",Visualization of 3D sound field using see-through head mounted display,NA:NA:NA:NA,2017
Jia-Wei Lin:Ping-Hsuan Han:Jiun-Yu Lee:Yang-Sheng Chen:Ting-Wei Chang:Kuan-Wen Chen:Yi-Ping Hung,"Recently, virtual reality (VR) becomes more and more popular and provides users an immersive experience with a head-mounted display (HMD). However, in some applications, users have to interact with physical objects while immersed in VR. With a non-see-through HMD, it is difficult to perceive visual information from the real world. Users must recall the spatial layout of the real surroundings and grope around to find the physical objects. After locating the objects, it is still inconvenient to use them without any visual feedback, which would detract the immersive experience.",Visualizing the keyboard in virtual reality for enhancing immersive experience,NA:NA:NA:NA:NA:NA:NA,2017
Ryohei Nagao:Keigo Matsumoto:Takuji Narumi:Tomohiro Tanikawa:Michitaka Hirose,"In the field of virtual reality, a few methods exist that allow a user to walk up and down the stairs in a virtual environment (VE); however, most of them are based on a complicated device system that generates physical steps using actuators (e.g., [Iwata et al. 2005; Schmidt et al. 2015]). Because it is difficult for users wearing head mounted displays (HMDs) to keep track of the surrounding environment, walking on physical steps could prove to be very dangerous and lead to injuries. Further, these systems have disadvantages in that the user cannot walk naturally. Therefore, a simple and low-cost system that allows users to walk safely and freely in the vertical direction in a VE is highly desirable.",Walking up virtual stairs based on visuo-haptic interaction,NA:NA:NA:NA:NA,2017
Keigo Matsumoto:Takuji Narumi:Tomohiro Tanikawa:Michitaka Hirose,This paper proposes a novel redirected walking technique that creates the feeling of walking uphill and downhill in the virtual environment while actually walking on a flat floor in the real world. We focus on the amount of energy consumed while walking uphill and downhill. Previous studies show that walking uphill takes three times as much energy as walking on a flat ground while walking downhill takes only half as much energy. We believe that the walking feeling in the virtual environment could be improved by changing the walking distance according to the gradient and bringing it closer to the energy consumption at the actual ascending and descending walk. We conducted a preliminary experiment to confirm that the proposed method is effective and the experimental results imply that our method is efficient for walking uphill.,Walking uphill and downhill: redirected walking in the vertical direction,NA:NA:NA:NA,2017
Ergun Akleman:Fermi Perumal:Youyou Wang,"In this work, we present an integrated model that can provide direct illumination, subsurface scattering and soft shadow effects in a single equation. We have implemented our model to render bas-reliefs, which are the shapes that are defined as height fields. Using our model it is possible to interactively obtain all three effects in a qualitatively consistent way. One of the most important properties of our model is that it provides cos θ for planar surfaces. Moreover, our formula is qualitatively related to exponential attenuation due to scattering and provides soft shadows. Therefore, this model provides qualitatively consistent shading for direct illuminated subsurface scattering and shadow regions.","Cos Θ shadows: an integrated model for direct illumination, subsurface scattering and shadow computation",NA:NA:NA,2017
Nahomi Maki:Toshiaki Yamanouchi:Teluhiko Hilano:Kazuhisa Yanaka,"A four-plane depth-fused display (DFD) is an autostereoscopic system that can display four images at different depth positions using a single liquid crystal display and mirrors or half mirrors. This system increases the number of images in the depth direction, thereby enhancing stereoscopic effect. To date, however, the contents of proposed DFD remain limited to still images. Therefore, we introduced an animation that included object motion in the XYZ space in four planes into DFD. This approach considerably increased the sense of depth.",Creation of 3DCG animation using a four-plane depth-fused display,NA:NA:NA:NA,2017
Junho Choi:Yong Yi Lee:Yong Hwi Kim:Bilal Ahmed:Jong Hun Lee:Moon Gu Son:Junbum Kim:Kwan H. Lee,"Various vision-based measurement systems have been developed to reconstruct the 3D shape and appearance of an object. To achieve this, a large number of the samples need to be captured. However, most of the existing measurement system requires a long acquisition time because of system complexity. Although some systems present effective acquisition strategy in the adaptive manner, they focus on only 2D planar samples so that they cannot handle complex 3D object and its reflectance property. In this paper, we present the multi-camera and multi-light source based measurement system that capture the 3D geometry and appearance simultaneously. We also proposed a novel curvature-aware acquisition strategy for reducing the acquisition time and data storage requirement. Since the proposed method can efficiently capture the appearance of 3D objects with complicated shape, expect to progress the digitization in the various field such as museum and industry.",Curvature-aware adaptive capture of 3D geometry and appearance,NA:NA:NA:NA:NA:NA:NA:NA,2017
Ibragim Atadjanov:Seungkyu Lee,"In this work, we propose a novel light field approximation method for multi-layer light field display. Our target light field display consists of two liquid crystal panels with a uniform back-light with no time multiplexing. LCD panels are not necessarily to be parallel. For wide angle of view configuration, we introduce quadratic penalization term to alleviate ghost effects. This creates perceptually improved approximation of light field and increases the possibility of usage in design with a wider field of view configuration.",Dual-layered light field display: maximizing image perceptibility,NA:NA,2017
Daito Kakeya:Masashi Baba:Shinsaku Hiura,"When synthesizing CG images with real image as a background, it is necessary to estimate the lighting environment such as intensities and directions of light sources from real images. In previous study [Baba et al. 2016], except for the high intensity area such as the sun, light source information was estimated by concerning pixels of the captured image as light sources having corresponding intensities[Debevec 1998]. For a high intensity area, the intensity of the light source was estimated from the ratio of the intensity of the shadow area and the sunshine area[Sato et al. 2003]. However, since the sun is used as one parallel light source, there is a problem that it can express only a sharp shadow and it can not be reproduced as an HDR light probe images. Figure 2(a) shows a real image. Figure 2(b) shows a rendering result by the conventional method. In the Figure 2(b), the penumbra of the upper part of the shadow can not be expressed and the shadow boundary is clear comparing with the real image.",Generation of omni-directional HDR light probe images based on the intensity distribution model around the sun,NA:NA:NA,2017
Martin Misiak:Arnulph Fuhrmann,"We present a new, physically plausible, real-time approach to compute directional occlusion for dynamic objects, lit with image based lighting. For this, we partition the hemisphere into multiple sectors and pre-convolve these into separate irradiance maps. At runtime the contributions of each sector are then individually occluded and summed together.",Directional occlusion via multi-irradiance mapping,NA:NA,2017
Mehmet Ömer Özek:Engin Demir,"As the GPU's processing power has been improving much faster than the CPU's. the terrain rendering algorithms have evolved to use the graphics hardware as much as possible. One of the recently developed GPU-based Level of Detail (LOD) algorithm is Continuous View Dependent Adaptive LOD using hardware tessellation. In this study, Continuous View Dependent Adaptive LOD using hardware tessellation is enhanced using three additional methods. First method is determining pixel-based LOD using occlusion query. Because of hidden surface culling, render time is reduced. Second method is determining pixel-based LOD using occlusion query that newly developed by using OpenGL and CUDA interoperability. Third method is extension of second method that includes quad-tree based query for each terrain node. Third method aims to increase rendering quality of partial rendered terrain node.",Pixel-based level of detail on hardware tessellated terrain rendering,NA:NA,2017
Antoine Toisoul:Abhijeet Ghosh,"We present a novel approach for real-time rendering of realistic diffraction effects in surface reflectance under environmental illumination. Renderings in arbitrary environments require the computation of a convolution. In the case of diffraction, the convolution kernel is large due to the high frequency details contained in diffraction patterns, making computations at real time framerate impractical. We propose a low rank factorisation of the diffraction kernel that allows the computation of the convolution in two passes with smaller kernels instead of a large 2D kernel. We present renderings of the diffraction produced by several surfaces and reach a performance of 50 to 100 FPS.",Real time rendering of realistic surface diffraction with low rank factorisation,NA:NA,2017
Shohei Anraku:Fumihiko Ishiwata:Nahomi Maki:Toshiaki Yamanouchi:Kazuhisa Yanaka,"To use the advanced content creation functions of a game engine and develop contents in which displaying real-time integral photography images is important, we implemented multi-viewpoint rendering and IP image synthesis functions by adding a shader and C# scripts to the game engine.",Real-time integral photography using a game engine,NA:NA:NA:NA:NA,2017
Kumar Ayush:Parag Chaudhuri,"We present a generic and principled Monte Carlo raytracing approach to visualizing curved spacetime. In contrast to earlier work, our method can trace rays in curved spacetime while resolving usual ray-object intersections. This not only allows us to visualize complex cosmological phenomena, but also create plausible visualizations of what happens when a black hole or a wormhole appears in a more known environment, like a room with regular specular and diffuse surfaces.",Rendering curved spacetime in everyday scenes,NA:NA,2017
Julio Marco:Adrian Jarabo:Wojciech Jarosz:Diego Gutierrez,"Accurate simulation of light transport in participating media is expensive, due to the many scattering events. However, the band-limiting effect of scattering in media makes this kind of light transport very suitable for adaptive sampling and reconstruction techniques. In this work we present a novel algorithm that adaptively samples radiance from sparse points in the medium using up-to second-order occlusion-aware derivatives to determine when interpolation is appropriate. We derive our metric from each point's incoming light field. We use a proxy triangulation-based representation of the radiance reflected by the surrounding medium and geometry to efficiently compute the first- and second-order derivatives of the radiance at the cache points while accounting for occlusion changes. We validate the quality of our approach on a self-contained two-dimensional model for light transport in media. Then we show how our results generalize to practical three-dimensional scenarios, where we show much better results while reducing computation time up to a 30% compared to previous work.",Second-order occlusion-aware volumetric radiance caching,NA:NA:NA:NA,2017
Bekir Öztürk:Ahmet Oğuz Akyüz,"Light mapping is an important optimization technique, in which the lighting of a scene is precomputed into a texture during a stage known as light baking. However, the primary drawback of this technique is that lights and objects must be static. Our work relaxes several important requirements of light mapping, such as the requirement of the color, intensity, and on-off state of lights as well as the presence or absence of shadow casting objects to be static.",Semi-dynamic light maps,NA:NA,2017
Cheng Zheng:Caowei Zhang:Xuan Li:Xin Liu:Chuqi Tang:Guanyun Wang:Cheng Yao:Fan Zhang:Wenjie Xu:Fangtian Ying,"Children with Autism Spectrum Disorder (ASD) have social communication difficulties partly due to unusual visual processing strategy on human faces. However, their strategies are similar on cartoon faces as normal chilren. In this paper, we present Toon-Chat, a video chat system with virtual cartoon masks to help ASD children enhance communication and emotion comprehension skills. The system is tested in a series of ABA training lessons and the results are promising.",Toon-chat: a cartoon-masked chat system for children with autism,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2017
Jin Ryong Kim:Seunghyup Shin,"We present Touch3D, an interactive mobile platform that provides realistic viewing and touching experiences through glasses-free 3D visualization with electrovibration. Touch3D is designed to take advantage of both visual and tactile illusions to maximize multimodal experience in touchscreen interaction. We seamlessly integrate two technologies: Automultiscopic 3D Display and Electrovibration Display; and weave both hardware and software into one fluid interface. Our museum application using Touch3D demonstrates important implications for the improvement of 3D perception in both visual and tactile modalities for enhanced touchscreen interaction.",Touch3D: touchscreen interaction on multiscopic 3D with electrovibration haptics,NA:NA,2017
Julio Marco:Wojciech Jarosz:Diego Gutierrez:Adrian Jarabo,"Recent advances on transient imaging and their applications have opened the necessity of forward models that allow precise generation and analysis of time-resolved light transport data. However, traditional steady-state rendering techniques are not suitable for computing transient light transport due to the aggravation of inherent Monte Carlo variance over time, specially problematic in participating media. We address this problem by presenting the first photon-based method for transient rendering of participating media that performs density estimations on time-resolved precomputed photon maps. We first introduce the transient integral form of the radiative transfer equation into the computer graphics community, including transient delays on the scattering events. Based on this formulation we leverage the high density and parameterized continuity provided by photon beams algorithms to present a new transient method that allows to significantly mitigate variance and efficiently render participating media effects in transient state.",Transient photon beams,NA:NA:NA:NA,2017
Takaki Murakami:Tanner Person:Charith Lasantha Fernando:Kouta Minamizawa,"In this paper, (1) we developed a fingertip haptic display with integrated force, tactile and thermal feedback in a miniature form-factor such that it can be worn easily and used with augmented reality applications without affecting the existing tracking technologies. (2) we propose the concept of ""Altered Touch"", where the integrated fingertip haptic display stated in (1) was used to alter the haptic properties of real objects by rendering projected visual and haptic feedback. The system consists of our own force display Gravity Grabber mechanism[Minamizawa et al. 2007] to render vertical, shearing forces, high frequency tactile vibrations, and a peltier module for thermal display. The integrated haptic display module weighs less than 50g, can be easily interfaced to a PC with just one micro USB cable, and works standalone from any other additional hardware. In this paper we use this wearable haptic actuator in several augmented reality applications to alter the softness/hardness and hot/cold sensation and several use cases have been discussed. Furthermore, the haptic display could be expanded to design a haptic glove that can interact with both virtual and augmented worlds.","Altered touch: miniature haptic display with force, thermal and tactile feedback for augmented haptics",NA:NA:NA:NA,2017
Yung-Long Chu:Hung-En Hsieh:Wen-Hsiung Lin:Hui-Ju Chen:Chien-Hsing Chou,"Reading is an essential part of daily life. When reading books, drug information, textual icons on electronic devices (e.g., microwave), and information on signs and maps (e.g., location and floor level), people must be able to recognize the words to obtain the information they need. However, for foreigners or people with visual impairment, reading text can be challenging. To assist people with visual impairment in reading English books, Shilkrot et al. have designed a wearable device called the FingerReader [Shilkrot et al. 2014]. The FingerReader utilizes a text-to-speech engine to enable visually impaired users to listen to printed text. Anhong et al. developed a mobile application to assist blind people in using otherwise inaccessible interfaces [Guo et al. 2016]. Through utilizing the phone camera, the software captures the text on an interface and then interactively describes the text beneath the user's finger.",Chinese FingerReader: a wearable device to explore Chinese printed text,NA:NA:NA:NA:NA,2017
Tomoya Sasaki:MHD Yamen Saraiji:Charith Lasantha Fernando:Kouta Minamizawa:Masahiko Inami,"If we could have the capability to edit or customize our body scheme by technology, could our abilities and activities be enhanced? This research proposes a novel interaction to alternate body scheme using artificial limbs substitution metamorphosis. In this work, two additional robotic arms are added to user's body, and are manipulated by legs movement. Limbs control is achieved using two sets of tracking systems: global motion tracking of legs using an optical tracker and local motion tracking for manipulation purposes using socks type device. These data are mapped into artificial limbs' arms, hands and fingers motion. Lastly, force feedback is provided to the feet and mapped to manipulator's touch sensors.",MetaLimbs: metamorphosis for multiple arms interaction using artificial limbs,NA:NA:NA:NA:NA,2017
Kazuki Takazawa:Satoshi Hashizume:Ryuichiro Sasaki:Yoshikuni Hashimoto:Yoichi Ochiai,"In recent years, many materials mass-produced in industrialized societies are flat, thin, and with many squares. Within such a social context, customized machines like conventional shape-changing interfaces will take much time and labor to become popular. We aimed to overcome the weaknesses of such conventional shape-changing interfaces and make them easy to manufacture and apply even for PC users. In order to achieve this, it is necessary to revise the manufacturing method. If it is possible to prepare a flat plate which is inexpensive, available and easily processed, it becomes possible to disseminate the shape changing interface at low cost. In recent times, processing machines such as laser cutters have become more widely available so it is becoming increasingly possible to reduce the cost of estuaries. Therefore, we redesigned the manufacturing method for shape changing interfaces using flat plate which, with our method, can be produced at lower cost and with less labor. Many objects in the world are made by processing flat plates, so the processing of flat plates is an important factor. By using the manufacturing method of shape changing interfaces proposed by us, it becomes possible to embed them naturally in interiors such as furniture made from flat plate.",Morpho sculptures: digital fabrication methods of engraving flat materials into shape changing user interfaces,NA:NA:NA:NA:NA,2017
Satoshi Hashizume:Amy Koike:Takayuki Hoshi:Yoichi Ochiai,"Aerial haptic feedback is a popular topic in research fields on real-world-oriented interaction, augmented reality (AR), and virtual reality (VR). Various methods such as magnetic force, ultrasound, and air vortices have been proposed for this purpose.",Sonovortex: rendering multi-resolution aerial haptics by aerodynamic vortex and focused ultrasound,NA:NA:NA:NA,2017
Yuan-Ling Feng:Charith Lasantha Fernando:Jan Rod:Kouta Minamizawa,"In this paper, we present a novel method of creating a waterproof wearable fingertip haptic display ""AeroFinger"" that is very light and small enough to fit on the fingertip and uses no electro-mechanical actuation. The display consists of 4 miniature airbags which are made out of 3D printed Rubber-Like material so that the display size, strength and shape can be customized by the user. A small sized full range speaker is mounted on a closed air chamber where the air is transferred back and forth through a tiny nozzle to the airbag. The Speaker movements creates a difference in air pressure and translated into airbag inflation and deflation. Therefore, AeroFinger can display the low frequency vibrations as force sensation and high frequency vibrations as tactile sensation. Unlike most ungrounded haptic devices which contains electrical components such as motors or vibration actuators, AeroFinger uses no electro-mechanical actuation and thus can be completely submerged on water or could be used in magnetic resonance environments.",Submerged haptics: a 3-DOF fingertip haptic display using miniature 3D printed airbags,NA:NA:NA:NA,2017
John Kasper Svergja:Henrik Lieng,"The gradient mesh tool, implemented in vector graphics software like Adobe Illustrator, is a popular tool for creating and manipulating complex colour gradients. The mesh-based tool is restricted to rectangular gradient meshes, making it hard for the user to work with more complicated shapes such as shapes with holes. We propose a new gradient mesh tool that supports non-rectangular meshes, with native support for a wide range of different shapes. A user study indicates that our tool is easier to use when drawing colour gradients inside complicated shapes.",A gradient mesh tool for non-rectangular gradient meshes,NA:NA,2017
Yuanming Hu:Yu Fang,"We propose a novel asynchronous time integration scheme for the Material Point Method (MPM), which offers temporal adaptivity when objects of different stiffness or velocity coexist. We show via multiple test scenes that our asynchronous MPM leads to 6X speed up over traditional synchronous MPM without sacrificing accuracy.",An asynchronous material point method,NA:NA,2017
Marta Ortin:Adrian Jarabo:Belen Masia:Diego Gutierrez,"With the increasing number of available consumer light field cameras, this new form of photography is progressively becoming more common. However, there are still very few tools for light field editing, and the interfaces to create those edits remain largely unexplored. We perform a state sequence analysis and hidden Markov-chain analysis based on the sequence of tools and interaction paradigms users employ while editing light fields. These insights can aid researchers and designers in creating new light field editing tools and interfaces, thus helping close the gap between 4D and 2D image editing.",Analyzing interfaces and workflows for light field editing,NA:NA:NA:NA,2017
Chloe LeGendre:David Krissman:Paul Debevec,"We present a technique for improving the alpha matting of challenging green-screen video sequences involving hair strands. As hair strands are thin and can be semi-translucent, they are especially hard to separate from a background. However, they appear as extended lines and thus have a strong response when convolved with oriented filters, even in the presence of noise. We leverage this oriented filter response to robustly locate hair strands within each frame of an actor's performance filmed in front of a green-screen. We demonstrate using production video footage that individual hair fibers excluded from a coarse artist's matte can be located and then added to the foreground element, qualitatively improving the composite result without added manual labor.",Improved chromakey of hair strands via orientation filter convolution,NA:NA:NA,2017
Jannik Boll Nielsen:Rasmus Ramsbøl Jensen,"Action cameras have allowed footage from previously unseen points of view, drones allow for innovative aerial shots, which a few years ago, were either impossible or required a helicopter, and finally DSLR video and lens adaptors have made any lens available for video production. These advances provide especially low-budget movie productions with completely new flexibility and potential.",Photon rectify: undistort any footage on the timeline,NA:NA,2017
Rahul Dey:Jason G. Doig:Christos Gatzidis,"In this work we present separate procedural methods to generate features that are found in natural terrains which are difficult to reproduce with heightmap-based methods. We approximate overhangs, arches and caves using procedural functions and a reduced set of parameters. This produces visually plausible terrain feature topologies as well as a high degree of artistic control. Our approach is more intuitive and art-directable than other existing volumetric methods that are more complex to integrate into existing voxel engines, due to the framework changes necessary, or rely on automatic procedural generation, thus reducing the ability to provide creative input.",Procedural feature generation for volumetric terrains,NA:NA:NA,2017
Ippei Suzuki:Yoichi Ochiai,"We present a new method to protect projected content from secret photography using high-speed projection. Protection techniques for digital copies have been discussed over many years from the viewpoint of data protection. However, content displayed by general display techniques is not only visible to the human eye but also can be captured by cameras. Therefore, projected content is, at times, secretly taken by malicious small cameras even when protection techniques for digital copies are adopted. In this study, we aim to realize a protectable projection method that allows people to observe content with their eyes but not record content with camera devices.",Unphotogenic light: high-speed projection method to prevent secret photography by small cameras,NA:NA,2017
Codruta O. Ancuti:Cosmin Ancuti:Christophe De Vleeschouwer:Rafael Garcia,"In underwater the light propagation is distorted due to the absorption and scattering, which respectively affect the energy and direction of propagated light. These distortions result in scenes with foggy appearance and poor contrast. Moreover, in underwater the colors are faded because their composing wavelengths are cut according to the water depth. Since the deterioration of underwater scenes results from the combination of multiplicative and additive processes, enhancing the visibility in underwater is a challenging task. Underwater single image based techniques [Ancuti et al. 2012, 2016a] have been introduced only recently and in general have been inspired by the outdoor dehazing strategies [Ancuti et al. 2010], [He et al. 2011], [Ancuti and Ancuti 2013], [Ancuti et al. 2016b]. One of the most influential technique was introduced by He et al. [He et al. 2011] based on the Dark Channel Prior (DCP) shown to fail for underwater dehazing (see Figure 1). Indeed, underwater image restoration is more challenging since the attenuation medium factor is color dependent and higher than in aerial conditions. Even if the transmission is well estimated the result image can not be effectively restored without initial image color spectrum restoration (see UDCP result in Figure 1).",A semi-global color correction for underwater image restoration,NA:NA:NA:NA,2017
Yuhang Li:Xuejin Chen,"Geo-localization, aiming at aligning images with 3D models, is a key technique to many applications, such as image-based navigation, augmented reality, 3D city modeling, etc. We present a geo-localization method based on overhead images captured in low altitudes and point clouds of buildings. With two observations that 1) vertical facades of a point cloud typically correspond to edges of building roofs in the overhead image; and 2) building roofs of different altitudes are in different scales in the overhead image due to a perspective projection, we regard this geo-localization problem as a combination of a multi-layer shape matching and a global optimization of the camera pose. We test our approach on a variety of buildings with complex shapes. The experiment results demonstrate the accuracy of our geo-localization algorithm.",Accurate geo-localization of low-altitude overhead images from 3D point clouds,NA:NA,2017
Xuan Huang:Dianna Xu,"Mesh quality improvement is an important problem with a wide range of practical applications. The element quality of a mesh heavily affects the results of numerical simulation done using that mesh. In the context of finite element mesh smoothing, vertex repositioning is the primary technique employed, where we allow tangential vertex motion only and the connectivity of the mesh is unchanged. Element quality is measured either by max/min angles or aspect ratio (longest edge over shortest), or both. We investigate a smoothing method focusing on improving aspect ratio. For triangle meshes this is in theory not significantly different from angle-based smoothing methods which have been widely studied. However many focus on improving minimum angles only and we believe that aspect ratio will lead to a more balanced improvement on both the minimum and maximum angles. In addition, we are also motivated by aspect ratio improvments for quadrilateral meshes, which are unrelated to angles.",Aspect-ratio based triangular mesh smoothing,NA:NA,2017
Tiancheng Sun:Ana Serrano:Diego Gutierrez:Belen Masia,"Real-world materials present a wide variety of appearances, commonly described in computer graphics with the bidirectional reflectance distribution function (BRDF). Printers, on the other hand, have a predefined set of only a few inks, which defines the printer's gamut. As a consequence of this limitation, many materials cannot be exactly reproduced by the printer, creating distortions in the printed appearance that are hard to control. Finding the best approximation of the input BRDF that falls within the printer's gamut while minimizing such distortions as much as possible is the problem known as gamut mapping. We present a novel two-step gamut mapping algorithm that allows users to specify which perceptual attribute of the original material they want to preserve. In the first step, we work in the low-dimensional intuitive appearance space recently proposed by Serrano et al. [Serrano et al. 2016], and adjust achromatic reflectance via an objective function that strives to preserve certain attributes. From such intermediate representation, we then perform an image-based optimization including color information, to bring the BRDF into gamut. We show how our method yields superior results compared to the state of the art, with the additional advantage that the user can specify which visual attributes need to be preserved. For more details we refer to the reader to the full paper [Sun et al. 2017].",Attribute-preserving gamut mapping of measured BRDFs,NA:NA:NA:NA,2017
Stefanie Gassel:Thomas Neumann:Markus Wacker,"Statistical body shape modelling can be used to realistically generate complex muscle deformation effects on the skin. However, purely data-driven models still ignore the biomechanical nature of surface deformations. Reliable anatomically and biomechanically consistent predictions are barely possible. Our research aims at combining the previously separate paradigms - data-driven and simulation-driven 3D surface modeling - to a hybrid body shape model. Our first goal consists of synthesizing the skin surface from simulated biomechanical data. As a first step in this direction we show preliminary results of our model of an elbow flexion motion with separate biceps and triceps muscle bulging that exhibits believable muscular deformation effects on the skin surface while enabling singular control over specific muscle regions. Our model is separately controllable in shape and pose and extensible to a wider range of human body shapes, joint motion and muscle regions.",Combining biomechanical and data-driven body surface models,NA:NA:NA,2017
Jen Rogers:Matthieu Poyade:Frank Pollick,"With advancement in research in a given field, there should be parallel development in visualisation methods to understand the data accrued. 3D visualisation and interactive visual applications can facilitate synthesis and understanding of high dimensional data. This concept has been applied within varying fields of research, though it has yet to be explored significantly in the field of functional neural mapping. This project documents the development of an interactive application for mobile and tablet devices visualising multivariate functional mapping of fMRI data within a 3D structural model of the brain. The application is developed as a proof of concept for the efficacy of interactive 3D visualisation for representing research in functional mapping, as well as the potential for Unity 3D game enginefis use as a visualisation tool for the complex data involved in the research of functional neural activity.",Constellations of movement: an interactive application to visualise research in motor imagery decoding,NA:NA:NA,2017
Naoki Hashimoto:Koki Kosaka,"In this research, we propose a photometric compensation technique for deformable objects, such as a curtain that is continuously swinging. In photometric compensation, it is necessary to exactly obtain an inter-pixel correspondence and a response function between a projector and a camera. Therefore, compensation for deformable objects is a major challenge. In our proposal, we reconstruct the inter-pixel correspondence by using the uniformity of a re-estimated reflectance property of the response function. By using a fast implementation with a GPU, it is possible to provide continuous photometric compensation, even for deformable objects, without using a coaxial projector-camera system.",Continuous photometric compensation for deformable objects,NA:NA,2017
Ryuji Hirayama:Hirotaka Nakayama:Atsushi Shiraki:Takashi Kakue:Tomoyoshi Shimobaba:Tomoyoshi Ito,"In this study, we present a 3D structure projecting multiple dynamic full-color images in different directions. The 3D structure is represented as a crowd of controllable color particles rendered in a cylindrical 3D crystal with a CG software. We confirmed that five images could be successfully observed from different viewpoints. Such 3D structures can be applied to information service systems including digital signage and security system.",Controllable color particles in a 3D crystal projecting multiple dynamic full-color images,NA:NA:NA:NA:NA:NA,2017
Takahiro Itazuri:Tsukasa Fukusato:Shugo Yamaguchi:Shigeo Morishima,We propose a rally-rank evaluation based on the court transition information for volleyball video summarization considering the contents of the game. Our method uses the court transition information instead of non-robust visual features such as the position of a ball and players. Experimental results demonstrate the effectiveness that our method reflects viewers' preferences over previous methods.,Court-aware volleyball video summarization,NA:NA:NA:NA,2017
Matthias Schröder:Helge Ritter,"Recent advances in the development of optical head-mounted displays (HMDs), such as the Microsoft HoloLens, Google Glass, or Epson Moverio, which overlay visual information directly in the user's field of vision, have opened up new possibilities for augmented reality (AR) applications. We propose a system that uses such an optical HMD to assist the user during goal-oriented activities (e.g. manufacturing work) in an intuitive and unobtrusive way (Essig et al. 2016). To this end, our system observes and recognizes the user's actions and generates context-sensitive feedback. Figure 1 shows an overview of our approach, exemplified with the task of assembling a bird house.",Deep learning for action recognition in augmented reality assistance systems,NA:NA,2017
Amy Koike:Satoshi Hashizume:Kazuki Takazawa:Mose Sakashita:Daitetsu Sato:Keisuke Kawahara:Yoichi Ochiai,"Underwater expression is attractive. It seems like underwater objects are floating like anti-gravity scape by buoyancy and it is also impressive that bubbles rise while refracting the light. In this work, we aim to combine digital fabrication with interactive technology and expand underwater expression. To achieve this, we focused on a classic science experiment called the Cartesian Diver. Because of growing interest in the materialization of computer graphics, digital fabrication technologies have recently emerged as one of the most important application fields in real-world-oriented computer graphics. In particular, research on digital fabrication that gives dynamics properties is common. Spin-it [Bächer et al. 2014] presents design method for spinning objects by optimizing rotational dynamics properties. Some studies use non-contact manipulation. For example, ZeroN [Lee et al. 2011] controls the magnetic field to manipulate the object and uses it as a floating screen and input user interface(UI). Our work connects digital fabrication and non-contact manipulation that uses the space transmission power (water pressure) around the object (the diver). [Koike et al. 2016] proposes a design and manipulation method for the diver. In this work, we updated the method and investigate stability of PID control. Furthermore, we propose some applications.",Digital fabrication and manipulation method for underwater display and entertainment,NA:NA:NA:NA:NA:NA:NA,2017
Mie Sato:Haruna Kimura,"We have been developing an augmented reality (AR) system that allows a user to grasp a virtual object with a bare hand. To enhance the user's perception of grasping the virtual object, we employ multisensory integration in our AR system. Our experimental results show that presenting a virtual object with an auditory cue is statistically more effective than presenting one without an auditory cue as regards grasping, holding, translating, rotating and releasing the virtual object.",Effects of auditory cues on grasping a virtual object with a bare hand,NA:NA,2017
Samar M. Alsaleh:Angelica I. Aviles:Alicia Casals:James Hahn,"The appearance of objects is significantly affected by the illumination conditions in the environment. Particularly with objects that have strong reflectivity as they suffer from more dominant specular highlights, causing information loss and discontinuity in the image domain. Many computer vision algorithms are vulnerable to errors in the presence of specular highlights because they violate the image consistency assumption and hinder the performance of many vision tasks, such as object recognition, tracking and surface reconstruction [Artusi et al. 2011]. This is further complicated when we consider video sequences with free-moving cameras or dynamic objects, which is the focus of this work.",Escaping specularity: recovering specular-free video sequences from rank-constrained data,NA:NA:NA:NA,2017
Victor Arellano:Diego Gutierrez:Adrian Jarabo,"Recent works have demonstrated non-line of sight (NLOS) reconstruction by using the time-resolved signal from multiply scattered light. These works combine ultrafast imaging systems with computation, which back-projects the recorded space-time signal to build a probabilistic map of the hidden geometry. Unfortunately, this computation is slow, becoming a bottleneck as the imaging technology improves. In this work, we propose a new back-projection technique for NLOS reconstruction, which is up to a thousand times faster than previous work, with negligible quality loss.",Fast back-projection for non-line of sight reconstruction,NA:NA:NA,2017
Vlastimil Havran:Jan Hošek:Šárka Němcová:Jiří Čáp:Jiří Bittner,We present a portable instrument for on site measuring of surface reflectance represented by the bidirectional texture function (BTF) and the bidirectional reflectance distribution function (BRDF). Our device allows for measurement application scenarios outside the laboratory without the necessity to extract the measured sample from its environment because the instrument is taken to the measured sample. The concept is a rotational lightweight light stage with a compact hemispherical dome and cameras along the meridian and light emitting diode (LED) modules illuminating the sample surface. The LED modules are fixed on the hemisphere and the six cameras can move along the arc in the range of the elevation angle from 0 to 75 degrees. By rotating the hemispherical dome along its axis we can set all possible camera directions to a measured sample. We use an auto-collimator to adjust the correct perpendicular direction of the instrument against the sample. The proposed instrument is portable and fast while maintaining a high degree of accuracy achieving a quality similar to existing stationary BTF gantries that can be only used in a laboratory. The instrument design provides a good tradeoff between the accuracy of measurements and the practical applicability for measurement of locally flat samples. The instrument provides approximately 1000 HDR photographs in a minute that are necessary to capture spatially varying surface reflectance.,Lightdrum: surface reflectance measurement on site,NA:NA:NA:NA:NA,2017
Yiqun Wang:Dong-Ming Yan:Chengcheng Tang:Xiaohan Liu,"In this poster, we propose a simple yet efficient algorithm for eliminating obtuse triangles for isotropic remeshing. Our method can either be applied directly on the input mesh, or be used as a post-processing step on the results from other remeshing algorithms. Our approach outperforms the state-of-the-art approaches in terms of the mesh quality.",Obtuse triangle elimination for isotropic remeshing,NA:NA:NA:NA,2017
David C. Schedl:Clemens Birklbauer:Oliver Bimber,We present an angular superresolution method for light fields captured with a sparse camera array. Our method uses local dictionaries extracted from a sampling mask for upsampling a sparse light field to a dense light field by applying compressed sensing reconstruction. We derive optimal sampling masks by minimizing the coherence for representative global dictionaries. The desired output perspectives and the number of available cameras can be arbitrarily specified. We show that our method yields qualitative improvements compared to previous techniques.,Optimized sampling for view interpolation in light fields using local dictionaries,NA:NA:NA,2017
Kouta Takeuchi:Kazuki Okami:Daisuke Ochi:Hideaki Kimata,"We propose a partial plane sweep volume that can be a more suitable input format for deep-learning-based view synthesis approaches. Our approach makes it possible to synthesize higher quality images with a smaller number of learning iterations, while keeping the number of depth planes.",Partial plane sweep volume for deep learning based view synthesis,NA:NA:NA:NA,2017
Jaewon Kim:Abhijeet Ghosh,"We present a novel, practical method for acquisition of optical properties of common everyday translucent liquids using a simple acquisition setup involving an LCD panel. Previous work on acquiring liquids has required specialized procedures such as dyeing with a fluorescent agent [Ihrke et al. 2005] for volumetric reconstruction of liquid flow, or dilution of liquid in a specialized water tank [Narasimhan et al. 2006] for acquiring its optical properties for rendering. In this work, we build upon the recent work of Kim et al. [2017] who employ direct transmission imaging for single-view reconstruction of axially-symmetric transparent objects such as glasses, goblets, carafes, etc. We observe that many optically interesting everyday liquids such as cocktails, juices, whiskey, wine, oil, etc., are commonly contained in such axially-symmetric transparent containers. Hence, we propose a much more natural acquisition process where we image the transmission of backlit illumination through a liquid volume contained in such a glass object to estimate its optical properties including its absorption and scattering coefficients, and refractive index. Figure 1 demonstrates renderings of various acquired translucent liquids with our proposed method separated into two types: those exhibiting only absorption (a), and those that exhibit both absorption and scattering (b).",Practical acquisition of translucent liquids using polarized transmission imaging,NA:NA,2017
Jianwei Guo:Zhanglin Cheng:Shibiao Xu:Xiaopeng Zhang,"Plants are ubiquitous in the nature, and realistic plant modeling plays an important role in a variety of applications. Over the last decades, an immense amount of efforts have been dedicated to plant modeling. These approaches can be classified into two major categories: procedural modeling [Palubicki et al. 2009; Stava et al. 2014] and data-driven reconstruction approaches (e.g., photographs [Li et al. 2011; Tan et al. 2007] or scanned points [Livny et al. 2010; Xu et al. 2007]). Each approach has its own pros and cons. For example, procedural modeling approaches work well for synthesizing local branch structure details to produce botanically correct trees, but they lack the ability to control the growth of trees under certain shape constraints. While the data-driven approaches might precisely reconstruct skeletal structures, the botanical fidelity of trees are difficult to maintain.",Realistic procedural plant modeling guided by 3D point cloud,NA:NA:NA:NA,2017
Yuka Nakamura:Naoki Hashimoto,"In this study, we demonstrate that it is possible to provide accurate multi-projection over a whole space using a simple procedure and commercially available cameras by arranging multiple projectors freely. Increasing research interest in projection mapping is providing growing opportunities for application of this technique, resulting in a mounting need for simple and highly accurate image projection techniques for entire spaces. Effective projection is difficult to achieve using only one projector, but the use of multiple projectors requires highly complex projection techniques. In this study, we propose a novel and effective geometric correction technique that combines the use of a fish-eye lens camera and a standard lens camera.",Simple and accurate geometric correction with multiple projectors,NA:NA,2017
Peter Tieryas:Henry Garcia:Stacey Truman:Evan Bonifacio,"In Pixar's Lou, a combination of lost and found items comes to life, multiple pieces assembling to create the eponymous character. There were many visual and technical challenges to creating a character that can take on almost any form, using many of the random objects around him to convey emotion and feeling. We've highlighted several of the ways animation, modeling, rigging, simulation, and shading worked in conjunction to develop artistic and technical solutions to make Lou feel as real as the world around him.",Bringing Lou to life: a study in creating Lou,NA:NA:NA:NA,2017
George Nguyen:Peter Tieryas:Jae Hyung Kim:Josh Holtsclaw,"Cars 3's main antagonist is Jackson Storm, the first of a new breed of Next Gen racers. Lightning McQueen is definitely a classic, but Storm represents a sea change in many ways. Much of our specialized technical work on Storm reflects this progression to support his characterization on screen.",Revving up a storm: a talk on creating Jackson storm,NA:NA:NA:NA,2017
Kim Keech:Rachel Bibb:Brian Whited:Brett Achorn,"The art direction of Moana called for the use of hand-drawn animation to be intimately mixed with the primarily CG film. This direction pushed us to develop new workflows in order to not only achieve the very specific look but also allow for the direct interaction between the various CG and hand-drawn elements. Exploring these workflows has further bridged the gap between hand-drawn animation and CG animation, opening the way for continued exploration into hybrid animation.",The role of hand-drawn animation in Disney's Moana,NA:NA:NA:NA,2017
Tomohiro Hasegawa,"In this session, we will explain the issues that we set our sights on when creating monster art (for the Behemoth and Leviathan) given the advanced and sophisticated expressiveness that is now afforded by present-day consoles, as well as the opinions/initiatives we tackled in order to solve these issues.",A fantasy based on reality the art of Final Fantasy XV,NA,2017
Jens Jebens:Damien Gray:Simon Bull:Aidan Sarsfield,"The demand for asset complexity has increased by several orders of magnitude since The LEGO Movie. This has resulted in the need for the team at Animal Logic to further develop their proprietary render and shading pipeline, while significantly optimising nearly all aspects of asset creation. Animal Logic's already extensive library of LEGO bricks was expanded considerably, and centralised for use across multiple shows and multiple locations. Continued development of asset creation tools, and significant increases in pipeline automation ensured increased review cycles, greater consistency and minimal duplication of effort.","Evolving complexity management on ""the LEGO Batman movie""",NA:NA:NA:NA,2017
Hannes Ricklefs:Stefan Puschendorf:Sandilya Bhamidipati:Brian Eriksson:Akshay Pushparaja,"VFX production companies are currently challenged by the increasing complexity of visual effects shots combined with constant schedule demands. The ability to execute in an efficient and cost-effective manner requires extensive coordination between different sites, different departments, and different artists. This coordination demands data-intensive analysis of VFX workflows beyond standard project management practices and existing tools. In this paper, we propose a novel solution centered around a general evaluation data model and APIs that convert production data (job/scene/shot/schedule/task) to business intelligence insights enabling performance analytics and generation of data summarization for process controlling. These analytics provide an impact measuring framework for analyzing performance over time, with the introduction of new production technologies, and across separate jobs. Finally, we show how the historical production data can be used to create predictive analytics for the accurate forecasting of future VFX production process performance.",From VFX project management to predictive forecasting,NA:NA:NA:NA:NA,2017
Dhruv Govil,"In this talk, we describe the use of depth based compositing to accelerate collaboration between multiple artists on single shots. This workflow allows for increasingly complex shots to be finished in shorter timelines, even when dealing with large numbers of hero characters. SplitComp is a workflow for artists using depth compositing. It was originally developed for Cloudy With a Chance of Meatballs 2, but has since been used on every Sony Pictures Imageworks animated feature by animators and the cloth simulation team. It saves several hours of production time per artist every day of production. Over the course of a single feature film, it saves teams entire weeks worth of time. Older collaboration systems required artists to export their geometry so that it may be imported into other scene files, which is both time consuming and required artists to make a conscious effort. SplitComp, however, only requires playblasts which are a natural product of the artists workflow. This allows for significantly less effort from the artists and more frequent updates.",Animation collaboration with depth compositing,NA,2017
Daniel Heckenberg:Luke Emrose:Matthew Reid:Michael Balzer:Antoine Roille:Max Liani,"The technical and creative challenges of The LEGO Batman Movie motivated many changes to rendering at Animal Logic. The project was the first feature animation to be entirely rendered with the studio's proprietary path-tracer, Glimpse. Brick-based modelling, animation and destruction techniques taken to the extents of Gotham City required extraordinary scalability and control. The desire to separate complexity from artistic intent led to the development of a novel material composition system. Lensing and lighting choices also drove technical development for efficient in-render lens distortion, depth-of-field effects and accelerated handling of thousands of city and interior lights.",Rendering the darkness: glimpse on the LEGO Batman movie,NA:NA:NA:NA:NA:NA,2017
Ciaran Moloney:Jamie Haydock:Mathew Puchala:Miguel Perez Senent,"The Jedha sequence involved a colossal wave of destruction emanating from the epicenter of the Death Star attack on the planet Jedha. At ILM London, our task was to carefully plan the evolution of planet scale destruction from initial impact to final escape. Tectonic sized plates of rock, earth and sand had to rise up into the sky and form a wave 30,000 feet high. To achieve this we created a wide range of elements using well established ILM workflows. The scale of the scenes and simulations also meant that new workflows needed to be developed and the setups needed to be efficiently art directable.",Rogue One: A Star Wars Story - Jedha destruction,NA:NA:NA:NA,2017
Marc Bryant:Ian Coony:Jonathan Garcia,"For Disney's Moana, the challenges presented by our story's fiery foe, Te Kā, required cross-departmental collaboration and the creation of new pipeline technology. From raging fires and erupting molten lava to churning pyroclastic plumes of steam and smoke, Te Kā was comprised of a large number of layered environmental elements. Effects artists composed heavily art-directed simulations alongside reusable effects assets. This hybrid approach allowed artists to quickly block in and visualize large portions of their shot prior to simulation or rendering. This Foundation Effects (or FFX) workflow became a core strategy for delivering Te Kā's complex effects across multiple sequences.",Moana: Foundation of a Lava Monster,NA:NA:NA,2017
Dong Joo Byun:Shant Ergenian:Gregory Culp,"In the ""Lair of Tamatoa"" sequence of our latest movie Moana, we had 56 disco ball lighting effects shots. Our effects and lighting departments collaborated closely to create the bizarre and ludicrous environment of the scene. We developed a geometry-based lighting pipeline which allowed us to interactively design the light effects..",Moana: geometry based disco ball lighting for tamatoa's lair,NA:NA:NA,2017
Matt Ebb:Richard Sutherland:Daniel Heckenberg:Miles Green,"Building the digital sets for Guardians of the Galaxy Vol. 2 presented a unique challenge for Animal Logic's asset and FX teams. The creative brief involved two separate alien environments made from complex mathematical shapes, with an unprecedented amount of detail. As an additional challenge the environments had to match the look and feel of specifically styled concept art with a grand, monumental design. To meet the artistic requirements required a high level of creative control and manipulation of set elements that were to be rendered alongside many other highly detailed objects, and propagated quickly through the pipeline for fast feedback iterations. The team used a novel approach to modelling fractal objects using point clouds, taking advantage of pipeline capabilities to integrate FX objects with environment set-pieces. Additionally the team, leveraged instancing and high levels of geometric complexity using the in-house renderer Glimpse.","Building detailed fractal sets for ""Guardians of the Galaxy Vol. 2""",NA:NA:NA:NA,2017
Jim Malmros,In this technical post mortem we talk about how The Coalition implemented new custom graphics features and optimized the rendering technology to achieve the performance needed for the high visual bar for Gears Of War 41.,Gears of War 4: custom high-end graphics features and performance techniques,NA,2017
Colin Matisz:Andy Yi Shen,"Gears of War 4 is one of the first titles to take full advantage of the High Dynamic Range (HDR) TV output of the Xbox One S. We developed techniques & technologies to do this, including using HDR reference photography, a modified tonemapper, lumen-based lighting, HDR sky materials, camera exposure ranges, post processes and a tuning environment for emissive surfaces and visual effects. We built a strong foundation of physically based materials and other shader techniques to create a wide variety of realistic surfaces that react beautifully to light. Our artists had the challenge of achieving the goals of art direction, while meeting the performance goals of 1080p30 in single player and 1080p60 in multiplayer gameplay. Using available lighting techniques in UE4 + custom tools, we implemented best practices to optimize our artist workflow resulting in stunning visuals.",HDR TV output and lighting Gears of War 4,NA:NA,2017
Prasert Prasertvithyakarn:Tatsuhiro Joudan:Hidekazu Kato:Seiji Nanase:Masayoshi Miyamoto:Isamu Hasegawa,"In FINAL FANTASY XV, a triple-A open world RPG, we have proposed a new method of smart gameplay sharing by introducing a novel mechanism of automatic gameplay photograph generation. Unlike the classic screenshots that most players are familiar with, the photographs generated are depicted as though they were seen from the perspective of the in-game AI companion ""Prompto"". This system enhances the photos with several features such as shot facing, facial-body motion exaggeration, auto triggering, auto framing, auto focusing, auto post-filtering and auto album management. The system is capable of generating photographs that are stylish and unique, yet represent your gameplay in a new way no other games have accomplished before. With an in-game social network posting interface, generated photos can be easily shared. As a result, since the release of the game, our photos are flooding Facebook and Twitter, while creating a new benchmark to the world in the field of smart gameplay sharing.",Procedural photograph generation from actual gameplay: snapshot AI in FINAL FANTASY XV,NA:NA:NA:NA:NA:NA,2017
Kleber Garcia,"Circular Separable Convolution Depth of Field (CSC DoF) is a mathematical adaptation and implementation of a separable circular filter, which utilizes complex plane phasors to create very accurate and fast bokeh. At its core, this technique convolves a circular pattern blur in the frequency domain using a horizontal and a vertical pass, representing the frame buffer using complex numbers. This technique renders at magnitudes faster than brute-force and sprite-based approaches, since it is a separable convolution. Important properties of this technique include convolution separability, low memory bandwidth and large radii circles. The technique has been shipped on Madden NFL 15, Madden NFL 16, Madden NFL 17, Fifa 17 and PGA Tour Rory McIlroy. The implementation includes an offline shader code generation step containing pre-computed frequency domain filters, multiple weighted passes for imaginary and real number processing. We will present the mathematical derivation and some caveats to achieve the required precision for intermediate frequency domain frame buffers.",Circular separable convolution depth of field,NA,2017
Daniel Heckenberg:Steve Agland:Jean Pascal leBlanc:Raphael Barth,"We created an efficient pipeline for automated, HDR light probes for the hybrid live-action / animated feature film Peter Rabbit. A specially developed ""360°"" spherical camera allows on-set acquisition at more positions and in less time than traditional techniques. Reduced capture time, drastically simplified stitching and a custom multiple-exposure raw to HDR process minimizes artefacts in the resulting images. A semi-automated system recovers clipped radiance in direct sunlight using surfaces with known properties. By recording capture location and orientation and combining with other scene data we produce automated rendering setups using the light probes for illumination and projection onto 3d render geometry.",Automated light probes from capture to render for Peter Rabbit,NA:NA:NA:NA,2017
Lucio Moser:Darren Hendler:Doug Roble,"We present Masquerade, a novel modular and expandable tool for adding fine-scale details to facial motion capture data from head-mounted cameras. After studying two important related works we developed a framework to reproduce the original approaches as well as to test equally promising alternatives. This framework has been vital for understanding the limitations of previous approaches and to explore ways to improve the results. Our final solution was a combination of algorithms and data representations that produced better results than previous works when tested with our evaluation data. Since then, Masquerade is being actively used in production for enhancing marker data with fine-scale details.",Masquerade: fine-scale details for head-mounted camera motion capture data,NA:NA:NA,2017
Andrew Feng:Evan Suma:Ari Shapiro,"We demonstrate a system that can generate a photorealistic, interactive 3D character from a human subject that is capable of movement, emotion, speech and gesture in less than 20 minutes without the need for 3D artist intervention or specialized technical knowledge through a near automatic process. Our method uses mostly commodity- or of-the-shelf hardware. We demonstrate the just-in-time use of generating such 3D models for virtual and augmented reality, games, simulation and communication. We anticipate that the inexpensive generation of such photorealistic models will be useful in many venues where a just-in-time 3D construction of digital avatars that resemble particular human subjects is necessary. Figure 1 shows the overall workflow of our virtual character creation pipeline.","Just-in-time, viable, 3d avatars from scans",NA:NA:NA,2017
Adrien Kaiser:Jose Alonso Ybanez Zepeda:Tamy Boubekeur,"Modern RGB-D sensors are widely used for indoor 3D capture, with applications ranging from modeling to robotics, through gaming. Nevertheless, their use is limited by their low resolution, with frames often corrupted with noise, missing data and temporal inconsistencies. In order to cope with all these issues, we present Proxy Clouds, a multiplanar superstructure for unified real-time processing of RGB-D data. By generating and updating through time a single set of rich statistics parameterized over planar proxies from raw RGB-D data, several processing primitives can be applied to improve the quality of the RGB-D stream on-the-fly or lighten further operations. We illustrate the use of Proxy Clouds on several applications, including noise and temporal flickering removal, hole filling, resampling, color processing and compression. We present experiments performed with our framework in indoor scenes of different natures captured with a consumer depth sensor.",Proxy clouds for RGB-D stream processing: an insight,NA:NA:NA,2017
Matthew Cong:Lana Lan:Ronald Fedkiw,"For Kong: Skull Island, Industrial Light & Magic created an anatomically motivated facial simulation model for Kong that includes the facial skeleton and musculature. We applied a muscle simulation framework that allowed us to target facial shapes while maintaining desirable physical properties to ensure that the simulations stayed on-model. This allowed muscle simulations to be used as a powerful tool for adding physical detail to and improving the anatomical validity of both blendshapes and blendshape animations in order to achieve more realistic facial animation with less hand sculpting.",Muscle simulation for facial animation in Kong: Skull Island,NA:NA:NA,2017
David Bollo,"In this talk, we present three of the techniques that we developed to deliver high performance animation in the Gears of War 41 video game. First, we present a ""warp point""-driven system for dealing with character traversal through an irregular environment. This system builds on previous motion warping work and is over twice as fast as a more traditional blend space based approach. Next, we introduce a novel approach to handling motion transitions that eliminates traditional blended transitions and replaces them with an animation post-processing step that is 60% cheaper to compute overall. Finally, we introduce a fast but effective heuristic for improving the quality of these motion transitions by automatically matching the locomotion foot phase between the transitioning animations.",High performance animation in Gears of War 4,NA,2017
Gene S. Lee:Christian Eisenacher:Andy Lin:Noel Villegas,"This paper presents a conservative, uniform method for handling scene constraints, such as look at and parent, in a pose-based caching system. The constraints are organized into a dependency graph where nodes represent the control caches of a rig, and directed arcs link the rigs to specific control caches. For any animation update, the dependency graph indicates which cache to clear and which other rigs to subsequently update. This method supports pre-evaluation, avoids expensive state tracking, and is easy to implement. The result is a seamless experience that works with all types of constraints while preserving real-time performance.",Handling scene constraints for pose-based caching,NA:NA:NA:NA,2017
Pilar Molina Lopez:Jake Richards,"Eyes are often the most important feature in a character's performance, conveying emotion, timing and intention as well as hints about what comes next in the story. Stories are driven by characters and audience investment comes from their empathy for those characters. Unless the viewer is making a concerted effort to look elsewhere on screen, they usually concentrate on the eyes of the main character. Therefore, a great amount of effort and time is spent making the eyes of our characters look as expressive as possible. When done improperly, the eyes will make a character look dead and unappealing. Our technology utilized to create our characters' eyes gives artists the flexibility to push the boundaries of their craft; it helps them portray characters that communicate the emotions that a story requires. In order to achieve this we designed a set of techniques that compose our eye pipeline. It has been refined over many years in a continued effort and collaboration among several departments at our studio, from modeling to lighting passing through animation and rigging. It allows animators to follow their expressive style while also providing materials artists and lighters with the necessary input to achieve a realistic look.",The eyes have it: comprehensive eye control for animated characters,NA:NA,2017
Marc Thyng:Christopher Evart:Toby Jones:Aleka McAdams,"Beginning with the early concept art, Moana featured characters with long curly hair interacting heavily with both the characters and their environment. This level of complexity in hair interactions and dynamics presented demanding simulation needs which led to changes throughout the hair simulation pipeline, from grooming to technical animation. In order to overcome these challenges we implemented a new hair model and data type, as well as overhauled how we handled hair collisions. We discuss the motivation and details of our hair simulation and technical animation process, as well as the implications of the new model both to artist interactions and our overall pipeline.",The art and technology of hair simulation in Disney's Moana,NA:NA:NA:NA,2017
Brian Missey:Amaury Aubel:Arunachalam Somasundaram:Megha Davalath,"Hair plays a feature role in the film Trolls. It is a crucial part of the overall character design of the Trolls themselves, typically composing over half the silhouette of the character. However, the use of hair on the show went well beyond the standard coif and bled into acting beats, traditional effects, environments, and set pieces. This talk presents the wide variety of unique and challenging hair effects in the film and the techniques used to create them.",Hairy effects in Trolls,NA:NA:NA:NA,2017
Chloe LeGendre:Loc Hyunh:Shanhe Wang:Paul Debevec,"We present a technique for modeling the vellus hair over the face based on observations of asperity scattering along a subject's silhouette. We photograph the backlit subject in profile and three-quarters views with a high-resolution DSLR camera to observe the vellus hair on the side and front of the face and separately acquire a 3D scan of the face geometry and texture. We render a library of backlit vellus hair patch samples with different geometric parameters such as density, orientation, and curvature, and we compute image statistics for each set of parameters. We trace the silhouette contour in each face image and straighten the backlit hair silhouettes using image resampling. We compute image statistics for each section of the facial silhouette and determine which set of hair modeling parameters best matches the statistics. We then generate a complete set of vellus hairs for the face by interpolating and extrapolating the matched parameters over the skin. We add the modeled vellus hairs to the 3D facial scan and generate renderings under novel lighting conditions, generally matching the appearance of real photographs.",Modeling vellus facial hair from asperity scattering silhouettes,NA:NA:NA:NA,2017
Elias Saliba:Mustafa Barkaoui:Hind Wakil,"Lighthouse VFX production, and post production house is specialized in visual effects.","Behind the scenes of VFX in the Middle East & Syria: ""in art we trust""",NA:NA:NA,2017
Sean Palmer:Jonathan Garcia:Sara Drakeley:Patrick Kelly:Ralf Habel,"Disney's Moana was the largest and most complex water project the studio had ever undertaken. Over 900 shots required ocean interaction, which included boat wakes, splashes, shorelines, walls of water, and highly art-directed sentient water. Our previous films' water techniques would not scale to address the complexity and volume of work required by Moana and staffing and time constraints necessitated automating large parts of the process. We redesigned our pipeline to provide a flexible authoring process for a lightweight implicit ocean representation. This new workflow allowed artists to visualize and edit specific parts of the water setup and easily share their updates with other departments.",The ocean and water pipeline of Disney's Moana,NA:NA:NA:NA:NA,2017
Ben Frost:Alexey Stomakhin:Hiroaki Narita,"For Disney's Moana, water was a dominant part of island life, in fact it had a life of itfis own. Presenting itself as a character, water was ever present, in a multitude of shapes and scales. An end-to-end water pipeline was developed for this film [Garcia et al. 2016], including the creation of proprietary fluid APIC solver [Jiang et al. 2015] named Splash. This gave us physically accurate simulations. The challenge with performing water was to provide art-directed simulations, defying physics, yet remaining in a grounded sense of possibility. Incorporating natural swells and flows to support the building of designed shapes limited anthropomorphic features, and played to our goal of communicating that this character is the ocean as a whole.",Moana: performing water,NA:NA:NA,2017
Rob Hopper:Kai Wolter,"For Pirates of the Caribbean: Dead Men Tell no Tales MPC faced the creative challenge to produce highly believable ocean and water effects interacting with full-CG ships and characters. This included pirate ships emerging from the bottom of the sea, a parting ocean giving space to an enormous three dimensional set, and a model ship in a bottle containing a full-sized ocean. The varied scale and nature of these effects required us to rethink our simulation techniques and toolset. In this talk we present our approaches to animate, simulate and render these using our newly developed ocean toolkit and tighter integration of Autodesk Bifrost and SideFX Houdini into our FX and rendering pipeline.",The water effects of Pirates of the Caribbean: Dead Men Tell no Tales,NA:NA,2017
Stephen Marshall:Tim Speltz:Greg Gladstone:Krzysztof Rost:Jon Reisch,"The world of Disney Pixar's Cars 3 finds our hero Lightning McQueen on a journey to reconnect to the roots of ""real"" racing as he struggles to stage a comeback in a sport which is quickly evolving past him. Over the course of the film, our characters race on beaches alongside lapping waves, in abandoned ghost tracks, through winding mountain forests, and even in a raucus, muddy demolition derby. Providing a sense of believable interaction between our characters and these varied environments in over 600 shots was one of the key responsibilities of our FX team on Cars 3. In order to achieve the scope and scale of this work efficiently, we built on sequence-wide workflows and independent ""clustered"" simulations presented last year in (Reisch et al. 2016), extending these strategies to effects unique to the show. Creating a common shared core to our simulation and effects-asset rigs provided artists with a familiar starting point regardless of whether they were working on volumetric dust, rigid-body debris, point-based dynamics sand, or even viscous mud simulations. A focus on stability, artist experience, and optimized workflows which scaled to take advantage of our render farm, allowed our team to achieve visually consistent, high quality results on an accelerated schedule.",Racing to the finish line: effects challenges on Cars 3,NA:NA:NA:NA:NA,2017
Alejandro Conty Estevez:Christopher Kulla,We present a technique to importance sample large collections of lights. A bounding volume hierarchy over all lights is traversed at each shading point using a single random number in a way that importance samples their predicted contribution. We further improve the performance of the algorithm by forcing splitting until the importance of a cluster is sufficiently representative of its contents.,Importance sampling of many lights with adaptive tree splitting,NA:NA,2017
Alexander Keller:Carsten Wächter:Matthias Raab:Daniel Seibert:Dietger van Antwerpen:Johann Korndörfer:Lutz Kettner,"While ray tracing has become increasingly common and path tracing is well understood by now, a major challenge consists of crafting an easy-to-use and efficient system implementing these technologies. Following a purely physically-based paradigm while still allowing for artistic workflows, the Iray light transport simulation and rendering system allows for rendering complex scenes by the push of a button and thus makes accurate light transport simulation widely available. We discuss the challenges and implementation choices that follow from our primary design decisions, demonstrating that such a rendering system can be made a practical, scalable, and efficient real-world application that is in use by many industry professionals today.",The iray light transport simulation and rendering system,NA:NA:NA:NA:NA:NA:NA,2017
Beibei Wang:Nicolas Holzschuch,"Illumination simulation involving participating media is computationally intensive. The overall aspect of the material depends on simulating a large number of scattering events inside the material. Combined, the contributions of these scattering events are a smooth illumination. Computing them using ray-tracing or photon-mapping algorithms is expensive: convergence time is high, and pictures before convergence are low quality (see Figure 1). In this paper, we precompute the result of multiple scattering events, assuming an infinite medium, and store it in two 4D tables. These precomputed tables can be used with many rendering algorithms, such as Virtual Ray Lights (VRL), Unified Point Beams and Paths (UPBP) or Manifold Exploration Metropolis Light Transport (MEMLT), greatly reducing the convergence time. The original algorithm takes care of low order scattering (single and double scattering), while our precomputations are used for multiple scattering (more than two scattering events).",Precomputed multiple scattering for light simulation in participating medium,NA:NA,2017
Norbert Bus:Tamy Boubekeur,"We propose a novel representation of the light field tailored to improve importance sampling for Monte Carlo rendering. The domain of the light field i.e., the product space of spatial positions and directions is hierarchically subdivided into subsets on which local models characterize the light transport.The data structure is based on double trees, and only approximates the exact light field, but enables efficient queries for importance sampling and easy setup by tracing photons in the scene. The framework is simple yet flexible, supports any type of local model for representing the light field, provided it can be efficiently importance sampled, and progressive refinement with an arbitrary number of photons. Last, we provide a reference open source implementation of our method.",Double hierarchies for efficient sampling in Monte Carlo rendering,NA:NA,2017
Colin Penty:Ian Wong,We have created a new material system for Gears of War 4 inside Unreal Engine that allows artists to layer dozens of materials with complete material tuning control and flexibility - then cook out the results in-engine for efficient run-time performance.,Gears of War 4: creating a layered material system for 60fps,NA:NA,2017
Priyamvad Deshmukh:Feng Xie:Eric Tabellion,"Since Shrek 2, DreamWorks artists have used the fabric model developed by [Glumac and Doepp 2004] extensively on cloth material shading. Even after we developed the physically based microcylinderical cloth model by [Sadeghi et al. 2013], they continued to prefer the intuitive control of the DreamWorks fabric shading model, which is also a cyindrical shading model, with easy to use artistic controls for highlights, and highlight directions.",DreamWorks fabric shading model: from artist friendly to physically plausible,NA:NA:NA,2017
Lutz Kettner,"Using a term rewriting system, simplifications of physically-based materials described in a declarative programming language can be created automatically. Sets of rules for the term rewriting system allow for customizing simplifications according to use cases. Examples include automatic level-of-detail generation or simplification of materials for faster rendering in realtime viewports and games.",Fast automatic level of detail for physically-based materials,NA,2017
Yuxiao Du:Ergun Akleman,"In this work, we have developed an approach to include any cross-hatching technique into any rendering system with global illumination effects (see Figure 1). Our new approach provide a robust computation to obtain hand-drawn effects for a wide variety of diffuse and specular materials. Our contributions can be summarized as follows: (1) A Barycentric shader that can provide generalized cross-hatching with multi-textures; and (2) A texture synthesis method that can automatically produce crosshatching textures from any given image.",Designing look-and-feel using generalized crosshatching,NA:NA,2017
Dong Joo Byun:Alexey Stomakhin,"We used two different solutions for generating crashing waves for more than 40 shots in Moana. Our profile curve based wave deformer was developed and used for art-directed design of shapes, motion, and composition of running and crashing waves. In contrast to previously developed wave deformers, we designed a cross section shape animation by providing a series of profile curves which represented the animation keys. These profile curves could be hand plotted curves or mathematically calculated changing profiles, which means any kind of choreographic touch could be applied for designing the wave shapes. We could design multiple crashing waves for huge scale tsunami scenes and we could art direct the timing and composition of the waves which would fit well with the character animation and camera works. For scenarios demanding more realism, motion complexity and physical accuracy, we adopted a fully simulated approach. Our APIC-based fluid solver [Jiang et al. 2015] was equipped with control mechanisms allowing us to precisely choreograph the motion of breaking waves to the needs of a specific shot. Though more expensive than procedural approaches, this solution was much more preferable for ""hero"" shots with close up interaction with boats and characters.",Moana: crashing waves,NA:NA,2017
Gergely Klár:Jeff Budsberg:Matt Titus:Stephen Jones:Ken Museth,"We present two complementary techniques for Material Point Method (MPM) based simulations to improve their performance and to allow for fine-grained artistic control. Our entirely GPU-based solver is able perform up to five times faster than its multithreaded CPU counterpart as a result of our novel particle and grid transfer algorithms. On top of this, we introduce Adaptive Particle Activation, that both makes it possible to simulate only a reduced number of particles, and to give artists means for fine direction over the simulation.",Production ready MPM simulations,NA:NA:NA:NA:NA,2017
Todd Keeler:Robert Bridson,We propose a novel method of compressing a fluid effect for realtime playback by using a compact mathematical representation of the spatio-temporal fluid surface. To create the surface representation we use as input a set of fluid meshes from standard techniques along with the simulation's surface velocity to construct a spatially adaptive and temporally coherent Lagrangian least-squares representation of the surface. We then compress the Lagrangian point data using a technique called Fourier extensions for further compression gains. The resulting surface is easily decompressed and amenable to being evaluated in parallel. We demonstrate real-time and interactive decompression and meshing of surfaces using a dual-contouring method that efficiently uses the decompressed particle data and least-squares representation to create a view dependent triangulation.,Compact iso-surface representation and compression for fluid phenomena,NA:NA,2017
Michael B. Nielsen:Konstantinos Stamatelos:Adrian Graham:Marcus Nordenstam:Robert Bridson,"A guided liquid simulation [Nielsen and Bridson 2011] re-simulates a thin surface layer of an existing liquid simulation at higher resolution, or simulates just a thin layer near the surface of an animated input sequence produced e.g. by hand or by spectral wave methods. The movement of the simulated surface layer is guided by the underlying animation and this technique has been used to achieve high surface detail and art-directed water effects on several movies including ""Hobbit - The Desolation of Smaug"" and ""Tintin - Secret of the Unicorn"". Despite the successful application of guided simulations in production, the method as originally proposed requires a fair amount of manual setup time and can be computationally costly in scenarios where the area of focus (such as a moving ship) covers a large and non-regular area over time. In this talk we present an outline of a novel set of algorithms facilitating localized guided simulations, where the guided simulation takes place only within a - possibly animated - local region specified by the user. We have implemented our algorithms in Autodesk Maya's procedural Bifrost framework and integrated them with Bifrost's adaptive FLIP solver. We demonstrate with several examples the benefits of our approach in terms of computational efficiency and ease of use. Additionally our tool was utilized by Moving Picture Company (MPC) to create high resolution art-directed water simulations on Pirates of the Caribbean - Dead Men Tell No Tales.",Localized guided liquid simulations in bifrost,NA:NA:NA:NA:NA,2017
Chris Kramer,NA,Evolution of AR in Pokémon go,NA,2017
Kent Bye,NA,How VR changes the sense of ourselves & reality,NA,2017
Graham Roberts,NA,A new (virtual) reality at the New York Times,NA,2017
Mike Jutan:Steve Ellis,"For Rogue One: A Star Wars Story, executive producer John Knoll wanted the all-CG shots to feel consistent with the signature handheld camera style that director Gareth Edwards captured on set. To achieve this, the Industrial Light & Magic (ILM) R&D team created a director-centric virtual camera system that encourages open set exploration of the all-CG Star Wars worlds. We enabled the director to achieve his artistic vision via our low footprint, flexible, iteration-based production toolset.",Director-centric virtual camera production tools for rogue one,NA:NA,2017
Vincent Serritella:David Lally:Brian Larsen:Farhez Rayani:Jason Kim:Matt Silas,"In 2016, Pixar launched an internal, experimental storytelling initiative to enable new creative voices, as well as explore alternative storytelling techniques, pipelines, and workflows in production. Filmmakers are granted total creative freedom to develop a story, design a world, and produce a short film, within six months, and without any executive supervision. Smash and Grab is a seven minute short film that explores the use of comic book sketches, virtual production, motion/camera capture and procedural shading and lighting techniques. With the backdrop of a busy feature studio, limited resources, and a minimal crew, this talk is the story of our journey.",Smash and grab: off the rails filmmaking at Pixar,NA:NA:NA:NA:NA:NA,2017
Jeff Stringer:Owen Nelson:Tony Aiello,"To manage the increased complexity in its hybrid stop-motion/CG animated features, LAIKA built a custom production scheduling system, converting the traditional tactile and laborious approach of planning a stop-motion shooting schedule on a big board into a fully digital, live and interactive experience connecting the entire crew.",LAIKA's digital big boards,NA:NA:NA,2017
Kenneth Vanhoey:Carlos Eduardo Porto de Oliveira:Hayko Riemenschneider:András Bódis-Szomorú:Santiago Manén:Danda Pani Paudel:Michael Gygli:Nikolay Kobyshev:Till Kroeger:Dengxin Dai:Luc Van Gool,"VarCity - the Video is a short documentary-style CGI movie explaining the main outcomes of the 5-year Computer Vision research project VarCity. Besides a coarse overview of the research, we present the challenges that were faced in its production, induced by two factors: i) usage of imperfect research data produced by automatic algorithms, and ii) human factors, like federating researchers and a CG artist around a similar goal many had a different conception of, while no one had a detailed overview of all the content. Successive achievement was driven by some ad-hoc technical developments but more importantly of detailed and abundant communication and agreement on common best practices.",VarCity - the video: the struggles and triumphs of leveraging fundamental research results in a graphics video production,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2017
Andrzej Zarzycki:Martina Decker,"Adaptive designs and intelligent spaces are in the forefront of the current architectural and product design discourse. They engage users in interactive dialogue, allow for public domain authoring, and are critical factors in sustainable designs where buildings monitor their own performance and respond to environmental factors or user needs (figure 1).",Programmable buildings: architecture as an interaction interface powered with programmable matter,NA:NA,2017
Jochen Suessmuth:Sky Asay:Conor Fitzgerald:Mario Poerner:Davoud Ohadi:Detlef Mueller,"The aim of the adidas digital creation program is to build an industry leading system of 3-D tools that empowers creativity and connects creators digitally. Our pipeline is tailored for a product design and creation community that is non-technical. Our users benefit from better visualization and information for decision-making such as concept reviews or improved factory handovers. The brand benefits from more useful assets for sell-in, and consumers benefit from products that it ever changing trends. 3-D tools for product design that enable constant change through non-destructive behavior at a speed of workflow that is competitive with traditional 2-D methods haven't been available in the past. In this talk, we'll share how working side by side with our footwear designers, we were able to find novel workflows that inspire new tools and integrations.",Concept through creation: establishing a 3-D design process in the footwear industry,NA:NA:NA:NA:NA:NA,2017
Don Derek Haddad:Gershon Dublon:Brian Mayton:Spencer Russell:Xiao Xiao:Ken Perlin:Joseph A. Paradiso,"The rise of ubiquitous sensing enables the harvesting of massive amounts of data from the physical world. This data is often used to drive the behavior of devices, and when presented to users, it is most commonly visualized quantitatively, as graphs and charts. Another approach for the representation of sensor network data presents the data within a rich, virtual environment. These scenes can be generated based on the physical environment, and their appearance can change based on the state of sensor nodes. By freely exploring these environments, users gain a vivid, multi-modal, and experiential perspective into large, multi-dimensional datasets. This paper presents the concept of ""Resynthesizing Reality"" through a case study we have created based on a network of environmental sensors deployed at a large-scale wetland restoration site. We describe the technical implementation of our system, present techniques to visualize sensor data within the virtual environment, and discuss potential applications for such Resynthesized Realities.",Resynthesizing reality: driving vivid virtual environments from sensor networks,NA:NA:NA:NA:NA:NA:NA,2017
Daniela Hasenbring:Jeremy Hoey,"Sprout is our proprietary Maya-based tool for hand-dressing digital environments with large quantities of high-resolution assets like trees, plants and rocks. It was developed at Sony Picture Imageworks (SPI) to address the need for an interactive artist-friendly tool that was fully integrated into SPIfis existing pipeline. Prior to the development of Sprout, environment dressing at SPI was done primarily in Houdini or procedurally at render-time and was thus the province of FX TDs. In Sprout, artists can load any asset and quickly fipaintfi instances onto any other geometry using a brush paradigm familiar to anyone who has used Photoshop. Sophisticated lightweight OpenGL representations keep performance nimble, and all instances remain fully editable by the artist to allow for highly art-directed environment dressing. Sprout has made environment dressing at SPI available to a larger variety of artists, being leveraged most recently for photoreal jungle environments for an upcoming VFX motion picture as shown in Figure 1.",Interactive environment creation with sprout,NA:NA,2017
James Bartolozzi:Matt Kuruc,"The curve hierarchy skeleton has been a foundational component of Pixar's vegetation pipeline since Cars 2 (2011). This skeleton is leveraged when building flow fields for texture synthesis, generating procedural secondary vegetation detail, and as a basis for simulation rigs. Our current skeletonization pipeline is built around mesh contraction [Shek et al. 2010] [Au et al. 2008] which is sensitive to the underlying topology. This method creates undesirable curve structures when modelers add musculature to the trunk mesh. Technical directors would reapply manual fixes to the skeleton over the course of iterating on the model. Recent work examining constructing skeletons using point clouds and volumetrics inspired us to develop a new hybrid approach. This alternative to the traditional mesh contraction algorithm has shown to be fast, reliable, accurate, and minimizes constraints on our modeling artists.",A hybrid approach to procedural tree skeletonization,NA:NA,2017
Wanho Choi:Nayoung Kim:Julie Jang:Sanghun Kim:Dohyun Yang,"Although there is commercially available software for producing digital fur and feathers, creating photorealistic digital creatures under a low budget is still no trivial matter. Because no software could fulfill our purposes at the time of the making of our first movie Mr. Go, we decided to develop our own custom solution, ZelosFur [Choi et al. 2013]. While it made possible furry digital creature creation for subsequent projects, this prototypical system lacked the flexibility to easily add new features with backward compatibility and did not provide artists with enough freedom or control over the grooming process. Zelos Node Network (ZENN) is a new procedural solution that allows for quick, easy, and art-directable creation of all kinds of body coverings for digital creatures (e.g. fur, feathers, scales, etc.) By extension, it can also be used to create forests, rocks, and verdant landscapes for digital environments. In this talk, we discuss how to design and implement a procedural grooming workflow within ZENN and briefly address our caching and rendering process.",Build your own procedural grooming pipeline,NA:NA:NA:NA:NA,2017
Arunachalam Somasundaram,"We present FurCollide, a fast, robust, and artist friendly tool used for collision detection and collision resolution of fur curves with meshes. The tool helps artists interact with and control tens of thousands of curves with ease while providing high fidelity realistic and/or artistic collision results. This tool is in use at DreamWorks Animation and has been used in a wide variety of fur and/or grass collision situations in various films.","FurCollide: fast, robust, and controllable fur collisions with meshes",NA,2017
Andy Rowan-Robinson,"Field Trip to Mars is the first-ever headset-free group virtual reality vehicle experience. Taking the literal shape of a classic yellow school bus, the vehicle is home to an immersive virtual experience that transports school children to the surface of the Red Planet.",Field trip to Mars,NA,2017
Oculus,"In many ways, Oculus Story Studio's VR experience, ""Dear Angelica"" as shown in Figure 1, is a unique project. We wanted to immerse the viewers inside a series of hand-drawn illustrations and tell a story by artfully transitioning between the drawings, and breathe life into these drawings by adding various animations and visual effects. In trying to create this ground-breaking experience, we had to create a brand-new pipeline: from inventing the tools to create these 3D illustrations, to processing and animating the drawings, to rendering them in virtual reality.",Dear angelica: breathing life into VR illustrations,NA,2017
Dave Mauriello:Jason Kirk:Jeremy Fernsler,"Presented are two novel approaches to visualizing both internal and external anatomy of the heart through the cardiac cycle. The first uses ""windows"" manually cut through each chamber of a virtual heart model. The use of windows allows for internal and external views simultaneously while showing the varying thickness of the ventricular walls through the cardiac cycle. Internal structures such as both semilunar valves, both AV valves, chordae tendineae, and papillary muscles are kept intact and can be visualized in motion. The second approach is a rigging and control system that allows for independent rotation directions for the base, midpoint and apex of each ventricle both internally and externally, allowing for a more accurate wringing motion.",Two novel approaches to visualizing internal and external anatomy of the cardiac cycle with a windowed virtual heart model,NA:NA:NA,2017
Matthew Chambers:Justin Israel:Andy Wright,"To ensure peak utilization of hardware resources, as well as handle the increasingly dynamic demands placed on its render farm infrastructure, WETA Digital developed custom queuing, scheduling, job description and submission systems - which work in concert to maximize the available cores across a large range of non-uniform task types. The render farm is one of the most important, high traffic components of a modern VFX pipeline. Beyond the hardware itself a render farm requires careful management and maintenance to ensure it is operating at peak efficiency. In WETAs case this hardware consists of a mix of over 80,000 CPU cores and a number of GPU resources, and as this has grown it has introduced many interesting scalability challenges. In this talk we aim to present our end-to-end solutions in the render farm space, from the structure of the resource and the inherent problems introduced at this scale, through the development of Plow - our management, queuing and monitoring software. Finally we will detail the deployment process and production benefits realized. Within each section we intend to present the scalability issues encountered, and detail our strategy, process and results in solving these problems. The ever increasing complexity and computational demands of modern VFX drives WETAs need to innovate in all areas, from surfacing, rendering and simulation but also to core pipeline infrastructure.",Large scale VFX pipelines,NA:NA:NA,2017
Daniel Bergel:Craig Dibble:Pauline Koh:James Pearson:Hannes Ricklefs,"Disney's The Jungle Book required MPC to deliver work of an unprecedented visual complexity and quality. To enable Disney to fully realise their creative vision, MPC wanted to ensure it had burst compute capacity available through flexible and scalable cloud based resources. The major technical challenge was to provide this burst capacity whilst meeting the strict security requirements of our client, something which had not previously been achieved for a production of this scale or sensitivity. The project needed dedicated resources across Technology, Operations and Production to holistically capture and address everyone's requirements and process constraints. Across all these domains the project was considered a huge success. This talk presents the key challenges faced including a technical overview of the architecture, the essential management tools, and the interaction with production from identifying appropriate job types to effective utilisation of these virtual resources.",Cloudy with a chance of rendering,NA:NA:NA:NA:NA,2017
Mungo Pay:Damien Maupu:Martin Pražák,"The complexity of crowd shots can vary greatly, from simple vignetting tasks that add life to an environment, to large and complex battle sequences involving thousands of characters. For this reason, a ""one size fits all"" crowd solution might not be optimal, both in terms of design and usability, but also allocation of crew. In this talk we present a suite of tools, developed across multiple platforms, each optimised for specific crowd tasks. These are underpinned by a data interchange library to allow for modification at any stage of the pipeline.",Flexible pipeline for crowd production,NA:NA:NA,2017
Ton Roosendaal:Francesco Siddi,"For ""Cosmos Laundromat"" - CAF 2016 Jury Award winner - the Blender team, headed by CG pioneer and producer Ton Roosendaal, developed and used a complete open source creation pipeline. The team released several other shorts since then, including a 360-degrees VR experience and a pitch for the feature animation film ""Agent 327"". Developing and sharing open source technologies is a great challenge, and leads to great benefits for the small and medium animation studios.","Beyond ""cosmos laundromat"": blender's open source studio pipeline",NA:NA,2017
Dominik P. Käser:Evan Parker:Adam Glazier:Mike Podwal:Matt Seegmiller:Chun-Po Wang:Per Karlsson:Nadav Ashkenazi:Joanna Kim:Andre Le:Matthias Bühlmann:Joshua Moshier,"One of the great promises of virtual reality is that it can allow people to visit places in the world that they might otherwise be unable to. Since the recent renaissance of virtual reality, content creators have exercised various techniques such as 360-degree cameras and photogrammetry to make this promise come true. At Google, we spent more than 10 years capturing every part of the world as part of the Google Earth project. The result is a rich 3D mesh that contains trillions of triangles [Kontkanen and Parker 2014] and as such is predestined to be a good data source for VR content. In [Kaeser and Buehlmann 2016] we discussed some of our early experiments with bringing Google Earth to virtual reality, but without a focus on developing a product. Following these experiments, we worked extensively to create a well-rounded product, Google Earth VR, which we eventually launched to the world in November 2016. Google Earth VR quickly became one of the most actively used VR applications in the market and has won several awards since. This talk discusses the journey of the Google Earth VR project from its early prototypes to its final launched stage.",The making of Google earth VR,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2017
Bruna Berford:Carlos Diaz-Padron:Terry Kaleas:Irem Oz:Devon Penney,"Penrose Studios is an animation studio that produces content for VR and AR devices. There are a multitude of fascinating new artistic challenges facing animators and directors creating this type of content, such as animating without a fixed camera and very few cuts. Studios currently leverage existing animation and production tools to create content rendered in real time. However, this new medium is hampered by traditional processes that greatly reduce artist productivity and creativity. For instance, animating in traditional third party tools and importing into VR often leads to unexpected results, which results in a suboptimal iteration loop. We present a variety of tools that improve the traditional animation pipeline to address these issues and reduce the frequency of context switches between virtual reality and 2D monitors. The result is a toolset where artists feel empowered to explore animation in this new art form without being crippled by inefficient workflows.",Building an animation pipeline for VR stories,NA:NA:NA:NA:NA,2017
Chris Healer,"Chris discusses the VR creation process using examples ranging from Hollywood VR projects to those from The New York Times. He'll dive into live-action VR photography and photogrammetric reality capture techniques, and will explore the linkage between tried and true VFX trickery and the illusion of a Virtual Reality. From this presentation audience members will take away a greater understanding of the VR Post Production process, especially in relation to VFX workflows. He'll use recent recognizable VR projects as examples, including ""Lincoln in the Bardo"" from The New York Times, Doug Liman's ""Invisible,"" and others.",Visual effects for VR,NA,2017
Brett Achorn:Sean Palmer:Larry Wu,"In Walt Disney Animation Studio's Moana, a key moment in the story calls for the protagonists to confront a floating island barge crewed by coconut-clad pirates: the Kakamora. Combining a large moving set on an animated ocean with active crowds, sails, ropes and effects simulations required coordination across multiple departments and some on-the-fly innovations. To handle the challenges the sequence presented, we made extensive use of automation and procedural tools along with some creative design decisions to save artist time and allow for greater reuse.",Building moana's kakamora barge,NA:NA:NA,2017
Melt van der Spuy,"On Storks, we were asked to design and animate complex structures composed of hundreds of interacting characters. This lead us to develop PackIT - an artist focused approach to efficiently deal with large numbers of characters. Our approach is in contrast to other methods which often rely on simulation or group behaviors. We treat each character individually and efficiently manage the conversion between rigs and geometry caches, putting the control back with the artist. Using the extensive library of in-house building blocks we reduce development time and impact on the pipeline. As a result of the huge performance gains and reduced scene load times, a single artists can easily complete shots involving hundreds of characters.",PackIT: animating complicated character groups easily,NA,2017
Greg Mourino:Mason Evans:Kevin Edzenga:Svetla Cavaleri:Mark Adams:Justin Bisceglio,"With the help of new tools, we streamlined our review and render processes for crowds to triple our shot count on our latest show, Ferdinand. At the same time, we integrated some novel approaches to complex deformation features for cloth and facial animation, which elevated the quality of our crowd animations.",Populating the crowds in Ferdinand,NA:NA:NA:NA:NA:NA,2017
Damien Maupu:Emanuele Goffredo:Nile Hylton:Mungo Pay:Martin Pražák,"While crowd simulation frameworks can be very powerful for virtual crowd generation, in a VFX context they can also be unwieldy due to their chaotic nature. Small changes on the inputs can produce markedly different results, which can be problematic when attempting to adhere to a director's vision. Artist driven tools allow much more flexibility when constructing scenes, speed up turn-around time and can produce extremely dynamic crowd shots. To generate virtual crowds, Double Negative VFX (Dneg) has recently transitioned from an in-house standalone simulation-based solution to an artist-driven framework integrated into SideFX's Houdini.",Artist-driven crowd authoring tools,NA:NA:NA:NA:NA,2017
Curtis Andrus:Endre Balint:Chong Deng:Simon Coupe,"In The Mummy, much of MPC's work involved augmenting the Ahmanet character with various CG elements. This includes, eye splitting, runes, rotten/torn skin, etc. See Figure 1 for an example. These elements needed to be added on top of a live performance, so tracking a 3D model to Ahamanet's face was necessary. Doing this sort of work isn't uncommon, but with the high volume of shots MPC did for this show, it was clear that some new tools would be necessary to help simplify this process.",Optical flow-based face tracking in The Mummy,NA:NA:NA:NA,2017
Andreas Bauer,"Ray-traced contours are inherently challenged by highly detailed geometry, as commonly found in organic shapes. Existing contour methods cannot reflect such complexity in an artistically pleasing way (Figure 1 A), and animations are prone to flicker. After a brief explanation of contour generation and its inherent challenges, this talk presents a novel approach to rendering aesthetic and flicker-free contours on highly detailed geometry (Figure 1 B). The new method employs sub-pixel-level sub-sampling to achieve a high level of detail quality, and supports contours in transparency, reflection and refraction. The implementation uses mental ray (Unified Sampling mode) but could be realized in other ray tracing renderers as well.",A new contour method for highly detailed geometry,NA,2017
Xinling Chen:Christopher Kulla:Lucas Miller:Alan Chen,"In Smurfs: The Lost Village, the enchanted forest is central to the environment and story. The task was to create millions of exotic plants of hundreds of different species spreading across the forest. The plants in the forest emit light when in the shade, and stop when they are hit by sunlight. We revisited the way in which our lighting tool creates lights to scale to huge light counts without compromising on the ability to fine tune lighting through features like light linking. This lead to simplified workflows for the artists who were able to easily manipulate shots with up to several millions of individual lights.",Lighting up the smurfs enchanted forest,NA:NA:NA:NA,2017
Ken Dahm:Alexander Keller,We introduce a rendering algorithm that is as simple as a path tracer but dramatically improves light transport simulation and even outperforms the Metropolis light transport algorithm. The underlying method of importance sampling learns where radiance is coming from and in fact coincides with reinforcement learning. The cost for the improvement is a data structure similar to irradiance volumes as used in realtime games.,Learning light transport the reinforced way,NA:NA,2017
Ken Museth,"We present a new efficient algorithm for computing signed distance fields by means of the Fast Sweeping Method. Unlike existing algorithms ours is explicitly designed to explore the benefits of sparse (vs dense) grids as well as concurrency, i.e. mutli-threading.",Novel algorithm for sparse and parallel fast sweeping: efficient computation of sparse signed distance fields,NA,2017
Ran Dong:Dongsheng Cai:Nobuyoshi Asai,"Human motions (especially, dance motions) are very noisy and it is difficult to analyze the motions. To resolve this problem, we propose a new method to decompose and edit the motions using the Hilbert-Huang transform (HHT). The HHT decomposes a chromatic signal into ""monochromatic"" signals that are the so-called Intrinsic Mode Functions (IMFs) using an Empirical Mode Decomposition (EMD)[Huang 2014]. The HHT has the advantage to analyze non-stationary and nonlinear signals like human joint motions over the FFT or Wavelet transform. In the present research, we propose a new framework to analyze a famous Japanese threesome pop singer group ""Perfume"". Then using the NA-MEMD, we decompose dance motions into motion (choreographic) primitives or IMFs, which can be scaled, combined, subtracted, exchanged, and modified self-consistently.",Dance motion analysis and editing using hilbert-huang transform,NA:NA:NA,2017
Danil Nagy,"This talk will describe a recent collaboration between our group and the aircraft manufacturer Airbus for the design of a new aerospace component which uses cutting-edge design and fabrication techniques to radically reduce the weight of the component while maintaining the same structural performance. To achieve these results, we developed a novel computational geometry system which combines a bottom-up growth strategy based on slime mold behavior from nature, with a top-down genetic algorithm framework for optimizing the final design.",Nature-based hybrid computational geometry system for optimizing the interior structure of aerospace components,NA,2017
Nitish Padmanaban:Robert Konrad:Emily A. Cooper:Gordon Wetzstein,"Personal computing devices have evolved steadily, from desktops to mobile devices, and now to emerging trends in wearable computing. Wearables are expected to be integral to consumer electronics, with the primary mode of interaction often being a near-eye display. However, current-generation near-eye displays are unable to provide fully natural focus cues for all users, which often leads to discomfort. This core limitation is due to the optics of the systems themselves, with current displays being unable to change focus as required by natural vision. Furthermore, the form factor often makes it difficult for users to wear corrective eyewear. With two prototype near-eye displays, we address these issues using display modes that adapt to the user via computational optics. These prototypes make use of focus-tunable lenses, mechanically actuated displays, and gaze tracking technology to correct common refractive errors per user, and provide natural focus cues by dynamically updating scene depth based on where a user looks. Recent advances in computational optics hint at a future in which some users experience better vision in the virtual world than in the real one.",Optimizing VR for all users through adaptive focus displays,NA:NA:NA:NA,2017
Nathan Matsuda:Alexander Fix:Douglas Lanman,"Optimization-based design of optical systems can yield configurations that would be impractical to achieve with manual parameter adjustment. Nonetheless, most approaches are geared toward one-time, offline generation of static configurations to be fabricated physically. Recently, challenging computational imaging problems, such as seeing around corners or through scattering media, have utilized dynamically addressable optical elements to probe scene light transport. A new class of optimization techniques targeted at these dynamic applications has emerged in which stochastic raytracing replaces the fixed operators applied with conventional optimization methods. By modeling optical systems as raytracing operators, more complex non-linear phenomena and larger problem sizes can be considered. We introduce a simple raytracing-in-the-loop optimization model for a head-mounted display (HMD) containing a spatial light modulator (SLM). Using this approach, we are able to compute color images to be displayed in concert with spatially varying SLM phase maps at a resolution that would otherwise be computationally in-feasible. We also consider extensions of this model that may further enhance the performance of the target system.",A case study on raytracing-in-the-loop optimization: focal surface displays,NA:NA:NA,2017
Konrad Tollmar:Pietro Lungaro:Alfredo Fanghella Valero:Ashutosh Mittal,Smart Eye-tracking Enabled Networking (SEEN) is a novel end-to-end framework using real-time eye-gaze information beyond state-of-the-art solutions. Our approach can effectively combine the computational savings of foveal rendering with the bandwidth savings required to enable future mobile VR content provision.,Beyond foveal rendering: smart eye-tracking enabled networking (SEEN),NA:NA:NA:NA,2017
I. Yosun Chang,"Augmented Reality (AR) Interfaces for the Internet of Things (IoT) is an implementation of a provisional universal software platform for IoT devices, created by a solo independent developer. We showcase several use-cases of being able to point one's camera-bearing device or head mounted display (HMD) to IoT objects and the appropriate interfaces that arise to read and/or control these devices, as well as the infrastructure to enable accuracy in these interactions.",Augmented reality interfaces for the internet of things: extended abstract,NA,2018
Jisun Jang:Tomasz Bednarz,"The HoloSensor project aims to enhance the visualisation and visual analytics data sourced from various sensors through use of Augmented Reality (AR) technology, allowing users to anchor information throughout various locations inside a building. The output of the project is an application, that connects networked sensors (Arduino with temperature, humidity, light sensors) communicating its data though a Python-based server. Users are able to interact with this data on holograms in real-time through the Microsoft HoloLens. This integration of the immersive AR technology with Internet of Things (IoT) shows its versatile usage in all three areas of human experience: health, home and entertainment.","HoloSensor for smart home, health, entertainment",NA:NA,2018
Fangwei Lee:Janet Lin:Elliot Segal,"REALITEER Corp. created a cross-platform and kid-friendly digital mirror that can be used for education and body exercise utilizing AR/VR technologies. In a gamified manner, we take users through educational research-based exercises that will not only tackle the psychiatric and physical conditions but better overall well-being.",Kid-friendly digital mirror for education and exercise,NA:NA:NA,2018
Max Reimann:Amir Semmo:Jürgen Döllner:Sebastian Pasewaldt:Mandy Klingbeil,"We present MaeSTrO, a mobile app for image stylization that empowers users to direct, edit and perform a neural style transfer with creative control. The app uses iterative style transfer, multi-style generative and adaptive networks to compute and apply flexible yet comprehensive style models of arbitrary images at run-time. Compared to other mobile applications, MaeSTrO introduces an interactive user interface that empowers users to orchestrate style transfers in a two-stage process for an individual visual expression: first, initial semantic segmentation of a style image can be complemented by on-screen painting to direct sub-styles in a spatially-aware manner. Second, semantic masks can be virtually drawn on top of a content image to adjust neural activations within local image regions, and thus direct the transfer of learned sub-styles. This way, the general feed-forward neural style transfer is evolved towards an interactive tool that is able to consider composition variables and mechanisms of general artwork production, such as color, size and location-based filtering. MaeSTrO additionally enables users to define new styles directly on a device and synthesize high-quality images based on prior segmentations via a service-based implementation of compute-intensive iterative style transfer techniques.",MaeSTrO: mobile style transfer orchestration using adaptive neural networks,NA:NA:NA:NA:NA,2018
Roberto Lopez Mendez,"Up to now in mobile Virtual Reality (VR), we have been able to only control the camera orientation with our head. However, premium smartphones already incorporate the essential technology to track user position. Apple ARKit and Google ARCore designed for Augmented Reality (AR) applications are already enabled in millions of phones. Both libraries can be used to achieve 6DoF mobile VR. This contribution combines head orientation tracking provided by the VR headset with the position tracking capability provided by Google ARCore to achieve 6DoF tracking in mobile VR.","Mobile inside-out VR tracking, now available on your phone: extended abstract",NA,2018
Alyn Rockwood:Kun Gao,"The SuperD 3D modeling app rapidly creates high quality, sleek and intricate shapes for 3D concept design. It employs the widely known SubD interface, which facilitates learning and provides intuitive shape controls; but without any of the troublesome extraordinary points or patch clusters.. The uniquely defined surfaces are smooth (often C2, approaching Class A) and are watertight for 3D printing. SuperD is VR/AR enabled.",SuperD: conceptual 3D modeling on mobiles,NA:NA,2018
Kevin J. Bruggeman:Skylar W. Wurster,"This document explains the design, concept, and purpose behind The Hiatus System. This project aims to identify the possibility of using virtual reality to enhance the effectiveness of mindfulness based stress reduction (MBSR) practice on individuals with low cognitive memory. The Hiatus System was developed from the ongoing research at The Ohio State University on Virtual Healing Spaces. Healing Spaces, a phrase coined by Dr. Esther Sternberg, is a space that promotes a stress reductive state. This virtual experience is designed to attain and maintain user attentions towards the meditative practice. The hypothesis is that the virtual environment, combined with real time biofeedback of the breath, will create a system that can effectively teach the user how to meditate and reduce stress.",The Hiatus system: virtual healing spaces: low dose mindfulness based stress reduction virtual reality application,NA:NA,2018
Tobias Klein,"3D printing allows unprecedented freedom in the design and manufacturing of even the most geometric complex forms---seemingly through a simple click of a button. In comparison, the making of glass is an analogue craftsmanship, coordinating an intricate interplay of individual tools and personal skills, giving shape to a material during the short time of its temperature-based plasticity. The two artworks discussed in this article, Augmented Fauna and Glass Mutations, were created during the artist's residence at the Pilchuck Glass School and articulate a synthesis between digital workflows and traditional craft processes to establish a digital craftsmanship.",Augmented fauna and glass mutations: a dialogue between material and technique in glassblowing and 3D printing: best paper award,NA,2018
Haru Hyunkyung Ji:Graham Wakefield,"Inhabitat is a mixed-reality artwork in which participants become part of an imaginary ecology through three simultaneous perspectives of scale and agency; three distinct ways to see with other eyes. This imaginary world was exhibited at a children's science museum for five months, using an interactive projection-augmented sculpture, a large screen and speaker array, and a virtual reality head-mounted display. This paper documents the work's motivations and design contributions, along with accounts of visitors' playful engagements and reflections within the complex interconnectivity of an artificial nature.",Inhabitat: an imaginary ecosystem in a children's science museum,NA:NA,2018
Todd Berreth,There is a crisis in our communities about the tributes to a shared civic life represented in existing public artwork and monuments. Culture wars are being waged herein and appear increasingly unreconcilable. This paper discusses this moment and describes the range of strategies artists and designers have used to remediate these works. It presents a project description of an interactive artwork that suggests innovative approaches in this realm. The author introduces a conceptual model which served as inspiration for the piece that may be useful when discussing and designing such interventions.,Cop to conductor: negotiating and remapping meaning in existing public art,NA,2018
Nicole L'Huillier:Valentina Montero,"Diastrophisms is a sound installation with a modular system that sends images through rhythmic patterns. It is built on a set of debris from the Alto Río building that was destroyed by the 27F earthquake in 2010 in Chile. Diastrophisms explores poetical, critical and political crossings between technology and matter in order to raise questions about the relationship between human beings and nature, to consider the construction of memory in a community by questioning the notion of monument, and to imagine new forms of communication in times of crisis.",Diastrophisms: visual and sound assembly in remembrance of an earthquake,NA:NA,2018
David Gochfeld:Corinne Brenner:Kris Layng:Sebastian Herscher:Connor DeFanti:Marta Olko:David Shinn:Stephanie Riggs:Clara Fernández-Vara:Ken Perlin,"Holojam in Wonderland is a prototype of a new type of performance activity, ""Immersive Mixed Reality Theater"" (IMRT). With unique and novel properties possessed by neither cinema nor traditional theater, IMRT promises exciting new expressive possibilities for multi-user, participatory, immersive digital narratives. The authors describe the piece, the technology used to create it and some of the key aesthetic choices and takeaways.",Holojam in wonderland: immersive mixed reality theater,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Paul Charisse:Alex Counsell,"This paper is an exploration of the processes used and ideas behind an animated full CGI feature film project that attempts to reach blockbuster production values, while retaining Art House sensibilities. It examines methods used to achieve these production values in an academic production environment and ways costs can be minimized while high quality levels are retained. It also examines the film's status as an Art House project, by comparing its narrative design and use of symbolism to existing works of Art House cinema.",Alienating the familiar with CGI: a recipe for making a full CGI art house animated feature,NA:NA,2018
Nicolas Henchoz:Allison Crank,"To revive the Montreux Jazz Festival's archival live-concert footage, three immersive installations were designed using three different principles of augmentation, physicality and interaction. The primary aim was to engage the user in a new relationship with digitized heritage. Audience observations indicated a strong emotional connection to the content, the artist and the crowd, as well as the development of new social interactions. Experimentation showed close interaction between the three principles, while the three installations suggested methodologies for reviving audio-visual archives.",Digital heritage: bringing new life to the montreux jazz festival's audio-visual archives with immersive installations,NA:NA,2018
Courtney Starrett:Susan Reiser:Tom Pacio,"Data materialization is a workflow developed to create 3D objects from data-informed designs. Building upon traditional metalwork and craft, and new technology's data visualization with generative art, this workflow expresses conceptually relevant data through 3D forms which are fabricated in traditional media. The process allows for the subtle application of data in visual art, allowing the aesthetic allure of the art object or installation to inspire intellectual intrigue. This paper describes the technical and creative process of Modern Dowry, a silver-plated 3D-print teapot on view at the Museum of the City of New York, June 2017--June 2018.",Data materialization: a hybrid process of crafting a teapot,NA:NA:NA,2018
Yuichiro Katsumoto,"Humans use letters, which are two-dimensional static symbols, for communication. Writing these letters requires body movement as well as spending a certain amount of time; therefore, it can be demonstrated that a letter is a trajectory of movement and time. Based on this notion, the author conducted studies regarding multidimensional kinetic typography, primarily using robots to display a letter and visualize its time and movement simultaneously. This paper describes the project background and design of the three types of robotic displays that were developed and discusses possible expressions using robotic displays.","Robotype: studies of kinetic typography by robot display for expressing letters, time and movement",NA,2018
Brittany Myburgh,"Examining the use of new media in works by Ruben Komangapik, Kent Monkman and the Wikiup Indigenous Knowledge Network reveals the diverse ways in which technologies are used to disrupt linear time and Western visions of history. New media works challenge those misleading stories that have been told about Canada's indigenous peoples and assert indigenous presence in both the digital and physical landscape. These artists employ QR codes, video and augmented reality to push artistic boundaries and create representations of the past and present.","Here and now: indigenous canadian perspectives and new media in works by ruben komangapik, kent monkman and adrian duke",NA,2018
Yiyun Kang,"This paper investigates CASTING, Yiyun Kang's site-specific projection mapping installation at the Victoria and Albert Museum in London, U.K., and the acquisition of the piece by the V&A in the following year. It identifies how CASTING developed distinctive properties in the field of projected moving-image installation artworks and how these novel characteristics were reflected in the acquisition by the V&A.",Casting: site-specific projection mapping installation,NA,2018
Daniel Temkin,"Coding, the translating of human intent into logical steps, reinforces a compulsive way of thinking, as described in Joseph Weitzenbaum's ""Science and the Compulsive Programmer"" (1976). Two projects by the author, Entropy (2010) and FatFinger (2017), challenge this by encouraging gestural approaches to code. In the Entropy programming language, data becomes slightly more approximate each time it is used, drifting from its original values, forcing programmers to be less precise. FatFinger, a Javascript dialect, allows the programmer to misspell code and interprets it as the closest runnable variation, strategically guessing at the programmer's intent.",Entropy and fatfinger: challenging the compulsiveness of code with programmatic anti-styles,NA,2018
Daniel C. Howe:Qianxun Chen:Zong Chen,"Advertising Positions integrates 3D scanning, motion capture, novel image mapping algorithms and custom animation to create data portraits from the advertisements served by online trackers. Project volunteers use bespoke software to harvest the ads they receive over months of browsing. When enough ads have been collected, the volunteer is interviewed, 3D scanned and motion captured. Each ad is then mapped to a single polygon on the textured skin of their virtual avatar. Outcomes have been displayed as 2D/3D images, animations and interactive installations.",Advertising positions: data portraiture as aesthetic critique,NA:NA:NA,2018
Jason Edward Lewis:Skawennati,"We started reading science fiction as teenagers. We fell in love with the fantastic worlds, the strange societies, the alien cultures and the amazing technologies. As we got older, though, we began to notice the lack of Native people in those futures. In fact, there were barely any nonwhite people at all.",The future is indigenous,NA:NA,2018
Matthew Allen,"Should we see the early development of computer-aided design as an aesthetic movement? Just as eighteenth-century England had picturesque gardens and the world of social media today has spawned its own universe of visual conventions (to take two examples at random), was there such a thing as a ""computational aesthetics"" some 50 years ago? These are questions not about ""computer art"" per se, but about how a new visual culture might emerge alongside new practices and new concepts. They are particularly tricky questions to ask of early computational images because such images come from an era when people were eager to frame their work as scientific research, and aesthetics was often ruled out of bounds.","The computational imagination: notes on the exhibition designing the computational image, imagining computational design",NA,2018
Ernest Edmonds,"In this note I describe my personal development of art systems over 50 years. In all of this work I have used computers and computational processes both to make the works and to advance my conception of art. This history is marked by a trace of publications in the journal Leonardo, itself being 50 years old. I will relate the story with specific reference to these publications. Each of the following sections relates to one Leonardo publication and includes quotations from that paper. In the earlier writing ""he"" or ""him"" is used sometimes to refer to a person who could well be female, as used to be the custom.",Art systems: 1968 to 2018,NA,2018
Daniel Cardoso Llach,"Archaeology of CAD is an ongoing project that examines the origins of Computer-Aided Design by bringing to life some of its pioneering technologies, which were central to re-shape design practices in the image of computation during the second half of the twentieth century. On display at SIGGRAPH will be two interactive installations from this project: the reconstructions of Steven A. Coons's ""Coons Patch"" and of Ivan Sutherland's ""Sketchpad."" Drawing from primary archives and oral sources, these interactive installations playfully revisit these transformative technologies from the 1960s, and enable visitors to approximate the experience of designing with the first Computer-Aided Design systems. Developed with computational design students at Carnegie Mellon University using present-day hardware and software languages, these reconstructions are inquisitive artifacts of historical inquiry. By evoking the embodied experience of interacting with these technologies, they shed light on the new forms of human-machine work that emerged with the rise of interactive computing during the Cold-War years, and highlight the sensual and gestural dimensions of the ""computer revolution."" Along with the two reconstructions, a selection of rare handwritten notes and documents by Coons, and a selection of key contractual documents between the US Air Force and MIT, are displayed to offer glimpses of the institutional and intellectual context that motivated these foundational technologies of computational design.","Reconstructing ""sketchpad"" and the ""coons patch"": toward an archaeology of CAD",NA,2018
Skawennati,"This sci-fi retelling of the Haudenosaunee (Iroquois) creation story reimagines Sky World as a futuristic utopia and Sky Woman as a brave astronaut and world-builder. When she learns that her planet is dying, Sky Woman volunteers to become the seed of the new world---an Earth yet covered in water. She Falls For Ages boldly mixes ancient storytelling with science fiction to connect the deep past with the far future.",She falls for ages,NA,2018
Nā 'Anae Mahiki,"He Ao Hou is a point-and-click adventure game set in the far future, when Native Hawaiians have attained the next level of navigation---space travel. The gameplay is based on kānaka maoli (Native Hawaiian) stories and knowledge, and focuses in particular on the uses of the kukui nut, itself a symbol of knowledge.",He ao hou (a new world),NA,2018
Amy Fredeen:Dima Veryovka,"Never Alone (Kisima Inŋitchuŋa) is the product of an uncommon partnership of an Alaska Native community and game developers. Through all stages of development, members of both communities met extensively to ensure that all creative and business decisions were appropriately considered and supported the goals of all stakeholders. Throughout the game and in supporting material, players will hear directly from members of both communities who were instrumental in shaping the game.",Never alone: the art and the people of the story: E-line media,NA:NA,2018
Microsoft Garage:Andy Klein:Shawn Hunt,"The Raven, the ultimate trickster, has become a cyborg. In this collaboration with Microsoft Vancouver, Shawn Hunt moves away from engaging with the handmade, exploring authenticity and our expectations of what it means to be indigenous through the removal of the hand-carved surface. The mask appropriates the traditional aspects of metamorphosis with the transformation from bird mask to human, yet in this adaptation the human mask has been altered, upgraded and merged with the machine. Incorporating aspects of technology, sound and space, each part of the work reflects Hunt's interest in how we understand and identify with the term indigenous.",Transformation mask,NA:NA:NA,2018
Danny Bazo:Marko Peljhan:Karl Yerkes,"Somnium is a robotic and audiovisual installation that provides visitors with the ability to contemplate and experience exoplanetary discoveries, their macro- and microdimensions and the potential for life in our galaxy.",Somnium,NA:NA:NA,2018
Ruth West:Violet Johnson:I Chen Yeh:Zach Thomas:Eitan Mendelowitz:Lars Berg,"INSTRUMENT | One Antarctic Night (IOAN) is a performative, multiparticipant reconfigurable artwork that engages open astronomical data in combination with data generated by robotic telescopes in Antarctica. IOAN places visitors inside a virtual star field of over 800,000 astronomical objects that form part of the Large Magellanic Cloud. This star field, created from observations in Antarctica and fused with additional data from multiple open astronomical repositories, is situated waist high within the virtual environment and stretches out beyond participants in all directions. Multiple participants can walk about the environment and collaboratively explore the star field by taking hold of the ""fabric"" of space, creating ripples and waves, and interacting with individual or sets of objects to create visual and auditory data remixes. The interaction places the astronomical data within a virtual reality visual and sonic remix engine that is a fundamental component of the artwork and is used to construct the virtual world. All graphics and spatialized ambisonic audio are procedurally generated from the data via real-time database queries. Our work incorporates machine learning approaches combined with granular and concatenative synthesis for generating the environment's unique soundscape. INSTRUMENT | One Antarctic Night evolved from our ongoing work in developing aesthetic data remixing and immersive data-driven experiences. Dataremix proposes the creation of the ""datamade,"" a concept analogous to Duchamp's ""readymade."" IOAN is a meta-datamade in that it is a virtual instrument within which participants collaboratively create datamades through visual and auditory aesthetically driven remixes of astronomical data.",Instrument | one antarctic night,NA:NA:NA:NA:NA:NA,2018
Milton Sogabe:Fabio Oliveira Nunes:Carolina Peres:Soraya Braz:Rodrigo Dorta:Cleber Gazana:Mirian Steinberg:Melina Furquim:Daniel Malva:Fernando Luiz Fogliano,"Considering the paradox between energy production and the contamination of the environment and reduction of biodiversity, cAt research group develops its work considering the discussion on sustainable sources of energy. The group's recent projects---Sopro (The Blow) and Toque (Touch)-have sought to aesthetically use the audience body's energy to interact and to animate the artworks. Simple devices are used to seek, in a kind of technological minimalist and interactive-art way, to raise public awareness of the issue of sustainability.",Sopro and toque (the blow and touch),NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Özge Samanci:Gabriel Caniglia,"This interactive installation allows participants to control a digitally simulated ocean using only their brainwaves. Calm seas and storms alike are powered by the viewer's thoughts; the sheer act of concentration can conjure a squall or sunshine. Participants intentionally control their thinking while surrounded by the magnified consequence of their thoughts. Created in 2017, You Are the Ocean is about the theme of origins and one of the key concepts of Haudenosaunee and Anishnaabe cosmologies: ""land is alive.""",You are the ocean,NA:NA,2018
Alex Beim,"Haven is a place of security and tranquility. Reminiscent of a mother's womb, it recalls our origins, where it all begins. The installation allows guests to leave their phones and all other technology at the door so they can be fully present without any of the prevailing modern distractions. They go in, spend some time, find themselves and maybe come out and start their day again. Fresh. A new beginning.",Haven,NA,2018
Shingo Kagami:Koichi Hashimoto,"We demonstrate a 24-bit full-color projector that achieves over 2400-fps motion adaptability to a fast moving planar surface using single-chip DLP technology, which will be useful for projection mapping applications in highly dynamic scenes. The projector can be interfaced with a host PC via standard HDMI and USB without need of high computational burden.",A full-color single-chip-DLP projector with an embedded 2400-fps homography warping engine,NA:NA,2018
Azumi Maekawa:Ryuma Niiyama:Shunji Yamanaka,"We present a biped robot which can move agiler than conventional robots. Our robot can generate bipedal walking motion automatically using the proposed method. By using a quadrotor for balance and movement it is possible to make an agiler movement, and generate a gait interactively and in real time according to the motion of the quadrotor using the optimized control policy of the legs. Our system takes the velocity of the quadrotor as an input and legs motions are produced so that the velocity of the foot in contact with the ground to zero, and bipedal walking motion is generated. The control policy is optimized using reinforcement learning with a physics engine.",Aerial-biped: a new physical expression by the biped robot using a quadrotor,NA:NA:NA,2018
Nitish Padmanaban:Robert Konrad:Gordon Wetzstein,"Presbyopia, the loss of accommodation due to the stiffening of the crystalline lens, affects nearly 20% of the population worldwide. Traditional forms of presbyopia correction use fixed focal elements that inherently trade off field of view or stereo vision for a greater range of distances at which the wearer can see clearly. However, none of these offer the same natural refocusing enjoyed in youth. In this work, we built a new presbyopia correction, dubbed Autofocals, which externally mimics the natural accommodation response by combining data from eye trackers and a depth sensor, and then automatically drives focus-tunable lenses. In our testing, wearers generally reported that the Autofocals compare favorably with their own current corrective eyewear.",Autofocals: gaze-contingent eyeglasses for presbyopes,NA:NA:NA,2018
Yong-Ho Lee:Mincheol Kim:Hwang-Youn Kim:Dongmyoung Lee:Bum-Jae You,"In the research, we propose a cost-effective 3-finger exoskeleton hand motion-capturing device and a physics engine-based hand interaction module for immersive experience in manipulation of virtual objects. The developed device provides 12 DOFs data of finger motion by a unique bevel-gear structure as well as the use of six 3D magnetic sensors. It shows a small error in relative distance between two fingertips less than 2 mm and allows the user to reproduce precise hand motion while processing the complex joint data in real-time. We synchronize hand motion with a physics engine-based interaction framework that includes a grasp interpreter and multi-modal feedback operation in virtual reality to minimize penetration of a hand into an object. The system enables feasibility of object manipulation as far as the needs go in various tasks in virtual environment.",CHICAP: low-cost hand motion capture device using 3D magnetic sensors for manipulation of virtual objects,NA:NA:NA:NA:NA,2018
Qian Zhou:Georg Hagemann:Sidney Fels:Dylan Fafard:Andrew Wagemakers:Chris Chamberlain:Ian Stavness,"Fish Tank Virtual Reality (FTVR) creates a compelling 3D illusion for a single person by rendering to their perspective with head-tracking. However, typically, other participants cannot share in the experience since they see a weirdly distorted image when they look at the FTVR display making it difficult to work and play together. To overcome this problem, we have created CoGlobe: a large spherical FTVR display for multiple users. Using CoGlobe, Siggraph attendees will experience the latest advance of FTVR that supports multiple people co-located in a shared space working and playing together through two different multiplayer games and tasks. We have created a competitive two-person 3D Pong game (Figure 1b) for attendees to experience a highly interactive two-person game looking at the CoGlobe. Onlookers can also watch using a variation of mixed reality with a tracked mobile smartphone. Using a smartphone as a second screen registered to the same virtual world enables multiple people to interact together as well. We have also created a cooperative multi-person 3D drone game (Figure 1c) to illustrate cooperation in FTVR. Attendees will also see how effective co-located 3D FTVR is when cooperating on a complex 3D mental rotation (Figure 1d) and a path-tracing task (Figure 1a). CoGlobe overcomes the limited situation awareness of headset VR, while retaining the benefits of cooperative 3D interaction and thus is an exciting direction for the next wave of 3D displays for work and fun for Siggraph attendees to experience.",Coglobe: a co-located multi-person FTVR experience,NA:NA:NA:NA:NA:NA:NA,2018
Yu Matsuura:Naoya Koizumi,"FairLift is an interaction system involving mid-air images, which are visible to the naked eye under and on a water surface. In this system, the water surface reflects the light from micro-mirror array plates, and a mid-air image appears. The system enables a user to interact with the mid-air image by controlling the image position of a light-source display from the water level measured with an ultrasonic sensor. The contributions of this system are enriching interaction with mid-air images and addressing the limitations of conventional water-display systems.",Fairlift: interaction with mid-air images on water surface,NA:NA,2018
MHD Yamen Saraiji:Tomoya Sasaki:Reo Matsumura:Kouta Minamizawa:Masahiko Inami,"Effective communication is a key factor in social and professional contexts which involve sharing the skills and actions of more than one person. This research proposes a novel system to enable full body sharing over a remotely operated wearable system, allowing one person to dive into someone's else body. ""Fusion"" enables body surrogacy by sharing the same point of view of two-person: a surrogate and an operator, and it extends the limbs mobility and actions of the operator using two robotic arms mounted on the surrogate body. These arms can be used independently of the surrogate arms for collaborative scenarios or can be linked to surrogate's arms to be used in remote assisting and supporting scenarios. Using Fusion, we realize three levels of bodily driven communication: Direct, Enforced, and Induced. We demonstrate through this system the possibilities of truly embodying and transferring our body actions from one person to another, realizing true body communication.",Fusion: full body surrogacy for collaborative communication,NA:NA:NA:NA:NA,2018
Shunki Yamashita:Ryota Ishida:Arihide Takahashi:Hsueh-Han Wu:Hironori Mitake:Shoichi Hasegawa,"Many people sometimes imagine if they can wield superhuman abilities like that appear in games and animation. Among these abilities, we focused particularly on representing the experience of arm stretching beyond the limits of the human body. We proposed a method for inducing a sense of arm stretching by designing the device attached to forearm and giving the user a visual cue by changing the body structure of the user's avatar in the virtual environment. Our device shifts the mass from the elbow to the wrist while stretching the skin of the forearm according to the animation in the virtual environment. The sensation of the elongation of the arm skin as well as the change in the weight of arm is thought to be the feeling when the arms are stretched out. As a result, we introduce these two mechanisms into our device, which allows the user to feel the sense of arm stretching.",Gum-gum shooting: inducing a sense of arm elongation via forearm skin-stretch and the change in the center of gravity,NA:NA:NA:NA:NA:NA,2018
Alon Grinshpoon:Shirin Sadri:Gabrielle J. Loeb:Carmine Elvezio:Samantha Siu:Steven K. Feiner,"During a vascular intervention (a type of minimally invasive surgical procedure), physicians maneuver catheters and wires through a patient's blood vessels to reach a desired location in the body. Since the relevant anatomy is typically not directly visible in these procedures, virtual reality and augmented reality systems have been developed to assist in 3D navigation. Because both of a physician's hands may already be occupied, we developed an augmented reality system supporting hands-free interaction techniques that use voice and head tracking to enable the physician to interact with 3D virtual content on a head-worn display while leaving both hands available intraoperatively. We demonstrate how a virtual 3D anatomical model can be rotated and scaled using small head rotations through first-order (rate) control, and can be rigidly coupled to the head for combined translation and rotation through zero-order control. This enables easy manipulation of a model while it stays close to the center of the physician's field of view.",Hands-free augmented reality for vascular interventions,NA:NA:NA:NA:NA:NA,2018
Hwan Kim:HyeonBeom Yi:Richard Chulwoo Park:Woohun Lee,"We developed a tactile actuator named HapCube that provides tangential and normal pseudo-force feedback on user's fingertip. The tangential feedback is generated by synthesizing two orthogonal asymmetric vibrations, and it simulates frictional force in any desired tangential directions. The normal feedback simulates tactile sensations when pressing various types of button. In addition, by combining the two feedbacks, it can produce frictional force and surface texture simultaneously.",Hapcube: a tactile actuator providing tangential and normal pseudo-force feedback on a fingertip,NA:NA:NA:NA,2018
Shunichi Kasahara,"Visual augmentation to the real environment has potential not only to display information but also to provide a new perception of the physical world. However, the currently available mixed reality technologies could not provide enough angle of view. Thus, we introduce ""Headlight"", a wearable projector system that provides wide egocentric visual augmentation. Our system consists of a small laser projector with a fish-eye wider conversion lens, a headphone and a pose tracker. HeadLight provides projection angle with approx. 105 deg. horizontal and 55 deg. vertical from the point of view of the user. In this system, the three-dimensional virtual space that is consistent with the physical environment is rendered with a virtual camera based on tracking information of the device. By processing inverse correction of the lens distortion and projecting the rendered image from the projector, HeadLight performs consistent visual augmentation in the real world. With Headlight, we envision that physical phenomena that human could not perceive will be perceived through visual augmentation.",Headlight: egocentric visual augmentation by wearable wide projector,NA,2018
Takashi Yamamoto:Tamaki Nishino:Hideki Kajima:Mitsunori Ohta:Koichi Ikeda,"There has been an increasing interest in mobile manipulators that is capable of performing physical work in living spaces worldwide, corresponding to population aging with declining birth rates with the expectation of improving quality of life (QOL). Research and development is a must in intelligent sensing and software which enable advanced recognition, judgment, and motion to realize household work by robots. In order to accelerate this research, we have developed a compact and safe research platform, Human Support Robot (HSR), which can be operated in an actual home environment. We assume that overall R&D will accelerate by using a common robot platform among many researchers since that enables them to share their research results. In this paper, we introduce HSR design and its utilization.",Human support robot (HSR),NA:NA:NA:NA:NA,2018
Tomoya Sasaki:Richard Sahala Hartanto:Kao-Hua Liu:Keitarou Tsuchiya:Atsushi Hiyama:Masahiko Inami,"We present LevioPole, a rod-like device that provides mid-air haptic feedback for full-body interaction in virtual reality, augmented reality, or other daily activities. The device is constructed from two rotor units, which are designed using propellers, motors, speed controllers, batteries, and sensors, allowing portability and ease of use. Having each group of rotor units on both ends of the pole, these rotors generate both rotational and linear forces that can be driven according to the target application. In this paper, we introduce example applications in both VR and physical environment; embodied gaming with haptic feedback and walking navigation in a specific direction.",Leviopole: mid-air haptic interactions using multirotor,NA:NA:NA:NA:NA:NA,2018
Yoichi Ochiai:Kazuki Otao:Yuta Itoh:Shouki Imai:Kazuki Takazawa:Hiroyuki Osone:Atsushi Mori:Ippei Suzuki,"Retinal projection is required for xR applications that can deliver immersive visual experience throughout the day. If general-purpose retinal projection methods can be realized at a low cost, not only could the image be displayed on the retina using less energy, but there is also a possibility of cutting off the weight of projection unit itself from the AR goggles. Several retinal projection methods have been previously proposed. Maxwellian optics based retinal projection was proposed in 1990s [Kollin 1993]. Laser scanning [Liao and Tsai 2009], laser projection using spatial light modulator (SLM) or holographic optical elements were also explored [Jang et al. 2017]. In the commercial field, QD Laser1 with a viewing angle of 26 degrees is available. However, as the lenses and iris of an eyeball are in front of the retina, which is a limitation of a human eyeball, the proposal of retinal projection is generally fraught with narrow viewing angles and small eyebox problems. Due to these problems, retinal projection displays are still a rare commodity because of their difficulty in optical schematics design.",Make your own retinal projector: retinal near-eye displays via metamaterials,NA:NA:NA:NA:NA:NA:NA:NA,2018
Matthew O'Toole:David B. Lindell:Gordon Wetzstein,"Non-line-of-sight (NLOS) imaging aims at recovering the shape of objects hidden outside the direct line of sight of a camera. In this work, we report on a new approach for acquiring time-resolved measurements that are suitable for NLOS imaging. The system uses a confocalized single-photon detector and pulsed laser. As opposed to previously-proposed NLOS imaging systems, our setup is very similar to LIDAR systems used for autonomous vehicles and it facilitates a closed-form solution of the associated inverse problem, which we derive in this work. This algorithm, dubbed the Light Cone Transform, is three orders of magnitude faster and more memory efficient than existing methods. We demonstrate experimental results for indoor and outdoor scenes captured and reconstructed with the proposed confocal NLOS imaging system.",Real-time non-line-of-sight imaging,NA:NA:NA,2018
Takayuki Todo,"SEER (Simulative Emotional Expression Robot) is an animatronic humanoid robot that generates gaze and emotional facial expressions to improve animativity, lifelikeness, and impresssiveness by the integrated design of modeling, mechanism, materials, and computing. The robot can simulated a user?s movement, gaze, and facial expressions detected by a camera sensor. This system can be applied to puppetry, telepresence avatar, and interactive automation.",SEER: simulative emotional expression robot,NA,2018
Hiroaki Yano:Tomohiro Yendo,"We present an optical system design for a 3D display that is spherical, full-parallax, and occlusion-capable with a wide viewing zone and no head tracking. The proposed system provides a new approach for the 3D display and thereby addresses limitations of the conventional light-field display structure. Specifically, a spherical full-parallax light-field display is difficult to achieve because it is challenging to curve the conventional structure of the light-field displays. The key elements of the system are a specially designed ball mirror and a high-speed projector. The ball mirror uniaxially rotates and reflects rays from the projector to various angles. The intensities of these rays are controlled by the projector. Rays from a virtual object inside the ball mirror are reconstructed, and the system acts as a light-field display based on the time-division multiplexing method. We implemented this ball mirror by 3D printing and metal plating. The prototype successfully displays a 3D image and the system feasibility is confirmed. Our system is thus suitable for displaying 3D images to many viewers simultaneously and it can be effectively employed as in art or advertisement installation.",Spherical full-parallax light-field display using ball of fly-eye mirror,NA:NA,2018
Kishore Rathinavel:Praneeth Chakravarthula:Kaan Akşit:Josef Spjut:Ben Boudaoud:Turner Whitted:David Luebke:Henry Fuchs,"The design challenges of see-through near-eye displays can be mitigated by specializing an augmented reality device for a particular application. We present a novel optical design for augmented reality near-eye displays exploiting 3D stereolithography printing techniques to achieve similar characteristics to progressive prescription binoculars. We propose to manufacture inter-changeable optical components using 3D printing, leading to arbitrary shaped static projection screen surfaces that are adaptive to the targeted applications. We identify a computational optical design methodology to generate various optical components accordingly, leading to small compute and power demands. To this end, we introduce our augmented reality prototype with a moderate form-factor, large field of view. We have also presented that our prototype is promising high resolutions for a foveation technique using a moving lens in front of a projection system. We believe our display technique provides a gate-way to application-adaptive, easily replicable, customizable, and cost-effective near-eye display designs.",Steerable application-adaptive near eye displays,NA:NA:NA:NA:NA:NA:NA:NA,2018
Kazuma Aoyama:Kenta Sakurai:Akinobu Morishima:Taro Maeda:Hideyuki Ando,"Galvanic tongue stimulation (GTS) is a technology used to change and induce taste sensation with electrical stimulation. It is known from previous studies that cathodal current stimulation induces two types of effects. The first is the taste suppression that renders the taste induced by electrolytic materials weaker during the stimulation. The second is taste enhancement that makes taste stronger shortly after ending the stimulation. These effects stand a better possibility to affect the ability to emulate taste, which can ultimately control the strength of taste sensation with freedom. Taste emulation has been considered in various applications, such as in virtual reality, in diet efforts, and in other applications. However, conventional GTS is associated with some problems. For example, the duration of taste enhancement is too short for use in diet efforts, and it necessitates the attachment of electrodes in the mouth. Moreover, conventional GTS cannot induce taste at the throat but at the mouth instead. Thus, this study and our associated demonstration introduces some approaches to address and solve these problems. Our approaches realize that taste changes voluntarily and the effects persist for lengthy periods of time.","Taste controller: galvanic chin stimulation enhances, inhibits, and creates tastes",NA:NA:NA:NA:NA,2018
Jotaro Shigeyama:Takeru Hashimoto:Shigeo Yoshida:Taiju Aoki:Takuji Narumi:Tomohiro Tanikawa:Michitaka Hirose,We introduce a dynamic weight moving VR controller for 2d haptic shape rendering using a haptic shape illusion. This allows users to perceive the feeling of various shapes in virtual space with a single controller. We designed a device that drives weight on a 2d planar area to alter mass properties of the hand-held controller. Our user study showed that the system succeeded in providing shape perception over a wide range.,Transcalibur: weight moving VR controller for dynamic rendering of 2D shape using haptic shape illusion,NA:NA:NA:NA:NA:NA:NA,2018
Kazuki Otao:Yuta Itoh:Kazuki Takazawa:Hiroyuki Osone:Yoichi Ochiai,"We present a transmissive mirror device (TMD) based near-eye see-through displays with a wide viewing angle and high resolution for virtual reality and augmented reality. In past years, many optical elements, such as transmissive liquid-crystal display (LCD), half-mirror, waveguide and holographic optical element (HOE) have been adopted for near-eye see-through displays. However, it is difficult to obtain wide field of view with see-through capability for beginner developer. To accomplish this, we develop a simple see-through display that easily setup from a combination of off-the-shelf HMD and TMD. In the proposed method, we render ""virtual lens,"" which has the same function as the HMD lens in the air. By using TMD, it is possible to shorten the optical length between the virtual lens and the eyeball. Therefore, the aerial lens provides a wide viewing angle with see-through capability. We demonstrate a prototype with a diagonal viewing angle of 100 degrees.",Transmissive mirror device based near-eye displays with wide field of view,NA:NA:NA:NA:NA,2018
Pierre-Yves Laffont:Ali Hasnain:Pierre-Yves Guillemet:Samuel Wirajaya:Joe Khoo:Deng Teng:Jean-Charles Bazin,"The vergence-accommodation conflict is a fundamental cause of discomfort in today's Virtual and Augmented Reality (VR/AR). We present a novel software platform and hardware for varifocal head-mounted displays (HMDs) to generate consistent accommodation cues and account for the user's prescription. We investigate multiple varifocal optical systems and propose the world's first varifocal mobile HMD based on Alvarez lenses. We also introduce a varifocal rendering pipeline, which corrects for distortion introduced by the optical focus adjustment, approximates retinal blur, incorporates eye tracking and leverages on rendered content to correct noisy eye tracking results. We demonstrate the platform running in compact VR headsets and present initial results in video pass-through AR.",Verifocal: a platform for vision correction and accommodation in head-mounted displays,NA:NA:NA:NA:NA:NA:NA,2018
Simon Spielmann:Volker Helzle:Andreas Schuster:Jonas Trottnow:Kai Götz:Patricia Rohr,"The work on intuitive Virtual Production tools at Filmakademie Baden-Württemberg has focused on an open platform tied to existing film creation pipelines. The Virtual Production Editing Tools (VPET) started in a former project on Virtual Production funded by the European Union and are published and constantly updated on the open source software development platform Github. We introduce an intuitive workflow where Augmented Reality, inside-out tracking and real-time color keying can be applied on the fly to extend a real movie set with editable, virtual extensions in a collaborative setup.",VPET: virtual production editing tools,NA:NA:NA:NA:NA:NA,2018
Theresa-Marie Rhyne:Nicholas (Nick) Bazarian:Jose Echevarria:Michael J. Murdoch:Danielle Feinberg,"Designing and capturing a color scheme for a digital media composition is an important step in the creation pipeline. Whether it is an immersive experience, animation or visualization, color selection is key to conveying the message or story. In this panel, we assemble a group of color experts, aka ""Color Mavens,"" to convey and define color appearance and colorization methods. Each panelist represents a particular color advice approach whether it includes a recommended set of guidelines for color appearance, suggested color schemes, a tool for color capture, an application for color palette creation or tips from colorization experiences. Each panelist will highlight their methods with a team discussion about optimal colorization approaches to follow. The panel will also identify gaps in our understanding about the use of color in digital media composition as well as identifying future application and research directions.",Color mavens advise on digital media creation and tools: SIGGRAPH 2018 panel,NA:NA:NA:NA:NA,2018
Dmytro Korolov:Doug Roble:Jean-Charles Bazin:Renaldas Zioma:Rob Pieke:Jeff Kember:David Luebke,"This panel discusses trends and prospects for using AI tools in the VFX pipeline. Panel experts will talk about the current AI tools that work in the industry, give answers to questions and their vision of their technology development.",Future of artificial intelligence and deep learning tools for VFX,NA:NA:NA:NA:NA:NA:NA,2018
Ryan Ulyate:David Bianciardi:Judith Crow:Greg Hermanovic,"Twenty years ago in Orlando The Interactive Dance Club (IDC) brought together the SIGGRAPH community in a grand social experiment. For four nights conference attendees gathered to participate in creating a dynamic confluence of music, computer graphics and lighting - all driven by those who danced. Computers were big and expensive back then! There were no iPhones, no bluetooth, no wearables lighter than a small car. So why were we crazy enough to think we could pull this off? To what lengths did we go to make this happen?",Interactive dance club '98: a legend in the making!,NA:NA:NA:NA,2018
Matt Pharr:Brent Burley:Per Christensen:Marcos Fajardo:Luca Fascione:Christopher Kulla,"Over the past decade, production rendering has moved from primarily using Reyes-based algorithms to being based on path tracing and physically based approaches. The developers of five of the most significant production renderers have each recently written comprehensive systems papers on their renderers, describing the challenges of modern production rendering and the systems they have respectively built to solve them. In May 2018, these papers were published in a special issue of ACM Transactions on Graphics. In this panel, the developers of these renderers will go into depth on how the challenges of production rendering have influenced the systems they've built and how they have developed new techniques to make path tracing viable in practice. These systems are remarkably varied in some of their core design decisions; the panelists will also compare and contrast their own design decisions with respect to topics like precomputation versus runtime computation, RGB versus spectral rendering, and out-of-core rendering versus requiring scene geometry to fit into memory.",Design and implementation of modern production renderers,NA:NA:NA:NA:NA:NA,2018
"Henry Fuchs:Ivan E. Sutherland:Robert F. Sproull:Charles L. Seitz:Frederick P. Brooks:H. Quintin Foster, Jr.",NA,[email protected]: celebrating ivan sutherland's 1968 head-mounted 3D display system,NA:NA:NA:NA:NA:NA,2018
Pol Jeremias-Vila:Kim Libreri:Guido Quaroni:Natalya Tatarchuk:Damien Fagnou,"The movie frames we see when watching films today are typically generated using an offline renderer, which could take multiple hours per frame to produce. This time-consuming process makes previewing content difficult, so creators have worked around this issue by utilizing real-time graphics to iterate their content more efficiently. While real-time graphics could be used for previewing, the level of quality had not yet reached the standards needed for a final movie frame. However, over the years, the previews generated by real-time graphics have gotten more refined and have even enabled pre-visualizations using virtual reality. This provides even more context to creative minds. In addition to the ever growing use of real-time graphics, the quality of technology has improved, potentially allowing for the generation of final frames, that can start to look like a movie. This panel will bring together engineers, artists and executives representing various areas of expertise to provide information about how real-time graphics are being used in multiple studios today. They will also describe some of their challenges and how they foresee the future of real-time graphics in film.",The present and future of real-time graphics in film,NA:NA:NA:NA:NA,2018
Mark Wiebe:Jason Fotter:Dan Wexler:Panos Zompolas:Phil Peterson,"The Visual Effects industry is presently grappling with how to best take advantage of cloud computing, a technology which has transformed the practice of software in many industries. The ability to treat the provisioning and configuration of render farm hardware with the flexibility of software is highly attractive, but the learning curve can be challenging to juggle with busy production schedules. Fully managed web services have also taken hold in some parts of the production pipeline, with more likely to come. Software vendors creating web services need to enable studios with the right combination of security, backwards compatibility, ease of use, and programmability, so they may adopt these technologies without interrupting their Visual Effects production. In this panel, we will discuss current usage of cloud computing in Visual Effects, how it is trending, and how it interacts with other factors like the growth of VFX-oriented open source software. Studios range in their use of render farms from full on-premises setups through hybrid setups blending their premises with the cloud to all-in cloud rendering. We will explore how fast internet connections and efficient streaming desktop technology are enabling full end-to-end production to move to the cloud with Zero Client workstations. Our panel consists of a diverse group of technologists, representing both Visual Effects studios and the creators of software for the industry.",Visual effects in the age of the cloud,NA:NA:NA:NA:NA,2018
Yufeng Zhu:Robert Bridson:Danny M. Kaufman,"Optimizing distortion energies over a mesh, in two or three dimensions, is a common and critical problem in physical simulation and geometry processing. We present three new improvements to the state of the art: a barrier-aware line-search filter that cures blocked descent steps due to element barrier terms and so enables rapid progress; an energy proxy model that adaptively blends the Sobolev (inverse-Laplacian-processed) gradient and L-BFGS descent to gain the advantages of both, while avoiding L-BFGS's current limitations in distortion optimization tasks; and a characteristic gradient norm providing a robust and largely mesh- and energy-independent convergence criterion that avoids wrongful termination when algorithms temporarily slow their progress. Together these improvements form the basis for Blended Cured Quasi-Newton (BCQN), a new distortion optimization algorithm. Over a wide range of problems over all scales we show that BCQN is generally the fastest and most robust method available, making some previously intractable problems practical while offering up to an order of magnitude improvement in others.",Blended cured quasi-newton for distortion optimization,NA:NA:NA,2018
Ligang Liu:Chunyang Ye:Ruiqi Ni:Xiao-Ming Fu,"We propose a novel approach, called Progressive Parameterizations, to compute foldover-free parameterizations with low isometric distortion on disk topology meshes. Instead of using the input mesh as a reference to define the objective function, we introduce a progressive reference that contains bounded distortion to the parameterized mesh and is as close as possible to the input mesh. After optimizing the bounded distortion energy between the progressive reference and the parameterized mesh, the parameterized mesh easily approaches the progressive reference, thereby also coming close to the input. By iteratively generating the progressive reference and optimizing the bounded distortion energy to update the parameterized mesh, our algorithm achieves high-quality parameterizations with strong practical reliability and high efficiency. We have demonstrated that our algorithm succeeds on a massive test data set containing over 20712 complex disk topology meshes. Compared to the state-of-the-art methods, our method has achieved higher computational efficiency and practical reliability.",Progressive parameterizations,NA:NA:NA:NA,2018
Yue Peng:Bailin Deng:Juyong Zhang:Fanyu Geng:Wenjie Qin:Ligang Liu,"Many computer graphics problems require computing geometric shapes subject to certain constraints. This often results in non-linear and non-convex optimization problems with globally coupled variables, which pose great challenge for interactive applications. Local-global solvers developed in recent years can quickly compute an approximate solution to such problems, making them an attractive choice for applications that prioritize efficiency over accuracy. However, these solvers suffer from lower convergence rate, and may take a long time to compute an accurate result. In this paper, we propose a simple and effective technique to accelerate the convergence of such solvers. By treating each local-global step as a fixed-point iteration, we apply Anderson acceleration, a well-established technique for fixed-point solvers, to speed up the convergence of a local-global solver. To address the stability issue of classical Anderson acceleration, we propose a simple strategy to guarantee the decrease of target energy and ensure its global convergence. In addition, we analyze the connection between Anderson acceleration and quasi-Newton methods, and show that the canonical choice of its mixing parameter is suitable for accelerating local-global solvers. Moreover, our technique is effective beyond classical local-global solvers, and can be applied to iterative methods with a common structure. We evaluate the performance of our technique on a variety of geometry optimization and physics simulation problems. Our approach significantly reduces the number of iterations required to compute an accurate result, with only a slight increase of computational cost per iteration. Its simplicity and effectiveness makes it a promising tool for accelerating existing algorithms as well as designing efficient new algorithms.",Anderson acceleration for geometry optimization and physics simulation,NA:NA:NA:NA:NA:NA,2018
Gavin Barill:Neil G. Dickson:Ryan Schmidt:David I. W. Levin:Alec Jacobson,"Inside-outside determination is a basic building block for higher-level geometry processing operations. Generalized winding numbers provide a robust answer for triangle meshes, regardless of defects such as self-intersections, holes or degeneracies. In this paper, we further generalize the winding number to point clouds. Previous methods for evaluating the winding number are slow for completely disconnected surfaces, such as triangle soups or-in the extreme case- point clouds. We propose a tree-based algorithm to reduce the asymptotic complexity of generalized winding number computation, while closely approximating the exact value. Armed with a fast evaluation, we demonstrate the winding number in a variety of new applications: voxelization, signing distances, generating 3D printer paths, defect-tolerant mesh booleans and point set surfaces.",Fast winding numbers for soups and clouds,NA:NA:NA:NA:NA,2018
Yajie Yan:David Letscher:Tao Ju,"We present a novel algorithm for computing the medial axes of 3D shapes. We make the observation that the medial axis of a voxel shape can be simply yet faithfully approximated by the interior Voronoi diagram of the boundary vertices, which we call the voxel core. We further show that voxel cores can approximate the medial axes of any smooth shape with homotopy equivalence and geometric convergence. These insights motivate an algorithm that is simple, efficient, numerically stable, and equipped with theoretical guarantees. Compared with existing voxel-based methods, our method inherits their simplicity but is more scalable and can process significantly larger inputs. Compared with sampling-based methods that offer similar theoretical guarantees, our method produces visually comparable results but more robustly captures the topology of the input shape.","Voxel cores: efficient, robust, and provably good approximation of 3D medial axes",NA:NA:NA,2018
Yijing Li:Jernej Barbič,"Self-intersecting, or nearly self-intersecting, meshes are commonly found in 2D and 3D computer graphics practice. Self-intersections occur, for example, in the process of artist manual work, as a by-product of procedural methods for mesh generation, or due to modeling errors introduced by scanning equipment. If the space bounded by such inputs is meshed naively, the resulting mesh joins (""glues"") self-overlapping parts, precluding efficient further modeling and animation of the underlying geometry. Similarly, near self-intersections force the simulation algorithm to employ an unnecessarily detailed mesh to separate the nearly self-intersecting regions. Our work addresses both of these challenges, by giving an algorithm to generate an ""un-glued"" simulation mesh, of arbitrary user-chosen resolution, that properly accounts for self-intersections and near self-intersections. In order to achieve this result, we study the mathematical concept of immersion, and give a deterministic and constructive algorithm to determine if the input self-intersecting triangle mesh is the boundary of an immersion. For near self-intersections, we give a robust algorithm to properly duplicate mesh elements and correctly embed the underlying geometry into the mesh element copies. Both the self-intersections and near self-intersections are combined into one algorithm that permits successful meshing at arbitrary resolution. Applications of our work include volumetric shape editing, physically based simulation and animation, and volumetric weight and geodesic distance computation on self-intersecting inputs.",Immersion of self-intersecting solids and surfaces,NA:NA,2018
Roee Lazar:Nadav Dym:Yam Kushinsky:Zhiyang Huang:Tao Ju:Yaron Lipman,"Surface reconstruction is one of the central problems in computer graphics. Existing research on this problem has primarily focused on improving the geometric aspects of the reconstruction (e.g., smoothness, features, element quality, etc.), and little attention has been paid to ensure it also has desired topological properties (e.g., connectedness and genus). In this paper, we propose a novel and general optimization method for surface reconstruction under topological constraints. The input to our method is a prescribed genus for the reconstructed surface, a partition of the ambient volume into cells, and a set of possible surface candidates and their associated energy within each cell. Our method computes one candidate per cell so that their union is a connected surface with the prescribed genus that minimizes the total energy. We formulate the task as an integer program, and propose a novel solution that combines convex relaxations within a branch and bound framework. As our method is oblivious of the type of input cells, surface candidates, and energy, it can be applied to a variety of reconstruction scenarios, and we explore two of them in the paper: reconstruction from cross-section slices and iso-surfacing an intensity volume. In the first scenario, our method outperforms an existing topology-aware method particularly for complex inputs and higher genus constraints. In the second scenario, we demonstrate the benefit of topology control over classical topology-oblivious methods such as Marching Cubes.",Robust optimization for topological surface reconstruction,NA:NA:NA:NA:NA:NA,2018
Mingming He:Dongdong Chen:Jing Liao:Pedro V. Sander:Lu Yuan,"We propose the first deep learning approach for exemplar-based local colorization. Given a reference color image, our convolutional neural network directly maps a grayscale image to an output colorized image. Rather than using hand-crafted rules as in traditional exemplar-based methods, our end-to-end colorization network learns how to select, propagate, and predict colors from the large-scale data. The approach performs robustly and generalizes well even when using reference images that are unrelated to the input grayscale image. More importantly, as opposed to other learning-based colorization methods, our network allows the user to achieve customizable results by simply feeding different references. In order to further reduce manual effort in selecting the references, the system automatically recommends references with our proposed image retrieval algorithm, which considers both semantic and luminance information. The colorization can be performed fully automatically by simply picking the top reference suggestion. Our approach is validated through a user study and favorable quantitative comparisons to the-state-of-the-art methods. Furthermore, our approach can be naturally extended to video colorization. Our code and models are freely available for public use.",Deep exemplar-based colorization,NA:NA:NA:NA:NA,2018
Tae-Hoon Kim:Sang Il Park,"A fully automatic method for descreening halftone images is presented based on convolutional neural networks with end-to-end learning. Incorporating context level information, the proposed method not only removes halftone artifacts but also synthesizes the fine details lost during halftone. The method consists of two main stages. In the first stage, intrinsic features of the scene are extracted, the low-frequency reconstruction of the image is estimated, and halftone patterns are removed. For the intrinsic features, the edges and object-categories are estimated and fed to the next stage as strong visual and contextual cues. In the second stage, fine details are synthesized on top of the low-frequency output based on an adversarial generative model. In addition, the novel problem of rescreening is addressed, where a natural input image is halftoned so as to be similar to a separately given reference halftone image. To this end, a two-stage convolutional neural network is also presented. Both networks are trained with millions of before-and-after example image pairs of various halftone styles. Qualitative and quantitative evaluations are provided, which demonstrates the effectiveness of the proposed methods.",Deep context-aware descreening and rescreening of halftone images,NA:NA,2018
Yang Zhou:Zhen Zhu:Xiang Bai:Dani Lischinski:Daniel Cohen-Or:Hui Huang,"The real world exhibits an abundance of non-stationary textures. Examples include textures with large scale structures, as well as spatially variant and inhomogeneous textures. While existing example-based texture synthesis methods can cope well with stationary textures, non-stationary textures still pose a considerable challenge, which remains unresolved. In this paper, we propose a new approach for example-based non-stationary texture synthesis. Our approach uses a generative adversarial network (GAN), trained to double the spatial extent of texture blocks extracted from a specific texture exemplar. Once trained, the fully convolutional generator is able to expand the size of the entire exemplar, as well as of any of its sub-blocks. We demonstrate that this conceptually simple approach is highly effective for capturing large scale structures, as well as other non-stationary attributes of the input exemplar. As a result, it can cope with challenging textures, which, to our knowledge, no other existing method can handle.",Non-stationary texture synthesis by adversarial expansion,NA:NA:NA:NA:NA:NA,2018
Nicholas J. Weidner:Kyle Piddington:David I. W. Levin:Shinjiro Sueda,"We resolve the longstanding problem of simulating the contact-mediated interaction of cloth and sharp geometric features by introducing an Eulerian-on-Lagrangian (EOL) approach to cloth simulation. Unlike traditional Lagrangian approaches to cloth simulation, our EOL approach permits bending exactly at and sliding over sharp edges, avoiding parasitic locking caused by over-constraining contact constraints. Wherever the cloth is in contact with sharp features, we insert EOL vertices into the cloth, while the rest of the cloth is simulated in the standard Lagrangian fashion. Our algorithm manifests as new equations of motion for EOL vertices, a contact-conforming remesher, and a set of simple constraint assignment rules, all of which can be incorporated into existing state-of-the-art cloth simulators to enable smooth, inequality-constrained contact between cloth and objects in the world.",Eulerian-on-lagrangian cloth simulation,NA:NA:NA:NA,2018
Yun (Raymond) Fei:Christopher Batty:Eitan Grinspun:Changxi Zheng,"We propose a method for simulating the complex dynamics of partially and fully saturated woven and knit fabrics interacting with liquid, including the effects of buoyancy, nonlinear drag, pore (capillary) pressure, dripping, and convection-diffusion. Our model evolves the velocity fields of both the liquid and solid relying on mixture theory, as well as tracking a scalar saturation variable that affects the pore pressure forces in the fluid. We consider the porous microstructure implied by the fibers composing individual threads, and use it to derive homogenized drag and pore pressure models that faithfully reflect the anisotropy of fabrics. In addition to the bulk liquid and fabric motion, we derive a quasi-static flow model that accounts for liquid spreading within the fabric itself. Our implementation significantly extends standard numerical cloth and fluid models to support the diverse behaviors of wet fabric, and includes a numerical method tailored to cope with the challenging nonlinearities of the problem. We explore a range of fabric-water interactions to validate our model, including challenging animation scenarios involving splashing, wringing, and collisions with obstacles, along with qualitative comparisons against simple physical experiments.",A multi-scale model for simulating liquid-fabric interactions,NA:NA:NA:NA,2018
Jie Li:Gilles Daviet:Rahul Narain:Florence Bertails-Descoubes:Matthew Overby:George E. Brown:Laurence Boissieux,"Cloth dynamics plays an important role in the visual appearance of moving characters. Properly accounting for contact and friction is of utmost importance to avoid cloth-body and cloth-cloth penetration and to capture typical folding and stick-slip behavior due to dry friction. We present here the first method able to account for cloth contact with exact Coulomb friction, treating both cloth self-contacts and contacts occurring between the cloth and an underlying character. Our key contribution is to observe that for a nodal system like cloth, the frictional contact problem may be formulated based on velocities as primary variables, without having to compute the costly Delassus operator. Then, by reversing the roles classically played by the velocities and the contact impulses, conical complementarity solvers of the literature can be adapted to solve for compatible velocities at nodes. To handle the full complexity of cloth dynamics scenarios, we have extended this base algorithm in two ways: first, towards the accurate treatment of frictional contact at any location of the cloth, through an adaptive node refinement strategy; second, towards the handling of multiple constraints at each node, through the duplication of constrained nodes and the adding of pin constraints between duplicata. Our method allows us to handle the complex cloth-cloth and cloth-body interactions in full-size garments with an unprecedented level of realism compared to former methods, while maintaining reasonable computational timings.",An implicit frictional contact solver for adaptive cloth simulation,NA:NA:NA:NA:NA:NA:NA,2018
Huamin Wang,"Being able to customize sewing patterns for different human bodies without using any pre-defined adjustment rule will not only improve the realism of virtual humans in the entertainment industry, but also deeply affect the fashion industry by making fast fashion and made-to-measure garments more accessible. To meet the requirement set by the fashion industry, a sewing pattern adjustment system must be both efficient and precise, which unfortunately cannot be achieved by existing techniques. In this paper, we propose to solve sewing pattern adjustment as a nonlinear optimization problem immediately, rather than in two phases: a garment shape optimization phase and an inverse pattern design phase as in previous systems. This allows us to directly minimize the objective function that evaluates the fitting quality of the garment sewn from a pattern, without any compromise caused by the nonexistence of the solution to inverse pattern design. To improve the efficiency of our system, we carry out systematic research on a variety of optimization topics, including pattern parametrization, initialization, an inexact strategy, acceleration, and CPU-GPU implementation. We verify the usability of our system through automatic grading tests and made-to-measure tests. Designers and pattern makers confirm that our pattern results are able to preserve design details and their fitting qualities are acceptable. In our computational experiment, the system further demonstrates its efficiency, reliability, and flexibility of handling various pattern designs. While our current system still needs to overcome certain limitations, we believe it is a crucial step toward fully automatic pattern design and adjustment in the future.",Rule-free sewing pattern adjustment with precision and efficiency,NA,2018
Jingwen Wang:Ravi Ramamoorthi,"Spherical Harmonic (SH) lighting is widely used for real-time rendering within Precomputed Radiance Transfer (PRT) systems. SH coefficients are precomputed and stored at object vertices, and combined interactively with SH lighting coefficients to enable effects like soft shadows, interreflections, and glossy reflection. However, the most common PRT techniques assume distant, low-frequency environment lighting, for which SH lighting coefficients can easily be computed once per frame. There is currently limited support for near-field illumination and area lights, since it is non-trivial to compute the SH coefficients for an area light, and the incident lighting (SH coefficients) varies over the object geometry. We present an efficient closed-form solution for projection of uniform polygonal area lights to spherical harmonic coefficients of arbitrary order, enabling easy adoption of accurate area lighting in PRT systems, with no modifications required to the core PRT framework. Our method only requires computing zonal harmonic (ZH) coefficients, for which we introduce a novel recurrence relation. In practice, ZH coefficients are built up iteratively, with computation linear in the desired SH order. General SH coefficients can then be obtained by the recently developed sparse zonal harmonic rotation method.",Analytic spherical harmonic coefficients for polygonal area lights,NA:NA,2018
Thomas Leimkühler:Hans-Peter Seidel:Tobias Ritschel,"Simulating combinations of depth-of-field and motion blur is an important factor to cinematic quality in synthetic images but can take long to compute. Splatting the point-spread function (PSF) of every pixel is general and provides high quality, but requires prohibitive compute time. We accelerate this in two steps: In a pre-process we optimize for sparse representations of the Laplacian of all possible PSFs that we call spreadlets. At runtime, spreadlets can be splat efficiently to the Laplacian of an image. Integrating this image produces the final result. Our approach scales faithfully to strong motion and large out-of-focus areas and compares favorably in speed and quality with off-line and interactive approaches. It is applicable to both synthesizing from pinhole as well as reconstructing from stochastic images, with or without layering.",Laplacian kernel splatting for efficient depth-of-field and motion blur synthesis or reconstruction,NA:NA:NA,2018
Masaki Nakada:Tao Zhou:Honglin Chen:Tomer Weiss:Demetri Terzopoulos,"We introduce a biomimetic framework for human sensorimotor control, which features a biomechanically simulated human musculoskeletal model actuated by numerous muscles, with eyes whose retinas have nonuniformly distributed photoreceptors. The virtual human's sensorimotor control system comprises 20 trained deep neural networks (DNNs), half constituting the neuromuscular motor subsystem, while the other half compose the visual sensory subsystem. Directly from the photoreceptor responses, 2 vision DNNs drive eye and head movements, while 8 vision DNNs extract visual information required to direct arm and leg actions. Ten DNNs achieve neuromuscular control---2 DNNs control the 216 neck muscles that actuate the cervicocephalic musculoskeletal complex to produce natural head movements, and 2 DNNs control each limb; i.e., the 29 muscles of each arm and 39 muscles of each leg. By synthesizing its own training data, our virtual human automatically learns efficient, online, active visuomotor control of its eyes, head, and limbs in order to perform nontrivial tasks involving the foveation and visual pursuit of target objects coupled with visually-guided limb-reaching actions to intercept the moving targets, as well as to carry out drawing and writing tasks.",Deep learning of biomimetic sensorimotor control for biomechanical human animation,NA:NA:NA:NA:NA,2018
Seunghwan Lee:Ri Yu:Jungnam Park:Mridul Aanjaneya:Eftychios Sifakis:Jehee Lee,"We propose a framework for simulation and control of the human musculoskeletal system, capable of reproducing realistic animations of dexterous activities with high-level coordination. We present the first controllable system in this class that incorporates volumetric muscle actuators, tightly coupled with the motion controller, in enhancement of line-segment approximations that prior art is overwhelmingly restricted to. The theoretical framework put forth by our methodology computes all the necessary Jacobians for control, even with the drastically increased dimensionality of the state descriptors associated with three-dimensional, volumetric muscles. The direct coupling of volumetric actuators in the controller allows us to model muscular deficiencies that manifest in shape and geometry, in ways that cannot be captured with line-segment approximations. Our controller is coupled with a trajectory optimization framework, and its efficacy is demonstrated in complex motion tasks such as juggling, and weightlifting sequences with variable anatomic parameters and interaction constraints.",Dexterous manipulation and control with volumetric muscles,NA:NA:NA:NA:NA:NA,2018
Dinesh K. Pai:Austin Rothwell:Pearson Wyder-Hodge:Alistair Wick:Ye Fan:Egor Larionov:Darcy Harrison:Debanga Raj Neog:Cole Shing,"Simulating how the human body deforms in contact with external objects, tight clothing, or other humans is of central importance to many fields. Despite great advances in numerical methods, the material properties required to accurately simulate the body of a real human have been sorely lacking. Here we show that mechanical properties of the human body can be directly measured using a novel hand-held device. We describe a complete pipeline for measurement, modeling, parameter estimation, and simulation using the finite element method. We introduce a phenomenological model (the sliding thick skin model) that is effective for both simulation and parameter estimation. Our data also provide new insights into how the human body actually behaves. The methods described here can be used to create personalized models of an individual human or of a population. Consequently, our methods have many potential applications in computer animation, product design, e-commerce, and medicine.",The human touch: measuring contact with real human soft tissues,NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Gaspard Zoss:Derek Bradley:Pascal Bérard:Thabo Beeler,"In computer graphics the motion of the jaw is commonly modelled by up-down and left-right rotation around a fixed pivot plus a forward-backward translation, yielding a three dimensional rig that is highly suited for intuitive artistic control. The anatomical motion of the jaw is, however, much more complex since the joints that connect the jaw to the skull exhibit both rotational and translational components. In reality the jaw does not move in a three dimensional subspace but on a constrained manifold in six dimensions. We analyze this manifold in the context of computer animation and show how the manifold can be parameterized with three degrees of freedom, providing a novel jaw rig that preserves the intuitive control while providing more accurate jaw positioning. The chosen parameterization furthermore places anatomically correct limits on the motion, preventing the rig from entering physiologically infeasible poses. Our new jaw rig is empirically designed from accurate capture data, and we provide a simple method to retarget the rig to new characters, both human and fantasy.",An empirical rig for jaw animation,NA:NA:NA:NA,2018
Yixin Hu:Qingnan Zhou:Xifeng Gao:Alec Jacobson:Denis Zorin:Daniele Panozzo,"We propose a novel tetrahedral meshing technique that is unconditionally robust, requires no user interaction, and can directly convert a triangle soup into an analysis-ready volumetric mesh. The approach is based on several core principles: (1) initial mesh construction based on a fully robust, yet efficient, filtered exact computation (2) explicit (automatic or user-defined) tolerancing of the mesh relative to the surface input (3) iterative mesh improvement with guarantees, at every step, of the output validity. The quality of the resulting mesh is a direct function of the target mesh size and allowed tolerance: increasing allowed deviation from the initial mesh and decreasing the target edge length both lead to higher mesh quality. Our approach enables ""black-box"" analysis, i.e. it allows to automatically solve partial differential equations on geometrical models available in the wild, offering a robustness and reliability comparable to, e.g., image processing algorithms, opening the door to automatic, large scale processing of real-world geometric data.",Tetrahedral meshing in the wild,NA:NA:NA:NA:NA:NA,2018
Leman Feng:Pierre Alliez:Laurent Busé:Hervé Delingette:Mathieu Desbrun,"Meshes with curvilinear elements hold the appealing promise of enhanced geometric flexibility and higher-order numerical accuracy compared to their commonly-used straight-edge counterparts. However, the generation of curved meshes remains a computationally expensive endeavor with current meshing approaches: high-order parametric elements are notoriously difficult to conform to a given boundary geometry, and enforcing a smooth and non-degenerate Jacobian everywhere brings additional numerical difficulties to the meshing of complex domains. In this paper, we propose an extension of Optimal Delaunay Triangulations (ODT) to curved and graded isotropic meshes. By exploiting a continuum mechanics interpretation of ODT instead of the usual approximation theoretical foundations, we formulate a very robust geometry and topology optimization of Bézier meshes based on a new simple functional promoting isotropic and uniform Jacobians throughout the domain. We demonstrate that our resulting curved meshes can adapt to complex domains with high precision even for a small count of elements thanks to the added flexibility afforded by more control points and higher order basis functions.",Curved optimal delaunay triangulation,NA:NA:NA:NA:NA,2018
Zichun Zhong:Wenping Wang:Bruno Lévy:Jing Hua:Xiaohu Guo,"This article presents a new method to compute a self-intersection free high-dimensional Euclidean embedding (SIFHDE2) for surfaces and volumes equipped with an arbitrary Riemannian metric. It is already known that given a high-dimensional (high-d) embedding, one can easily compute an anisotropic Voronoi diagram by back-mapping it to 3D space. We show here how to solve the inverse problem, i.e., given an input metric, compute a smooth intersection-free high-d embedding of the input such that the pullback metric of the embedding matches the input metric. Our numerical solution mechanism matches the deformation gradient of the 3D → higher-d mapping with the given Riemannian metric. We demonstrate the applicability of our method, by using it to construct anisotropic Restricted Voronoi Diagram (RVD) and anisotropic meshing, that are otherwise extremely difficult to compute. In SIFHDE2-space constructed by our algorithm, difficult 3D anisotropic computations are replaced with simple Euclidean computations, resulting in an isotropic RVD and its dual mesh on this high-d embedding. Results are compared with the state-of-the-art in anisotropic surface and volume meshings using several examples and evaluation metrics.",Computing a high-dimensional euclidean embedding from an arbitrary smooth riemannian metric,NA:NA:NA:NA:NA,2018
Albert Chern:Felix Knöppel:Ulrich Pinkall:Peter Schröder,"We study the isometric immersion problem for orientable surface triangle meshes endowed with only a metric: given the combinatorics of the mesh together with edge lengths, approximate an isometric immersion into R3. To address this challenge we develop a discrete theory for surface immersions into R3. It precisely characterizes a discrete immersion, up to subdivision and small perturbations. In particular our discrete theory correctly represents the topology of the space of immersions, i.e., the regular homotopy classes which represent its connected components. Our approach relies on unit quaternions to represent triangle orientations and to encode, in their parallel transport, the topology of the immersion. In unison with this theory we develop a computational apparatus based on a variational principle. Minimizing a non-linear Dirichlet energy optimally finds extrinsic geometry for the given intrinsic geometry and ensures low metric approximation error. We demonstrate our algorithm with a number of applications from mathematical visualization and art directed isometric shape deformation, which mimics the behavior of thin materials with high membrane stiffness.",Shape from metric,NA:NA:NA:NA,2018
Neal Wadhwa:Rahul Garg:David E. Jacobs:Bryan E. Feldman:Nori Kanazawa:Robert Carroll:Yair Movshovitz-Attias:Jonathan T. Barron:Yael Pritch:Marc Levoy,"Shallow depth-of-field is commonly used by photographers to isolate a subject from a distracting background. However, standard cell phone cameras cannot produce such images optically, as their short focal lengths and small apertures capture nearly all-in-focus images. We present a system to computationally synthesize shallow depth-of-field images with a single mobile camera and a single button press. If the image is of a person, we use a person segmentation network to separate the person and their accessories from the background. If available, we also use dense dual-pixel auto-focus hardware, effectively a 2-sample light field with an approximately 1 millimeter baseline, to compute a dense depth map. These two signals are combined and used to render a defocused image. Our system can process a 5.4 megapixel image in 4 seconds on a mobile phone, is fully automatic, and is robust enough to be used by non-experts. The modular nature of our system allows it to degrade naturally in the absence of a dual-pixel sensor or a human subject.",Synthetic depth-of-field with a single-camera mobile phone,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Tinghui Zhou:Richard Tucker:John Flynn:Graham Fyffe:Noah Snavely,"The view synthesis problem---generating novel views of a scene from known imagery---has garnered recent attention due in part to compelling applications in virtual and augmented reality. In this paper, we explore an intriguing scenario for view synthesis: extrapolating views from imagery captured by narrow-baseline stereo cameras, including VR cameras and now-widespread dual-lens camera phones. We call this problem stereo magnification, and propose a learning framework that leverages a new layered representation that we call multiplane images (MPIs). Our method also uses a massive new data source for learning view extrapolation: online videos on YouTube. Using data mined from such videos, we train a deep network that predicts an MPI from an input stereo image pair. This inferred MPI can then be used to synthesize a range of novel views of the scene, including views that extrapolate significantly beyond the input baseline. We show that our method compares favorably with several recent view synthesis methods, and demonstrate applications in magnifying narrow-baseline stereo images.",Stereo magnification: learning view synthesis using multiplane images,NA:NA:NA:NA:NA,2018
Eike Langbehn:Frank Steinicke:Markus Lappe:Gregory F. Welch:Gerd Bruder,"Immersive computer-generated environments (aka virtual reality, VR) are limited by the physical space around them, e.g., enabling natural walking in VR is only possible by perceptually-inspired locomotion techniques such as redirected walking (RDW). We introduce a completely new approach to imperceptible position and orientation redirection that takes advantage of the fact that even healthy humans are functionally blind for circa ten percent of the time under normal circumstances due to motor processes preventing light from reaching the retina (such as eye blinks) or perceptual processes suppressing degraded visual information (such as blink-induced suppression). During such periods of missing visual input, change blindness occurs, which denotes the inability to perceive a visual change such as the motion of an object or self-motion of the observer. We show that this phenomenon can be exploited in VR by synchronizing the computer graphics rendering system with the human visual processes for imperceptible camera movements, in particular to implement position and orientation redirection. We analyzed human sensitivity to such visual changes with detection thresholds, which revealed that commercial off-the-shelf eye trackers and head-mounted displays suffice to translate a user by circa 4 -- 9 cm and rotate the user by circa 2 -- 5 degrees in any direction, which could be accumulated each time the user blinks. Moreover, we show the potential for RDW, whose performance could be improved by approximately 50% when using our technique.",In the blink of an eye: leveraging blink-induced suppression for imperceptible position and orientation redirection in virtual reality,NA:NA:NA:NA:NA,2018
Qi Sun:Anjul Patney:Li-Yi Wei:Omer Shapira:Jingwan Lu:Paul Asente:Suwen Zhu:Morgan Mcguire:David Luebke:Arie Kaufman,"Redirected walking techniques can enhance the immersion and visual-vestibular comfort of virtual reality (VR) navigation, but are often limited by the size, shape, and content of the physical environments. We propose a redirected walking technique that can apply to small physical environments with static or dynamic obstacles. Via a head- and eye-tracking VR headset, our method detects saccadic suppression and redirects the users during the resulting temporary blindness. Our dynamic path planning runs in real-time on a GPU, and thus can avoid static and dynamic obstacles, including walls, furniture, and other VR users sharing the same physical space. To further enhance saccadic redirection, we propose subtle gaze direction methods tailored for VR perception. We demonstrate that saccades can significantly increase the rotation gains during redirection without introducing visual distortions or simulator sickness. This allows our method to apply to large open virtual spaces and small physical environments for room-scale VR. We evaluate our system via numerical simulations and real user studies.",Towards virtual reality infinite walking: dynamic saccadic redirection,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Stephen Lombardi:Jason Saragih:Tomas Simon:Yaser Sheikh,"We introduce a deep appearance model for rendering the human face. Inspired by Active Appearance Models, we develop a data-driven rendering pipeline that learns a joint representation of facial geometry and appearance from a multiview capture setup. Vertex positions and view-specific textures are modeled using a deep variational autoencoder that captures complex nonlinear effects while producing a smooth and compact latent representation. View-specific texture enables the modeling of view-dependent effects such as specularity. In addition, it can also correct for imperfect geometry stemming from biased or low resolution estimates. This is a significant departure from the traditional graphics pipeline, which requires highly accurate geometry as well as all elements of the shading model to achieve realism through physically-inspired light transport. Acquiring such a high level of accuracy is difficult in practice, especially for complex and intricate parts of the face, such as eyelashes and the oral cavity. These are handled naturally by our approach, which does not rely on precise estimates of geometry. Instead, the shading model accommodates deficiencies in geometry though the flexibility afforded by the neural network employed. At inference time, we condition the decoding network on the viewpoint of the camera in order to generate the appropriate texture for rendering. The resulting system can be implemented simply using existing rendering engines through dynamic textures with flat lighting. This representation, together with a novel unsupervised technique for mapping images to facial states, results in a system that is naturally suited to real-time interactive settings such as Virtual Reality (VR).",Deep appearance models for face rendering,NA:NA:NA:NA,2018
Kfir Aberman:Jing Liao:Mingyi Shi:Dani Lischinski:Baoquan Chen:Daniel Cohen-Or,"Correspondence between images is a fundamental problem in computer vision, with a variety of graphics applications. This paper presents a novel method for sparse cross-domain correspondence. Our method is designed for pairs of images where the main objects of interest may belong to different semantic categories and differ drastically in shape and appearance, yet still contain semantically related or geometrically similar parts. Our approach operates on hierarchies of deep features, extracted from the input images by a pre-trained CNN. Specifically, starting from the coarsest layer in both hierarchies, we search for Neural Best Buddies (NBB): pairs of neurons that are mutual nearest neighbors. The key idea is then to percolate NBBs through the hierarchy, while narrowing down the search regions at each level and retaining only NBBs with significant activations. Furthermore, in order to overcome differences in appearance, each pair of search regions is transformed into a common appearance. We evaluate our method via a user study, in addition to comparisons with alternative correspondence approaches. The usefulness of our method is demonstrated using a variety of graphics applications, including cross-domain image alignment, creation of hybrid images, automatic image morphing, and more.",Neural best-buddies: sparse cross-domain correspondence,NA:NA:NA:NA:NA:NA,2018
Kai Wang:Manolis Savva:Angel X. Chang:Daniel Ritchie,"We present a convolutional neural network based approach for indoor scene synthesis. By representing 3D scenes with a semantically-enriched image-based representation based on orthographic top-down views, we learn convolutional object placement priors from the entire context of a room. Our approach iteratively generates rooms from scratch, given only the room architecture as input. Through a series of perceptual studies we compare the plausibility of scenes generated using our method against baselines for object selection and object arrangement, as well as scenes modeled by people. We find that our method generates scenes that are preferred over the baselines, and in some cases are equally preferred to human-created scenes.",Deep convolutional priors for indoor scene synthesis,NA:NA:NA:NA,2018
Matan Atzmon:Haggai Maron:Yaron Lipman,"This paper presents Point Convolutional Neural Networks (PCNN): a novel framework for applying convolutional neural networks to point clouds. The framework consists of two operators: extension and restriction, mapping point cloud functions to volumetric functions and vise-versa. A point cloud convolution is defined by pull-back of the Euclidean volumetric convolution via an extension-restriction mechanism. The point cloud convolution is computationally efficient, invariant to the order of points in the point cloud, robust to different samplings and varying densities, and translation invariant, that is the same convolution kernel is used at all points. PCNN generalizes image CNNs and allows readily adapting their architectures to the point cloud setting. Evaluation of PCNN on three central point cloud learning benchmarks convincingly outperform competing point cloud learning methods, and the vast majority of methods working with more informative shape representations such as surfaces and/or normals.",Point convolutional neural networks by extension operators,NA:NA:NA,2018
Yağiz Aksoy:Tae-Hyun Oh:Sylvain Paris:Marc Pollefeys:Wojciech Matusik,"Accurate representation of soft transitions between image regions is essential for high-quality image editing and compositing. Current techniques for generating such representations depend heavily on interaction by a skilled visual artist, as creating such accurate object selections is a tedious task. In this work, we introduce semantic soft segments, a set of layers that correspond to semantically meaningful regions in an image with accurate soft transitions between different objects. We approach this problem from a spectral segmentation angle and propose a graph structure that embeds texture and color features from the image as well as higher-level semantic information generated by a neural network. The soft segments are generated via eigendecomposition of the carefully constructed Laplacian matrix fully automatically. We demonstrate that otherwise complex image editing tasks can be done with little effort using semantic soft segments.",Semantic soft segmentation,NA:NA:NA:NA:NA,2018
Laurent Belcour,"We derive a novel framework for the efficient analysis and computation of light transport within layered materials. Our derivation consists in two steps. First, we decompose light transport into a set of atomic operators that act on its directional statistics. Specifically, our operators consist of reflection, refraction, scattering, and absorption, whose combinations are sufficient to describe the statistics of light scattering multiple times within layered structures. We show that the first three directional moments (energy, mean and variance) already provide an accurate summary. Second, we extend the adding-doubling method to support arbitrary combinations of such operators efficiently. During shading, we map the directional moments to BSDF lobes. We validate that the resulting BSDF closely matches the ground truth in a lightweight and efficient form. Unlike previous methods we support an arbitrary number of textured layers, and demonstrate a practical and accurate rendering of layered materials with both an offline and real-time implementation that are free from per-material precomputation.",Efficient rendering of layered materials using an atomic decomposition with statistical operators,NA,2018
Tizian Zeltner:Wenzel Jakob,"We present a versatile computational framework for modeling the reflective and transmissive properties of arbitrarily layered anisotropic material structures. Given a set of input layers, our model synthesizes an effective BSDF of the entire structure, which accounts for all orders of internal scattering and is efficient to sample and evaluate in modern rendering systems. Our technique builds on the insight that reflectance data is sparse when expanded into a suitable frequency-space representation, and that this property extends to the class of anisotropic materials. This sparsity enables an efficient matrix calculus that admits the entire space of BSDFs and considerably expands the scope of prior work on layered material modeling. We show how both measured data and the popular class of microfacet models can be expressed in our representation, and how the presence of anisotropy leads to a weak coupling between Fourier orders in frequency space. In addition to additive composition, our models supports subtractive composition, a fascinating new operation that reconstructs the BSDF of a material that can only be observed indirectly through another layer with known reflectance properties. The operation produces a new BSDF of the desired layer as if measured in isolation. Subtractive composition can be interpreted as a type of deconvolution that removes both internal scattering and blurring due to transmission through the known layer. We experimentally demonstrate the accuracy and scope of our model and validate both additive and subtractive composition using measurements of real-world layered materials. Both implementation and data will be released to ensure full reproducibility of all of our results.1",The layer laboratory: a calculus for additive and subtractive composition of anisotropic surface reflectance,NA:NA,2018
Ling-Qi Yan:Miloš Hašan:Bruce Walter:Steve Marschner:Ravi Ramamoorthi,"Simulation of light reflection from specular surfaces is a core problem of computer graphics. Existing solutions either make the approximation of providing only a large-area average solution in terms of a fixed BRDF (ignoring spatial detail), or are specialized for specific microgeometry (e.g. 1D scratches), or are based only on geometric optics (which is an approximation to more accurate wave optics). We design the first rendering algorithm based on a wave optics model that is also able to compute spatially-varying specular highlights with high-resolution detail on general surface microgeometry. We compute a wave optics reflection integral over the coherence area; our solution is based on approximating the phase-delay grating representation of a micron-resolution surface heightfield using Gabor kernels. We found that the appearance difference between the geometric and wave solution is more dramatic when spatial detail is taken into account. The visualizations of the corresponding BRDF lobes differ significantly. Moreover, the wave optics solution varies as a function of wavelength, predicting noticeable color effects in the highlights. Our results show both single-wavelength and spectral solution to reflection from common everyday objects, such as brushed, scratched and bumpy metals.",Rendering specular microgeometry with wave optics,NA:NA:NA:NA:NA,2018
Károly Zsolnai-Fehér:Peter Wonka:Michael Wimmer,"We present a learning-based system for rapid mass-scale material synthesis that is useful for novice and expert users alike. The user preferences are learned via Gaussian Process Regression and can be easily sampled for new recommendations. Typically, each recommendation takes 40-60 seconds to render with global illumination, which makes this process impracticable for real-world workflows. Our neural network eliminates this bottleneck by providing high-quality image predictions in real time, after which it is possible to pick the desired materials from a gallery and assign them to a scene in an intuitive manner. Workflow timings against Disney's ""principled"" shader reveal that our system scales well with the number of sought materials, thus empowering even novice users to generate hundreds of high-quality material models without any expertise in material modeling. Similarly, expert users experience a significant decrease in the total modeling time when populating a scene with materials. Furthermore, our proposed solution also offers controllable recommendations and a novel latent space variant generation step to enable the real-time fine-tuning of materials without requiring any domain expertise.",Gaussian material synthesis,NA:NA:NA,2018
Oded Stein:Eitan Grinspun:Keenan Crane,"Developable surfaces are those that can be made by smoothly bending flat pieces without stretching or shearing. We introduce a definition of developability for triangle meshes which exactly captures two key properties of smooth developable surfaces, namely flattenability and presence of straight ruling lines. This definition provides a starting point for algorithms in developable surface modeling---we consider a variational approach that drives a given mesh toward developable pieces separated by regular seam curves. Computation amounts to gradient descent on an energy with support in the vertex star, without the need to explicitly cluster patches or identify seams. We briefly explore applications to developable design and manufacturing.",Developability of triangle meshes,NA:NA:NA,2018
Christian Schüller:Roi Poranne:Olga Sorkine-Hornung,"Fabrication from developable parts is the basis for arts such as papercraft and needlework, as well as modern architecture and CAD in general, and it has inspired much research. We observe that the assembly of complex 3D shapes created by existing methods often requires first fabricating many small parts and then carefully following instructions to assemble them together. Despite its significance, this error prone and tedious process is generally neglected in the discussion. We present the concept of zippables - single, two dimensional, branching, ribbon-like pieces of fabric that can be quickly zipped up without any instructions to form 3D objects. Our inspiration comes from the so-called zipit bags [zipit 2017], which are made of a single, long ribbon with a zipper around its boundary. In order to ""assemble"" the bag, one simply needs to zip up the ribbon. Our method operates in the same fashion, but it can be used to approximate a wide variety of shapes. Given a 3D model, our algorithm produces plans for a single 2D shape that can be laser cut in few parts from fabric or paper. A zipper can then be attached along the boundary by sewing, or by gluing using a custom-built fastening rig. We show physical and virtual results that demonstrate the capabilities of our method and the ease with which shapes can be assembled.",Shape representation by zippables,NA:NA:NA,2018
Dimitar Dinev:Tiantian Liu:Jing Li:Bernhard Thomaszewski:Ladislav Kavan,"We propose a novel projection scheme that corrects energy fluctuations in simulations of deformable objects, thereby removing unwanted numerical dissipation and numerical ""explosions"". The key idea of our method is to first take a step using a conventional integrator, then project the result back to the constant energy-momentum manifold. We implement this strategy using fast projection, which only adds a small amount of overhead to existing physics-based solvers. We test our method with several implicit integration rules and demonstrate its benefits when used in conjunction with Position Based Dynamics and Projective Dynamics. When added to a dissipative integrator such as backward Euler, our method corrects the artificial damping and thus produces more vivid motion. Our projection scheme also effectively prevents instabilities that can arise due to approximate solves or large time steps. Our method is fast, stable, and easy to implement---traits that make it well-suited for real-time physics applications such as games or training simulators.",FEPR: fast energy projection for real-time simulation of deformable objects,NA:NA:NA:NA:NA,2018
Christopher Brandt:Elmar Eisemann:Klaus Hildebrandt,"We present a method for the real-time simulation of deformable objects that combines the robustness, generality, and high performance of Projective Dynamics with the efficiency and scalability offered by model reduction techniques. The method decouples the cost for time integration from the mesh resolution and can simulate large meshes in real-time. The proposed hyper-reduction of Projective Dynamics combines a novel fast approximation method for constraint projections and a scalable construction of sparse subspace bases. The resulting system achieves real-time rates for large sub-spaces enabling rich dynamics and can resolve general user interactions, collision constraints, external forces and changes to the materials. The construction of the hyper-reduced system does not require user-interaction and refrains from using training data or modal analysis, which results in a fast preprocessing stage.",Hyper-reduced projective dynamics,NA:NA:NA,2018
Fernando De Goes:Doug L. James,"We introduce Dynamic Kelvinlets, a new analytical technique for real-time physically based animation of virtual elastic materials. Our formulation is based on the dynamic response to time-varying force distributions applied to an infinite elastic medium. The resulting displacements provide the plausibility of volumetric elasticity, the dynamics of compressive and shear waves, and the interactivity of closed-form expressions. Our approach builds upon the work of de Goes and James [2017] by presenting an extension of the regularized Kelvinlet solutions from elastostatics to the elastodynamic regime. To finely control our elastic deformations, we also describe the construction of compound solutions that resolve pointwise and keyframe constraints. We demonstrate the versatility and efficiency of our method with a series of examples in a production grade implementation.",Dynamic kelvinlets: secondary motions based on fundamental solutions of elastodynamics,NA:NA,2018
Adrien Gruson:Binh-Son Hua:Nicolas Vibert:Derek Nowrouzezahrai:Toshiya Hachisuka,"Gradient-domain rendering can improve the convergence of surface-based light transport by exploiting smoothness in image space. Scenes with participating media exhibit similar smoothness and could potentially benefit from gradient-domain techniques. We introduce the first gradient-domain formulation of image synthesis with homogeneous participating media, including four novel and efficient gradient-domain volumetric density estimation algorithms. We show that naïve extensions of gradient domain path-space and density estimation methods to volumetric media, while functional, can result in inefficient estimators. Focussing on point-, beam- and plane-based gradient-domain estimators, we introduce a novel shift mapping that eliminates redundancies in the naïve formulations using spatial relaxation within the volume. We show that gradient-domain volumetric rendering improve convergence compared to primal domain state-of-the-art, across a suite of scenes. Our formulation and algorithms support progressive estimation and are easy to incorporate atop existing renderers.",Gradient-domain volumetric photon density estimation,NA:NA:NA:NA:NA,2018
Adrian Jarabo:Carlos Aliaga:Diego Gutierrez,"We introduce a non-exponential radiative framework that takes into account the local spatial correlation of scattering particles in a medium. Most previous works in graphics have ignored this, assuming uncorrelated media with a uniform, random local distribution of particles. However, positive and negative correlation lead to slower- and faster-than-exponential attenuation respectively, which cannot be predicted by the Beer-Lambert law. As our results show, this has a major effect on extinction, and thus appearance. From recent advances in neutron transport, we first introduce our Extended Generalized Boltzmann Equation, and develop a general framework for light transport in correlated media. We lift the limitations of the original formulation, including an analysis of the boundary conditions, and present a model suitable for computer graphics, based on optical properties of the media and statistical distributions of scatterers. In addition, we present an analytic expression for transmittance in the case of positive correlation, and show how to incorporate it efficiently into a Monte Carlo renderer. We show results with a wide range of both positive and negative correlation, and demonstrate the differences compared to classic light transport.",A radiative transfer framework for spatially-correlated materials,NA:NA:NA,2018
Syuhei Sato:Yoshinori Dobashi:Theodore Kim:Tomoyuki Nishita,"Generating realistic fluid simulations remains computationally expensive, and animators can expend enormous effort trying to achieve a desired motion. To reduce such costs, several methods have been developed in which high-resolution turbulence is synthesized as a post process. Since global motion can then be obtained using a fast, low-resolution simulation, less effort is needed to create a realistic animation with the desired behavior. While much research has focused on accelerating the low-resolution simulation, the problem controlling the behavior of the turbulent, high-resolution motion has received little attention. In this paper, we show that style transfer methods from image editing can be adapted to transfer the turbulent style of an existing fluid simulation onto a new one. We do this by extending example-based image synthesis methods to handle velocity fields using a combination of patch-based and optimization-based texture synthesis. This approach allows us to take into account the incompressibility condition, which we have found to be a important factor during synthesis. Using our method, a user can easily and intuitively create high-resolution fluid animations that have a desired turbulent motion.",Example-based turbulence style transfer,NA:NA:NA:NA,2018
Jonas Zehnder:Rahul Narain:Bernhard Thomaszewski,"Advection-projection methods for fluid animation are widely appreciated for their stability and efficiency. However, the projection step dissipates energy from the system, leading to artificial viscosity and suppression of small-scale details. We propose an alternative approach for detail-preserving fluid animation that is surprisingly simple and effective. We replace the energy-dissipating projection operator applied at the end of a simulation step by an energy-preserving reflection operator applied at mid-step. We show that doing so leads to two orders of magnitude reduction in energy loss, which in turn yields vastly improved detail-preservation. We evaluate our reflection solver on a set of 2D and 3D numerical experiments and show that it compares favorably to state-of-the-art methods. Finally, our method integrates seamlessly with existing projection-advection solvers and requires very little additional implementation.",An advection-reflection solver for detail-preserving fluid simulation,NA:NA:NA,2018
Muzaffer Akbay:Nicholas Nobles:Victor Zordan:Tamar Shinar,"We present a novel extended partitioned method for two-way solid-fluid coupling, where the fluid and solid solvers are treated as black boxes with limited exposed interfaces, facilitating modularity and code reusability. Our method achieves improved stability and extended range of applicability over standard partitioned approaches through three techniques. First, we couple the black-box solvers through a small, reduced-order monolithic system, which is constructed on the fly from input/output pairs generated by the solid and fluid solvers. Second, we use a conservative, impulse-based interaction term to couple the solid and fluid rather than typical pressure-based forces. We show that both of these techniques significantly improve stability and reduce the number of iterations needed for convergence. Finally, we propose a novel boundary pressure projection method that allows for the partitioned simulation of a fully enclosed fluid coupled to a dynamic solid, a scenario that has been problematic for partitioned methods. We demonstrate the benefits of our extended partitioned method by coupling Eulerian fluid solvers for smoke and water to Lagrangian solid solvers for volumetric and thin deformable and rigid objects in a variety of challenging scenarios. We further demonstrate our method by coupling a Lagrangian SPH fluid solver to a rigid body solver.",An extended partitioned method for conservative solid-fluid coupling,NA:NA:NA:NA,2018
Qiaodong Cui:Pradeep Sen:Theodore Kim,"The Laplacian Eigenfunction method for fluid simulation, which we refer to as Eigenfluids, introduced an elegant new way to capture intricate fluid flows with near-zero viscosity. However, the approach does not scale well, as the memory cost grows prohibitively with the number of eigenfunctions. The method also lacks generality, because the dynamics are constrained to a closed box with Dirichlet boundaries, while open, Neumann boundaries are also needed in most practical scenarios. To address these limitations, we present a set of analytic eigenfunctions that supports uniform Neumann and Dirichlet conditions along each domain boundary, and show that by carefully applying the discrete sine and cosine transforms, the storage costs of the eigenfunctions can be made completely negligible. The resulting algorithm is both faster and more memory-efficient than previous approaches, and able to achieve lower viscosities than similar pseudo-spectral methods. We are able to surpass the scalability of the original Laplacian Eigenfunction approach by over two orders of magnitude when simulating rectangular domains. Finally, we show that the formulation allows forward scattering to be directed in a way that is not possible with any other method.",Scalable laplacian eigenfluids,NA:NA:NA,2018
Ke Xie:Hao Yang:Shengqiu Huang:Dani Lischinski:Marc Christie:Kai Xu:Minglun Gong:Daniel Cohen-Or:Hui Huang,"Capturing aerial videos with a quadrotor-mounted camera is a challenging creative task, as it requires the simultaneous control of the quadrotor's motion and the mounted camera's orientation. Letting the drone follow a pre-planned trajectory is a much more appealing option, and recent research has proposed a number of tools designed to automate the generation of feasible camera motion plans; however, these tools typically require the user to specify and edit the camera path, for example by providing a complete and ordered sequence of key viewpoints. In this paper, we propose a higher level tool designed to enable even novice users to easily capture compelling aerial videos of large-scale outdoor scenes. Using a coarse 2.5D model of a scene, the user is only expected to specify starting and ending viewpoints and designate a set of landmarks, with or without a particular order. Our system automatically generates a diverse set of candidate local camera moves for observing each landmark, which are collision-free, smooth, and adapted to the shape of the landmark. These moves are guided by a landmark-centric view quality field, which combines visual interest and frame composition. An optimal global camera trajectory is then constructed that chains together a sequence of local camera moves, by choosing one move for each landmark and connecting them with suitable transition trajectories. This task is formulated and solved as an instance of the Set Traveling Salesman Problem.",Creating and chaining camera moves for qadrotor videography,NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Nobuyuki Umetani:Bernd Bickel,"We present a data-driven technique to instantly predict how fluid flows around various three-dimensional objects. Such simulation is useful for computational fabrication and engineering, but is usually computationally expensive since it requires solving the Navier-Stokes equation for many time steps. To accelerate the process, we propose a machine learning framework which predicts aerodynamic forces and velocity and pressure fields given a three-dimensional shape input. Handling detailed free-form three-dimensional shapes in a data-driven framework is challenging because machine learning approaches usually require a consistent parametrization of input and output. We present a novel PolyCube maps-based parametrization that can be computed for three-dimensional shapes at interactive rates. This allows us to efficiently learn the nonlinear response of the flow using a Gaussian process regression. We demonstrate the effectiveness of our approach for the interactive design and optimization of a car body.",Learning three-dimensional flow for interactive aerodynamic design,NA:NA,2018
Christoph Gebhardt:Stefan Stevšić:Otmar Hilliges,"In this paper we first contribute a large scale online study (N ≈ 400) to better understand aesthetic perception of aerial video. The results indicate that it is paramount to optimize smoothness of trajectories across all keyframes. However, for experts timing control remains an essential tool. Satisfying this dual goal is technically challenging because it requires giving up desirable properties in the optimization formulation. Second, informed by this study we propose a method that optimizes positional and temporal reference fit jointly. This allows to generate globally smooth trajectories, while retaining user control over reference timings. The formulation is posed as a variable, infinite horizon, contour-following algorithm. Finally, a comparative lab study indicates that our optimization scheme outperforms the state-of-the-art in terms of perceived usability and preference of resulting videos. For novices our method produces smoother and better looking results and also experts benefit from generated timings.",Optimizing for aesthetically pleasing quadrotor camera motion,NA:NA:NA,2018
Nahum Farchi:Mirela Ben-Chen,"We propose a new iterative algorithm for computing smooth cross fields on triangle meshes that is simple, easily parallelizable on the GPU, and finds solutions with lower energy and fewer cone singularities than state-of-the-art methods. Our approach is based on a formal equivalence, which we prove, between two formulations of the optimization problem. This equivalence allows us to eliminate the real variables and design an efficient grid search algorithm for the cone singularities. We leverage a recent graph-theoretical approximation of the resistance distance matrix of the triangle mesh to speed up the computation and enable a trade-off between the computation time and the smoothness of the output.",Integer-only cross field computation,NA:NA,2018
Xianzhong Fang:Hujun Bao:Yiying Tong:Mathieu Desbrun:Jin Huang,"We introduce an approach to quadrilateral meshing of arbitrary triangulated surfaces that combines the theoretical guarantees of Morse-based approaches with the practical advantages of parameterization methods. We first construct, through an eigensolver followed by a few Gauss-Newton iterations, a periodic four-dimensional vector field that aligns with a user-provided frame field and/or a set of features over the input mesh. A field-aligned parameterization is then greedily computed along a spanning tree based on the Dirichlet energy of the optimal periodic vector field, from which quad elements are efficiently extracted over most of the surface. The few regions not yet covered by elements are then upsampled and the first component of the periodic vector field is used as a Morse function to extract the remaining quadrangles. This hybrid parameterization- and Morse-based quad meshing method is not only fast (the parameterization is greedily constructed, and the Morse function only needs to be upsampled in the few uncovered patches), but is guaranteed to provide a feature-aligned quad mesh with non-degenerate cells that closely matches the input frame field over an arbitrary surface. We show that our approach is much faster than Morse-based techniques since it does not require a densely tessellated input mesh, and is significantly more robust than parameterization-based techniques on models with complex features.",Quadrangulation through morse-parameterization hybridization,NA:NA:NA:NA:NA,2018
Heng Liu:Paul Zhang:Edward Chien:Justin Solomon:David Bommes,"Despite high practical demand, algorithmic hexahedral meshing with guarantees on robustness and quality remains unsolved. A promising direction follows the idea of integer-grid maps, which pull back the Cartesian hexahedral grid formed by integer isoplanes from a parametric domain to a surface-conforming hexahedral mesh of the input object. Since directly optimizing for a high-quality integer-grid map is mathematically challenging, the construction is usually split into two steps: (1) generation of a surface-aligned octahedral field and (2) generation of an integer-grid map that best aligns to the octahedral field. The main robustness issue stems from the fact that smooth octahedral fields frequently exhibit singularity graphs that are not appropriate for hexahedral meshing and induce heavily degenerate integer-grid maps. The first contribution of this work is an enumeration of all local configurations that exist in hex meshes with bounded edge valence, and a generalization of the Hopf-Poincaré formula to octahedral fields, leading to necessary local and global conditions for the hex-meshability of an octahedral field in terms of its singularity graph. The second contribution is a novel algorithm to generate octahedral fields with prescribed hex-meshable singularity graphs, which requires the solution of a large nonlinear mixed-integer algebraic system. This algorithm is an important step toward robust automatic hexahedral meshing since it enables the generation of a hex-meshable octahedral field.",Singularity-constrained octahedral fields for hexahedral meshing,NA:NA:NA:NA:NA,2018
Stefan Jeschke:Tomáš Skřivan:Matthias Müller-Fischer:Nuttapong Chentanez:Miles Macklin:Chris Wojtan,"The current state of the art in real-time two-dimensional water wave simulation requires developers to choose between efficient Fourier-based methods, which lack interactions with moving obstacles, and finite-difference or finite element methods, which handle environmental interactions but are significantly more expensive. This paper attempts to bridge this long-standing gap between complexity and performance, by proposing a new wave simulation method that can faithfully simulate wave interactions with moving obstacles in real time while simultaneously preserving minute details and accommodating very large simulation domains. Previous methods for simulating 2D water waves directly compute the change in height of the water surface, a strategy which imposes limitations based on the CFL condition (fast moving waves require small time steps) and Nyquist's limit (small wave details require closely-spaced simulation variables). This paper proposes a novel wavelet transformation that discretizes the liquid motion in terms of amplitude-like functions that vary over space, frequency, and direction, effectively generalizing Fourier-based methods to handle local interactions. Because these new variables change much more slowly over space than the original water height function, our change of variables drastically reduces the limitations of the CFL condition and Nyquist limit, allowing us to simulate highly detailed water waves at very large visual resolutions. Our discretization is amenable to fast summation and easy to parallelize. We also present basic extensions like pre-computed wave paths and two-way solid fluid coupling. Finally, we argue that our discretization provides a convenient set of variables for artistic manipulation, which we illustrate with a novel wave-painting interface.",Water surface wavelets,NA:NA:NA:NA:NA:NA,2018
You Xie:Erik Franz:Mengyu Chu:Nils Thuerey,"We propose a temporally coherent generative model addressing the super-resolution problem for fluid flows. Our work represents a first approach to synthesize four-dimensional physics fields with neural networks. Based on a conditional generative adversarial network that is designed for the inference of three-dimensional volumetric data, our model generates consistent and detailed results by using a novel temporal discriminator, in addition to the commonly used spatial one. Our experiments show that the generator is able to infer more realistic high-resolution details by using additional physical quantities, such as low-resolution velocities or vorticities. Besides improvements in the training process and in the generated outputs, these inputs offer means for artistic control as well. We additionally employ a physics-aware data augmentation step, which is crucial to avoid overfitting and to reduce memory requirements. In this way, our network learns to generate adverted quantities with highly detailed, realistic, and temporally coherent features. Our method works instantaneously, using only a single time-step of low-resolution fluid data. We demonstrate the abilities of our method using a variety of complex inputs and applications in two and three dimensions.","tempoGAN: a temporally coherent, volumetric GAN for super-resolution fluid flow",NA:NA:NA:NA,2018
Pingchuan Ma:Yunsheng Tian:Zherong Pan:Bo Ren:Dinesh Manocha,"We present a learning-based method to control a coupled 2D system involving both fluid and rigid bodies. Our approach is used to modify the fluid/rigid simulator's behavior by applying control forces only at the simulation domain boundaries. The rest of the domain, corresponding to the interior, is governed by the Navier-Stokes equation for fluids and Newton-Euler's equation for the rigid bodies. We represent our controller using a general neural-net, which is trained using deep reinforcement learning. Our formulation decomposes a control task into two stages: a precomputation training stage and an online generation stage. We utilize various fluid properties, e.g., the liquid's velocity field or the smoke's density field, to enhance the controller's performance. We set up our evaluation benchmark by letting controller drive fluid jets move on the domain boundary and allowing them to shoot fluids towards a rigid body to accomplish a set of challenging 2D tasks such as keeping a rigid body balanced, playing a two-player ping-pong game, and driving a rigid body to sequentially hit specified points on the wall. In practice, our approach can generate physically plausible animations.",Fluid directed rigid body control using deep reinforcement learning,NA:NA:NA:NA:NA,2018
Chenxi Liu:Enrique Rosales:Alla Sheffer,"When creating line drawings, artists frequently depict intended curves using multiple, tightly clustered, or overdrawn, strokes. Given such sketches, human observers can readily envision these intended, aggregate, curves, and mentally assemble the artist's envisioned 2D imagery. Algorithmic stroke consolidation---replacement of overdrawn stroke clusters by corresponding aggregate curves---can benefit a range of sketch processing and sketch-based modeling applications which are designed to operate on consolidated, intended curves. We propose StrokeAggregator, a novel stroke consolidation method that significantly improves on the state of the art, and produces aggregate curve drawings validated to be consistent with viewer expectations. Our framework clusters strokes into groups that jointly define intended aggregate curves by leveraging principles derived from human perception research and observation of artistic practices. We employ these principles within a coarse-to-fine clustering method that starts with an initial clustering based on pairwise stroke compatibility analysis, and then refines it by analyzing interactions both within and in-between clusters of strokes. We facilitate this analysis by computing a common 1D parameterization for groups of strokes via common aggregate curve fitting. We demonstrate our method on a large range of line drawings, and validate its ability to generate consolidated drawings that are consistent with viewer perception via qualitative user evaluation, and comparisons to manually consolidated drawings and algorithmic alternatives.",StrokeAggregator: consolidating raw sketches into artist-intended curve drawings,NA:NA:NA,2018
Edgar Simo-Serra:Satoshi Iizuka:Hiroshi Ishikawa,"We present an interactive approach for inking, which is the process of turning a pencil rough sketch into a clean line drawing. The approach, which we call the Smart Inker, consists of several ""smart"" tools that intuitively react to user input, while guided by the input rough sketch, to efficiently and naturally connect lines, erase shading, and fine-tune the line drawing output. Our approach is data-driven: the tools are based on fully convolutional networks, which we train to exploit both the user edits and inaccurate rough sketch to produce accurate line drawings, allowing high-performance interactive editing in real-time on a variety of challenging rough sketch images. For the training of the tools, we developed two key techniques: one is the creation of training data by simulation of vague and quick user edits; the other is a line normalization based on learning from vector data. These techniques, in combination with our sketch-specific data augmentation, allow us to train the tools on heterogeneous data without actual user interaction. We validate our approach with an in-depth user study, comparing it with professional illustration software, and show that our approach is able to reduce inking time by a factor of 1.8X, while improving the results of amateur users.",Real-time data-driven interactive rough sketch inking,NA:NA:NA,2018
Tiziano Portenier:Qiyang Hu:Attila Szabó:Siavash Arjomand Bigdeli:Paolo Favaro:Matthias Zwicker,"We present a novel system for sketch-based face image editing, enabling users to edit images intuitively by sketching a few strokes on a region of interest. Our interface features tools to express a desired image manipulation by providing both geometry and color constraints as user-drawn strokes. As an alternative to the direct user input, our proposed system naturally supports a copy-paste mode, which allows users to edit a given image region by using parts of another exemplar image without the need of hand-drawn sketching at all. The proposed interface runs in real-time and facilitates an interactive and iterative workflow to quickly express the intended edits. Our system is based on a novel sketch domain and a convolutional neural network trained end-to-end to automatically learn to render image regions corresponding to the input strokes. To achieve high quality and semantically consistent results we train our neural network on two simultaneous tasks, namely image completion and image translation. To the best of our knowledge, we are the first to combine these two tasks in a unified framework for interactive image editing. Our results show that the proposed sketch domain, network architecture, and training procedure generalize well to real user input and enable high quality synthesis results without additional post-processing.",Faceshop: deep sketch-based face image editing,NA:NA:NA:NA:NA:NA,2018
Guangming Zang:Ramzi Idoughi:Ran Tao:Gilles Lubineau:Peter Wonka:Wolfgang Heidrich,"X-ray computed tomography (CT) is a valuable tool for analyzing objects with interesting internal structure or complex geometries that are not accessible with optical means. Unfortunately, tomographic reconstruction of complex shapes requires a multitude (often hundreds or thousands) of projections from different viewpoints. Such a large number of projections can only be acquired in a time-sequential fashion. This significantly limits the ability to use x-ray tomography for either objects that undergo uncontrolled shape change at the time scale of a scan, or else for analyzing dynamic phenomena, where the motion itself is under investigation. In this work, we present a non-parametric space-time tomographic method for tackling such dynamic settings. Through a combination of a new CT image acquisition strategy, a space-time tomographic image formation model, and an alternating, multi-scale solver, we achieve a general approach that can be used to analyze a wide range of dynamic phenomena. We demonstrate our method with extensive experiments on both real and simulated data.",Space-time tomography for continuously deforming objects,NA:NA:NA:NA:NA:NA,2018
Peter Hedman:Johannes Kopf,"We present an algorithm for constructing 3D panoramas from a sequence of aligned color-and-depth image pairs. Such sequences can be conveniently captured using dual lens cell phone cameras that reconstruct depth maps from synchronized stereo image capture. Due to the small baseline and resulting triangulation error the depth maps are considerably degraded and contain low-frequency error, which prevents alignment using simple global transformations. We propose a novel optimization that jointly estimates the camera poses as well as spatially-varying adjustment maps that are applied to deform the depth maps and bring them into good alignment. When fusing the aligned images into a seamless mosaic we utilize a carefully designed data term and the high quality of our depth alignment to achieve two orders of magnitude speedup w.r.t. previous solutions that rely on discrete optimization by removing the need for label smoothness optimization. Our algorithm processes about one input image per second, resulting in an end-to-end runtime of about one minute for mid-sized panoramas. The final 3D panoramas are highly detailed and can be viewed with binocular and head motion parallax in VR.",Instant 3D photography,NA:NA,2018
Thomas Whelan:Michael Goesele:Steven J. Lovegrove:Julian Straub:Simon Green:Richard Szeliski:Steven Butterfield:Shobhit Verma:Richard Newcombe,"Planar reflective surfaces such as glass and mirrors are notoriously hard to reconstruct for most current 3D scanning techniques. When treated naïvely, they introduce duplicate scene structures, effectively destroying the reconstruction altogether. Our key insight is that an easy to identify structure attached to the scanner---in our case an AprilTag---can yield reliable information about the existence and the geometry of glass and mirror surfaces in a scene. We introduce a fully automatic pipeline that allows us to reconstruct the geometry and extent of planar glass and mirror surfaces while being able to distinguish between the two. Furthermore, our system can automatically segment observations of multiple reflective surfaces in a scene based on their estimated planes and locations. In the proposed setup, minimal additional hardware is needed to create high-quality results. We demonstrate this using reconstructions of several scenes with a variety of real mirrors and glass.",Reconstructing scenes with mirror and glass surfaces,NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Bojian Wu:Yang Zhou:Yiming Qian:Minglun Cong:Hui Huang,"Numerous techniques have been proposed for reconstructing 3D models for opaque objects in past decades. However, none of them can be directly applied to transparent objects. This paper presents a fully automatic approach for reconstructing complete 3D shapes of transparent objects. Through positioning an object on a turntable, its silhouettes and light refraction paths under different viewing directions are captured. Then, starting from an initial rough model generated from space carving, our algorithm progressively optimizes the model under three constraints: surface and refraction normal consistency, surface projection and silhouette consistency, and surface smoothness. Experimental results on both synthetic and real objects demonstrate that our method can successfully recover the complex shapes of transparent objects and faithfully reproduce their light refraction properties.",Full 3D reconstruction of transparent objects,NA:NA:NA:NA:NA,2018
Ligang Liu:Xi Xia:Han Sun:Qi Shen:Juzhan Xu:Bin Chen:Hui Huang:Kai Xu,"To carry out autonomous 3D scanning and online reconstruction of unknown indoor scenes, one has to find a balance between global exploration of the entire scene and local scanning of the objects within it. In this work, we propose a novel approach, which provides object-aware guidance for autoscanning, for exploring, reconstructing, and understanding an unknown scene within one navigation pass. Our approach interleaves between object analysis to identify the next best object (NBO) for global exploration, and object-aware information gain analysis to plan the next best view (NBV) for local scanning. First, an objectness-based segmentation method is introduced to extract semantic objects from the current scene surface via a multi-class graph cuts minimization. Then, an object of interest (OOI) is identified as the NBO which the robot aims to visit and scan. The robot then conducts fine scanning on the OOI with views determined by the NBV strategy. When the OOI is recognized as a full object, it can be replaced by its most similar 3D model in a shape database. The algorithm iterates until all of the objects are recognized and reconstructed in the scene. Various experiments and comparisons have shown the feasibility of our proposed approach.",Object-aware guidance for autonomous scene reconstruction,NA:NA:NA:NA:NA:NA:NA:NA,2018
Yousuf Soliman:Dejan Slepčev:Keenan Crane,"Angle-preserving or conformal surface parameterization has proven to be a powerful tool across applications ranging from geometry processing, to digital manufacturing, to machine learning, yet conformal maps can still suffer from severe area distortion. Cone singularities provide a way to mitigate this distortion, but finding the best configuration of cones is notoriously difficult. This paper develops a strategy that is globally optimal in the sense that it minimizes total area distortion among all possible cone configurations (number, placement, and size) that have no more than a fixed total cone angle. A key insight is that, for the purpose of optimization, one should not work directly with curvature measures (which naturally represent cone configurations), but can instead apply Fenchel-Rockafellar duality to obtain a formulation involving only ordinary functions. The result is a convex optimization problem, which can be solved via a sequence of sparse linear systems easily built from the usual cotangent Laplacian. The method supports user-defined notions of importance, constraints on cone angles (e.g., positive, or within a given range), and sophisticated boundary conditions (e.g., convex, or polygonal). We compare our approach to previous techniques on a variety of challenging models, often achieving dramatically lower distortion, and demonstrating that global optimality leads to extreme robustness in the presence of noise or poor discretization.",Optimal cone singularities for conformal flattening,NA:NA:NA,2018
Mina Konaković-Luković:Julian Panetta:Keenan Crane:Mark Pauly,"Deployable structures are physical mechanisms that can easily transition between two or more geometric configurations; such structures enable industrial, scientific, and consumer applications at a wide variety of scales. This paper develops novel deployable structures that can approximate a large class of doubly-curved surfaces and are easily actuated from a flat initial state via inflation or gravitational loading. The structures are based on two-dimensional rigid mechanical linkages that implicitly encode the curvature of the target shape via a user-programmable pattern that permits locally isotropic scaling under load. We explicitly characterize the shapes that can be realized by such structures---in particular, we show that they can approximate target surfaces of positive mean curvature and bounded scale distortion relative to a given reference domain. Based on this observation, we develop efficient computational design algorithms for approximating a given input geometry. The resulting designs can be rapidly manufactured via digital fabrication technologies such as laser cutting, CNC milling, or 3D printing. We validate our approach through a series of physical prototypes and present several application case studies, ranging from surgical implants to large-scale deployable architecture.",Rapid deployment of curved surfaces via programmable auxetics,NA:NA:NA:NA,2018
Chi-Han Peng:Helmut Pottmann:Peter Wonka,"We present a framework to generate mesh patterns that consist of a hybrid of both triangles and quads. Given a 3D surface, the generated patterns fit the surface boundaries and curvatures. Such regular and near regular triangle-quad hybrid meshes provide two key advantages: first, novel-looking polygonal patterns achieved by mixing different arrangements of triangles and quads together; second, a finer discretization of angle deficits than utilizing triangles or quads alone. Users have controls over the generated patterns in global and local levels. We demonstrate applications of our approach in architectural geometry and pattern design on surfaces.",Designing patterns using triangle-quad hybrid meshes,NA:NA:NA,2018
Nikunj Raghuvanshi:John Snyder,"Convincing audio for games and virtual reality requires modeling directional propagation effects. The initial sound's arrival direction is particularly salient and derives from multiply-diffracted paths in complex scenes. When source and listener straddle occluders, the initial sound and multiply-scattered reverberation stream through gaps and portals, helping the listener navigate. Geometry near the source and/or listener reveals its presence through anisotropic reflections. We propose the first precomputed wave technique to capture such directional effects in general scenes comprising millions of polygons. These effects are formally represented with the 9D directional response function of 3D source and listener location, time, and direction at the listener, making memory use the major concern. We propose a novel parametric encoder that compresses this function within a budget of ~100MB for large scenes, while capturing many salient acoustic effects indoors and outdoors. The encoder is complemented with a lightweight signal processing algorithm whose filtering cost is largely insensitive to the number of sound sources, resulting in an immediately practical system.",Parametric directional coding for precomputed sound propagation,NA:NA,2018
Jui-Hsien Wang:Ante Qu:Timothy R. Langlois:Doug L. James,"We explore an integrated approach to sound generation that supports a wide variety of physics-based simulation models and computer-animated phenomena. Targeting high-quality offline sound synthesis, we seek to resolve animation-driven sound radiation with near-field scattering and diffraction effects. The core of our approach is a sharp-interface finite-difference time-domain (FDTD) wavesolver, with a series of supporting algorithms to handle rapidly deforming and vibrating embedded interfaces arising in physics-based animation sound. Once the solver rasterizes these interfaces, it must evaluate acceleration boundary conditions (BCs) that involve model-and phenomena-specific computations. We introduce acoustic shaders as a mechanism to abstract away these complexities, and describe a variety of implementations for computer animation: near-rigid objects with ringing and acceleration noise, deformable (finite element) models such as thin shells, bubble-based water, and virtual characters. Since time-domain wave synthesis is expensive, we only simulate pressure waves in a small region about each sound source, then estimate a far-field pressure signal. To further improve scalability beyond multi-threading, we propose a fully time-parallel sound synthesis method that is demonstrated on commodity cloud computing resources. In addition to presenting results for multiple animation phenomena (water, rigid, shells, kinematic deformers, etc.) we also propose 3D automatic dialogue replacement (3DADR) for virtual characters so that pre-recorded dialogue can include character movement, and near-field shadowing and scattering sound effects.",Toward wave-based sound synthesis for computer animation,NA:NA:NA:NA,2018
Gabriel Cirio:Ante Qu:George Drettakis:Eitan Grinspun:Changxi Zheng,"Thin shells --- solids that are thin in one dimension compared to the other two --- often emit rich nonlinear sounds when struck. Strong excitations can even cause chaotic thin-shell vibrations, producing sounds whose energy spectrum diffuses from low to high frequencies over time --- a phenomenon known as wave turbulence. It is all these nonlinearities that grant shells such as cymbals and gongs their characteristic ""glinting"" sound. Yet, simulation models that efficiently capture these sound effects remain elusive. We propose a physically based, multi-scale reduced simulation method to synthesize nonlinear thin-shell sounds. We first split nonlinear vibrations into two scales, with a small low-frequency part simulated in a fully nonlinear way, and a high-frequency part containing many more modes approximated through time-varying linearization. This allows us to capture interesting nonlinearities in the shells' deformation, tens of times faster than previous approaches. Furthermore, we propose a method that enriches simulated sounds with wave turbulent sound details through a phenomenological diffusion model in the frequency domain, and thereby sidestep the expensive simulation of chaotic high-frequency dynamics. We show several examples of our simulations, illustrating the efficiency and realism of our model.",Multi-scale simulation of nonlinear thin-shell sound with wave turbulence,NA:NA:NA:NA:NA,2018
Dingzeyu Li:Timothy R. Langlois:Changxi Zheng,"Although 360° cameras ease the capture of panoramic footage, it remains challenging to add realistic 360° audio that blends into the captured scene and is synchronized with the camera motion. We present a method for adding scene-aware spatial audio to 360° videos in typical indoor scenes, using only a conventional mono-channel microphone and a speaker. We observe that the late reverberation of a room's impulse response is usually diffuse spatially and directionally. Exploiting this fact, we propose a method that synthesizes the directional impulse response between any source and listening locations by combining a synthesized early reverberation part and a measured late reverberation tail. The early reverberation is simulated using a geometric acoustic simulation and then enhanced using a frequency modulation method to capture room resonances. The late reverberation is extracted from a recorded impulse response, with a carefully chosen time duration that separates out the late reverberation from the early reverberation. In our validations, we show that our synthesized spatial audio matches closely with recordings using ambisonic microphones. Lastly, we demonstrate the strength of our method in several applications.",Scene-aware audio for 360° videos,NA:NA:NA,2018
Ariel Ephrat:Inbar Mosseri:Oran Lang:Tali Dekel:Kevin Wilson:Avinatan Hassidim:William T. Freeman:Michael Rubinstein,"We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to ""focus"" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).",Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation,NA:NA:NA:NA:NA:NA:NA:NA,2018
David B. Lindell:Matthew O'Toole:Gordon Wetzstein,"Sensors which capture 3D scene information provide useful data for tasks in vehicle navigation, gesture recognition, human pose estimation, and geometric reconstruction. Active illumination time-of-flight sensors in particular have become widely used to estimate a 3D representation of a scene. However, the maximum range, density of acquired spatial samples, and overall acquisition time of these sensors is fundamentally limited by the minimum signal required to estimate depth reliably. In this paper, we propose a data-driven method for photon-efficient 3D imaging which leverages sensor fusion and computational reconstruction to rapidly and robustly estimate a dense depth map from low photon counts. Our sensor fusion approach uses measurements of single photon arrival times from a low-resolution single-photon detector array and an intensity image from a conventional high-resolution camera. Using a multi-scale deep convolutional network, we jointly process the raw measurements from both sensors and output a high-resolution depth map. To demonstrate the efficacy of our approach, we implement a hardware prototype and show results using captured data. At low signal-to-background levels, our depth reconstruction algorithm with sensor fusion outperforms other methods for depth estimation from noisy measurements of photon arrival times.",Single-photon 3D imaging with deep sensor fusion,NA:NA:NA,2018
Vincent Sitzmann:Steven Diamond:Yifan Peng:Xiong Dun:Stephen Boyd:Wolfgang Heidrich:Felix Heide:Gordon Wetzstein,"In typical cameras the optical system is designed first; once it is fixed, the parameters in the image processing algorithm are tuned to get good image reproduction. In contrast to this sequential design approach, we consider joint optimization of an optical system (for example, the physical shape of the lens) together with the parameters of the reconstruction algorithm. We build a fully-differentiable simulation model that maps the true source image to the reconstructed one. The model includes diffractive light propagation, depth and wavelength-dependent effects, noise and nonlinearities, and the image post-processing. We jointly optimize the optical parameters and the image processing algorithm parameters so as to minimize the deviation between the true and reconstructed image, over a large set of images. We implement our joint optimization method using autodifferentiation to efficiently compute parameter gradients in a stochastic optimization algorithm. We demonstrate the efficacy of this approach by applying it to achromatic extended depth of field and snapshot super-resolution imaging.",End-to-end optimization of optics and image processing for achromatic extended depth of field and super-resolution imaging,NA:NA:NA:NA:NA:NA:NA:NA,2018
Congli Wang:Qiang Fu:Xiong Dun:Wolfgang Heidrich,"Adaptive optics has become a valuable tool for correcting minor optical aberrations in applications such as astronomy and microscopy. However, due to the limited resolution of both the wavefront sensing and the wavefront correction hardware, it has so far not been feasible to use adaptive optics for correcting large-scale waveform deformations that occur naturally in regular photography and other imaging applications. In this work, we demonstrate an adaptive optics system for regular cameras. We achieve a significant improvement in focus for large wavefront distortions by improving upon a recently developed high resolution coded wavefront sensor, and combining it with a spatial phase modulator to create a megapixel adaptive optics system with unprecedented capability to sense and correct large distortions.",Megapixel adaptive optics: towards correcting large-scale distortions in computational cameras,NA:NA:NA:NA,2018
Nanxuan Zhao:Ying Cao:Rynson W. H. Lau,"Graphic designers often manipulate the overall look and feel of their designs to convey certain personalities (e.g., cute, mysterious and romantic) to impress potential audiences and achieve business goals. However, understanding the factors that determine the personality of a design is challenging, as a graphic design is often a result of thousands of decisions on numerous factors, such as font, color, image, and layout. In this paper, we aim to answer the question of what characterizes the personality of a graphic design. To this end, we propose a deep learning framework for exploring the effects of various design factors on the perceived personalities of graphic designs. Our framework learns a convolutional neural network (called personality scoring network) to estimate the personality scores of graphic designs by ranking the crawled web data. Our personality scoring network automatically learns a visual representation that captures the semantics necessary to predict graphic design personality. With our personality scoring network, we systematically and quantitatively investigate how various design factors (e.g., color, font, and layout) affect design personality across different scales (from pixels, regions to elements). We also demonstrate a number of practical application scenarios of our network, including element-level design suggestion and example-based personality transfer.",What characterizes personalities of graphic designs?,NA:NA:NA,2018
You-En Lin:Yong-Liang Yang:Hung-Kuo Chu,"Flat design is a modern style of graphics design that minimizes the number of design attributes required to convey 3D shapes. This approach suits design contexts requiring simplicity and efficiency, such as mobile computing devices. This `less-is-more' design inspiration has posed significant challenges in practice since it selects from a restricted range of design elements (e.g., color and resolution) to represent complex shapes. In this work, we investigate a means of computationally generating a specialized 2D flat representation - image formed by black-and-white patches - from 3D shapes. We present a novel framework that automatically abstracts 3D man-made shapes into 2D binary images at multiple scales. Based on a set of identified design principles related to the inference of geometry and structure, our framework jointly analyzes the input 3D shape and its counterpart 2D representation, followed by executing a carefully devised layout optimization algorithm. The robustness and effectiveness of our method are demonstrated by testing it on a wide variety of man-made shapes and comparing the results with baseline methods via a pilot user study. We further present two practical applications that are likely to benefit from our work.",Scale-aware black-and-white abstraction of 3D shapes,NA:NA:NA,2018
Shayan Hoshyari:Edoardo Alberto Dominici:Alla Sheffer:Nathan Carr:Zhaowen Wang:Duygu Ceylan:I-Chao Shen,"Artist-drawn images with distinctly colored, piecewise continuous boundaries, which we refer to as semi-structured imagery, are very common in online raster databases and typically allow for a perceptually unambiguous mental vector interpretation. Yet, perhaps surprisingly, existing vectorization algorithms frequently fail to generate these viewer-expected interpretations on such imagery. In particular, the vectorized region boundaries they produce frequently diverge from those anticipated by viewers. We propose a new approach to region boundary vectorization that targets semi-structured inputs and leverages observations about human perception of shapes to generate vector images consistent with viewer expectations. When viewing raster imagery observers expect the vector output to be an accurate representation of the raster input. However, perception studies suggest that viewers implicitly account for the lossy nature of the rasterization process and mentally smooth and simplify the observed boundaries. Our core algorithmic challenge is to balance these conflicting cues and obtain a piecewise continuous vectorization whose discontinuities, or corners, are aligned with human expectations. Our framework centers around a simultaneous spline fitting and corner detection method that combines a learned metric, that approximates human perception of boundary discontinuities on raster inputs, with perception-driven algorithmic discontinuity analysis. The resulting method balances local cues provided by the learned metric with global cues obtained by balancing simplicity and continuity expectations. Given the finalized set of corners, our framework connects those using simple, continuous curves that capture input regularities. We demonstrate our method on a range of inputs and validate its superiority over existing alternatives via an extensive comparative user study.",Perception-driven semi-structured boundary vectorization,NA:NA:NA:NA:NA:NA:NA,2018
Stephen W. Bailey:Dave Otte:Paul Dilorenzo:James F. O'Brien,"Character rigs are procedural systems that compute the shape of an animated character for a given pose. They can be highly complex and must account for bulges, wrinkles, and other aspects of a character's appearance. When comparing film-quality character rigs with those designed for real-time applications, there is typically a substantial and readily apparent difference in the quality of the mesh deformations. Real-time rigs are limited by a computational budget and often trade realism for performance. Rigs for film do not have this same limitation, and character riggers can make the rig as complicated as necessary to achieve realistic deformations. However, increasing the rig complexity slows rig evaluation, and the animators working with it can become less efficient and may experience frustration. In this paper, we present a method to reduce the time required to compute mesh deformations for film-quality rigs, allowing better interactivity during animation authoring and use in real-time games and applications. Our approach learns the deformations from an existing rig by splitting the mesh deformation into linear and nonlinear portions. The linear deformations are computed directly from the transformations of the rig's underlying skeleton. We use deep learning methods to approximate the remaining nonlinear portion. In the examples we show from production rigs used to animate lead characters, our approach reduces the computational time spent on evaluating deformations by a factor of 5X-10X. This significant savings allows us to run the complex, film-quality rigs in real-time even when using a CPU-only implementation on a mobile device.",Fast and deep deformation approximations,NA:NA:NA:NA,2018
Jiong Chen:Hujun Bao:Tianyu Wang:Mathieu Desbrun:Jin Huang,"In this paper, an efficient and scalable approach for simulating inhomogeneous and non-linear elastic objects is introduced. Our numerical coarsening approach consists in optimizing non-conforming and matrix-valued shape functions to allow for predictive simulation of heterogeneous materials with non-linear constitutive laws even on coarse grids, thus saving orders of magnitude in computational time compared to traditional finite element computations. The set of local shape functions over coarse elements is carefully tailored in a preprocessing step to balance geometric continuity and local material stiffness. In particular, we do not impose continuity of our material-aware shape functions between neighboring elements to significantly reduce the fictitious numerical stiffness that conforming bases induce; however, we enforce crucial geometric and physical properties such as partition of unity and exact reproduction of representative fine displacements to eschew the use of discontinuous Galerkin methods. We demonstrate that we can simulate, with no parameter tuning, inhomogeneous and non-linear materials significantly better than previous approaches that traditionally try to homogenize the constitutive model instead.",Numerical coarsening using discontinuous shape functions,NA:NA:NA:NA:NA,2018
Seung-Wook Kim:Sun Young Park:Junghyun Han,"The goal of this paper is to simulate the interactions between magnetic objects in a physically correct way. The simulation scheme is based on magnetization dynamics, which describes the temporal change of magnetic moments. For magnetization dynamics, the Landau-Lifshitz-Gilbert equation is adopted, which is widely used in micromagnetics. Through effectively-designed novel models of magnets, it is extended into the macro scale so as to be combined with real-time rigid-body dynamics. The overall simulation is stable and enables us to implement mutual induction and remanence that have not been tackled by the state-of-the-art technique in magnet simulation. The proposed method can be applied to various fields including magnet experiments in the virtual world.",Magnetization dynamics for magnetic object interactions,NA:NA:NA,2018
Abe Davis:Maneesh Agrawala,"We present a visual analogue for musical rhythm derived from an analysis of motion in video, and show that alignment of visual rhythm with its musical counterpart results in the appearance of dance. Central to our work is the concept of visual beats --- patterns of motion that can be shifted in time to control visual rhythm. By warping visual beats into alignment with musical beats, we can create or manipulate the appearance of dance in video. Using this approach we demonstrate a variety of retargeting applications that control musical synchronization of audio and video: we can change what song performers are dancing to, warp irregular motion into alignment with music so that it appears to be dancing, or search collections of video for moments of accidentally dance-like motion that can be used to synthesize musical performances.",Visual rhythm and beat,NA:NA,2018
Michal Piovarči:David I. W. Levin:Danny M. Kaufman:Piotr Didyk,"Digital drawing is becoming a favorite technique for many artists. It allows for quick swaps between different materials, reverting changes, and applying selective modifications to finished artwork. These features enable artists to be more efficient and creative. A significant disadvantage of digital drawing is poor haptic feedback. Artists are usually limited to one surface and a few different stylus nibs, and while they try to find a combination that suits their needs, this is typically challenging. In this work, we address this problem and propose a method for designing, evaluating, and optimizing different stylus designs. We begin with collecting a representative set of traditional drawing tools. We measure their physical properties and conduct a user experiment to build a perceptual space that encodes perceptually-relevant attributes of drawing materials. The space is optimized to both explain our experimental data and correlate it with measurable physical properties. To embed new drawing tool designs into the space without conducting additional experiments and measurements, we propose a new, data-driven simulation technique for characterizing stylus-surface interaction. We finally leverage the perceptual space, our simulation, and recent advancements in multi-material 3D printing to demonstrate the application of our system in the design of new digital drawing tools that mimic traditional drawing materials.",Perception-aware modeling and fabrication of digital drawing tools,NA:NA:NA:NA,2018
Thijs Vogels:Fabrice Rousselle:Brian Mcwilliams:Gerhard Röthlin:Alex Harvill:David Adler:Mark Meyer:Jan Novák,"We present a modular convolutional architecture for denoising rendered images. We expand on the capabilities of kernel-predicting networks by combining them with a number of task-specific modules, and optimizing the assembly using an asymmetric loss. The source-aware encoder---the first module in the assembly---extracts low-level features and embeds them into a common feature space, enabling quick adaptation of a trained network to novel data. The spatial and temporal modules extract abstract, high-level features for kernel-based reconstruction, which is performed at three different spatial scales to reduce low-frequency artifacts. The complete network is trained using a class of asymmetric loss functions that are designed to preserve details and provide the user with a direct control over the variance-bias trade-off during inference. We also propose an error-predicting module for inferring reconstruction error maps that can be used to drive adaptive sampling. Finally, we present a theoretical analysis of convergence rates of kernel-predicting architectures, shedding light on why kernel prediction performs better than synthesizing the colors directly, complementing the empirical evidence presented in this and previous works. We demonstrate that our networks attain results that compare favorably to state-of-the-art methods in terms of detail preservation, low-frequency noise removal, and temporal stability on a variety of production and academic datasets.",Denoising with kernel prediction and asymmetric loss functions,NA:NA:NA:NA:NA:NA:NA:NA,2018
Petr Vévoda:Ivo Kondapaneni:Jaroslav Křivánek,"Direct illumination calculation is an important component of any physically-based Tenderer with a substantial impact on the overall performance. We present a novel adaptive solution for unbiased Monte Carlo direct illumination sampling, based on online learning of the light selection probability distributions. Our main contribution is a formulation of the learning process as Bayesian regression, based on a new, specifically designed statistical model of direct illumination. The net result is a set of regularization strategies to prevent over-fitting and ensure robustness even in early stages of calculation, when the observed information is sparse. The regression model captures spatial variation of illumination, which enables aggregating statistics over relatively large scene regions and, in turn, ensures a fast learning rate. We make the method scalable by adopting a light clustering strategy from the Lightcuts method, and further reduce variance through the use of control variates. As a main design feature, the resulting algorithm is virtually free of any preprocessing, which enables its use for interactive progressive rendering, while the online learning still enables super-linear convergence.",Bayesian online regression for adaptive direct illumination sampling,NA:NA:NA,2018
Zexiang Xu:Kalyan Sunkavalli:Sunil Hadap:Ravi Ramamoorthi,"We present an image-based relighting method that can synthesize scene appearance under novel, distant illumination from the visible hemisphere, from only five images captured under pre-defined directional lights. Our method uses a deep convolutional neural network to regress the relit image from these five images; this relighting network is trained on a large synthetic dataset comprised of procedurally generated shapes with real-world reflectances. We show that by combining a custom-designed sampling network with the relighting network, we can jointly learn both the optimal input light directions and the relighting function. We present an extensive evaluation of our network, including an empirical analysis of reconstruction quality, optimal lighting configurations for different scenarios, and alternative network architectures. We demonstrate, on both synthetic and real scenes, that our method is able to reproduce complex, high-frequency lighting effects like specularities and cast shadows, and outperforms other image-based relighting methods that require an order of magnitude more images.",Deep image-based relighting from optimal sparse samples,NA:NA:NA:NA,2018
Kaizhang Kang:Zimin Chen:Jiaping Wang:Kun Zhou:Hongzhi Wu,"We propose a novel framework that automatically learns the lighting patterns for efficient reflectance acquisition, as well as how to faithfully reconstruct spatially varying anisotropic BRDFs and local frames from measurements under such patterns. The core of our framework is an asymmetric deep autoencoder, consisting of a nonnegative, linear encoder which directly corresponds to the lighting patterns used in physical acquisition, and a stacked, nonlinear decoder which computationally recovers the BRDF information from captured photographs. The autoencoder is trained with a large amount of synthetic reflectance data, and can adapt to various factors, including the geometry of the setup and the properties of appearance. We demonstrate the effectiveness of our framework on a wide range of physical materials, using as few as 16 ~ 32 lighting patterns, which correspond to 12 ~ 25 seconds of acquisition time. We also validate our results with the ground truth data and captured photographs. Our framework is useful for increasing the efficiency in both novel and existing acquisition setups.",Efficient reflectance capture using an autoencoder,NA:NA:NA:NA:NA,2018
Valentin Deschaintre:Miika Aittala:Fredo Durand:George Drettakis:Adrien Bousseau,"Texture, highlights, and shading are some of many visual cues that allow humans to perceive material appearance in single pictures. Yet, recovering spatially-varying bi-directional reflectance distribution functions (SVBRDFs) from a single image based on such cues has challenged researchers in computer graphics for decades. We tackle lightweight appearance capture by training a deep neural network to automatically extract and make sense of these visual cues. Once trained, our network is capable of recovering per-pixel normal, diffuse albedo, specular albedo and specular roughness from a single picture of a flat surface lit by a hand-held flash. We achieve this goal by introducing several innovations on training data acquisition and network design. For training, we leverage a large dataset of artist-created, procedural SVBRDFs which we sample and render under multiple lighting directions. We further amplify the data by material mixing to cover a wide diversity of shading effects, which allows our network to work across many material classes. Motivated by the observation that distant regions of a material sample often offer complementary visual cues, we design a network that combines an encoder-decoder convolutional track for local feature extraction with a fully-connected track for global feature extraction and propagation. Many important material effects are view-dependent, and as such ambiguous when observed in a single image. We tackle this challenge by defining the loss as a differentiable SVBRDF similarity metric that compares the renderings of the predicted maps against renderings of the ground truth from several lighting and viewing directions. Combined together, these novel ingredients bring clear improvement over state of the art methods for single-shot capture of spatially varying BRDFs.",Single-image SVBRDF capture with a rendering-aware deep network,NA:NA:NA:NA:NA,2018
Jonàs Martínez:Samuel Hornus:Haichuan Song:Sylvain Lefebvre,"A critical advantage of additive manufacturing is its ability to fabricate complex small-scale structures. These microstructures can be understood as a metamaterial: they exist at a much smaller scale than the volume they fill, and are collectively responsible for an average elastic behavior different from that of the base printing material making the fabricated object lighter and/or flexible along specific directions. In addition, the average behavior can be graded spatially by progressively modifying the micro structure geometry. The definition of a microstructure is a careful trade-off between the geometric requirements of manufacturing and the properties one seeks to obtain within a shape: in our case a wide range of elastic behaviors. Most existing microstructures are designed for stereolithography (SLA) and laser sintering (SLS) processes. The requirements are however different than those of continuous deposition systems such as fused filament fabrication (FFF), for which there is currently a lack of microstructures enabling graded elastic behaviors. In this work we introduce a novel type of microstructures that strictly enforce all the requirements of FFF-like processes: continuity, self-support and overhang angles. They offer a range of orthotropic elastic responses that can be graded spatially. This allows to fabricate parts usually reserved to the most advanced technologies on widely available inexpensive printers that also benefit from a continuously expanding range of materials.",Polyhedral voronoi diagrams for additive manufacturing,NA:NA:NA:NA,2018
Kui Wu:Xifeng Gao:Zachary Ferguson:Daniele Panozzo:Cem Yuksel,"We introduce the first fully automatic pipeline to convert arbitrary 3D shapes into knit models. Our pipeline is based on a global parametrization remeshing pipeline to produce an isotropic quad-dominant mesh aligned with a 2-RoSy field. The knitting directions over the surface are determined using a set of custom topological operations and a two-step global optimization that minimizes the number of irregularities. The resulting mesh is converted into a valid stitch mesh that represents the knit model. The yarn curves are generated from the stitch mesh and the final yarn geometry is computed using a yarn-level relaxation process. Thus, we produce topologically valid models that can be used with a yarn-level simulation. We validate our algorithm by automatically generating knit models from complex 3D shapes and processing over a hundred models with various shapes without any user input or parameter tuning. We also demonstrate applications of our approach for custom knit model generation using fabrication via 3D printing.",Stitch meshing,NA:NA:NA:NA:NA,2018
Adriana Schulz:Harrison Wang:Eitan Grinspun:Justin Solomon:Wojciech Matusik,"Typical design for manufacturing applications requires simultaneous optimization of conflicting performance objectives: Design variations that improve one performance metric may decrease another performance metric. In these scenarios, there is no unique optimal design but rather a set of designs that are optimal for different trade-offs (called Pareto-optimal). In this work, we propose a novel approach to discover the Pareto front, allowing designers to navigate the landscape of compromises efficiently. Our approach is based on a first-order approximation of the Pareto front, which allows entire neighborhoods rather than individual points on the Pareto front to be captured. In addition to allowing for efficient discovery of the Pareto front and the corresponding mapping to the design space, this approach allows us to represent the entire trade-off manifold as a small collection of patches that comprise a high-quality and piecewise-smooth approximation. We illustrate how this technique can be used for navigating performance trade-offs in computer-aided design (CAD) models.",Interactive exploration of design trade-offs,NA:NA:NA:NA:NA,2018
Mengqi Peng:Jun Xing:Li-Yi Wei,"Digital sculpting is a popular means to create 3D models but remains a challenging task. We propose a 3D sculpting system that assists users, especially novices, in freely creating models with reduced input labor and enhanced output quality. With an interactive sculpting interface, our system silently records and analyzes users' workflows including brush strokes and camera movements, and predicts what they might do in the future. Users can accept, partially accept, or ignore the suggestions and thus retain full control and individual style. They can also explicitly select and clone past workflows over output model regions. Our key idea is to consider how a model is authored via dynamic workflows in addition to what is shaped in static geometry. This allows our method for more accurate analysis of user intentions and more general synthesis of shape structures than prior workflow or geometry methods, such as large overlapping deformations. We evaluate our method via user feedbacks and authored models.",Autocomplete 3D sculpting,NA:NA:NA,2018
Minchen Li:Alla Sheffer:Eitan Grinspun:Nicholas Vining,"While folds and pleats add interest to garments and cloth objects, incorporating them into an existing design manually or using existing software requires expertise and time. We present FoldSketch, a new system that supports simple and intuitive fold and pleat design. FoldSketch users specify the fold or pleat configuration they seek using a simple schematic sketching interface; the system then algorithmically generates both the fold-enhanced 3D garment geometry that conforms to user specifications, and the corresponding 2D patterns that reproduce this geometry within a simulation engine. While previous work aspired to compute the desired patterns for a given target 3D garment geometry, our main algorithmic challenge is that we do not have target geometry to start with. Real-life garment folds have complex profile shapes, and their exact geometry and location on a garment are intricately linked to a range of physical factors such as fabric properties and the garment's interaction with the wearer's body; it is therefore virtually impossible to predict the 3D shape of a fold-enhanced garment using purely geometric means. At the same time, using physical simulation to model folds requires appropriate 2D patterns and initial drape, neither of which can be easily provided by the user. We obtain both the 3D fold-enhanced garment and its corresponding patterns and initial drape via an alternating 2D-3D algorithm. We first expand the input patterns by allocating excess material for the expected fold formation; we then use these patterns to produce an estimated fold-enhanced drape geometry that balances designer expectations against physical reproducibility. We use the patterns and the estimated drape as input to a simulation generating an initial reproducible output. We improve the output's alignment with designer expectations by progressively refining the patterns and the estimated drape, converging to a final fully physically reproducible fold-enhanced garment. Our experiments confirm that FoldSketch reliably converges to a desired garment geometry and corresponding patterns and drape, and works well with different physical simulators. We demonstrate the versatility of our approach by showcasing a collection of garments augmented with diverse fold and pleat layouts specified via the FoldSketch interface, and further validate our approach via comparisons to alternative solutions and feedback from potential users.",Foldsketch: enriching garments with physically reproducible folds,NA:NA:NA:NA,2018
Chengkai Dai:Charlie C. L. Wang:Chenming Wu:Sylvain Lefebvre:Guoxin Fang:Yong-Jin Liu,"This paper presents a new method to fabricate 3D models on a robotic printing system equipped with multi-axis motion. Materials are accumulated inside the volume along curved tool-paths so that the need of supporting structures can be tremendously reduced - if not completely abandoned - on all models. Our strategy to tackle the challenge of tool-path planning for multi-axis 3D printing is to perform two successive decompositions, first volume-to-surfaces and then surfaces-to-curves. The volume-to-surfaces decomposition is achieved by optimizing a scalar field within the volume that represents the fabrication sequence. The field is constrained such that its iso-values represent curved layers that are supported from below, and present a convex surface affording for collision-free navigation of the printer head. After extracting all curved layers, the surfaces-to-curves decomposition covers them with tool-paths while taking into account constraints from the robotic printing system. Our method successfully generates tool-paths for 3D printing models with large overhangs and high-genus topology. We fabricated several challenging cases on our robotic platform to verify and demonstrate its capabilities.",Support-free volume printing by multi-axis motion,NA:NA:NA:NA:NA:NA,2018
Kazutaka Nakashima:Thomas Auzinger:Emmanuel Iarussi:Ran Zhang:Takeo Igarashi:Bernd Bickel,"Molding is a popular mass production method, in which the initial expenses for the mold are offset by the low per-unit production cost. However, the physical fabrication constraints of the molding technique commonly restrict the shape of moldable objects. For a complex shape, a decomposition of the object into moldable parts is a common strategy to address these constraints, with plastic model kits being a popular and illustrative example. However, conducting such a decomposition requires considerable expertise, and it depends on the technical aspects of the fabrication technique, as well as aesthetic considerations. We present an interactive technique to create such decompositions for two-piece molding, in which each part of the object is cast between two rigid mold pieces. Given the surface description of an object, we decompose its thin-shell equivalent into moldable parts by first performing a coarse decomposition and then utilizing an active contour model for the boundaries between individual parts. Formulated as an optimization problem, the movement of the contours is guided by an energy reflecting fabrication constraints to ensure the moldability of each part. Simultaneously the user is provided with editing capabilities to enforce aesthetic guidelines. Our interactive interface provides control of the contour positions by allowing, for example, the alignment of part boundaries with object features. Our technique enables a novel workflow, as it empowers novice users to explore the design space, and it generates fabrication-ready two-piece molds that can be used either for casting or industrial injection molding of free-form objects.",CoreCavity: interactive shell decomposition for fabrication with two-piece rigid molds,NA:NA:NA:NA:NA:NA,2018
Thomas Alderighi:Luigi Malomo:Daniela Giorgi:Nico Pietroni:Bernd Bickel:Paolo Cignoni,"We propose a new method for fabricating digital objects through reusable silicone molds. Molds are generated by casting liquid silicone into custom 3D printed containers called metamolds. Metamolds automatically define the cuts that are needed to extract the cast object from the silicone mold. The shape of metamolds is designed through a novel segmentation technique, which takes into account both geometric and topological constraints involved in the process of mold casting. Our technique is simple, does not require changing the shape or topology of the input objects, and only requires of-the-shelf materials and technologies. We successfully tested our method on a set of challenging examples with complex shapes and rich geometric detail.",Metamolds: computational design of silicone molds,NA:NA:NA:NA:NA:NA,2018
Haisen Zhao:Hao Zhang:Shiqing Xin:Yuanmin Deng:Changhe Tu:Wenping Wang:Daniel Cohen-Or:Baoquan Chen,"We present an automatic algorithm for subtractive manufacturing of freeform 3D objects using high-speed machining (HSM) via CNC. A CNC machine operates a cylindrical cutter to carve off material from a 3D shape stock, following a tool path, to ""expose"" the target object. Our method decomposes the input object's surface into a small number of patches each of which is fully accessible and machinable by the CNC machine, in continuous fashion, under a fixed cutter-object setup configuration. This is achieved by covering the input surface with a minimum number of accessible regions and then extracting a set of machinable patches from each accessible region. For each patch obtained, we compute a continuous, space-filling, and iso-scallop tool path which conforms to the patch boundary, enabling efficient carving with high-quality surface finishing. The tool path is generated in the form of connected Fermat spirals, which have been generalized from a 2D fill pattern for layered manufacturing to work for curved surfaces. Furthermore, we develop a novel method to control the spacing of Fermat spirals based on directional surface curvature and adapt the heat method to obtain iso-scallop carving. We demonstrate automatic generation of accessible and machinable surface decompositions and iso-scallop Fermat spiral carving paths for freeform 3D objects. Comparisons are made to tool paths generated by commercial software in terms of real machining time and surface quality.",DSCarver: decompose-and-spiral-carve for subtractive manufacturing,NA:NA:NA:NA:NA:NA:NA:NA,2018
Alex Poms:Will Crichton:Pat Hanrahan:Kayvon Fatahalian,"A growing number of visual computing applications depend on the analysis of large video collections. The challenge is that scaling applications to operate on these datasets requires efficient systems for pixel data access and parallel processing across large numbers of machines. Few programmers have the capability to operate efficiently at these scales, limiting the field's ability to explore new applications that leverage big video data. In response, we have created Scanner, a system for productive and efficient video analysis at scale. Scanner organizes video collections as tables in a data store optimized for sampling frames from compressed video, and executes pixel processing computations, expressed as dataflow graphs, on these frames. Scanner schedules video analysis applications expressed using these abstractions onto heterogeneous throughput computing hardware, such as multi-core CPUs, GPUs, and media processing ASICs, for high-throughput pixel processing. We demonstrate the productivity of Scanner by authoring a variety of video processing applications including the synthesis of stereo VR video streams from multi-camera rigs, markerless 3D human pose reconstruction from video, and data-mining big video datasets such as hundreds of feature-length films or over 70,000 hours of TV news. These applications achieve near-expert performance on a single machine and scale efficiently to hundreds of machines, enabling formerly long-running big video data analysis tasks to be carried out in minutes to hours.",Scanner: efficient video analysis at scale,NA:NA:NA:NA,2018
Tzu-Mao Li:Michaël Gharbi:Andrew Adams:Frédo Durand:Jonathan Ragan-Kelley,"Gradient-based optimization has enabled dramatic advances in computational imaging through techniques like deep learning and nonlinear optimization. These methods require gradients not just of simple mathematical functions, but of general programs which encode complex transformations of images and graphical data. Unfortunately, practitioners have traditionally been limited to either hand-deriving gradients of complex computations, or composing programs from a limited set of coarse-grained operators in deep learning frameworks. At the same time, writing programs with the level of performance needed for imaging and deep learning is prohibitively difficult for most programmers. We extend the image processing language Halide with general reverse-mode automatic differentiation (AD), and the ability to automatically optimize the implementation of gradient computations. This enables automatic computation of the gradients of arbitrary Halide programs, at high performance, with little programmer effort. A key challenge is to structure the gradient code to retain parallelism. We define a simple algorithm to automatically schedule these pipelines, and show how Halide's existing scheduling primitives can express and extend the key AD optimization of ""checkpointing."" Using this new tool, we show how to easily define new neural network layers which automatically compile to high-performance GPU implementations, and how to solve nonlinear inverse problems from computational imaging. Finally, we show how differentiable programming enables dramatically improving the quality of even traditional, feed-forward image processing algorithms, blurring the distinction between classical and deep methods.",Differentiable programming for image processing and deep learning in halide,NA:NA:NA:NA:NA,2018
Michael Kenzel:Bernhard Kerbl:Dieter Schmalstieg:Markus Steinberger,"In this paper, we present a real-time graphics pipeline implemented entirely in software on a modern GPU. As opposed to previous work, our approach features a fully-concurrent, multi-stage, streaming design with dynamic load balancing, capable of operating efficiently within bounded memory. We address issues such as primitive order, vertex reuse, and screen-space derivatives of dependent variables, which are essential to real-world applications, but have largely been ignored by comparable work in the past. The power of a software approach lies in the ability to tailor the graphics pipeline to any given application. In exploration of this potential, we design and implement four novel pipeline modifications. Evaluation of the performance of our approach on more than 100 real-world scenes collected from video games shows rendering speeds within one order of magnitude of the hardware graphics pipeline as well as significant improvements over previous work, not only in terms of capabilities and performance, but also robustness.",A high-performance software graphics pipeline architecture for the GPU,NA:NA:NA:NA,2018
Yong He:Kayvon Fatahalian:Tim Foley,"Designers of real-time rendering engines must balance the conflicting goals of maintaining clear, extensible shading systems and achieving high rendering performance. In response, engine architects have established effective design patterns for authoring shading systems, and developed engine-specific code synthesis tools, ranging from preprocessor hacking to domain-specific shading languages, to productively implement these patterns. The problem is that proprietary tools add significant complexity to modern engines, lack advanced language features, and create additional challenges for learning and adoption. We argue that the advantages of engine-specific code generation tools can be achieved using the underlying GPU shading language directly, provided the shading language is extended with a small number of best-practice principles from modern, well-established programming languages. We identify that adding generics with interface constraints, associated types, and interface/structure extensions to existing C-like GPU shading languages enables real-time Tenderer developers to build shading systems that are extensible, maintainable, and execute efficiently on modern GPUs without the need for additional domain-specific tools. We embody these ideas in an extension of HLSL called Slang, and provide a reference design for a large, extensible shader library implemented using Slang's features. We rearchitect an open source Tenderer to use this library and Slang's compiler services, and demonstrate the resulting shading system is substantially simpler, easier to extend with new features, and achieves higher rendering performance than the original HLSL-based implementation.",Slang: language mechanisms for extensible real-time shading systems,NA:NA:NA,2018
Libin Liu:Jessica Hodgins,"Basketball is one of the world's most popular sports because of the agility and speed demonstrated by the players. This agility and speed makes designing controllers to realize robust control of basketball skills a challenge for physics-based character animation. The highly dynamic behaviors and precise manipulation of the ball that occur in the game are difficult to reproduce for simulated players. In this paper, we present an approach for learning robust basketball dribbling controllers from motion capture data. Our system decouples a basketball controller into locomotion control and arm control components and learns each component separately. To achieve robust control of the ball, we develop an efficient pipeline based on trajectory optimization and deep reinforcement learning and learn non-linear arm control policies. We also present a technique for learning skills and the transition between skills simultaneously. Our system is capable of learning robust controllers for various basketball dribbling skills, such as dribbling between the legs and crossover moves. The resulting control graphs enable a simulated player to perform transitions between these skills and respond to user interaction.",Learning basketball dribbling skills using trajectory optimization and deep reinforcement learning,NA:NA,2018
Xue Bin Peng:Pieter Abbeel:Sergey Levine:Michiel van de Panne,"A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a physical simulation, thus enabling realistic responses to perturbations and environmental variation. We show that well-known reinforcement learning (RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex recoveries, adapting to changes in morphology, and accomplishing user-specified goals. Our method handles keyframed motions, highly-dynamic actions such as motion-captured flips and spins, and retargeted motions. By combining a motion-imitation objective with a task objective, we can train characters that react intelligently in interactive settings, e.g., by walking in a desired direction or throwing a ball at a user-specified target. This approach thus combines the convenience and motion quality of using motion clips to define the desired style and appearance, with the flexibility and generality afforded by RL methods and physics-based animation. We further explore a number of methods for integrating multiple clips into the learning process to develop multi-skilled agents capable of performing a rich repertoire of diverse skills. We demonstrate results using multiple characters (human, Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including locomotion, acrobatics, and martial arts.",DeepMimic: example-guided deep reinforcement learning of physics-based character skills,NA:NA:NA:NA,2018
Wenhao Yu:Greg Turk:C. Karen Liu,"Learning locomotion skills is a challenging problem. To generate realistic and smooth locomotion, existing methods use motion capture, finite state machines or morphology-specific knowledge to guide the motion generation algorithms. Deep reinforcement learning (DRL) is a promising approach for the automatic creation of locomotion control. Indeed, a standard benchmark for DRL is to automatically create a running controller for a biped character from a simple reward function [Duan et al. 2016]. Although several different DRL algorithms can successfully create a running controller, the resulting motions usually look nothing like a real runner. This paper takes a minimalist learning approach to the locomotion problem, without the use of motion examples, finite state machines, or morphology-specific knowledge. We introduce two modifications to the DRL approach that, when used together, produce locomotion behaviors that are symmetric, low-energy, and much closer to that of a real person. First, we introduce a new term to the loss function (not the reward function) that encourages symmetric actions. Second, we introduce a new curriculum learning method that provides modulated physical assistance to help the character with left/right balance and forward movement. The algorithm automatically computes appropriate assistance to the character and gradually relaxes this assistance, so that eventually the character learns to move entirely without help. Because our method does not make use of motion capture data, it can be applied to a variety of character morphologies. We demonstrate locomotion controllers for the lower half of a biped, a full humanoid, a quadruped, and a hexapod. Our results show that learned policies are able to produce symmetric, low-energy gaits. In addition, speed-appropriate gait patterns emerge without any guidance from motion examples or contact planning.",Learning symmetric and low-energy locomotion,NA:NA:NA,2018
He Zhang:Sebastian Starke:Taku Komura:Jun Saito,"Quadruped motion includes a wide variation of gaits such as walk, pace, trot and canter, and actions such as jumping, sitting, turning and idling. Applying existing data-driven character control frameworks to such data requires a significant amount of data preprocessing such as motion labeling and alignment. In this paper, we propose a novel neural network architecture called Mode-Adaptive Neural Networks for controlling quadruped characters. The system is composed of the motion prediction network and the gating network. At each frame, the motion prediction network computes the character state in the current frame given the state in the previous frame and the user-provided control signals. The gating network dynamically updates the weights of the motion prediction network by selecting and blending what we call the expert weights, each of which specializes in a particular movement. Due to the increased flexibility, the system can learn consistent expert weights across a wide range of non-periodic/periodic actions, from unstructured motion capture data, in an end-to-end fashion. In addition, the users are released from performing complex labeling of phases in different gaits. We show that this architecture is suitable for encoding the multi-modality of quadruped locomotion and synthesizing responsive motion in real-time.",Mode-adaptive neural networks for quadruped motion control,NA:NA:NA:NA,2018
Hsiao-Yu Chen:Arnav Sastry:Wim M. van Rees:Etienne Vouga,"We present a physically accurate low-order elastic shell model that incorporates active material response to dynamically changing stimuli such as heat, moisture, and growth. Our continuous formulation of the geometrically non-linear elastic energy derives from the principles of differential geometry, and as such naturally incorporates shell thickness, non-zero rest curvature, and physical material properties. By modeling the environmental stimulus as local, dynamic changes in the rest metric of the material, we are able to solve for the corresponding shape changes by integrating the equations of motions given this non-Euclidean rest state. We present models for differential growth and shrinking due to moisture and temperature gradients along and across the surface, and incorporate anisotropic growth by defining an intrinsic machine direction within the material. Comparisons with experiments and volumetric finite elements show that our simulations achieve excellent qualitative and quantitative agreement. By combining the reduced-order shell theory with appropriate physical models, our approach accurately captures all the physical phenomena while avoiding expensive volumetric discretization of the shell volume.",Physical simulation of environmentally induced thin shell deformation,NA:NA:NA:NA,2018
Qi Guo:Xuchen Han:Chuyuan Fu:Theodore Gast:Rasmus Tamstorf:Joseph Teran,"We present a novel method for simulation of thin shells with frictional contact using a combination of the Material Point Method (MPM) and subdivision finite elements. The shell kinematics are assumed to follow a continuum shell model which is decomposed into a Kirchhoff-Love motion that rotates the mid-surface normals followed by shearing and compression/extension of the material along the mid-surface normal. We use this decomposition to design an elastoplastic constitutive model to resolve frictional contact by decoupling resistance to contact and shearing from the bending resistance components of stress. We show that by resolving frictional contact with a continuum approach, our hybrid Lagrangian/Eulerian approach is capable of simulating challenging shell contact scenarios with hundreds of thousands to millions of degrees of freedom. Without the need for collision detection or resolution, our method runs in a few minutes per frame in these high resolution examples. Furthermore we show that our technique naturally couples with other traditional MPM methods for simulating granular and related materials.",A material point method for thin shells with frictional contact,NA:NA:NA:NA:NA:NA,2018
Christian Schumacher:Steve Marschner:Markus Gross:Bernhard Thomaszewski,"We propose a comprehensive approach to characterizing the mechanical properties of structured sheet materials, i.e., planar rod networks whose mechanics and aesthetics are inextricably linked. We establish a connection between the complex mesoscopic deformation behavior of such structures and their macroscopic elastic properties through numerical homogenization. Our approach leverages 3D Kirchhoff rod simulation in order to capture nonlinear effects for both in-plane and bending deformations. We apply our method to different families of structures based on isohedral tilings---a simple yet extensive and aesthetically interesting group of space-filling patterns. We show that these tilings admit a wide range of material properties, and our homogenization approach allows us to create concise and intuitive descriptions of a material's direction-dependent macromechanical behavior that are easy to communicate even to non-experts. We perform this characterization for an extensive set of structures and organize these data in a material browser to enable efficient forward exploration of the aesthetic-mechanical space of structured sheet materials. We also propose an inverse design method to automatically find structure parameters that best approximate a user-specified target behavior.",Mechanical characterization of structured sheet materials,NA:NA:NA:NA,2018
Ming Gao:Andre Pradhana:Xuchen Han:Qi Guo:Grant Kot:Eftychios Sifakis:Chenfanfu Jiang,"In this paper, we present a mixed explicit and semi-implicit Material Point Method for simulating particle-laden flows. We develop a Multigrid Preconditioned fluid solver for the Locally Averaged Navier Stokes equation. This is discretized purely on a semi-staggered standard MPM grid. Sedimentation is modeled with the Drucker-Prager elastoplasticity flow rule, enhanced by a novel particle density estimation method for converting particles between representations of either continuum or discrete points. Fluid and sediment are two-way coupled through a momentum exchange force that can be easily resolved with two MPM background grids. We present various results to demonstrate the efficacy of our method.",Animating fluid sediment mixture in particle-laden flows,NA:NA:NA:NA:NA:NA:NA,2018
Yuanming Hu:Yu Fang:Ziheng Ge:Ziyin Qu:Yixin Zhu:Andre Pradhana:Chenfanfu Jiang,"In this paper, we introduce the Moving Least Squares Material Point Method (MLS-MPM). MLS-MPM naturally leads to the formulation of Affine Particle-In-Cell (APIC) [Jiang et al. 2015] and Polynomial Particle-In-Cell [Fu et al. 2017] in a way that is consistent with a Galerkin-style weak form discretization of the governing equations. Additionally, it enables a new stress divergence discretization that effortlessly allows all MPM simulations to run two times faster than before. We also develop a Compatible Particle-In-Cell (CPIC) algorithm on top of MLS-MPM. Utilizing a colored distance field representation and a novel compatibility condition for particles and grid nodes, our framework enables the simulation of various new phenomena that are not previously supported by MPM, including material cutting, dynamic open boundaries, and two-way coupling with rigid bodies. MLS-MPM with CPIC is easy to implement and friendly to performance optimization.",A moving least squares material point method with displacement discontinuity and two-way rigid body coupling,NA:NA:NA:NA:NA:NA:NA,2018
Ruizhen Hu:Zihao Yan:Jingwen Zhang:Oliver Van Kaick:Ariel Shamir:Hao Zhang:Hui Huang,"Humans can predict the functionality of an object even without any surroundings, since their knowledge and experience would allow them to ""hallucinate"" the interaction or usage scenarios involving the object. We develop predictive and generative deep convolutional neural networks to replicate this feat. Specifically, our work focuses on functionalities of man-made 3D objects characterized by human-object or object-object interactions. Our networks are trained on a database of scene contexts, called interaction contexts, each consisting of a central object and one or more surrounding objects, that represent object functionalities. Given a 3D object in isolation, our functional similarity network (fSIM-NET), a variation of the triplet network, is trained to predict the functionality of the object by inferring functionality-revealing interaction contexts. fSIM-NET is complemented by a generative network (iGEN-NET) and a segmentation network (iSEG-NET). iGEN-NET takes a single voxelized 3D object with a functionality label and synthesizes a voxelized surround, i.e., the interaction context which visually demonstrates the corresponding functionality. iSEG-NET further separates the interacting objects into different groups according to their interaction types.",Predictive and generative neural networks for object functionality,NA:NA:NA:NA:NA:NA:NA,2018
Kangxue Yin:Hui Huang:Daniel Cohen-Or:Hao Zhang,"We introduce P2P-NET, a general-purpose deep neural network which learns geometric transformations between point-based shape representations from two domains, e.g., meso-skeletons and surfaces, partial and complete scans, etc. The architecture of the P2P-NET is that of a bi-directional point displacement network, which transforms a source point set to a prediction of the target point set with the same cardinality, and vice versa, by applying point-wise displacement vectors learned from data. P2P-NET is trained on paired shapes from the source and target domains, but without relying on point-to-point correspondences between the source and target point sets. The training loss combines two uni-directional geometric losses, each enforcing a shape-wise similarity between the predicted and the target point sets, and a cross-regularization term to encourage consistency between displacement vectors going in opposite directions. We develop and present several different applications enabled by our general-purpose bidirectional P2P-NET to highlight the effectiveness, versatility, and potential of our network in solving a variety of point-based shape transformation problems.",P2P-NET: bidirectional point displacement net for shape transform,NA:NA:NA:NA,2018
Max Limper:Nicholas Vining:ALLA SHEFFER,"Packed atlases, consisting of 2D parameterized charts, are ubiquitously used to store surface signals such as texture or normals. Tight packing is similarly used to arrange and cut-out 2D panels for fabrication from sheet materials. Packing efficiency, or the ratio between the areas of the packed atlas and its bounding box, significantly impacts downstream applications. We propose Box Cutter, a new method for optimizing packing efficiency suitable for both settings. Our algorithm improves packing efficiency without changing distortion by strategically cutting and repacking the atlas charts or panels. It preserves the local mapping between the 3D surface and the atlas charts and retains global mapping continuity across the newly formed cuts. We balance packing efficiency improvement against increase in chart boundary length and enable users to directly control the acceptable amount of boundary elongation. While the problem we address is NP-hard, we provide an effective practical solution by iteratively detecting large rectangular empty spaces, or void boxes, in the current atlas packing and eliminating them by first refining the atlas using strategically placed axis-aligned cuts and then repacking the refined charts. We repeat this process until no further improvement is possible, or until the desired balance between packing improvement and boundary elongation is achieved. Packed chart atlases are only useful for the applications we address if their charts are overlap-free; yet many popular parameterization methods, used as-is, produce atlases with global overlaps. Our pre-processing step eliminates all input overlaps while explicitly minimizing the boundary length of the resulting overlap-free charts. We demonstrate our combined strategy on a large range of input atlases produced by diverse parameterization methods, as well as on multiple sets of 2D fabrication panels. Our framework dramatically improves the output packing efficiency on all inputs; for instance with boundary length increase capped at 50% we improve packing efficiency by 68% on average.",Box cutter: atlas refinement for efficient packing via void elimination,NA:NA:NA,2018
Fabián Prada:Misha Kazhdan:Ming Chuang:Hugues Hoppe,"Processing signals on surfaces often involves resampling the signal over the vertices of a dense mesh and applying mesh-based filtering operators. We present a framework to process a signal directly in a texture atlas domain. The benefits are twofold: avoiding resampling degradation and exploiting the regularity of the texture image grid. The main challenges are to preserve continuity across atlas chart boundaries and to adapt differential operators to the non-uniform parameterization. We introduce a novel function space and multigrid solver that jointly enable robust, interactive, and geometry-aware signal processing. We demonstrate our approach using several applications including smoothing and sharpening, multiview stitching, geodesic distance computation, and line integral convolution.",Gradient-domain processing within a texture atlas,NA:NA:NA:NA,2018
Nico Schertler:Daniele Panozzo:Stefan Gumhold:Marco Tarini,"We introduce a practical pipeline to create UV T-layouts for real-world quad dominant semi-regular meshes. Our algorithm creates large rectangular patches by relaxing the notion of motorcycle graphs and making it insensitive to local irregularities in the mesh structure such as non-quad elements, redundant irregular vertices, T-junctions, and others. Each surface patch, which can contain multiple singularities and/or polygonal elements, is mapped to an axis-aligned rectangle, leading to a simple and efficient UV layout, which is ideal for texture mapping (allowing for mipmapping and artifact-free bilinear interpolation). We demonstrate that our algorithm is an ideal solution for both recent semi-regular, quad-dominant meshing methods, and for the low-poly meshes typically used in games and movies.",Generalized motorcycle graphs for imperfect quad-dominant meshes,NA:NA:NA:NA,2018
Nicholas Sharp:Keenan Crane,"This paper develops a global variational approach to cutting curved surfaces so that they can be flattened into the plane with low metric distortion. Such cuts are a critical component in a variety of algorithms that seek to parameterize surfaces over flat domains, or fabricate structures from flat materials. Rather than evaluate the quality of a cut solely based on properties of the curve itself (e.g., its length or curvature), we formulate a flow that directly optimizes the distortion induced by cutting and flattening. Notably, we do not have to explicitly parameterize the surface in order to evaluate the cost of a cut, but can instead integrate a simple evolution equation defined on the cut curve itself. We arrive at this flow via a novel application of shape derivatives to the Yamabe equation from conformal geometry. We then develop an Eulerian numerical integrator on triangulated surfaces, which does not restrict cuts to mesh edges and can incorporate user-defined data such as importance or occlusion. The resulting cut curves can be used to drive distortion to arbitrarily low levels, and have a very different character from cuts obtained via purely discrete formulations. We briefly explore potential applications to computational design, as well as connections to space filling curves and the problem of uniform heat distribution.",Variational surface cutting,NA:NA,2018
Alan Brunton:Can Ates Arikan:Tejas Madan Tanksale:Philipp Urban,"We present an efficient and scalable pipeline for fabricating full-colored objects with spatially-varying translucency from practical and accessible input data via multi-material 3D printing. Observing that the costs associated with BSSRDF measurement and processing are high, the range of 3D printable BSSRDFs are severely limited, and that the human visual system relies only on simple high-level cues to perceive translucency, we propose a method based on reproducing perceptual translucency cues. The input to our pipeline is an RGBA signal defined on the surface of an object, making our approach accessible and practical for designers. We propose a framework for extending standard color management and profiling to combined color and translucency management using a gamut correspondence strategy we call opaque relative processing. We present an efficient streaming method to compute voxel-level material arrangements, achieving both realistic reproduction of measured translucent materials and artistic effects involving multiple fully or partially transparent geometries.",3D printing spatially varying color and translucency,NA:NA:NA:NA,2018
Kaisei Sakurai:Yoshinori Dobashi:Kei Iwasaki:Tomoyuki Nishita,"A great deal of attention has been devoted to the fabrication of reflectors that can display different color images when viewed from different directions not only in industry but also for the arts. Although such reflectors have previously been successfully fabricated, the number of images displayed has been limited to two or they suffer from ghosting artifacts where mixed images appear. Furthermore, the previous methods need special hardware and/or materials to fabricate the reflectors. Thus, those techniques are not suitable for printing reflectors on everyday personal objects made of different materials, such as name cards, letter sheets, envelopes, and plastic cases. To overcome these limitations, we propose a method for fabricating reflectors using a standard ultraviolet printer (UV printer). UV printer can render a specified 2D color pattern on an arbitrary material and by overprinting the printed pattern can be raised, that is, the printed pattern becomes a microstructure having color and height. We propose using these micro structures to formulate a method for designing spatially varying reflections that can display different target images when viewed from different directions. The microstructure is calculated by minimizing an objective function that measures the differences between the intensities of the light reflected from the reflector and that of the target image. We show several fabricated reflectors to demonstrate the usefulness of the proposed method.",Fabricating reflectors for displaying multiple images,NA:NA:NA:NA,2018
Thomas Auzinger:Wolfgang Heidrich:Bernd Bickel,"Additive manufacturing has recently seen drastic improvements in resolution, making it now possible to fabricate features at scales of hundreds or even dozens of nanometers, which previously required very expensive lithographic methods. As a result, additive manufacturing now seems poised for optical applications, including those relevant to computer graphics, such as material design, as well as display and imaging applications. In this work, we explore the use of additive manufacturing for generating structural colors, where the structures are designed using a fabrication-aware optimization process. This requires a combination of full-wave simulation, a feasible parameterization of the design space, and a tailored optimization procedure. Many of these components should be re-usable for the design of other optical structures at this scale. We show initial results of material samples fabricated based on our designs. While these suffer from the prototype character of state-of-the-art fabrication hardware, we believe they clearly demonstrate the potential of additive nanofabrication for structural colors and other graphics applications.",Computational design of nanostructural color for additive manufacturing,NA:NA:NA,2018
Moritz Geilinger:Roi Poranne:Ruta Desai:Bernhard Thomaszewski:Stelian Coros,"We present a computation-driven approach to design optimization and motion synthesis for robotic creatures that locomote using arbitrary arrangements of legs and wheels. Through an intuitive interface, designers first create unique robots by combining different types of servomotors, 3D printable connectors, wheels and feet in a mix-and-match manner. With the resulting robot as input, a novel trajectory optimization formulation generates walking, rolling, gliding and skating motions. These motions emerge naturally based on the components used to design each individual robot. We exploit the particular structure of our formulation and make targeted simplifications to significantly accelerate the underlying numerical solver without compromising quality. This allows designers to interactively choreograph stable, physically-valid motions that are agile and compelling. We furthermore develop a suite of user-guided, semi-automatic, and fully-automatic optimization tools that enable motion-aware edits of the robot's physical structure. We demonstrate the efficacy of our design methodology by creating a diverse array of hybrid legged/wheeled mobile robots which we validate using physics simulation and through fabricated prototypes.",Skaterbots: optimization-based design and motion synthesis for robotic creatures with legs and wheels,NA:NA:NA:NA:NA,2018
Yang Zhou:Zhan Xu:Chris Landreth:Evangelos Kalogerakis:Subhransu Maji:Karan Singh,"We present a novel deep-learning based approach to producing animator-centric speech motion curves that drive a JALI or standard FACS-based production face-rig, directly from input audio. Our three-stage Long Short-Term Memory (LSTM) network architecture is motivated by psycho-linguistic insights: segmenting speech audio into a stream of phonetic-groups is sufficient for viseme construction; speech styles like mumbling or shouting are strongly co-related to the motion of facial landmarks; and animator style is encoded in viseme motion curve profiles. Our contribution is an automatic real-time lip-synchronization from audio solution that integrates seamlessly into existing animation pipelines. We evaluate our results by: cross-validation to ground-truth data; animator critique and edits; visual comparison to recent deep-learning lip-synchronization solutions; and showing our approach to be resilient to diversity in speaker and language.",Visemenet: audio-driven animator-centric speech animation,NA:NA:NA:NA:NA:NA,2018
Shugo Yamaguchi:Shunsuke Saito:Koki Nagano:Yajie Zhao:Weikai Chen:Kyle Olszewski:Shigeo Morishima:Hao Li,"We present a deep learning-based technique to infer high-quality facial reflectance and geometry given a single unconstrained image of the subject, which may contain partial occlusions and arbitrary illumination conditions. The reconstructed high-resolution textures, which are generated in only a few seconds, include high-resolution skin surface reflectance maps, representing both the diffuse and specular albedo, and medium- and high-frequency displacement maps, thereby allowing us to render compelling digital avatars under novel lighting conditions. To extract this data, we train our deep neural networks with a high-quality skin reflectance and geometry database created with a state-of-the-art multi-view photometric stereo system using polarized gradient illumination. Given the raw facial texture map extracted from the input image, our neural networks synthesize complete reflectance and displacement maps, as well as complete missing regions caused by occlusions. The completed textures exhibit consistent quality throughout the face due to our network architecture, which propagates texture features from the visible region, resulting in high-fidelity details that are consistent with those seen in visible regions. We describe how this highly underconstrained problem is made tractable by dividing the full inference into smaller tasks, which are addressed by dedicated neural networks. We demonstrate the effectiveness of our network design with robust texture completion from images of faces that are largely occluded. With the inferred reflectance and geometry data, we demonstrate the rendering of high-fidelity 3D avatars from a variety of subjects captured under different lighting conditions. In addition, we perform evaluations demonstrating that our method can infer plausible facial reflectance and geometric details comparable to those obtained from high-end capture devices, and outperform alternative approaches that require only a single unconstrained input image.",High-fidelity facial reflectance and geometry inference from an unconstrained image,NA:NA:NA:NA:NA:NA:NA:NA,2018
Hyeongwoo Kim:Pablo Garrido:Ayush Tewari:Weipeng Xu:Justus Thies:Matthias Niessner:Patrick Pérez:Christian Richardt:Michael Zollhöfer:Christian Theobalt,"We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted to manipulations of facial expressions only, we are the first to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye blinking from a source actor to a portrait video of a target actor. The core of our approach is a generative neural network with a novel space-time architecture. The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor. The realism in this rendering-to-video transfer is achieved by careful adversarial training, and as a result, we can create modified target videos that mimic the behavior of the synthetically-created input. In order to enable source-to-target video re-animation, we render a synthetic target video with the reconstructed head animation parameters from a source video, and feed it into the trained network - thus taking full control of the target. With the ability to freely recombine source and target parameters, we are able to demonstrate a large variety of video rewrite applications without explicitly modeling hair, body or background. For instance, we can reenact the full head using interactive user-controlled editing, and realize high-fidelity visual dubbing. To demonstrate the high quality of our output, we conduct an extensive series of experiments and evaluations, where for instance a user study shows that our video edits are hard to detect.",Deep video portraits,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Justus Thies:Michael Zollhöfer:Christian Theobalt:Marc Stamminger:Matthias Niessner,"We propose HeadOn, the first real-time source-to-target reenactment approach for complete human portrait videos that enables transfer of torso and head motion, face expression, and eye gaze. Given a short RGB-D video of the target actor, we automatically construct a personalized geometry proxy that embeds a parametric head, eye, and kinematic torso model. A novel realtime reenactment algorithm employs this proxy to photo-realistically map the captured motion from the source actor to the target actor. On top of the coarse geometric proxy, we propose a video-based rendering technique that composites the modified target portrait video via view- and pose-dependent texturing, and creates photo-realistic imagery of the target actor under novel torso and head poses, facial expressions, and gaze directions. To this end, we propose a robust tracking of the face and torso of the source actor. We extensively evaluate our approach and show significant improvements in enabling much greater flexibility in creating realistic reenacted output videos.",Headon: real-time reenactment of human portrait videos,NA:NA:NA:NA:NA,2018
Daniel Holden,"Raw optical motion capture data often includes errors such as occluded markers, mislabeled markers, and high frequency noise or jitter. Typically these errors must be fixed by hand - an extremely time-consuming and tedious task. Due to this, there is a large demand for tools or techniques which can alleviate this burden. In this research we present a tool that sidesteps this problem, and produces joint transforms directly from raw marker data (a task commonly called ""solving"") in a way that is extremely robust to errors in the input data using the machine learning technique of denoising. Starting with a set of marker configurations, and a large database of skeletal motion data such as the CMU motion capture database [CMU 2013b], we synthetically reconstruct marker locations using linear blend skinning and apply a unique noise function for corrupting this marker data - randomly removing and shifting markers to dynamically produce billions of examples of poses with errors similar to those found in real motion capture data. We then train a deep denoising feed-forward neural network to learn a mapping from this corrupted marker data to the corresponding transforms of the joints. Once trained, our neural network can be used as a replacement for the solving part of the motion capture pipeline, and, as it is very robust to errors, it completely removes the need for any manual clean-up of data. Our system is accurate enough to be used in production, generally achieving precision to within a few millimeters, while additionally being extremely fast to compute with low memory requirements.",Robust solving of optical motion capture data by denoising,NA,2018
Shangchen Han:Beibei Liu:Robert Wang:Yuting Ye:Christopher D. Twigg:Kenrick Kin,"Optical marker-based motion capture is the dominant way for obtaining high-fidelity human body animation for special effects, movies, and video games. However, motion capture has seen limited application to the human hand due to the difficulty of automatically identifying (or labeling) identical markers on self-similar fingers. We propose a technique that frames the labeling problem as a keypoint regression problem conducive to a solution using convolutional neural networks. We demonstrate robustness of our labeling solution to occlusion, ghost markers, hand shape, and even motions involving two hands or handheld objects. Our technique is equally applicable to sparse or dense marker sets and can run in real-time to support interaction prototyping with high-fidelity hand tracking and hand presence in virtual reality.",Online optical marker-based hand tracking with deep labels,NA:NA:NA:NA:NA:NA,2018
Kazuki Miyazaki:Issei Fujishiro,"Photoelasticity is known as one of the phenomena related to polarization and is defined as the change in birefringence of transparent material when internal force is applied. Interference fringes appear by irradiating the material with polarized light when viewing it through the polarizer. In this study, we attempt to apply the concept of photoelasticity to generative art. Assuming there is virtual stress distribution in the two-dimensional material, our method automatically generates artworks with photoelasticity. A GPU-based acceleration of the current implementation is also discussed.",Automatic generation of artworks using virtual photoelastic material,NA:NA,2018
Néill O'Dwyer:Nicholas Johnson:Rafael Pagés:Jan Ondřej:Konstantinos Amplianitis:Enda Bates:David Monaghan:Aljoša Smolić,"This poster describes a reinterpretation of Samuel Beckett's theatrical text Play for virtual reality (VR). It is an aesthetic reflection on practice that follows up an a technical project description submitted to ISMAR 2017 [O'Dwyer et al. 2017]. Actors are captured in a green screen environment using free-viewpoint video (FVV) techniques, and the scene is built in a game engine, complete with binaural spatial audio and six degrees of freedom of movement. The project explores how ludic qualities in the original text help elicit the conversational and interactive specificities of the digital medium. The work affirms the potential for interactive narrative in VR, opens new experiences of the text, and highlights the reorganisation of the author-audience dynamic.",Beckett in VR: exploring narrative using free viewpoint video,NA:NA:NA:NA:NA:NA:NA:NA,2018
Seungbae Bang:Sung-Hee Lee,"Among many approaches for object and character deformation, closed-form skinning methods, such as Linear Blend Skinning (LBS) and Dual Quaternion Skinning (DQS), are widely used as they are fast and intuitive. The quality of these skinning methods highly depends on specifying appropriate skinning weights to vertices, which requires the intensive efforts of professional artists in production animation.",Computation of skinning weight using spline interface,NA:NA,2018
Zihao Song:Serguei A. Mokhov:Miao Song:Sudhir P. Mudur,"Illimitable Space System (ISS) is a real-time interactive configurable toolbox for use by artists to create interactive visual effects in theatre performances and in documentaries through user inputs such as gestures and voice. Kinect has been the primary input device for motion and video data capture. In this work in addition to the existing motion based visual and geometric data processing facilities present in ISSv2, we describe our efforts to incorporate audio processing with the help of Modular Audio Recognition Framework (MARF). The combination of computer vision and audio processing to interpret both music and human motion to create imagery in real time is both artistically interesting and technically challenging. With these additional modules, ISSv2 can help interactive performance authoring that employs visual tracking and signal processing in order to create trackable human-shaped animations in real time. These new modules are incorporated into the Processing software sketchbook and language framework used by ISSv2. We verify the effects of these modules, through live demonstrations which are briefly described.",Creative use of signal processing and MARF in ISSv2 and beyond,NA:NA:NA:NA,2018
Martina R. Fröschl:Alfred Vendl,"CRISPR/Cas9-NHEJ: Action in the Nucleus (2017) is derived from an interdisciplinary creative process. This paper discusses the creation of this 210° scientific visualization, the usage of data from the worldwide Protein Data Bank, and the audio-visual presentation in an interactive dome setup. Since the topic is significant for the future of humanity, immersive experiences should be considered to convey tacit knowledge of gene-editing processes to make them approachable for the general public.",CRISPR/Cas9-NHEJ: action in the nucleus,NA:NA,2018
Kohei Ogawa:Kengo Tanaka:Tatsuya Minagawa:Yoichi Ochiai,"In this study, We propose a method to develop a spring glass dip pen by using a 3D printer and reproduce different types of writing feeling. There have been several studies on different types of pens to change the feel of writing. For example, EV-Pen [Wang et al. 2016] and haptics pens [Lee et al. 2004] changes the feel of pen writing with using vibration. However, our proposed method does not reproduce tactile sensation of softness by using vibrations.",Design method of digitally fabricated spring glass pen,NA:NA:NA:NA,2018
Christy Spangler:Eric Stolzenberg,"The National Transportation Safety Board (NTSB) is an independent agency charged with determining the probable cause of transportation accidents and promoting transportation safety. We collect a large volume of highly complex and diverse data, which is often integrated into digital illustrations to help explain the accident events, probable cause, and relevant safety issues. One major accident investigation in which digital illustrations were key was our investigation into the sinking of the cargo ship SS El Faro in 2015. (Fig. 1)","El faro: developing a digital illustration of hull wreckage 15,400 feet below the surface of the Atlantic ocean",NA:NA,2018
Richard Cottrell,"Using commercially available parts, Ephemeral Sandscaper produces complex layered landscapes by semi-randomly selecting predefined elements and sculpting them onto a material field with compelling implications for Soft Architecture.",Ephemeral sandscapes: using robotics to generate temporal landscapes,NA,2018
Tim McGraw,"Fractal shapes reflect the behavior of complex natural systems, but can be generated by simple mathematical equations. Images of 3D fractals almost exclusively depict opaque surfaces, and use reflected light and shadows to simulate a physical realization of these virtual objects. But rich inner detail can be revealed by reinterpreting the fractal as a volume and considering material transparency, light absorption and refraction. This work explores the range of images made possible by employing volume rendering techniques inspired by medical image visualization.",Fractal anatomy: imaging internal and ambient structures,NA,2018
Maria Lantin:Simon Lysander Overstall:Hongzhu Zhao,"We present a multi-user networked VR application, I Am Afraid, which uses voice as an interface to create sonic objects in a virtual environment. Words are spoken and added to the environment as three-dimensional textual objects. Other vocalizations are rendered as abstract shapes. The sculptural elements embed the sound of the voice that initiated their creation, and can be played as instruments via user-controlled interactions such as scrubbing, shaking, or looping. Multiple users can simultaneously be in the environment, mixing their voices in an evolving, dynamic, sound sculpture. I Am Afraid has been used for fun, performance, and therapeutic purposes.",I am afraid: voice as sonic sculpture,NA:NA:NA,2018
Shinji Mizuno:Yuka Oba:Nao Kotani:Yoichi Shinchi:Kenji Funahashi:Shinya Oguri:Koji Oguri:Takami Yasuda,"We introduce interactive projection mappings in a traditional Japanese house. In Japanese traditional houses, sliding doors / windows called shoji are often used. The shoji is a panel stuck with paper on the frame of the tree, and it can be used as a projector screen. We created two types of interactive projection mappings on shoji (Figure 1(a)(b)). Other characteristics of Japanese traditional houses is tatami: straw mats flooring. We also created an interactive projection mapping on tatami flooring (Figure 1(c)).",Interactive projection mappings in a Japanese traditional house,NA:NA:NA:NA:NA:NA:NA:NA,2018
Jaedong Lee:Jehee Lee,"The main goal of the crowd simulation is to generate realistic movements of agents. Reproducing the mechanism that seeing the environments, understanding current situation, and deciding where to step is crucial point to simulating crowd movements. We formulate the process of walking mechanism using deep reinforcement learning. And we experiment some typical scenarios.",Learning to move in crowd,NA:NA,2018
Steve Caruso,"Most photograph-to-oil-painting algorithms are based off of the techniques described by [Litwinowicz 1997] and improved upon by [Hertzmann 2001]. They are essentially fully-automated processes which place strokes at random, choosing stroke orientations to follow local gradient normals. These strokes are built up over several layers, each layer in a decreasing order of stroke size - painting broad strokes and then filling in details. Outside of the initial chosen parameters and some masking considerations, the user has little agency in how the algorithm chooses stroke placement, nor has the ability to make direct changes or touch-ups until every stroke is laid down on the canvas - essentially it behaves like an ""image filter"" applied as one would adjust contrast or add texture.",Painting with DEGAS: (digitally extrapolated graphics via algorithmic strokes),NA,2018
Ya-Bo Huang:Mei-Yun Chen:Ming Ouhyoung,"In the poster, we propose a model to predict the mixture of water-color pigments using convolutional neural networks (CNN). With a watercolor dataset, we train our model to minimize the loss function of sRGB differences. In metric of color difference ΔELab, our model achieves 88.7 % of data that ΔELab < 5 on the test set, which means the difference can not easily be detected by human eye. In addition, an interesting phenomenon is found; Even if the reflectance curve of the predicted color is not as smooth as the ground truth curve, the RGB color is still close to the ground truth.",Perceptual-based CNN model for watercolor mixing prediction,NA:NA:NA,2018
Yi-Lung Kao:Yu-Sheng Chen:Ming Ouhyoung,"A camera is a good instrument for measuring scene radiance. However, to please the human eye, the resulting image brightness is not linear to the scene radiance, so solving the mapping function between scene radiance and image brightness is very important. We propose a Progressive-CRF-net for radiometric calibration. By stacking multiple networks and using the pre-trained weights, this approach can reduce the training time and reach better performance than that of previous work. Our experiments show a significant improvement based on PSNR and SSIM.",Progressive-CRF-net: single image radiometric calibration using stacked CNNs,NA:NA:NA,2018
Jane Prophet:Yong Ming Kow:Mark Hurry,"Our prototype app, Pocket Penjing, built using Unity3D, takes its name from the Chinese ""Penjing."" These tray plantings of miniature trees pre-date bonsai, often including miniature benches or figures to allude to people's relationship to the tree. App users choose a species, then create and name their tree. Swiping rotates a 3D globe showing flagged locations. Each flag represents a live online air quality monitoring station data stream that the app can scrape. Data is pulled in from the selected station and the AR window loads. The AR tree grows in real-time 3D. Its L-Systems form is determined by the selected live air quality data. We used this prototype as the basis of a two-part formative participatory design workshop with 63 participants.","Small trees, big data: augmented reality model of air quality data via the chinese art of ""artificial"" tray planting",NA:NA:NA,2018
Yuka Takahashi:Tsukasa Fukusato,"This paper presents a system that aims to assist with the design of hand-sewn embroidery. With our system, a user can edit his/her design until he/she is satisfied with the simulated embroidery. We demonstrate the effectiveness of our approach, showing that visually pleasing results can be generated with minimal effort.",Stitch: an interactive design system for hand-sewn embroidery,NA:NA,2018
Predrag K. Nikolić:Hua Yang:Jyunjye Chen:George Peter Stankevich,"Project Syntropic Counterpoints has been conceptualized in the form of a series of discussions between artificial intelligence (historical persons) clones, related to topics we want to expose to AI interpretation. The project is an artist response to rising technology singularity and emerging Artificial Intelligence implementation in every aspect of everyday life which changes the social interaction landscape forever. With this project we intend to point to questions such as: Are we using AI to make humans smarter or to create a new living entity equal to us? How will this reflect on human society and its present planetary supremacy? Can we share the world and accept equality with a new AI living entity? What could be the consequences of that decision? We are also trying to point to AI limitations and to examine the cultural, creative, historical and social benefits we can gain by using AI.",Syntropic counterpoints: art of AI sense or machine made context art,NA:NA:NA:NA,2018
Águeda Simó,Stereoscopic techniques help to perceive spatial depth; hence they are used to create realistic representations of the three-dimensional world. They can also be used to manipulate spatial dimensions and create alternative spaces that challenge our understanding of visual reality. The video installation Eccentric Spaces is part of an art project that combines stereoscopic live action video with a Holobench-type display to depict alternative spaces that appear physically real.,The stereoscopic art installation eccentric spaces,NA,2018
Kyoung Lee Swearingen:Scott Swearingen,"Wall Mounted Level is a cooperative mixed-reality game that leverages multimodal interactions to support its narrative of 'reconciliation'. In it, players control their digitally projected characters and navigate them across a hand drawn physical sculpture as they collaborate towards a shared goal: finding one another. The digital and physical characteristics of the game are further reflected in the ways in which players interact with it, by making use of digital input devices and physical 'touch'. The abstract and poster discuss the design choices that were made for creating the varying modes of engagement and the motivation behind player collaboration in 'Wall Mounted Level.",Wall mounted level: a cooperative mixed reality game about reconciliation,NA:NA,2018
Gyorgy Denes:Kuba Maruszczyk:Rafał K. Mantiuk,"Increasingly higher virtual reality (VR) display resolutions and good-quality anti-aliasing make rendering in VR prohibitively expensive. The generation of these complex frames 90 times per second in a binocular setup demands substantial computational power. Wireless transmission of the frames from the GPU to the VR headset poses another challenge, requiring high-bandwidth dedicated links.",Exploiting the limitations of spatio-temporal vision for more efficient VR rendering,NA:NA:NA,2018
Alberto Badias:Iciar Alfaro:David González:Francisco Chinesta:Elías Cueto,"We present a new way of adding augmented information based on the computation of the physical equations that truly govern the behavior of objects. In computer graphics, it is common to use big simplifications to be able to solve this type of equations in real time, obtaining in many occasions behaviors that differ remarkably from reality. However, using model order reduction (MOR) techniques we are able to pre-compute a parametric solution that is only evaluated in the visualization stage, greatly reducing the computation time in this on-line phase. We present also several examples that support our method, showing computational fluid dynamics (CFD) examples and deformable solids with nonlinear material behaviors. Since it is a mixed-reality implementation, we decided to create an interactive poster that allows the visualization of augmented reality videos using augmented reality techniques, what we call (AR)2.",Improving the realism of mixed reality through physical simulation,NA:NA:NA:NA:NA,2018
Hui- Ju Chen:Zi-Xin You:Yun-Ho Yu:Jen-Ming Chen:Chia-Chun Chang:Chien-Hsing Chou,"Learning essentials of anatomy and physiology[R. Richardson et al. 2018] can make students knowing more about the connection between bones and muscles of human bodies. In the past, we can only use books, pictures, videos or fixed bone model to teach. This kind of teaching may suit for student over 15. But, for student under 15, it's hard to increase their interest or studying time for learning. If there are some models that can be assembled during the class, as Alison James[A. James et al. 2014] said, building LEGO helps us to think more about the 3D shape of the object. Can also increase student's interest of learning.",Interactive teaching aids design for essentials of anatomy and physiology: using bones and muscles as example,NA:NA:NA:NA:NA:NA,2018
Takuro Nakao:Yun Suen Pai:Megumi Isogai:Hideaki Kimata:Kai Kunze,"Current devices aim to be more hands-free by providing users with the means to interact with them using other forms of input, such as voice which can be intrusive. We propose Make-a-Face; a wearable device that allows the user to use tongue, mouth, or cheek gestures via a mask-shaped device that senses muscle movement on the lower half of the face. The significance of this approach is threefold: 1) It allows a more non-intrusive approach to interaction, 2) we designed both the hardware and software from the ground-up to accommodate the sensor electrodes and 3) we proposed several use-case scenarios ranging from smartphones to interactions with virtual reality (VR) content.","Make-a-face: a hands-free, non-intrusive device for tongue/mouth/cheek input using EMG",NA:NA:NA:NA:NA,2018
Yiming Lin:Pieter Peers:Abhijeet Ghosh,"We present a novel example-based material appearance modeling method for digital content creation. The proposed method requires a single HDR photograph of an exemplar object made of a desired material under known environmental illumination. While conventional methods for appearance modeling require the object shape to be known, our method does not require prior knowledge of the shape of the exemplar, nor does it require recovering the shape, which improves robustness as well as simplify on-site appearance acquisition by non-expert users.",On-site example-based material appearance digitization,NA:NA:NA,2018
Ping-Hsuan Han:Jia-Wei Lin:Chen-Hsin Hsieh:Jhih-Hong Hsu:Yi-Ping Hung,"In the aging society, people are paying more attention to having good exercise habits. The advancement of technology grants the possibility of learning various kinds of exercises using multi-media equipment, for example, watching instruction videos. However, it is difficult for users to learn accurate movements due to lack of feedback information.",tARget: limbs movement guidance for learning physical activities with a video see-through head-mounted display,NA:NA:NA:NA:NA,2018
Katharina Krösl:Anna Felnhofer:Johanna X. Kafka:Laura Schuster:Alexandra Rinnerthaler:Michael Wimmer:Oswald D. Kothgassner,"This work presents a virtual reality simulation for training different attentional abilities in children and adolescents. In an interdisciplinary project between psychology and computer science, we developed four mini-games that are used during therapy sessions to battle different aspects of attentional disorders. First experiments show that the immersive game-like application is well received by children. Our tool is also currently part of a treatment program in an ongoing clinical study.",The virtual schoolyard: attention training in virtual reality for children with attentional disorders,NA:NA:NA:NA:NA:NA:NA,2018
Wan-Lun Tsai:Min-Chun Hu,"Tactic training plays a crucial role in basketball offensive plays. With the aid of virtual reality, we propose a framework to improve the effectiveness and experience of tactic learning. The framework consists of a tactic input device and a wireless VR interaction system, which allows the user to conveniently input target tactic and practice in a high-fidelity circumstance. By the assistance of our VR training system, the user can vividly experience how the tactics are executed by viewing from the a specific player's viewing direction. Additionally, tactic movement guidance, action hint of how to offense aggressively, and virtual defenders are rendered in our system to make the training more realistic. By using the proposed framework, players can strengthen their tactical nous and improve the efficiency of tactic training.",Training assistant: strengthen your tactical nous with proficient virtual basketball players,NA:NA,2018
Jotaro Shigeyama:Takeru Hashimoto:Shigeo Yoshida:Taiju Aoki:Takuji Narumi:Tomohiro Tanikawa:Michitaka Hirose,"We introduce a dynamic weight moving VR controller for 2d haptic shape rendering using a haptic shape illusion. This allows users to perceive the feeling of various shapes in virtual space with a single controller. In this paper, we describe the mechanical design of prototype device that drives weight on a 2d planar area to alter mass properties of the hand-held controller. Based on the experiment, our system succeeded in providing shape perception over a wide range. We discuss limitation and further capability of our device.",Transcalibur: dynamic 2D haptic shape illusion of virtual object by weight moving VR controller,NA:NA:NA:NA:NA:NA:NA,2018
Matthew Justice:Ergun Akleman,"In this work, we present a process that use Barycentric shading method to create dynamic landscape paintings that change based on time of the day. Our process can allow creating dynamic paintings for any time of the day using simply a limited number of control paintings. To create a proof of concept, we have used landscape paintings of Edgar Payne, one of the leading landscape painters of the American West. His specific style of painting that blends Impressionism with the style of other painters of the American West is particularly appropriate for the demonstration of the power of our Barycentric shading method.",A process to create dynamic landscape paintings using barycentric shading with control paintings,NA:NA,2018
Naoki Hashiomoto:Kyosuke Hamamoto,"In the present study, we propose a new wide-viewing-angle aerial imaging display that can display aerial three-dimensional images to the surroundings. Aerial imaging has a strong visual impact, and significant efforts have been made to realize aerial imaging. In recent years, attention has been drawn to optical elements that realize retroreflective transmission, which makes it possible to easily project such images into the air. However, the viewing angle of the aerial image is narrow, and it is difficult for multiple people to simultaneously observe aerial images or to observe aerial images from all angles. Therefore, in the present study, by symmetrically arranging the mirrors at the end of the retroreflective optical transfer system, the maximum viewing angle of the aerial image is enlarged and observation from the entire circumference becomes possible.",Aerial 3D display using a symmetrical mirror structure,NA:NA,2018
Yoshiki Terashima:Kengo Fujii:Hirotsugu Yamamoto:Masaki Yasugi:Shiro Suyama:Yukihiro Takeda,"This paper proposes a novel optical system to show an aerial 3D image for a user in front of the display and to show its 2D image for the surrounding viewers. Our optics forms two-layered aerial images that are visible in a limited viewing area. Outside the viewing area, only the rear aerial 2D image is visible. The viewing area is controlled by the area of a retro-reflector in AIRR (Aerial Imaging by Retro-Reflection). The center perceives depth in the aerial screen based on DFD (Depth-Fused 3D) display.",Aerial 3D/2D composite display: depth-fused 3D for the central user and 2D for surrounding audiences,NA:NA:NA:NA:NA:NA,2018
Martin Ritz:Pedro Santos:Dieter Fellner,"We created a fully automatic system for acquisition of spatially varying optical material behavior of real object surfaces under a hemisphere of individual incident light directions. The resulting measured material model is flexibly applicable to arbitrary 3D model geometries, can be photorealistically rendered and interacted with in real-time and is not constrained to isotropic materials.",Automated acquisition and real-time rendering of spatially varying optical material behavior,NA:NA:NA,2018
Yusuke Tokuyoshi:Tomohiro Mizokuchi,"This paper presents a pipeline to accelerate frustum traced irregular z-buffers (IZBs). The IZB proposed by Wyman et al. is used to render accurate hard shadows for real-time applications such as video games, while it is expensive compared to shadow mapping. To improve the performance of hard shadows, we use a two-pass visibility test by integrating a conservative shadow map into the pipeline of the IZB. This paper also presents a more precise implementation of the conservative shadow map than the previous implementation. In our experiments for 4K screen resolution, the performance of the hard shadow computation is improved by more than double on average using the two-pass visibility test, though there is still room for optimization.",Conservative Z-prepass for frustum-traced irregular Z-buffers,NA:NA,2018
Vineet Batra:Ankit Phogat:Mridul Kavidayal,"We propose a novel and intuitive method for coloring vector graphics which is easy to use and creates richly colored artwork with very little effort. Further, it preserves the underlying geometry of the vector graphic primitives, thereby, making it easy to perform subsequent edits. Our method builds upon the concepts of shape-coverage, color and opacity and thus is applicable to all vector graphics constructs including non-convex paths and text. Furthermore, our method is highly performant and provides real-time results irrespective of the number of coloring primitives used.",General primitives for smooth coloring of vector graphics,NA:NA:NA,2018
Fei Wang:Shujin Lin:Ruomei Wang:Yi Li:Baoquan Zhao:Xiaonan Luo,"Our method shortens the time of fluid simulation by coupling the two conditions of density-invariant and divergence-free, and achieves the same simulation effect compared with other methods. Further, we regard the displacement of particles as the only basic variable of the continuity equation, which improves the stability of the fluid to a certain extent.",Improving incompressible SPH simulation efficiency by integrating density-invariant and divergence-free conditions,NA:NA:NA:NA:NA:NA,2018
Jiangyan Han:Ishtiaq Rasool Khan:Susanto Rahardja,"We propose an adaptive tone mapping method for displaying HDR images according to ambient light conditions. To compensate the loss of perceived luminance in brighter viewing conditions, we enhance the HDR image by an algorithm based on the Naka-Rushton model. Changes of the HVS response under different adaptation levels are considered and we match the response under the ambient conditions with the plateau response to the original HDR scene. The enhanced HDR image is tone mapped through a tone mapping curve constructed by the original image luminance histogram to produce visually pleasing images under given viewing conditions.",Lighting condition adaptive tone mapping method,NA:NA:NA,2018
Tobias Bertel:Christian Richardt,"Capturing 360° panoramas has become straightforward now that this functionality is implemented on every phone. However, it remains difficult to capture immersive 360° panoramas with motion parallax, which provide different views for different viewpoints. Alternatives such as omnidirectional stereo panoramas provide different views for each eye (binocular disparity), but do not support motion parallax, while Casual 3D Photography [Hedman et al. 2017] reconstructs textured 3D geometry that provides motion parallax but suffers from reconstruction artefacts. We propose a new image-based approach for capturing and rendering high-quality 360° panoramas with motion parallax. We use novel-view synthesis with flow-based blending to turn a standard monoscopic video into an enriched 360° panoramic experience that can be explored in real time. Our approach makes it possible for casual consumers to capture and view high-quality 360° panoramas with motion parallax.",MegaParallax: 360° panoramas with motion parallax,NA:NA,2018
Antoine Toisoul:Daljit Singh J. Dhillon:Abhijeet Ghosh,We present a novel approach to measure the appearance of commonly found spatially varying holographic surfaces. Such surfaces are made of one dimensional diffraction gratings that vary in orientations and periodicities over a sample to create impressive visual effects. Our method is able to recover the orientation and periodicity maps simply using a flash illumination and a DSLR camera. We present real-time renderings under environmental illumination using the measured maps that match the observed appearance.,Practical acquisition and rendering of common spatially varying holographic surfaces,NA:NA:NA,2018
Yuliya Gitlina:Daljit Singh J. Dhillon:Jan Hansen:Dinesh K. Pai:Abhijeet Ghosh,"Realistic appearance modeling of human skin is an important research topic with a variety of application in computer graphics. Various diffusion based BSSRDF models [Jensen et al. 2001, Donner and Jensen 2005, Donner and Jensen 2006] have been introduced in graphics to efficiently simulate subsurface scattering in skin including modeling its layered structure. These models however assume homogeneous subsurface scattering parameters and produce spatial color variation using an albedo map. In this work, we build upon the spectral scattering model of [Donner and Jensen 2006] and target a practical measurement-based rendering approach for such a spectral BSSRDF. The model assumes scattering in the two primary layers of skin (epidermis and dermis respectively) can be modeled with relative melanin and hemoglobin chromophore concentrations respectively. To drive this model for realistic rendering, we employ measurements of skin patches using an off-the-shelf Miravex Antera 3D camera which provides spatially varying maps of these chromophore concentrations as well as corresponding 3D surface geometry (see Figure 1) using a custom imaging setup.",Practical measurement-based spectral rendering of human skin,NA:NA:NA:NA:NA,2018
Markus Schuetz:Michael Wimmer,"Rendering tens of millions of points in real time usually requires either high-end graphics cards, or the use of spatial acceleration structures. We introduce a method to progressively display as many points as the GPU memory can hold in real time by reprojecting what was visible and randomly adding additional points to uniformly converge towards the full result within a few frames. Our method heavily limits the number of points that have to be rendered each frame and it converges quickly and in a visually pleasing way, which makes it suitable even for notebooks with low-end GPUs. The data structure consists of a randomly shuffled array of points that is incrementally generated on-the-fly while points are being loaded. Due to this, it can be used to directly view point clouds in common sequential formats such as LAS or LAZ while they are being loaded and without the need to generate spatial acceleration structures in advance, as long as the data fits into GPU memory.",Progressive real-time rendering of unprocessed point clouds,NA:NA,2018
Anastasia Feygina:Dmitry I. Ignatov:Ilya Makarov,"In this talk, we show a realistic post-processing rendering based on generative adversarial network CycleWGAN. We propose to use CycleGAN architecture and Wasserstein loss function with additional identity component in order to transfer graphics from Grand Theft Auto V to the older version of GTA video-game, Grand Theft Auto: San Andreas. We aim to present the application of modern art style transfer and unpaired image-to-image translations methods for graphics improvement using deep neural networks with adversarial loss.",Realistic post-processing of rendered 3D scenes,NA:NA:NA,2018
Yen-Chih Chiang:Shih-Song Cheng:Huei-Siou Chen:Le-Jean Wei:Li-Min Huang:David KT Chu,"Currently1, Visual Reality Head-mounted Display has several problems that need to be overcome, such as insufficient resolution of the display, latency, Vergence-accommodation Conflict, etc., while the resolution is not high enough, causing the virtual image of the display to have graininess or Screen-door Effect. These problems have brought VR users an imperfect image quality experience and are unable to achieve a good sense of immersion. Therefore, it is necessary to solve the problem of insufficient display resolution. INT TECH Co., is working towards this goal and has made very good progress.",Retinal resolution display technology brings impact to VR industry,NA:NA:NA:NA:NA:NA,2018
Keiko Nakamoto:Takafumi Koike,"We present an improved method for rendering heterogeneous translucent materials with existing BSSRDF models. In the general BSSRDF models, the optical properties of the target object are constant. Sone et al. have proposed a method to combine with existing BSSRDF models for rendering heterogeneous materials. However, the method generates more bright and blurred images compared with correctly simulated images. We have experimented with various BSSRDF models by the method and rendered heterogeneous materials. As a result, the rendered image with the better dipole model is the closest to the result of Monte carlo simulation. If incorporating the better dipole model into the method proposed by Sone et al., we can render more realistic images of heterogeneous materials.",Which BSSRDF model is better for heterogeneous materials?,NA:NA,2018
Nao Asano:Katsutoshi Masai:Yuta Sugiura:Maki Sugimoto,"Facial performance capture is used for animation production that projects a performer's facial expression to a computer graphics model. Retro-reflective markers and cameras are widely used for the performance capture. To capture expressions, we need to place markers on the performer's face and calibrate the intrinsic and extrinsic parameters of cameras in advance. However, the measurable space is limited to the calibrated area. In this study, we propose a system to capture facial performance using a smart eyewear with photo-reflective sensors and machine learning technique. Also, we show a result of principal components analysis of facial geometry to determine a good estimation parameter set.",3D facial geometry analysis and estimation using embedded optical sensors on smart eyewear,NA:NA:NA:NA,2018
Paul Canada:George Ventura:Christopher Iossa:Orquidia Moreno:William J. Joel,"Motion capture (MoCap) has been one of the leading and most useful tools within the field of animation to capture fluid and detailed motion. However, it can be quite expensive for animators, game developers and educators on a tight budgets. By using Raspberry Pi Zeros, with NoIR cameras and IR LED light rings, the cost of a four-camera system can potentially be reduced to less than 1000 USD. The research described should lead to an effective and useful system, able to detect multiple markers, record their coordinates, and keep track of them as they move. With a setup of three or more cameras, one would be able to triangulate the data on a low-cost host computer. All software and hardware designs will be disseminated open source, providing anyone who is interested in MoCap, whether it be for hobbyist, semi-professional, or educational purposes, a system for a fraction of the typical cost.",Development of an open source motion capture system,NA:NA:NA:NA:NA,2018
Kaizhang Kang:Zimin Chen:Jiaping Wang:Kun Zhou:Hongzhi Wu,"Digitally acquiring high-quality material appearance from the real-world is challenging, with applications in visual effects, e-commerce and entertainment. One popular class of existing work is based on hand-derived illumination multiplexing [Ghosh et al. 2009], using hundreds of patterns in the most general case [Chen et al. 2014].",Learning optimal lighting patterns for efficient SVBRDF acquisition,NA:NA:NA:NA:NA,2018
Yoichi Ochiai:Kazuki Otao:Yuta Itoh:Shouki Imai:Kazuki Takazawa:Hiroyuki Osone:Atsushi Mori:Ippei Suzuki,"Retinal projection is required for xR applications that can deliver immersive visual experience throughout the day. If general-purpose retinal projection methods can be realized at a low cost, not only could the image be displayed on the retina using less energy, but there is also a possibility of cutting off the weight of projection unit itself from the AR goggles. Several retinal projection methods have been previously proposed. Maxwellian optics based retinal projection was proposed in 1990s [Kollin 1993]. Laser scanning [Liao and Tsai 2009], laser projection using spatial light modulator (SLM) or holographic optical elements were also explored [Jang et al. 2017]. In the commercial field, QD Laser1 with a viewing angle of 26 degrees is available. However, as the lenses and iris of an eyeball are in front of the retina, which is a limitation of a human eyeball, the proposal of retinal projection is generally fraught with narrow viewing angles and small eyebox problems. Due to these problems, retinal projection displays are still a rare commodity because of their difficulty in optical schematics design.",Make your own retinal projector: retinal near-eye displays via metamaterials,NA:NA:NA:NA:NA:NA:NA:NA,2018
Kenta Yamamoto:Kotaro Omomo:Kazuki Takazawa:Yoichi Ochiai,"The sun is the most universal, powerful and familiar energy available on the planet. Every organism and plant has evolved over the years, corresponding to the energy brought by the sun. Humanity is no exception. We have invented many artificial lights since Edison invented light bulbs. In recent years, LEDs are one of the most representative examples. Displays and projectors using LEDs are still being actively developed. However, it is difficult to reproduce ideal light with high brightness and wide wavelength like sunlight. Furthermore, considering low energy sustainability and environmental contamination in the manufacturing process, artificial light can not surpass the sunlight. Against this backdrop, projects that utilize sunlight have been actively carried out in the world. Concentrating Solar Power (CSP) generate electricity using the heat of sunlight to turn turbines [Müller-Steinhagen and Trieb 2004]. [Koizumi 2017] is an aerial image presentation system using the sun as a light source. Digital sundials use the shadow of sunlight to inform digital time [Scharstein et al. 1996]. These projects attempt to use the direct sunlight without any conversion and minimize the energy loss.",Solar projector,NA:NA:NA:NA,2018
Simone Barbieri:Tao Jiang:Ben Cawthorne:Zhidong Xiao:Xiaosong Yang,"While 3D animation is constantly increasing its popularity, 2D is still largely in use in animation production. In fact, 2D has two main advantages. The first one is economic, as it is more rapid to produce, having a dimension less to consider. The second one is important for the artists, as 2D characters usually have highly distinctive traits, which are lost in a 3D transposition. An iconic example is Mickey Mouse, whom ears appear circular no matter which way he is facing.",3D content creation exploiting 2D character animation,NA:NA:NA:NA:NA,2018
Huiyi Fang:Kenji Funahashi,"Human eyes have an adjustment function to adjust for different distances of seeing. However, it becomes weaker as you get older. When you move paper closer to read small letters, it is not in focus. When you move it away to bring it into focus, it is too small to read. This condition is called Presbyopia. People suffering from presbyopia also suffer from this condition when they use a smartphone or tablet. Although they can magnify the display using the pinch operation, it is a bother. A method for automatic display zoom, to see detail and an overview, was proposed in [Satake et al. 2016]. This method measures the distance between a face and a screen to judge whether you want to see detail or an overview. When you move it close to your face, it judges you want to see detail and zooms in. When you move it away from your face, it judges that you want to see overview and zooms out. In this paper, we improve and apply this method for presbyopia. First we observe and analyze the behavior of presbyopic people when trying to read small letters. Then we propose a suitable zooming function, for example, a screen is zoomed in also when it is moved away if the person suffers from presbyopia.",Automatic display zoom for people suffering from Presbyopia,NA:NA,2018
Buck Barbieri:Naomi Hutchens:Kayleigh Harrison,"Massive Collaborative Animation Projects (MCAP) was founded in 2016 by Dr. William Joel (Western Connecticut State University) to test students' collaborative abilities and provide experience that will allow them to grow professionally and academically. The MCAP 1 production is a children's ghost story designed to test the massive collaborative structure. The goal of MCAP 2 is to create an animation for use in planetariums worldwide. Currently, there are nearly one hundred student contributors from universities in Alaska, California, Colorado, Connecticut, Japan, Michigan, South Korea, and Taiwan.",Collaborative animation production from students' perspective: creating short 3D CG films through international team-work,NA:NA:NA,2018
Martin Kilian:Hui Wang:Eike Schling:Jonas Schikore:Helmut Pottmann,"The computation and construction of curved beams along freeform skins pose many challenges. We show how to use surfaces of constant mean curvature (CMC) to compute beam networks with beneficial properties, both aesthetically and from a fabrication perspective. To explore variations of such networks we introduce a new discretization of CMC surfaces as quadrilateral meshes with spherical vertex stars and right node angles. The computed non-CMC surface variations can be seen as a path in design space - exploring possible solutions in a neighborhood, or represent an actual erection sequence exploiting elastic material behavior.",Curved support structures and meshes with spherical vertex stars,NA:NA:NA:NA:NA,2018
Or Fleisher:Shirin Anlen,"This paper presents Volume, a software toolkit that enables users to experiment with expressive reconstructions of archival and/or historical materials as volumetric renderings. Making use of contemporary deep learning methods, Volume re-imagines 2D images as volumetric 3D assets. These assets can then be incorporated into virtual, augmented and mixed reality experiences.",Volume: 3D reconstruction of history for immersive platforms,NA:NA,2018
Vincent Gaubert:Enki Londe:Thibaut Poittevin:Alain Lioret,We propose a new approach to 3D mesh fracturing for the fields of animation and game production. Through the use of machine learning and computer vision to analyze real fractures we produced a solution capable of creating realistic fractures in real-time.,3D-mesh cutting based on fracture photographs,NA:NA:NA:NA,2018
Hye sun Kim:Yun ji Ban:Chang joon Park,We present a technique to generate realistic high quality texture with no seams suitable to reconstruct large-scale 3D terrains. We focused on adjusting color difference caused by camera variations and illumination transition for texture reconstruction pipelines. Seams between separated processing areas should also be considered important in large terrain models. The proposed technique corrects these problems by normalizing texture colors and interpolating texture adjustment colors.,A seamless texture color adjustment method for large-scale terrain reconstruction,NA:NA:NA,2018
Kenta Yamamoto:Riku Iwasaki:Tatsuya Minagawa:Ryota Kawamura:Bektur Ryskeldiev:Yoichi Ochiai,"3D printing failures can occur without completion of printing process due to shaking, errors in printer settings, and shape of the support material and 3D model. In such case it could be difficult to restart printing process from the last printed layer in conventional 3D printers, as the printing parts to which the nozzles are supposed to be attached are lost. In order to restart printing from the middle layer, Wu et al.[Wu et al. 2017] proposed a method of printing while rotating the base of a 3D printer. However, such approach required time for two objects to bond after segmentation, with limited availability of methods for adhesion between parts. Wu et al.[Wu et al. 2016] have also proposed a method to print 3D models at any angle through 5-axis rotation of the base of a 3D printer, but the manufacturing cost of such approach was relatively high. Therefore, we propose a system that prints 3D models on existing object by utilizing an infrared depth camera. Our method makes it possible to attach a 3D-printed object into a free-formed object in the middle of printing by recognizing its shape with a depth camera.",BOLCOF: base optimization for middle layer completion of 3D-printed objects without failure,NA:NA:NA:NA:NA:NA,2018
Byungjun Kwon:Moonwon Yu:Hanyoung Jang:KyuHyun Cho:Hyundong Lee:Taesung Hahn,"This paper presents a novel motion transfer algorithm that copies content motion into a specific style character. The input consists of two motions. One is a content motion such as walking or running, and the other is movement style such as zombie or Krall. The algorithm automatically generates the synthesized motion such as walking zombie, walking Krall, running zombie, or running Krall. In order to obtain natural results, the method adopts the generative power of deep neural networks. Compared to previous neural approaches, the proposed algorithm shows better quality, runs extremely fast, does not require big data, and supports user-controllable style weights.",Deep motion transfer without big data,NA:NA:NA:NA:NA:NA,2018
Xiaodong Cun:Feng Xu:Chi-Man Pun:Hao Gao,"Synthesizing images of novel viewpoints is widely investigated in computer vision and graphics. Most works in this topic focus on using multi-view images to synthesize viewpoints in-between. In this paper, we consider extrapolation, and we take a step further to do extrapolation from one single input image. This task is very challenging for two major reasons. First, some parts of the scene may not be observed in the input viewpoint but are required for novel ones. Second, 3D information is lacking for single view input but is crucial to determine pixel movements between viewpoints. Although very challenging, we observe that human brains are always able to imagine novel viewpoints. The reason is that human brains have learned in our daily lives to understand the depth order of objects in a scene [Chen et al. 2016] and infer what the scene looks like when viewing from another viewpoint.",Depth assisted full resolution network for single image-based view synthesis,NA:NA:NA:NA,2018
Sherzod Salokhiddinov:Seungkyu Lee,"Depth estimation from differently focused set of images has been a practical approach for 3D reconstruction with existing color cameras. In this paper, we propose a depth from focus (DFF) method for accurate depth estimation using single commodity color camera. We investigate the appearance changes in spatial and frequency domain along the focused image frames in iterative manner. In order to achieve sub-frame level accuracy in depth estimation, optimal location of in-focus frame is estimated by fitting a parameterized polynomial curve on the dissimilarity measurements of each pixel. Quantitative and qualitative evaluations on various test image sets show promising performance of the proposed method in depth estimation.",Depth from focus for 3D reconstruction by iteratively building uniformly focused image set,NA:NA,2018
Chloe LeGendre:Kalle Bladin:Bipin Kishore:Xinglei Ren:Xueming Yu:Paul Debevec,"We propose a variant to polarized gradient illumination facial scanning which uses monochrome instead of color cameras to achieve more efficient and higher-resolution results. In typical polarized gradient facial scanning, sub-millimeter geometric detail is acquired by photographing the subject in eight or more polarized spherical gradient lighting conditions made with white LEDs, and RGB cameras are used to acquire color texture maps of the subject's appearance. In our approach, we replace the color cameras and white LEDs with monochrome cameras and multispectral, colored LEDs, leveraging that color images can be formed from successive monochrome images recorded under different illumination colors. While a naive extension of the scanning process to this setup would require multiplying the number of images by number of color channels, we show that the surface detail maps can be estimated directly from monochrome imagery, so that only an additional n photographs are required, where n is the number of added spectral channels. We also introduce a new multispectral optical flow approach to align images across spectral channels in the presence of slight subject motion. Lastly, for the case where a capture system's white light sources are polarized and its multispectral colored LEDs are not, we introduce the technique of multispectral polarization promotion, where we estimate the cross- and parallel-polarized monochrome images for each spectral channel from their corresponding images under a full sphere of even, unpolarized illumination. We demonstrate that this technique allows us to efficiently acquire a full color (or even multispectral) facial scan using monochrome cameras, unpolarized multispectral colored LEDs, and polarized white LEDs.",Efficient multispectral facial capture with monochrome cameras,NA:NA:NA:NA:NA:NA,2018
Nobuhiko Mukai:Taishi Nishikawa:Youngha Chang,"In this paper, we report evaluation of thin stretched thread lengths in spinnability simulations. There are many previous studies related to viscoelastic fluid, however, there are few studies that represent ""spinnability"", which is a feature that the material is stretched thin and long. Although some studies represented thread-forming property, they did not evaluate the stretched length of the material. We also tried to represent spinnability of viscoelastic fluid, however, the simulation results were not similar to a real material. Therefore, we try to perform spinnability simulations with three kinds of models, and evaluate stretched thread lengths by comparison of simulation results with a literature datum.",Evaluation of stretched thread lengths in spinnability simulations,NA:NA:NA,2018
Yuji Suzuki:Jotaro Shigeyama:Shigeo Yoshida:Takuji Narumi:Tomohiro Tanikawa:Michitaka Hirose,"Food texture plays an important role in the experience of food. Researchers have proposed various methods to manipulate the perception of food texture using auditory and physical stimulation. In this paper, we demonstrate a system to present visually modified mastication movements in real-time to manipulate the perception of food texture, because visual stimuli efficiently work to enrich other food-related perceptions and showing someone their deformed posture changes somatosensory perception. The result of our experiments suggested that adding real-time feedback of facial deformation when participants open their mouths can increase the perceived chewiness of foods. Moreover, perceptions of hardness and adhesiveness were improved when the participants saw their modified face or listened to their non-modified chewing sound, while both perceptions were decreased when participants were presented with both stimuli. These results indicate the occurrence of the contrast effect.",Food texture manipulation by face deformation,NA:NA:NA:NA:NA:NA,2018
Quan Qi:Qingde Li,"Converting a surface-based objects into a thin-surface solid representation is an essential problem for additive manufacturing. This paper proposes a simple way to thicken surfaces to thin solids based on implicit modelling technique. With the proposed technique, any surface-based object can be converted into a 3D printing friendly form that seamlessly combines both the geometric shape and its interior material structures in one single representation.",From visible to printable: thin surface with implicit interior structures,NA:NA,2018
Ivo Aluízio Stinghen Filho:Estevam Nicolas Chen:Jucimar Maia da Silva Junior:Ricardo da Silva Barboza,"In this paper we compare the effectiveness of various methods of machine learning algorithms for real-time hand gesture recognition, in order to find the most optimal way to identify static hand gestures, as well as the most optimal sample size for use during the training step of the algorithms. In our framework, Leap Motion and Unity were used to extract the data. The data was then used to be trained using Python and scikit-learn. Utilizing normalized information regarding the hands and fingers, we managed to get a hit rate of 97% using the decision tree classifier.",Gesture recognition using leap motion: a comparison between machine learning algorithms,NA:NA:NA:NA,2018
Yifan Men:Zeyu Shen:Dawar Khan:Dong-Ming Yan,"We present a novel method for valence optimization of the Centroidal Voronoi Tessellation (CVT). We first identify three commonly appeared atomic configurations of local irregular Voronoi cells, and then design specific atomic operations for each configuration to improve the regularity within the CVT framework.",Improving regularity of the centoridal voronoi tessellation,NA:NA:NA:NA,2018
Yeonho Kim:Daijin Kim,"This paper presents a dance performance evaluation how well a learner mimics the teacher's dance as follows. We estimate the human skeletons, then extract dance features such as torso and first and second-degree feature, and compute the similarity score between the teacher and the learner dance sequence in terms of timing and pose accuracies. To validate the proposed dance evaluation method, we conducted several experiments on a large K-Pop dance database. The proposed methods achieved 98% concordance with experts' evaluation on dance performance.",Interactive dance performance evaluation using timing and accuracy similarity,NA:NA,2018
Ming-Shiuan Chen:I-Chao Shen:Chun-Kai Hunag:Bing-Yu Chen,"In recent years, personalized fabrication has attracted many attentions due to the widespread of consumer-level 3D printers. However, consumer 3D printers still suffer from shortcomings such as long production time and limited output size, which are undesirable factors to large-scale rapid-prototyping. We propose a hybrid 3D fabrication method that combines 3D printing and Zometool structure for both time/cost-effective fabrication of large objects. The key of our approach is to utilize compact, sturdy and re-usable internal structure (Zometool) to infill fabrications and replace both time and material-consuming 3D-printed materials. Unlike the laser-cutted shape used in [Song et al. 2016], we are able to reuse the inner structure. As a result, we can significantly reduce the cost and time by printing thin 3D external shells only.",Large-scale fabrication with interior zometool structure,NA:NA:NA:NA,2018
Ryota Natsume:Tatsuya Yatagawa:Shigeo Morishima,"This abstract introduces a generative neural network for face swapping and editing face images. We refer to this network as ""region-separative generative adversarial network (RSGAN)"". In existing deep generative models such as Variational autoencoder (VAE) and Generative adversarial network (GAN), training data must represent what the generative models synthesize. For example, image inpainting is achieved by training images with and without holes. However, it is difficult or even impossible to prepare a dataset which includes face images both before and after face swapping because faces of real people cannot be swapped without surgical operations. We tackle this problem by training the network so that it synthesizes synthesize a natural face image from an arbitrary pair of face and hair appearances. In addition to face swapping, the proposed network can be applied to other editing applications, such as visual attribute editing and random face parts synthesis.",RSGAN: face swapping and editing using face and hair representation in latent spaces,NA:NA:NA,2018
Danny Huang:Ian Stavness,"Many thin tissues, such as leaves and flower petals, exhibit rippling and buckling patterns along their edge as they grow (Figure 1). Experiments with plastic materials have replicated the rippling patterns found in nature and shown that such patterns exhibit a fractal quality of ripples upon ripples --- a so called ""buckling cascade"" [Eran et al. 2004]. Such patterns are influenced by many physical mechanisms, including stress forces, physical properties of materials (e.g., stiffness), and space constraints [Prusinkiewicz and Barbier de Reuille 2010]. Physics-based computer animation that produces emergent rippling patterns on thin surface can improve the realism of virtual flowers and leaves, and also help to explain which physical mechanisms are most important for controlling the morphology of tissues with buckling cascades.",Simulation of emergent rippling on growing thin-shells,NA:NA,2018
Feier Cao:MHD Yamen Saraiji:Kouta Minamizawa,"Wearable technologies have been supporting and augmenting our body and sensory functions for a long time. Skin+ introduces a novel bidirectional on-skin interface that serve not only as haptic feedback to oneself but also as a visual display to mediate touch sensation to others as well. In this paper, we describe the design of Skin+ and its usability in a variety of applications. We use a shape-changing auxetic structure to build this programmable coherent visuo-tactile interface. The combination of shape-memory alloy with an auxetic structure enables a lightweight haptic device that can be worn seamlessly on top of our skin.",Skin+: programmable skin as a visuo-tactile interface,NA:NA:NA,2018
Abdelhak Saouli:Mohamed Chaouki Babahenini,"The human brain is constantly solving enormous and challenging optimization problems in vision. Due to the formidable meta-heuristics engine our brain equipped with, in addition to the widespread associative inputs from all other senses that act as the perfect initial guesses for a heuristic algorithm, the produced solutions are guaranteed to be optimal. By the same token, we address the problem of computing the depth and normal maps of a given scene under a natural but unknown illumination utilizing particle swarm optimization (PSO) to maximize a sophisticated photo-consistency function. For each output pixel, the swarm is initialized with good guesses starting with SIFT features as well as the optimal solution (depth, normal) found previously during the optimization. This leads to significantly better accuracy and robustness to textureless or quite specular surfaces.",Towards a stochastic depth maps estimation for textureless and quite specular surfaces,NA:NA,2018
Dominic Branchaud:Walter Muskovic:Maria Kavallaris:Daniel Filonik:Tomasz Bednarz,"An innovative fully interactive and ultra-high resolution navigation tool has been developed to browse and analyze gene expression levels from human cancer cells, acting as a visual microscope on data. The tool uses high-performance visualization and computer graphics technology to enable genome scientists to observe the evolution of regulatory elements across time and gain valuable insights from their dataset as never before.","Visual microscope for massive genomics datasets, expanded perception and interaction",NA:NA:NA:NA:NA,2018
Richard Clegg:Richard Hoover:Chris McLaughlin,"35 years after the release of the original ""Blade Runner"" film, the visual effects teams behind ""Blade Runner 2049"" were tasked with the challenge of crafting a dystopian world in the next phase of one of the most-beloved sci-fi films of all time. Set 30 years after the first film, the sequel follows a new blade runner as he unearths a long-buried secret that has the potential to plunge what's left of society into chaos. From the creation of the LA cityscapes, Las Vegas, and Trash Mesa environments to the development of a holographic Joi and the return of Rachael, join the filmmakers from DNEG, Framestore, and MPC as they discuss their Academy-Award winning work that paid tribute to the original picture while creating a film of the future.","DNEG, framestore, and MPC present: the visual effects of ""Blade Runner 2049""",NA:NA:NA,2018
Thomas Hullin:Isabelle Langlois,"In this production session, we will share our story of working on the legendary show, ""Game of Thrones, ""since the series' fourth season, detailing the learnings and knowledge we have gained from our multi-season experience on the groundbreaking show. We will go in depth on two of season 7's most intense sequences, starting from the concept art and working through the processes that got us to the final shots.",Game of Thrones season 7: orchestrating sea battles and blowing up a big wall,NA:NA,2018
Ian Failes:Rob Bredow:Matt Estela:Mark Hodgkins:Michael Kaschalk:Andy Hayes,"In 1996, SideFX released Houdini version 1.0, bringing the power of procedural methods to visual effects artists around the world. This year, more than two decades since Houdini's original release, SideFX was awarded a Scientific and Technical Academy Award of Merit to recognize its continual innovation and dedication to visual effects artists.",Generations of Houdini in film,NA:NA:NA:NA:NA:NA,2018
Rob Bredow:Patrick Tubach:Greg Kegel:Joseph Kasparian,Join the visual effects team as they take you behind the scenes on one of 2018's biggest films. The team will showcase the innovative shooting techniques developed for the film and the unique collaboration with Director Ron Howard that allowed this chapter in the Star Wars universe to be brought to the screen. The team will also pull back the curtain on how they took old school methodologies and combined them with cutting edge technologies to create the film's groundbreaking visual effects work.,"Making the kessel run in less than 12 parsecs: the VFX of ""Soloa: Star Wars Story""",NA:NA:NA:NA,2018
Mahyar Abousaeedi:Beth Albright:Evan Bonifacio:Chris Burrows:Gordon Cameron:Ralph Eggleston:Nathan Fariss:Fran Kalal:Paul Kanyuk:Ted Mathot:Philip Metschan:Tom Nettleship:Bret Parker:Darwyn Peachey:Reid Sandros:Rick Sayre:Stephen Schaffer:Erik Smitt:Esdras Varagnolo:Bill Watral:Bill Wise,"In a conversation that will not only span multiple disciplines, but also multiple years of technological advancement at Pixar, the team behind ""Incredibles 2"" - many of whom also worked on the first film - will compare and contrast the filmmaking process then and now. With a sequel, there's always the challenge of making a film true to the original, yet different in every detail. In building the world of ""Incredibles 2"" the team tackled one of the most technically daunting films in Pixar's canon, all while needing it to hue to the familiar tone established by the first film. Hear from this super group as they examine how they used the past to inform the present and, incredibly, achieved the near-impossible.","The Incredibles 2: suit up, it might get weird!",NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Grady Cofer:David Shirk:David Dally:Scott Meadows:Ryan Magid,"In this deep dive into Steven Spielberg's ""Ready Player One"" teams from Industrial Light & Magic and Digital Domain will showcase the break through virtual production techniques and technology deployed for the film and the visual effects involved in bringing the film's dystopian vision of life in 2045 to the screen. In addition, the teams will delve into the immense artistic and technical challenges of designing, building and animating every aspect of the expansive virtual universe known as the OASIS.","Three keys to creating the world of ""ready player one"" visual effects & virtual production",NA:NA:NA:NA:NA,2018
Koki Nagano:Jaewoo Seo:Kyle San:Aaron Hong:Mclean Goldwhite:Jun Xing:Stuti Rastogi:Jiale Kuang:Aviral Agarwal:Hanwei Kung:Caleb Arthur:Carrie Sun:Stephen Chen:Jens Fursund:Hao Li,A deep learning-based technology for generating photo-realistic 3D avatars with dynamic facial textures from a single input image is presented. Real-time performance-driven animations and renderings are demonstrated on an iPhone X and we show how these avatars can be integrated into compelling virtual worlds and used for 3D chats.,Deep learning-based photoreal avatars for online virtual worlds in iOS,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Cory Strassberger:Remco Sikkema,"Kite & Lighting reveals how Xsens inertial mocap technology, used in tandem with an iPhone X, can be used for full body and facial performance capture - wirelessly and without the need for a mocap volume - with the results live-streamed to Autodesk Maya in real time.","Democratising mocap: real-time full-performance motion capture with an iPhone X, Xsens, and Maya",NA:NA,2018
Sam Glassenberg:Matthew Yaeger,"Enter Gastro Ex for on smartphones and VR. The entire environment surrounding you is interactable and ""squishy,"" featuring advanced soft-body physics and 3D interactive fluid dynamics. Grab anything. Cut anything. Inject anywhere. Unleash argon plasma. Enjoy emergent surgical gameplay, rendered with breathtaking real-time GI and subsurface scattering.",Gastro Ex: real-time interactive fluids and soft tissues on mobile and VR,NA:NA,2018
Tobias Soffner:Christopher Baumbach,"IKEA Immerse is available in select IKEA stores in Germany. This application enables consumers to create, experience, and share their own configurations in a virtual living and kitchen room set. With seamless e-commerce integration, a high level of detail, and real-time interaction, the VR experience represents an engaging, valuable touch-point.",IKEA immerse interior designer,NA:NA,2018
Taehyun Rhee:Andrew Chalmers:Ian Loh:Ben Allen:Lohit Petikam:Stephen Thompson:Tom Revill,"An interactive mixed reality system using live streamed 360° panoramic videos is presented. A live demo for real-time image-based lighting, light detection, mixed reality rendering, and composition of 3D objects into a live-streamed 360° video of a real-world environment with dynamically changing real-world lights is shown.",Mixed reality 360 live: live blending of virtual objects into 360° streamed video,NA:NA:NA:NA:NA:NA:NA,2018
Chris Harvey:Mike Blomkamp:Isabelle Riva:Neill Blomkamp,"Come see how Oats Studios modified their traditional VFX pipeline to create the breakthrough real-time shorts ADAM Chapter 2 & 3 using Photogrammetry, Alembic, and the Unity real-time engine.","Oats studios VFX workflow for real-time production with photogrammetry, alembic, and unity",NA:NA:NA:NA,2018
Jean-Colas Prunier:Armelle Bauer:Yvain Raeymaekers:Stephane Tayeb,"PocketStudio is designed to allow filmmakers to easily create, play, and stream 3D animation sequences in real time using real-time collaborative editing, a unified workflow, and other real-time technologies, such as augmented reality.",The power of real-time collaborative filmmaking,NA:NA:NA:NA,2018
Gavin Moran:Mohen Leo,"Epic Games, Nvidia, and ILMxLAB would like to present 2018's GDC demo, ""Reflections,"" set in the ""Star Wars"" universe. In addition, we will record a character performance live using virtual production/virtual reality directly into Unreal Engine Sequencer, and then play the demo with real-time ray tracing live at 24fps.",The 'reflections' ray-tracing demo presented in real time and captured live using virtual production techniques,NA:NA,2018
Francesco Giordana:Veselin Efremov:Gael Sourimant:Silvia Rasheva:Natasha Tatarchuk:Callum James,"We demonstrate a Unity-powered virtual production platform that pushes the boundaries of real-time technologies to empower filmmakers with full multi-user collaboration and live manipulation of whole environments and characters. Special attention is dedicated to high-quality real-time graphics, as evidenced by Unity's ""Book of the Dead.""","Virtual production in 'book of the dead': technicolor's genesis platform, powered by unity",NA:NA:NA:NA:NA:NA,2018
Ayaka Ebisu:Satoshi Hashizume:Yoichi Ochiai,"A sense of rhythm is essential for playing instruments. However, many beginners learning how to play musical instruments have difficulty with rhythm. We have proposed ""Stimulated Percussions,"" which is a musical instrument performance system using electrical muscle stimulation (EMS) in the past. In this study, we apply it to the learning of rhythm. By the movement of muscles stimulated using EMS, users are able to acquire what kind of arms and legs to move at what timing. In addition to small percussion instruments such as castanets, users can play the rhythm patterns of drums that the require the simultaneous movement of their limbs.",Building a feedback loop between electrical stimulation and percussion learning,NA:NA:NA,2018
Matthew Griffin:Lizabeth Arum,"Since its release, ""The Design Engine"" has been played by groups of students, teachers, and individuals looking to spark self-guided training. ""The Design Engine"" is a direct response to educators' requests for better classroom tools surrounding inspiration and 3D printing. By prompting participants to create their own original, imaginative works-instead of using pre-selected examples-teachers can keep their students better motivated through the process of mastering desktop 3D printing. We are hosting a brand new SIGGRAPH-edition of ""The Design Engine,"" a constantly evolving series of challenges hosted within the Studio. Participants of all backgrounds can join for a short startup round, or stick around to design and develop their projects using the tools available in the SIGGRAPH Studio Workshop.",Design engine community project: generate quick adhoc inventions to explore at SIGGRAPH and in the studio,NA:NA,2018
Kengo Tanaka:Kohei Ogawa:Tatsuya Minagawa:Yoichi Ochiai,"In this study, We propose a method to develop a spring glass dip pen by using a 3D printer and reproduce different types of writing feeling. There have been several studies on different types of pens to change the feel of writing. For example, EV-Pen [Wang et al. 2016] and haptics pens [Lee et al. 2004] changes the feel of pen writing with using vibration. However, our proposed method does not reproduce tactile sensation of softness by using vibrations.",Design method of digitally fabricated spring glass pen,NA:NA:NA:NA,2018
Quentin Galvane:I-Sheng Lin:Marc Christie:Tsai-Yen Li,"Creatives in animated and real movie productions have been exploring new modalities to visually design filmic sequences before realizing them in studios, through techniques like hand-drawn storyboards, physical mockups or more recently virtual 3D environments. A central issue in using virtual 3D environments is the complexity of content creation tools for non technical film creatives. To overcome this issue, we present One Man Movie, a VR authoring system which enables the crafting of filmic sequences with no prior knowledge in 3D animation. The system is designed to reflect the traditional creative process in film pre-production through stages like (i) scene layout (ii) animation of characters, (iii) placement and control of cameras and (iv) montage of the filmic sequence, while enabling a fully novel and seamless back-and-forth between all stages of the process thanks to real-time engines. This research tool has been designed and evaluated with students and experts from film schools, and should therefore raise a significant interest among Siggraph participants.",Immersive previz: VR authoring for film previsualisation,NA:NA:NA:NA,2018
Robin Mange:Kepa Iturrioz Zabala,NA,IMVERSE livemaker: create a 3D model from a single 2D photo inside VR,NA:NA,2018
Brittany Factura:Laura LaPerche:Phil Reyneri:Brett Jones:Kevin Karsch,"Projected augmented reality, also called projection mapping or video mapping, is a form of augmented reality that uses projected light to directly augment 3D surfaces, as opposed to using pass-through screens or headsets. The value of projected AR is its ability to add a layer of digital content directly onto physical objects or environments in a way that can be instantaneously viewed by multiple people, unencumbered by a screen or additional setup.",Lightform: procedural effects for projected AR,NA:NA:NA:NA:NA,2018
Alexandra Ion:Patrick Baudisch,"In our hands-on demonstration, we show several objects, the functionality of which is defined by the objects' internal micro-structure. Such metamaterial machines can (1) be mechanisms based on their microstructures, (2) employ simple mechanical computation, or (3) change their outside to interact with their environment. They are 3D printed from one piece and we support their creating by providing interactive software tools.",Metamaterial devices,NA:NA,2018
Wataru Date:Yasuaki Kakehi,"In this research, we propose a system which makes paper through additive manufacturing process by using a dispenser mounted on XY plotter. By using our system, graphic designers can design and output paper itself which is hard in an existing paper production process. This time, we designed and implemented a machine for fabricating paper and created several output examples. In SIGGRAPH, we will provide a workshop for participants to design their original paper using our machines.",Paperprinting: a machine for prototyping paper and its applications for graphic design,NA:NA,2018
Kevin Watters:Fernando Ramallo,"Raymarching signed distance fields is a technique used by graphics experts and demoscene enthusiasts to construct scenes with features unusual in traditional polygonal workflows-blending shapes, kaleidoscopic patterns, reflections, and infinite fractal detail all become possible and are represented in compact representations that live mostly on the graphics card. Until now these scenes have had to be constructed in shaders by hand, but the Raymarching Toolkit for Unity is an extension that combines Unity's highly visual scene editor with the power of raymarched visuals by automatically generating the raymarching shader for the scene an artist is creating, live.",Raymarching toolkit for unity: a highly interactive unity toolkit for constructing signed distance fields visually,NA:NA,2018
