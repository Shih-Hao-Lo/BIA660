Column1,Column2,Column3,Column4,Column5
Gianluca Stringhini:Jeff Z. Pan,"It is our great pleasure to welcome you to the Poster Track, associated with WWW 2016. The poster track is a forum to foster interactions among researchers and practitioners, by allowing them to present their new and innovative work in-progress. By presenting their ideas to the WWW 2016, researchers will have a chance to collect feedback from the WWW community and start fruitful conversations. We have a wide variety of topics in the poster track, which cover many topics of interest to the WWW community. We hope that you will enjoy attending the track, and you will find it useful for your future research. The call for papers attracted submissions from United States and Europe. The program committee reviewed and accepted the following: Full Technical Papers Reviewed 182 Accepted 72.",Session details: Posters,NA:NA,2016
Behnoush Abdollahi:Olfa Nasraoui,"Explanations have been shown to increase the user's trust in recommendations in addition to providing other benefits such as scrutability, which is the ability to verify the validity of recommendations. Most explanation methods are designed for classical neighborhood-based Collaborative Filtering (CF) or rule-based methods. For the state of the art Matrix Factorization (MF) recommender systems, recent explanation methods, require an additional data source, such as item content data, in addition to rating data. In this paper, we address the case where no such additional data is available and propose a new Explainable Matrix Factorization (EMF) technique that computes an accurate top-$n$ recommendation list of items that are explainable. We also introduce new explanation quality metrics, that we call Mean Explainability Precision (MEP) and Mean Explainability Recall (MER).",Explainable Matrix Factorization for Collaborative Filtering,NA:NA,2016
Medha Atre,"Evaluating SPARQL queries with the DISTINCT clause may become memory intensive due to the requirement of additional auxiliary data structures, like hash-maps, to discard the duplicates. DISTINCT queries make up to 16% of all the queries (e.g., DBPedia), and thus are non-negligible. In this poster we propose a novel method for such queries, by just manipulating the compressed bit-vector indexes called BitMats, for acyclic basic graph pattern (BGP) queries.",For the DISTINCT Clause of SPARQL Queries,NA,2016
Xiaomei Bai:Jun Zhang:Hai Cui:Zhaolong Ning:Feng Xia,"Evaluating the impact of an article is a significant topic and has attracted extensive attention. Citation-based assessment methods currently face a limitation, i.e. the anomalous citations patterns still remain poorly understand. To remedy this drawback, we propose a Positive and Negative Conflict of Interest (COI)-based Rank algorithm, named PNCOIRank, to acquire positive COI, negative COI, positive suspected COI and negative suspected COI relationships. We investigate the citation relationships by the following scholarly factors: citing times, the interval of citing time, collaboration times, the interval of collaboration time, and team of citing authors with the purpose of weakening the COI relationships in citation network. A weighted PageRank is finally constructed and employed, with HITS algorithm to assess the impact of articles. Through experiments on American Physical Society (APS) dataset, we show that PNCOIRank significantly outperforms the existing methods in terms of recommendation intensity.",PNCOIRank: Evaluating the Impact of Scholarly Articles with Positive and Negative Citations,NA:NA:NA:NA:NA,2016
Rajesh Basak:Niloy Ganguly:Shamik Sural:Soumya K. Ghosh,"Online social networks (OSNs) are often flooded with scathing remarks against individuals or businesses on their perceived wrongdoing. This paper studies three such events to get insight into various aspects of shaming done through twitter. An important contribution of our work is categorization of shaming tweets, which helps in understanding the dynamics of spread of online shaming events. It also facilitates automated segregation of shaming tweets from non-shaming ones.",Look Before You Shame: A Study on Shaming Activities on Twitter,NA:NA:NA:NA,2016
Amna Basharat:I. Budak Arpinar:Khaled Rasheed,"In this paper, we illustrate how we leverage crowdsourcing to create workflows for knowledge engineering in specialized and knowledge intensive domains. We undertake the special case of the Arabic script of the Qur'an, a widely studied manuscript, and attempt to employ crowdsourcing methods for its thematic annotation at the sub-verse level, for which, there is no standardized knowledge model available to date. We demonstrate that our proposed method presents feasibility to achieve reliable annotations in an efficient and scalable manner. The proposed methodology and framework is meant to be generalizable to other knowledge intensive and specialized domains.",Leveraging Crowdsourcing for the Thematic Annotation of the Qur'an,NA:NA:NA,2016
Murat Ali Bayir:Ismail Hakki Toroslu,"This paper introduces a new method for the session construction problem, which is the first main step of the web usage mining process. The proposed method is capable of extracting all possible maximal navigation sequences of web users. Through experiments, it is shown that when our new technique is used, it outperforms previous approaches in web usage mining applications such as next-page prediction.",Finding All Maximal Paths In Web User Sessions,NA:NA,2016
Martin Becker:Hauke Mewes:Andreas Hotho:Dimitar Dimitrov:Florian Lemmerich:Markus Strohmaier,"HypTrails is a bayesian approach for comparing different hypotheses about human trails on the web. While a standard implementation exists, it exposes performance issues when working with large-scale data. In this paper, we propose a distributed implementation of HypTrails based on Apache Spark taking advantage of several structural properties inherent to HypTrails. The performance improves substantially. Our implementation is publicly available.",SparkTrails: A MapReduce Implementation of HypTrails for Comparing Hypotheses About Human Trails,NA:NA:NA:NA:NA:NA,2016
Atef Chaudhury:Myunghwan Kim:Mitul Tiwari,"In both the online and offline world, networks that people form within certain communities are critical for their engagement and growth in those communities. In this work, we analyze the growth of ego-networks on LinkedIn for new employees of companies, and study how the pattern of network formation in the new company affects one's growth and engagement in that company. We observe that the initial state of ego-network growth in a newly joined company shows strong correlations with the future status in the company -- such as network size, network diversity, and retention. We also present some key patterns that demonstrate the importance of the first few connections in the new company as well as how they lead to the phenomena we observed.","Importance of First Steps in a Community for Growth, Diversity, and Engagement",NA:NA:NA,2016
Xu Chen:Pengfei Wang:Zheng Qin:Yongfeng Zhang,"Bayesian Personal Ranking(BPR) method is a well-known model due to its high performance in the task of item recommendation. However, this method fail to distinguish user preference among the non-interacted items. In this paper, to enhance traditional BPR's performance, we introduce and analyse a hybrid method, namely Hybrid Local Bayesian Personal Ranking method(HLBPR for short). Our main idea is to construct additional item preference pairs among the products which haven't been purchased, and then utilize the extened pairs to optimize the ranking object. Experiments on two real-world transaction datasets demonstrated the effectiveness of our approach as compared with the state-of-the-art methods.",HLBPR: A Hybrid Local Bayesian Personal Ranking Method,NA:NA:NA:NA,2016
Hélène de Ribaupierre:Francesco Osborne:Enrico Motta,"The natural language processing (NLP) community has developed a variety of methods for extracting and disambiguating information from research publications. However, they usually focus only on standard research entities such as authors, affiliations, venues, references and keywords. We propose a novel approach, which combines NLP and semantic technologies for generating from the text of research publications an OWL ontology describing software technologies used or introduced by researchers, such as applications, systems, frameworks, programming languages, and formats. The method was tested on a sample of 300 publications in the Semantic Web field, yielding promising results.",Combining NLP And Semantics For Mining Software Technologies From Research Publications,NA:NA:NA,2016
Angelo Di Iorio:Alejandra Gonzalez-Beltran:Francesco Osborne:Silvio Peroni:Francesco Poggi:Fabio Vitali,"In this poster paper we introduce the RASH Online Conversion Service, i.e., a Web application that allows the conversion of ODT documents into RASH, a HTML-based markup language for writing scholarly articles, and from RASH into LaTeX according to Springer LNCS and ACM ICPS.",It ROCS!: The RASH Online Conversion Service,NA:NA:NA:NA:NA:NA,2016
Dimitar Dimitrov:Philipp Singer:Florian Lemmerich:Markus Strohmaier,"In this work, we study the visual position of links and their clicks on Wikipedia, particularly where links are visually located, at which screen positions users click on links, and which areas on the screen exhibit more or less clicks per links. For that purpose, we introduce a novel dataset containing the on-screen coordinate position for all links between pages in the English Wikipedia and additionally resort to navigation logs of Wikipedia users. Using this data, we can observe a preference of certain link and click locations on Wikipedia including first evidence of positional click bias. For example, our results suggest that users have a tendency to prefer to click on the left side of the screen which exceeds what one would expect from the presence of links on pages. We believe that presented data and research can be useful for optimizing the process of link creation and link consumption on Wikipedia and other Web platforms.",Visual Positions of Links and Clicks on Wikipedia,NA:NA:NA:NA,2016
Hui Du:Xueke Xu:Xueqi Cheng:Dayong Wu:Yue Liu:Zhihua Yu,"Recently, Deep Convolutional Neural Networks (CNNs) have been widely applied to sentiment analysis of short texts. Naturally, word embedding techniques are used to learn continuous word representations for constructing sentence matrix as input to CNN. As for sentiment analysis of customer reviews, we argue that it is problematic to learn a single representation for a word while ignoring sentiment information and the discussed aspects. In this poster, we propose a novel word embedding model to learn sentimental word embedding given specific aspects by modeling both sentiment and syntactic context under the specific aspects. We apply our method as input to CNN for sentiment analysis in multiple domains. Experiments show that the CNN based on the proposed model can consistently achieve superior performance compared to CNN based on traditional word embedding method.",Aspect-Specific Sentimental Word Embedding for Sentiment Analysis of Online Reviews,NA:NA:NA:NA:NA:NA,2016
Nemanja Duric:Mihajlo Grbovic:Vladan Radosavljevic:Jaikit Savla:Varun Bhagwan:Doug Sharp,"Tourism industry has grown tremendously in the previous several decades. Despite its global impact, there still remain a number of open questions related to better understanding of tourists and their habits. In this work we analyze the largest data set of travel receipts considered thus far, and focus on exploring and modeling booking behavior of online customers. We extract useful, actionable insights into the booking behavior, and tackle the task of predicting the booking time. The presented results can be directly used to improve booking experience of customers and optimize targeting campaigns of travel operators.",Travel the World: Analyzing and Predicting Booking Behavior using E-mail Travel Receipts,NA:NA:NA:NA:NA:NA,2016
Jinhua Gao:Huawei Shen:Shenghua Liu:Xueqi Cheng,"Modeling and predicting retweeting dynamics in social media has important implications to an array of applications. Existing models either fail to model the triggering effect of retweeting dynamics, e.g., the model based on reinforced Poisson process, or are hard to be trained using only the retweeting dynamics of individual tweet, e.g., the model based on self-exciting Hawkes process. In this paper, motivated by the observation that each retweeting dynamics is generally dominated by a handful of key nodes that separately trigger a high number of retweets, we propose a mixture process to model and predict retweeting dynamics, with each subprocess capturing the retweeting dynamics initiated by a key node. Experiments demonstrate that the proposed model outperforms the state-of-the-art model.",Modeling and Predicting Retweeting Dynamics via a Mixture Process,NA:NA:NA:NA,2016
Shreya Ghosh:Soumya K. Ghosh,"Exploring human movement pattern from raw GPS traces is an interesting and challenging task. This paper aims at analysing a large volume of GPS data in spatio-temporal context, clustering trajectories using geographic and semantic location information and identifying different categories of people. It tries to exploit the fact that human moves with an intent. The proposed framework yields encouraging results using a large scale GPS dataset of Microsoft GeoLife.",THUMP: Semantic Analysis on Trajectory Traces to Explore Human Movement Pattern,NA:NA,2016
Zafar Gilani:Liang Wang:Jon Crowcroft:Mario Almeida:Reza Farahbakhsh,"The WWW has seen a massive growth in variety and usage of OSNs. The rising population of users on Twitter and its open nature has made it an ideal platform for various kinds of opportunistic pursuits, such as news and emergency communication, business promotion, political campaigning, spamming and spreading malicious content. Most of these opportunistic pursuits are exploited through automated programs, known as bots. In this study we propose a framework (Stweeler) to study bot impact and influence on Twitter from systems and social media perspectives.",Stweeler: A Framework for Twitter Bot Analysis,NA:NA:NA:NA:NA,2016
Michel Héon:Roger Nkambou:Christian Langheit,"The Web Ontology Language (OWL-2) aims at offering a family of syntax such as RDF/XML, Manchester Turtle and others, for building ontologies. Ontology engineering is a complex task that requires skills that are rarely accessible to content experts. On the other hand, to model contents pertaining to a specific domain, graphical modeling is a technique that is often used to offer a knowledge representation tool to content experts that are not well acquainted with the process of formal ontology design. In this paper, we present the way in which the usage of polymorphism and symbol typing of graphical vocabulary have allowed us to design the G-OWL syntax, a graphical syntax that aims to graphically represent domain-specific knowledge using the OWL-2.","Toward G-OWL: A Graphical, Polymorphic And Typed Syntax For Building Formal OWL2 Ontologies",NA:NA:NA,2016
Aurelie Herbelot,"This paper presents PeARS (Peer-to-peer Agent for Reciprocated Search), an algorithm for distributed Web search that emulates the offline behaviour of a human with an information need. Specifically, the algorithm models the process of 'calling a friend', i.e. directing one's query to the knowledge holder most likely to answer it. The system allows network users to index and share part of their browsing history in a way that makes them 'experts' on some topics. A layer of distributional semantics agents then performs targeted information retrieval in response to user queries, in a fully automated fashion.",PeARS: a Peer-to-peer Agent for Reciprocated Search,NA,2016
Manel Hmimida:Rushed Kanawati,"In this paper we propose a new graph-based tag recommendation approach. The approach is structured into an offline step and an online one. Offline, the hypergraph depicting the history of tags assignment by users to resources is abstracted. On online, for a given target user and a resource, we first compute the set of recommended abstract tags (i.e tag clusters) applying a basic graph-based approach to the abstract graph. A new reduced graph is computed by unfolding the abstract subgraph composed of the set of recommended abstract tags and nodes representing the cluster of users (resp. resources) to which the target user (resp. resource) belongs to. Again the same basic graph-based tag recommendation approach is applied to this new reduced graph in order to compute the final set of tags to recommend. Experiments on real dataset show the effectiveness of the proposed approach.",A Graph-Coarsening Approach for Tag Recommendation,NA:NA,2016
Tin Kam Ho:Luis A. Lastras:Oded Shmueli,"We propose a method for a concept-centric semantic analysis of an evolving corpus, highlighting the persistent concepts, emergence of new concepts, and the changes in the semantic associations between concepts. We report our findings on a corpus of computer science literature that spans six decades, revealing interesting patterns about the progress of the discipline.",Concept Evolution Modeling Using Semantic Vectors,NA:NA:NA,2016
Kai Hui:Klaus Berberich,"Offline evaluation for information retrieval aims to compare the performance of retrieval systems based on relevance judgments for a set of test queries. Since manual judgments are expensive, selective labeling has been developed to semi-automatically label documents, in the wake of the similarity relationship among retrieved documents. Intuitively, the agreement w.r.t the cluster hypothesis can directly determine the amount of manual judgments that can be saved by creating labels with a semi-automatic method. Meanwhile, in representing documents, certain information is lost. We argue that better document representation can lead to better agreement with the cluster hypothesis. To this end, we investigate different document representations on established benchmarks in the context of low-cost evaluation, showing that different document representations vary in how well they capture document similarity relative to a query.",Cluster Hypothesis in Low-Cost IR Evaluation with Different Document Representations,NA:NA,2016
Ganesh J:Soumyajit Ganguly:Manish Gupta:Vasudeva Varma:Vikram Pudi,"In this paper, we consider the problem of learning representations for authors from bibliographic co-authorship networks. Existing methods for deep learning on graphs, such as DeepWalk, suffer from link sparsity problem as they focus on modeling the link information only. We hypothesize that capturing both the content and link information in a unified way will help mitigate the sparsity problem. To this end, we present a novel model 'Author2Vec', which learns low-dimensional author representations such that authors who write similar content and share similar network structure are closer in vector space. Such embeddings are useful in a variety of applications such as link prediction, node classification, recommendation and visualization. The author embeddings we learn are empirically shown to outperform DeepWalk by 2.35% and 0.83% for link prediction and clustering task respectively.",Author2Vec: Learning Author Representations by Combining Content and Link Information,NA:NA:NA:NA:NA,2016
Takamu Kaneko:Keiji Yanai,"Twitter is a unique microblog, which is different from conventional social media in terms of its quickness and on-the-spot-ness. Many Twitter's users send messages, which is commonly called ""tweets"", to Twitter on the spot with mobile phones or smart phones, and some of them send photos and geotags as well as tweets. Most of the photos are sent to Twitter soon after taken. In case of photos related to some events, most of them are taken during the events. We think that Twitter event photo mining is more useful to under- stand what happens currently over the world than only text-based Twitter event mining. In this paper, we propose a system to mine events visually from the Twitter stream. To do that, we use not only tweets having both geotags and photos but also tweets having geotags or photos for textual analysis or visual analysis. Although there exist many works related to Twitter mining using only text analysis such as typhoon and earthquake detection by Sakaki et al. [1], only a limited number of works exist on Twitter mining using image analysis. Nakaji et al. [2] proposed a system to mine representative photos related to the given keyword or term from a large number of geo-tweet photos. They extracted representative photos related to events such as ""typhoon"" and ""New Year's Day"". They used only geotagged photo tweets the number of which are limited compared to all the photo tweets. Gao et al. [3] proposed a method to mine brand product photos fromWeibo which employs supervised image recognition, which is different from event detection. They integrated visual features and social factors (users, relations, and locations) as well as textual features for brand product photo mining. In this paper, we detect visual events using geotagged non-photo tweets and non-geotagged photo tweets as well as geotagged photo tweets. In the experiments, we show some examples of detected events and their photos such as ""rainbow"", ""fireworks"" and ""festival"".",Visual Event Mining from the Twitter Stream,NA:NA,2016
Fariba Karimi:Claudia Wagner:Florian Lemmerich:Mohsen Jadidi:Markus Strohmaier,"Computational social scientists often harness the Web as a ""societal observatory"" where data about human social behavior is collected. This data enables novel investigations of psychological, anthropological and sociological research questions. However, in the absence of demographic information, such as gender, many relevant research questions cannot be addressed. To tackle this problem, researchers often rely on automated methods to infer gender from name information provided on the web. However, little is known about the accuracy of existing gender-detection methods and how biased they are against certain sub-populations. In this paper, we address this question by systematically comparing several gender detection methods on a random sample of scientists for whom we know their full name, their gender and the country of their workplace. We further suggest a novel method that employs web-based image retrieval and gender recognition in facial images in order to augment name-based approaches. Our findings show that the performance of name-based gender detection approaches can be biased towards countries of origin and such biases can be reduced by combining name-based an image-based gender detection methods.",Inferring Gender from Names on the Web: A Comparative Evaluation of Gender Detection Methods,NA:NA:NA:NA:NA,2016
Noriaki Kawamae,"We present a time series analysis employing natural language processing (NLP) techniques, and show the effect of N-gram over Context (NOC), that is a one of topic models that enjoy success in NLP, in this analysis.",Time Series Analysis Using NOC,NA,2016
Sungchul Kim:Jinyoung Yeo:Eunyee Koh:Nedim Lipka,"Web logs in e-commerce sites consist of user actions on items such as visiting an item description page, adding an item to a wishlist, and purchasing an item. Those items could be represented as nodes in a graph while viewing their relationships as edges according to the user actions. Based on the item graph, identifying items that attract users to purchase the target item could be practically used for supporting business decisions. To do this, we introduce a new task, called `Purchase Influence Mining', that finds the top-k items (PIM-items) maximizing the estimated purchase influence from them to a target item. We solve this problem by modeling the purchase influence as the shortest path between item pair. According to the result, our approach more consistently finds the k PIM-items than the baseline.",Purchase Influence Mining: Identifying Top-k Items Attracting Purchase of Target Item,NA:NA:NA:NA,2016
Yun-Yong Ko:Dong-Kyu Chae:Sang-Wook Kim,"This paper proposes a novel approach to target-oriented influence estimation, which remedies the drawback of state-of-the-art, thereby understanding information diffusion more accurately in a social network.",Accurate Path-based Methods for Influence Maximization in Social Networks,NA:NA:NA,2016
Peter Kratky:Daniela Chuda,"Biometric data are affected by physiological properties of people, including gender or age. The paper describes an experiment of discovering gender and age in computer mouse movement data that might be notably beneficial for profiling anonymous visitors browsing the Web. The proposed method extracts features, such as velocity, path straightness or pauses duration, that are used by a multiclassifier system to make an estimate. Age category estimation shows encouraging results of the early method, especially for statistical analysis of a website audience.",Estimating Gender and Age of Web Page Visitors from the Way They Use Their Mouse,NA:NA,2016
Srijan Kumar,"Citations are important to track and understand the evolution of human knowledge. At the same time, it is widely accepted that all the citations made in a paper are not equal. However, there is no thorough understanding of how citations are created that explicitly criticize or endorse others. In this paper, we do a detailed study of such citations made within the NLP community by differentiating citations into endorsement (positive), criticism (negative) and neutral categories. We analyse this signed network created between papers and between authors for the first time from a social networks perspective. We make many observations - we find that the citations follow a heavy-tailed distribution and they are created in a way that follows weak balance theory and status theories. Moreover, we find that authors do not change their opinion towards others over time and rarely reciprocate the opinion that they receive. Overall, the paper builds the understanding of the structure and dynamics of positive, negative and neutral citations.",Structure and Dynamics of Signed Citation Networks,NA,2016
Hemank Lamba:Vaishnavh Nagarajan:Kijung Shin:Naji Shajarisales,"Matrix and tensor completion techniques have proven useful in many applications such as recommender systems, image/video restoration, and web search. We explore the idea of using external information in completing missing values in tensors. In this work, we present a framework that employs side information as kernel matrices for tensor factorization. We apply our framework to problems of recommender systems and video restoration and show that our framework effectively deals with the cold-start problem.",Incorporating Side Information in Tensor Completion,NA:NA:NA:NA,2016
Hemank Lamba:Jürgen Pfeffer,"Influence maximization has found applications in various fields such as sensor placement, viral marketing, controlling rumor outbreak, etc. In this paper, we propose a targeted approach to influence maximization in polarized networks i.e. networks where we already know or can predict node's opinion about a product or topic. The goal is to find a set of individuals to target, such that positive opinion about a specific topic or the product to be launched is maximized. Another key aspect that is present in most of the existing viral marketing algorithms is that they do not take into account the timeliness of the product adoption. In this paper, we present a framework where we infer the polarity, activity levels of the users, and then select seeds to launch viral marketing campaigns such that positive influence about the product is maximized by the given deadline.",Maximizing the Spread of Positive Influence by Deadline,NA:NA,2016
Kevin Lange Di Cesare:Michel Gagnon:Amal Zouaq:Ludovic Jean-Louis,"The TAC KBP English slot filling track is an evaluation campaign that targets the extraction of 41 pre-identified relations related to specific named entities. In this work, we present a machine learning filter whose aim is to enhance the precision of relation extractors while minimizing the impact on recall. Our approach aims at filtering relation extractors' output using a binary classifier based on a wide array of features including syntactic, lexical and statistical features. We experimented the classifier on 14 of the 18 participating systems in the TAC KBP English slot filling track 2013. The results show that our filter is able to improve the precision of the best 2013 system by nearly 20\% and improve the F1-score for 17 relations out of 33 considered.",A Machine learning Filter for Relation Extraction,NA:NA:NA:NA,2016
Yang-Yin Lee:Hao Ke:Hen-Hsen Huang:Hsin-Hsi Chen,"GloVe, global vectors for word representation, performs well in some word analogy and semantic relatedness tasks. However, we find that some dimensions of the trained word embedding are abnormal. We verify our conjecture via removing these abnormal dimensions using Kolmogorov-Smimov test and experiment on several benchmark datasets for semantic relatedness measurement. The experimental results confirm our finding. Interestingly, some of the tasks outperform the state-of-the-art model SensEmbed by simply removing these abnormal dimensions. The novel rule of thumb technique which leads to better performance is expected to be useful in practice.",Less is More: Filtering Abnormal Dimensions in GloVe,NA:NA:NA:NA,2016
Yang-Yin Lee:Hao Ke:Hen-Hsen Huang:Hsin-Hsi Chen,"While many traditional studies on semantic relatedness utilize the lexical databases, such as WordNet or Wikitionary, the recent word embedding learning approaches demonstrate their abilities to capture syntactic and semantic information, and outperform the lexicon-based methods. However, word senses are not disambiguated in the training phase of both Word2Vec and GloVe, two famous word embedding algorithms, and the path length between any two senses of words in lexical databases cannot reflect their true semantic relatedness. In this paper, a novel approach that linearly combines Word2Vec and GloVe with the lexical database WordNet is proposed for measuring semantic relatedness. The experiments show that the simple method outperforms the state-of-the-art model SensEmbed.",Combining Word Embedding and Lexical Database for Semantic Relatedness Measurement,NA:NA:NA:NA,2016
Oliver Lehmberg:Dominique Ritze:Robert Meusel:Christian Bizer,NA,A Large Public Corpus of Web Tables containing Time and Context Metadata,NA:NA:NA:NA,2016
Manling Li:Yantao Jia:Yuanzhuo Wang:Jingyuan Li:Xueqi Cheng,"Link prediction over a knowledge graph aims to predict the missing entity h or t for a triple (h,r,t). Existing knowledge graph embedding based predictive methods represent entities and relations in knowledge graphs as elements of a vector space, and employ the structural information for link prediction. However, knowledge graphs contain many hierarchical relations, which existing methods have pay little attention to. In this paper, we propose a hierarchy-constrained locally adaptive knowledge graph embedding based link prediction method, called hTransA, by integrating hierarchical structures into the predictive work. Experiments over two benchmark data sets demonstrate the superiority of hTransA.",Hierarchy-Based Link Prediction in Knowledge Graphs,NA:NA:NA:NA:NA,2016
Yuchen Liu:Dmitry Chechik:Junghoo Cho,This paper introduces human curation signals and demonstrates incorporating human curation signals improves the relevance of state-of-art recommendation system models by up to 30% by experiments on a large-scale Pinterest dataset.,Power of Human Curation in Recommendation System,NA:NA:NA,2016
Fred Morstatter:Harsh Dani:Justin Sampson:Huan Liu,"While social media mining continues to be an active area of research, obtaining data for research is a perennial problem. Even more, obtaining unbiased data is a challenge for researchers who wish to study human behavior, and not technical artifacts induced by the sampling algorithm of a social media site. In this work, we evaluate one social media data outlet that gives data to its users in the form of a stream: Twitter's Sample API. We show that in its current form, this API can be poisoned by bots or spammers who wish to promote their content, jeopardizing the credibility of the data collected through this API. We design a proof-of-concept algorithm that shows how malicious users could increase the probability of their content appearing in the Sample API, thus biasing the content towards spam and bot content and harming the representativity of this data outlet.",Can One Tamper with the Sample API?: Toward Neutralizing Bias from Spam and Bot Content,NA:NA:NA:NA,2016
Eric Nalisnick:Bhaskar Mitra:Nick Craswell:Rich Caruana,"This paper investigates the popular neural word embedding method Word2vec as a source of evidence in document ranking. In contrast to NLP applications of word2vec, which tend to use only the input embeddings, we retain both the input and the output embeddings, allowing us to calculate a different word similarity that may be more suitable for document ranking. We map the query words into the input space and the document words into the output space, and compute a relevance score by aggregating the cosine similarities across all the query-document word pairs. We postulate that the proposed Dual Embedding Space Model (DESM) provides evidence that a document is about a query term, in addition to and complementing the traditional term frequency based approach.",Improving Document Ranking with Dual Word Embeddings,NA:NA:NA:NA,2016
Luan Minh Nguyen,"In this paper, we propose CaTER, which learns a novel context-aware joint representation of text and user by incorporating semantic text embedding of unlabeled tweets as well as social relation information. CaTER leverages the wealth of user contextual information available apart from user's utterances for sentiment analysis. Our approach is inspired by social science about emotional behaviors of connected users, who perhaps more likely to consensus on similar opinions. Our method outperforms numerous baselines on two real-world Twitter datasets.",Context-Aware Text Representation for Social Relation Aided Sentiment Analysis,NA,2016
Shumpei Okura:Yukihiro Tagami:Akira Tajima,"In news recommendation systems, eliminating redundant information is important as well as providing interesting articles for users. We propose a method that quantifies the similarity of articles based on their distributed representation, learned with the category information as weak supervision. This method is useful for evaluation under tight time constraints, since it only requires low-dimensional inner product calculation for estimating similarities. The experimental results from human evaluation and online performance in A/B testing suggest the effectiveness of our proposed method, especially for quantifying middle-level similarities. Currently, this method is used on Yahoo!\ JAPAN's front page, which has millions of users per day and billions of page views per month.",Article De-duplication Using Distributed Representations,NA:NA:NA,2016
Chanyoung Park:Donghyun Kim:Jinoh Oh:Hwanjo Yu,"Due to the data sparsity problem, social network information is often additionally used to improve the performance of recommender system. While most existing works exploit social information to reduce the rating prediction error, e.g., RMSE, a few had aimed to improve the top-k ranking prediction accuracy. This paper proposes a novel top-k oriented recommendation method, TRecSo, which incorporates social information into recommendation by modeling two different roles of users as trusters and trustees while considering the structural information of the network. Empirical studies on real-world datasets demonstrate that TRecSo leads to remarkable improvement compared to previous methods in top-k recommendation.",TRecSo: Enhancing Top-k Recommendation With Social Information,NA:NA:NA:NA,2016
Dinesh Pradhan:Tanmoy Chakraborty:Saswata Pandit:Subrata Nandi,"Understanding the qualitative patterns of research endeavor of scientific authors in terms of publication count and their impact (citation) is important in order to quantify success trajectories. Here, we examine the career profile of authors in computer science and physics domains and discover at least six different success trajectories in terms of normalized citation count in longitudinal scale. Initial observations of individual trajectories lead us to characterize the authors in each category. We further leverage this trajectory information to build a two-stage stratification model to predict future success of an author at the early stage of her career. Our model outperforms the baseline with an average improvement of 15.68% for both the datasets.",On the Discovery of Success Trajectories of Authors,NA:NA:NA:NA,2016
Vladan Radosavljevic:Mihajlo Grbovic:Nemanja Djuric:Narayan Bhamidipati:Daneo Zhang:Jack Wang:Jiankai Dang:Haiying Huang:Ananth Nagarajan:Peiji Chen,"Last decade has witnessed a tremendous expansion of mobile devices, which brought an unprecedented opportunity to reach a large number of mobile users at any point in time. This resulted in a surge of interest of mobile operators and ad publishers to understand usage patterns of mobile apps and allow more relevant content recommendations. Due to a large input space, a critical step in understanding app usage patterns is reducing sparseness by classifying apps into predefined interest taxonomies. However, besides short name and noisy description majority of apps have very limited information available, which makes classification a challenging task. We address this issue and present a novel method to classify apps into interest categories by: 1) embedding apps into low-dimensional space using a neural language model applied on smartphone logs; and 2) applying k-nearest-neighbors classification in the embedding space. To validate the method we run experiments on more than one billion device logs covering hundreds of thousands of apps. To the best of our knowledge this is the first app categorization study at this scale. Empirical results show that the proposed method outperforms the current state-of-the-art.",Smartphone App Categorization for Interest Targeting in Advertising Marketplace,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2016
Jing Ren:Jialie Shen:Robert J. Kauffman,"Tens of thousands of music tracks are uploaded to the Internet every day through social networks that focus on music and videos, as well as portal websites. While some of the content has been popular for decades, some tracks that have just been released have been completely ignored. So what makes a music track popular? Can we predict the popularity of a music track before it is released? In this research, we will focus on an online music social network, Last.fm, and investigate three key factors of a music track that may have impact on its popularity. They include: the music content, the artist reputation and the social context of the music. The results suggest that we can predict the future popularity of music with around 80% accuracy using just these three factors. We also found out that in the social networks scenario, the content of the music seems to be an surprisingly important factor that determines the popularity of a track online.",What Makes a Music Track Popular in Online Social Networks?,NA:NA:NA,2016
Surendra Sedhai:Aixin Sun,"Presence of spam tweets in a dataset may affect the choices of feature selection, algorithm formulation, and system evaluation for many applications. However, most existing studies have not considered the impact of spam tweets. In this paper, we study the impact of spam tweets on hashtag recommendation for hyperlinked tweets (i.e., tweets containing URLs) in HSpam14 dataset. HSpam14 is a collection of 14 million tweets with annotations of being spam and ham (i.e., non-spam). In our experiments, we observe that it is much easier to recommend ""correct"" hashtags for spam tweets than ham tweets, because of the near duplicates in spam tweets. Simple approaches like recommending most popular hashtags achieves very good accuracy on spam tweets. On the other hand, features that are highly effective on ham tweets may not be effective on spam tweets. Our findings suggest that without removing spam tweets from the data collection (as in most studies), the results obtained could be misleading for hashtag recommendation tasks.",Effect of Spam on Hashtag Recommendation for Tweets,NA:NA,2016
Wafa Shafqat:Seunghun Lee:Sehrish Malik:Hyun-chul Kim,"Crowdfunding sites with recent explosive growth are equally attractive platforms for swindlers or scammers. Though the growing number of articles on crowdfunding scams indicate that the fraud threats are accelerating, there has been little knowledge on the scamming practices and patterns. The key contribution of this research is to discover the hidden clues in the text by exploring linguistic features to distinguish scam campaigns from non-scams. Our results indicate that by providing less information and writing more carefully (and less informally), scammers deliberately try to deceive people; (i) they use less number of words, verbs, and sentences in their campaign pages. (ii) scammers make less typographical errors, 4.5-4.7 times lower than non-scammers.(iii) Expressivity of scams is 2.6-8.5 times lower as well.",The Language of Deceivers: Linguistic Features of Crowdfunding Scams,NA:NA:NA:NA,2016
Baoxu Shi:Tim Weninger,"Traditional fact checking by experts and analysts cannot keep pace with the volume of newly created information. It is important and necessary, therefore, to enhance our ability to computationally determine whether some statement of fact is true or false. We view this problem as a link-prediction task in a knowledge graph, and show that a new model of the top discriminative meta paths is able to understand the meaning of some statement and accurately determine its veracity. We evaluate our approach by examining thousands of claims related to history, geography, biology, and politics using public, million node knowledge graphs extracted from Wikipedia and SemMedDB. Not only does our approach significantly outperform related models, we also find that the discriminative path model is easily interpretable and provides sensible reasons for the final determination.",Fact Checking in Heterogeneous Information Networks,NA:NA,2016
Alexander Shishkin:Ekaterina Gladkikh:Aleksandr Vorobev,"Implicit user feedback is known to be a strong signal of user preferences in web search. Hence, solving the exploration-exploitation dilemma [5] became an important direction of improvement of ranking algorithms in the last years. In this poster, in the case of commercial queries, we consider a new negative effect of exploration on the user utility -- distracting and confusing users by shifting well-known documents from their common positions -- and propose an approach to take it into account within Multi-Armed Bandit algorithms, usually applied to solve the dilemma.",Selective Exploration of Commercial Documents in Web Search,NA:NA:NA,2016
Rina Singh:Jeffrey A. Graves:Douglas A. Talbert,"In recent years, there has been huge growth in the amount of graph data generated from various sources. These types of data are often represented by vertices and edges in a graph with real-valued attributes, topological properties, and temporal information associated with the vertices. Until recently, most pattern mining techniques focus solely on vertex attributes, topological properties, or a combination of these in a static sense; mining attribute and topological changes simultaneously over time has largely been overlooked. In this work-in-progress paper, we propose to extend an existing state-of-the-art technique to mine for patterns in dynamic attributed graphs which appear to trigger changes in attribute values.",Complex Patterns in Dynamic Attributed Graphs,NA:NA:NA,2016
Marcin Skowron:Marko Tkalčič:Bruce Ferwerda:Markus Schedl,"Incorporating users' personality traits has shown to be instrumental in many personalized retrieval and recommender systems. Analysis of users' digital traces has become an important resource for inferring personality traits. To date, the analysis of users' explicit and latent characteristics is typically restricted to a single social networking site (SNS). In this work, we propose a novel method that integrates text, image, and users' meta features from two different SNSs: Twitter and Instagram. Our preliminary results indicate that the joint analysis of users' simultaneous activities in two popular SNSs seems to lead to a consistent decrease of the prediction errors for each personality trait.",Fusing Social Media Cues: Personality Prediction from Twitter and Instagram,NA:NA:NA:NA,2016
Sucheta Soundarajan:Acar Tamersoy:Elias B. Khalil:Tina Eliassi-Rad:Duen Horng Chau:Brian Gallagher:Kevin Roundy,"We study the problem of determining the proper aggregation granularity for a stream of time-stamped edges. Such streams are used to build time-evolving networks, which are subsequently used to study topics such as network growth. Currently, aggregation lengths are chosen arbitrarily, based on intuition or convenience. We describe ADAGE, which detects the appropriate aggregation intervals from streaming edges and outputs a sequence of structurally mature graphs. We demonstrate the value of ADAGE in automatically finding the appropriate aggregation intervals on edge streams for belief propagation to detect malicious files and machines.",Generating Graph Snapshots from Streaming Edge Data,NA:NA:NA:NA:NA:NA:NA,2016
Ajitesh Srivastava:Charalampos Chelmis:Viktor K. Prasanna,"Several applications including community detection in social networks and discovering correlated genes involve finding large subgraphs of high density. We propose the problem of finding the largest subgraph of a given density. The problem is a generalization of the Max-Clique problem which seeks the largest subgraph that has an edge density of 1. We define an objective function and prove that its optimization results in the largest graph of given density. We propose an algorithm that finds the subgraph by running multiple local search heuristics with random restarts. For massive graphs, where running the algorithm directly may be intractable, we use a sampling technique that reduces the graph to a smaller one which is likely to contain large dense subgraphs. We evaluate our algorithm on multiple real life and synthetic datasets. Our experiments show that our algorithm performs as well as the state-of-the-art for finding large subgraphs of high density, while providing density guarantees.",Mining Large Dense Subgraphs,NA:NA:NA,2016
Io Taxidou:Peter M. Fischer:Tom De Nies:Erik Mannens:Rik Van de Walle,"This paper sheds light on the different interaction types among social media users that benefit information diffusion and provenance analysis. In particular, we identify explicit and implicit interactions in Twitter, including informal conventions applied by users. In our empirical evaluation considering only retweets, the most common means of information propagation in Twitter, we can infer 50% of message provenance. However, if we consider other types of interactions, we can explain another 13%. Accordingly, we enrich the PROV-SAID model for information diffusion, which extends the W3C PROV standard for provenance.",Information Diffusion and Provenance of Interactions in Twitter: Is it only about Retweets?,NA:NA:NA:NA:NA,2016
Ramine Tinati:Markus Luczak-Roesch:Wendy Hall:Nigel Shadbolt,NA,More than an Edit: Using Transcendental Information Cascades to Capture Hidden Structure in Wikipedia,NA:NA:NA:NA,2016
Fujio Toriumi:Seigo Baba,"During a disaster, appropriate information must be collected quickly. For example, residents along the coast require information about tsunamis and those who have lost their houses need information about shelters. Twitter can attract more attention than other forms of mass media under these circumstances because it can quickly provide such information. Since Twitter has an enormous amount of tweets, they must be classified to provide users with the information they need. Previous works on extracting information from Twitter focused on the text data of tweets. However, in some cases, text mining has difficulty extracting information. For example, it might be difficult for text mining to group tweets with URLs. On the other hand, by assuming that users who retweet the same tweet are interested in the same topic, we can classify tweets that are required by users with similar interests based on retweets. Thus, we employ the tweet classification method that focuses on retweets. In this paper, we demonstrated that our method works quickly in disaster situations and that it can quickly classify the required information based on the needs in disaster situations and is helpful for collecting information under them.",Real-time Tweet Classification in Disaster Situation,NA:NA,2016
Herbert Van de Sompel:Martin Klein:Shawn M. Jones,"We quantify the extent to which references to papers in scholarly literature use persistent HTTP URIs that leverage the Digital Object Identifier infrastructure. We find a significant number of references that do not, speculate why authors would use brittle URIs when persistent ones are available, and propose an approach to alleviate the problem.",Persistent URIs Must Be Used To Be Persistent,NA:NA:NA,2016
Bo Wang:Yanshu Yu:Peng Zhang,"Instead of studying the properties of social relationship from an objective view, in this paper, we focus on the two individuals' subjective and asymmetric opinions on their interrelationships. The sociolinguistics theories propose to characterize the individuals' opinions of their interrelationship with interactive language features. With this inspiration, we investigate the subjective asymmetry of the interrelationship with the asymmetry of the interactive language features including the frequency, quantity, quality and emotion. Experimental results with Enron email corpus provide suggestive evidences and thus reveal that the pair-wise language styles on an interrelationship are asymmetric, and this asymmetry can be a joint effect of the individuals' opinions of the interrelationship and their personal language habits. The results also indicate that the degree of the asymmetry could be related to the individuals' personality traits.",Investigation of the Subjective Asymmetry of Social Interrelationship with Interactive Language,NA:NA:NA,2016
Chengyu Wang:Rong Zhang:Xiaofeng He:Aoying Zhou,"While most of the entity ranking research focuses on Web corpora with user queries as input, little has been done to rank entities directly from documents. We propose a ranking algorithm NERank to address this issue. NERank employs a random walk process on a weighted tripartite graph mined from the document collection. We evaluate NERank over real-life document datasets and compare it with baselines. Experimental results show the effectiveness of our method.",NERank: Ranking Named Entities in Document Collections,NA:NA:NA:NA,2016
Dongjing Wang:Shuiguang Deng:Songguo Liu:Guandong Xu,"In this paper, a music recommendation approach based on distributed representation is presented. The proposed approach firstly learns the distributed representations of music pieces and acquires users' preferences from listening records. Then, it recommends appropriate music pieces whose distributed representations are in accordance with target users' preferences. Experiments on a real world dataset demonstrate that the proposed approach outperforms the state-of-the-art methods.",Improving Music Recommendation Using Distributed Representation,NA:NA:NA:NA,2016
Chao-Yuan Wu:Alex Beutel:Amr Ahmed:Alexander J. Smola,"Understanding a user's motivations provides valuable information beyond the ability to recommend items. Quite often this can be accomplished by perusing both ratings and review texts. Unfortunately matrix factorization approaches to recommendation result in large, complex models that are difficult to interpret. In this paper, we attack this problem through succinct additive co-clustering on both ratings and reviews. Our model yields accurate and interpretable recommendations.",Explaining Reviews and Ratings with PACO: Poisson Additive Co-Clustering,NA:NA:NA:NA,2016
Kohei Yamamoto:Hayato Kobayashi:Yukihiro Tagami:Hideki Nakayama,"News article recommendation has the key problem of needing to eliminate the redundant information in a ranked list in order to provide more relevant information within a limited time and space. In this study, we tackle this problem by using image thumbnailing, which can be regarded as the summarization of news images. We propose a multimodal image thumbnailing method considering news text as well as images themselves. We evaluate this approach on a real data set based on news articles that appeared on Yahoo! JAPAN. Experimental results demonstrate the effectiveness of our proposed method.",Multimodal Content-Aware Image Thumbnailing,NA:NA:NA:NA,2016
Tomoya Yamazaki:Nobuyuki Shimizu:Hayato Kobayashi:Satoshi Yamauchi,"We propose a simple and scalable method for soft community detection that makes use of both graph structures and vertex attributes. Our method is based on micro-clustering, which is a scalable and efficient clique-based method for detecting overlapping communities in unweighted graphs. We extend this method to graphs with vertex attributes so that we can make use of information supplied by vertex attributes. Our method still requires the same time complexity as micro-clustering. We confirm the validity and efficiency of our method by applying it to a large-scale co-purchasing network of real online auction data.",Weighted Micro-Clustering: Application to Community Detection in Large-Scale Co-Purchasing Networks with User Attributes,NA:NA:NA:NA,2016
Jinyoung Yeo:Sungchul Kim:Eunyee Koh:Seung-won Hwang:Nedim Lipka,"This paper covers a sales forecasting problem on e-commerce sites. To predict product sales, we need to understand customers' browsing behavior and identify whether it is for purchase purpose or not. For this goal, we propose a new customer model, B2P, of aggregating predictive features extracted from customers' browsing history. We perform experiments on a real world e-commerce site and show that sales predictions by our model are consistently more accurate than those by existing state-of-the-art baselines.",Browsing2purchase: Online Customer Model for Sales Forecasting in an E-Commerce Site,NA:NA:NA:NA:NA,2016
Li'ang Yin:Jianhua Han:Yong Yu,"Label aggregation is one of the key topics in crowdsourcing research. Most researchers make their efforts in modeling ability of users and difficulty of instances. In this paper, we consider label aggregation from the view of grouping instances. We assume instances are sampled from latent groups and they share the same true label with their corresponding groups. We construct a graphical model named InGroup(Instance Grouping model) to infer latent group assignment as well as true labels. The experimental results show the advantages of our model compared with baselines.",Label Aggregation with Instance Grouping Model,NA:NA:NA,2016
Haochao Ying:Liang Chen:Yuwen Xiong:Jian Wu,"Point-of-interest (POI) recommendation has become more and more important, since it could discover user behavior pattern and find interesting venues for them. To address this problem, we propose a rank-based method, PGRank, which integrates user geographical preference and latent preference into Bayesian personalized ranking framework. The experimental results on a real dataset show its effective.",PGRank: Personalized Geographical Ranking for Point-of-Interest Recommendation,NA:NA:NA:NA,2016
Takeru Yokoi:Masato Fukuchi:Michihiro Kobayakawa:Roliana Ibrahim:Ali Selamat,"News articles are a type of instantaneous and regional media, and provide the daily concerns of its publication area. This work proposes an analytical framework of the relations among nations using the similarities in the content of news articles published in different nations. Our key idea is that those relations exist in the news articles miss-classified by different nations from their original publication area. In order to clearly illustrate those relations, the classification results are visualized as bar graphs. We also carried out some experiments for the proposed framework using a small collection of news articles.",Analytical Framework of Relations among Nations using News Articles,NA:NA:NA:NA:NA,2016
Shuangfei Zhai:Keng-hao Chang:Ruofei Zhang:Zhongfei Zhang,"We investigate the use of recurrent neural networks (RNNs) in the context of online advertising, where we use RNNs to map both query and ads to real valued vectors. In addition, we propose an attention network that assigns scores to different word locations according to their intent importance. The vector output is computed by a weighted sum of the vectors at each word. We perform end-to-end training of both the RNN and attention network under the guidance of user click logs. We show that the attention network improves the quality of learned vector representations evaluated by AUC on a manually labeled dataset. Moreover, we show that keywords extracted according to the attention scores are easy to interpret and significantly outperform the state-of-the-art query intent extraction methods.",Attention Based Recurrent Neural Networks for Online Advertising,NA:NA:NA:NA,2016
Zhongyi Zhai:Bo Cheng:Zhaoning Wang:Xuan Liu:Meng Liu:Junliang Chen,"In this paper, we present an ecosystem for mobile application development by end-users. An advantage of this ecosystem is that the graphical user interface (GUI), as well as the application logic, can both be developed in a rapid and simple way. This ecosystem is mainly implemented through development and integration of two sub-systems, namely EasyApp and LSCE. EasyApp is responsible for developing the mobile app with the compatibility of multiple mobile platforms, while LSCE is in charge of creating the service process that can be invoked by mobile app directly. A case study is presented to illustrate the development process using this ecosystem.",Design and Implementation: the End User Development Ecosystem for Cross-platform Mobile Applications,NA:NA:NA:NA:NA:NA,2016
Shanshan Zhang:Slobodan Vucetic,"This paper describes a case study of sampling bias in LinkedIn, a major professional social network. The study collected a sample of 1,989 STEM students who graduated from a major public university between 2002 and 2014. Overall, 40\% of the graduates had a LinkedIn profile in summer of 2015. It was observed that LinkedIn participation significantly fluctuated among different majors, and ranged from 30\% for biochemistry majors to 51\% for information science majors. Year of graduation, gender, and grade point average surprisingly did not seem to create a large difference in LinkedIn participation. These results should be useful for design and interpretation of empirical studies which use LinkedIn data or select participants from LinkedIn social network.",Sampling Bias in LinkedIn: A Case Study,NA:NA,2016
Matteo Zignani:Sabrina Gaito:Gian Paolo Rossi,"Measurements of online social networks (OSNs) support the common fact that not all links carry the same social value, and that the strength of each link is strictly related to the frequency of interactions between the connected users. In this paper, we investigate the predictability of the interactions on OSN links by wondering if it is possible to categorize interactive or non-interactive links at their creation time. We turn the problem into a binary classification task and introduce a set of features which leverage the temporal and topological properties of the social and interaction networks, without requiring the knowledge of the interaction history of the link. The best classifier trained on a Facebook dataset obtained 0.72 as AUC. The above performance suggests that we can distinguish between interactive/non-interactive links at the time of link creation.","Predicting the Link Strength of ""Newborn"" Links",NA:NA:NA,2016
Alfredo Cuzzocrea:Abdulmotaleb El Saddik,"It is our great pleasure to welcome you to the Demo Track of WWW 2016, The 25th International World Wide Web Conference, held in Montreal, Canada, during April 11-15, 2016. The WWW 2016 Demo Track, like in the tradition of WWW Demo conference series, allows researchers and practitioners to demonstrate new systems in a dedicated session. Demo contributions are based on an implemented and tested system that pursues one or more innovative ideas in the interest areas of Web data and information management, Web search, Web intelligence tools, Web mining, social network applications and so forth. Topics of interest for the 2016 edition's conference include (but are not limited to) the following ones: Behavioral Analysis and Personalization Big Data on the Web Crowdsourcing Systems and Social Media Content Analysis Graph Data Management and Mining High-Performance Infrastructures for Data- Intensive Web Tasks Internet Economics and Monetization Pervasive Web and Mobility Security and Privacy Semantic Web Social Networks and Graph Analysis Web Information Retrieval Web Infrastructure: Datacenters, Content Delivery Networks, and Cloud Computing Web Mining Web Science Web Search Systems and Applications Demo contributions come from academic researchers, industrial practitioners with prototypes or inproduction deployments, as well as from any W3C-related activities. All have in common to show innovative use of Web-based techniques. The WWW 2016 Demo Track call for papers attracted 65 submissions from all over the world (USA, North America, South America, Europe, Australia, Asia, Africa). The program committee reviewed and accepted a very selected collection of 29 papers, and the final statistics is the following: WWW 2016 Demo Track Statistics Number of Submitted Papers 65 Number of Accepted Papers 29.",Session details: Demonstrations,NA:NA,2016
Marie Al-Ghossein:Talel Abdessalem,"This demo presents SoMap, a web-based platform that provides new scalable methods to aggregate, analyse and valorise large collections of heterogeneous social data in urban contexts. The platform relies on geotagged data extracted from social networks and microblogging applications such as Instagram, Flickr and Twitter and on Points Of Interest gathered from OpenStreetMap. It could be very insightful and interesting for data scientists and decision-makers. SoMap enables dynamic clustering of filtered social data in order to display it on a map in a combined form. The key components of this platform are the clustering module, which relies on a scalable algorithm described in this paper, and the ranking algorithm that combines the popularity of the posts, their location and their link to the points of interest found in the neighbourhood. The system further detects mobility patterns by identifying and aggregating trajectories for all the users. SoMap will be demonstrated through several examples that highlight all of its functionalities and reveal its effectiveness and usefulness.",SoMap: Dynamic Clustering and Ranking of Geotagged Posts,NA:NA,2016
Ana Paula Appel:Heloisa Candello:Beatriz S.R. de Souza:Bruna D. Andrade,"In this work we present Destiny a cognitive mobile guide for Olympics games in Brazil that identifies user characteristics to deliver content. It will help visitors, athletes, and athletes' parents to localize themselves and receive tailored historical, cultural and entertainment information at a particular point of interest (POI). The application will recommend places to go and show POI contents according to user context based on several attributes such as personality (closeness, curiosity, adventurous), nationality, favorite places already visited, trip length, price range and favorite sports",Destiny: A Cognitive Mobile Guide for the Olympics,NA:NA:NA:NA,2016
Mouhamadou Lamine Ba:Laure Berti-Equille:Kushal Shah:Hossam M. Hammady,"Social networks and the Web in general are characterized by multiple information sources often claiming conflicting data values. Data veracity is hard to estimate, especially when there is no prior knowledge about the sources or the claims and in time-dependent scenarios where initially very few observers can report first information. Despite the wide set of recently proposed truth discovery approaches, ""no-one-fits-all"" solution emerges for estimating the veracity of on-line information in open contexts. However, analyzing the space of conflicting information and disagreeing sources might be relevant, as well as ensembling multiple truth discovery methods. This demonstration presents VERA, a Web-based platform that supports information extraction from Web textual data and micro-texts from Twitter and estimates data veracity. Given a user query, VERA systematically extracts entities and relations from Web content, structures them as claims relevant to the query and gathers more conflicting/corroborating information. VERA combines multiple truth discovery algorithms through ensembling returns the veracity label and score of each data value and the trustworthiness scores of the sources. VERA will be demonstrated through several real-world scenarios to show its potential value for fact-checking from Web data.",VERA: A Platform for Veracity Estimation over Web Data,NA:NA:NA:NA,2016
Ciro Baron Neto:Kay Müller:Martin Brümmer:Dimitris Kontokostas:Sebastian Hellmann,"The Linked Open Data (LOD) cloud is in danger of becoming a black box. Simple questions such as ""What kind of datasets are in the LOD cloud?"", ""In what way(s) are these datasets connected?"" -- albeit frequently asked -- are at the moment still difficult to answer due to the lack of proper tooling support. The infrequent update of the static LOD cloud diagram adds to the current dilemma, since there is neither reliable nor timely-updated information to perform an interactive search, analysis or in particular visualization in order to gain insight into the current state of Linked Open Data. In this paper, we propose a new hybrid system which combines LOD Visualisation, Analytics and DiscovERy (LODVader) to aid in answering the above questions. LODVader is equipped with (1) a multi-layer LOD cloud visualization component comprising datasets, subsets and vocabularies, (2) dataset analysis components that extend the state of the art with new similarity measures and efficient link extracting techniques and (3) a fast search index that is an entry point for dataset discovery. At its core, LODVader employs a timely-updated index using a complex cluster of Bloom filters as a fast search index with low memory footprint. This BF cluster is able to efficiently perform analysis on link and dataset similarities based on stored predicate and object information, which -- once inverted -- can be employed to discover invalid links by displaying the Dark LOD Cloud. By combining all these features, we allow for an up-to-date, multi-dimensional LOD cloud analysis, which -- to the best of our knowledge -- was not possible before.","LODVader: An Interface to LOD Visualization, Analyticsand DiscovERy in Real-time",NA:NA:NA:NA:NA,2016
Emre Çelikten:Géraud Le Falher:Michael Mathioudakis,"We demonstrate Geotopics, a system to explore geographical patterns of urban activity. The system collects publicly shared check-ins generated by Foursquare users, that reveal who spends time where, when, and on what type of activity. It then employs sparse probabilistic modeling techniques to learn associations between different regions of a city and multi-feature descriptions of urban activity. Through a web interface, users of the system can select a city of interest and explore visualizations that highlight how different types of activity are spatially and temporally distributed in the city. We discuss the opportunities that web data offer to understand urban activity and the challenges one faces in that task. We then describe our approach and the architecture of Geotopics. Finally, we lay out the demonstration scenario.",What Is the City but the People?: Exploring Urban Activity Using Social Web Traces,NA:NA:NA,2016
Diego Collarana:Christoph Lange:Sören Auer,"The increasing amount of structured and semi-structured information available on the Web and in distributed information systems, as well as the Web's diversification into different segments such as the Social Web, the Deep Web, or the Dark Web, requires new methods for horizontal search. FuhSen is a federated, RDF-based, hybrid search platform that searches, integrates and summarizes information about entities from distributed heterogeneous information sources using Linked Data. As a use case, we present scenarios where law enforcement institutions search and integrate data spread across these different Web segments to identify cases of organized crime. We present the architecture and implementation of FuhSen and explain the queries that can be addressed with this new approach.","FuhSen: A Platform for Federated, RDF-based Hybrid Search",NA:NA:NA,2016
Achille Fokoue:Oktie Hassanzadeh:Mohammad Sadoghi:Ping Zhang,"Drug-Drug Interactions (DDIs) are a major cause of preventable adverse drug reactions and a huge burden on public health and the healthcare system. On the other hand, there is a large amount of drug-related (open) data published on the Web, describing various properties of drugs and their relationships to other drugs, genes, diseases, and related concepts and entities. In this demonstration, we describe an end-to-end system we have designed to take in various Web data sources as input and provide as output a prediction of DDIs along with an explanation of why two drugs may interact. The system first creates a knowledge graph out of input data sources through large-scale semantic integration, and then performs link prediction among drug entities in the graph through large-scale similarity analysis and machine learning. The link prediction is performed using a logistic regression model over several similarity matrices built using different drug similarity measures. We present both the efficient link prediction framework implemented in Apache Spark, and our APIs and Web interface for predicting DDIs and exploring their potential causes and nature.",Predicting Drug-Drug Interactions Through Similarity-Based Link Prediction Over Web Data,NA:NA:NA:NA,2016
Michele M. Franceschini:Livio B. Soares:Luis A. Lastras Montaño,"Watson Concept Insights (WCI) is a service that was recently made publicly available by IBM. WCI provides an information retrieval framework that is designed to facilitate search and exploration of text documents, and is particularly effective on sparse data sets. Its methodology consists of first defining a dictionary of concepts which are interconnected in a concept graph and then modeling a document by predicting its relevance to any given concept in the concept graph using the concepts that are directly mentioned in the document itself. This technique in effect increases the document recall for any given query, even for very sparse data sets, exposing the user to a variety of connections between their query and a data set of interest.",Watson Concept Insights: A Conceptual Association Framework,NA:NA:NA,2016
Andrea Gallidabino:Cesare Pautasso,"We are heading toward an era in which users own more than one single Web-enabled device. These devices range from smart phones, tablets and personal computers to smart Web-enabled devices found in houses and cars. The access mechanisms and usage patterns of Web applications are changing accordingly, as users interact more and more with Web applications through all their devices, even if the majority of Web applications are not ready to offer a good user experience taking full advantage of multiple devices. In this demonstration we introduce Liquid.js, a framework whose goal is to enable Web developers to take advantage of multiple heterogeneous devices and offer to their users a liquid user experience, whereby any device can be used sequentially or concurrently with Web applications that can effortlessly roam from one device to another. This way, as highlighted in the demonstration users do not need to stop and resume their work on their Web application as they migrate and clone them across different devices. The demo will also show how developers can easily add such liquid behavior to any Polymer Web component.",The Liquid.js Framework for Migrating and Cloning Stateful Web Components across Multiple Devices,NA:NA,2016
Stefan Hagedorn:Kai-Uwe Sattler,"Data analytics has gained more and more focus during recent years and many data processing platforms have been developed. They all provide a powerful but often complex API that users have to learn. Furthermore, results can only be stored or printed, without any possibility for visualization. In this paper we present Piglet, a compiler for the high-level Pig Latin script language that generates code for various platforms like Spark, Flink, Storm, and PipeFabric. Piglet lets users write elegant code with extensions for SPARQL and RDF, as well as support for streaming data. An integration into the notebook-based frontend Zeppelin provides a homogeneous and interactive user interface for exploring, analyzing, and visualizing data from different sources and lets users share their scripts and results.",Piglet: Interactive and Platform Transparent Analytics for RDF & Dynamic Data,NA:NA,2016
Aisha Hasan:Mohammad Hammoud:Reza Nouri:Sherif Sakr,"RDF and SPARQL query language are gaining wide popularity and acceptance. This demonstration paper presents DREAM, a hybrid RDF system, which combines the advantages and averts the disadvantages of the centralized and distributed RDF schemes. In particular, DREAM avoids partitioning RDF datasets and reversely partitions SPARQL queries. By not partitioning datasets, DREAM offers a general paradigm for different types of pattern matching queries and entirely precludes intermediate data shuffling (only auxiliary data are shuffled). By partitioning only queries, DREAM suggests an adaptive scheme, which runs queries on different numbers of machines depending on their complexities. DREAM achieves these goals and significantly outperforms related systems via employing a novel graph-based, rule-oriented query planner and a new cost model. This paper proposes demonstrating DREAM live over the cloud using a friendly graphical user interface (GUI). The GUI allows participants to execute and visualize pre-defined and user-defined (which can be written by participants on-the-fly) SPARQL queries over various real-world and synthetic RDF datasets. Furthermore, participants can empirically compare and contrast DREAM against three state-of-the-art RDF systems.",DREAM in Action: A Distributed and Adaptive RDF System on the Cloud,NA:NA:NA:NA,2016
Viet Ha-Thuc:Ye Xu:Satya Pradeep Kanduri:Xianren Wu:Vijay Dialani:Yan Yan:Abhishek Gupta:Shakti Sinha,"One key challenge in talent search is how to translate complex criteria of a hiring position into a search query. This typically requires deep knowledge on which skills are typically needed for the position, what are their alternatives, which companies are likely to have such candidates, etc. However, listing examples of suitable candidates for a given position is a relatively easy job. Therefore, in order to help searchers overcome this challenge, we design a next generation of talent search paradigm at LinkedIn: Search by Ideal Candidates. This new system only needs the searcher to input one or several examples of suitable candidates for the position. The system will generate a query based on the input candidates and then retrieve and rank results based on the query as well as the input candidates. The query is also shown to the searcher to make the system transparent and to allow the searcher to interact with it. As the searcher modifies the initial query and makes it deviate from the ideal candidates, the search ranking function dynamically adjusts an refreshes the ranking results balancing between the roles of query and ideal candidates. As of writing this paper, the new system is being launched to our customers.",Search by Ideal Candidates: Next Generation of Talent Search at LinkedIn,NA:NA:NA:NA:NA:NA:NA:NA,2016
Ruining He:Chunbin Lin:Julian McAuley,"To build a fashion recommendation system, we need to help users retrieve fashionable items that are visually similar to a particular query, for reasons ranging from searching alternatives (i.e., substitutes), to generating stylish outfits that are visually consistent, among other applications. In domains like clothing and accessories, such considerations are particularly paramount as the visual appearance of items is a critical feature that guides users' decisions. However, existing systems like Amazon and eBay still rely mainly on keyword search and recommending loosely consistent items (e.g. based on co-purchasing or browsing data), without an interface that makes use of visual information to serve the above needs. In this paper, we attempt to fill this gap by designing and implementing an image-based query system, called Fashionista, which provides a graphical interface to help users efficiently explore those items that are not only visually similar to a given query, but which are also fashionable, as determined by visually-aware recommendation approaches. Methodologically, Fashionista learns a low-dimensional visual space as well as the evolution of fashion trends from large corpora of binary feedback data such as purchase histories of Women's Clothing & Accessories from Amazon, which we use for this demonstration.",Fashionista: A Fashion-aware Graphical System for Exploring Visually Similar Items,NA:NA:NA,2016
Johannes Hoffart:Dragan Milchevski:Gerhard Weikum:Avishek Anand:Jaspreet Singh,"Entity search over news, social media and the Web allows users to precisely retrieve concise information about specific people, organizations, movies and their characters, and other kinds of entities. This expressive search mode builds on two major assets: 1) a knowledge base (KB) that contains the entities of interest and 2) entity markup in the documents of interest derived by automatic disambiguation of entity names (NED) and linking names to the KB. These prerequisites are not easily available, though, in the important case when a user is interested in a newly emerging entity (EE) such as new movies, new songs, etc. Automatic methods for detecting and canonicalizing EEs are not nearly at the same level as the NED methods for prominent entities that have rich descriptions in the KB. To overcome this major limitation, we have developed an approach and prototype system that allows searching for EEs in a user-friendly manner. The approach leverages the human in the loop by prompting for user feedback on candidate entities and on characteristic keyphrases for EEs. For convenience and low burden on users, this process is supported by the automatic harvesting oftentative keyphrases. Our demo system shows this interactive process and its high usability.",The Knowledge Awakens: Keeping Knowledge Bases Fresh with Emerging Entities,NA:NA:NA:NA:NA,2016
Helge Holzmann:Avishek Anand,"Limited search and access patterns over Web archives have been well documented. One of the key reasons is the lack of understanding of the user access patterns over such collections, which in turn is attributed to the lack of effective search interfaces. Current search interfaces for Web archives are (a) either purely navigational or (b) have sub-optimal search experience due to ineffective retrieval models or query modeling. We identify that external longitudinal resources, such as social bookmarking data, are crucial sources to identify important and popular websites in the past. To this extent we present Tempas, a tag-based temporal search engine for Web archives. Websites are posted at specific times of interest on several external platforms, such as bookmarking sites like Delicious. Attached tags not only act as relevant descriptors useful for retrieval, but also encode the time of relevance. With Tempas we tackle the challenge of temporally searching a Web archive by indexing tags and time. We allow temporal selections for search terms, rank documents based on their popularity and also provide meaningful query recommendations by exploiting tag-tag and tag-document co-occurrence statistics in arbitrary time windows. Finally, Tempas operates as a fairly non-invasive indexing framework. By not dealing with contents from the actual Web archive it constitutes an attractive and low-overhead approach for quick access into Web archives.",Tempas: Temporal Archive Search Based on Tags,NA:NA,2016
Salman Hooshmand:Akib Mahmud:Gregor V. Bochmann:Muhammad Faheem:Guy-Vincent Jourdan:Russ Couturier:Iosif-Viorel Onut,"We present D-ForenRIA, a distributed forensic tool to automatically reconstruct user-sessions in Rich Internet Applications (RIAs), using solely the full HTTP traces of the sessions as input. D-ForenRIA recovers automatically each browser state, reconstructs the DOMs and re-creates screenshots of what was displayed to the user. The tool also recovers every action taken by the user on each state, including the user-input data. Our application domain is security forensics, where sometimes months-old sessions must be quickly reconstructed for immediate inspection. We will demonstrate our tool on a series of RIAs, including a vulnerable banking application created by IBM Security for testing purposes. In that case study, the attacker visits the vulnerable web site, and exploits several vulnerabilities (SQL-injections, XSS...) to gain access to private information and to perform unauthorized transactions. D-ForenRIA can reconstruct the session, including screenshots of all pages seen by the hacker, DOM of each page and the steps taken for unauthorized login and the inputs hacker exploited for the SQL-injection attack. D-ForenRIA is made efficient by applying advanced reconstruction techniques and by using several browsers concurrently to speed up the reconstruction process. Although we developed D-ForenRIA in the context of security forensics, the tool can also be useful in other contexts such as aided RIAs debugging and automated RIAs scanning.",D-ForenRIA: Distributed Reconstruction of User-Interactions for Rich Internet Applications,NA:NA:NA:NA:NA:NA:NA,2016
Thomas Kowark:Keven Richly:Matthias Uflacker:Hasso Plattner,"Ontology matching enables applications, such as automated data transformation or query rewriting. As it requires domain knowledge, it needs to be carried out by expert users, whose time is scarce and, therefore, should be used efficiently. To this end, the RepMine system presented in this paper does not treat ontology matching as a task of its own, but integrates it into a semi-automated query translation process. By that, users perform a task with immediate benefit for them and simultaneously contribute to alignments between ontologies. Furthermore, the overall task of matching two ontologies is split on a per-query basis and, thus, can be performed incrementally by all system users","Incremental, Per-Query Ontology Matching with RepMine",NA:NA:NA:NA,2016
Jukyoung Lee:Yonghwa Choi:Suhkyung Kim:Seongsoon Kim:Jaewoo Kang,"Many people seek majority opinions by searching for question-answers that are uploaded by others or uploading their own questions on social media sites. However, people have to read through a large number of documents returned by search services to find the majority opinions. Moreover, even when users upload questions on social media sites, they cannot immediately obtain answers. To address these problems, we present Searching Majority Opinions System (SEMO), a novel majority opinion-based search system that uses QA threads uploaded on SNS and cQA websites. SEMO returns entities based on majority opinions for opinion-finding queries in real time. We also tackled a data sparsity problem using a novel query component expansion approach. To prove SEMO's usefulness in finding majority opinions, we implemented a prototype of SEMO for the movie domain. We believe that our method can cause a paradigm shift in opinion-finding query search and help people make decisions. SEMO is available at http://semo.korea.ac.kr/",SEMO: Searching Majority Opinions on Movies using SNS QA Threads,NA:NA:NA:NA:NA,2016
Essam Mansour:Andrei Vlad Sambra:Sandro Hawke:Maged Zereba:Sarven Capadisli:Abdurrahman Ghanem:Ashraf Aboulnaga:Tim Berners-Lee,"Solid is a decentralized platform for social Web applications. In the Solid platform, users' data is managed independently of the applications that create and consume this data. Each user stores their data in a Web-accessible personal online datastore (or pod). Each user can have one or more pods from different pod providers, and can easily switch between providers. Applications access data in users' pods using well defined protocols, and a decentralized authentication and access control mechanism guarantees the privacy of the data. In this decentralized architecture, applications can operate on users' data wherever it is stored. Users control access to their data, and have the option to switch between applications at any time. We will demonstrate the utility of Solid and how it is experienced from the point of view of end users and application developers. For this, we will use a set of Solid servers and multiple Web applications that use these servers. We believe that experience with a concrete platform such as Solid is highly valuable in truly appreciating the power of a decentralized social Web.",A Demonstration of the Solid Platform for Social Web Applications,NA:NA:NA:NA:NA:NA:NA:NA,2016
Luis G. Moyano:Ana Paula Appel:Vagner F. de Santana:Marcia Ito:Thiago D. dos Santos,"Healthcare insurance data represent a rich source of information and has the potential to contribute significantly in guiding business decision making. In this work we present GraPhys, a Graph Analysis platform designed for exploration, visualization and analysis of healthcare insurance data and its corresponding metadata. By taking advantage of relationships contained in healthcare claims data, we are able to apply Graph Analytics methods and algorithms in order to devise useful business metrics to guide data analysis and exploration. Our tool focuses in better understanding physicians, patients and their practices. We illustrate our approach by demonstrating two use cases where we show how graph analytics metrics, combined with other data, may lead to useful insights not directly available to traditional Business Analytics.",GraPhys: Understanding Health Care Insurance Data through Graph Analytics.,NA:NA:NA:NA:NA,2016
Brice Nédelec:Pascal Molli:Achour Mostefaoui,"Real-time collaborative editors are common tools for distributing work across space, time, and organizations. Unfortunately, mainstream editors such as Google Docs rely on central servers and raise privacy and scalability issues. CRATE is a real-time decentralized collaborative editor that runs directly in web browsers thanks to WebRTC. Compared to state-of-the-art, CRATE is the first real-time editor that only requires browsers in order to support collaborative editing and to transparently handle from small to large groups of users. Consequently, CRATE can also be used in massive online lectures, TV shows or large conferences to allow users to share their notes. CRATE's properties rely on two scientific results: (i) a replicated sequence structure with sub-linear upper bound on space complexity; this prevents the editor from running costly distributed garbage collectors, (ii) an adaptive peer sampling protocol; this prevent the editor from oversizing routing tables, hence from letting small networks pay the price of large networks. This paper describes CRATE, its properties and its usage.",CRATE: Writing Stories Together with our Browsers,NA:NA:NA,2016
Giuseppe Pirrò:Alfredo Cuzzocrea,"We describe RECAP, a tool that, given a pair of entities defined in some Knowledge Graph (KG), builds an explanation, that is, a graph (of manageable size) reflecting their relatedness. Explanations enable to discover new knowledge and browse toward other entities of interest. We discuss different kinds of explanations based on information theory and diversity. The KG-agnostic approach adopted by RECAP, which retrieves the necessary information via SPARQL queries, makes it readily usable on a variety of KGs.",RECAP: Building Relatedness Explanations on the Web,NA:NA,2016
Vinodkumar Prabhakaran:MIchael Saltzman:Owen Rambow,"We present the SPIN system, a computational tool to detect linguistic and dialog structure patterns in a social interaction that reveal the underlying power relations between its participants. The SPIN system labels sentences in an interaction with their dialog acts (i.e., communicative intents), detects instances of overt display of power, and predicts social power relations between its participants. We also describe a Google Chrome browser extension, namely gSPIN, to illustrate an exciting use-case of the SPIN system, which will be demonstrated at the demo session during the conference.",How Powerful are You?: gSPIN: Bringing Power Analysis to Your Finger Tips,NA:NA:NA,2016
Christopher Riederer:Daniel Echickson:Stephanie Huang:Augustin Chaintreau,"The ubiquitous availability of location data to smartphone apps and online social networks has caused the collection of such information to grow at an unprecedented rate. However, the discriminative power and potential uses of this data collection is not always clear to the end user. In this work, we present FindYou, a web-based application that gives users the ability to perform a location data privacy audit. FindYou lets users import and visualize the location data collected by popular web services in order to understand what these companies know or can easily infer about them. Additionally, FindYou gives users the option to donate their data to the scientific community, creating new mobile datasets linked to user properties that will be open to use by academic institutions. We hope that FindYou will increase awareness of the privacy issues surrounding the collection and use of location data, the potential problem of ""digital red-lining"", and also create valuable new datasets with the full informed consent of interested users.",FindYou: A Personal Location Privacy Auditing Tool,NA:NA:NA:NA,2016
Martin Saveski:Eric Chu:Soroush Vosoughi:Deb Roy,"Most social network analyses focus on online social networks. While these networks encode important aspects of our lives they fail to capture many real-world social connections. Most of these connections are, in fact, public and known to the members of the community. Mapping them is a task very suitable for crowdsourcing: it is easily broken down in many simple and independent subtasks. Due to the nature of social networks-presence of highly connected nodes and tightly knit groups-if we allow users to map their immediate connections and the connections between them, we will need few participants to map most connections within a community. To this end, we built the Human Atlas, a web-based tool for mapping social networks. To test it, we partially mapped the social network of the MIT Media Lab. We ran a user study and invited members of the community to use the tool. In 4.6 man-hours, 22 participants mapped 984 connections within the lab, demonstrating the potential of the tool.",Human Atlas: A Tool for Mapping Social Networks,NA:NA:NA:NA,2016
Stephan Seufert:Patrick Ernst:Srikanta J. Bedathur:Sarath Kumar Kondreddi:Klaus Berberich:Gerhard Weikum,"We demonstrate InstantEspresso, a system to explain the relationship between two sets of entities in knowledge graphs. Instant-Espresso answers questions of the form. Which European politicians are related to politicians in the United States, and how? or How can one summarize the relationship between China and countries from the Middle East? Each question is specified by two sets of query entities. These sets (e.g. European politicians or United States politicians) can be determined by an initial graph query over a knowledge graph capturing relationships between real-world entities. Instant-Espresso analyzes the (indirect) relationships that connect entities from both sets and provides a user-friendly explanation of the answer in the form of concise subgraphs. These so-called relatedness cores correspond to important event complexes involving entities from the two sets. Our system provides a user interface for the specification of entity sets and displays a visually appealing visualization of the extracted subgraph to the user. The demonstrated system can be used to provide background information on the current state-of-affairs between real-world entities such as politicians, organizations, and the like, e.g. to a journalist preparing an article involving the entities of interest. InstantEspresso is available for an online demonstration at the URL http://espresso.mpi-inf.mpg.de/.",Instant Espresso: Interactive Analysis of Relationships in Knowledge Graphs,NA:NA:NA:NA:NA:NA,2016
Kaisong Song:Ling Chen:Wei Gao:Shi Feng:Daling Wang:Chengqi Zhang,"Microblogging services are playing increasingly important roles in our daily life today. It is useful for microblog users to instantly understand the sentiment of a large number of microblogs posted by their friends and make appropriate response. Despite considerable progress on microblog sentiment classification, most of the existing works ignore the influence of personal distinctions of different microblog users on the sentiments they convey, and none of them has provided real-world personalized sentiment classification systems. Considering personal distinctions in sentiment analysis is natural and necessary as different people have different language habits, personal characters, opinion bias and so on. In this demonstration, we present a live system based on Twitter called PerSentiment, an individuality-dependent sentiment classification system which makes the first attempt to analyze the personalized sentiment of recent tweets and retweets posted by the authenticated user and the users he/she follows. Our system consists of four steps, i.e., requesting tweets via Twitter API, preprocessing collected tweets for extracting features, building personalized sentiment classifier based on a novel and extensible Latent Factor Model (LFM) trained on emoticon-tagged tweets, and finally visualizing the sentiment of friends' tweets to provide a guide for better sentiment understanding.",PerSentiment: A Personalized Sentiment Classification System for Microblog Users,NA:NA:NA:NA:NA:NA,2016
Mehdi Terdjimi:Lionel Médini:Michael Mrissa,Web applications that rely on datasets of limited sizes to handle small but frequent updates and numerous queries have no simple way to define where data should be stored and processed. We propose a reasoning framework that can be integrated in Web applications and is able to perform the same reasoning tasks on both client or server sides. This framework embeds a rule-based reasoning engine that uses an algorithm relying on both incremental reasoning and named graphs. We evaluate the performance of our approach and compare the effects of incremental reasoning and named graphs in different experimental conditions. Results show that our reasoner can significantly reduce response times to INSERT and DELETE queries. During the demo we will exhibit how it can be used to perform reasoning tasks based on client-generated information and improve Web applications with location-agnostic reasoning.,HyLAR+: Improving Hybrid Location-Agnostic Reasoning with Incremental Rule-based Update,NA:NA:NA,2016
Shoko Wakamiya:Adam Jatowt:Yukiko Kawai:Toyokazu Akiyama,"Microblogging has been recently used for detecting common opinions of users at different geographic places. In this paper we propose a novel spatial visualization system for uncovering collective spatial attention and interest of users not at but rather towards different locations. In other words, we aim to answer questions of the type: what do users collectively talk about when they refer to certain geographical places? In addition, we analyze relations between geographical locations from where Twitter users issue messages and the locations they tweet about. This allows answering questions such as: what do users at a certain place commonly talk about when they refer to another geographical place? We demonstrate an online visualization system that supports the interactive analysis of collective spatial attention over time using 4 months' long collection of tweets in USA.",Analyzing Global and Pairwise Collective Spatial Attention for Geo-social Event Detection in Microblogs,NA:NA:NA:NA,2016
Haofen Wang:Zhijia Fang:Tong Ruan,"Recently Web search engines have built knowledge graphs to support entity search and to provide structural summaries called \emph{knowledge cards} for entities mentioned in queries. Different knowledge cards might be complementary or even have conflicts on values of the equivalent property. Thus, it is essential to achieve a more comprehensive fused card from those individual cards representing the same entity. In this paper, we present a system with technical details of card disambiguation, property alignment, value deduplication and card ranking to fuse knowledge cards from various search engines. We further develop a Javascript library called KCF.js based on the card fusion engine and demonstrates its usability via three possible applications.",KCF.js: A Javascript Library for Knowledge Cards Fusion,NA:NA:NA,2016
Oshani Seneviratne:Lalana Kagal:Andrei Sambra,"It is our great pleasure to welcome you to the rebooted Developers Day, at with WWW 2016. The success of the World Wide Web depends on its developers. From Hypertext and Web Browsers to the APIs that extend the capabilities of the Web, developers have played a very important role in making the Web as ubiquitous as it is today. To celebrate this great driving force behind the Web, we have rebooted the Developers Day at the World Wide Web Conference. The presenters at Developers Day are developers who have interesting open-source software to showcase to the Web developer community. The submissions that were accepted for presentation clearly showed an innovative use of technology, a potential to advance the state of the art of the Web, and a real-world application of the software. Some of the presentations include social bots, Web-based payment protocols, collaborative web environments, tracking changes in large knowledge organization systems, and testing frameworks suited for the Web. All of these topics are timely and are of interest to the general Web developer community. The program for the Developers Day consists of a round of lightning talks followed by Birds of a Feather (BoF) sessions where attendees are able to get an in-depth look at the software presented in the form of a hands-on tutorial as well as a discussion of open challenges, next steps, and application areas. We believe that such developer discourse is essential for the advancement of the Web.",Session details: Developers Track,NA:NA:NA,2016
Clayton Allen Davis:Onur Varol:Emilio Ferrara:Alessandro Flammini:Filippo Menczer,"While most online social media accounts are controlled by humans, these platforms also host automated agents called social bots or sybil accounts. Recent literature reported on cases of social bots imitating humans to manipulate discussions, alter the popularity of users, pollute content and spread misinformation, and even perform terrorist propaganda and recruitment actions. Here we present BotOrNot, a publicly-available service that leverages more than one thousand features to evaluate the extent to which a Twitter account exhibits similarity to the known characteristics of social bots. Since its release in May 2014, BotOrNot has served over one million requests via our website and APIs.",BotOrNot: A System to Evaluate Social Bots,NA:NA:NA:NA:NA,2016
Joachim Neubert,"What's new? and ""What has changed?"" are questions users of knowledge organizations systems, such as thesauri, classifications or taxonomies, ask, when new versions of such vocabularies are published. Until recently, it had been difficult for the publishers to provide this information (normally resorting to custom change logging within maintenance applications), and almost impossible for anybody else. With the widespread acceptance of SKOS as the standard publication and exchange format, the situation has changed fundamentally: Exact deltas between two sets of RDF triples can be computed, and the differences can be organized in a meaningful way through SPARQL queries, taking advantage of regular SKOS structures. skos-history combines a script for creating a ""version store"" with queries accessing this store to generate standard reports such as ""added concepts"" or ""changed notations"", or more subtle changes like concept splits, as well as aggregated statistics on certain change types, or a complete change history for a single concept across multiple versions. To allow for interactive sorting, filtering and downloading these reports, and for editing the queries in an IDE-like environment to adapt them to different vocabularies or user needs, other open source libraries are integrated.",skos-history: Exploiting Web Standards for Change Tracking in Knowledge Organization Systems,NA,2016
Michael Nolting:Jan Eike von Seggern,"Data-driven and continuous development and deployment of modern web applications depend critically on registering changes as fast aspossible, paving the way for short innovation cycles. A/B testing is a popular tool for comparing the performance of different variants. Despite the widespread use of A/B tests, there is little research on how to assert the validity of such tests. Even small changes in the application's user base, hard- or software stack not related to the variants under test can transform on possibly hidden paths into significant disturbances of the overall evaluation criterion (OEC) of an A/B test and, hence, invalidate such a test. Therefore, the highly dynamic server and client run-time environments of modern web applications make it difficult to assert correctly the validity of an A/B test. We propose the concept of test context to capture data relevant for the chosen OEC. We use pre-test data for dynamic base-lining of the target space of the system under test and to increase the statistical power. During an A/B experiment, the contexts of each variant are compared to the pre-test context to ensure the validity of the test. We have implemented this method using a generic parameter-free statistical test based on the bootstrap method focussing on frontend performance metrics.",Context-based A/B Test Validation,NA:NA,2016
Evan Schwartz,"The history of the Web is full of attempts to enable micropayments for content and services. All have failed to achieve widespread adoption. This has fueled recurring debates about the merits or fundamental flaws of the concept of asking users to pay small amounts for what they use online. As a result, however, of the Web's lack of native payment infrastructure, the only viable business models concentrate earnings and power in a small group of content and advertising aggregators and increase demand for privacy-infringing technologies. We need to learn from the failures of previous micropayment schemes and we need to create a payment protocol that is of the Web, for the Web. We present a demo browser extension that uses the new Interledger Protocol (ILP) to demonstrate how payments and micropayments can be seamlessly built into the Web. ILP is an open payment protocol for payments across different payment networks that is being developed in the W3C Interledger Community Group. It enables new possibilities for developers and a better experience for users of the Open Web Platform.","A Payment Protocol of the Web, for the Web: Or, Finally Enabling Web Micropayments with the Interledger Protocol",NA,2016
Adrian Hope-Bailie:Stefan Thomas,"The web has enabled free and open information exchange for a vast number of users around the world. However, it has so far failed to do the same for payments. Instead of finding the cheapest route for each payment from a competitive network of providers, we rely on a small number of proprietary operators with global reach. The work happening at the Web Payments Working Group at W3C is attempting to remove some of the friction in performing payments on the Web by defining a standard payment API and messaging in browsers. This will make payments on the Web easier but not entirely frictionless or integrated. As active participants in the W3C's Web Payments Working Group we present a browser polyfill of one of the prosed payment APIs and will walk the audience through the goals of the WG and vision of how payments will work on the Web in the future. Building on this, we will introduce the Interledger Protocol (ILP), a new neutral payments protocol being incubated in the Interledger Payments Community Group, also at the W3C. We will demonstrate how, in the future, the combination of the W3C's Web Payments APIs and the power of ILP payments will not only be frictionless but fully integrated into how we use the Web. Ubiquitous payments in an Internet of Value.",Interledger: Creating a Standard for Payments,NA:NA,2016
Tobias Weller:Maria Maleshkova,"Existing standards in capturing processes concentrate on client tools. Furthermore, semantic information are often available that cannot be captured in a structured way with the proposed standard formats. In addition, processes are usually used and maintained by multiple persons. Therefore, a collaborative platform to discuss and share information about processes is valuable. In order to address the challenge of maintaining and sharing knowledge about processes, we provide a tool to capture and annotate processes using Semantic MediaWiki as a collaborative platform. We demonstrate the practical applicability of our tool by presenting a demo available in the World Wide Web.",Capturing and Annotating Processes using a Collaborative Platform,NA:NA,2016
Eyhab Al-Masri:Tie-Yan Liu,"It is our great pleasure to welcome you to the PhD Symposium that is held in conjunction with the 25th International World Wide Web Conference, April 11 -- April 15, 2016, Montreal, Canada. The PhD Symposium of WWW2016 provides an excellent opportunity for PhD students at different stages in their research to present their ideas, and receive feedback on their work by experienced researchers and other PhD students working in research areas related to the World Wide Web. The call for papers attracted 16 submissions from Brazil, Canada, China, France, Germany, Greece, India, Ireland, United Kingdom, and the United States. The program committee reviewed and accepted 7 papers that cover a variety of topics including search and recommendation, web mining, social networks and graph analysis, crowdsourcing analysis, semantics and big data, among others. We hope that the program will serve as a valuable reference for researchers and developers in the field of World Wide Web. Putting together the WWW2016 PhD Symposium was a team effort. We first thank the authors for their contributions to the program. We must also thank the program committee members for their invaluable efforts in reviewing papers and providing constructive feedback to authors. We are also grateful to the General Chairs, James Hendler and Roger Nkambou, the Local Organization Committee Members and ACM SIGs for their guidance, support and great help in the preparation and organization of this program. We hope that you will find this program interesting and thought-provoking and that the PhD Symposium will continue its excellence and serve as an important forum for PhD candidates around the world to share their original research results in the field of World Wide Web.",Session details: PhD Symposium,NA:NA,2016
Andrejs Abele,"Since the beginning of the Linked Open Data initiative, the number of published open datasets has gradually increased, but the datasets often do not contain description about content such as the dataset domain (e.g., medicine, cancer), when this information is available, it is usually coarse-grained e.g. organic-edunet contains the metadata about a collection of learning objects exposed through the Organic.Edunet portal, but it is classified as Life science. In this work we propose approaches that will provide a detailed description of existing datasets as well as linking assistance when publishing new datasets by generating detailed descriptions of the publishers dataset.",Linked Data Profiling: Identifying the Domain of Datasets Based on Data Content and Metadata,NA,2016
Anupama Aggarwal,"In recent times, online social networks (OSNs) are being used not only to communicate but to also create a public/social image. Artists, celebrities and even common people are using social networks to build their brand value and gain more visibility either amongst a restricted set of people or public. In order to enable user to connect to other users in the OSN and gain following and appreciation from them, various OSNs provide different social metrics to the user such as Facebook likes, Twitter followers and Tumblr reblogs. Hence, these metrics give a sense of social reputation to the OSN user. As more users are trying to leverage social media to create a brand value and become more influential, spammers are luring such users to help manipulate their social reputation with the help of paid service (black markets) or collusion networks. In this work, we aim to build a robust alternate social reputation system and detect users with manipulated social reputation. In order to do so, we first start by understanding the underlying structure of various sources of crowdsourced social reputation manipulation like blackmarkets, supply-driven microtask websites and collusion networks. We then build a mechanism for an early detection of users with manipulated social reputation. Our initial results are encouraging and substantiate the possibility of a robust social reputation system.",Detecting and Mitigating the Effect of Manipulated Reputation on Online Social Networks,NA,2016
Hend Alrasheed,"In graph theory, the δ-hyperbolicity is a global property that shows how close a given graph's structure is to the tree's structure metrically. It embeds multiple properties that facilitate solving several problems that found to be hard in the general graph form. Interestingly, not only that δ-hyperbolicity provides an idea about the structure of the graph, but also it explains how information navigates throughout the network. Therefore, δ-hyperbolicity has several applications in diverse applied fields. My PhD dissertation focuses on analyzing and exploiting structural properties of hyperbolic networks for different applications.",Structural Properties in δ-Hyperbolic Networks: Algorithmic Analysis and Implications,NA,2016
Álvaro García-Recuero,"In this position paper we present the challenge of detecting abuse in a modern Online Social Network (OSN) while balancing data utility and privacy, with the goal of limiting the amount of user sensitive information processed during data collection, extraction and analysis. While we are working with public domain data available in a contemporary OSN, our goal is to design a thorough method for future alternative OSN designs that both protect user's sensitive information and discourage abuse. In this summary, we present initial results for detecting abusive behavior on Twitter. We plan to further investigate the impact of reducing input metadata on the quality of the abuse detection. In addition, we will consider defeating Byzantine behavior by opponents in the system.",Discouraging Abusive Behavior in Privacy-Preserving Online Social Networking Applications,NA,2016
Lu Jiang,"The World Wide Web has been witnessing an explosion of video content. Video data are becoming one of the most valuable sources to assess insights and information. However, existing video search methods are still based on text matching (text-to-text search), and could fail for the huge volumes of videos that have little relevant metadata or no metadata at all. In this paper, we propose an accurate, efficient and scalable semantic search method for Internet videos that allows for intelligent and flexible search schemes over the video content (text-to-video search and text&video-to-video search). To achieve this ambitious goal, we propose several novel methods to improve accuracy and efficiency. The extensive experiments demonstrate that the proposed methods are able to surpass state-of-the-art accuracy and efficiency on multiple datasets. Based on the proposed methods, we implement E-Lamp Lite, the first of its kind large-scale semantic search engine for Internet videos. According to National Institute of Standards and Technology (NIST), it achieved the best accuracy in the TRECVID Multimedia Event Detection (MED) 2013, 2014 and 2015, one of the most representative task for content-based video search. To the best of our knowledge, E-Lamp Lite is the first content-based semantic search system that is capable of indexing and searching a collection of 100 million videos.",Web-scale Multimedia Search for Internet Video Content,NA,2016
Julien Plu,"We identify two main factors that can cause numerous difficulties when developing a generic entity linking system: i) the amount of data currently available on the Web that do not stop to increase and where a large part comes in the form of natural language texts; ii) the velocity at which data is published that may impose to process streams of text in near real-time. Social media platforms such as Twitter, Facebook or LinkedIn become a reliable source of news and play a key role for being aware of events around the world. Encyclopedia and newspaper articles contain general knowledge of our world and they can be used to explain concepts and known entities. Videos can be associated with subtitles and images may have captions. Depending on where a text comes from, it can have different properties such as a specific language, style of writing or topic. In this research, we present a preliminary framework based on a novel hybrid architecture for an entity linking system, that combines methods from the Natural Language Processing (NLP), information retrieval and semantic fields. In particular, we propose a modular approach in order to be as independent as possible of the text to be processed. Our evaluation suggests that this framework can outperform the state-of-the-art systems or show encouraging results on three datasets: OKE2015, #Micropost 2014 and #Micropost 2015. We identify the current limitations and we provide promising future research directions.","Knowledge Extraction in Web Media: At The Frontier of NLP, Machine Learning and Semantics",NA,2016
Julio Vega,"Parkinson's Disease (PD) affects patients' motor and non-motor functionality. Traditional assessment techniques are inaccurate because PD symptoms vary throughout the day and are evaluated in sporadic and subjective sessions. Although recent works have utilised wearable devices to try to overcome these issues, most are unsuitable for following patients regularly for a long time. In contrast, my approach aims to monitor PD continuously in a longitudinal, naturalistic, non-disruptive and non-intrusive way. It uses smartphones to log and transmit over the Internet social, environmental, and interaction data about patients and their surroundings. This data is complemented with other web data sources (i.e., geographical and weather data) and then processed to infer a set of metrics (a latent behavioural variable or LBV) of people's activities and habits. Then, the LBV's trends are measured and mapped to the progression of the disease. As a part of the pilot study to test the proposed methodology, I have collected ~290 million records from 2 patients, making this dataset 34.5x bigger and 4x richer than state-of-the-art sets. I used the collected data to identify six possible PD-related LBVs. This project aims to get a more accurate disease picture and to reduce the physical and psychological burden of traditional assessment methods. Ultimately, the work has the potential to save patients' time and improve the efficiency and effectiveness of health services.","Monitoring Parkinson's Disease Progression Using Behavioural Inferences, Mobile Devices and Web Technologies",NA,2016
Marijn Janssen:Leonidas Anthopoulos:Vishanth Weerakkody,"It is our great pleasure to welcome you to the 2016 ACM Workshop on WEB APPLICATIONS AND SMART CITIES -- AW4City'16, in conjunction with WWW 2016. This second event follows up last year's setup of a premier forum to address web-based application and Apps' design and development in the smart city context, which is a rapidly emerging domain and suggests a steadily evolving dominant market. Such applications are crucial, since they deliver smart services of all types to smart city habitants, visitors and businesses, while they create opportunities for new business installation and growth. Various exemplars are well known across the globe and they usually enable transactions between physical and virtual worlds. The mission of this year's workshop, is to emphasize on the contribution of web applications and Apps to current sustainability smart city challenges like urban efficiency against climate change, economic viability, adoption etc. Our short call for papers attracted submissions from Asia, Europe, and the United States. The program committee, with the contribution of additional scholars performed a blind peer-review process and accepted the following: Venue or Track - Reviewed - 8 Accepted - 6.",Session details: AW4City'16,NA:NA:NA,2016
Beth Coleman,"I argue for a vital relationship between current understanding of Big Data and how we might move toward new modes of mobile application design that draw on affective and generative ""data sets"". Toward this end, I look at projects that use the city as a living laboratory, including mobile apps, social media mapping, and various civic uses of Internet of Things (IoT) and surveillance technology. I chose these case studies for their appropriation of psychogeographic ""small data"" to map a city as they engage affective and immersive aspects of interaction and spatial design.",Generative Mappings of New Data Publics,NA,2016
Alois Paulin,"The paper argues that the Smart City idea lacks grounding in shared base technology and instead yields black-box artefacts. The reliance on black-box systems in public governance is considered a great hazard since it may result in sinecures, stifles democratic control of the public domain, and results in neo-feudal monopolies. Base technology (such as the WWW technology stack) on the other hand is use-neutral, implementation-neutral, open, and teach-/learn-able, thus enabling the emergence of cascading technological ecosystems, which can drive large-scale economic and societal progress. The concepts of a primary, secondary, and tertiary technological ecosystem are introduced to delineate the role and importance of base technology. The paper calls for stronger focus on Smart City foundational research and a change in culture from quick fixes to solutions that would survive generations.",Technological Ecosystems' Role in Preventing Neo-Feudalism in Smart-City Informatization,NA,2016
Iraklis Argyriou,"This paper offers a brief review of basic literature on smart cities to outline research issues in this field that require further attention from a governance perspective. It then maps the current urban planning landscape in China, a country which puts increasing emphasis on smart cities as a mechanism to promote sustainable development, in order to elicit key policy aspects that need to be considered in empirical analysis when ""planning the Chinese smart city"". The paper concludes by introducing a case of local innovation in the area of digital economy, the so-called Dream Town, undertaken in the city of Hangzhou as an illustrative example of urban China's current efforts on planning for smart city development.",Planning the Smart City in China: Key Policy Issues and the Case of Dream Town in the City of Hangzhou,NA,2016
João Pedro Gouveia:Júlia Seixas:George Giannakidis,"This paper presents an innovative analytical framework to address incomplete interpretations and dispersed data of the energy system in cities, which usually generate multiple inefficiencies. Integrative city planning takes the city energy system from the supply to the demand while considering its spatial representativeness, and drives optimal cost-efficient assessment towards future sustainable energy targets. This holistic approach delivers more adequate policies and measures towards higher energy use efficiency. The proposed analytical framework has been developed within the INSMART EU funded project and focuses on data gathering procedures and data processing tools and models, covering a wide range of city's energy consumers, as residential buildings, transport and utilities. The results, mapped into a GIS, can be further exploited either for awareness increase of citizens and for decision support of city energy planners.",Smart City Energy Planning: Integrating Data and Tools,NA:NA:NA,2016
Leonidas G. Anthopoulos:Christopher G. Reddick,"Smart City is an emerging and multidisciplinary domain. It has been recently defined as innovation, not necessarily but mainly through information and communications technologies (ICT), which enhance urban life in terms of people, living, economy, mobility and governance. Smart government is also an emerging topic, which attracts increasing attention from scholars who work in public administration, political and information sciences. There is no widely accepted definition for smart government, but it appears to be the next step of e-government with the use of technology and innovation by governments for better performance. However, it is not clear whether these two terms co-exist or concern different domains. The aim of this paper is to investigate the term smart government and to clarify its meaning in relationship to the smart city. In this respect this paper performed a comprehensive literature review analysis and concluded that smart government is shown not to be synonymous with smart city. Our findings show that smart city has a dimension of smart government, and smart government uses smart city as an area of practice. The authors conclude that smart city is complimentary, part of larger smart government movement.",Smart City and Smart Government: Synonymous or Complementary?,NA:NA,2016
Leonidas Anthopoulos:Marijn Janssen:Vishanth Weerakkody,"Smart services concern the core element of a smart city, since they support the realization of urban ""intelligence"" in terms of people, economy, governance, environment, mobility and leaving. Smart services aim to enhance quality of life within a city and in this respect to improve ""livability"". The types and purposes of smart services cannot be easily pre-defined, since they are the outcome of innovation, which cannot be pre-defined either, but instead it is the product of citizens' and businesses' creativity. However, standard bodies that work on smart city definition have described smart city portfolios, which are suggested to city policy makers and potential entrepreneurs. The aim of this paper is to validate whether standardized smart service portfolios are being followed by smart cities in practice. In this regard, a set of more than 70 smart cities are examined and their smart services are matched to these portfolios. The outcomes are extremely important and leave space for future research in this regard.",Smart Service Portfolios: Do the Cities Follow Standards?,NA:NA:NA,2016
Sudha Ram:Yun Wang:Faiz Currim:Fan Dong:Ezequiel Dantas:Luiz Alberto Sabóia,"Systematic evaluation is crucial to the management and development of smart urban transportation, as it allows transportation planners to better understand the impact of their decisions and design targeted interventions to improve efficiency. Implementation of smart and adaptable public transportation is an important challenge in developing cities and newly industrialized economies where growth characteristics contribute to and can be impacted by factors like overcrowding and travel delays. In this paper, we focus on bus transportation, and present the design and implementation of a 3-layer web-based system for performance evaluation and decision support. This is part of a ""Smart Cities"" initiative, which is an international collaboration between academia and government. The first layer estimates fundamental indicators such as bus travel time and passenger demands by integrating heterogeneous data sources. A novel bus-stop network is then designed in the second layer, which enables the derivation of passenger patterns in public transit using network analysis. The third layer provides decision support by analyzing causal relationships between indicators. The proposed web-based system called SMARTBUS is being developed and validated with the city of Fortaleza in Brazil. We believe the use of generally available urban transportation data makes our methodology adaptable and customizable for other cities.",SMARTBUS: A Web Application for Smart Urban Mobility and Transportation,NA:NA:NA:NA:NA:NA,2016
Feng Xia:Huan Liu:Irwin King:Kuansan Wang,"It is our great pleasure to welcome you to BigScholar 2016, The Third WWW Workshop on Big Scholarly Data: Towards the Web of Scholars. The workshop is held in Montreal, Canada, April 2016, as part of the 25th International World Wide Web Conference (WWW 2016). The BigScholar workshop aims at bringing together researchers and practitioners working on Big Scholarly Data to discuss what are emerging research issues and how to explore the Web of Scholars. Several core challenges, such as the tools and methods for analyzing and mining scholarly data will be the main center of discussions at the workshop. The goal is to contribute to the birth of a community having a shared interest around the Web of Scholars and exploring it using data mining, recommender systems, social network analysis and other appropriate technologies. In response to the call-for-papers, this third edition of the workshop received 22 submissions from Asia, Europe, South America, Canada, and the United States of America. Each paper was reviewed by at least two members of the program committee. As a result of the rigorous review process, 12 high-quality papers were accepted for presentation at the workshop and inclusion in the proceedings. In addition to paper presentations, the workshop also features two Invited Keynote Speeches delivered by Prof. C. Lee Giles from Pennsylvania State University and Prof. Jie Tang from Tsinghua University, respectively.",Session details: BigScholar'16,NA:NA:NA:NA,2016
C. Lee Giles,NA,Scholarly Big Data Knowledge and Semantics,NA,2016
Jie Tang,"AMiner is the second generation of the ArnetMiner system. We focus on developing author-centric analytic and mining tools for gaining a deep understanding of the large and heterogeneous networks formed by authors, papers, venues, and knowledge concepts. One fundamental goal is how to extract and integrate semantics from different sources. We have developed algorithms to automatically extract researchers' profiles from the Web and re- solve the name ambiguity problem, and connect different professional networks. We also developed methodologies to incorporate knowledge from the Wikipedia and other sources into the system to bridge the gap between network science and the web mining research. In this talk, I will focus on answering two fundamental questions for author-centric network analysis: who is who? and who are similar to each other? The system has been in operation since 2006 and has collected more than 100,000,000 author profiles, 100,000,000 publication papers, and 7,800,000 knowledge concepts. It has been widely used for collaboration recommendation, similarity analysis, and community evolution.",AMiner: Mining Deep Knowledge from Big Scholar Data,NA,2016
Chun-Hua Tsai:Yu-Ru Lin,"Academic publication is a key indicator for measuring scholars' scientific productivity and has a crucial impact on their future career. Previous work has identified the positive association between the number of collaborators and academic productivity, which motivates the problem of tracing and predicting potential collaborators for junior scholars. Nevertheless, the insufficient publication record makes current approaches less effective for junior scholars. In this paper, we present an exploratory study of predicting junior scholars' future co-authorship in three different network density. By combining features based on affiliation, geographic and content information, the proposed model significantly outperforms the baseline methods by 12% in terms of sensitivity. Furthermore, the experiment result shows the association between network density and feature selection strategy. Our study sheds light on the re-evaluation of existing approaches to connect scholars in the emerging worldwide Web of Scholars.",Tracing and Predicting Collaboration for Junior Scholars,NA:NA,2016
Hui Shi:Kurt Maly:Dazhi Chong:Gongjun Yan:Wu He,"In the semantic web, content is tagged with ""meaning"" or ""semantics"" to facilitate machine processing and web searching. In general, question answering systems that are built on top of reasoning and inference face a number of difficult issues. In this paper, we analyze scalability issues faced by a question answering system used by a knowledge base with science information that has been harvested from the web. Using this system, we will be able to answer questions that contain qualitative descriptors such as ""groundbreaking"", ""top researcher"", and ""tenurable at university x"". This question answering system has been built using ontologies, reasoning systems and custom based rules for the reasoning system. Furthermore, we evaluated the performance of our optimized backward chaining engine on supporting custom rules and designed the experimental environment including scalable datasets, rule sets, query sets and metrics and compared the experimental results with other in-memory ontology reasoning systems. The results show that our developed backward chaining ontology reasoning system has better scalability than in-memory reasoning systems.",Backward Chaining Ontology Reasoning Systems with Custom Rules,NA:NA:NA:NA:NA,2016
Ian Wesley-Smith:Jevin D. West,"The body of scientific literature is growing at an exponential rate. This expansion of scientific knowledge has increased the need for tools to help users find relevant articles. However, researchers developing new scholarly article recommendation algorithms face two substantial hurdles: acquiring high-quality, large-scale scholarly metadata and mechanisms for evaluating their recommendation algorithms. To address these problems we created Babel---an open-source platform uniting publisher, researchers, and users. Babel includes tens of millions of scholarly articles, several recommendation algorithms, and tools for integrating recommendations into publisher websites and other scholarly platforms.",Babel: A Platform for Facilitating Research in Scholarly Article Discovery,NA:NA,2016
Jun Zhang:Feng Xia:Wei Wang:Xiaomei Bai:Shuo Yu:Teshome Megersa Bekele:Zhong Peng,"Evaluating the scientific impact of scholars has been studied by researchers from various disciplines for a long time. However, very few efforts have been devoted to evaluate the future potential of researchers based on their performance at the initial stage of scientific careers. Academic rising stars represent junior researchers who may not be very outstanding among the peers at the initial stage of their careers, but tend to become influential scholars in the future. In this paper, we propose a novel method named CocaRank, which integrates our proposed new indicator called the collaboration caliber, the typical indicator citation counts and hybrid calculation results on heterogeneous academic networks, to find academic rising stars. In addition, we investigate the appropriate time interval for the prediction of rising stars. The experimental results on real datasets demonstrate that our method can find more top ranked rising stars with higher average citation counts than other state-of-art methods.",CocaRank: A Collaboration Caliber-based Method for Finding Academic Rising Stars,NA:NA:NA:NA:NA:NA:NA,2016
Luam C. Totti:Prasenjit Mitra:Mourad Ouzzani:Mohammed J. Zaki,"Finding a relevant set of publications for a given topic of interest is a challenging problem. We propose a two-stage query-dependent approach for retrieving relevant papers given a keyword-based query. In the first stage, we utilize content similarity to select an initial seed set of publications; we then augment them by citation links weighted with information such as citation context relevance and age-based attenuation. In the second stage, we construct a multi-layer graph that expands the publications subgraph by including links to the authors, venues, and keywords. This allows us to return recommendations that are both highly authoritative, and also textually related to the query. We show that our staged approach gives superior results on three different benchmark query sets.",A Query-oriented Approach for Relevance in Citation Networks,NA:NA:NA:NA,2016
Marlies Olensky:Tsung-Han Tsai:Kuan-Ta Chen,"This study presents the first analysis of h-index sequences on a larger scale. Exemplarily, we investigated researchers from three different fields within Computer Science. We use Google Scholar citation profiles as data source to construct the h-index sequences of individual researchers. Our ultimate goal is to develop a self-evaluation tool, to assess one's own development of the h-index in comparison to other researchers in the same field, maybe identify career role models in the field and assess career development with future chances of success. The results of this study show that the average h-index sequences behave differently for the datasets, which is partly due to the different sample sizes. Hence, further research will be needed to confirm if every research field behaves differently. In addition, we applied the algorithm developed by Wu et al. to our data to classify the h-index sequences of individual authors according to five different shape categories. The majority of researchers has an S-shaped h-index sequence, followed by IS-shaped and linear sequences. Purely concave or convex sequences hardly ever occur. The researchers with the highest h-indices after 10 career years respectively belong to the S-shaped and IS-shaped categories with a few linear category occurrences. Hence, having a linear h-index is not only very hard to achieve, it is also not a guaranty to be the researcher with the highest h-index in a field.",H-index Sequences across Fields: A Comparative Analysis,NA:NA:NA,2016
Po-shen Lee:Jevin D. West:Bill Howe,"We present VizioMetrix, a platform that extracts visual information from the scientific literature and makes it available for use in new information retrieval applications and for studies that look at patterns of visual information across millions of papers. New ideas are conveyed visually in the scientific literature through figures --- diagrams, photos, visualizations, tables --- but these visual elements remain ensconced in the surrounding paper and difficult to use directly to facilitate information discovery tasks or longitudinal analytics. Very few applications in information retrieval, academic search, or bibliometrics make direct use of the figures, and none attempt to recognize and exploit the type of figure, which can be used to augment interactions with a large corpus of scholarly literature. The VizioMetrix platform processes a corpus of documents, classifies the figures, organizes the results into a cloud-hosted databases, and drives three distinct applications to support bibliometric analysis and information retrieval. The first application supports information retrieval tasks by allowing rapid browsing of classified figures. The second application supports longitudinal analysis of visual patterns in the literature and facilitates data mining of these figures. The third application supports crowdsourced tagging of figures to improve classification, augment search, and facilitate new kinds of analyses. Our initial corpus is the entirety of PubMed Central (PMC), and will be released to the public alongside this paper; we welcome other researchers to make use of these resources.",VizioMetrix: A Platform for Analyzing the Visual Information in Big Scholarly Data,NA:NA:NA,2016
Ryan Whalen:Yun Huang:Craig Tanis:Anup Sawant:Brian Uzzi:Noshir Contractor,"Using latent semantic analysis on the full text of scientific articles, we measure the distance between 36 million citing/cited article pairs and chart changes in citation proximity over time. The analysis shows that the mean distance between citing and cited articles has steadily increased since 1990. This demonstrates that current scholars are more likely to cite distantly related research than their peers of 20 years ago who tended to cite more proximate work. These changes coincide with the introduction of new information technologies like the Internet, and the increasing popularity of interdisciplinary and multidisciplinary research. The ""citation distance"" measure shows promise in improving our understanding of the evolution of knowledge. It also offers a method to add nuance to scholarly impact measures by assessing the extent to which an article influences proximate or distant future work.",Citation Distance: Measuring Changes in Scientific Search Strategies,NA:NA:NA:NA:NA:NA,2016
Suhendry Effendy:Roland H.C. Yap,"The rating of Computer Science (CS) conferences are important as it influences how papers published at the conferences and may also be used to evaluate research. In this paper, we proposed a method, \rsit{}, based on a small given set of top conference ({\em pivots}) and a relatedness measure based this set as well as basic baseline methods using citation count and field rating. We experimented with a snapshot dataset from Microsoft Academic Graph together with conference data from Microsoft Academic Search. We evaluated the conference ratings from our methods with the CCF conference rating list. We showed that \rsit{} correlates well with CCF rating and correlates better than ratings from using a baseline ranking with citation count or field rating.",Investigations on Rating Computer Sciences Conferences: An Experiment with the Microsoft Academic Graph Dataset,NA:NA,2016
Yan Wu:Srini Venkat:Dah Ming Chiu,"Citations serve as an important metric for identifying experts and opinion leaders in academic communities. In this paper, we analyze the evolution of yearly rankings of top-cited (C-list) authors in the domain of Computer Science. In searching for factors that help authors become top-cited, we also gather authors in the top-collaboration list (A-list) and top-publication list (P-list) for each year, and analyze cross-correlation of the C-list with the corresponding A-list and P-list for each year. Results show that the A-list and P-list serve as (unreliable) indicators for appearance on the C-list, but their effect is quick and short-lived. Through further case studies we find other key factors, such as the seminal importance of an author's publication and the association on an author's work with hot topic trends, may significantly affect rank dynamics. Based on the study of citation rank dynamics in academia, we then discuss the modeling of rank dynamics, specifically a model based on item visibility and item strength, and the general applicability of such a model.",Get To the Top and Stay There: A Study of Citation Rank Dynamics in Academia,NA:NA:NA,2016
Zhaowei Tan:Changfeng Liu:Yuning Mao:Yunqi Guo:Jiaming Shen:Xinbing Wang,"A large number of papers are being published every year, which makes it difficult for researchers to grasp the relationship among the scientific literatures and the big picture of academic fields. The new challenges have thus been raised, such as analyzing the complicated citation and author network, mining valuable scientific knowledge, and visualizing big scholarly data. The existing academic systems, such as Google Scholar and DBLP have mainly adopted text-based methods, while some other systems make attempts to better navigate the literatures, for example, AMiner and Science Navigation Map. Although these systems show improvements, they fail to present the academic data in a holistic way, and also have limited functions. Therefore, we need to develop new tools which can realize more modules and further explore the academic literatures. In this paper, we conceptualize and design a novel academic system, AceMap, to analyze the big scholarly data and present the results through a ``map'' approach. AceMap integrates several algorithms in the field of network analysis and data mining, and then displays the information in a clear and intuitive way, aiming to help the researchers facilitate their work. After describing the big picture, we present achieved results and our work in progress. By far, AceMap has implemented the following functions: dynamic citation network display, paper clustering, academic genealogy, author and conference homepage, etc. We have also designed and performed distributed network analysis algorithms in a cutting-edge Spark system and utilized modern visualization tools to present the results. Finally, we conclude our paper by proposing the future outlooks.",AceMap: A Novel Approach towards Displaying Relationship among Academic Literatures,NA:NA:NA:NA:NA:NA,2016
Zhiya Zuo:Xi Wang:David Eichmann:Kang Zhao,"Although closely related, multidisciplinarity and interdisciplinarity are different. The former indicates the co-existence of multiple disciplines while the latter is more about the integration among various areas. As collaboration between researchers from different areas is one of the major approaches for interdisciplinarity, this research investigated whether higher levels of multidisciplinarity in academic institutions are related to more collaborations, especially more interdisciplinary collaborations, among its faculty members. Using U.S. iSchools as a case study, we applied social network analysis and text mining techniques to faculty members' educational background and publication data, and proposed metrics for multidisciplinarity and collaboration interdisciplinarity. Our analysis results revealed that the multidisciplinarity of an iSchool is actually negatively correlated with the frequency and interdisciplinarity of research collaborations among its faculty members. This finding suggests that having a multidisciplinary environment alone is not sufficient to promote collaborations, nor interdisciplinary collaborations.",Research Collaborations in Multidisciplinary Institutions: a Case Study of iSchools,NA:NA:NA:NA,2016
Michael Huth:Lotfi ben Othmane:Martin Gilje Jaatun:Edgar Weippl,"It is our great pleasure to welcome you to the Workshop on Empirical Research Methods in Information Security, associated with WWW 2016. This workshop seeks to engage empirical researchers in information security to think critically about the nusage of methods that support such empirical research, to evaluate current practice, and to offer new insights or proposals for establishing methods that enable a fairer and more robust evaluation of empirical research results, even when tensioned with particular constraints of information security such as the restricted access to meaningful data. The call for papers attracted submissions from United States, Asia and Europe. The program committee reviewed and accepted the following: Full Technical Papers Venue or Track Reviewed - 11 Accepted - 5.",Session details: ERMIS'16,NA:NA:NA:NA,2016
Omar Alrawi:Aziz Mohaisen,"Digital certificates are key component of trust used by many operating systems. Modern operating systems implement a form of digital signature verification for various applications, including kernel driver installation, software execution, etc. Digital signatures rely on digital certificates that authenticate the signature, which then verify the validity of a given signature for a signed binary. Malware attempts to subvert the chain of trust through several techniques to achieve execution, evasion, and persistence. In this paper, we examine a large corpus of malware ($3.3$ million samples) to extract digital signatures and their corresponding certificates. We examine several characteristics of the digital certificates to study features in the process of malware authorship that will potentially be used for characterizing and classifying malware. We look at many features including the certificate's chain length, the issue and expiration year, the validity duration of a certificate, the issuing country, validity, top issuing certificate authorities (CAs), and others, highlighting potentially discriminatory features.",Chains of Distrust: Towards Understanding Certificates Used for Signing Malicious Applications,NA:NA,2016
Ping Chen:Lieven Desmet:Christophe Huygens:Wouter Joosen,"As the web rapidly expands and gets integrated into all kinds of business, browsing the web has become an important part of people's daily lives. With the rising importance of various web applications sit in a browser, attackers also shifted their focus towards client-side attacks. To defend against these attacks, numerous client-side security mechanisms for the browser are proposed. The presence of these mechanisms on a website can be used as an indicator of the security awareness and practices of that website. In this paper, through a large-scale analysis of more than 18,000 European websites over two years, we analyze the longitudinal trends of the adoption of client-side security mechanisms. We validate that the most popular websites were adopting new security features quicker that less popular websites in the two year timeframe. By examining the websites based on their business vertical, we observe that the websites in the Finance and Education category are outperforming other verticals in the data set, with respect to the usage of client-side security mechanisms.",Longitudinal Study of the Use of Client-side Security Mechanisms on the European Web,NA:NA:NA:NA,2016
Gokhan Kul:Duc Luong:Ting Xie:Patrick Coonan:Varun Chandola:Oliver Kennedy:Shambhu Upadhyaya,Insider threats to databases in the financial sector have become a very serious and pervasive security problem. This paper proposes a framework to analyze access patterns to databases by clustering SQL queries issued to the database. Our system Ettu works by grouping queries with other similarly structured queries. The small number of intent groups that result can then be efficiently labeled by human operators. We show how our system is designed and how the components of the system work. Our preliminary results show that our system accurately models user intent.,Ettu: Analyzing Query Intents in Corporate Databases,NA:NA:NA:NA:NA:NA:NA,2016
Stefan Marschalek:Manfred Kaiser:Robert Luh:Sebastian Schrittwieser,"Behavioural analysis has become an important method of today's malware research. Malicious software is executed inside a controlled environment where its runtime behaviour can be studied. Recently, we proposed the concept of not only observing individual executables but a computer system as a whole. The basic idea is to identify malware by detecting anomalies in the way a system behaves. In this paper we discuss our methodology for empirical malware research and highlight its strengths and limitations. Furthermore, we explain the challenges we faced during our research and describe our lessons learned.",Empirical Malware Research through Observation of System Behaviour,NA:NA:NA:NA,2016
Wilfried Mayer:Martin Schmiedecker,"For billions of users, today's Internet has become a critical infrastructure for information retrieval, social interaction and online commerce. However, in recent years research has shown that mechanisms to increase security and privacy like HTTPS are seldomly employed by default. With the exception of some notable key players like Google or Facebook, the transition to protecting not only sensitive information flows but all communication content using TLS is still in the early stages. While non-significant portion of the web can be reached securely using an open-source browser extension called HTTPS Everywhere by the EFF, the rules fueling it are so far manually created and maintained by a small set of people. In this paper we present our findings in creating and validating rules for HTTPS Everywhere using crowdsourcing approaches. We created a publicly reachable platform at tlscompare.org to validate new as well as existing rules at large scale. Over a period of approximately 5 months we obtained results for more than 7,500 websites, using multiple seeding approaches. In total, the users of TLScompare spent more than 28 hours of comparing time to validate existing and new rules for HTTPS Everywhere. One of our key findings is that users tend to disagree even regarding binary decisions like whether two websites are similar over port 80 and 443.",TLScompare: Crowdsourcing Rules for HTTPS Everywhere,NA:NA,2016
Martin Pirker:Andreas Nusser,"As the internet continuously expands, information security research is a never ending challenge. It is impossible to know all internet participants, protocols and their applications. Instead, security research focuses on empirically collected real-world data; stores, processes, transforms and analyses the data, in order to learn from it and its anomalies --- security issues --- as they happen. This paper presents one practical work-flow for collection and processing of security related data. It present a hard- and software setup, experiences made, and estimates future developments. This gives others the opportunity to learn and identify areas for improvement, especially those in the early stages of setting up a research project based on empirically gathered data.",A Work-Flow for Empirical Exploration of Security Events,NA:NA,2016
Stefan Dietze:Mathieu d'Aquin:Eelco Herder:Dragan Gasevic:Harald Sack,"Distance teaching and openly available educational resources on the Web are becoming common practices. Public higher education institutions as well as private training organisations increasingly realise the benefits of online resources. In addition, informal learning and knowledge exchange are inherent to the online interactions found on the Web in general. These interactions involve, for instance, learning and knowledgecentric social networks -- such as Bibsonomy, Slideshare or Videolectures -- but also general-purpose social environments such as LinkedIn, where matters related to skills, competence development or training are central concerns of involved stakeholders. These interactions generate a vast amount of informal knowledge resources of varying granularity, as well as indicators for learning and competences, which are currently under-investigated. On the other hand, the widespread adoption of Linked Data principles [2], as well as the more recent widespread adoption of embedded annotations through schema.org, Microformats and RDFa, has led to the availability of vast amounts of semi-structured data [1], which facilitates interpretation and reuse of Web content and data [4]. This includes schemas and vocabularies directly focused on learning (e.g., LRMI, AAISO, BIBO) and more knowledge-oriented datasets, such as the ones gathered by LinkedEducation.org1, LinkedUniversities.org2 and LinkedUp3. These repositories offer data from The Open University (UK), Learning Analytics datasets and resources [3], or the mEducator Linked Educational Resources [5] , as well as general purpose knowledge graphs, such as DBpedia, WordNet RDF. This has led to the creation of an embryonic ""Web of Educational Data"", which is largely focused on sharing semi-structured metadata about resources, but which still lacks sufficient recognition of learning-related activities and knowledge resources that are prevalent in less structured and informal online settings. On the other hand, progress in methods and tools for Entity-centric approaches for analysing and understanding the wealth of data on the Web -- such as entity extraction, linking and retrieval -- have paved the way for the exploration of Web data and knowledge relevant to learning and education. The widespread analysis of both informal and formal learning activities and resources has the potential to fundamentally aid and transform the production, recommendation and consumption of learning services and content. However, widespread take-up of such approaches is still hindered by issues that are both technical as well interdisciplinary. Building on the success of previous editions (LILE 2011-2015) 4, LILE20165 addresses such challenges by providing a forum for researchers and practitioners who make innovative use of Web Data for educational purposes. After extensive peer review (each submission was reviewed by at least three independent reviewers) we were able to select 7 papers for presentation in the program. The workshop would not have been possible without contributions of many people and institutions. We are very thankful to the organizers of the WWW 2016 conference for providing us with the opportunity to organize the workshop, for their excellent collaboration, and for looking after many important logistic issues. We are also very grateful to the members of the program committee for their commitment in reviewing the papers and assuring the good quality of the workshop program. We also thank all authors and invited speakers for their invaluable contributions to the workshop. Of course, great appreciation goes to our sponsors GNOSS6, AFEL and EATEL. We thank all supporters of LILE2016 for making this event possible.",Session details: LILE'16,NA:NA:NA:NA:NA,2016
Rafa Absar:Anatoliy Gruzd:Caroline Haythornthwaite:Drew Paulin,"In this paper, we examine how multiple social media platforms are being used for formal and informal learning by examining data from two connectivist MOOCs (or cMOOCs). Our overarching goal is to develop and evaluate methods for learning analytics to detect and study collaborative learning processes. For this paper, we focus on how to link multiple online identities of learners and their contributions across several social media platforms in order to study their learning behaviours in open online environments. Many challenges were found in collection, processing, and analyzing the data; results are presented here to provide others with insight into such issues for examining data across multiple open media platforms.",Linking Online Identities and Content in Connectivist MOOCs across Multiple Social Media Platforms,NA:NA:NA:NA,2016
Guillaume Durand:Nabil Belacel:Cyril Goutte,"In this paper, we present, discuss and summarize different research works we carried out toward the exploitation of the Web of data for learning and training purpose (Web of learning data). For several years now, we have conducted efforts to explore this main objective through two complementary directions. The first direction is the scalability and particularly the need to develop methods able to provide learners with adequate learning path in the world of big data. The second direction is related to the transition from Web data to Web of learning data and particularly the extraction of cognitive attributes from Web content. For this purpose, we proposed different text mining techniques as well as the development of competency framework engineering tools. Resulting evidence-based techniques allow us to properly evaluate and improve the relationships between learning materials, performance records and student competencies. Although some questions remain unanswered and challenging technology improvements are still required, promising results and developments are arising.",Competency Based Learning in the Web of Learning Data,NA:NA:NA,2016
Yang Liu:Carey Williamson,"Modern educational Web sites often feature a rich assortment of linked media content. In this paper, we present a workload study of such an educational Web site hosted at the University of Calgary. Three main insights emerge from our study. First, educational Web sites can generate large volumes of Internet traffic, even when the number of users is limited. Second, network usage is highly influenced by course-related events, such as midterms and finals. Third, the approach used by the site for displaying videos can have adverse impacts on user experience and network traffic. We demonstrate these effects with active measurement of different Web browser and video player implementations.",Workload Study of a Media-Rich Educational Web Site,NA:NA,2016
Dmitry Mouromtsev:Aleksei Romanov:Dmitry Volchek:Fedor Kozlov,"The paper describes use cases and architecture of the course extraction plugin for the Open edX platform build upon Linked Open Data. The issue of frequent repetitions of educational materials within the MOOC and relativity of recommendation tools for course developers is considered. Comprehensive review of the designed ontology and mapping, as well as evaluation using test courses are given. The last part of the paper discusses new possibilities and opportunities for the future work.",Metadata Extraction from Open edX Online Courses Using Dynamic Mapping of NoSQL Queries,NA:NA:NA:NA,2016
Bruno Elias Penteado,"The advance in quality of public education is a challenge to public managers in contemporary society. In this sense, many studies point to the strong influence of socioeconomical factors in school performance but it is a challenge to select proper data to perform analyses on this matter. In tandem, it has happening a growth in provision of big quantities of educational indicators data, but in isolate cases, and by different agencies of Brazilian government. For this work, we use both education and economic indicators for analysis. The following socioeconomical indicators were selected: municipal human development index (MHDI), social vulnerability index (SVI), Gini coefficient and variables extracted from DBpedia, as part of the connection of this data to the Web of data: GDP per capita and municipal population. These data were used as independent variables to look into their correlations with Brazilian Basic Education Development Index (IDEB) performances at municipal level, supported by the application of linked open data principles. OpenRefine was used to extract the data from different sources, convert to RDF triples and then the mapping of the variables to existing ontologies and vocabularies in this domain, aiming at the reuse of existing semantics. The correlational analysis of the variables showed coherence with the literature about the theme, with significative magnitude between IDEB performances and the indicators related to income and parent education (SVI and HDI), besides moderate relations with the other varibles, except for the municipal population. Finally, the consolidated dataset, enriched by information extracted DBpedia was made available by a SPARQL endpoint for queries of humans and software agents, allowing other applications and researchers to explore the data from other platforms.",Correlational Analysis Between School Performance and Municipal Indicators in Brazil Supported by Linked Open Data,NA,2016
Davide Taibi:Stefan Dietze,"Embedded markup of Web pages have emerged as a significant source of structured data on the Web. In this context, the LRMI initiative has provided a set of vocabulary terms, now part of the schema.org vocabulary, to enable the markup of resources of educational value. In this paper we present a preliminary analysis of the use of LRMI terms on the Web by assessing LRMI-based statements extracted from the Web Data Commons dataset.",Towards Embedded Markup of Learning Resources on the Web: An Initial Quantitative Analysis of LRMI Terms Usage,NA:NA,2016
Shuting Wang:Lei Liu,"Traditional assessment modes usually give identical set of questions to each student, thus are inefficient for students to fix their problems. In order to perform an efficient assessment, we utilize prerequisite concept maps to find students' learning gaps and work on closing these gaps and proposed a two-phase model for concept map construction. Experiments on concept pairs with prerequisite relationships which are manually created show the promise of our proposed method. In order to meet the challenge of using concept maps in automatic assessments, we also derive a top-k concept selection algorithm which allows students to view different numbers of concepts.",Prerequisite Concept Maps Extraction for AutomaticAssessment,NA:NA,2016
Dirk Ahlers:Erik Wilde:Bruno Martins,"It is our great pleasure to welcome you to the 6th International Workshop on Location and the Web, associated with WWW 2016. This is the sixth workshop in its series, having previously been held at WWW (LocWeb 2008), CHI (LocWeb 2009), IoT (LocWeb 2010), CIKM (LocWeb 2014), and WWW (LocWeb 2015). LocWeb continues following its main objective of bringing together a community of researchers at the intersection of location and the Web, serving as a unique venue to integrate different backgrounds and stimulating the exchange of ideas and fostering closer cooperation. LocWeb will provide a topic-specific venue where researchers from different fields, be it data mining, recommendation, search, systems, services, social media, applications, or standards, can discuss and develop the role of location. Its focus lies in Web-scale services and systems facilitating location-aware information access. We aim for a highly interactive, collaborative workshop with ample room for discussion that will explore and advance the geospatial topic. The location topic is understood as a crosscutting issue equally concerning information access, semantics and standards, and Web-scale systems and services. The workshop establishes an integrated venue where the location aspect can be discussed in depth within an interested community. LocWeb follows the main theme of Location-Aware Information Access, with subtopics related to Search, Analytics, Mobility, Apps, Services, and Systems. It is designed to reflect the multitude of fields that demand and utilize location features from an interdisciplinary perspective. The call for papers attracted submissions from Europe, the Americas, Asia, and the Middle East. The program committee reviewed and accepted the following: Venue or Track Reviewed - 5, Accepted - 3. We encourage WWW attendees to attend the three accepted paper presentations, a keynote talk, and a discussion session. The detailed programme will be available on the workshop website.",Session details: LocWeb'16,NA:NA:NA,2016
Luca Maria Aiello,"Our daily urban experiences are the product of our perceptions and senses, yet the complete sensorial range is strikingly absent from urban studies. Sight has been historically privileged over the other senses and urban studies. However, smell and sound have also a huge influence over how we perceive places, they impact our behavior, attitudes and health. Yet, city planning is concerned only with a few bad smells and with limiting noise levels. We propose a new way of capturing nuanced sensorial perceptions of cities from data implicitly generated by social media users and of producing detailed sensorial maps of our cities.",The Sensorial Map of the City,NA,2016
Hakan Bagci:Pinar Karagoz,"The location-based social networks (LBSN) facilitate users to check-in their current location and share it with other users. The accumulated check-in data can be employed for the benefit of users by providing personalized recommendations. In this paper, we propose a random walk based context-aware friend recommendation algorithm (RWCFR). RWCFR considers the current context (i.e. current social relations, personal preferences and current location) of the user to provide personalized recommendations. Our LBSN model is an undirected unweighted graph model that represents users, locations, and their relationships. We build a graph according to the current context of the user depending on this LBSN model. In order to rank the recommendation scores of the users for friend recommendation, a random walk with restart approach is employed. We compare RWCFR with popularity-based, friend-based and expert-based baseline approaches. According to the results, our friend recommendation algorithm outperforms these approaches in all the tests.",Context-Aware Friend Recommendation for Location Based Social Networks using Random Walk,NA:NA,2016
Lisette Espín Noboa:Florian Lemmerich:Philipp Singer:Markus Strohmaier,"Nowadays, human movement in urban spaces can be traced digitally in many cases. It can be observed that movement patterns are not constant, but vary across time and space. In this work, we characterize such spatio-temporal patterns with an innovative combination of two separate approaches that have been utilized for studying human mobility in the past. First, by using non-negative tensor factorization (NTF), we are able to cluster human behavior based on spatio-temporal dimensions. Second, for characterizing these clusters, we propose to use HypTrails, a Bayesian approach for expressing and comparing hypotheses about human trails. To formalize hypotheses, we utilize publicly available Web data (i.e., Foursquare and census data). By studying taxi data in Manhattan, we can discover and characterize human mobility patterns that cannot be identified in a collective analysis. As one example, we find a group of taxi rides that end at locations with a high number of party venues on weekend nights. Our findings argue for a more fine-grained analysis of human mobility in order to make informed decisions for e.g., enhancing urban structures, tailored traffic control and location-based recommender systems.",Discovering and Characterizing Mobility Patterns in Urban Spaces: A Study of Manhattan Taxi Data,NA:NA:NA:NA,2016
Peter Rushforth,"The Web has a long history of Web mapping, being originally described at the first WWW Conference in 1994. Web maps have evolved significantly since then, and patterns have developed. The patterns of modern Web mapping underlie mapping programs across economic sectors. Key issues which remain are that Web standards do not directly address the needs of mapping, and Web mapping standards do not rely on Web architecture. As a result of this lack of collaboration, Web mapping and Web standards continue to evolve independently, with little coordination based on a broader interest. This has led to a situation where newcomers to Web mapping are faced with the problem as to what technologies to use for creating and publishing Web maps, of which the unintended consequence is increasing centralization of Web mapping. This paper documents the results of design and development done by Natural Resources Canada within the scope of the Maps For HTML Community Group. A declarative Web map extension to the HTML standard is proposed, together with a new supporting hypermedia type. Taken together, the proposed standards will progressively support simple to advanced Web map applications, including considerations of layers, projections and feature styling. If widely implemented, the proposed Web standards could help realize the value of the substantial investments in spatial data across all sectors of society. The paper concludes with several propositions drawn from the discussion, and proposed actions to be undertaken by the Maps for HTML Community Group, in which the reader is invited to participate.",Maps for HTML: A New Media Type and Prototype Client for Web Mapping,NA,2016
Martin Atzmueller:Alvin Chin:Christoph Trattner,"In our first workshop on Modeling Social Media (MSM 2010 in Toronto, Canada), we explored various different models of social media ranging from user modeling, hypertext models, software engineering models, sociological models and framework models. In our second workshop (MSM 2011 in Boston, USA), we addressed the user interface aspects of modeling social media. In our third workshop (MSM 2012 in Milwaukee, USA), we looked at the collective intelligence in social media, i.e. making sense of the content and context from social media websites such as Facebook, Twitter, Google+ and Foursquare by banalyzing tweets, tags, blog posts, likes, posts and check-ins, in order to create a new knowledge and semantic meaning. Our fourth workshop (MSM 2013 in Paris, France) then especially considered ""recommender systems"" for social media, also tackling the increasing information overload problem for recommending ""things"" in social media. The workshop in the last two years (MSM 2014 in Seoul, Korea and MSM 2015 in Florence, Italy) focused on mining Big Data on social media and the web. Behavioral analytics is an important topic, e.g., concerning web applications as well as mobile and ubiquitous applications, for understanding user behavior. Following the discussion at our workshop at WWW 2015 we aim to continue our focus on behavioral analytics on social media and the web, however, with a special focus: We aim to go beyond standard analytics approaches and try to answer the ""why"" question, which is often missing in analytical papers. The call for papers attracted 17 submissions, from which we were able to accept 8 submissions (five full papers and three short papers) based on a rigorous reviewing process. The accepted papers cover a variety of topics, including social media and dynamic behavioral analytics, usage analysis, recommendation, and behavior prediction. We hope that these proceedings will serve as a valuable reference for researchers and developers.",Session details: MSM'16,NA:NA:NA,2016
Anatoliy Gruzd,"Computational techniques such as Behavioural Analytics (BA) have been extremely effective at transforming social media data into useful insights for applications such as recommender systems [1] and customer relation management [2]. However, due to the rise of smarter, more sophisticated social bots [3] and the increasing reliance on algorithmic filtering (which nudges online users to make certain choices and take specific actions) [4], it begs the question, if we are using data from social media for modelling, are we modelling human behavior in social media or simply reverse engineering how bots and other algorithms operate?",Who are We Modelling: Bots or Humans?,NA,2016
Martin Atzmueller:Andreas Schmidt:Mark Kibanov,"The analysis of sequential trails and patterns is a prominent research topic. However, typically only explicitly observed trails are considered. In contrast, this paper proposes the DASHTrails approach that enables the modeling and analysis of distribution-adapted sequential trails and hypotheses. It presents a method for deriving transition matrices given a probability distribution over certain events. We demonstrate the applicability of the proposed approach using real-world data in the mobility domain, i.e., car trajectories and spatio-temporal distributions on car accidents.",DASHTrails: An Approach for Modeling and Analysis of Distribution-Adapted Sequential Hypotheses and Trails,NA:NA:NA,2016
Lukas Eberhard:Christoph Trattner,"Social information such as stated interests or geographic check-ins in social networks has shown to be useful in many recommender tasks recently. Although many successful examples exist, not much attention has been put on exploring the extent to which social impact is useful for the task of recommending sellers to buyers in virtual marketplaces. To contribute to this sparse field of research we collected data of a marketplace and a social network in the virtual world of Second Life and introduced several social features and similarity metrics that we used as input for a user-based $k$-nearest neighbor collaborative filtering method. As our results reveal, most of the types of social information and features which we used are useful to tackle the problem we defined. Social information such as joined groups or stated interests are more useful, while others such as places users have been checking in, do not help much for recommending sellers to buyers. Furthermore, we find that some of the features significantly vary in their predictive power over time, while others show more stable behaviors. This research is relevant for researchers interested in recommender systems and online marketplace research as well as for engineers interested in feature engineering.",Recommending Sellers to Buyers in Virtual Marketplaces Leveraging Social Information,NA:NA,2016
Bruce Ferwerda:Markus Schedl:Marko Tkalcic,"Applications increasingly use personality traits to provide a personalized service to the user. To acquire personality, social media trails showed to be a reliable source. However, until now, analysis of social media trails have been focusing on \textit{what} has been disclosed: content of disclosed items. These methods fail to acquire personality when there is a lack of content (non-disclosure). In this study we do not look at the disclosed content, but whether disclosure occurred or not. We extracted 40 items of different Facebook profile sections that users can disclose or not disclose. We asked participants to indicate to which extent they disclose the items in an online survey, and additionally asked them to fill in a personality questionnaire. Among 100 participants we found that users' personality can be predicted by solely looking at whether they disclose particular sections of their profiles. This allows for personality acquisition when content is missing.",Personality Traits and the Relationship with (Non-) Disclosure Behavior on Facebook,NA:NA:NA,2016
Luciano Gallegos:Kristina Lerman:Arhur Huang:David Garcia,"During the last years, researchers explored the geographic and environmental factors that affect happiness. More recently, location-sharing services provided by the social media has given an unprecedented access to geo-located data for studying the interplay between these factors on a much bigger scale. Do location-sharing services help in turn at distinguishing emotions in places within a city? Which aspects contribute better at understanding happier places? To answer these questions, we use data from Foursquare location-sharing service to identify areas within a major US metropolitan area with many check-ins, i.e., areas that people like to use. We then use data from the Twitter microblogging platform to analyze the properties of these areas. Specifically, we have extracted a large corpus of geo-tagged messages, called tweets, from a major metropolitan area and linked them US Census data through their locations. This allows us to measure the sentiment expressed in tweets that are posted from a specific area, and also use that area's demographic properties in analysis. Our results reveal that areas with many check-ins are different from other areas within the metropolitan region. In particular, these areas have happier tweets, which also encourage people living in it or from other areas to commute longer distances to these places. These findings shed light on the influence certain places play within a city regarding people's emotions and mobility, which in turn can be used for city planners for designing happier and more equitable cities.",Geography of Emotion: Where in a City are People Happier?,NA:NA:NA:NA,2016
Taraneh Khazaei:Lu Xiao:Robert Mercer:Atif Khan,"The dichotomy between users' privacy behaviours and their privacy attitudes is a widely observed phenomenon in online social media. Such a disparity can be mainly attributed to the users' lack of awareness about the default privacy setting in social networking websites, which is often open and permissive. This problem has led to a large number of publicly available accounts that may belong to privacy-concerned users. As an initial step toward addressing this issue, we examined whether profile attributes of Twitter users with varying privacy settings are configured differently. As a result of the analysis, a set of features is identified and used to predict user privacy settings. For our best classifier, we obtained an F-score of 0.71, which outperforms the baselines considerably. Hence, profile attributes proved valuable for our task and suggest the possibility of the automatic detection of public accounts intended to be private based on online social footprints.",Privacy Behaviour and Profile Configuration in Twitter,NA:NA:NA:NA,2016
Maja R. Rudolph:Matthew Hoffman:Aaron Hertzmann,"Good recommendations are a key tool to increase user engagement and user satisfaction on many social networks. Here we focus on Behance, a social network for artist from various fields such as typography, street art, industrial design, and fashion. On Behance, the artists can connect by following each other, display their work in online portfolios, and brows each other's work. Each user has a personalized dashboard which is an integral part of the Behance experience. In this work we create a joint behavior model which jointly models the users' viewing behavior and the social network. The joint model which we fit with variational inference is capable of producing both who-to-follow and what-to-view recommendations. We show on real data from Behance that the joint behavior model outperforms a Poisson factorization approach which treats both data sources separately.",A Joint Model for Who-to-Follow and What-to-View Recommendations on Behance,NA:NA:NA,2016
Steffen Schnitzer:Svenja Neitzel:Sebastian Schmidt:Christoph Rensing,"Crowdsourcing platforms support the assignment of jobs while relying on the workers' search capabilities. Recommenders can support the workers' decisions to improve quality and outcome for both worker and requester. A precedent study showed, that many workers expect to get tasks recommended, which are similar to previously finished ones. In order to create genuine task recommendation, similarities between tasks have to be identified and analyzed. Therefore, this work provides an empirical study about how workers perceive task similarities. The perceived task similarities may vary between workers with different cultural background and may depend e.g. on the complexity, required action or the requester of the task.",Perceived Task Similarities for Task Recommendation in Crowdsourcing Systems,NA:NA:NA:NA,2016
Pengyu Wei:Ning Wang,"Vast volumes of online information related to news stories, blogs and online social media have an observable effect on investor's opinions towards financial markets. But do these particular information reflect or impact people's decision-making in investment? This paper investigates whether data generated from Internet usage can be used to predict the movements in the financial market. We provide evidence that data on how often a company's Wikipedia page is being viewed is linked to its subsequent performance in the stock market. We then develop a portfolio in line with the Wikipedia usages and demonstrate that our investment strategy based on Wikipedia views is profitable both financially and statistically. Our finding implies that online web data such as Wikipedia presents an alternative insights on collecting and quantifying investor's sentiments towards financial markets, which can be further employed as a timely approximation of investor's behaviours in decision-making.",Wikipedia and Stock Return: Wikipedia Usage Pattern Helps to Predict the Individual Stock Movement,NA:NA,2016
Mena B. Habib:Florian Kunneman:Maurice van Keulen,"It is our great pleasure to welcome you to the 2nd International Workshop on Natural Language Processing for Informal Text (NLPIT), associated with WWW 2016. The rapid growth of Internet usage in the last two decades adds new challenges to understand the informal user generated content (UGC) on the Internet. Textual UGC refers to textual posts on social media, blogs, emails, chat conversations, instant messages, forums, reviews, or advertisements that are created by endusers of an online system. A large portion of language used on textual UGC is informal. Informal text is the style of writing that disregards language grammars and uses a mixture of abbreviations and context dependent terms. The straightforward application of state-of-the-art Natural Language Processing approaches on informal text typically results in a significantly degraded performance due to the following reasons: the lack of sentence structure; the lack of enough context required; the seldom entities involved; the noisy sparse contents of users' contributions; and the untrusted facts contained. The NLPIT workshop hopes to bring opportunities and challenges involved in informal text processing under the attention of researchers. In particular, we are interested in discussing informal text modeling, normalization, mining, and understanding in addition to various application areas in which UGC is involved. The workshop is a follow-up of the first NLPIT workshop that was held in conjunction with ICWE: the International Conference on Web Engineering held in Rotterdam, The Netherlands, from 23rd to 26th of July 2015. The call for papers attracted submissions from 15 different countries. The program committee reviewed and accepted the following: Venue or Track: Reviewed - 16 Accepted - 6. The workshop started with a keynote presentation given by Raphaël Troncy from EURECOM, France entitled ""Linking Entities for Enriching and Structuring Social Media Content"". The keynote is followed by 6 research presentations. The common theme of the research presentations is, analogous to the first edition of NLPIT, NLP for a multitude of languages. Among them the papers and presentations feature Arabic, Spanish, Russian, Chinese, Yoruba (West Africa), and various variations of English.",Session details: NLPIT'16,NA:NA:NA,2016
Raphaël Troncy,"Social media platforms such as Twitter, Facebook or LinkedIn become a reliable source of news and play a key role for being aware of events around the world. Using social media to recognize, enrich or summarize events is however very challenging. In the first part of this talk, we will present ADEL, a novel hybrid architecture for an adaptive entity linking system, that combines methods from the natural language processing, information retrieval and semantic fields. The framework enables to link all the mentions occurring in a text to their entity counterparts in a knowledge base. It is modular and adaptive since it enables to process text written in different languages and of different kind (newswire, tweets, blog posts, etc.) while entities can be of common types (PERSON, LOCATION and ORGANIZATION) or specific ones (dates, numbers) and be disambiguated in generic or specialized knowledge bases. We will show how ADEL can outperform the state-of-the-art systems on the reference NEEL challenges that happens in the yearly #Micropost workshop (2014-2016). In the second part of this talk, we will present a framework that can collect microposts from more than 12 social platforms and that contain media items, as a result of a query -- for example a trending event. We will then show how we can automatically create different visual storyboards that reflect what users have shared about this particular event. The visualization emphasizes the different aspects of storyboards. A graph view shows the relationships between microposts and topics that we automatically extract, while the timeline view emphasizes the time dimension. The user can watch and interact with the summarized view of all the topics or select a particular one with the additional details. In addition, the states of different views are persistent through the URLs which makes easy sharing possible.",Linking Entities for Enriching and Structuring Social Media Content,NA,2016
Tunde Adegbola,"The Unsupervised induction of morphological rules from a simple list of words in a language of interest is a productive approach to Computational Morphology. The most popular algorithms used for this purpose in the literature are based on the assumption that the relatively high occurrence frequencies of certain word segments described as recurrent partials in a lexicon suggests the existence of morpheme boundaries around such high frequency word segments. Even though this word-segment-frequency approach works well for concatenative morphology, it does not cater for some of the most productive morphological processes in Yorùbá and some other African languages. In this paper, unsupervised induction of the morphological rules of Yorùbá was achieved based on a word-pattern-frequency rather than a word-segment-frequency approach. Words in a Yorùbá lexicon were clustered according to the morphological processes on which their formation are based, producing results that hitherto were achievable only by painstaking rule-based manual classification.",Pattern-based Unsupervised Induction Of Yorùbá Morphology,NA,2016
Jhon Adrián Cerón-Guzmán:Elizabeth León-Guzmán,"Twitter data have brought new opportunities to know what happens in the world in real-time, and conduct studies on the human subjectivity on a diversity of issues and topics at large scale, which would not be feasible using traditional methods. However, as well as these data represent a valuable source, a vast amount of noise can be found in them. Because of the brevity of texts and the widespread use of mobile devices, non-standard word forms abound in tweets, which degrade the performance of Natural Language Processing tools. In this paper, a lexical normalization system of tweets written in Spanish is presented. The system suggests normalization candidates for out-of-vocabulary (OOV) words based on similarity of graphemes or phonemes. Using contextual information, the best correction candidate for a word is selected. Experimental results show that the system correctly detects OOV words and the most of cases suggests the proper corrections. Together with this, results indicate a room for improvement in the correction candidate selection. Compared with other methods, the overall performance of the system is above-average and competitive to different approaches in the literature.",Lexical Normalization of Spanish Tweets,NA:NA,2016
Pedro Miguel Dias Cardoso:Anindya Roy,"Conversations on social media and microblogging websites such as Twitter typically consist of short and noisy texts. Due to the presence of slang, misspellings, and special elements such as hashtags, user mentions and URLs, such texts present a challenging case for the task of language identification. Furthermore, the extensive use of transliteration for languages such as Arabic and Russian that do not use Latin script raises yet another problem. This work studies the performance of language identification algorithms applied to tweets, i.e. short messages on Twitter. It uses a previously trained general purpose language identification model to semi-automatically label a large corpus of tweets - in order to train a tweet-specific language identification model. It gives special attention to text written in transliterated Arabic and Russian.",Language Identification for Social Media: Short Messages and Transliteration,NA:NA,2016
Anna Jørgensen:Anders Søgaard,"We present a suite of 12 datasets for evaluating POS taggers across varieties of English to enable researchers to evaluate the robustness of their models. The suite includes three new datasets, sampled from lyrics from black American hip-hop artists, southeastern American Twitter, and the subtitles from the TV series The Wire. We present an example eval- uation of an off-the-shelf POS tagger across these datasets.",A Test Suite for Evaluating POS Taggers across Varieties of English,NA:NA,2016
Ming Yang:William H. Hsu,"We present a new approach towards capturing topic interests corresponding to all the observed latent topics generated by an author in documents to which he or she has contributed. Topic models based on Latent Dirichlet Allocation (LDA) have been built for this purpose but are brittle as to the number of topics allowed for a collection and for each author of documents within the collection. Meanwhile, topic models based upon Hierarchical Dirichlet Processes (HDPs) allow an arbitrary number of topics to be discovered and generative distributions of interest inferred from text corpora, but this approach is not directly extensible to generative models of authors as contributors to documents with variable topical expertise. Our approach combines an existing HDP framework for learning topics from free text with latent authorship learning within a generative model using author list information. This model adds another layer into the current hierarchy of HDPs to represent topic groups shared by authors, and the document topic distribution is represented as a mixture of topic distribution of its authors. Our model automatically learns author contribution partitions for documents in addition to topics.",HDPauthor: A New Hybrid Author-Topic Model using Latent Dirichlet Allocation and Hierarchical Dirichlet Processes,NA:NA,2016
Qian Zhang:Bruno Goncalves,"Sina Weibo, China's most popular microblogging platform, is considered to be a proxy of Chinese social life. In this study, we contrast the discussions occurring on Sina Weibo and on Chinese language Twitter in order to observe two different strands of Chinese culture: people within China who use Sina Weibo with its government imposed restrictions and those outside that are free to speak completely anonymously. We first propose a simple ad-hoc algorithm to identify topics of Tweets and Weibos. Different from previous works on micro-message topic detection, our algorithm considers topics of the same contents but with different #tags. Our algorithm can also detect topics for Tweets and Weibos without any #tags. Using a large corpus of Weibo and Chinese language tweets, covering the entire year of 2012, we obtain a list of topics using clustered #tags and compare them on two platforms. Surprisingly, we find that there are no common entries among the Top 100 most popular topics. Only 9.2% of tweets correspond to the Top 1000 topics of Weibo, and conversely only 4.4% of weibos were found to discuss the most popular Twitter topics. Our results reveal significant differences in social attention on the two platforms, with most popular topics on Weibo relating to entertainment while most tweets corresponded to cultural or political contents that is practically non existent in Weibo.",Topical differences between Chinese language Twitter and Sina Weibo,NA:NA,2016
Eric Charton:Nizar Ghoula:Marie-Jean Meurs,"It is our great pleasure to welcome you to the 1st Workshop on Open Data for Local Search, associated with WWW 2016. Local search engines are specialized information retrieval systems enabling users to discover amenities and services in their neighborhood (schools, businesses, hospitals, etc.). Developing a local search system still raises scientific questions, as well as very specific technical issues. One of the main problems encountered is the partial availability or even the absence of informative contents related to local actors, merchants or service providers. Introducing open data in the architecture of local search engines supports the identification and collection of structured content. Collaborative data such as those made available by the OpenStreetMap Foundation can be of help to identify new dealers, and improve their geolocation. Semantic Web resources such as DBpedia contain keywords or content for enriching ontologies associated with a local search service. Open data provided by cities or national organizations, such as descriptions of public institutions, opening hours, location of shopping centers are other usable resources. Available open data can be exploited to dramatically improve the design of local search engines and their contents. The aim of this workshop is to explore new fields of investigation both in terms of algorithmic approaches as well as originality of usable data. The workshop focuses on how open data can be used to enhance the capabilities of local search engines. Target audience will include researchers, and professionals interested in: semantic web and open data usage to improve local search, enhance ontologies and their alignment, and discover keywords and concepts geo-content improvement using open data to develop geo-search algorithms, build maps and content, improve and enrich geo-data. information extraction involving open data for knowledge management, named entity, business, and content discovery. The call for papers attracted submissions from the United States, Canada, Switzerland, India, and China. The program committee reviewed and accepted the following: Venue or Track Reviewed - 10 Accepted - 6. We encourage attendees to attend demonstrations, list of which will be available on the website http://od4ls.uqam.ca",Session details: OD4LS'16,NA:NA:NA,2016
Mazen AlObaidi:Khalid Mahmood:Susan Sabra,"Local search engines are a vital part of presence of local businesses on the Internet. Local search engines improvement is an important element to ensure that local businesses can be found by millions of people whom are using web to find services. However, web presence can be disguised or not properly documented. Our approach will improve the effectiveness of local search, and increase the ranking of local businesses. We introduce an approach for enhancing local search engines efficiency in returning more accurate results. Our approach consists of semantically enriching the results of a query using Linked Open Data (LOD) web content. Our preliminary evaluation demonstrated evidence that our approach has a better search with more accurate results.",Semantic Enrichment for Local Search Engine using Linked Open Data,NA:NA:NA,2016
Chuankai An:Dan Rockmore,"Local search helps users find certain types of business units (restaurant, gas stations, hospitals, etc.) in the surrounding area. However, some merchants might not have much online content (e.g. customer reviews, business descriptions, opening hours, telephone numbers, etc.). This can pose a problem for traditional local search algorithms such as vector space based approaches. With this difficulty in mind, in this paper we present an approach to local search that incorporates geographic open data. Using the publicly available {\em Yelp} dataset we are able to uncover patterns that link geographic features and user preferences. From this, we propose a model to infer user preferences that integrates geographic parameters. Through this model and estimation of user preference, we develop a new framework for ``local'' (in the sense of geography) search that offsets the absence of contexts regarding physical business units. Our initial analysis points to the meaningful integration of open geographic data in local search and points out several directions for further research.",Improving Local Search with Open Geographic Data,NA:NA,2016
Eric Charton:Nizar Ghoula:Marie-Jean Meurs,"Local search engines are specialized information retrieval systems enabling users to discover amenities and services in their neighbourhood. Developing a local search system still raises scientific questions, as well as very specific technical issues. Those issues come for example from the lack of information about local events and actors, or the specific form taken by the indexable data. Available open data can be exploited to dramatically improve the design of local search engines and their content. The purpose of this workshop is to explore new fields of investigation both in terms of algorithmic approaches as well as originality of usable data. The workshop focuses on how open data can be used to enhance the capabilities of local search engines.",Open Data for Local Search: Challenges and Perspectives,NA:NA:NA,2016
Robert F. Lytle,"This paper addresses the use of Open Data business licence records in the course of local web search. It assesses the feasibility of increasing the search ranking of authorised service providers and rank reduction or removal of providers with an invalid licence status. A case study was conducted to identify usable results returned with a local search across multiple providers. Result records were then analysed against an applicable Open Data set to determine the usability of the search results for end-users seeking service. A model for adjusting search result ranking is proposed for further analysis. Finally, findings based on the analysis lead to additional proposed research efforts in this space.",Open Data Business Licence Usage to Improve Local Search Engine Content Ranking,NA,2016
Jonathan Milot:Patrick Munroe:Eric Beaudry:Francois Grondin:Guillaume Bourdeau,Lookupia is an intelligent real estate search engine for finding houses optimally geolocated to reach points of interest. It uses data from OpenStreetMap and most of public transit corporations that publish transit schedules in the General Transit Feed Specification (GTFS) format.,Lookupia: An Intelligent Real Estate Search Engine for Finding Houses Optimally Geolocated to Reach Points of Interest,NA:NA:NA:NA:NA,2016
Michael Peterman:Omar Benomar:Hacene Mechedou:Felix-Herve Bachand,"Geocoding is the process of converting addresses to geocoordinates. It is widely used in several fields such as public health to monitor socioeconomic inequalities for example or in Geographical Information Systems (GIS) to be able to use with its provided features. In this work, we describe a method to create an address geocoder from a free and open government street lines data source. The address geocoder transforms a street address into a location typically measured in latitude-longitude coordinates. The address geocoder is used in a search engine to relate spatial data to search results and improve accuracy.",Address Geocoding using Street Profiles for Local Search,NA:NA:NA:NA,2016
Camille Tardy:Laurent Moccozet:Gilles Falquet,"There exist many popular crowdsourcing and social services (Volunteered Geographic Information (VGI)) to share information and documents such as Flickr, Foursquare, Twitter , Facebook, etc. They all use metadata, folksonomy and more importantly a geographic axis with GPS coordinates and/or geographic tags. Using this available folksonomy in VGI services we propose a logical approach to highlight and possibly discover the characteristics of geographic places. The approach is based on the notion of spatial coverage and a model of tags categorization and on their semantic identification, using semantic services such as GeoNames, OpenStreetMap or WordNet. We illustrate our model with Flickr to retrieve the characteristics (function, usage?) of places even if those places have a small number of related photos. Those found characteristics allow tag disambiguation and can be use to complete the semantic gap on places and POIs such as the function of buildings, which can exist in geographic services.",A Simple Tags Categorization Framework Using Spatial Coverage to Discover Geospatial Semantics,NA:NA:NA,2016
Philipp Cimiano:Jean-Michel Dalle:Fabien Gandon,"It is our great pleasure to welcome you to the 1st Workshop on Question Answering And Activity Analysis in Participatory Sites (Q4APS) associated with WWW 2016. This workshop intends to bring together researchers and practitioners of Question Answering sites and services on the Web to present and discuss latest advances in analyzing, supporting and automating tasks of the life-cycle of such applications. The goal is to cover and bring together the different approaches existing in managing and answering natural language questions of users on the Web. This includes methods, models and algorithms from automated question-answering, for question-answering forums mining, as well as for monitoring and management automation. Topics included in the call for papers were the following: question routing, question answering question and answer recommendation question and need analysis, question modeling expert finding, expertise categorization, expertise labelling debate analysis, argument mining, argument schemes moderating support and automation animation fostering, targeted solicitation and notification answer generation, multiple and/or heterogeneous answer sources answer detection and ranking, best answer identification answer building and answer improving questions and expert topic labelling role detection (questioner, answerer, editor, etc.), user modelling social aspects of question answering fact checking, cross-validation, supporting evidence spam or abuse prevention in questions and answers answer personalization challenges, datasets, benchmarks for question-answering evaluation We received seven submissions for the workshop. Each submission received at least three reviews by members of the program committee. On the basis of these reviews, we decided to accept six papers to be presented at the workshop and to be included in the conference proceedings. The paper by Burel et al. ""Structural Normalisation Methods for Improving Best Answer Identification in Question Answering Communities"" investigates the problem of how to identify the best answers in community-based Q/A portals. They propose to apply structural normalization techniques to feature-based best answer identification models. The paper by Jenders et al. ""Which Answer is Best? Predicting Accepted Answers in MOOC Forums"" deals with a similar topic, and investigates the identification of best answers in the context of MOOCS. They present a method that exploits historical data to find best answers for a question. In their paper ""Using Semantics to Search Answers for Unanswered Questions in Q&A Forums"", Singh et al. present an approach that tackles the problem that in many Q/A community sites there are many unanswered questions. To mitigate this problem, they propose a system that finds answered questions that are similar to unanswered questions. Sandor et al. present an approach to the detection of user issues and request types in technical forum question posts. They propose a system that categorizes posts into these types using techniques from discourse analysis. In their paper ""Enriching Topic Modelling with Users' histories for Improving Tag Prediction in Q&A Systems"", Loeckx et al. describe an approach to predict the tags for questions in social Q/A sites such as StackExchange. They propose a model that factors in the user context and show that this extension improves upon a purely textual content-based baseline. Finally, Shekarpour et al., in their paper ""Question Answering on Linked Data: Challenges and Future Directions"", present an overview of open and future challenges in the field of question answering from Linked Data. Presenters are encouraged to bring demos to the workshop to enhance oral discussions and presentations. Acknowledgment. We thank the ANR for the ANR-12-CORD-0026 grant supporting the Ocktopus project and this workshop.",Session details: Q4APS'16,NA:NA:NA,2016
Xavier Amatriain,"Q&A sites like Quora aim at growing the world's knowledge. In order to do this, they need to get the right questions to the right people to answer them, but also the existing answers to people who are interested in them. In order to accomplish this, they need to build a complex ecosystem where issues such as content quality, engagement, demand, interests, or reputation are taken into account. It is not possible to build a system like this unless most of the process are highly automated and scalable. The good news is that using high-quality data you can build machine learning solutions that can help address all of the previous requirements. In this talk I will describe some interesting uses of machine learning for Q&A that range from different recommendation approaches such as personalized ranking to classifiers built to detect duplicate questions or spam. I will describe some of the modeling and feature engineering approaches that go into building these systems. I will also share some of the challenges faced when building such a large-scale knowledge base of human-generated knowledge. Finally, I will describe some of the unresolved research challenges in the Q&A Space. I will use my experience at Quora as the main driving example. Quora is a Q&A site that despite having over 80 million unique visitors a month, it is known for keeping a high-quality of answers and content in general.",Machine Learning for Q&A Sites: State of the Art and Research Directions,NA,2016
Jean-Michel Dalle:Catherine Faron-Zucker:Fabien Gandon:Mathieu Lacage:Zide Meng,"This position paper provides an overview of the OCKTOPUS project whose goal is to increase the social and economic benefit of user-generated content, by transforming it into knowledge which can be shared and reused broadly.","Online Knowledge Triage: Searching, Detecting, Labelling and Orienting User Generated Content",NA:NA:NA:NA:NA,2016
Glenn Boudaer:Johan Loeckx,"The automatic attribution of tags in Question & Answering (Q&A) systems like Stack Exchange can significantly reduce the human effort in tagging as well as improve the consistency among users. Existing approaches typically either rely on Natural Language Processing solely or employ collaborative filtering techniques. In this paper, we attempt to combine the best of both worlds by investigating whether incorporating a personal profile, consisting of a user's history or its social network can significantly improve the predictions of state-of-the-art text-based methods. Our research has found that enriching content-based text features with this personal profile allows to trade-off the precision of predictions for recall and as such improve the ""exact match"" (predicting the number of tags and the tags themselves correctly) in a multi-label setting from a baseline of 18.2% text-only to 54.3%.",Enriching Topic Modelling with Users' Histories for Improving Tag Prediction in Q&A Systems,NA:NA,2016
Gregoire Burel:Paul Mulholland:Harith Alani,"Nowadays, Question Answering (Q&A) websites are popular source of information for finding answers to all kind of questions. Due to this popularity it is critical to help the identification of best answers to existing questions for simplifying the access to relevant information. Although it is possible to identify relatively accurately best answers by using binary classifiers coupled with user, content and thread} features, existing works have generally ignored to incorporate the thread-like structure of Q&A communities in the design of best answer identification predictors and algorithms. This paper investigates this particular issue by studying structural normalisation techniques for improving the accuracy of feature based best answer identification models. Thread-based normalisation methods are introduced for improving the accuracy of identification models by introducing a systematic normalisation approach that normalise predictors by taking into account relations between features and the thread-like structure of Q&A communities. Compared to similar non normalised models, better results are obtained for each of the three communities studied. These results show that structural normalisation methods can improve the identification of best answers compared to non-normalised models.",Structural Normalisation Methods for Improving Best Answer Identification in Question Answering Communities,NA:NA:NA,2016
Maximilian Jenders:Ralf Krestel:Felix Naumann,"Massive Open Online Courses (MOOCs) have grown in reach and importance over the last few years, enabling a vast userbase to enroll in online courses. Besides watching videos, user participate in discussion forums to further their understanding of the course material. As in other community-based question-answering communities, in many MOOC forums a user posting a question can mark the answer they are most satisfied with. In this paper, we present a machine learning model that predicts this accepted answer to a forum question using historical forum data.",Which Answer is Best?: Predicting Accepted Answers in MOOC Forums,NA:NA:NA,2016
Agnes Sandor:Nikolaos Lagos:Ngoc-Phuoc-An Vo:Caroline Brun,"In this paper we propose the detection of user issues and request types in technical forum question posts with a twofold purpose: supporting up-to-date knowledge generation in organizations that provide (semi-) automated customer-care services, and enriching forum metadata in order to enhance the effectiveness of search. We present a categorization system for detecting the proposed question post types based on discourse analysis, and show the advantage of using discourse patterns compared to a baseline relying on standard linguistic features. Besides the detailed description of our method, we also release our annotated corpus to the community.",Identifying User Issues and Request Types in Forum Question Posts Based on Discourse Analysis,NA:NA:NA:NA,2016
Saeedeh Shekarpour:Kemele M. Endris:Ashwini Jaya Kumar:Denis Lukovnikov:Kuldeep Singh:Harsh Thakkar:Christoph Lange,"Question Answering (QA) systems are becoming the inspiring model for the future of search engines. While, recently, datasets underlying QA systems have been promoted from unstructured datasets to structured datasets with semantically highly enriched metadata, question answering systems are still facing serious challenges and are therefore not meeting users' expectations. This paper provides an exhaustive insight of challenges known so far for building QA systems, with a special focus on employing structured data (i.e. knowledge graphs).It thus helps researchers to easily spot gaps to fill with their future research agendas.",Question Answering on Linked Data: Challenges and Future Directions,NA:NA:NA:NA:NA:NA:NA,2016
Priyanka Singh:Elena Simperl,"The expert based question and answering forums are crowdsourced and rely on people to provide answers for questions. This paper focuses on technology based Q&A systems like StackOverflow and Reddit. These websites are popular and yet many questions remain unanswered. The Suman system uses semantic keyword search in combination with traditional text search techniques to find similar questions with answers for unanswered questions. Furthermore, the Suman system also recommends experts who can answer those questions. This helps to narrow down the long tail of unanswered questions. The Suman system utilises Semantic Web and Linked Data technologies to integrate the datasets from two websites, structure them and link them to Linked Data Cloud. It uses available tools to solve name entity disambiguation problem and expands the query term with added semantics. The Suman system was evaluated and results were analysed to show its viability.",Using Semantics to Search Answers for Unanswered Questions in Q&A Forums,NA:NA,2016
Gianmarco De Francisci Morales:Luca Maria Aiello:Symeon Papadopoulos:Haewoon Kwak,"It is our great pleasure to welcome you to the Third Workshop on Social News on the Web -- SNOW 2016 -- held in conjunction with the WWW 2016 conference on April 12th 2016 in Montreal, Canada. In recent years, the topics addressed by SNOW have become very popular among diverse scientific communities. Computer scientists have studied how information spreads and diffuses in social networks. Journalists, social scientists, and economists are typical professionals who can improve their respective fields and practice by adopting technologies of interest to SNOW. In addition, there are emerging regulatory and legal issues that are of interest to law researchers and practitioners. The workshop aims to provide a forum to foster communication between these communities. In particular, the workshop has attracted a number of high-quality submissions on the relationships between online news and social media. With this workshop we aim at continuing a tradition of interdisciplinary exchange and cross-domain fertilization among different research communities. The goal of the workshop is to share novel ideas and to discuss future directions in the emerging areas of news search, news mining, news verification, and news recommendation. It especially focuses on the interplay between news content, generated by professional journalists, and social media content, generated by millions of users in real time and subject to social media dynamics. We also welcome investigations on the topics of citizen journalism and computational journalism. SNOW aspires to give researchers and practitioners a unique opportunity to share their perspectives with others interested in news and social media. The call for papers attracted 11 submissions from Asia, America, and Europe. The program committee accepted 6 papers that cover a variety of topics, including news credibility and verification, robot journalism, news consumption and social sharing, and news production on social media. The major criterion for the selection of papers was their potential to generate discussion and influence future research directions. The program also includes four keynote talks from distinguished experts in the field, with themes ranging from event understanding and narration to tracking misinformation, from curation of online news comments to the role of journalism on social media. We hope you will find this program interesting and thought provoking, and that the workshop will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.",Session details: SNOW'16,NA:NA:NA:NA,2016
Nicholas Diakopoulos,"National news outlets routinely publish articles that attract hundreds and even thousands of user comments. These comments often provide valuable feedback and critique, personal perspectives, new information and expertise, and opportunities for discussion (not to mention profanity and vitriol). The varying quality of comments demands a high level of moderation and curatorial attention in order to cultivate a successful online community around news. Amongst publishers there is a growing awareness that finding and publicly highlighting high quality comments can in turn promote the general quality of the discourse. Further journalistic value can be gleaned by identifying and developing new sources of information and expertise from comments. In this talk I will present an editorially-aware visual analytics system called CommentIQ that supports moderators in curating high quality news comments at scale. The possibilities and ramifications of algorithmically infused social media moderation will be discussed in terms of journalistic ideals and norms of free speech and inclusion.",CommentIQ: Enhancing Journalistic Curation of Online News Comments,NA,2016
Filippo Menczer,"As social media become major channels for the diffusion of news and information, they are also increasingly attractive and targeted for abuse and manipulation. This talk overviews ongoing network analytics, data mining, and modeling efforts to understand the spread of misinformation online and offline. I present machine learning methods to detect astroturf and social bots, and outline initial steps toward computational fact checking, as well as theoretical models to study how truthful and truthy facts compete for our collective attention. These efforts will be framed by a case study in which, ironically, our own research became the target of a coordinated disinformation campaign.",The Spread of Misinformation in Social Media,NA,2016
Devipsita Bhattacharya:Sudha Ram,"Social media has emerged as a mechanism for online news propagation. This in turn has changed the competitive landscape of news providers, a landscape that was previously partitioned based on the traditional channels of news dispersion. The channels of news distribution refer to - television, newspaper, magazine, radio, news agency and online only. In this paper, we examine similarities and differences in news propagation patterns on social media based on the primary channel of a news provider. We collected news article propagation activity data from Twitter for 32 news providers over a three-week period and analyzed their propagation networks. Our analysis shows that the structural properties of the propagation networks are statistically different based on the type of primary channel. Our study has useful implications for understanding the competition between news providers in an online environment.",Understanding the Competitive Landscape of News Providers on Social Media,NA:NA,2016
Ram Meshulam:Roy Sasson,"This paper analyzes two types of user interactions with online content: (1) private engagement with content, measured by page-views and click-through rate; and (2) social engagement, measured by the number of shares on Facebook as well as share-rate. Based on more than a billion data points across hundreds of publishers worldwide and two time periods, it is shown that the correlation between these signals is generally low. Potential reasons for the low correlation are discussed, and the notion of private-social dissonance is defined. A more in-depth analysis shows that the dissonance between private engagement and social engagement consistently depends on content category. Categories such as Sex, Crime and Celebrities have higher private engagement than social engagement. On the other hand, categories such as Books, Careers and Music have higher social engagement than private engagement. In addition to the offline analysis, a model which utilizes the different signals was trained and deployed on a live recommendation system. The resulting weights ranked the social signal lower than click-through rate. The results are relevant for publishers, content marketers, architects of recommendation systems and researchers who wish to use social signals in order to measure and predict user engagement.",For Your Eyes Only: Consuming vs. Sharing Content,NA:NA,2016
Pieter-Jan Ombelet:Aleksandra Kuczerawy:Peggy Valcke,"Algorithmic processes that convert data into narrative news texts allow news rooms to publish stories with limited to no human intervention (Carlson, 2015, p. 416). The new trend creates many opportunities, but also raises significant legal questions. Aside from financial benefits, further refinement could make the smart algorithms capable of writing less standard, maybe even opinion, pieces. The responsible human merely needs to define clear questions about what the algorithm needs to discuss in the article and in what manner. But how does it square with the traditional rules of publishing, editorial control and the privacy and data protection framework? This paper analyses the legal implications when employing robot journalists. More specifically, the question of authorship for algorithmic output and the liability issues that could arise when the algorithmic output includes unlawful personal data processing as well as inaccurate, harmful or even illegal content will be assessed. The analysis is performed analyzing European legislation on copyright and data protection and applying Belgian legislation on press liability as a consistent country example to support certain legal considerations and conclusions. Furthermore, the paper answers the question as to how publishers could prevent the creation of inaccurate content by the algorithms they use.","Employing Robot Journalists: Legal Implications, Considerations and Recommendations",NA:NA:NA,2016
Georgios Rizos:Symeon Papadopoulos:Yiannis Kompatsiaris,"The paper presents a framework for the prediction of several news story popularity indicators, such as comment count, number of users, vote score and a measure of controversiality. The framework employs a feature engineering approach, focusing on features from two sources of social interactions inherent in online discussions: the comment tree and the user graph. We show that the proposed graph-based features capture the complexities of both these social interaction graphs and lead to improvements on the prediction of all popularity indicators in three online news post datasets and to significant improvement on the task of identifying controversial stories. Specifically, we noted a 5% relative improvement in mean square error for controversiality prediction on a news-focused Reddit dataset compared to a method employing only rudimentary comment tree features that were used by past studies.",Predicting News Popularity by Mining Online Discussions,NA:NA:NA,2016
Stephen Schifferes,The recent transformation of news is one of a series of major revolutions in news delivery. By studying previous episodes we may find some clues as to the future of social news on the web.,Technological Transformations of News: A Long Term Perspective,NA,2016
Chengcheng Shao:Giovanni Luca Ciampaglia:Alessandro Flammini:Filippo Menczer,"Massive amounts of misinformation have been observed to spread in uncontrolled fashion across social media. Examples include rumors, hoaxes, fake news, and conspiracy theories. At the same time, several journalistic organizations devote significant efforts to high-quality fact checking of online claims. The resulting information cascades contain instances of both accurate and inaccurate information, unfold over multiple time scales, and often reach audiences of considerable size. All these factors pose challenges for the study of the social dynamics of online news sharing. Here we introduce Hoaxy, a platform for the collection, detection, and analysis of online misinformation and its related fact-checking efforts. We discuss the design of the platform and present a preliminary analysis of a sample of public tweets containing both fake news and fact checking. We find that, in the aggregate, the sharing of fact-checking content typically lags that of misinformation by 10-20 hours. Moreover, fake news are dominated by very active users, while fact checking is a more grass-roots activity. With the increasing risks connected to massive online misinformation, social news observatories have the potential to help researchers, journalists, and the general public understand the dynamics of real and fake news sharing.",Hoaxy: A Platform for Tracking Online Misinformation,NA:NA:NA:NA,2016
Stefanie Wiegand:Stuart E. Middleton,"Social media sources are becoming increasingly important in journalism. Under breaking news deadlines semi-automated support for identification and verification of content is critical. We describe a large scale content-level analysis of over 6 million Twitter, You Tube and Instagram records covering the first 6 hours of the November 2015 Paris shootings. We ground our analysis by tracing how 5 ground truth images used in actual news reports went viral. We look at velocity of newsworthy content and its veracity with regards trusted source attribution. We also examine temporal segmentation combined with statistical frequency counters to identify likely eyewitness content for input to real-time breaking content feeds. Our results suggest attribution to trusted sources might be a good indicator of content veracity, and that temporal segmentation coupled with frequency statistical metrics could be used to highlight in real-time eyewitness content if applied with some additional text filters.",Veracity and Velocity of Social Media Content during Breaking News: Analysis of November 2015 Paris Shootings,NA:NA,2016
Sir Nigel Shadbolt:Elena Simperl:Thanassis Tiropanis,"It is our great pleasure to welcome you to the 4th International Workshop on the Theory and Practice of Social Machines, associated with WWW 2016. The workshop is a continuation of 3rd International Workshop on the Theory and Practice of Social Machines (SOCM2015), held at WWW2015. Continuing from previous years' ""Theory and Practice of Social Machines"" workshops at WWW2013, 2014, and 2015, the 2016 edition of the SOCM workshop will look deeply at social machines that have, or may yet soon have, a profound impact on the lives of individuals, businesses, governments, and the society as a whole. Our goal is to discuss issues pertinent to the observation of both extant and yet unrealized social machines building on work of the Web Observatory Workshops of the last two years (WOW2013 and WOW2014). SOCM2016 aims to identify factors that govern the growth or impede these systems to develop, and to identify unmet observation needs or the kinds of loosely-coordinated distributed social systems the Web enables. We also intend to discuss methods to analyze and explore social machines, as essential mechanisms for deriving the guidelines and best practices that will inform the design of social machine observatories. The workshop fosters multidisciplinary discussion on the following areas: Analyzing social machines: analytics and visualisations that provide insights about social machines and their impact. Designing social machine observatories: analyses of the design of effective (extant and future) social machines. Methodology and methods: papers describing approaches and methods for observing social machines. Philosophical Theories and Framework: describing and analysing the philosophical implications of social machines. The call for papers attracted submissions from United States and Europe. The program committee reviewed and accepted the following: Paper Submissions Reviewed - 8 Accepted - 8.",Session details: SOCM'16,NA:NA:NA,2016
Dirk Ahlers:Patrick Driscoll:Erica Löfström:John Krogstie:Annemie Wyckmans,"Smart Cities denote a stronger integration of information technology into the organisation of a city and the interaction and participation of its citizens. In developing the concept further, we propose to understand Smart Cities through the lens of Social Machines and thus stronger focus on the city as a socio-technical construct. We draw from an interdisciplinary background of computer science and urban planning to reexamine and combine existing theories and find a common understanding. We substantiate our claim to the validity of the concept of Smart-City-as-a-Social-Machine with a thorough literature study and comparison. We discuss the resulting system complexity issues and ways to address them. We further propose areas where this understanding can be useful in furthering research on both the Smart City and the Social Machine topics.",Understanding Smart Cities as Social Machines,NA:NA:NA:NA:NA,2016
Sally A. Applin:Michael D. Fischer,"As humans become more and more immersed in a networked world of connected and mobile devices, cooperation and sociability to achieve valued outcomes within geographic locales appears to be waning in favour of extended personal networks and interaction using semi-automated agents to support communications, transportation and other services. From a messaging structure that is complex, multiplexed and much of the time asynchronous, conditions emerge that disrupt symmetry of information exchange. People thus encounter circumstances that seem unpredictable given the information available to them, resulting in limited or failed cooperation and consequent quality of outcomes. We explore the role of Social Machines to support, change, and enhance human cooperation within a blended reality context.",Exploring Cooperation with Social Machines,NA:NA,2016
Caroline A. Halcrow:Leslie Carr:Susan Halford,"Online/offline community (O/OC), the integrated performance of community in a blend of online/offline activities is increasingly prevalent as online systems organise, mediate and broadcast forms of communal engagement. O/OCs are social machines where the focus is on the social achievement, rather than the computational outcomes, of the combined human-technical infrastructure. An O/OC model SPENCE is proposed as an analytical tool for describing social machines from the perspective of sociality. Twitter is a technical infrastructure and social network of shared online/offline community phenomena that is also a social machine combining social participation with conventional forms of machine-based computation. Drawing from the extensive Twitter research literature, a sample of papers are analysed against SPENCE, demonstrating the clarity of the organisation of inter-relating themes of a range of perspectives in current Twitter research. It is concluded that SPENCE provides a lens of synthesis for the sociality dimension of a social machine and can be used in taxonomic activities (such as the social machines observatory) to differentiate social machines.",Using the SPENCE Model of Online/Offline Community to Analyse Sociality of Social Machines,NA:NA:NA,2016
Aastha Madaan:Thanassis Tiropanis:Srinath Srinivasa:Wendy Hall,"The Web observatory is proposed as a global catalogue for sharing data-sets and analytic applications to support researchers from a variety of disciplines for analysing huge amount of research data for Web Science research. However, often these users fail to understand various transformations and consequences of complex data processing involved in a data analytic application. Therefore, there is a need to enable these users develop and re-use analytic applications on web observatory. In this study, we propose formal design patterns called ""Observlets"" for analytic applications to ""observe"" various web phenomena. The observlets provide abstract definitions for intermediate analysis required for a data analytic application. The users can share observlets across distributed web observatory nodes. The observlets are aimed to enhance end-users' awareness and engagement on web observatory and support programmers for innovating various data analytic applications.",Observlets: Empowering Analytical Observations on Web Observatory,NA:NA:NA:NA,2016
Paul Matthews,"This paper presents research into the way norms are developed, expressed and enforced on sites that are part of the Stack Exchange (SE) social question-answering network. This network has a number of topical knowledge exchange communities using similar underlying software, enabling a focus on variation in social design. SE also separates community-related discussion from topic-specific content through the use of its ""Meta"" sub-sites. These were analysed together with their main sites for variation in the development and enforcement of norms. Norms expressed through explicit community policies seem rather less important than those embodied in busy discussion threads on the Meta sites. While Meta participation was fairly uniform across communities, different emphasis on scope and quality led to variation in Meta discussion and the way that norms were enacted through question closures. The social distribution of moderation work was also uneven between sites, with some sites having a few highly active moderators involved in question closure. The level of closures across the sites studied did not seem to significantly discourage participation. Indeed, modelling the effect of closures on quality and engagement indicated that low levels of closure enable ""legitimate peripheral participation"", the process by which newcomers can become inducted and make contributions of increasing quality over time.",Going Meta: Norm Formation and Enactment on the Stack Exchange Network,NA,2016
Arpit Merchant:Tushant Jha:Navjyoti Singh,"Trust plays an important role in the effective working of Social Machines by allowing for cooperative behaviour amongst human and digital components of the system. A detailed study of trust helps in gaining insights into the working of social machines, and allows designers to create better systems which are able to engage more people and allow for efficient operations. In this paper, we undertake a discussion on the variety of ways in which trust can be observed in Social Machines by outlining a three class taxonomy (personal, social and functional). We build upon earlier observations in past literature while seeking a broader definition. We discuss the problem of trust, that of promoting trust amongst the trustworthy in social machines, and present the various insights, challenges and frontiers that arise in response. This includes the role of institutions, communication processes and value aligned technologies in social, personal and functional trust respectively.",The Use of Trust in Social Machines,NA:NA:NA,2016
Shivani Poddar:Sindhu Kiranmai Ernala:Navjyoti Singh:Ashin Samvara,"It is the activities of individuals that lead to formation and changes within any social system. Hence, the ""social"" component in a social machine (socio-technical machine) can be understood constructively from the conception of an individual as a machine. In this work, we present a stochastic finite state machine model of an individual based on Abhidhamma tradition of Buddhism. The machine models moment to moment states of consciousness of an individual in terms of the Buddhist formal ontology that constitutes an individual. Thus, the key contribution of our research is a ubiquitous framework of an individual which unifies the idea of a human agent across all possible social machines. It is shown that from web data of a particular individual this machine can be populated. We expound how our model solves issues pertaining to varied temporal granules and sparsity of data. We further illustrate through an example as to how our approach can unify the conceptualizations of an individual from the numerous ideologies and definitions of a social machine. As a part of our future work, we hope to align this proposed stochastic machine with social observatories on internet.",Towards a Ubiquitous Model of an Individual in Social Machines,NA:NA:NA:NA,2016
Jun Zhao:Reuben Binns:Max Van Kleek:Nigel Shadbolt,"Privacy protection is one of the most prominent concerns for web users. Despite numerous efforts, users remain powerless in controlling how their personal information should be used and by whom, and find limited options to actually opt-out of dominating service providers, who often process users information with limited transparency or respect for their privacy preferences. Privacy languages are designed to express the privacy-related preferences of users and the practices of organisations, in order to establish a privacy-preserved data handling protocol. However, in practice there has been limited adoption of these languages, by either users or data controllers. This survey paper attempts to understand the strengths and limitations of existing policy languages, focusing on their capacity of enabling users to express their privacy preferences. Our preliminary results show a lack of focus on normal web users, in both language design and their tooling design. This systematic survey lays the ground work for future privacy protection designs that aim to be centred around web users for empowering their control of data privacy.",Privacy Languages: Are we there yet to enable user controls?,NA:NA:NA:NA,2016
Kristine Maria Gloria:Stéphane B. Bazan:Su White,"It is our great pleasure to welcome you to the 1st Workshop on ""Web Education"": Teaching Digital Literacies associated with WWW 2016. The dynamics of Web Education and Digital Literacies are among today's most important issues surrounding the development of the Web as an efficient, safe and universal information system. A wide range of disciplines including sociology, economics, political studies, health and management science have integrated courses and specializations to teach about the Web, its nature, its realities, its impact its evolution and its integration into every dimension of human activity. The workshop will gather a very broad community of participants: professors involved in digital literacy programs or courses, consultants empowering employees in a company, students or faculty in an interdisciplinary program or activists in an NGO teaching the Web to kids. The call for short papers attracted submissions from Asia, Europe and Canada. The program committee has reviewed and accepted 7 submissions. The workshop will also be preceded and followed by online activities on the Bookwitty.com platform. These activities aim to not only strengthen links within the Web Education Community, but also to gather and present the outcomes of the workshop.",Session details: TeachWeb'16,NA:NA:NA,2016
Elisabeth Coskun:Su White,"This text describes a project which aims to explore the scope of the discipline Web Science; an emerging subject which is fundamentally inter-disciplinary. There are very few definitive subject definitions currently available for Web Science. Additionally, the nature of the subject is constantly evolving as an increasing number of different disciplines begin to practice what might identifiably be called Web Science. This potentially provides educators and students with a problem; how do you teach or learn about Web Science when there is no clear definition? This text provides a brief overview of a PhD project, the final aim of which involves the emergence of a framework for a working definition of Web Science. This will be achieved by an examination and overview of current existing Web Science curricula, as well as available Web Science literature.",Emerging a Web Science Curriculum,NA:NA,2016
Lisa J. Harris:Nicholas S.R. Fair:Sarah Hewitt,"This paper introduces the theoretical framework and design rationale for an innovative undergraduate module entitled ""Living and Working on the Web"" at the University of Southampton. The module design is based on the principles of collaborative social learning and the co-construction of knowledge. At the workshop a model of best practice will be presented, featuring a ""blog-comment-reflect-feedback"" cycle, which has derived from the synthesis of relevant literature and which will be reflected upon through an informal content analysis of the students' blogs. One of the problems facing this type of curriculum innovation is the difficulties faced when scaling up such modules to very large student groups, particularly in relation to feedback and assessment. To date, the single largest cohort has been 45 students. It is therefore also the intention of the presenters to engage the attendees in a discussion of how a module such as this could be feasibly extended into far larger cohort groups.",Collaborative Social Learning: Rewards and Challenges in Mainstream Higher Education,NA:NA:NA,2016
Ricardo Hoar:Randy Connolly,"Web development is widely considered to be a difficult topic to teach successfully within post-secondary computing programs. One reason for this difficulty is the large number of shifting technologies that need to be taught along with the conceptual complexity that needs to be mastered by both student and professor. Another challenge is helping students see the scope of web development, and their role in an era where the web is a part of everyday human affairs. This paper describes our 2014 textbook [2], its reception, and our plans for a second edition revision (which will be published in early 2017). Our hope is that a discussion of current teaching materials will provide opportunities to spark a dialogue not only about current technological topics, but perspectives on web development from other disciplines.",The Garden of Earthly Delights: Constructing and Revising a Web Development Textbook,NA:NA,2016
Andreja Istenič Starčič:Žiga Turk,"In this paper, we discuss digital literacy and preservice teacher education and reflect on the current state of web-based education and development of digital literacy in teacher education in Slovenia. The literacy context is discussed in the context of the Educational Technology course which is delivered in teacher education. The aim of this course for preservice preprimary and primary classroom teachers is to develop student teachers' digital literacy and prepare them for the efficient integration of ICT into their teaching. This will in turn influence learners' digital literacy and competence for active engagement in the emerging culture of participation. The paper discusses web-based teaching methodology with a focus on instructional design, learning resources and high-order learning outcomes. The affordance of mobile technology fosters ubiquitous learning which has been integrated into the teacher education curriculum. The notion of tools in learning and literacies is discussed in the context of the transition from traditional written culture to digital culture. In integrating mobile learning, three important dimensions converge, underlining the development of digital literacy: the technology dimension of wireless mobile providing instant access, the social dimension and the learning behaviours dimension. A survey was conducted to examine undergraduate student-teachers' attitudes on the application of ubiquitous education and the development of digital literacy through the integration of mobile learning. The results indicate that student-teachers have developed competences in a variety of mobile learning and teaching activities They believe that mobile technology increases connection between learner and teacher but are neutral about the integration of children's social practices from their free time to school environment.",Ubiquitous Learning and Digital Literacy Practices Connecting Teacher and Learner,NA:NA,2016
Brigitte Jellinek,"In this position paper, we describe our experience in designing and delivering a bachelor and a master program in web development at Salzburg University of Applied Sciences, Austria. While aiming to archive similar learning objectives as other programs in web development or web engineering, our historical roots in an arts program and our decision to focus on dynamic programming languages have led us to a unique program.",Experiences with Curricula for a BSc and MSc in Web Development: 2008-2016,NA,2016
Kate Mori:Lucy Ractliffe,"This paper evaluates the effectiveness of a massive open online course (MOOC) as a professional development tool in higher education. The transition from the MOOC's initial intended use as a low cost way for students to access education and aid their studies has evolved to facilitate continuing professional development (CPD), particularly within the commercial sector [1]. Findings from this study indicate there is an increase in participation and satisfaction amongst higher education staff who undertook a MOOC compared to attending traditional staff development days. Recommendations from this study?s findings highlight that staff were keen to engage with the MOOC format, but felt they needed face-to-face meetings as well to reinforce, contextualize and discuss the key messages of the MOOC. In addition to this, time allocation within workloads should be considered for any future inclusion of MOOCs for staff development.",Evaluating the use of a MOOC within Higher Education Professional Development Training,NA:NA,2016
Xiaoxuan Wang:Jiale Gao,"People who design and develop web product need interdisciplinary knowledge and skills from various fields. Thus in most Chinese vocational colleges, students are lack of a completed experience to fulfill employers' needs before they leave colleges. In this paper, we propose a project-based approach to design the course, in which students unify business, product design and development theories into one project practice to build the web product. Over different stages and increasingly complexity, students learn how to interact with customers, build project management skills, design product with business mind instead of focusing on design aesthetics and developing programming skills only.",The Practice of Web Product Design and Development Course Design,NA:NA,2016
Marc Spaniol:Ricardo Baeza-Yates:Julien Masanàs,"Time is a key dimension to understand the Web. It is fair to say that it has not received yet all the attention it deserves and TempWeb is an attempt to help remedy this situation by putting time as the center of its reflection. Studying time in this context actually covers a large spectrum, from the extraction of temporal information and knowledge, to diachronic studies for the design of infrastructural and experimental settings enabling a proper observation of this dimension. For its sixth edition, TempWeb accepted six out of eleven submissions for oral presentation. We interpret the high quality of the submissions and the frequent contributors to TempWeb, as indicators of an evolving community. It shows a clear sign of a positive dynamic in the study of time in the scope of the Web and evidence of the relevance of this effort. The workshop proceedings are published by ACM DL as part of the WWW 2016 Companion Publication. We hope you will find in these papers as well as the keynotes of Wolfgang Nejdl (L3S Hanover, Germany) and Omar Alonso (Microsoft, USA), and the discussion and exchanges of this edition of TempWeb, some motivations to look more into this important aspect of the Web. TempWeb 2016 was jointly organized by Caen University (Caen, France), Yahoo Labs (Sunnyvale, USA) and Internet Memory Foundation (Paris, France).",Session details: TempWeb'16,NA:NA:NA,2016
Omar Alonso,"There is new and exciting research work in analyzing and exploiting information from corpora with temporal information such as news, email, or social media. We review some of the current activities around extracting temporal information from social media and present a number of examples from real-world systems. We also outline a number of open problems and potential new research areas.",Time to ship: Some Examples from the Real-World,NA,2016
Erdal Kuzey:Jannik Strötgen:Vinay Setty:Gerhard Weikum,"For many NLP and IR applications, anchored temporal information extracted from textual documents is of utmost importance. Thus, temporal tagging -- the extraction and normalization of temporal expressions -- has gained a lot of attention in recent years and several tools such as HeidelTime and SUTime are proposed. However, such tools do not address textual phrases with temporal scopes like ""Clinton's time as First Lady"". While such phrases (so-called temponyms) are not temporal expressions per se, information about their temporal scopes can be helpful in many scenarios, e.g., in the context of temporal information retrieval. In this paper, we describe the integration of a wide range of temponyms to the publicly available temporal tagger HeidelTime to include temponym tagging.",Temponym Tagging: Temporal Scopes for Textual Phrases,NA:NA:NA:NA,2016
Vera Zaychik Moffitt:Julia Stoyanovich,"Graphs are used to represent a plethora of phenomena, from the Web and social networks, to biological pathways, to semantic knowledge bases. Arguably the most interesting and important questions one can ask about graphs have to do with their evolution. Which Web pages are showing an increasing popularity trend? How does influence propagate in social networks? How does knowledge evolve? In this paper we present our ongoing work on the Portal system, an open-source distributed framework for evolving graphs. Portal streamlines exploratory analysis of evolving graphs, making it efficient and usable, and providing critical tools to computational and data scientists. Our system implements a declarative query language by the same name, which we briefly describe in this paper. Our basic abstraction is a TGraph, which logically represents a series of adjacent snapshots. We present different physical representations of TGraphs and show results of a preliminary experimental evaluation of these physical representations for an important class of evolving graph analytics.",Towards a Distributed Infrastructure for Evolving Graph Analytics,NA:NA,2016
Aécio Santos:Bruno Pasini:Juliana Freire,"While much work has been devoted to understanding Web dynamics and using this knowledge to efficiently maintain the freshness of the indexes of generic search engines, the same is not true for domain-specific indexes constructed by focused crawlers. For the latter, the problem is compounded by the fact that it is important not only to maintain already-crawled pages fresh, but also to identify new relevant content and expand the collection. In this paper, we discuss the challenges involved in this problem and describe our preliminary efforts in building a testbed to better understand the dynamics of specific topics and characterize how they evolve over time. We propose a data collection methodology and a set of experiments to answer important questions about temporal dynamics and evolution of topics. We also present the results of the experimental analysis we carried out using data collected over a period of four weeks using two distinct topics. These results suggest that topic-specific refreshing strategies can be beneficial for focused crawlers.",A First Study on Temporal Dynamics of Topics on the Web,NA:NA:NA,2016
Mohsen Shahriari:Stephen Gunashekar:Marven von Domarus:Ralf Klamma,"Digital media has some observable traces named communities. Several events such as split, merge, dissolve and survive happen to communities in social media. But what are significant features to predict these events? And to which extent a feature is relevant in a social media? To answer these questions, we perform a study on community evolution analysis and prediction. We employ three overlapping community detection (OCD) algorithms from literature to the case of time-evolving networks including social, email communication and co-authorship networks. Group evolution discovery (GED) technique is applied to track the identified communities. We compare structural properties of OCD algorithms and investigate most persistent communities over time. Furthermore, static and temporal features of a community are applied to build a logistic classifier for community evolution prediction (CEP). Results reveal important features to predict events happening to a community.",Predictive Analysis of Temporal and Overlapping Community Structures in Social Media,NA:NA:NA:NA,2016
Matthias Steinbauer:Gabriele Anderst-Kotsis,Graph models have a long standing history as models for real world structures and processes. In recent research two important dimensions of graphs are described of particular importance. (1) Temporal aspects of graphs cannot be neglected for many current application scenarios such as social network analysis or the analysis of the global web graph. (2) The mentioned graph structures have grown to very large sizes such that traditional methodologies no longer hold. In this work a distributed computing framework designed for storing and processing of large-scale temporal graphs is presented. For this system a reference implementation in Java was created. In this paper first insight on the implementation and observations in using the system are discussed.,"DynamoGraph: A Distributed System for Large-scale, Temporal Graph Processing, its Implementation and First Observations",NA:NA,2016
Staffan Truvé,"Recorded Future has developed its Temporal Analytics Engine as a general purpose platform for harvesting and analyzing unstructured text from the open, deep, and dark web, and for transforming that content into a structured representation suitable for different analyses. In this paper we present some of the key components of our system, and show how it has been adapted to the increasingly important domain of cyber threat intelligence. We also describe how our data can be used for predictive analytics, e.g. to predict the likelihood of a product vulnerability being exploited or to assess the maliciousness of an IP address.",Temporal Analytics for Predictive Cyber Threat Intelligence,NA,2016
Bettina Berendt:Laura Hollink:Markus Luczak-Roesch,"It is our great pleasure to welcome you to the 6th International Workshop on Usage Analysis and the Web of Data (USEWOD), associated with WWW 2016. The workshop is dedicated to the diverse ecosystem of Web of Data access mechanisms. From academic to government data, from complex SPARQL queries to Linked Data Fragments, from DBpedia to Wikidata: mthe data sources on the Web of Data and the ways in which these sources can be created and consumed vary greatly and raise fundamental questions. The call for papers attracted submissions from United States, Canada, Asia, and Europe. The program committee reviewed and accepted the following: Venue or Track Reviewed -4 Accepted - 2. Additionally, we decided to publish an invited paper that presents a particularly interesting crossdisciplinary perspective on the Web of Data and outline current research directions and challenges in an extended 'Message from the USEWOD Chairs'. The USEWOD 2016 Research Dataset As in previous years, a standard research dataset of usage data from well-recognized Web-of-Data datasets has been published to promote reproducible research on the workshop themes. A particular highlight of this year's dataset is overlapping usage data from the official DBpedia servers as well as the Linked Data Fragments interface to DBpedia and Wikidata. This dataset allows researchers to study alternative Web of Data usage mechanisms in an unprecedented way and could therefore become a unique resource of great importance for the field. For more information on the datasets released in previous years, please see http://usewod.org/data-sets.html. Special thanks got to Open Link Software for providing us with DBpedia logs as well as Ruben Verborgh for access to Linked Data Fragments usage data.",Session details: USEWOD'16,NA:NA:NA,2016
Nathalie Casemajor,This paper analyses the specificities of metadata embedded in photographic images. It investigates how embedded metadata can help studying the usage patterns and conditions of circulation of images on digital networks.,Embedded Metadata and the Digital Lifecycle of Images: Methodological Challenges,NA,2016
Pieter Colpaert:Alvin Chua:Ruben Verborgh:Erik Mannens:Rik Van de Walle:Andrew Vande Moere,"In the field of smart cities, researchers need an indication of how people move in and between cities. Yet, getting statistics of travel flows within public transit systems has proven to be troublesome. In order to get an indication of public transit travel flows in Belgium, we analyzed the query logs of the iRail API, a highly expressive route planning API for the Belgian railways. We were able to study 100k to 500k requests for each month between October 2012 and November 2015, which is between 0.56% and 1.66% of the amount of monthly passengers. Using data visualizations, we illustrate the commuting patterns in Belgium and confirm that Brussels, the capital, acts as a central hub. The Flemish region appears to be polycentric, while in the Walloon region, everything converges on Brussels. The findings correspond to the real travel demand, according to experts of the passenger federation Trein Tram Bus. We conclude that query logs of route planners are of high importance in getting an indication of travel flows. However, better travel intentions would be acquirable using dedicated HTTP POST requests.",What Public Transit API Logs Tell Us about Travel Flows,NA:NA:NA:NA:NA:NA,2016
Choudur Lakshminarayan:Ram Kosuru:Meichun Hsu,"As the website is a primary customer touch-point, millions are spent to gather web data about customer visits. Sadly, the trove of data and corresponding analytics have not lived up to the promise. Current marketing practice relies on ambiguous summary statistics or small-sample usability studies. Idiosyncratic browsing and low conversion (browser-to-buyer) make modeling hard. In this paper, we model browsing patterns (sequence of clicks) via Markov chain theory to predict users' propensity to buy within a session. We focus on model complexity, imputing missing values, data augmentation, and other attendant issues that impact performance. The paper addresses the following aspects; (1) Determine appropriate order of the Markov chain (assess the influence of prior history in prediction), (2) Impute missing transitions by exploiting the inherent link structure in the page sequences, (3) predict the likelihood of a purchase based on variable-length page sequences, and (4) Augment the training set of buyers (which is typically very small: 2% by viewing the page transitions as a graph and exploiting its link structure to improve performance. The cocktail of solutions address important issues in practical digital marketing. Extensive analysis of data applied to a large commercial web-site shows that Markov chain based classifiers are useful predictors of user intent.",Modeling Complex Clickstream Data by Stochastic Models: Theory and Methods,NA:NA:NA,2016
Markus Luczak-Roesch:Laura Hollink:Bettina Berendt,"Usage mining always was and still is a key topic for research in the context of the Web [16]. This is evidenced by the series of papers that appear in the scientific tracks of the WWW conference year by year. Web usage is being studied to create economic value by placing targeted ads or delivering personalized content, but also in order to better understand how people behave online in mass movements and collective action.",Current Directions for Usage Analysis and the Web of Data: The Diverse Ecosystem of Web of Data Access Mechanisms,NA:NA:NA,2016
Jacqueline Bourdeau:Bebo White:Irwin King,"It is our great pleasure to welcome you to the ACM WWW2016 Workshop on Web Science and Technology for Education (WebED2016), co-located with the 2016 International WWW Conference. This workshop series began as The Workshop on Web-based Education Technologies (WebET) at WWW2014 in Seoul, Korea. However, this year's workshop has expanded its scope to explore the influence the growing field of Web Science. By doing so it is our goal to bring together educational technologists, Web researchers, and members of social science communities seeking to investigate the impact of Web technology on teaching and learning. The mission of the workshop is for attendees to share novel solutions that fulfill the needs of heterogeneous applications and environments and identify new directions for future research and development. It is also our hope that WebED2016 attendees might identify others with similar interests possibly leading to new collaborations and joint efforts. We encourage workshop attendees to attend the keynote speaker presentation, the accepted paper presentations, and the expert panel discussion. Keynote: ""Web Science, Social Media and Education,"" Dame Wendy Hall, University of Southampton, Panel: ""Evaluating Educational Software in the Web Era,"" Jutta Treviranus (Ontario College of Art and Design University), Jean-Philippe Bradette (Ellicom), Irwin King (The Chinese University of Hong Kong), Beverly Woolf (University of Massachusetts Amherst), and Irina Muhina (iecarus, moderator)",Session details: WEBED'16,NA:NA:NA,2016
Wendy Hall,"Over the last 25 years the Web has evolved into a critical global infrastructure. Since its emergence in the 1990s, it has exploded into hundreds of billions of pages that touch almost all aspects of modern life. Little appreciated, however, is the fact that the Web is more than the sum of its pages and it is more than its technical protocols. Vast emergent properties have arisen that are transforming society. Web Science is the study of the Web as a socio-technical system. As the Web becomes increasingly significant in all our lives, studying it from an interdisciplinary perspective becomes even more important. We are now rapidly moving into a world of data on and about the Web, which gives rise to even more opportunities and challenges. In this talk we will explore the role of observatories and data analytics for the development of new methodologies for longitudinal research in Web Science that could help us understand more about how the Web evolves as a social-technical network. After many years of speculation about the potential of on-line learning, web technology has finally developed to a point where on-line learning is today a reality -- from MOOCS to complete on-line degree courses. All rely on the use of social media to enhance the learning environment for the students. We will discuss the application of the web observatory approach to the study of on-line learning and the insights such an approach can reveal that would not be possible in a traditional learning environment -- potentially giving rise to world-wide studies in this area.","Web Science, Social Media and Education",NA,2016
Rubiela Carrillo:Elise Lavoué:Yannick Prié,"Learning Sciences argue that student engagement is composed of behavioral, motivational and cognitive dimensions. Many proposals in Learning Analytics have provided teachers with quantitative indicators focusing only on students' behaviors, such as the number and the duration of their actions with the learning environment. In this paper, we propose visual representations of cognitive indicators to add explanatory elements to behavioral indicators. We describe our general architecture for collecting and aggregating data used to build the proposed visualizations. We illustrate the use of these indicators in various pedagogical scenarios oriented towards supporting teachers in students' actions and performances understanding.",Towards Qualitative Insights for Visualizing Student Engagement in Web-based Learning Environments,NA:NA:NA,2016
Hou Pong Chan:Tong Zhao:Irwin King,"Massive Open Online Coursers (MOOCs) offer a convenient way for people to access quality courses via the internet. However, the problem of grading open-ended assignments at such a large scale still remains challenging. Although peer assessment have been proposed to handle the large-scale grading problem in MOOCs, existing methods still suffer several limitations: (1) most current peer assessment research ignore the importance of how to allocate the assessment tasks among peers, (2) existing approaches for peer grading learn the complete ranking in an offline manner, (3) theoretical analysis for trust-aware peer grading is missing. In this work, we consider the case that we have prior knowledge about all students' reliability. We formulate the problem of peer assessment as a sequential noisy ranking aggregation problem. We derive a trust-aware allocation scheme for peer assessment to maximize the probability of constructing a correct ranking of assignments with a budget constraint.Moreover, we also derive an upper bound for the probability of prediction error on the inferred ranking of assignments. Furthermore, we propose the Trust-aware Ranking-based Multi-armed Bandit Algorithms to sequentially allocate the assessment tasks to the students based on the derived allocation scheme and learn an accurate peer grading result by taking students' reliability into consideration.",Trust-aware Peer Assessment using Multi-armed Bandit Algorithms,NA:NA:NA,2016
Mariheida Cordova-Sanchez:Pinar Yanardag,"The use of micro-blogging in classrooms is a recently trending concept in computer-aided education. Micro-blogs offer an effective way of communication in large classrooms, and engage students in meaningful discussions. However, existing micro-blogging systems in education setting suffer from a few drawbacks. First, relevant content might be overwhelmed by irrelevant posts to the lecture which could jeopardize effective learning. Second, students might generate redundant content by posting similar questions to each other and create substantial information overload. Third, posts covering different aspects of the class might be left undiscovered due to real-time characteristics of micro-blogs. To address these issues, we present a principled approach for picking a set of posts that promotes relevant and diverse content while effectively turning down the noise created by redundant posts. We formulate this task as a submodular optimization problem for which we provide an efficient and near-optimal solution. We evaluate our framework on real micro-blog based classroom datasets and our empirical results demonstrate that our framework is effectively able to cover the most important and diverse content that is being discussed in classrooms.",Turning Down the Noise in Classrooms,NA:NA,2016
Sivaldo J. de Santana:Hugo A. Souza:Victor A.F. Florentin:Ranilson Paiva:Ig Ibert Bittencourt:Seiji Isotani,"In the last decade, many researchers have studied the use of game elements in education. The term ""gamification"" refers to the application of elements used in the development of video games, such as mechanics and dynamics in other contexts unrelated to games, in order to generate more enjoyable and positive attitudes from the students. The gamication process involves using several elements present in video games, like: points, levels, rankings, rewards (badges/achievements) and missions. In this study, we assess whether or not, gamification elements can help and motivate students enrolled in a gamified ontology-based adaptive online learning environment called MeuTutor. In this context, we followed the Pedagogical Recommendation Process to discover which gamification elements were relevant to promote learning, in order to recommend improvements to the environment. To do that, this study shows a quantitative analysis(correlation analysis) of the gamification elements from MeuTutor.",A Quantitative Analysis of the Most Relevant Gamification Elements in an Online Learning Environment,NA:NA:NA:NA:NA:NA,2016
Sergio Gutierrez-Santos:Stefano Capuzzi:Ken Kahn:Sokratis Karkalas:Alexandra Poulovassilis,"We present and evaluate a web-based architecture for monitoring student-system interaction indicators in Exploratory Learning Environments (ELEs),using as our case study a microworld for secondary school algebra. We discuss the challenging role of teachers in exploratory learning settings and motivate the need for visualisation and notification tools that can assist teachers in focusing their attention across the class and inform teachers' interventions. We present an architecture that can support such Teacher Assistance tools and demonstrate its scalability to allow concurrent usage by thousands of users (students and teachers).",Scalable Monitoring of Student Interaction Indicators in Exploratory Learning Environments,NA:NA:NA:NA:NA,2016
Alexandra Luccioni:Roger Nkambou:Jean Massardi:Jacqueline Bourdeau:Claude Coulombe,"In this paper, we describe an innovative project where Web technologies are exploited to develop an Intelligent Tutoring System (ITS) that uses a Learning Management System (LMS) as its learning interface. The resulting ITS has been instantiated into a specific system called STI-DICO which aims at helping future French primary school teachers to acquire the knowledge and skills needed to use the French dictionary. The learning process in the ITS takes place via a number of authentic learning scenarios that represent situations that the future teachers will face in the classroom. By using a LMS as the learning interface component of the system, we enable it to be directly deployable on the Web to a large population of students, all the while retaining the adaptive components of an ITS to deliver a personalized learning experience to its users.",STI-DICO: A Web-Based System for Intelligent Tutoring of Dictionary Skills,NA:NA:NA:NA:NA,2016
Bart Pursel:Chen Liang:Shuting Wang:Zhaohui Wu:Kyle Williams:Benjamin Brautigam:Sherwyn Saul:Hannah Williams:Kyle Bowen:C. Lee Giles,"We describe BBookX, a web-based tool that uses a human-computing approach to facilitate the creation of open source textbooks. The goal of BBookX is to create a system that can search various Open Educational Resource (OER) repositories such as Wikipedia, based on a set of user-generated criteria, and return various resources that can be combined, remixed, and re-used to support specific learning goals. As BBookX is a work-in-progress, we are in the midst of a design-based research study, where user testing guided multiple rounds of iteration in the design of the user interface (UI) as well as the query engine. From an interface perspective, the challenges we present are the matching of the UI to users' mental models from similar systems, as well as educating users how to best work with the algorithms in an iterative manner to find and refine content for inclusion into open textbooks.",BBookX: Design of an Automated Web-based Recommender System for the Creation of Open Learning Content,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2016
Rajendra Akerkar:Pierre Maret:Laurent Vercouter,"It is with great pleasure, and on behalf of the organizing committee, we would like to welcome you to the 8th International Workshop on Web Intelligence & Communities (WI&C 2016) taking place on April 11th in Montreal, Canada and collocated with the WWW 2016 conference. This workshop, the eighth in a series of workshops, is intended to stimulate discussions on the forefront of research concerned with web intelligence applied to collaborative networks. Web Intelligence consists of a multidisciplinary area dealing with exploiting data and services over the Web, to create new data and services using both Information and Communication Technologies (ICT) and Artificial Intelligence (AI) techniques. Communities appear as a first-class object in the areas of web intelligence and agent technologies, as well as a crucial crossroads of several sub-domains (i.e. user modelling, protocols, data management, data mining, content modelling, etc.). These sub-domains impact the nature of the communities and the applications which are related to them. These applications are numerous, and the success of well-known Social Network Sites for entertainment should not be allowed to over-shadow the other application domains, for instance in education, health, design, knowledge management, and so forth. The workshop will provide presentation and discussion opportunities for researchers working on web intelligence applied to collaborative networks, such as virtual communities. The possibilities and consequences of the web usage for collaborative networks are tremendous and new tools are required to satisfy users and service providers.",Session details: WI&C'16,NA:NA:NA,2016
Babak Esfandiari:Alan Davoust,"Social networks can play an important role in the process of decentralizing authority in distributed systems. We will focus on distributed wiki systems, and we show how, in the special case of a peer-to-peer wiki, there is a rational incentive for users to self-organize and form a meaningful social network. We discuss to that effect the basic metrics that can be derived from the topology of the social network to help assess the subjective quality of wiki entries. Demos and experimental results will illustrate and support our discussion. We finally speculate as to how these results may also translate to discussion forums or recommender systems.",Distributed Wikis and Social Networks: A Good Fit,NA:NA,2016
Alexandr Chernov:Nikolaos Lagos:Matthias Gallé:Ágnes Sándor,"The World Wide Web contains a large number of community created knowledge of instructional nature. Similarly, in a commercial setting, databases of instructions are used by customer-care providers to guide clients in the resolution of issues. Most of these instructions are expressed in natural language. Knowledge Bases including such information are valuable through the sum of their single entries. However, as each entry is created mostly independently, users (e.g. other community members) cannot take advantage of the accumulated knowledge that can be developed via the aggregation of related entries. In this paper we consider the problem of inter-linking Knowledge Base entries, in order to get relevant information from other parts of the Knowledge Base. To achieve this, we propose to detect \textit{actionable phrases} -- text fragments that describe how to perform a certain action -- and link them to other entries. The extraction method that we implement achieves an F-score of 67.35\%. We also show that using actionable phrases results in better linking quality than using coarser-grained spans of text, as proposed in the literature. Besides the evaluation of both steps, we also include a detailed error analysis and release our annotation to the community.",Enriching How-to Guides by Linking Actionable Phrases,NA:NA:NA:NA,2016
Pierre Maret:Rajendra Akerkar:Laurent Vercouter,"Web-based community is a self-defined web-based network of interactive communication organized around a shared interest or purpose. It provides the means of interactions among people in which they create, share, and exchange information and ideas in virtual space and networks. Working with big data often requires querying and reasoning that data to isolate information of interest and manipulate it in various ways. This editorial paper explores recent big data research topics -- stream querying and reasoning -- over data from web based communities. It combines aspects from some well-studied research domains, such as, social network analysis, graph databases, and data streams. We provide a brief synopsis of some research issues in supporting reasoning and querying tasks. This editorial also presents the WI&C-16 workshop's goal and programme.",Web Communities in Big Data Era. Editorial,NA:NA:NA,2016
"Lucas Fonseca Navarro:Estevam Rafael Hruschka, Jr.:Ana Paula Appel","With the exponentially growing amount of available data on the Web over the last years, several projects have been created to automatically extract knowledge from this information set. As the data domains on the Web are too wide, most of these projects store the acquired knowledge in ontological knowledge bases (OKBs). Mapping it into graph-based representation makes possible to apply graph-mining techniques to extract implicit information. However most of these projects treat the mapping process using different adjustments in several ways, thus, there is not a standard mapping process or a formal way defifined to do this task. In this paper we formally describe a graph structure called Ontological Network and how it can be used to map an Ontological Knowledge Base. We also show some graph-mining based algorithms to add new facts and to extend the ontology of an OKB while mapped into an Ontological Network as example.",Ontological Networks: Mapping Ontological Knowledge Bases into Graphs,NA:NA:NA,2016
Zaher Yamak:Julien Saunier:Laurent Vercouter,"Various techniques are used to manipulate users in OSN environments such as social spam, identity theft, spear phishing and Sybil attacks... In this article, we are interested in analyzing the behavior of multiple fake accounts that try to bypass the OSN regulation. In the context of social media manipulation detection, we focus on the special case of multiple Identity accounts (Sockpuppet) created on English Wikipedia (EnWiki). We set up a complete methodology spanning from the data extraction from EnWiki to the training and testing of our selected data using several machine learning algorithms. In our methodology we propose a set of features that grows on previous literature to use in automatic data analysis in order to detect the Sockpuppets accounts created on EnWiki. We apply them on a database of 10.000 user accounts. The results compare several machine learning algorithms to show that our new features and training data enable to detect 99\% of fake accounts, improving previous results from the literature.",Detection of Multiple Identity Manipulation in Collaborative Projects,NA:NA:NA,2016
Qi Ye:Feng Wang:Bo Li,"Query intent mining is a critical problem in various real-world search applications. In the past few years we have witnessed dramatic advances in the field of query intent mining area. In this paper, we present a practical system---StarrySky for identifying and inferring millions of query intents in daily sponsored search with high precision and acceptable coverage. We have already achieved great advantages by deploying this system in Sogou sponsored search engine\footnote {http://www.sogou.com}. The general architecture of StarrySky consists of three stages. First, we detect millions of fine-grained query clusters from two years of click logs which can represent different query intents. Second, we refine the qualities of query clusters with a series of well-designed operations, and call the final refined clusters as concepts. Third and foremost, we build a flexible real-time inference algorithm for assigning query intents to the detected concepts with high precision. Beyond the description of the system, we employ several experiments to evaluate its performance and flexibility. Our inference algorithm achieves up to 96% precision and 68% coverage on daily search requests. We believe StarrySky is a practical and valuable system for tracking query intents.",StarrySky: A Practical System to Track Millions of High-Precision Query Intents,NA:NA:NA,2016
Robert West:Leila Zia:Dario Taraborelli:Jure Leskovec,"It is our great pleasure to welcome you to Wiki Workshop, a forum for bringing together researchers exploring all aspects of Wikipedia and other Wikimedia sites. Like the editing aspect of Wikimedia sites, research on Wikimedia projects relies heavily on the community of researchers who explore the projects to improve our understanding of the current state and future directions of such projects. This workshop aims to bring this community together on an annual basis and to welcome new community members. As part of the workshop, we will share the latest research on Wikimedia projects, explore and share new directions for research, learn about the new data-sets that have been released publicly, and initiate or continue on new research initiatives. The target audience will include researchers in: natural language processing web mining and data mining artificial intelligence and machine learning graph and network theory social computing user generated content industry social sciences linguistics The workshop is the second workshop in these series, proceeding the workshop held in ICWSM 2015. The list of speakers and program can be found at the workshop's website.",Session details: Wiki Workshop'16,NA:NA:NA:NA,2016
Paolo Boldi:Corrado Monti,"We propose a novel general technique aimed at pruning and cleansing the Wikipedia category hierarchy, with a tunable level of aggregation. Our approach is endogenous, since it does not use any information coming from Wikipedia articles, but it is based solely on the user-generated (noisy) Wikipedia category folksonomy itself. We show how the proposed techniques can help reduce the level of noise in the hierarchy and discuss how alternative centrality measures can differently impact on the result.",Cleansing Wikipedia Categories using Centrality,NA:NA,2016
Freddy Brasileiro:João Paulo A. Almeida:Victorio A. Carvalho:Giancarlo Guizzardi,"Wikidata captures structured data on a number of subject domains, managing, among others, the information underlying Wikipedia and other Wikimedia projects. Wikidata serves as a repository of structured data, whose purpose is to support the consistent sharing and linking of data on the Web. To support these purposes, it is key that Wikidata is built on consistent data models and representation schemas, which are constructed and managed in a collaborative platform. In this paper, we address the quality of taxonomic hierarchies in Wikidata. We focus on taxonomic hierarchies with entities at different classification levels (particular individuals, types of individuals, types of types of individuals, etc.). We use an axiomatic theory for multi-level modeling to analyze current Wikidata content, and identify a significant number of problematic classification and taxonomic statements. The problems seem to arise from an inadequate use of instantiation and subclassing in certain Wikidata hierarchies.",Applying a Multi-Level Modeling Theory to Assess Taxonomic Hierarchies in Wikidata,NA:NA:NA:NA,2016
T. Chattopadhyay:Santa Maiti:Arindam Pal:Avik Ghose:Arpan Pal:Shanky Viswanathan:Narendran Sivakumar,"A business problem for the telecommunication companies is to provide an appropriate promotional coupon to suitable customers. This problem leads to the challenge of identifying behavioral patterns of customers and deliver the right customer engagement at the right time. So there is a need for a system that can enable the telecommunication companies to go for the best marketing strategy by leveraging customer intelligence to drive offer acceptance based on personas. Technically it is possible for the telecommunication companies to recommend suitable advertisements if they can classify the web sites browsed by their customers into classes like sports, e-commerce, social networking, streaming media etc. Another problem is to classify a new website when it doesn't belong to any of the existing clusters. In this paper, the authors are going to propose a method to automatically classify the websites and synthesize the cluster names in case it doesn't belong to any of the predefined clusters. We have experimented on a small set of data set and the classification results are quite convincing. Moreover, the phrases used to describe a website if it doesn't belong to existing classes are compliant to the phrases obtained from manual annotation. This proposed system uses the Wikipedia data to construct the document for the websites browsed by the customers.",Automatic Discovery of Emerging Trends using Cluster Name Synthesis on User Consumption Data: Extended Abstract,NA:NA:NA:NA:NA:NA:NA,2016
Johanna Geiß:Michael Gertz,"Driven by the popularity of social networks, there has been an increasing interest in employing such networks in the context of named entity linking. In this paper, we present a novel approach to person name disambiguation and linking that uses a large-scale social network extracted from the English Wikipedia. First, possible candidate matches for an ambiguous person name are determined. With each candidate match, a network substructure is associated. Based on the similarity between these network substructures and the latent network of an ambiguous person name in a document, we propose an efficient ranking method to resolve the ambiguity. We demonstrate the effectiveness of our approach, resulting in an overall precision of over 96% for disambiguating person names and linking them to real world entities.",With a Little Help from my Neighbors: Person Name Linking Using the Wikipedia Social Network,NA:NA,2016
Haggai Roitman:Shay Hummel:Ella Rabinovich:Benjamin Sznajder:Noam Slonim:Ehud Aharoni,"This work presents a novel claim-oriented document retrieval task. For a given controversial topic, relevant articles containing claims that support or contest the topic are retrieved from a Wikipedia corpus. For that, a two-step retrieval approach is proposed. At the first step, an initial pool of articles that are relevant to the topic are retrieved using state-of-the-art retrieval methods. At the second step, articles in the initial pool are re-ranked according to their potential to contain as many relevant claims as possible using several claim discovery features. Hence, the second step aims at maximizing the overall claim recall of the retrieval system. Using a recently published claims benchmark, the proposed retrieval approach is demonstrated to provide more relevant claims compared to several other retrieval alternatives.",On the Retrieval of Wikipedia Articles Containing Claims on Controversial Topics,NA:NA:NA:NA:NA:NA,2016
Thomas Steiner,"In this paper, we introduce the Wikipedia Tools for Google Spreadsheets. Google Spreadsheets is part of a free, Web-based software office suite offered by Google within its Google Docs service. It allows users to create and edit spreadsheets online, while collaborating with other users in realtime. Wikipedia is a free-access, free-content Internet encyclopedia, whose content and data is available, among other means, through an API. With the Wikipedia Tools for Google Spreadsheets, we have created a toolkit that facilitates working with Wikipedia data from within a spreadsheet context. We make these tools available as open-source on GitHub [https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets], released under the permissive Apache 2.0 license.",Wikipedia Tools for Google Spreadsheets,NA,2016
Yu Suzuki:Satoshi Nakamura,"In this paper, we propose a method for assessing the quality of Wikipedia editors. By effectively determining whether the text meaning persists over time, we can determine the actual contribution by editors. This is used in this paper to detect vandal. However, the meaning of text does not always change if a term in the text is added or removed. Therefore, we cannot capture the changes of text meaning automatically, so we cannot detect whether the meaning of text survives or not. To solve this problem, we use crowdsourcing to manually detect changes of text meaning. In our experiment, we confirmed that our proposed method improves the accuracy of detecting vandals by about 5%.",Assessing the Quality of Wikipedia Editors through Crowdsourcing,NA:NA,2016
Ramine Tinati:Markus Luczak-Roesch:Wendy Hall,"This paper documents a study of the real-time Wikipedia edit stream containing over 6 million edits on 1.5 million English Wikipedia articles, during 2015. We focus on answering questions related to identification and use of information cascades between Wikipedia articles, based on author editing activity. Our findings show that by constructing information cascades between Wikipedia articles using editing activity, we are able to construct an alternative linking structure in comparison to the embedded links within a Wikipedia page. This alternative article hyperlink structure was found to be relevant in topic, and timely in relation to external global events (e.g., political activity). Based on our analysis, we contextualise the findings against areas of interest such as events detection, vandalism, edit wars, and editing behaviour.",Finding Structure in Wikipedia Edit Activity: An Information Cascade Approach,NA:NA:NA,2016
Vikrant Yadav:Sandeep Kumar,"In this paper, we present a novel method to obtain a set of most appropriate queries for retrieval of relevant information about an entity from the Web. Using the body text of existing articles in a Wikipedia category, we generate a set of queries capable of fetching the most relevant content for any entity belonging to that category. We find the common topics discussed in the articles of a category using Latent Semantic Analysis (LSA) and use them to formulate the queries. Using Long Short-Term Memory (LSTM) neural network, we reduce the number of queries by removing the less sensible ones and then select the best ones out of them. The experimental results show that the proposed method outperforms the baselines. Existing approaches are performing better in generation of the relevant section title queries by extraction from the headings of the Wikipedia articles as compared to the generation of queries by extraction from the body text of the articles. Whereas, the experimental results show that the proposed approach can perform equally well and even better in extraction of the relevant queries from the body text of the Wikipedia articles.",Learning Web Queries for Retrieval of Relevant Information about an Entity in a Wikipedia Category,NA:NA,2016
Thanassis Tiropanis:Matthew Weber,"It is our great pleasure to welcome you to the WWW 2016 Tutorials. We received 21 proposals from all around the world covering a broad range of topics. We evaluated them regarding relevance, quality, and novelty, selecting 5 half-day tutorials and 2 full-day tutorials. We also took in account the coverage of the different areas related to WWW as well as the potential audience, to schedule them in two consecutive days with the minimal audience interest overlap. The morning of the first day includes the following four tutorials: Computational Social Science for the World Wide Web Centrality Measures on Big Graphs The afternoon of the first day includes the following four tutorials: Computational Social Science for the World Wide Web (continued) Cryptographic Currencies Crash Course The second day starts with three tutorials: Building Decentralized Applications for the Social Web Automatic Entity Recognition and Typing in Massive Text Corpora Mining Big Time-series Data on the Web The final afternoon includes the last three tutorials: Building Decentralized Applications for the Social Web (continued) Analyzing sequential User Behavior on the Web The call for tutorials attracted submissions from United States, Europe, Asia, Africa and South America. Review and acceptance statistics are as follows: WWW 2016 Tutorials Reviewed -21 Accepted - 7. We believe that the program provides a good balance between several trending topics such as deep learning, social media analysis, graph mining, crowdsourcing, knowledge databases, mobile data, etc. Hence we hope that you will find the tutorial program interesting, providing you with a valuable opportunity to learn and share ideas with other researchers and practitioners from institutions around the world.",Session details: Tutorials,NA:NA,2016
Francesco Bonchi:Gianmarco De Francisci Morales:Matteo Riondato,"Centrality measures allow to measure the relative importance of a node or an edge in a graph w.r.t.~other nodes or edges. Several measures of centrality have been developed in the literature to capture different aspects of the informal concept of importance, and algorithms for these different measures have been proposed. In this tutorial, we survey the different definitions of centrality measures and the algorithms to compute them. We start from the most common measures, such as closeness centrality and betweenness centrality, and move to more complex ones such as spanning-edge centrality. In our presentation, we begin from exact algorithms and then progress to approximation algorithms, including sampling-based ones, and to highly-scalable MapReduce algorithms for huge graphs, both for exact computation and for keeping the measures up-to-date on dynamic graphs where edges are inserted or removed over time. Our goal is to show how advanced algorithmic techniques and scalable systems can be used to obtain efficient algorithms for an important graph mining task, and to encourage research in the area by highlighting open problems and possible directions.","Centrality Measures on Big Graphs: Exact, Approximated, and Distributed Algorithms",NA:NA:NA,2016
Aljosha Judmayer:Edgar Weippl,"Bitcoin is a rare case where practice seems to be ahead of theory. Joseph Bonneau et al.[15] This tutorial aims to further close the gap between IT security research and the area of cryptographic currencies and block chains. We will describe and refer to Bitcoin as an example throughout the tutorial, as it is the most prominent representative of a such a system. It also is a good reference to discuss the underlying block chain mechanics which are the foundation of various altcoins (e.g. Namecoin) and other derived systems. In this tutorial, the topic of cryptographic currencies is solely addressed from a technical IT security point-of-view. Therefore we do not cover any legal, sociological, financial and economical aspects. The tutorial is designed for participants with a solid IT security background but will not assume any prior knowledge on cryptographic currencies. Thus, we will quickly advance our discussion into core aspects of this field.",Cryptographic Currencies Crash Course (C4): Tutorial,NA:NA,2016
Xiang Ren:Ahmed El-Kishky:Chi Wang:Jiawei Han,"In today's computerized and information-based society, we are soaked with vast amounts of natural language text data, ranging from news articles, product reviews, advertisements, to a wide range of user-generated content from social media. To turn such massive unstructured text data into actionable knowledge, one of the grand challenges is to gain an understanding of entities and the relationships between them. In this tutorial, we introduce data-driven methods to recognize typed entities of interest in different kinds of text corpora (especially in massive, domain-specific text corpora). These methods can automatically identify token spans as entity mentions in text and label their types (e.g., people, product, food) in a scalable way. We demonstrate on real datasets including news articles and yelp reviews how these typed entities aid in knowledge discovery and management.",Automatic Entity Recognition and Typing in Massive Text Corpora,NA:NA:NA:NA,2016
Yasushi Sakurai:Yasuko Matsubara:Christos Faloutsos,"Online news, blogs, SNS and many other Web-based services has been attracting considerable interest for business and marketing purposes. Given a large collection of time series, such as web-click logs, online search queries, blog and review entries, how can we efficiently and effectively find typical time-series patterns? What are the major tools for mining, forecasting and outlier detection? Time-series data analysis is becoming of increasingly high importance, thanks to the decreasing cost of hardware and the increasing on-line processing capability. The objective of this tutorial is to provide a concise and intuitive overview of the most important tools that can help us find meaningful patterns in large-scale time-series data. Specifically we review the state of the art in three related fields: (1) similarity search, pattern discovery and summarization, (2) non-linear modeling and forecasting, and (3) the extension of time-series mining and tensor analysis. We also introduce case studies that illustrate their practical use for social media and Web-based services.",Mining Big Time-series Data on the Web,NA:NA:NA,2016
Andrei Sambra:Amy Guy:Sarven Capadisli:Nicola Greco,"Recent advancements in technologies and protocols mean that it is easier than ever to integrate social features into diverse web applications, and increased awareness of privacy concerns means that it is pertinent to consider empowerment of application users when doing so. Many developers are already familiar with the notion of personal data stores; this tutorial will demonstrate how to access or provide such stores for users, and build simple web applications which read and write to the storage whilst remaining completely decoupled from it. This advantages developers in two ways: by removing the burden of storing and maintaining a canonical copy of user data; and by enabling access to and ease of integration with data created through other applications, creating richer, seamless experiences. From the application users' perspective, they need no longer commit and become bound to particular services, but can mix, match and move between those that best meet their needs. We will introduce Solid, a set of protocols based on existing W3C recommendations, for reading, writing and access control of the contents of a personal data store, which can be layered up in order to integrate various social features into new or existing web applications. Attendees will leave with an understanding of Solid and how different parts of the protocols can work together, and having written some code to implement the parts that interest them most. They will also have hands on experience with existing libraries and tooling to facilitate working with the Solid protocols. Those who stay for the full day will have an opportunity to build a small but complete web application with decentralized social features, and to collaborate with others to see the advantages of sharing data between multiple applications.",Building Decentralized Applications for the Social Web,NA:NA:NA:NA,2016
Philipp Singer:Florian Lemmerich,"This tutorial aims at outlining fundamental methods for studying categorical sequences on the Web. Categorical sequences can refer to any kind of transitional data between a set of states, for example human navigation (transitions) between Web sites (states). Presented methods focus on sequential pattern mining, modeling and inference aiming at better understanding the production of sequences. A core model utilized in this tutorial is the Markov chain model. We hope that this tutorial raises interest and awareness of the field at hand and provides participants with basic tools for analyzing sequential user behavior on the Web.",Analyzing Sequential User Behavior on the Web,NA:NA,2016
Ingmar Weber:Claudia Wagner:Markus Strohmaier:Luca Maria Aiello,"This tutorial aims at outlining fundamental methods for studying typical social science research questions with organic data (i.e., data that has not been designed for a specific research purpose but can be found on the Web). Further, social theories, statistical methods and models that help to understand the processes that generated the data will be discussed. Participants will learn (1) how to turn theoretical assumptions into models and test them, (2) how to validate measurements and (3) how to approximate causality when working with organic data.",Computational Social Science for the World Wide Web (CSSW3),NA:NA:NA:NA,2016
Sören Auer:Tom Heath:Christian Bizer:Tim Berners-Lee,"The ninth workshop on Linked Data (LDOW2016) on the Web is held in Montreal, Quebec, Canada on April 12, 2016 and co-located with the 25rd International World Wide Web Conference (WWW2016). The Web is developing from a medium for publishing textual documents into a medium for sharing structured data. This trend is fueled on the one hand by the adoption of the Linked Data principles by a growing number of data providers. On the other hand, large numbers of websites have started to semantically mark up the content of their HTML pages and thus also contribute to the wealth of structured data available on the Web. The 9th Workshop on Linked Data on the Web aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data from the Web as well as mining knowledge from the global Web of Data.",LDOW2016: 9th Workshop on Linked Data on the Web,NA:NA:NA:NA,2016
Amparo E. Cano:Daniel Preotiuc-Pietro:Danica Radovanović:Katrin Weller:Aba-Sah Dadzie,NA,#Microposts2016: 6th Workshop on Making Sense of Microposts: Big things come in small packages,NA:NA:NA:NA:NA,2016
Melanie Johnston-Hollitt,"Modern Astrophysics is one of the most data intensive research fields in the world and is driving many of the required innovations in the ""big data"" space. Foremost in astronomy in terms of data generation is radio astronomy, and in the last decade an increase in global interest and investment in the field had led to a large number of new or upgraded facilities which are each currently generating petabytes of data per annum. The peak of this so-called 'radio renaissance' will be the Square Kilometre Array (SKA) -- a global observatory designed to uncover the mysteries of the Universe. The SKA will create the highest resolution, fastest frame rate movie of the evolving Universe ever and in doing so will generate 160 terrabytes of data a day, or close to 5 zettabytes of data per annum. Furthermore, due to the extreme faintness of extraterrestrial radio signals, the telescope elements for the SKA must be located in radio quite parts of the world with very low population density. Thus the project aims to build the most data intensive scientific experiment ever, in some of the most remote places on Earth. Generating and serving scientific data products of this scale to a global community of researchers from remote locations is just the first of the ""big data"" challenges the project faces. Coordination of a global network of tiered data resources will be required along with software tools to exploit the vast sea of results generated. In fact, to fully realize the enormous scientific potential of this project, we will need not only better data distribution and coordination mechanisms, but also improved algorithms, artificial intelligence and ontologies to extract knowledge in an automated way at a scale not yet attempted in science. In this keynote I will present an overview of the SKA project, outline the ""big data"" challenges the project faces and discuss some of the approaches we are taking to tame the astronomical data deluge we face.",Taming the Data Deluge to Unravel the Mysteries of the Universe,NA,2017
Mark Pesce,"The great project of the World Wide Web has succeeded - a large portion of the world's information is now instantly accessible through open protocols and open presentation formats. The Web is as Sir Tim Berners-Lee envisioned it, a vast resource of interconnected knowledge. Yet that resource exists in a universe of its own. Meanwhile the real world has become crowded with connected devices, none more significant than the smartphone - bringing the Web to eighty percent of the planet's adult population by the end of this decade. Smartphones have become fantastically adept at navigating cyberspace, but - with the singular exception of maps - have few real connections to the world immediately at hand. In 2017 we live in two worlds: the Web, and the real. The time has come to knit these two together. To begin that integration, our first step must be a deep moment of contemplation about what the Web and the real world have to offer one another. How can each amplify the value and capacity of the other? Because of the Web, the real world is pregnant with data and knowledge - what does that world look like? How do we use it? How does it change the way we think and behave? In this simple act of design thinking - toward a ""Web-wide world"" - we can reframe the possibilities of what both the Web and the real world can offer - and what we can offer both. This is the next great project for the Web - finding its place in the world.",The Web-Wide World,NA,2017
Yoelle Maarek,"Many have noticed that personal communications have slowly moved from mail to social media and instant messaging platforms, especially with younger generation [6]. Yet Web Mail traffic continues to steadily grow. A paradox? Not really. We have observed at Yahoo Research that the nature of email traffic has significantly changed in the last two decades, and it is now dominated by machine-generated messages. These messages include hotel newsletters, from which users forgot to unsubscribe, repeated, and often annoying, notifications from a social media site, or critical information such as a flight e-ticket, a purchase invoice, or a telephone bill. In this talk, I first share some elements of this journey that led us to this critical finding that 90% of today's Web Mail is sent by automatic scripts [1]. I then discuss the challenges and opportunities this drastic change offers. First the key challenge: namely, the need for Web mail services to revisit their usage assumptions and their traditional features in light of this change. An obvious example is the ""reply"" button being displayed by default below messages sent from a ""[email protected]"" sender. Another feature is mail classification, which has finally experienced some changes in the last few years, [4]. I then discuss the opportunities in this era of big data. One first insight is that messages that have been generated by a same script, share some semantic commonality. Being able to automatically cluster such messages, and map such clusters into ""templates"" brings great value for discovering meaning, for generalizing findings and predicting behaviors [5]. A second insight is that within this commonality, the differences bring even more value, which allows highlighting what makes individuals unique within a crowd. In particular we discuss extraction techniques that automatically identify these unique elements [2]. Yet, they also present a clear risk in terms of privacy and I describe the absolute need for guaranteeing k-anonymity in our mining techniques, [3]. I conclude by encouraging the research community to explore this new domain of Web mail search and data mining.",Web Mail is not Dead!: It's Just Not Human Anymore,NA,2017
Vahab Mirrokni:Hamid Nazerzadeh,"Billions of dollars worth of display advertising are sold via contracts and deals. This paper presents a formal study of preferred deals, a new generation of contracts for selling online advertisement, that generalize the traditional reservation contracts; these contracts are suitable for advertisers with advanced targeting capabilities. We propose a constant-factor approximation algorithm for maximizing the revenue that can be obtained from these deals. We show, both theoretically and via data analysis, that deals, with appropriately chosen minimum-purchase guarantees, can yield significantly higher revenue than auctions. We evaluate our algorithm using data from Google's ad exchange platform. Our algorithm obtains about 90% of the optimal revenue where the second-price auction, even with personalized reserve, obtains at most 52% of the benchmark.",Deals or No Deals: Contract Design for Online Advertising,NA:NA,2017
Santiago Balseiro:Anthony Kim:Mohammad Mahdian:Vahab Mirrokni,"In online advertising, advertisers purchase ad placements by participating in a long sequence of repeated auctions. One of the most important features advertising platforms often provide, and advertisers often use, is budget management, which allows advertisers to control their cumulative expenditures. Advertisers typically declare the maximum daily amount they are willing to pay, and the platform adjusts allocations and payments to guarantee that cumulative expenditures do not exceed budgets. There are multiple ways to achieve this goal, and each one, when applied to all budget-constrained advertisers simultaneously, steers the system toward a different equilibrium. While previous research focused on online stochastic optimization techniques or game-theoretic equilibria of such settings, our goal in this paper is to compare the ``system equilibria'' of a range of budget management strategies in terms of the seller's profit and buyers' utility. In particular, we consider six different budget management strategies including probabilistic throttling, thresholding, bid shading, reserve pricing, and multiplicative boosting. We show these methods admit a system equilibrium in a rather general setting, and prove dominance relations between them in a simplified setting. Our study sheds light on the impact of budget management strategies on the tradeoff between the seller's profit and buyers' utility. Finally, we also empirically compare the system equilibria of these strategies using real ad auction data in sponsored search and randomly generated bids. The empirical study confirms our theoretical findings about the relative performances of budget management strategies.",Budget Management Strategies in Repeated Auctions,NA:NA:NA:NA,2017
Christopher A. Wilkens:Ruggiero Cavallo:Rad Niazadeh,"Nearly fifteen years ago, Google unveiled the generalized second price (GSP) auction. By all theoretical accounts including their own [Varian 14], this was the wrong auction --- the Vickrey-Clarke-Groves (VCG) auction would have been the proper choice --- yet GSP has succeeded spectacularly. We give a deep justification for GSP's success: advertisers' preferences map to a model we call value maximization; they do not maximize profit as the standard theory would believe. For value maximizers, GSP is the truthful auction [Aggarwal 09]. Moreover, this implies an axiomatization of GSP --- it is an auction whose prices are truthful for value maximizers --- that can be applied much more broadly than the simple model for which GSP was originally designed. In particular, applying it to arbitrary single-parameter domains recovers the folklore definition of GSP. Through the lens of value maximization, GSP metamorphosizes into a powerful auction, sound in its principles and elegant in its simplicity.",GSP: The Cinderella of Mechanism Design,NA:NA:NA,2017
Alexey Drutsa,"We study revenue optimization learning algorithms for repeated posted-price auctions where a seller interacts with a (truthful or strategic) buyer that holds a fixed valuation. We focus on a practical situation in which the seller does not know in advance the number of played rounds (the time horizon) and has thus to use a horizon-independent pricing. First, we consider straightforward modifications of previously best known algorithms and show that these horizon-independent modifications have worser or even linear regret bounds. Second, we provide a thorough theoretical analysis of some broad families of consistent algorithms and show that there does not exist a no-regret horizon-independent algorithm in those families. Finally, we introduce a novel deterministic pricing algorithm that, on the one hand, is independent of the time horizon T and, on the other hand, has an optimal strategic regret upper bound in O(log log T). This result closes the logarithmic gap between the previously best known upper and lower bounds on strategic regret.",Horizon-Independent Optimal Pricing in Repeated Auctions with Truthful and Strategic Buyers,NA,2017
Ruggiero Cavallo:Prabhakar Krishnamurthy:Maxim Sviridenko:Christopher A. Wilkens,"The generalized second price (GSP) auction has served as the core selling mechanism for sponsored search ads for over a decade. However, recent trends expanding the set of allowed ad formats---to include a variety of sizes, decorations, and other distinguishing features---have raised critical problems for GSP-based platforms. Alternatives such as the Vickrey-Clarke-Groves (VCG) auction raise different complications because they fundamentally change the way prices are computed. In this paper we report on our efforts to redesign a search ad selling system from the ground up in this new context, proposing a mechanism that optimizes an entire slate of ads globally and computes prices that achieve properties analogous to those held by GSP in the original, simpler setting of uniform ads. A careful algorithmic coupling of allocation-optimization and pricing-computation allows our auction to operate within the strict timing constraints inherent in real-time ad auctions. We report performance results of the auction in Yahoo's Gemini Search platform.",Sponsored Search Auctions with Rich Ads,NA:NA:NA:NA,2017
Zhixuan Fang:Longbo Huang:Adam Wierman,"The growth of the sharing economy is driven by the emergence of sharing platforms, e.g., Uber and Lyft, that match owners looking to share their resources with customers looking to rent them. The design of such platforms is a complex mixture of economics and engineering, and how to ""optimally"" design such platforms is still an open problem. In this paper, we focus on the design of prices and subsidies in sharing platforms. Our results provide insights into the tradeoff between revenue maximizing prices and social welfare maximizing prices. Specifically, we introduce a novel model of sharing platforms and characterize the profit and social welfare maximizing prices in this model. Further, we bound the efficiency loss under profit maximizing prices, showing that there is a strong alignment between profit and efficiency in practical settings. Our results highlight that the revenue of platforms may be limited in practice due to supply short- ages; thus platforms have a strong incentive to encourage sharing via subsidies. We provide an analytic characterization of when such subsidies are valuable and show how to optimize the size of the subsidy provided. Finally, we validate the insights from our analysis using data from Didi Chuxing, the largest ridesharing platform in China.",Prices and Subsidies in the Sharing Economy,NA:NA:NA,2017
Siddhartha Banerjee:Sreenivas Gollapudi:Kostas Kollias:Kamesh Munagala,"Recent years have witnessed the rise of many successful e-commerce marketplace platforms like the Amazon marketplace, AirBnB, Uber/Lyft, and Upwork, where a central platform mediates economic transactions between buyers and sellers. A common feature of many of these two-sided marketplaces is that the platform has full control over search and discovery, but prices are determined by the buyers and sellers. Motivated by this, we study the algorithmic aspects of market segmentation via directed discovery in two-sided markets with endogenous prices. We consider a model where an online platform knows each buyer/seller's characteristics, and associated demand/supply elasticities. Moreover, the platform can use discovery mechanisms (search, recommendation, etc.) to control which buyers/sellers are visible to each other. We develop efficient algorithms for throughput (i.e. volume of trade) and welfare maximization with provable guarantees under a variety of assumptions on the demand and supply functions. We also test the validity of our assumptions on demand curves inferred from NYC taxicab log-data, as well as show the performance of our algorithms on synthetic experiments.",Segmenting Two-Sided Markets,NA:NA:NA:NA,2017
Noam Nisan:Gali Noti,"Using data obtained in a controlled ad-auction experiment that we ran, we evaluate the regret-based approach to econometrics that was recently suggested by Nekipelov, Syrgkanis, and Tardos (EC 2015). We found that despite the weak regret-based assumptions, the results were (at least) as accurate as those obtained using classic equilibrium-based assumptions. En route we studied to what extent humans actually minimize regret in our ad auction, and found a significant difference between the ``high types'' (players with a high valuation) who indeed rationally minimized regret and the ``low types'' who significantly overbid. We suggest that correcting for these biases and adjusting the regret-based econometric method may improve the accuracy of estimated values.",An Experimental Evaluation of Regret-Based Econometrics,NA:NA,2017
Cinar Kilcioglu:Justin M. Rao:Aadharsh Kannan:R. Preston McAfee,"We examine the economics of demand and supply in cloud computing. The public cloud offers three main benefits to firms: 1) utilization can be scaled up or down easily; 2) capital expenditure (on-premises servers) can be converted to operating expenses, with the capital incurred by a specialist; 3) software can be ``pay-as-you-go.'' These benefits increase with the firm's ability to dynamically scale resource utilization and thus point to the need for dynamic prices to shape demand to the (short-run) fixed datacenter supply. Detailed utilization analysis reveals the large swings in utilization at the hourly, daily or weekly level are very rare at the customer level and non-existent at the datacenter level. Furthermore, few customers show volatility patterns that are excessively correlated with the market. These results explain why fixed prices currently prevail despite the seeming need for time-varying dynamics. Examining the actual CPU utilization provides a lens into the future. Here utilization varies by order half the datacenter capacity, but most firms are not dynamically scaling their assigned resources at-present to take advantage of these changes. If these gains are realized, demand fluctuations would be on par with the three classic industries where dynamic pricing is important (hotels, electricity, airlines) and dynamic prices would be essential for efficiency.",Usage Patterns and the Economics of the Public Cloud,NA:NA:NA:NA,2017
Yilin Wang:Jiliang Tang:Jundong Li:Baoxin Li:Yali Wan:Clayton Mellina:Neil O'Hare:Yi Chang,"Studies suggest that self-harm users found it easier to discuss self-harm-related thoughts and behaviors using social media than in the physical world. Given the enormous and increasing volume of social media data, on-line self-harm content is likely to be buried rapidly by other normal content. To enable voices of self-harm users to be heard, it is important to distinguish self-harm content from other types of content. In this paper, we aim to understand self-harm content and provide automatic approaches to its detection. We first perform a comprehensive analysis on self-harm social media using different input cues. Our analysis, the first of its kind in large scale, reveals a number of important findings. Then we propose frameworks that incorporate the findings to discover self-harm content under both supervised and unsupervised settings. Our experimental results on a large social media dataset from Flickr demonstrate the effectiveness of the proposed frameworks and the importance of our findings in discovering self-harm content.",Understanding and Discovering Deliberate Self-harm Content in Social Media,NA:NA:NA:NA:NA:NA:NA:NA,2017
Sandra Servia-Rodríguez:Kiran K. Rachuri:Cecilia Mascolo:Peter J. Rentfrow:Neal Lathia:Gillian M. Sandstrom,"Measuring mental well-being with mobile sensing has been an increasingly active research topic. Pervasiveness of smartphones combined with the convenience of mobile app distribution platforms (e.g., Google Play) provide a tremendous opportunity to reach out to millions of users. However, the studies at the confluence of mental health and mobile sensing have been longitudinally limited, controlled, or confined to a small number of participants. In this paper we report on what we believe is the largest longitudinal in-the-wild study of mood through smartphones. We describe an Android app to collect participants' self-reported moods and system triggered experience sampling data while passively measuring their physical activity, sociability, and mobility via their device's sensors. We report the results of a large-scale analysis of the data collected for about three years from 18,000 users. The paper makes three primary contributions. First, we show how we used physical and software sensors in smartphones to automatically and accurately identify routines. Then, we demonstrate the strong correlation between these routines and users' personality, well-being perception, and other psychological variables. Finally, we explore predictability of users' mood using their passive sensing data. Our findings show that, especially for weekends, mobile sensing can be used to predict users' mood with an accuracy of about 70%. These results have the potential to impact the design of future mobile apps for mood/behavior tracking and interventions.",Mobile Sensing at the Service of Mental Well-being: a Large-scale Longitudinal Study,NA:NA:NA:NA:NA:NA,2017
Tim Althoff:Eric Horvitz:Ryen W. White:Jamie Zeitzer,"Human cognitive performance is critical to productivity, learning, and accident avoidance. Cognitive performance varies throughout each day and is in part driven by intrinsic, near 24-hour circadian rhythms. Prior research on the impact of sleep and circadian rhythms on cognitive performance has typically been restricted to small-scale laboratory-based studies that do not capture the variability of real-world conditions, such as environmental factors, motivation, and sleep patterns in real-world settings. Given these limitations, leading sleep researchers have called for larger in situ monitoring of sleep and performance. We present the largest study to date on the impact of objectively measured real-world sleep on performance enabled through a reframing of everyday interactions with a web search engine as a series of performance tasks. Our analysis includes 3 million nights of sleep and 75 million interaction tasks. We measure cognitive performance through the speed of keystroke and click interactions on a web search engine and correlate them to wearable device-defined sleep measures over time. We demonstrate that real-world performance varies throughout the day and is influenced by both circadian rhythms, chronotype (morning/evening preference), and prior sleep duration and timing. We develop a statistical model that operationalizes a large body of work on sleep and performance and demonstrates that our estimates of circadian rhythms, homeostatic sleep drive, and sleep inertia align with expectations from laboratory-based sleep studies. Further, we quantify the impact of insufficient sleep on real-world performance and show that two consecutive nights with less than six hours of sleep are associated with decreases in performance which last for a period of six days. This work demonstrates the feasibility of using online interactions for large-scale physiological sensing.",Harnessing the Web for Population-Scale Physiological Sensing: A Case Study of Sleep and Performance,NA:NA:NA:NA,2017
Shaodian Zhang:Tian Kang:Lin Qiu:Weinan Zhang:Yong Yu:Noémie Elhadad,"A large number of patients discuss treatments in online health communities (OHCs). One research question of interest to health researchers is whether treatments being discussed in OHCs are eventually used by community members in their real lives. In this paper, we rely on machine learning methods to automatically identify attributions of mentions of treatments from an online autism community. The context of our work is online autism communities, where parents exchange support for the care of their children with autism spectrum disorder. Our methods are able to distinguish discussions of treatments that are associated with patients, caregivers, and others, as well as identify whether a treatment is actually taken. We investigate treatments that are not just discussed but also used by patients according to two types of content analysis, cross-sectional and longitudinal. The treatments identified through our content analysis help create a catalogue of real-world treatments. This study results lay the foundation for future research to compare real-world drug usage with established clinical guidelines.",Cataloguing Treatments Discussed and Used in Online Autism Communities,NA:NA:NA:NA:NA:NA,2017
Jin-woo Kwon:Soo-Mook Moon,"Due to its high portability and simplicity, web application (app) based on HTML/JavaScript/CSS has been widely used for various smart-device platforms. To take advantage of its wide platform pool, a new idea called app migration has been proposed for the web platform. Web app migration is a framework to serialize a web app running on a device and restore it in another device to continue its execution. In JavaScript semantics, one of the language features that does not allow easy app migration is a closure. A JavaScript function can access variables defined in its outer function even if the execution of the outer function is terminated. It is allowed because the inner function is created as a closure such that it contains the outer function's environment. This feature is widely used in web app development because it is the most common way to implement data encapsulation in web programming. Closures are not easy to serialize because environments can be shared by a number of closures and environments can be created in a nested way. In this paper, we propose a novel approach to fully serialize closures. We created mechanisms to extract information from a closure's environment through the JavaScript engine and to serialize the information in a proper order so that the original relationship between closures and environments can be restored properly. We implemented our mechanism on the WebKit browser and successfully migrated Octane benchmarks and seven real web apps which heavily exploit closures. We also show that our mechanism works correctly even for some extreme, closure-heavy cases.",Web Application Migration with Closure Reconstruction,NA:NA,2017
Mengwei Xu:Yun Ma:Xuanzhe Liu:Felix Xiaozhu Lin:Yunxin Liu,"Background activities on smartphones are essential to today's ""always-on"" mobile device experience. Yet, there lacks a clear understanding of the cooperative behaviors among background activities as well as a quantification of the consequences. In this paper, we present the first in-depth study of app collusion, in which one app surreptitiously launches others in the background without user's awareness. To enable the study, we develop AppHolmes, a static analysis tool for detecting app collusion by examining the app binaries. By analyzing 10,000 apps from top third-party app markets, we found that i) covert, cooperative behaviors in background app launch are surprisingly pervasive, ii) most collusion is caused by shared services, libraries, or common interest among apps, and iii) collusion has serious impact on performance, efficiency, and security. Overall, our work presents a strong implication on future mobile system design.",AppHolmes: Detecting and Characterizing App Collusion among Third-Party Android Markets,NA:NA:NA:NA:NA,2017
Elias P. Papadopoulos:Michalis Diamantaris:Panagiotis Papadopoulos:Thanasis Petsas:Sotiris Ioannidis:Evangelos P. Markatos,"The vast majority of online services nowadays, provide both a mobile friendly website and a mobile application to their users. Both of these choices are usually released for free, with their developers, usually gaining revenue by allowing advertisements from ad networks to be embedded into their content. In order to provide more personalized and thus more effective advertisements, ad networks usually deploy pervasive user tracking, raising this way significant privacy concerns. As a consequence, the users do not have to think only their convenience before deciding which choice to use while accessing a service: web or app, but also which one harms their privacy the least. In this paper, we aim to respond to this question: which of the two options protects the users' privacy in the best way apps or browsers? To tackle this question, we study a broad range of privacy related leaks in a comparison of several popular apps and their web counterpart. These leaks may contain not only personally identifying information (PII) but also device-specific information, able to cross-application and cross-site track the user into the network, and allow third parties to link web with app sessions. Finally, we propose an anti-tracking mechanism that enable the users to access an online service through a mobile app without risking their privacy. Our evaluation shows that our approach is able to preserve the privacy of the user by reducing the leaking identifiers of apps by 27.41% on average, while it imposes a practically negligible latency of less than 1 millisecond per request.",The Long-Standing Privacy Debate: Mobile Websites vs Mobile Apps,NA:NA:NA:NA:NA:NA,2017
Haoyu Wang:Zhe Liu:Yao Guo:Xiangqun Chen:Miao Zhang:Guoai Xu:Jason Hong,"With the prevalence of smartphones, app markets such as Apple App Store and Google Play has become the center stage in the mobile app ecosystem, with millions of apps developed by tens of thousands of app developers in each major market. This paper presents a study of the mobile app ecosystem from the perspective of app developers. Based on over one million Android apps and 320,000 developers from Google Play, we analyzed the Android app ecosystem from different aspects. Our analysis shows that while over half of the developers have released only one app in the market, many of them have released hundreds of apps. We classified developers into different groups based on the number of apps they have released, and compared their characteristics. Specially, we have analyzed the group of aggressive developers who have released more than 50 apps, trying to understand how and why they create so many apps. We also investigated the privacy behaviors of app developers, showing that some developers have a habit of producing apps with low privacy ratings. Our study shows that understanding the behavior of mobile developers can be helpful to not only other app developers, but also to app markets and mobile users.",An Explorative Study of the Mobile App Ecosystem from App Developers' Perspective,NA:NA:NA:NA:NA:NA:NA,2017
Xiangnan He:Lizi Liao:Hanwang Zhang:Liqiang Nie:Xia Hu:Tat-Seng Chua,"In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation --- collaborative filtering --- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering --- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",Neural Collaborative Filtering,NA:NA:NA:NA:NA:NA,2017
Peizhe Cheng:Shuaiqiang Wang:Jun Ma:Jiankai Sun:Hui Xiong,"In this study, we investigate diversified recommendation problem by supervised learning, seeking significant improvement in diversity while maintaining accuracy. In particular, we regard each user as a training instance, and heuristically choose a subset of accurate and diverse items as ground-truth for each user. We then represent each user or item as a vector resulted from the factorization of the user-item rating matrix. In our paper, we try to discover a factorization for matching the following supervised learning task. In doing this, we define two coupled optimization problems, parameterized matrix factorization and structural learning, to formulate our task. And we propose a diversified collaborative filtering algorithm (DCF) to solve the coupled problems. We also introduce a new pairwise accuracy metric and a normalized topic coverage diversity metric to measure the performance of accuracy and diversity respectively. Extensive experiments on benchmark datasets show the performance gains of DCF in comparison with the state-of-the-art algorithms.",Learning to Recommend Accurate and Diverse Items,NA:NA:NA:NA:NA,2017
Cheng-Kang Hsieh:Longqi Yang:Yin Cui:Tsung-Yi Lin:Serge Belongie:Deborah Estrin,"Metric learning algorithms produce distance metrics that capture the important relationships among data. In this work, we study the connection between metric learning and collaborative filtering. We propose Collaborative Metric Learning (CML) which learns a joint metric space to encode not only users' preferences but also the user-user and item-item similarity. The proposed algorithm outperforms state-of-the-art collaborative filtering algorithms on a wide range of recommendation tasks and uncovers the underlying spectrum of users' fine-grained preferences. CML also achieves significant speedup for Top-K recommendation tasks using off-the-shelf, approximate nearest-neighbor search, with negligible accuracy reduction.",Collaborative Metric Learning,NA:NA:NA:NA:NA:NA,2017
Alex Beutel:Ed H. Chi:Zhiyuan Cheng:Hubert Pham:John Anderson,"When building a recommender system, how can we ensure that all items are modeled well? Classically, recommender systems are built, optimized, and tuned to improve a global prediction objective, such as root mean squared error. However, as we demonstrate, these recommender systems often leave many items badly-modeled and thus under-served. Further, we give both empirical and theoretical evidence that no single matrix factorization, under current state-of-the-art methods, gives optimal results for each item. As a result, we ask: how can we learn additional models to improve the recommendation quality for a specified subset of items? We offer a new technique called focused learning, based on hyperparameter optimization and a customized matrix factorization objective. Applying focused learning on top of weighted matrix factorization, factorization machines, and LLORMA, we demonstrate prediction accuracy improvements on multiple datasets. For instance, on MovieLens we achieve as much as a 17% improvement in prediction accuracy for niche movies, cold-start items, and even the most badly-modeled items in the original model.",Beyond Globally Optimal: Focused Learning for Improved Recommendations,NA:NA:NA:NA:NA,2017
Tsubasa Takahashi:Bryan Hooi:Christos Faloutsos,"Given a collection of seasonal time-series, how can we find regular (cyclic) patterns and outliers (i.e. rare events)? These two types of patterns are hidden and mixed in the time-varying activities. How can we robustly separate regular patterns and outliers, without requiring any prior information? We present CycloneM, a unifying model to capture both cyclic patterns and outliers, and CycloneFact, a novel algorithm which solves the above problem. We also present an automatic mining framework AutoCyclone, based on CycloneM and CycloneFact. Our method has the following properties; (a) effective: it captures important cyclic features such as trend and seasonality, and distinguishes regular patterns and rare events clearly; (b) robust and accurate: it detects the above features and patterns accurately against outliers; (c) fast: CycloneFact takes linear time in the data size and typically converges in a few iterations; (d) parameter free: our modeling framework frees the user from having to provide parameter values. Extensive experiments on 4 real datasets demonstrate the benefits of the proposed model and algorithm, in that the model can capture latent cyclic patterns, trends and rare events, and the algorithm outperforms the existing state-of-the-art approaches. CycloneFact was up to 5 times more accurate and 20 times faster than top competitors.",AutoCyclone: Automatic Mining of Cyclic Online Activities with Robust Tensor Factorization,NA:NA:NA,2017
Yuan Lin:Wei Chen:Zhongzhi Zhang,"Percolation threshold of a network is the critical value such that when nodes or edges are randomly selected with probability below the value, the network is fragmented but when the probability is above the value, a giant component connecting a large portion of the network would emerge. Assessing the percolation threshold of networks has wide applications in network reliability, information spread, epidemic control, etc. The theoretical approach so far to assess the percolation threshold is mainly based on spectral radius of adjacency matrix or non-backtracking matrix, which is limited to dense graphs or locally treelike graphs, and is less effective for sparse networks with non-negligible amount of triangles and loops. In this paper, we study high-order non-backtracking matrices and their application to assessing percolation threshold. We first define high-order non-backtracking matrices and study the properties of their spectral radii. Then we focus on the 2nd-order non-backtracking matrix and demonstrate analytically that the reciprocal of its spectral radius gives a tighter lower bound than those of adjacency and standard non-backtracking matrices. We further build a smaller size matrix with the same largest eigenvalue as the 2nd-order non-backtracking matrix to improve computation efficiency. Finally, we use both synthetic networks and 42 real networks to illustrate that the use of the 2nd-order non-backtracking matrix does give better lower bound for assessing percolation threshold than adjacency and standard non-backtracking matrices.",Assessing Percolation Threshold Based on High-Order Non-Backtracking Matrices,NA:NA:NA,2017
Maximilien Danisch:T.-H. Hubert Chan:Mauro Sozio,"Algorithms for finding dense regions in an input graph have proved to be effective tools in graph mining and data analysis. Recently, Tatti and Gionis [WWW 2015] presented a novel graph decomposition (known as the locally-dense decomposition) that is similar to the well-known k-core decomposition, with the additional property that its components are arranged in order of their densities. Such a decomposition provides a valuable tool in graph mining. Unfortunately, their algorithm for computing the exact decomposition is based on a maximum-flow algorithm which cannot scale to massive graphs, while the approximate decomposition defined by the same authors misses several interesting properties. This calls for scalable algorithms for computing such a decomposition. In our work, we devise an efficient algorithm which is able to compute exact locally-dense decompositions in real-world graphs containing up to billions of edges. Moreover, we provide a new definition of approximate locally-dense decomposition which retains most of the properties of an exact decomposition, for which we devise an algorithm that can scale to real-world graphs containing up to tens of billions of edges. Our algorithm is based on the classic Frank-Wolfe algorithm which is similar to gradient descent and can be efficiently implemented in most of the modern architectures dealing with massive graphs. We provide a rigorous study of our algorithms and their convergence rates. We conduct an extensive experimental evaluation on multi-core architectures showing that our algorithms converge much faster in practice than their worst-case analysis. Our algorithm is even more efficient for the more specialized problem of computing a densest subgraph.",Large Scale Density-friendly Graph Decomposition via Convex Programming,NA:NA:NA,2017
Xinsheng Li:K. Selçuk Candan:Maria Luisa Sapino,"Tensor decomposition is used for many web and user data analysis operations from clustering, trend detection, anomaly detection, to correlation analysis. However, many of the tensor decomposition schemes are sensitive to noisy data, an inevitable problem in the real world that can lead to false conclusions. The problem is compounded by over-fitting when the user data is sparse. Recent research has shown that it is possible to avoid over-fitting by relying on probabilistic techniques. However, these have two major deficiencies: (a) firstly, they assume that all the data and intermediary results can fit in the main memory, and (b) they treat the entire tensor uniformly, ignoring potential non-uniformities in the noise distribution. In this paper, we propose a Noise-Profile Adaptive Tensor Decomposition (nTD) method, which aims to tackle both of these challenges. In particular, nTD leverages a grid-based two-phase decomposition strategy for two complementary purposes: firstly, the grid partitioning helps ensure that the memory footprint of the decomposition is kept low; secondly (and perhaps more importantly) any a priori knowledge about the noise profiles of the grid partitions enable us to develop a sample assignment strategy (or s-strategy) that best suits the noise distribution of the given tensor. Experiments show that nTD's performance is significantly better than conventional CP decomposition techniques on noisy user data tensors.",nTD: Noise-Profile Adaptive Tensor Decomposition,NA:NA:NA,2017
Osama Haq:Mamoon Raja:Fahad R. Dogar,"Many popular cloud applications use inter-data center paths; yet, little is known about the characteristics of these ``cloud paths''. Over an eighteen month period, we measure the inter-continental cloud paths of three providers (Amazon, Google, and Microsoft) using client side (VM-to-VM) measurements. We find that cloud paths are more predictable compared to public Internet paths, with an order of magnitude lower loss rate and jitter at the tail (95th percentile and beyond) compared to public Internet paths. We also investigate the nature of packet losses on these paths (e.g., random vs. bursty) and potential reasons why these paths may be better in quality. Based on our insights, we consider how we can further improve the quality of these paths with the help of existing loss mitigation techniques. We demonstrate that using the cloud path in conjunction with a detour path can mask most of the cloud losses, resulting in up to five 9's of network availability for applications.",Measuring and Improving the Reliability of Wide-Area Cloud Paths,NA:NA:NA,2017
Henrique Moniz:João Leitão:Ricardo J. Dias:Johannes Gehrke:Nuno Preguiça:Rodrigo Rodrigues,"Most geo-replicated storage systems use weak consistency to avoid the performance penalty of coordinating replicas in different data centers. This departure from strong semantics poses problems to application programmers, who need to address the anomalies enabled by weak consistency. In this paper we use a recently proposed isolation level, called Non-Monotonic Snapshot Isolation, to achieve ACID transactions with low latency. To this end, we present Blotter, a geo-replicated system that leverages these semantics in the design of a new concurrency control protocol that leaves a small amount of local state during reads to make commits more efficient, which is combined with a configuration of Paxos that is tailored for good performance in wide area settings. Read operations always run on the local data center, and update transactions complete in a small number of message steps to a subset of the replicas. We implemented Blotter as an extension to Cassandra. Our experimental evaluation shows that Blotter has a small overhead at the data center scale, and performs better across data centers when compared with our implementations of the core Spanner protocol and of Snapshot Isolation on the same codebase.",Blotter: Low Latency Transactions for Geo-Replicated Storage,NA:NA:NA:NA:NA:NA,2017
Charles L.A. Clarke:Gordon V. Cormack:Jimmy Lin:Adam Roegiest,"This paper explores a simple question: How would we provide a high-quality search experience on Mars, where the fundamental physical limit is speed-of-light propagation delays on the order of tens of minutes? On Earth, users are accustomed to nearly instantaneous responses from web services. Is it possible to overcome orders-of-magnitude longer latency to provide a tolerable user experience on Mars? In this paper, we formulate the searching from Mars problem as a tradeoff between ""effort"" (waiting for responses from Earth) and ""data transfer"" (pre-fetching or caching data on Mars). The contribution of our work is articulating this design space and presenting two case studies that explore the effectiveness of baseline techniques, using publicly available data from the TREC Total Recall and Sessions Tracks. We intend for this research problem to be aspirational as well as inspirational---even if one is not convinced by the premise of Mars colonization, there are Earth-based scenarios such as searching from rural villages in India that share similar constraints, thus making the problem worthy of exploration and attention from researchers.",Ten Blue Links on Mars,NA:NA:NA:NA,2017
Albert van der Linde:Pedro Fouto:João Leitão:Nuno Preguiça:Santiago Castiñeira:Annette Bieniusa,"Many web applications are built around direct interactions among users, from collaborative applications and social networks to multi-user games. Despite being user-centric, these applications are usually supported by services running on servers that mediate all interactions among clients. When users are in close vicinity of each other, relying on a centralized infrastructure for mediating user interactions leads to unnecessarily high latency while hampering fault-tolerance and scalability. In this paper, we propose to extend user-centric Internet services with peer-to-peer interactions. We have designed a framework named Legion that enables client web applications to securely replicate data from servers, and synchronize these replicas directly among them. Legion allows for client-side modules, that we dub adapters, to leverage existing web platforms for storing data and to assist in Legion operation. Using these adapters, legacy applications accessing directly the web platforms can co-exist with new applications that use our framework, while accessing the same shared objects.Our experimental evaluation shows that, besides supporting direct client interactions, even when disconnected from the servers, Legion provides lower latency for update propagation with decreased network traffic for servers.",Legion: Enriching Internet Services with Peer-to-Peer Interactions,NA:NA:NA:NA:NA:NA,2017
Luca Soldaini:Elad Yom-Tov,"Internet data has surfaced as a primary source for investigation of different aspects of human behavior. A crucial step in such studies is finding a suitable cohort (i.e., a set of users) that shares a common trait of interest to researchers. However, direct identification of users sharing this trait is often impossible, as the data available to researchers is usually anonymized to preserve user privacy. To facilitate research on specific topics of interest, especially in medicine, we introduce an algorithm for identifying a trait of interest in anonymous users. We illustrate how a small set of labeled examples, together with statistical information about the entire population, can be aggregated to obtain labels on unseen examples. We validate our approach using labeled data from the political domain. We provide two applications of the proposed algorithm to the medical domain. In the first, we demonstrate how to identify users whose search patterns indicate they might be suffering from certain types of cancer. This shows, for the first time, that search queries can be used as a screening device for diseases that are currently often discovered too late, because no early screening tests exists. In the second, we detail an algorithm to predict the distribution of diseases given their incidence in a subset of the population at study, making it possible to predict disease spread from partial epidemiological data.",Inferring Individual Attributes from Search Engine Queries and Auxiliary Information,NA:NA,2017
Daniela Perrotta:Michele Tizzoni:Daniela Paolotti,"Traditional surveillance of seasonal influenza is generally affected by reporting lags of at least one week and by continuous revisions of the numbers initially released. As a consequence, influenza forecasts are often limited by the time required to collect new and accurate data. On the other hand, the availability of novel data streams for disease detection can help in overcoming these issues by capturing an additional surveillance signal that can be used to complement data collected by public health agencies. In this study, we investigate how combining both traditional and participatory Web-based surveillance data can provide accurate predictions for seasonal influenza in real-time fashion. To this aim, we use two data sources available in Italy from two different monitoring systems: traditional surveillance data based on sentinel doctors reports and digital surveillance data deriving from a participatory system that monitors the influenza activity through Internet-based surveys. We integrate such digital component in a linear autoregressive exogenous (ARX) model based on traditional surveillance data and evaluate its predictive ability over the course of four influenza seasons in Italy, from 2012-2013 to 2015-2016, for each of the four weekly time horizons. Our results show that by using data extracted from a Web-based participatory surveillance system, which are usually available one week in advance with respect to traditional surveillance, it is possible to obtain accurate weekly predictions of influenza activity at national level up to four weeks in advance. Compared to a model that is only based on data from sentinel doctors, our approach significantly improves real-time forecasts of influenza activity, by increasing the Pearson's correlation up to 30% and by reducing the Mean Absolute Error up to 43% for the four weekly time horizons.",Using Participatory Web-based Surveillance Data to Improve Seasonal Influenza Forecasting in Italy,NA:NA:NA,2017
Qian Zhang:Nicola Perra:Daniela Perrotta:Michele Tizzoni:Daniela Paolotti:Alessandro Vespignani,"The availability of novel digital data streams that can be used as proxy for monitoring infectious disease incidence is ushering in a new era for real-time forecast approaches to disease spreading. Here, we propose the first seasonal influenza forecast framework based on a stochastic, spatially structured mechanistic model (individual level microsimulation) initialized with geo-localized microblogging data. The framework provides for more than 600 census areas in the United States, Italy and Spain, the initial conditions for a stochastic epidemic computational model that generates an ensemble of forecasts for the main indicators of the epidemic season: peak time and intensity. We evaluate the forecasts accuracy and reliability by comparing the results with the data from the official influenza surveillance systems in the US, Italy and Spain in the seasons 2014/15 and 2015/16. In all countries studied, the proposed framework provides reliable results with leads of up to 6 weeks that became more stable and accurate with progression of the season. The results for the United States have been generated in real-time in the context of the Centers for Disease Control and Prevention ``Forecasting the Influenza Season Challenge''. A characteristic feature of the mechanistic modeling approach is in the explicit estimate of key epidemiological parameters relevant for public health decision-making that cannot be achieved with statistical models that do not consider the disease dynamic. Furthermore, the presented framework allows the fusion of multiple data streams in the initialization stage and can be enriched with census, weather and socioeconomic data.",Forecasting Seasonal Influenza Fusing Digital Indicators and a Mechanistic Disease Model,NA:NA:NA:NA:NA:NA,2017
Maulik R. Kamdar:Mark A. Musen,"Integrated approaches for pharmacology are required for the mechanism-based predictions of adverse drug reactions that manifest due to concomitant intake of multiple drugs. These approaches require the integration and analysis of biomedical data and knowledge from multiple, heterogeneous sources with varying schemas, entity notations, and formats. To tackle these integrative challenges, the Semantic Web community has published and linked several datasets in the Life Sciences Linked Open Data (LSLOD) cloud using established W3C standards. We present the PhLeGrA platform for Linked Graph Analytics in Pharmacology in this paper. Through query federation, we integrate four sources from the LSLOD cloud and extract a drug-reaction network, composed of distinct entities. We represent this graph as a hidden conditional random field (HCRF), a discriminative latent variable model that is used for structured output predictions. We calculate the underlying probability distributions in the drug-reaction HCRF using the datasets from the U.S. Food and Drug Administration's Adverse Event Reporting System. We predict the occurrence of 146 adverse reactions due to multiple drug intake with an AUROC statistic greater than 0.75. The PhLeGrA platform can be extended to incorporate other sources published using Semantic Web technologies, as well as to discover other types of pharmacological associations.",PhLeGrA: Graph Analytics in Pharmacology over the Web of Life Sciences Linked Open Data,NA:NA,2017
Min Hong Yun:Songtao He:Lin Zhong,"Drawing or dragging an object on a mobile device is annoying today because the latency is manifested spatially with an obvious gap between the touch point and the line head or dragged object. This work identifies the multiple synchronization points in the input to display path of modern mobile systems as a major source of latency, contributing about 30 ms to the overall latency. We present Presto, an asynchronous design of the input to display path. By focusing on the main application and relaxing conventional requirements of no frame drop and no tearing effects, Presto is able to eliminate much of the latency due to synchrony. By carefully guarding against consecutive frame drops and limiting the risk of tearing to a small region around the touch point, Presto is able to reduce their visual impact to barely noticeable. Using a prototype based on Android 5, we are able to quantify the effectiveness, overhead and user experience of Presto through both objective measurements and subjective user assessment. We show that Presto is able to reduce the latency of legacy Android applications by close to half; and more importantly, we show this reduction is orthogonal to that by other popular approaches. When combined with touch prediction, Presto is able to reduce the touch latency below 10 ms, a remarkable achievement without any hardware support.",Reducing Latency by Eliminating Synchrony,NA:NA:NA,2017
Giovanni Campagna:Rakesh Ramesh:Silei Xu:Michael Fischer:Monica S. Lam,"This paper presents the architecture of Almond, an open, crowdsourced, privacy-preserving and programmable virtual assistant for online services and the Internet of Things (IoT). Included in Almond is Thingpedia, a crowdsourced public knowledge base of natural language interfaces and open APIs. Our proposal addresses four challenges in virtual assistant technology: generality, interoperability, privacy, and usability. Generality is addressed by crowdsourcing Thingpedia, while interoperability is provided by ThingTalk, a high-level domain-specific language that connects multiple devices or services via open APIs. For privacy, user credentials and user data are managed by our open-source ThingSystem, which can be run on personal phones or home servers. Finally, we address usability by providing a natural language interface, whose capability can be extended via training with the help of a menu-driven interface. We have created a fully working prototype, and crowdsourced a set of 187 functions across 45 different kinds of devices. Almond is the first virtual assistant that lets users specify trigger-action tasks in natural language. Despite the lack of real usage data, our experiment suggests that Almond can understand about 40% of the complex tasks when uttered by a user familiar with its capability.","Almond: The Architecture of an Open, Crowdsourced, Privacy-Preserving, Programmable Virtual Assistant",NA:NA:NA:NA:NA,2017
Shuochao Yao:Shaohan Hu:Yiran Zhao:Aston Zhang:Tarek Abdelzaher,"Mobile sensing and computing applications usually require time-series inputs from sensors, such as accelerometers, gyroscopes, and magnetometers. Some applications, such as tracking, can use sensed acceleration and rate of rotation to calculate displacement based on physical system models. Other applications, such as activity recognition, extract manually designed features from sensor inputs for classification. Such applications face two challenges. On one hand, on-device sensor measurements are noisy. For many mobile applications, it is hard to find a distribution that exactly describes the noise in practice. Unfortunately, calculating target quantities based on physical system and noise models is only as accurate as the noise assumptions. Similarly, in classification applications, although manually designed features have proven to be effective, it is not always straightforward to find the most robust features to accommodate diverse sensor noise patterns and heterogeneous user behaviors. To this end, we propose DeepSense, a deep learning framework that directly addresses the aforementioned noise and feature customization challenges in a unified manner. DeepSense integrates convolutional and recurrent neural networks to exploit local interactions among similar mobile sensors, merge local interactions of different sensory modalities into global interactions, and extract temporal relationships to model signal dynamics. DeepSense thus provides a general signal estimation and classification framework that accommodates a wide range of applications. We demonstrate the effectiveness of DeepSense using three representative and challenging tasks: car tracking with motion sensors, heterogeneous human activity recognition, and user identification with biometric motion analysis. DeepSense significantly outperforms the state-of-the-art methods for all three tasks. In addition, we show that DeepSense is feasible to implement on smartphones and embedded devices thanks to its moderate energy consumption and low latency.",DeepSense: A Unified Deep Learning Framework for Time-Series Mobile Sensing Data Processing,NA:NA:NA:NA:NA,2017
Chao Zhang:Keyang Zhang:Quan Yuan:Haoruo Peng:Yu Zheng:Tim Hanratty:Shaowen Wang:Jiawei Han,"With the ever-increasing urbanization process, systematically modeling people's activities in the urban space is being recognized as a crucial socioeconomic task. This task was nearly impossible years ago due to the lack of reliable data sources, yet the emergence of geo-tagged social media (GTSM) data sheds new light on it. Recently, there have been fruitful studies on discovering geographical topics from GTSM data. However, their high computational costs and strong distributional assumptions about the latent topics hinder them from fully unleashing the power of GTSM. To bridge the gap, we present CrossMap, a novel cross-modal representation learning method that uncovers urban dynamics with massive GTSM data. CrossMap first employs an accelerated mode seeking procedure to detect spatiotemporal hotspots underlying people's activities. Those detected hotspots not only address spatiotemporal variations, but also largely alleviate the sparsity of the GTSM data. With the detected hotspots, CrossMap then jointly embeds all spatial, temporal, and textual units into the same space using two different strategies: one is reconstruction-based and the other is graph-based. Both strategies capture the correlations among the units by encoding their co-occurrence and neighborhood relationships, and learn low-dimensional representations to preserve such correlations. Our experiments demonstrate that CrossMap not only significantly outperforms state-of-the-art methods for activity recovery and classification, but also achieves much better efficiency.","Regions, Periods, Activities: Uncovering Urban Dynamics via Cross-Modal Representation Learning",NA:NA:NA:NA:NA:NA:NA:NA,2017
Dimitris Serbos:Shuyao Qi:Nikos Mamoulis:Evaggelia Pitoura:Panayiotis Tsaparas,"Recommending packages of items to groups of users has several applications, including recommending vacation packages to groups of tourists, entertainment packages to groups of friends, or sets of courses to groups of students. In this paper, we focus on a novel aspect of package-to-group recommendations, that of fairness. Specifically, when we recommend a package to a group of people, we ask that this recommendation is fair in the sense that every group member is satisfied by a sufficient number of items in the package. We explore two definitions of fairness and show that for either definition the problem of finding the most fair package is NP-hard. We exploit the fact that our problem can be modeled as a coverage problem, and we propose greedy algorithms that find approximate solutions within reasonable time. In addition, we study two extensions of the problem, where we impose category or spatial constraints on the items to be included in the recommended packages. We evaluate the appropriateness of the fairness models and the performance of the proposed algorithms using real data from Yelp, and a user study.",Fairness in Package-to-Group Recommendations,NA:NA:NA:NA:NA,2017
Shiyu Chang:Yang Zhang:Jiliang Tang:Dawei Yin:Yi Chang:Mark A. Hasegawa-Johnson:Thomas S. Huang,"The increasing popularity of real-world recommender systems produces data continuously and rapidly, and it becomes more realistic to study recommender systems under streaming scenarios. Data streams present distinct properties such as temporally ordered, continuous and high-velocity, which poses tremendous challenges to traditional recommender systems. In this paper, we investigate the problem of recommendation with stream inputs. In particular, we provide a principled framework termed sRec, which provides explicit continuous-time random process models of the creation of users and topics, and of the evolution of their interests. A variational Bayesian approach called recursive meanfield approximation is proposed, which permits computationally efficient instantaneous on-line inference. Experimental results on several real-world datasets demonstrate the advantages of our sRec over other state-of-the-arts.",Streaming Recommender Systems,NA:NA:NA:NA:NA:NA:NA,2017
Suhang Wang:Yilin Wang:Jiliang Tang:Kai Shu:Suhas Ranganath:Huan Liu,"The rapid growth of Location-based Social Networks (LBSNs) provides a vast amount of check-in data, which facilitates the study of point-of-interest (POI) recommendation. The majority of the existing POI recommendation methods focus on four aspects, i.e., temporal patterns, geographical influence, social correlations and textual content indications. For example, user's visits to locations have temporal patterns and users are likely to visit POIs near them. In real-world LBSNs such as Instagram, users can upload photos associating with locations. Photos not only reflect users' interests but also provide informative descriptions about locations. For example, a user who posts many architecture photos is more likely to visit famous landmarks; while a user posts lots of images about food has more incentive to visit restaurants. Thus, images have potentials to improve the performance of POI recommendation. However, little work exists for POI recommendation by exploiting images. In this paper, we study the problem of enhancing POI recommendation with visual contents. In particular, we propose a new framework Visual Content Enhanced POI recommendation (VPOI), which incorporates visual contents for POI recommendations. Experimental results on real-world datasets demonstrate the effectiveness of the proposed framework.",What Your Images Reveal: Exploiting Visual Contents for Point-of-Interest Recommendation,NA:NA:NA:NA:NA:NA,2017
Tuan-Anh Nguyen Pham:Xutao Li:Gao Cong,"With the rapid growth of location-based social networks (LBSNs), it is now available to analyze and understand user mobility behavior in real world. Studies show that users usually visit nearby points of interest (POIs), located in small regions, especially when they travel out of their hometowns. However, previous out-of-town recommendation systems mainly focus on recommending individual POIs that may reside far from each other, which makes the recommendation results less useful. In this paper, we introduce a novel problem called Region Recommendation, which aims to recommend an out-of-town region of POIs that are likely to be visited by a user. The proximity characteristic of user mobility behavior implies that the probability of visiting one POI depends on those of nearby POIs. Thus, to make accurate region recommendation, our proposed model exploits the influence between POIs, instead of treating them individually. Moreover, to overcome the efficiency problem of searching the best region, we propose a sweeping line-based method, and subsequently an constant-bounded algorithm for better efficiency. Experiments on two real-world datasets demonstrate the improved effectiveness of our models over baseline methods and efficiency of the approximate algorithm.",A General Model for Out-of-town Region Recommendation,NA:NA:NA,2017
Ravi Kumar:Maithra Raghu:Tamás Sarlós:Andrew Tomkins,"We introduce LAMP: the Linear Additive Markov Process. Transitions in LAMP may be influenced by states visited in the distant history of the process, but unlike higher-order Markov processes, LAMP retains an efficient parameterization. LAMP also allows the specific dependence on history to be learned efficiently from data. We characterize some theoretical properties of LAMP, including its steady-state and mixing time. We then give an algorithm based on alternating minimization to learn LAMP models from data. Finally, we perform a series of real-world experiments to show that LAMP is more powerful than first-order Markov processes, and even holds its own against deep sequential models (LSTMs) with a negligible increase in parameter complexity.",Linear Additive Markov Processes,NA:NA:NA:NA,2017
Alessandro Epasto:Silvio Lattanzi:Sergei Vassilvitskii:Morteza Zadimoghaddam,"Maximizing submodular functions under cardinality constraints lies at the core of numerous data mining and machine learning applications, including data diversification, data summarization, and coverage problems. In this work, we study this question in the context of data streams, where elements arrive one at a time, and we want to design low-memory and fast update-time algorithms that maintain a good solution. Specifically, we focus on the sliding window model, where we are asked to maintain a solution that considers only the last W items. In this context, we provide the first non-trivial algorithm that maintains a provable approximation of the optimum using space sublinear in the size of the window. In particular we give a 1/3 - ε approximation algorithm that uses space polylogarithmic in the spread of the values of the elements, δ, and linear in the solution size k for any constant ε > 0. At the same time, processing each element only requires a polylogarithmic number of evaluations of the function itself. When a better approximation is desired, we show a different algorithm that, at the cost of using more memory, provides a 1/2 - ε approximation, and allows a tunable trade-off between average update time and space. This algorithm matches the best known approximation guarantees for submodular optimization in insertion-only streams, a less general formulation of the problem. We demonstrate the efficacy of the algorithms on a number of real world datasets, showing that their practical performance far exceeds the theoretical bounds. The algorithms preserve high quality solutions in streams with millions of items, while storing a negligible fraction of them.",Submodular Optimization Over Sliding Windows,NA:NA:NA:NA,2017
Aneesh Sharma:C. Seshadhri:Ashish Goel,"Finding similar user pairs is a fundamental task in social networks, with numerous applications in ranking and personalization tasks such as link prediction and tie strength detection. A common manifestation of user similarity is based upon network structure: each user is represented by a vector that represents the user's network connections, where pairwise cosine similarity among these vectors defines user similarity. The predominant task for user similarity applications is to discover all similar pairs that have a pairwise cosine similarity value larger than a given threshold τ. In contrast to previous work where τ is assumed to be quite close to 1, we focus on recommendation applications where τ is small, but still meaningful. The all pairs cosine similarity problem is computationally challenging on networks with billions of edges, and especially so for settings with small τ. To the best of our knowledge, there is no practical solution for computing all user pairs with, say τ = 0.2 on large social networks, even using the power of distributed algorithms. Our work directly addresses this challenge by introducing a new algorithm --- WHIMP --- that solves this problem efficiently in the MapReduce model. The key insight in WHIMP is to combine the ``wedge-sampling"" approach of Cohen-Lewis for approximate matrix multiplication with the SimHash random projection techniques of Charikar. We provide a theoretical analysis of WHIMP, proving that it has near optimal communication costs while maintaining computation cost comparable with the state of the art. We also empirically demonstrate WHIMP's scalability by computing all highly similar pairs on four massive data sets, and show that it accurately finds high similarity pairs. In particular, we note that WHIMP successfully processes the entire Twitter network, which has tens of billions of edges.",When Hashes Met Wedges: A Distributed Algorithm for Finding High Similarity Vectors,NA:NA:NA,2017
Shweta Jain:C. Seshadhri,"Clique counts reveal important properties about the structure of massive graphs, especially social networks. The simple setting of just 3-cliques (triangles) has received much attention from the research community. For larger cliques (even, say 6-cliques) the problem quickly becomes intractable because of combinatorial explosion. Most methods used for triangle counting do not scale for large cliques, and existing algorithms require massive parallelism to be feasible. We present a new randomized algorithm that provably approximates the number of k-cliques, for any constant k. The key insight is the use of (strengthenings of) the classic Turán's theorem: this claims that if the edge density of a graph is sufficiently high, the k-clique density must be non-trivial. We define a combinatorial structure called a Turàn shadow, the construction of which leads to fast algorithms for clique counting. We design a practical heuristic, called TURÀN-SHADOW, based on this theoretical algorithm, and test it on a large class of test graphs. In all cases, TURÀN-SHADOW has less than 2% error, in a fraction of the time used by well-tuned exact algorithms. We do detailed comparisons with a range of other sampling algorithms, and find that TURÀN-SHADOW is generally much faster and more accurate. For example, TURÀN-SHADOW estimates all cliques numbers up to size 10 in social network with over a hundred million edges. This is done in less than three hours on a single commodity machine.",A Fast and Provable Method for Estimating Clique Counts Using Turán's Theorem,NA:NA,2017
Gareth Tyson:Shan Huang:Felix Cuadrado:Ignacio Castro:Vasile C. Perta:Arjuna Sathiaseelan:Steve Uhlig,"Headers are a critical part of HTTP, and it has been shown that they are increasingly subject to middlebox manipulation. Although this is well known, little is understood about the general regional and network trends that underpin these manipulations. In this paper, we collect data on thousands of networks to understand how they intercept HTTP headers in-the-wild. Our analysis reveals that 25% of measured ASes modify HTTP headers. Beyond this, we witness distinct trends among different regions and AS types; e.g., we observe high numbers of cache headers in poorly connected regions. Finally, we perform an in-depth analysis of the types of manipulations and how they differ across regions.",Exploring HTTP Header Manipulation In-The-Wild,NA:NA:NA:NA:NA:NA:NA,2017
Sanae Rosen:Bo Han:Shuai Hao:Z. Morley Mao:Feng Qian,"In HTTP/1.1, it is necessary for the client to request an object (e.g. an image in a page) in order for the server to send it, even if the server knows in advance what the client will need. Server Push is a feature introduced in HTTP/2 that promises to improve page load times (PLT) by having the server push content to the browser in advance. In this paper, we investigate the benefits and challenges of using Server Push on mobile devices. We first examine whether pushing all content or just the CSS and Javascript files performs better, and find the former leads to much better web performance. Also, we find that sites making use of domain sharding or which otherwise have content divided across many servers do not benefit much from Server Push, a major challenge for Server Push going forward. Network performance characteristics also play a major role. Server Push is especially effective at improving performance at high loss rates (16% median PLT reduction with a 2% loss rate) and high latencies (14% PLT reduction with 100 ms latency), and has little benefit for high-speed Ethernet connections. This motivates its use on mobile devices, although we also find the limited processing power of these devices limits the benefits of Server Push. Server Push also offers modest energy benefits, with energy savings of 9% on LTE for one device. Overall, Server Push is a promising approach for improving web performance in mobile networks, but there are a number of challenges in achieving the full benefits of Server Push.",Push or Request: An Investigation of HTTP/2 Server Push for Improving Mobile Performance,NA:NA:NA:NA:NA,2017
Hiranya Jayathilaka:Chandra Krintz:Rich Wolski,"In this paper, we describe Roots - a system for automatically identifying the ""root cause"" of performance anomalies in web applications deployed in Platform-as-a-Service (PaaS) clouds. Roots does not require application-level instrumentation. Instead, it tracks events within the PaaS cloud that are triggered by application requests using a combination of metadata injection and platform-level instrumentation. We describe the extensible architecture of Roots, a prototype implementation of the system, and a statistical methodology for performance anomaly detection and diagnosis. We evaluate the efficacy of Roots using a set of PaaS-hosted web applications, and detail the performance overhead and scalability of the implementation.",Performance Monitoring and Root Cause Analysis for Cloud-hosted Web Applications,NA:NA:NA,2017
Valentin Dalibard:Michael Schaarschmidt:Eiko Yoneki,"Due to their complexity, modern systems expose many configuration parameters which users must tune to maximize performance. Auto-tuning has emerged as an alternative in which a black-box optimizer iteratively evaluates configurations to find efficient ones. Unfortunately, for many systems, such as distributed systems, evaluating performance takes too long and the space of configurations is too large for the optimizer to converge within a reasonable time. We present BOAT, a framework which allows developers to build efficient bespoke auto-tuners for their system, in situations where generic auto-tuners fail. At BOAT's core is structured Bayesian optimization (SBO), a novel extension of the Bayesian optimization algorithm. SBO leverages contextual information provided by system developers, in the form of a probabilistic model of the system's behavior, to make informed decisions about which configurations to evaluate. In a case study, we tune the scheduling of a neural network computation on a heterogeneous cluster. Our auto-tuner converges within ten iterations. The optimized configurations outperform those found by generic auto-tuners in thirty iterations by up to 2X.",BOAT: Building Auto-Tuners with Structured Bayesian Optimization,NA:NA:NA,2017
Christoph Trattner:David Elsweiler,"Food recommenders have the potential to positively influence the eating habits of users. To achieve this, however, we need to understand how healthy recommendations are and the factors which influence this. Focusing on two approaches from the literature (single item and daily meal plan recommendation) and utilizing a large Internet sourced dataset from Allrecipes.com, we show how algorithmic solutions relate to the healthiness of the underlying recipe collection. First, we analyze the healthiness of Allrecipes.com recipes using nutritional standards from the World Health Organisation and the United Kingdom Food Standards Agency. Second, we investigate user interaction patterns and how these relate to the healthiness of recipes. Third, we experiment with both recommendation approaches. Our results indicate that overall the recipes in the collection are quite unhealthy, but this varies across categories on the website. Users in general tend to interact most often with the least healthy recipes. Recommender algorithms tend to score popular items highly and thus on average promote unhealthy items. This can be tempered, however, with simple post-filtering approaches, which we show by experiment are better suited to some algorithms than others. Similarly, we show that the generation of meal plans can dramatically increase the number of healthy options open to users. One of the main findings is, nevertheless, that the utility of both approaches is strongly restricted by the recipe collection. Based on our findings we draw conclusions how researchers should attempt to make food recommendation systems promote healthy nutrition.",Investigating the Healthiness of Internet-Sourced Recipes: Implications for Meal Planning and Recommender Systems,NA:NA,2017
Deepika Yadav:Pushpendra Singh:Kyle Montague:Vijay Kumar:Deepak Sood:Madeline Balaam:Drishti Sharma:Mona Duggal:Tom Bartindale:Delvin Varghese:Patrick Olivier,"The Healthcare system of India provides outreach services to the rural population with a key focus on the maternal and child health through its flagship program of Community Health Workers (CHWs). The program since its launch has reached a scale of over 900000 health workers across the country and observed significant benefits on the health indicators. However, traditional face to face training mechanisms face persistent challenge in providing adequate training and capacity building opportunities to CHWs which leads to their sub-optimal knowledge and skill sets. In this paper, we propose Sangoshthi, a low-cost mobile based training and learning platform that fits well into the environment of low-Internet access. Sangoshthi leverages the architecture that combines Internet and IVR technology to host real time training sessions with the CHWs having access to basic phones only. We present our findings of a four week long field deployment with 40 CHWs using both qualitative and quantitative methods. Sangoshthi offers a lively environment of peer learning that was well received by the CHW community and resulted into their knowledge gains (16%) and increased confidence levels to handle the cases. Our study highlights the potential of complementary training platforms that can empower CHWs in-situ without the need of additional infrastructure.",Sangoshthi: Empowering Community Health Workers through Peer Learning in Rural India,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2017
Ferda Ofli:Yusuf Aytar:Ingmar Weber:Raggi al Hammouri:Antonio Torralba,"Food is an integral part of our life and what and how much we eat crucially affects our health. Our food choices largely depend on how we perceive certain characteristics of food, such as whether it is healthy, delicious or if it qualifies as a salad. But these perceptions differ from person to person and one person's ""single lettuce leaf"" might be another person's ""side salad"". Studying how food is perceived in relation to what it actually is typically involves a laboratory setup. Here we propose to use recent advances in image recognition to tackle this problem. Concretely, we use data for 1.9 million images from Instagram from the US to look at systematic differences in how a machine would objectively label an image compared to how a human subjectively does. We show that this difference, which we call the ""perception gap"", relates to a number of health outcomes observed at the county level. To the best of our knowledge, this is the first time that image recognition is being used to study the ""misalignment"" of how people describe food images vs. what they actually depict.",Is Saki #delicious?: The Food Perception Gap on Instagram and Its Relation to Health,NA:NA:NA:NA:NA,2017
David Stück:Haraldur Tómas Hallgrímsson:Greg Ver Steeg:Alessandro Epasto:Luca Foschini,"Many behaviors that lead to worsened health outcomes are modifiable, social, and visible. Social influence has thus the potential to foster adoption of habits that promote health and improve disease management. In this study, we consider the evolution of the physical activity of 44.5 thousand Fitbit users as they interact on the Fitbit social network, in relation to their health status. The users collectively recorded 9.3 million days of steps over the period of a year through a Fitbit device. 7,515 of the users also self-reported whether they were diagnosed with a major chronic condition. A time-aggregated analysis shows that ego net size, average alter physical activity, gender, and body mass index (BMI) are significantly predictive of ego physical activity. For users who self-reported chronic conditions, the direction and effect size of associations varied depending on the condition, with diabetic users specifically showing almost a 6-fold increase in additional daily steps for each additional social tie. Subsequently, we consider the co-evolution of activity and friendship longitudinally on a month by month basis. We show that the fluctuations in average alter activity significantly predict fluctuations in ego activity. By leveraging a class of novel non-parametric statistical tests we investigate the causal factors in these fluctuations. We find that under certain stationarity assumptions, non-null causal dependence exists between ego and alter's activity, even in the presence of unobserved stationary individual traits. We believe that our findings provide evidence that the study of online social networks have the potential to improve our understanding of factors affecting adoption of positive habits, especially in the context of chronic condition management.",The Spread of Physical Activity Through Social Networks,NA:NA:NA:NA:NA,2017
David Goldberg:Andrew Trotman:Xiao Wang:Wei Min:Zongru Wan,"The quality of a search engine is typically evaluated using hand-labeled data sets, where the labels indicate the relevance of documents to queries. Often the number of labels needed is too large to be created by the best annotators, and so less accurate labels (e.g. from crowdsourcing) must be used. This introduces errors in the labels, and thus errors in standard precision metrics (such as [email protected] and DCG); the lower the quality of the judge, the more errorful the labels, consequently the more inaccurate the metric. We introduce equations and algorithms that can adjust the metrics to the values they would have had if there were no annotation errors. This is especially important when two search engines are compared by comparing their metrics. We give examples where one engine appeared to be statistically significantly better than the other, but the effect disappeared after the metrics were corrected for annotation error. In other words the evidence supporting a statistical difference was illusory, and caused by a failure to account for annotation error.",Drawing Sound Conclusions from Noisy Judgments,NA:NA:NA:NA:NA,2017
Liangda Li:Hongbo Deng:Anlei Dong:Yi Chang:Ricardo Baeza-Yates:Hongyuan Zha,"Contextual data plays an important role in modeling search engine users' behaviors on both query auto-completion (QAC) log and normal query (click) log. User's recent search history on each log has been widely studied individually as the context to benefit the modeling of users' behaviors on that log. However, there is no existing work that explores or incorporates both logs together for contextual data. As QAC and click logs actually record users' sequential behaviors while interacting with a search engine, the available context of a user's current behavior based on the same type of log can be strengthened from the user's recent search history shown on the other type of log. Our paper proposes to model users' behaviors on both QAC and click logs simultaneously by utilizing both logs as the contextual data of each other. The key idea is to capture the correlation between users' behavior patterns on both logs. We model such correlation through a novel probabilistic model based on the Latent Dirichlet allocation (LDA) model. The learned users' behavior patterns on both logs are utilized to address not only the application of query auto-completion on QAC logs, but also the click prediction and relevance ranking of web documents on click logs. Experiments on real-world logs demonstrate the effectiveness of the proposed model on both applications.",Exploring Query Auto-Completion and Click Logs for Contextual-Aware Web Search and Query Suggestion,NA:NA:NA:NA:NA:NA,2017
Adam Fourney:Meredith Ringel Morris:Ryen W. White,"Many people rely on web search engines to check the spelling or grammatical correctness of input phrases. For example, one might search [recurring or reoccurring] to decide between these similar words. While language-related queries are common, they have low click-through rates, lack a strong intent signal, and are generally challenging to study. Perhaps for these reasons, they have yet to be characterized in the literature. In this paper we report the results of two surveys that investigate how, when, and why people use web search to support low-level, language-related tasks. The first survey was distributed by email, and asked participants to reflect on a recent search task. The second survey was embedded directly in search result pages, and captured information about searchers' intents in-situ. Our analysis confirms that language-related search tasks are indeed common, accounting for at least 2.7% of all queries posed by our respondents. Survey responses also reveal: (1) the range of language-related tasks people perform with search, (2) the contexts in which these tasks arise, and (3), the reasons why people elect to use web search rather than relying on traditional proofing tools (e.g., spelling and grammar checkers).",Web Search as a Linguistic Tool,NA:NA:NA,2017
Yashen Wang:Heyan Huang:Chong Feng,"We tackle the problem of improving microblog retrieval algorithms by proposing a Feedback Concept Model for query expansion. In particular, we expand the query using knowledge information derived from Probase so that the expanded one could better reflect users' search intent, which allows for microblog retrieval at a concept-level, rather than term-level. In the proposed feedback concept model: (i) we mine the concept information implicit in short-texts based on the external knowledge bases; (ii) with the relevant concepts associated with short-texts, a mixture model is generated to estimate a concept language model; (iii) finally, we utilize the concept language model for query expansion. Moreover, we incorporate temporal prior into the proposed query expansion method to satisfy real-time information need. Finally, we test the generalization power of the feedback concept model on the TREC Microblog corpora. The experimental results demonstrate that the proposed model outperforms the previous methods for microblog retrieval significantly.",Query Expansion Based on a Feedback Concept Model for Microblog Retrieval,NA:NA:NA,2017
Karol Wegrzycki:Piotr Sankowski:Andrzej Pacuk:Piotr Wygocki,"We introduce random directed acyclic graph and use it to model the information diffusion network. Subsequently, we analyze the cascade generation model (CGM) introduced by Leskovec et al. [19]. Until now only empirical studies of this model were done. In this paper, we present the first theoretical proof that the sizes of cascades generated by the CGM follow the power-law distribution, which is consistent with multiple empirical analysis of the large social networks. We compared the assumptions of our model with the Twitter social network and tested the goodness of approximation.",Why Do Cascade Sizes Follow a Power-Law?,NA:NA:NA:NA,2017
Cheng Li:Jiaqi Ma:Xiaoxiao Guo:Qiaozhu Mei,"Information cascades, effectively facilitated by most social network platforms, are recognized as a major factor in almost every social success and disaster in these networks. Can cascades be predicted? While many believe that they are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted by a machine learning algorithm that combines many features. These predictors all depend on a bag of hand-crafting features to represent the cascade network and the global network structures. Such features, always carefully and sometimes mysteriously designed, are not easy to extend or to generalize to a different platform or domain. Inspired by the recent successes of deep learning in multiple data mining tasks, we investigate whether an end-to-end deep learning approach could effectively predict the future size of cascades. Such a method automatically learns the representation of individual cascade graphs in the context of the global network structure, without hand-crafted features or heuristics. We find that node embeddings fall short of predictive power, and it is critical to learn the representation of a cascade graph as a whole. We present algorithms that learn the representation of cascade graphs in an end-to-end manner, which significantly improve the performance of cascade prediction over strong baselines including feature based methods, node embedding methods, and graph kernel methods. Our results also provide interesting implications for cascade prediction in general.",DeepCas: An End-to-end Predictor of Information Cascades,NA:NA:NA:NA,2017
Rahmtin Rotabi:Krishna Kamath:Jon Kleinberg:Aneesh Sharma,"Cascades on social and information networks have been a tremendously popular subject of study in the past decade, and there is a considerable literature on phenomena such as diffusion mechanisms, virality, cascade prediction, and peer network effects. Against the backdrop of this research, a basic question has received comparatively little attention: how desirable are cascades on a social media platform from the point of view of users' While versions of this question have been considered from the perspective of the producers of cascades, any answer to this question must also take into account the effect of cascades on their audience --- the viewers of the cascade who do not directly participate in generating the content that launched it. In this work, we seek to fill this gap by providing a consumer perspective of information cascades. Users on social and information networks play the dual role of producers and consumers, and our work focuses on how users perceive cascades as consumers. Starting from this perspective, we perform an empirical study of the interaction of Twitter users with retweet cascades. We measure how often users observe retweets in their home timeline, and observe a phenomenon that we term the Impressions Paradox: the share of impressions for cascades of size k decays much more slowly than frequency of cascades of size k. Thus, the audience for cascades can be quite large even for rare large cascades. We also measure audience engagement with retweet cascades in comparison to non-retweeted or organic content. Our results show that cascades often rival or exceed organic content in engagement received per impression. This result is perhaps surprising in that consumers didn't opt in to see tweets from these authors. Furthermore, although cascading content is widely popular, one would expect it to eventually reach parts of the audience that may not be interested in the content. Motivated by the tension in these empirical findings, we posit a simple theoretical model that focuses on the effect of cascades on the audience (rather than the cascade producers). Our results on this model highlight the balance between retweeting as a high-quality content selection mechanism and the role of network users in filtering irrelevant content. In particular, the results suggest that together these two effects enable the audience to consume a high quality stream of content in the presence of cascades.",Cascades: A View from Audience,NA:NA:NA:NA,2017
Karthik Subbian:B. Aditya Prakash:Lada Adamic,"Detecting large reshare cascades is an important problem in online social networks. There are a variety of attempts to model this problem, from using time series analysis methods to stochastic processes. Most of these approaches heavily depend on the underlying network features and use network information to detect the virality of cascades. In most cases, however, getting such detailed network information can be hard or even impossible. In contrast, in this paper, we propose SANSNET, a network-agnostic approach instead. Our method can be used to answer two important questions: (1) Will a cascade go viral? and (2) How early can we predict it? We use techniques from survival analysis to build a supervised classifier in the space of survival probabilities and show that the optimal decision boundary is a survival function. A notable feature of our approach is that it does not use any network-based features for the prediction tasks, making it very cheap to implement. Finally, we evaluate our approach on several real-life data sets, including popular social networks like Facebook and Twitter, on metrics like recall, F-measure and breakout coverage. We find that network agnostic SANSNET classifier outperforms several non-trivial competitors and baselines which utilize network information.",Detecting Large Reshare Cascades in Social Networks,NA:NA:NA,2017
Zhijing Li:Ana Nika:Xinyi Zhang:Yanzi Zhu:Yuanshun Yao:Ben Y. Zhao:Haitao Zheng,"While crowdsourcing is an attractive approach to collect large-scale wireless measurements, understanding the quality and variance of the resulting data is difficult. Our work analyzes the quality of crowdsourced cellular signal measurements in the context of basestation localization, using large international public datasets (419M signal measurements and 1M cells) and corresponding ground truth values. Performing localization using raw received signal strength (RSS) data produces poor results and very high variance. Applying supervised learning improves results moderately, but variance remains high. Instead, we propose feature clustering, a novel application of unsupervised learning to detect hidden correlation between measurement instances, their features, and localization accuracy. Our results identify RSS standard deviation and RSS-weighted dispersion mean as key features that correlate with highly predictive measurement samples for both sparse and dense measurements respectively. Finally, we show how optimizing crowdsourcing measurements for these two features dramatically improves localization accuracy and reduces variance.",Identifying Value in Crowdsourced Wireless Signal Measurements,NA:NA:NA:NA:NA:NA:NA,2017
Nikhil Garg:Vijay Kamble:Ashish Goel:David Marn:Kamesh Munagala,"Many societal decision problems lie in high-dimensional continuous spaces not amenable to the voting techniques common for their discrete or single-dimensional counterparts. These problems are typically discretized before running an election or decided upon through negotiation by representatives. We propose a meta-algorithm called Iterative Local Voting for collective decision-making in this setting, in which voters are sequentially sampled and asked to modify a candidate solution within some local neighborhood of its current value, as defined by a ball in some chosen norm. In general, such schemes do not converge, or, when they do, the resulting solution does not have a natural description. We first prove the convergence of this algorithm under appropriate choices of neighborhoods to plausible solutions in certain natural settings: when the voters' utilities can be expressed in terms of some form of distance from their ideal solution, and when these utilities are additively decomposable across dimensions. In many of these cases, we obtain convergence to the societal welfare maximizing solution. We then describe an experiment in which we test our algorithm for the decision of the U.S. Federal Budget on Mechanical Turk with over 4,000 workers, employing neighborhoods defined by §L1, §L2 and §L∞ balls. We make several observations that inform future implementations of such a procedure.",Collaborative Optimization for Collective Decision-making in Continuous Spaces,NA:NA:NA:NA:NA,2017
Leye Wang:Dingqi Yang:Xiao Han:Tianben Wang:Daqing Zhang:Xiaojuan Ma,"In traditional mobile crowdsensing applications, organizers need participants' precise locations for optimal task allocation, e.g., minimizing selected workers' travel distance to task locations. However, the exposure of their locations raises privacy concerns. Especially for those who are not eventually selected for any task, their location privacy is sacrificed in vain. Hence, in this paper, we propose a location privacy-preserving task allocation framework with geo-obfuscation to protect users' locations during task assignments. Specifically, we make participants obfuscate their reported locations under the guarantee of differential privacy, which can provide privacy protection regardless of adversaries' prior knowledge and without the involvement of any third-part entity. In order to achieve optimal task allocation with such differential geo-obfuscation, we formulate a mixed-integer non-linear programming problem to minimize the expected travel distance of the selected workers under the constraint of differential privacy. Evaluation results on both simulation and real-world user mobility traces show the effectiveness of our proposed framework. Particularly, our framework outperforms Laplace obfuscation, a state-of-the-art differential geo-obfuscation mechanism, by achieving 45% less average travel distance on the real-world data.",Location Privacy-Preserving Task Allocation for Mobile Crowdsensing with Differential Geo-Obfuscation,NA:NA:NA:NA:NA:NA,2017
Yaguang Li:Han Su:Ugur Demiryurek:Bolong Zheng:Tieke He:Cyrus Shahabi,"The turn-by-turn directions provided in existing navigation applications are exclusively derived from underlying road network topology information, i.e., the connectivity of edges to each other. Therefore, the turn-by-turn directions are simplified as metric translation of physical world (e.g. distance/time to turn) to spoken language. Such translation - that ignores human cognition of the geographic space - is often verbose and redundant for the drivers who have knowledge about the geographical areas. In this paper, we study a Personalized RoutE Guidance System dubbed PaRE - with which the goal is to generate more customized and intuitive directions based on user generated content. PaRE utilizes a wealth of user generated historical trajectory data to extract namely ""landmarks"" (e.g., point of interests or intersections) and frequently visited routes between them from the road network. The extracted information is used to obtain cognitive customized directions for each user. We formalize this task as a problem of finding the optimal partition for a given route that maximizes the familiarity while minimizing the number of segments in the partition, and propose two efficient algorithms to solve it. For empirical study, we apply our solution to both real and synthetic trajectory datasets to evaluate the performance and effectiveness of PaRE.",PaRE: A System for Personalized Route Guidance,NA:NA:NA:NA:NA:NA,2017
Milivoj Simeonovski:Giancarlo Pellegrino:Christian Rossow:Michael Backes,"The Internet is built on top of intertwined network services, e.g., email, DNS, and content distribution networks operated by private or governmental organizations. Recent events have shown that these organizations may, knowingly or unknowingly, be part of global-scale security incidents including state-sponsored mass surveillance programs and large-scale DDoS attacks. For example, in March 2015 the Great Cannon attack has shown that an Internet service provider can weaponize millions of Web browsers and turn them into DDoS bots by injecting malicious JavaScript code into transiting TCP connections. While attack techniques and root cause vulnerabilities are routinely studied, we still lack models and algorithms to study the intricate dependencies between services and providers, reason on their abuse, and assess the attack impact. To close this gap, we present a technique that models services, providers, and dependencies as a property graph. Moreover, we present a taint-style propagation-based technique to query the model, and present an evaluation of our framework on the top 100k Alexa domains.",Who Controls the Internet?: Analyzing Global Threats using Property Graph Traversals,NA:NA:NA:NA,2017
Rebecca S. Portnoff:Sadia Afroz:Greg Durrett:Jonathan K. Kummerfeld:Taylor Berg-Kirkpatrick:Damon McCoy:Kirill Levchenko:Vern Paxson,"Underground forums are widely used by criminals to buy and sell a host of stolen items, datasets, resources, and criminal services. These forums contain important resources for understanding cybercrime. However, the number of forums, their size, and the domain expertise required to understand the markets makes manual exploration of these forums unscalable. In this work, we propose an automated, top-down approach for analyzing underground forums. Our approach uses natural language processing and machine learning to automatically generate high-level information about underground forums, first identifying posts related to transactions, and then extracting products and prices. We also demonstrate, via a pair of case studies, how an analyst can use these automated approaches to investigate other categories of products and transactions. We use eight distinct forums to assess our tools: Antichat, Blackhat World, Carders, Darkode, Hack Forums, Hell, L33tCrew and Nulled. Our automated approach is fast and accurate, achieving over 80% accuracy in detecting post category, product, and prices.",Tools for Automated Analysis of Cybercriminal Markets,NA:NA:NA:NA:NA:NA:NA:NA,2017
Qian Cui:Guy-Vincent Jourdan:Gregor V. Bochmann:Russell Couturier:Iosif-Viorel Onut,"The so-called ``phishing'' attacks are one of the important threats to individuals and corporations in today's Internet. Combatting phishing is thus a top-priority, and has been the focus of much work, both on the academic and on the industry sides. In this paper, we look at this problem from a new angle. We have monitored a total of 19,066 phishing attacks over a period of ten months and found that over 90% of these attacks were actually replicas or variations of other attacks in the database. This provides several opportunities and insights for the fight against phishing: first, quickly and efficiently detecting replicas is a very effective prevention tool. We detail one such tool in this paper. Second, the widely held belief that phishing attacks are dealt with promptly is but an illusion. We have recorded numerous attacks that stay active throughout our observation period. This shows that the current prevention techniques are ineffective and need to be overhauled. We provide some suggestions in this direction. Third, our observation give a new perspective into the modus operandi of attackers. In particular, some of our observations suggest that a small group of attackers could be behind a large part of the current attacks. Taking down that group could potentially have a large impact on the phishing attacks observed today.",Tracking Phishing Attacks Over Time,NA:NA:NA:NA:NA,2017
Deepak Kumar:Zane Ma:Zakir Durumeric:Ariana Mirian:Joshua Mason:J. Alex Halderman:Michael Bailey,"Over the past 20 years, websites have grown increasingly complex and interconnected. In 2016, only a negligible number of sites are dependency free, and over 90% of sites rely on external content. In this paper, we investigate the current state of web dependencies and explore two security challenges associated with the increasing reliance on external services: (1) the expanded attack surface associated with serving unknown, implicitly trusted third-party content, and (2) how the increased set of external dependencies impacts HTTPS adoption. We hope that by shedding light on these issues, we can encourage developers to consider the security risks associated with serving third-party content and prompt service providers to more widely deploy HTTPS.",Security Challenges in an Increasingly Tangled Web,NA:NA:NA:NA:NA:NA:NA,2017
Xiaohan Li:Shu Wu:Liang Wang,"Recently, the percentage of people with hypertension is increasing, and this phenomenon is widely concerned. At the same time, wireless home Blood Pressure (BP) monitors become accessible in people's life. Since machine learning methods have made important contributions in different fields, many researchers have tried to employ them in dealing with medical problems. However, the existing studies for BP prediction are all based on clinical data with short time ranges. Besides, there do not exist works which can jointly make use of historical measurement data (e.g. BP and heart rate) and contextual data (e.g. age, gender, BMI and altitude). Recurrent Neural Networks (RNNs), especially those using Long Short-Term Memory (LSTM) units, can capture long range dependencies, so they are effective in modeling variable-length sequences. In this paper, we propose a novel model named recurrent models with contextual layer, which can model the sequential measurement data and contextual data simultaneously to predict the trend of users' BP. We conduct our experiments on the BP data set collected from a type of wireless home BP monitors, and experimental results show that the proposed models outperform several competitive compared methods.",Blood Pressure Prediction via Recurrent Models with Contextual Layer,NA:NA:NA,2017
Vasileios Lampos:Bin Zou:Ingemar Johansson Cox,"Health surveillance systems based on online user-generated content often rely on the identification of textual markers that are related to a target disease. Given the high volume of available data, these systems benefit from an automatic feature selection process. This is accomplished either by applying statistical learning techniques, which do not consider the semantic relationship between the selected features and the inference task, or by developing labour-intensive text classifiers. In this paper, we use neural word embeddings, trained on social media content from Twitter, to determine, in an unsupervised manner, how strongly textual features are semantically linked to an underlying health concept. We then refine conventional feature selection methods by a priori operating on textual variables that are sufficiently close to a target concept. Our experiments focus on the supervised learning problem of estimating influenza-like illness rates from Google search queries. A ""flu infection"" concept is formulated and used to reduce spurious and potentially confounding features that were selected by previously applied approaches. In this way, we also address forms of scepticism regarding the appropriateness of the feature space, alleviating potential cases of overfitting. Ultimately, the proposed hybrid feature selection method creates a more reliable model that, according to our empirical analysis, improves the inference performance (Mean Absolute Error) of linear and nonlinear regressors by 12% and 28.7%, respectively.",Enhancing Feature Selection Using Word Embeddings: The Case of Flu Surveillance,NA:NA:NA,2017
Kathy Lee:Ashequl Qadir:Sadid A. Hasan:Vivek Datla:Aaditya Prakash:Joey Liu:Oladimeji Farri,"Current Adverse Drug Events (ADE) surveillance systems are often associated with a sizable time lag before such events are published. Online social media such as Twitter could describe adverse drug events in real-time, prior to official reporting. Deep learning has significantly improved text classification performance in recent years and can potentially enhance ADE classification in tweets. However, these models typically require large corpora with human expert-derived labels, and such resources are very expensive to generate and are hardly available. Semi-supervised deep learning models, which offer a plausible alternative to fully supervised models, involve the use of a small set of labeled data and a relatively larger collection of unlabeled data for training. Traditionally, these models are trained on labeled and unlabeled data from similar topics or domains. In reality, millions of tweets generated daily often focus on disparate topics, and this could present a challenge for building deep learning models for ADE classification with random Twitter stream as unlabeled training data. In this work, we build several semi-supervised convolutional neural network (CNN) models for ADE classification in tweets, specifically leveraging different types of unlabeled data in developing the models to address the problem. We demonstrate that, with the selective use of a variety of unlabeled data, our semi-supervised CNN models outperform a strong state-of-the-art supervised classification model by +9.9% F1-score. We evaluated our models on the Twitter data set used in the PSB 2016 Social Media Shared Task. Our results present the new state-of-the-art for this data set.",Adverse Drug Event Detection in Tweets with Semi-Supervised Convolutional Neural Networks,NA:NA:NA:NA:NA:NA:NA,2017
Yoshihiko Suhara:Yinzhan Xu:Alex 'Sandy' Pentland,"Depression is a prevailing issue and is an increasing problem in many people's lives. Without observable diagnostic criteria, the signs of depression may go unnoticed, resulting in high demand for detecting depression in advance automatically. This paper tackles the challenging problem of forecasting severely depressed moods based on self-reported histories. Despite the large amount of research on understanding individual moods including depression, anxiety, and stress based on behavioral logs collected by pervasive computing devices such as smartphones, forecasting depressed moods is still an open question. This paper develops a recurrent neural network algorithm that incorporates categorical embedding layers for forecasting depression. We collected large-scale records from 2,382 self-declared depressed people to conduct the experiment. Experimental results show that our method forecast the severely depressed mood of a user based on self-reported histories, with higher accuracy than SVM. The results also showed that the long-term historical information of a user improves the accuracy of forecasting depressed mood.",DeepMood: Forecasting Depressed Mood Based on Self-Reported Histories via Recurrent Neural Networks,NA:NA:NA,2017
Minh X. Hoang:Xuan-Hong Dang:Xiang Wu:Zhenyu Yan:Ambuj K. Singh,"Predicting the popularity of online content in social networks is important in many applications, ranging from ad campaign design, web content caching and prefetching, to web-search result ranking. Earlier studies target this problem by learning models that either generalize behaviors of the entire network population or capture behaviors of each individual user. In this paper, we claim that a novel approach based on group-level popularity is necessary and more practical, given that users naturally organize themselves into clusters and that users within a cluster react to online content in a uniform manner. We develop a novel framework by first grouping users into cohesive clusters, and then adopt tensor decomposition to make predictions. In order to minimize the impact of noisy data and be more flexible in capturing changes in users' interests, our framework exploits both the network topology and interaction among users in learning a robust user clustering. The PARAFAC tensor decomposition is adapted to work with hierarchical constraint over user groups, and we show that optimizing this constrained function via gradient descent achieves faster convergence and leads to more stable solutions. Extensive experimental results over two social networks demonstrate that our framework is scalable, finds meaningful user groups, and significantly outperforms eight baseline methods in terms of prediction accuracy.",GPOP: Scalable Group-level Popularity Prediction for Online Content in Social Networks,NA:NA:NA:NA:NA,2017
Marian-Andrei Rizoiu:Lexing Xie:Scott Sanner:Manuel Cebrian:Honglin Yu:Pascal Van Hentenryck,"Modeling and predicting the popularity of online content is a significant problem for the practice of information dissemination, advertising, and consumption. Recent work analyzing massive datasets advances our understanding of popularity, but one major gap remains: To precisely quantify the relationship between the popularity of an online item and the external promotions it receives. This work supplies the missing link between exogenous inputs from public social media platforms, such as Twitter, and endogenous responses within the content platform, such as YouTube. We develop a novel mathematical model, the Hawkes intensity process, which can explain the complex popularity history of each video according to its type of content, network of diffusion, and sensitivity to promotion. Our model supplies a prototypical description of videos, called an endo-exo map. This map explains popularity as the result of an extrinsic factor -- the amount of promotions from the outside world that the video receives, acting upon two intrinsic factors -- sensitivity to promotion, and inherent virality. We use this model to forecast future popularity given promotions on a large 5-months feed of the most-tweeted videos, and found it to lower the average error by 28.6% from approaches based on popularity history. Finally, we can identify videos that have a high potential to become viral, as well as those for which promotions will have hardly any effect.",Expecting to be HIP: Hawkes Intensity Processes for Social Media Popularity,NA:NA:NA:NA:NA:NA,2017
Andrés Abeliuk:Gerardo Berbeglia:Pascal Van Hentenryck:Tad Hogg:Kristina Lerman,"Unpredictability is often portrayed as an undesirable outcome of social influence in cultural markets. Unpredictability stems from the ""rich get richer"" effect, whereby small fluctuations in the market share or popularity of products are amplified over time by social influence. In this paper, we report results of an experimental study that shows that unpredictability is not an inherent property of social influence. We investigate strategies for creating markets in which the popularity of products is better-and more predictably-aligned with their underlying quality. For our study, we created a cultural market of science stories and conducted randomized experiments on different policies for presenting the stories to study participants. Specifically, we varied how the stories were ranked, and whether or not participants were shown the ratings these stories received from others. We present a policy that leverages social influence and product positioning to help distinguish the product's market share (popularity) from underlying quality. Highlighting products with the highest estimated quality reduces the ""rich get richer"" effect highlighting popular products. We show that this policy allows us to more robustly and predictably identify high quality products and promote blockbusters. The policy can be used to create more efficient online cultural markets with a better allocation of resources to products.",Taming the Unpredictability of Cultural Markets with Social Influence,NA:NA:NA:NA:NA,2017
Julia Proskurnia:Przemyslaw Grabowicz:Ryota Kobayashi:Carlos Castillo:Philippe Cudré-Mauroux:Karl Aberer,"Applying classical time-series analysis techniques to online content is challenging, as web data tends to have data quality issues and is often incomplete, noisy, or poorly aligned. In this paper, we tackle the problem of predicting the evolution of a time series of user activity on the web in a manner that is both accurate and interpretable, using related time series to produce a more accurate prediction. We test our methods in the context of predicting signatures for online petitions using data from thousands of petitions posted on The Petition Site - one of the largest platforms of its kind. We observe that the success of these petitions is driven by a number of factors, including promotion through social media channels and on the front page of the petitions platform. We propose an interpretable model that incorporates seasonality, aging effects, self-excitation, and external effects. The interpretability of the model is important for understanding the elements that drives the activity of an online content. We show through an extensive empirical evaluation that our model is significantly better at predicting the outcome of a petition than state-of-the-art techniques.",Predicting the Success of Online Petitions Leveraging Multidimensional Time-Series,NA:NA:NA:NA:NA:NA,2017
Jiani Zhang:Xingjian Shi:Irwin King:Dit-Yan Yeung,"Knowledge Tracing (KT) is a task of tracing evolving knowledge state of students with respect to one or more concepts as they engage in a sequence of learning activities. One important purpose of KT is to personalize the practice sequence to help students learn knowledge concepts efficiently. However, existing methods such as Bayesian Knowledge Tracing and Deep Knowledge Tracing either model knowledge state for each predefined concept separately or fail to pinpoint exactly which concepts a student is good at or unfamiliar with. To solve these problems, this work introduces a new model called Dynamic Key-Value Memory Networks (DKVMN) that can exploit the relationships between underlying concepts and directly output a student's mastery level of each concept. Unlike standard memory-augmented neural networks that facilitate a single memory matrix or two static memory matrices, our model has one static matrix called key, which stores the knowledge concepts and the other dynamic matrix called value, which stores and updates the mastery levels of corresponding concepts. Experiments show that our model consistently outperforms the state-of-the-art model in a range of KT datasets. Moreover, the DKVMN model can automatically discover underlying concepts of exercises typically performed by human annotations and depict the changing knowledge state of a student.",Dynamic Key-Value Memory Networks for Knowledge Tracing,NA:NA:NA:NA,2017
Simon Walk:Lisette Esín-Noboa:Denis Helic:Markus Strohmaier:Mark A. Musen,"Ontologies in the biomedical domain are numerous, highly specialized and very expensive to develop. Thus, a crucial prerequisite for ontology adoption and reuse is effective support for exploring and finding existing ontologies. Towards that goal, the National Center for Biomedical Ontology (NCBO) has developed BioPortal---an online repository containing more than 500 biomedical ontologies. In 2016, BioPortal represents one of the largest portals for exploration of semantic biomedical vocabularies and terminologies, which is used by many researchers and practitioners. While usage of this portal is high, we know very little about how exactly users search and explore ontologies and what kind of usage patterns or user groups exist in the first place. Deeper insights into user behavior on such portals can provide valuable information to devise strategies for a better support of users in exploring and finding existing ontologies, and thereby enable better ontology reuse. To that end, we study and group users according to their browsing behavior on BioPortal and use data mining techniques to characterize and compare exploration strategies across ontologies. In particular, we were able to identify seven distinct browsing types, all relying on different functionality provided by BioPortal. For example, Search Explorers extensively use the search functionality while Ontology Tree Explorers mainly rely on the class hierarchy for exploring ontologies. Further, we show that specific characteristics of ontologies influence the way users explore and interact with the website. Our results may guide the development of more user-oriented systems for ontology exploration on the Web.",How Users Explore Ontologies on the Web: A Study of NCBO's BioPortal Usage Logs,NA:NA:NA:NA:NA,2017
HyeongSik Kim:Padmashree Ravindra:Kemafor Anyanwu,"Scalable query processing relies on early and aggressive determination and pruning of query-irrelevant data. Besides the traditional space-pruning techniques such as indexing, type-based optimizations that exploit integrity constraints defined on the types can be used to rewrite queries into more efficient ones. However, such optimizations are only applicable in strongly-typed data and query models which make it a challenge for semi-structured models such as RDF. Consequently, developing techniques for enabling typebased query optimizations will contribute new insight to improving the scalability of RDF processing systems. In this paper, we address the challenge of type-based query optimization for RDF graph pattern queries. The approach comprises of (i) a novel type system for RDF data induced from data and ontologies and (ii) a query optimization and evaluation framework for evaluating graph pattern queries using type-based optimizations. An implementation of this approach integrated into Apache Pig is presented and evaluated. Comprehensive experiments conducted on real-world and synthetic benchmark datasets show that our approach is up to 500X faster than existing approaches",Type-based Semantic Optimization for Scalable RDF Graph Pattern Matching,NA:NA:NA,2017
Marco Brambilla:Stefano Ceri:Emanuele Della Valle:Riccardo Volonterio:Felix Xavier Acero Salazar,"Massive data integration technologies have been recently used to produce very large ontologies. However, knowledge in the world continuously evolves, and ontologies are largely incomplete for what concerns low-frequency data, belonging to the so-called long tail. Socially produced content is an excellent source for discovering emerging knowledge: it is huge, and immediately reflects the relevant changes which hide emerging entities. Thus, we propose a method for discovering emerging entities by extracting them from social content. Once instrumented by experts through very simple initialization, the method is capable of finding emerging entities; we use a purely syntactic method as a baseline, and we propose several semantics-based variants. The method uses seeds, i.e. prototypes of emerging entities provided by experts, for generating candidates; then, it associates candidates to feature vectors, built by using terms occurring in their social content, and then ranks the candidates by using their distance from the centroid of seeds, returning the top candidates as result. The method can be continuously or periodically iterated, using the results as new seeds. We validate our method by applying it to a set of diverse domain-specific application scenarios, spanning fashion, literature, and exhibitions.",Extracting Emerging Knowledge from Social Media,NA:NA:NA:NA:NA,2017
Cuong Xuan Chu:Niket Tandon:Gerhard Weikum,"Knowledge graphs have become a fundamental asset for search engines. A fair amount of user queries seek information on problem-solving tasks such as building a fence or repairing a bicycle. However, knowledge graphs completely lack this kind of how-to knowledge. This paper presents a method for automatically constructing a formal knowledge base on tasks and task-solving steps, by tapping the contents of online communities such as WikiHow. We employ Open-IE techniques to extract noisy candidates for tasks, steps and the required tools and other items. For cleaning and properly organizing this data, we devise embedding-based clustering techniques. The resulting knowledge base, HowToKB, includes a hierarchical taxonomy of disambiguated tasks, temporal orders of sub-tasks, and attributes for involved items. A comprehensive evaluation of HowToKB shows high accuracy. As an extrinsic use case, we evaluate automatically searching related YouTube videos for HowToKB tasks.",Distilling Task Knowledge from How-To Communities,NA:NA:NA,2017
Seongsoon Kim:Seongwoon Lee:Donghyeon Park:Jaewoo Kang,"Opinion spam, intentionally written by spammers who do not have actual experience with services or products, has recently become a factor that undermines the credibility of information online. In recent years, studies have attempted to detect opinion spam using machine learning algorithms. However, limitations of gold-standard spam datasets still prove to be a major obstacle in opinion spam research. In this paper, we introduce a novel dataset called Paraphrased OPinion Spam (POPS), which contains a new type of review spam that imitates real human opinions using crowdsourcing. To create such a seemingly truthful review spam dataset, we asked task participants to paraphrase truthful reviews, and include factual information and domain knowledge in their reviews. The classification experiments and semantic analysis results show that our POPS dataset most linguistically and semantically resembles truthful reviews. We believe that our new deceptive opinion spam dataset will help advance opinion spam research.",Constructing and Evaluating a Novel Crowdsourcing-based Paraphrased Opinion Spam Dataset,NA:NA:NA:NA,2017
Abhijnan Chakraborty:Saptarshi Ghosh:Niloy Ganguly:Krishna P. Gummadi,"Online news media sites are emerging as the primary source of news for a large number of users. The selection of 'front-page' stories on these media sites usually takes into consideration several crowdsourced popularity metrics, such as number of views or shares by the readers. In this work, we focus on automatically recommending front-page stories in such media websites. When recommending news stories, there are two basic metrics of interest - recency and relevancy. Ideally, recommender systems should recommend the most relevant stories soon after they are published. However, the relevancy of a story only becomes evident as the story ages, thereby creating a tension between recency and relevancy. A systematic analysis of popular recommendation strategies in use today reveals that they lead to poor trade-offs between recency and relevancy in practice. So, in this paper, we propose a new recommendation strategy (called Highest Future-Impact) which attempts to optimize on both the axes. To implement our proposed strategy in practice, we develop an optimization framework combining the predicted future-impact of the stories with the uncertainties in the predictions. Evaluations over three real-world news datasets show that our implementation achieves good performance trade-offs between recency and relevancy.",Optimizing the Recency-Relevancy Trade-off in Online News Recommendations,NA:NA:NA:NA,2017
Behzad Tabibian:Isabel Valera:Mehrdad Farajtabar:Le Song:Bernhard Schölkopf:Manuel Gomez-Rodriguez,"Online knowledge repositories typically rely on their users or dedicated editors to evaluate the reliability of their contents. These explicit feedback mechanisms can be viewed as noisy measurements of both information reliability and information source trustworthiness. Can we leverage these noisy measurements, often biased, to distill a robust, unbiased and interpretable measure of both notions? In this paper, we argue that the large volume of digital traces left by the users within knowledge repositories also reflect information reliability and source trustworthiness. In particular, we propose a temporal point process modeling framework which links the temporal behavior of the users to information reliability and source trustworthiness. Furthermore, we develop an efficient convex optimization procedure to learn the parameters of the model from historical traces of the evaluations provided by these users. Experiments on real-world data gathered from Wikipedia and Stack Overflow show that our modeling framework accurately predicts evaluation events, provides an interpretable measure of information reliability and source trustworthiness, and yields interesting insights about real-world events.",Distilling Information Reliability and Source Trustworthiness from Digital Traces,NA:NA:NA:NA:NA:NA,2017
Srijan Kumar:Justin Cheng:Jure Leskovec:V.S. Subrahmanian,"In online discussion communities, users can interact and share information and opinions on a wide variety of topics. However, some users may create multiple identities, or sockpuppets, and engage in undesired behavior by deceiving others or manipulating discussions. In this work, we study sockpuppetry across nine discussion communities, and show that sockpuppets differ from ordinary users in terms of their posting behavior, linguistic traits, as well as social network structure. Sockpuppets tend to start fewer discussions, write shorter posts, use more personal pronouns such as ``I'', and have more clustered ego-networks. Further, pairs of sockpuppets controlled by the same individual are more likely to interact on the same discussion at the same time than pairs of ordinary users. Our analysis suggests a taxonomy of deceptive behavior in discussion communities. Pairs of sockpuppets can vary in their deceptiveness, i.e., whether they pretend to be different users, or their supportiveness, i.e., if they support arguments of other sockpuppets controlled by the same user. We apply these findings to a series of prediction tasks, notably, to identify whether a pair of accounts belongs to the same underlying user or not. Altogether, this work presents a data-driven view of deception in online discussion communities and paves the way towards the automatic detection of sockpuppets.",An Army of Me: Sockpuppets in Online Discussion Communities,NA:NA:NA:NA,2017
Chaoshun Zuo:Zhiqiang Lin,"Server URLs including domain names, resource path, and query parameters are important to many security applications such as hidden service identification, malicious website detection, and server vulnerability fuzzing. Unlike traditional desktop web apps in which server URLs are often directly visible, the server URLs of mobile apps are often hidden, only being exposed when the corresponding app code gets executed. Therefore, it is important to automatically analyze the mobile app code to expose the server URLs and enable the security applications with them. We have thus developed SMARTGEN to feature selective symbolic execution for the purpose of automatically generate server request messages to expose the server URLs by extracting and solving user input constraints in mobile apps. Our evaluation with 5,000 top-ranked mobile apps (each with over one million installs) in Google Play shows that with SMARTGEN we are able to reveal 297,780 URLs in total for these apps. We have then submitted all of these exposed URLs to a harmful URL detection service provided by VirusTotal, which further identified 8634 URLs being harmful. Among them, Phising belong to phishing sites, 3,722 malware sites and 3,228 malicious sites (there are 387 overlapped sites between malware and malicious sites).",SMARTGEN: Exposing Server URLs of Mobile Apps With Selective Symbolic Execution,NA:NA,2017
Dolière Francis Some:Nataliia Bielova:Tamara Rezk,"Modern browsers implement different security policies such as the Content Security Policy (CSP), a mechanism designed to mitigate popular web vulnerabilities, and the Same Origin Policy (SOP), a mechanism that governs interactions between resources of web pages. In this work, we describe how CSP may be violated due to the SOP when a page contains an embedded iframe from the same origin. We analyse 1 million pages from 10,000 top Alexa sites and report that at least 31.1% of current CSP-enabled pages are potentially vulnerable to CSP violations. Further considering real-world situations where those pages are involved in same-origin nested browsing contexts, we found that in at least 23.5% of the cases, CSP violations are possible. During our study, we also identified a divergence among browsers implementations in the enforcement of CSP in srcdoc sandboxed iframes, which actually reveals a problem in Gecko-based browsers CSP implementation. To ameliorate the problematic conflicts of the security mechanisms, we discuss measures to avoid CSP violations.",On the Content Security Policy Violations due to the Same-Origin Policy,NA:NA:NA,2017
Adam Bates:Wajih Ul Hassan:Kevin Butler:Alin Dobra:Bradley Reaves:Patrick Cable:Thomas Moyer:Nabil Schear,"Detecting and explaining the nature of attacks in distributed web services is often difficult -- determining the nature of suspicious activity requires following the trail of an attacker through a chain of heterogeneous software components including load balancers, proxies, worker nodes, and storage services. Unfortunately, existing forensic solutions cannot provide the necessary context to link events across complex workflows, particularly in instances where application layer semantics (e.g., SQL queries, RPCs) are needed to understand the attack. In this work, we present a transparent provenance-based approach for auditing web services through the introduction of Network Provenance Functions (NPFs). NPFs are a distributed architecture for capturing detailed data provenance for web service components, leveraging the key insight that mediation of an application's protocols can be used to infer its activities without requiring invasive instrumentation or developer cooperation. We design and implement NPF with consideration for the complexity of modern cloud-based web services, and evaluate our architecture against a variety of applications including DVDStore, RUBiS, and WikiBench to show that our system imposes as little as 9.3% average end-to-end overhead on connections for realistic workloads. Finally, we consider several scenarios in which our system can be used to concisely explain attacks. NPF thus enables the hassle-free deployment of semantically rich provenance-based auditing for complex applications workflows in the Cloud.",Transparent Web Service Auditing via Network Provenance Functions,NA:NA:NA:NA:NA:NA:NA:NA,2017
Kyungtae Kim:I Luk Kim:Chung Hwan Kim:Yonghwi Kwon:Yunhui Zheng:Xiangyu Zhang:Dongyan Xu,"Web-based malware equipped with stealthy cloaking and obfuscation techniques is becoming more sophisticated nowadays. In this paper, we propose J-FORCE, a crash-free forced JavaScript execution engine to systematically explore possible execution paths and reveal malicious behaviors in such malware. In particular, J-FORCE records branch outcomes and mutates them for further explorations. J-FORCE inspects function parameter values that may reveal malicious intentions and expose suspicious DOM injections. We addressed a number of technical challenges encountered. For instance, we keep track of missing objects and DOM elements, and create them on demand. To verify the efficacy of our techniques, we apply J-FORCE to detect Exploit Kit (EK) attacks and malicious Chrome extensions. We observe that J-FORCE is more effective compared to the existing tools.",J-Force: Forced Execution on JavaScript,NA:NA:NA:NA:NA:NA:NA,2017
Zheqian Chen:Ben Gao:Huimin Zhang:Zhou Zhao:Haifeng Liu:Deng Cai,"Community question answering(CQA) services have arisen as a popular knowledge sharing pattern for netizens. With abundant interactions among users, individuals are capable of obtaining satisfactory information. However, it is not effective for users to attain satisfying answers within minutes. Users have to check the progress over time until the appropriate answers submitted. We address this problem as a user personalized satisfaction prediction task. Existing methods usually exploit manual feature selection. It is not desirable as it requires careful design and is labor intensive. In this paper, we settle this issue by developing a new multiple instance deep learning framework. Specifically, in our settings, each question follows a multiple instance learning assumption, where its obtained answers can be regarded as instance sets in a bag and we define the question resolved with at least one satisfactory answer. We design an efficient framework exploiting multiple instance learning property with deep learning tactic to model the question-answer pairs relevance and rank the asker's satisfaction possibility. Extensive experiments on large-scale datasets from different forums of Stack Exchange demonstrate the feasibility of our proposed framework in predicting asker personalized satisfaction.",User Personalized Satisfaction Prediction via Multiple Instance Deep Learning,NA:NA:NA:NA:NA:NA,2017
Dimitar Dimitrov:Philipp Singer:Florian Lemmerich:Markus Strohmaier,"While a plethora of hypertext links exist on the Web, only a small amount of them are regularly clicked. Starting from this observation, we set out to study large-scale click data from Wikipedia in order to understand what makes a link successful. We systematically analyze effects of link properties on the popularity of links. By utilizing mixed-effects hurdle models supplemented with descriptive insights, we find evidence of user preference towards links leading to the periphery of the network, towards links leading to semantically similar articles, and towards links in the top and left-side of the screen. We integrate these findings as Bayesian priors into a navigational Markov chain model and by doing so successfully improve the model fits. We further adapt and improve the well-known classic PageRank algorithm that assumes random navigation by accounting for observed navigational preferences of users in a weighted variation. This work facilitates understanding navigational click behavior and thus can contribute to improving link structures and algorithms utilizing these structures.",What Makes a Link Successful on Wikipedia?,NA:NA:NA:NA,2017
Jack Hessel:Lillian Lee:David Mimno,"The content of today's social media is becoming more and more rich, increasingly mixing text, images, videos, and audio. It is an intriguing research question to model the interplay between these different modes in attracting user attention and engagement. But in order to pursue this study of multimodal content, we must also account for context: timing effects, community preferences, and social factors (e.g., which authors are already popular) also affect the amount of feedback and reaction that social-media posts receive. In this work, we separate out the influence of these non-content factors in several ways. First, we focus on ranking pairs of submissions posted to the same community in quick succession, e.g., within 30 seconds; this framing encourages models to focus on time-agnostic and community-specific content features. Within that setting, we determine the relative performance of author vs. content features. We find that victory usually belongs to ""cats and captions,"" as visual and textual features together tend to outperform identity-based features. Moreover, our experiments show that when considered in isolation, simple unigram text features and deep neural network visual features yield the highest accuracy individually, and that the combination of the two modalities generally leads to the best accuracies overall.",Cats and Captions vs. Creators and the Clock: Comparing Multimodal Content to Context in Predicting Relative Popularity,NA:NA:NA,2017
Lin Gong:Benjamin Haines:Hongning Wang,"We propose to capture humans' variable and idiosyncratic sentiment via building personalized sentiment classification models at a group level. Our solution roots in the social comparison theory that humans tend to form groups with others of similar minds and ability, and the cognitive consistency theory that mutual influence inside groups will eventually shape group norms and attitudes, with which group members will all shift to align. We formalize personalized sentiment classification as a multi-task learning problem. In particular, to exploit the clustering property of users' opinions, we impose a non-parametric Dirichlet Process prior over the personalized models, in which group members share the same customized sentiment model adapted from a global classifier. Extensive experimental evaluations on large collections of Amazon and Yelp reviews confirm the effectiveness of the proposed solution: it outperformed user-independent classification solutions, and several state-of-the-art model adaptation and multi-task learning algorithms.",Clustered Model Adaption for Personalized Sentiment Analysis,NA:NA:NA,2017
Takanori Maehara:Hirofumi Suzuki:Masakazu Ishihata,"Evaluating influence spread in social networks is a fundamental procedure to estimate the word-of-mouth effect in viral marketing. There are enormous studies about this topic; however, under the standard stochastic cascade models, the exact computation of influence spread is known to be #P-hard. Thus, the existing studies have used Monte-Carlo simulation-based approximations to avoid exact computation. We propose the first algorithm to compute influence spread exactly under the independent cascade model. The algorithm first constructs binary decision diagrams (BDDs) for all possible realizations of influence spread, then computes influence spread by dynamic programming on the constructed BDDs. To construct the BDDs efficiently, we designed a new frontier-based search-type procedure. The constructed BDDs can also be used to solve other influence-spread related problems, such as random sampling without rejection, conditional influence spread evaluation, dynamic probability update, and gradient computation for probability optimization problems. We conducted computational experiments to evaluate the proposed algorithm. The algorithm successfully computed influence spread on real-world networks with a hundred edges in a reasonable time, which is quite impossible by the naive algorithm. We also conducted an experiment to evaluate the accuracy of the Monte-Carlo simulation-based approximation by comparing exact influence spread obtained by the proposed algorithm.",Exact Computation of Influence Spread by Binary Decision Diagrams,NA:NA:NA,2017
Gilad Asharov:Francesco Bonchi:David Garcia-Soriano:Tamir Tassa,"Consider a multi-layered graph, where the different layers correspond to different proprietary social networks on the same ground set of users. Suppose that the owners of the different networks (called hosts) are mutually non-trusting parties: how can they compute a centrality score for each of the users using all the layers, but without disclosing information about their private graphs? Under this setting we study a suite of three centrality measures whose algebraic structure allows performing that computation with provable security and efficiency. The first measure counts the nodes reachable from a node within a given radius. The second measure extends the first one by counting the number of paths between any two nodes. The final one is a generalization to the multi-layered graph case: not only the number of paths is counted, but also the multiplicity of these paths in the different layers is considered. We devise a suite of multiparty protocols to compute those centrality measures, which are all provably secure in the information-theoretic sense. One typical challenge and limitation of secure multiparty computation protocols is their scalability. We tackle this problem and devise a protocol which is highly scalable and still provably secure. We test our protocols on several real-world multi-layered graphs: interestingly, the protocol to compute the most sensitive measure (i.e., the multi-layered centrality) is also the most scalable one and can be efficiently run on very large networks.",Secure Centrality Computation Over Multiple Networks,NA:NA:NA:NA,2017
Wei Chen:Shang-Hua Teng,"We study network centrality based on dynamic influence propagation models in social networks. To illustrate our integrated mathematical-algorithmic approach for understanding the fundamental interplay between dynamic influence processes and static network structures, we focus on two basic centrality measures: (a) Single Node Influence (SNI) centrality, which measures each node's significance by its influence spread; and (b) Shapley Centrality, which uses the Shapley value of the influence spread function --- formulated based on a fundamental cooperative-game-theoretical concept --- to measure the significance of nodes. We present a comprehensive comparative study of these two centrality measures. Mathematically, we present axiomatic characterizations, which precisely capture the essence of these two centrality measures and their fundamental differences. Algorithmically, we provide scalable algorithms for approximating them for a large family of social-influence instances. Empirically, we demonstrate their similarity and differences in a number of real-world social networks, as well as the efficiency of our scalable algorithms. Our results shed light on their applicability: SNI centrality is suitable for assessing individual influence in isolation while Shapley centrality assesses individuals' performance in group influence settings.",Interplay between Social Influence and Network Centrality: A Comparative Study on Shapley Centrality and Single-Node-Influence Centrality,NA:NA,2017
Naoto Ohsaka:Yuichi Yoshida,"Motivated by viral marketing, stochastic diffusion processes that model influence spread on a network have been studied intensively. The primary interest in such models has been to find a seed set of a fixed size that maximizes the expected size of the cascade from it. Practically, however, it is not desirable to have the risk of ending with a small cascade, even if the expected size of the cascade is large. To address this issue, we adopt conditional value at risk (CVaR) as a risk measure, and propose an algorithm that computes a portfolio over seed sets with a provable guarantee on its CVaR. Using real-world social networks, we demonstrate that the portfolio computed by our algorithm has a significantly better CVaR than seed sets computed by other baseline methods.",Portfolio Optimization for Influence Spread,NA:NA,2017
Ido Guy:Avihai Mejer:Alexander Nus:Fiana Raiber,"User-generated reviews are a key driving force behind some of the leading websites, such as Amazon, TripAdvisor, and Yelp. Yet, the proliferation of user reviews in such sites also poses an information overload challenge: many items, especially popular ones, have a large number of reviews, which cannot all be read by the user. In this work, we propose to extract short practical tips from user reviews. We focus on tips for travel attractions extracted from user reviews on TripAdvisor. Our method infers a list of templates from a small gold set of tips and applies them to user reviews to extract tip candidates. For each attraction, the associated candidates are then ranked according to their predicted usefulness. Evaluation based on labeling by professional annotators shows that our method produces high-quality tips, with good coverage of cities and attractions.",Extracting and Ranking Travel Tips from User-Generated Reviews,NA:NA:NA:NA,2017
Mayank Kejriwal:Pedro Szekely,"Extracting useful entities and attribute values from illicit domains such as human trafficking is a challenging problem with the potential for widespread social impact. Such domains employ atypical language models, have 'long tails' and suffer from the problem of concept drift. In this paper, we propose a lightweight, feature-agnostic Information Extraction (IE) paradigm specifically designed for such domains. Our approach uses raw, unlabeled text from an initial corpus, and a few (12-120) seed annotations per domain-specific attribute, to learn robust IE models for unobserved pages and websites. Empirically, we demonstrate that our approach can outperform feature-centric Conditional Random Field baselines by over 18% F-Measure on five annotated sets of real-world human trafficking datasets in both low-supervision and high-supervision settings. We also show that our approach is demonstrably robust to concept drift, and can be efficiently bootstrapped even in a serial computing environment.",Information Extraction in Illicit Web Domains,NA:NA,2017
Alexander Konovalov:Benjamin Strauss:Alan Ritter:Brendan O'Connor,"Broad-coverage knowledge bases (KBs) such as Wikipedia, Freebase, Microsoft's Satori and Google's Knowledge Graph contain structured data describing real-world entities. These data sources have become increasingly important for a wide range of intelligent systems: from information retrieval and question answering, to Facebook's Graph Search, IBM's Watson, and more. Previous work on learning to populate knowledge bases from text has, for the most part, made the simplifying assumption that facts remain constant over time. But this is inaccurate -- we live in a rapidly changing world. Knowledge should not be viewed as a static snapshot, but instead a rapidly evolving set of facts that must change as the world changes. In this paper we demonstrate the feasibility of accurately identifying entity-transition-events, from real-time news and social media text streams, that drive changes to a knowledge base. We use Wikipedia's edit history as distant supervision to learn event extractors, and evaluate the extractors based on their ability to predict online updates. Our weakly supervised event extractors are able to predict 10 KB revisions per month at 0.8 precision. By lowering our confidence threshold, we can suggest 34.3 correct edits per month at 0.4 precision. 64% of predicted edits were detected before they were added to Wikipedia. The average lead time of our forecasted knowledge revisions over Wikipedia's editors is 40 days, demonstrating the utility of our method for suggesting edits that can be quickly verified and added to the knowledge graph.",Learning to Extract Events from Knowledge Base Revisions,NA:NA:NA:NA,2017
Xiang Ren:Zeqiu Wu:Wenqi He:Meng Qu:Clare R. Voss:Heng Ji:Tarek F. Abdelzaher:Jiawei Han,"Extracting entities and relations for types of interest from text is important for understanding massive text corpora. Traditionally, systems of entity relation extraction have relied on human-annotated corpora for training and adopted an incremental pipeline. Such systems require additional human expertise to be ported to a new domain, and are vulnerable to errors cascading down the pipeline. In this paper, we investigate joint extraction of typed entities and relations with labeled data heuristically obtained from knowledge bases (i.e., distant supervision). As our algorithm for type labeling via distant supervision is context-agnostic, noisy training data poses unique challenges for the task. We propose a novel domain-independent framework, called CoType, that runs a data-driven text segmentation algorithm to extract entity mentions, and jointly embeds entity mentions, relation mentions, text features and type labels into two low-dimensional spaces (for entity and relation mentions respectively), where, in each space, objects whose types are close will also have similar representations. CoType, then using these learned embeddings, estimates the types of test (unlinkable) mentions. We formulate a joint optimization problem to learn embeddings from text corpora and knowledge bases, adopting a novel partial-label loss function for noisy labeled data and introducing an object ""translation"" function to capture the cross-constraints of entities and relations on each other. Experiments on three public datasets demonstrate the effectiveness of CoType across different domains (e.g., news, biomedical), with an average of 25% improvement in F1 score compared to the next best method.",CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases,NA:NA:NA:NA:NA:NA:NA:NA,2017
Nate Veldt:Anthony I. Wirth:David F. Gleich,"Correlation clustering is a technique for aggregating data based on qualitative information about which pairs of objects are labeled `similar' or `dissimilar.' Because the optimization problem is NP-hard, much of the previous literature focuses on finding approximation algorithms. In this paper we explore how to solve the correlation clustering objective exactly when the data to be clustered can be represented by a low-rank matrix. We prove in particular that correlation clustering can be solved in polynomial time when the underlying matrix is positive semidefinite with small constant rank, but that the task remains NP-hard in the presence of even one negative eigenvalue. Based on our theoretical results, we develop an algorithm for efficiently ``solving'' low-rank positive semidefinite correlation clustering by employing a procedure for zonotope vertex enumeration. We demonstrate the effectiveness and speed of our algorithm by using it to solve several clustering problems on both synthetic and real-world data.",Correlation Clustering with Low-Rank Matrices,NA:NA:NA,2017
Wei Wu:Bin Li:Ling Chen:Chengqi Zhang,"Min-Hash, which is widely used for efficiently estimating similarities of bag-of-words represented data, plays an increasingly important role in the era of big data. It has been extended to deal with real-value weighted sets -- Improved Consistent Weighted Sampling (ICWS) is considered as the state-of-the-art for this problem. In this paper, we propose a Practical CWS (PCWS) algorithm. We first transform the original form of ICWS into an equivalent expression, based on which we find some interesting properties that inspire us to make the ICWS algorithm simpler and more efficient in both space and time complexities. PCWS is not only mathematically equivalent to ICWS and preserves the same theoretical properties, but also saves 20% memory footprint and substantial computational cost compared to ICWS. The experimental results on a number of real-world text data sets demonstrate that PCWS obtains the same (even better) classification and retrieval performance as ICWS with 1/5~1/3 reduced empirical runtime.",Consistent Weighted Sampling Made More Practical,NA:NA:NA:NA,2017
Jan Deriu:Aurelien Lucchi:Valeria De Luca:Aliaksei Severyn:Simon Müller:Mark Cieliebak:Thomas Hofmann:Martin Jaggi,"This paper presents a novel approach for multi-lingual sentiment classification in short texts. This is a challenging task as the amount of training data in languages other than English is very limited. Previously proposed multi-lingual approaches typically require to establish a correspondence to English for which powerful classifiers are already available. In contrast, our method does not require such supervision. We leverage large amounts of weakly-supervised data in various languages to train a multi-layer convolutional network and demonstrate the importance of using pre-training of such networks. We thoroughly evaluate our approach on various multi-lingual datasets, including the recent SemEval-2016 sentiment prediction benchmark (Task 4), where we achieved state-of-the-art performance. We also compare the performance of our model trained individually for each language to a variant trained for all languages at once. We show that the latter model reaches slightly worse - but still acceptable - performance when compared to the single language model, while benefiting from better generalization properties across languages.",Leveraging Large Amounts of Weakly Supervised Data for Multi-Language Sentiment Classification,NA:NA:NA:NA:NA:NA:NA:NA,2017
Ping Li:Cun-Hui Zhang,"In web search, data mining, and machine learning, two popular measures of data similarity are the cosine and the resemblance (the latter is for binary data). In this study, we develop theoretical results for both the cosine and the GMM (generalized min-max) kernel, which is a generalization of the resemblance. GMM has direct applications in machine learning as a positive definite kernel and can be efficiently linearized via probabilistic hashing to handle big data. Owing to its discrete nature, the hashed values can also be used to build hash tables for efficient near neighbor search. We prove the theoretical limit of GMM and the consistency result, assuming that the data follow an elliptical distribution, which is a general family of distributions and includes the multivariate normal and t-distribution as special cases. The consistency result holds as long as the data have bounded first moment (an assumption which typically holds for data commonly encountered in practice). Furthermore, we establish the asymptotic normality of GMM. We also prove the limit of cosine under elliptical distributions. In comparison, the consistency of GMM requires much weaker conditions. For example, when data follow a t-distribution with ν degrees of freedom, GMM typically provides a better estimate of similarity than cosine when ν < 8 (ν = 8 means the distribution is very close to normal). These theoretical results help explain the recent success of GMM and lay the foundation for further research.",Theory of the GMM Kernel,NA:NA,2017
Huayi Li:Geli Fei:Shuai Wang:Bing Liu:Weixiang Shao:Arjun Mukherjee:Jidong Shao,"Online reviews play a crucial role in helping consumers evaluate and compare products and services. This critical importance of reviews also incentivizes fraudsters (or spammers) to write fake or spam reviews to secretly promote or demote some target products and services. Existing approaches to detecting spam reviews and reviewers employed review contents, reviewer behaviors, star rating patterns, and reviewer-product networks for detection. In this research, we further discovered that reviewers' posting rates (number of reviews written in a period of time) also follow an interesting distribution pattern, which has not been reported before. That is, their posting rates are bimodal. Multiple spammers also tend to collectively and actively post reviews to the same set of products within a short time frame, which we call co-bursting. Furthermore, we found some other interesting patterns in individual reviewers' temporal dynamics and their co-bursting behaviors with other reviewers. Inspired by these findings, we first propose a two-mode Labeled Hidden Markov Model to model spamming using only individual reviewers' review posting times. We then extend it to the Coupled Hidden Markov Model to capture both reviewer posting behaviors and co-bursting signals. Our experiments show that the proposed model significantly outperforms state-of-the-art baselines in identifying individual spammers. Furthermore, we propose a co-bursting network based on co-bursting relations, which helps detect groups of spammers more effectively than existing approaches.",Bimodal Distribution and Co-Bursting in Review Spam Detection,NA:NA:NA:NA:NA:NA:NA,2017
Yuli Liu:Yiqun Liu:Ke Zhou:Min Zhang:Shaoping Ma,"Community Question Answering (CQA) portals provide rich sources of information on a variety of topics. However, the authenticity and quality of questions and answers (Q&As) has proven hard to control. In a troubling direction, the widespread growth of crowdsourcing websites has created a large-scale, potentially difficult-to-detect workforce to manipulate malicious contents in CQA. The crowd workers who join the same crowdsourcing task about promotion campaigns in CQA collusively manipulate deceptive Q&As for promoting a target (product or service). The collusive spamming group can fully control the sentiment of the target. How to utilize the structure and the attributes for detecting manipulated Q&As? How to detect the collusive group and leverage the group information for the detection task? To shed light on these research questions, we propose a unified framework to tackle the challenge of detecting collusive spamming activities of CQA. First, we interpret the questions and answers in CQA as two independent networks. Second, we detect collusive question groups and answer groups from these two networks respectively by measuring the similarity of the contents posted within a short duration. Third, using attributes (individual-level and group-level) and correlations (user-based and content-based), we proposed a combined factor graph model to detect deceptive Q&As simultaneously by combining two independent factor graphs. With a large-scale practical data set, we find that the proposed framework can detect deceptive contents at early stage, and outperforms a number of competitive baselines.",Detecting Collusive Spamming Activities in Community Question Answering,NA:NA:NA:NA:NA,2017
Neil Shah,"Livestreaming platforms have become increasingly popular in recent years as a means of sharing and advertising creative content. Popular content streamers who attract large viewership to their live broadcasts can earn a living by means of ad revenue, donations and channel subscriptions. Unfortunately, this incentivized popularity has simultaneously resulted in incentive for fraudsters to provide services to astroturf, or artificially inflate viewership metrics by providing fake ``live'' views to customers. Our work provides a number of major contributions: (a) formulation: we are the first to introduce and characterize the viewbot fraud problem in livestreaming platforms, (b) methodology: we propose FLOCK, a principled and unsupervised method which efficiently and effectively identifies botted broadcasts and their constituent botted views, and (c) practicality: our approach achieves over 98% precision in identifying botted broadcasts and over 90% precision/recall against sizable synthetically generated viewbot attacks on a real-world livestreaming workload of over 16 million views and 92 thousand broadcasts. FLOCK successfully operates on larger datasets in practice and is regularly used at a large, undisclosed livestreaming corporation.",FLOCK: Combating Astroturfing on Livestreaming Platforms,NA,2017
David Mandell Freeman,"Online social networks (OSNs) are appealing platforms for spammers and fraudsters, who typically use fake or compromised accounts to connect with and defraud real users. To combat such abuse, OSNs allow users to report fraudulent profiles or activity. The OSN can then use reporting data to review and/or limit activity of reported accounts. Previous authors have suggested that an OSN can augment its takedown algorithms by identifying a ""trusted set"" of users whose reports are weighted more heavily in the disposition of flagged accounts. Such identification would allow the OSN to improve both speed and accuracy of fake account detection and thus reduce the impact of spam on users. In this work we provide the first public, data-driven assessment of whether the above assumption is true: are some users better at reporting than others? Specifically, is reporting skill both measurable, i.e., possible to distinguish from random guessing; and repeatable, i.e., persistent over repeated sampling? Our main contributions are to develop a statistical framework that describes these properties and to apply this framework to data from LinkedIn, the professional social network. Our data includes member reports of fake profiles as well as the more voluminous, albeit weaker, signal of member responses to connection requests. We find that members demonstrating measurable, repeatable skill in identifying fake profiles do exist but are rare: at most 2.4% of those reporting fakes and at most 1.3% of those rejecting connection requests. We conclude that any reliable ""trusted set"" of members will be too small to have noticeable impact on spam metrics.",Can You Spot the Fakes?: On the Limitations of User Feedback in Online Social Networks,NA,2017
Mengting Wan:Di Wang:Matt Goldman:Matt Taddy:Justin Rao:Jie Liu:Dimitrios Lymberopoulos:Julian McAuley,"In order to match shoppers with desired products and provide personalized promotions, whether in online or offline shopping worlds, it is critical to model both consumer preferences and price sensitivities simultaneously. Personalized preferences have been thoroughly studied in the field of recommender systems, though price (and price sensitivity) has received relatively little attention. At the same time, price sensitivity has been richly explored in the area of economics, though typically not in the context of developing scalable, working systems to generate recommendations. In this study, we seek to bridge the gap between large-scale recommender systems and established consumer theories from economics, and propose a nested feature-based matrix factorization framework to model both preferences and price sensitivities. Quantitative and qualitative results indicate the proposed personalized, interpretable and scalable framework is capable of providing satisfying recommendations (on two datasets of grocery transactions) and can be applied to obtain economic insights into consumer behavior.",Modeling Consumer Preferences and Price Sensitivities from Large-Scale Grocery Shopping Transaction Logs,NA:NA:NA:NA:NA:NA:NA:NA,2017
Chanyoung Park:Donghyun Kim:Jinoh Oh:Hwanjo Yu,"For online product recommendation engines, learning high-quality product embedding that captures various aspects of the product is critical to improving the accuracy of user rating prediction. In recent research, in conjunction with user feedback, the appearance of a product as side information has been shown to be helpful for learning product embedding. However, since a product has a variety of aspects such as functionality and specifications, taking into account only its appearance as side information does not suffice to accurately learn its embedding. In this paper, we propose a matrix co-factorization method that leverages information hidden in the so-called ""also-viewed"" products, i.e., a list of products that has also been viewed by users who have viewed a target product. ""Also-viewed"" products reflect various aspects of a given product that have been overlooked by visually-aware recommendation methods proposed in past research. Experiments on multiple real-world datasets demonstrate that our proposed method outperforms state-of-the-art baselines in terms of user rating prediction. We also perform classification on the product embedding learned by our method, and compare it with a state-of-the-art baseline to demonstrate the superiority of our method in generating high-quality product embedding that better represents the product.","Do ""Also-Viewed"" Products Help User Rating Prediction?",NA:NA:NA:NA,2017
Ying-Chun Lin:Chi-Hsuan Huang:Chu-Cheng Hsieh:Yu-Chen Shu:Kun-Ta Chuang,"The effectiveness of monetary promotions has been well reported in the literature to affect shopping decisions for products in real life experience. Nowadays, e-commerce retailers are facing more fierce competition on price promotion in that consumers can easily use a search engine to find another merchant selling an identical product for comparing price. To achieve more effectiveness on real-time promotion in pursuit of better profits, we propose two discount-giving strategies: an algorithm based on Kernel density estimation, and the other algorithm based on Thompson sampling strategy. We show that, given a pre-determined discount budget, our algorithms can significantly acquire better revenue in return than classical strategies with simply fixed discount on label price. We then demonstrate its feasibility to be a promising deployment in e-commerce services for real-time promotion.",Monetary Discount Strategies for Real-Time Promotion Campaign,NA:NA:NA:NA:NA,2017
Chao-Yuan Wu:Amr Ahmed:Gowtham Ramani Kumar:Ritendra Datta,"In online shopping, users usually express their intent through search queries. However, these queries are often ambiguous. For example, it is more likely (and easier) for users to write a query like ""high-end bike"" than ""21 speed carbon frames jamis or giant road bike"". It is challenging to interpret these ambiguous queries and thus search result accuracy suffers. A user oftentimes needs to go through the frustrating process of refining search queries or self-teaching from possibly unstructured information. However, shopping is indeed a structured domain, that is composed of category hierarchy, brands, product lines, features, etc. It would be much better if a shopping site could understand users' intent through this structure, present organized information, and then find the items with the right categories, brands or features. In this paper we study the problem of inferring the latent intent from unstructured queries and mapping them to structured attributes. We present a novel framework that jointly learns this knowledge from user consumption behaviors and product metadata. We present a hybrid Long Short-term Memory (LSTM) joint model that is accurate and robust, even though user queries are noisy and product catalog is rapidly growing. Our study is conducted on a large-scale dataset from Google Shopping, that is composed of millions of items and user queries along with their click responses. Extensive qualitative and quantitative evaluation shows that the proposed model is more accurate, concise, and robust than multiple possible alternatives. In terms of information retrieval (IR) performance, our model is able to improve the quality of current Google Shopping production system, which is a very strong baseline.",Predicting Latent Structured Intents from Shopping Queries,NA:NA:NA:NA,2017
Zhengxing Chen:Su Xue:John Kolen:Navid Aghdaie:Kazi A. Zaman:Yizhou Sun:Magy Seif El-Nasr,"Matchmaking connects multiple players to participate in online player-versus-player games. Current matchmaking systems depend on a single core strategy: create fair games at all times. These systems pair similarly skilled players on the assumption that a fair game is best player experience. We will demonstrate, however, that this intuitive assumption sometimes fails and that matchmaking based on fairness is not optimal for engagement. In this paper, we propose an Engagement Optimized Matchmaking (EOMM) framework that maximizes overall player engagement. We prove that equal-skill based matchmaking is a special case of EOMM on a highly simplified assumption that rarely holds in reality. Our simulation on real data from a popular game made by Electronic Arts, Inc. (EA) supports our theoretical results, showing significant improvement in enhancing player engagement compared to existing matchmaking methods.",EOMM: An Engagement Optimized Matchmaking Framework,NA:NA:NA:NA:NA:NA:NA,2017
Brunella Spinelli:L. Elisa Celis:Patrick Thiran,"Source localization, the act of finding the originator of a disease or rumor in a network, has become an important problem in sociology and epidemiology. The localization is done using the infection state and time of infection of a few designated sensor nodes; however, maintaining sensors can be very costly in practice. We propose the first online approach to source localization: We deploy a priori only a small number of sensors (which reveal if they are reached by an infection) and then iteratively choose the best location to place a new sensor in order to localize the source. This approach allows for source localization with a very small number of sensors; moreover, the source can be found while the epidemic is still ongoing. Our method applies to a general network topology and performs well even with random transmission delays.",Back To The Source: An Online Approach for Sensor Placement and Source Localization,NA:NA:NA,2017
Enrico Mariconti:Jeremiah Onaolapo:Syed Sharique Ahmad:Nicolas Nikiforou:Manuel Egele:Nick Nikiforakis:Gianluca Stringhini,"Users on Twitter are commonly identified by their profile names. These names are used when directly addressing users on Twitter, are part of their profile page URLs, and can become a trademark for popular accounts, with people referring to celebrities by their real name and their profile name, interchangeably. Twitter, however, has chosen to not permanently link profile names to their corresponding user accounts. In fact, Twitter allows users to change their profile name, and afterwards makes the old profile names available for other users to take. In this paper, we provide a large-scale study of the phenomenon of profile name reuse on Twitter. We show that this phenomenon is not uncommon, investigate the dynamics of profile name reuse, and characterize the accounts that are involved in it. We find that many of these accounts adopt abandoned profile names for questionable purposes, such as spreading malicious content, and using the profile name's popularity for search engine optimization. Finally, we show that this problem is not unique to Twitter (as other popular online social networks also release profile names) and argue that the risks involved with profile-name reuse outnumber the advantages provided by this feature.",What's in a Name?: Understanding Profile Name Reuse on Twitter,NA:NA:NA:NA:NA:NA:NA,2017
Muhammad Bilal Zafar:Isabel Valera:Manuel Gomez Rodriguez:Krishna P. Gummadi,"Automated data-driven decision making systems are increasingly being used to assist, or even replace humans in many settings. These systems function by learning from historical decisions, often taken by humans. In order to maximize the utility of these systems (or, classifiers), their training involves minimizing the errors (or, misclassifications) over the given historical data. However, it is quite possible that the optimally trained classifier makes decisions for people belonging to different social groups with different misclassification rates (e.g., misclassification rates for females are higher than for males), thereby placing these groups at an unfair disadvantage. To account for and avoid such unfairness, in this paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in terms of misclassification rates. We then propose intuitive measures of disparate mistreatment for decision boundary-based classifiers, which can be easily incorporated into their formulation as convex-concave constraints. Experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment, often at a small cost in terms of accuracy.",Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment,NA:NA:NA:NA,2017
Claudia Wagner:Philipp Singer:Fariba Karimi:Jürgen Pfeffer:Markus Strohmaier,"Sampling from large networks represents a fundamental challenge for social network research. In this paper, we explore the sensitivity of different sampling techniques (node sampling, edge sampling, random walk sampling, and snowball sampling) on social networks with attributes. We consider the special case of networks (i) where we have one attribute with two values (e.g., male and female in the case of gender), (ii) where the size of the two groups is unequal (e.g., a male majority and a female minority), and (iii) where nodes with the same or different attribute value attract or repel each other (i.e., homophilic or heterophilic behavior). We evaluate the different sampling techniques with respect to conserving the position of nodes and the visibility of groups in such networks. Experiments are conducted both on synthetic and empirical social networks. Our results provide evidence that different network sampling techniques are highly sensitive with regard to capturing the expected centrality of nodes, and that their accuracy depends on relative group size differences and on the level of homophily that can be observed in the network. We conclude that uninformed sampling from social networks with attributes thus can significantly impair the ability of researchers to draw valid conclusions about the centrality of nodes and the visibility or invisibility of groups in social networks.",Sampling from Social Networks with Attributes,NA:NA:NA:NA:NA,2017
Abdalghani Abujabal:Mohamed Yahya:Mirek Riedewald:Gerhard Weikum,"Templates are an important asset for question answering over knowledge graphs, simplifying the semantic parsing of input utterances and generating structured queries for interpretable answers. State-of-the-art methods rely on hand-crafted templates with limited coverage. This paper presents QUINT, a system that automatically learns utterance-query templates solely from user questions paired with their answers. Additionally, QUINT is able to harness language compositionality for answering complex questions without having any templates for the entire question. Experiments with different benchmarks demonstrate the high quality of QUINT.",Automated Template Generation for Question Answering over Knowledge Graphs,NA:NA:NA:NA,2017
Long Chen:Joemon M. Jose:Haitao Yu:Fajie Yuan,"In the age of Web 2.0, a substantial amount of unstructured content are distributed through multiple text streams in an asynchronous fashion, which makes it increasingly difficult to glean and distill useful information. An effective way to explore the information in text streams is topic modelling, which can further facilitate other applications such as search, information browsing, and pattern mining. In this paper, we propose a semantic graph based topic modelling approach for structuring asynchronous text streams. Our model integrates topic mining and time synchronization, two core modules for addressing the problem, into a unified model. Specifically, for handling the lexical gap issues, we use global semantic graphs of each timestamp for capturing the hidden interaction among entities from all the text streams. For dealing with the sources asynchronism problem, local semantic graphs are employed to discover similar topics of different entities that can be potentially separated by time gaps. Our experiment on two real-world datasets shows that the proposed model significantly outperforms the existing ones.",A Semantic Graph-Based Approach for Mining Common Topics from Multiple Asynchronous Text Streams,NA:NA:NA:NA,2017
Denis Lukovnikov:Asja Fischer:Jens Lehmann:Sören Auer,"Question Answering (QA) systems over Knowledge Graphs (KG) automatically answer natural language questions using facts contained in a knowledge graph. Simple questions, which can be answered by the extraction of a single fact, constitute a large part of questions asked on the web but still pose challenges to QA systems, especially when asked against a large knowledge resource. Existing QA systems usually rely on various components each specialised in solving different sub-tasks of the problem (such as segmentation, entity recognition, disambiguation, and relation classification etc.). In this work, we follow a quite different approach: We train a neural network for answering simple questions in an end-to-end manner, leaving all decisions to the model. It learns to rank subject-predicate pairs to enable the retrieval of relevant facts given a question. The network contains a nested word/character-level question encoder which allows to handle out-of-vocabulary and rare word problems while still being able to exploit word-level semantics. Our approach achieves results competitive with state-of-the-art end-to-end approaches that rely on an attention mechanism.",Neural Network-based Question Answering over Knowledge Graphs on Word and Character Level,NA:NA:NA:NA,2017
Wei Emma Zhang:Quan Z. Sheng:Jey Han Lau:Ermyas Abebe,"Programming community-based question-answering (PCQA) websites such as Stack Overflow enable programmers to find working solutions to their questions. Despite detailed posting guidelines, duplicate questions that have been answered are frequently created. To tackle this problem, Stack Overflow provides a mechanism for reputable users to manually mark duplicate questions. This is a laborious effort, and leads to many duplicate questions remain undetected. Existing duplicate detection methodologies from traditional community based question-answering (CQA) websites are difficult to be adopted directly to PCQA, as PCQA posts often contain source code which is linguistically very different from natural languages. In this paper, we propose a methodology designed for the PCQA domain to detect duplicate questions. We model the detection as a classification problem over question pairs. To extract features for question pairs, our methodology leverages continuous word vectors from the deep learning literature, topic model features and phrases pairs that co-occur frequently in duplicate questions mined using machine translation systems. These features capture semantic similarities between questions and produce a strong performance for duplicate detection. Experiments on a range of real-world datasets demonstrate that our method works very well; in some cases over 30% improvement compared to state-of-the-art benchmarks. As a product of one of the proposed features, the association score feature, we have mined a set of associated phrases from duplicate questions on Stack Overflow and open the dataset to the public.",Detecting Duplicate Posts in Programming QA Communities via Latent Semantics and Association Rules,NA:NA:NA:NA,2017
Camille Cobb:Tadayoshi Kohno,"Online dating services let users expand their dating pool beyond their social network and specify important characteristics of potential partners. To assess compatibility, users share personal information -- e.g., identifying details or sensitive opinions about sexual preferences or worldviews -- in profiles or in one-on-one communication. Thus, participating in online dating poses inherent privacy risks. How people reason about these privacy risks in modern online dating ecosystems has not been extensively studied. We present the results of a survey we designed to examine privacy-related risks, practices, and expectations of people who use or have used online dating, then delve deeper using semi-structured interviews. We additionally analyzed 400 Tinder profiles to explore how these issues manifest in practice. Our results reveal tensions between privacy and competing user values and goals, and we demonstrate how these results can inform future designs.",How Public Is My Private Life?: Privacy in Online Dating,NA:NA,2017
Fengli Xu:Zhen Tu:Yong Li:Pengyu Zhang:Xiaoming Fu:Depeng Jin,"Human mobility data has been ubiquitously collected through cellular networks and mobile applications, and publicly released for academic research and commercial purposes for the last decade. Since releasing individual's mobility records usually gives rise to privacy issues, datasets owners tend to only publish aggregated mobility data, such as the number of users covered by a cellular tower at a specific timestamp, which is believed to be sufficient for preserving users' privacy. However, in this paper, we argue and prove that even publishing aggregated mobility data could lead to privacy breach in individuals' trajectories. We develop an attack system that is able to exploit the uniqueness and regularity of human mobility to recover individual's trajectories from the aggregated mobility data without any prior knowledge. By conducting experiments on two real-world datasets collected from both mobile application and cellular network, we reveal that the attack system is able to recover users' trajectories with accuracy about 73%~91% at the scale of tens of thousands to hundreds of thousands users, which indicates severe privacy leakage in such datasets. Through the investigation on aggregated mobility data, our work recognizes a novel privacy problem in publishing statistic data, which appeals for immediate attentions from both academy and industry.",Trajectory Recovery From Ash: User Privacy Is NOT Preserved in Aggregated Mobility Data,NA:NA:NA:NA:NA:NA,2017
Iskander Sanchez-Rola:Davide Balzarotti:Igor Santos,"Tor is a well known and widely used darknet, known for its anonymity. However, while its protocol and relay security have already been extensively studied, to date there is no comprehensive analysis of the structure and privacy of its Web Hidden Service. To fill this gap, we developed a dedicated analysis platform and used it to crawl and analyze over 1.5M URLs hosted in 7257 onion domains. For each page we analyzed its links, resources, and redirections graphs, as well as the language and category distribution. According to our experiments, Tor hidden services are organized in a sparse but highly connected graph, in which around 10% of the onions sites are completely isolated. Our study also measures for the first time the tight connection that exists between Tor hidden services and the Surface Web. In fact, more than 20% of the onion domains we visited imported resources from the Surface Web, and links to the Surface Web are even more prevalent than to other onion domains. Finally, we measured for the first time the prevalence and the nature of web tracking in Tor hidden services, showing that, albeit not as widespread as in the Surface Web, tracking is notably present also in the Dark Web: more than 40% of the scripts are used for this purpose, with the 70% of them being completely new tracking scripts unknown by existing anti-tracking solutions.",The Onions Have Eyes: A Comprehensive Structure and Privacy Analysis of Tor Hidden Services,NA:NA:NA,2017
Jessica Su:Ansh Shukla:Sharad Goel:Arvind Narayanan,"Can online trackers and network adversaries de-anonymize web browsing data readily available to them? We show---theoretically, via simulation, and through experiments on real user data---that de-identified web browsing histories can be linked to social media profiles using only publicly available data. Our approach is based on a simple observation: each person has a distinctive social network, and thus the set of links appearing in one's feed is unique. Assuming users visit links in their feed with higher probability than a random user, browsing histories contain tell-tale marks of identity. We formalize this intuition by specifying a model of web browsing behavior and then deriving the maximum likelihood estimate of a user's social profile. We evaluate this strategy on simulated browsing histories, and show that given a history with 30 links originating from Twitter, we can deduce the corresponding Twitter profile more than 50% of the time.To gauge the real-world effectiveness of this approach, we recruited nearly 400 people to donate their web browsing histories, and we were able to correctly identify more than 70% of them. We further show that several online trackers are embedded on sufficiently many websites to carry out this attack with high accuracy. Our theoretical contribution applies to any type of transactional data and is robust to noisy observations, generalizing a wide range of previous de-anonymization attacks. Finally, since our attack attempts to find the correct Twitter profile out of over 300 million candidates, it is---to our knowledge---the largest-scale demonstrated de-anonymization to date.",De-anonymizing Web Browsing Data with Social Networks,NA:NA:NA:NA,2017
Chenyan Xiong:Russell Power:Jamie Callan,"This paper introduces Explicit Semantic Ranking (ESR), a new ranking technique that leverages knowledge graph embedding. Analysis of the query log from our academic search engine, SemanticScholar.org, reveals that a major error source is its inability to understand the meaning of research concepts in queries. To addresses this challenge, ESR represents queries and documents in the entity space and ranks them based on their semantic connections from their knowledge graph embedding. Experiments demonstrate ESR's ability in improving Semantic Scholar's online production system, especially on hard queries where word-based ranking fails.",Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding,NA:NA:NA,2017
Sourav Dutta:Pratik Nayek:Arnab Bhattacharya,"Labeled graphs provide a natural way of representing entities, relationships and structures within real datasets such as knowledge graphs and protein interactions. Applications such as question answering, semantic search, and motif discovery entail efficient approaches for subgraph matching involving both label and structural similarities. Given the NP-completeness of subgraph isomorphism and the presence of noise, approximate graph matching techniques are required to handle queries in a robust and real-time manner. This paper presents a novel technique to characterize the subgraph similarity based on statistical significance captured by chi-square statistic. The statistical significance model takes into account the background structure and label distribution in the neighborhood of vertices to obtain the best matching subgraph and, therefore, robustly handles partial label and structural mismatches. Based on the model, we propose two algorithms, VELSET and NAGA, that, given a query graph, return the top-k most similar subgraphs from a (large) database graph. While VELSET is more accurate and robust to noise, NAGA is faster and more applicable for scenarios with low label noise. Experiments on large real-life graph datasets depict significant improvements in terms of accuracy and running time in comparison to the state-of-the-art methods.",Neighbor-Aware Search for Approximate Labeled Graph Matching using the Chi-Square Statistics,NA:NA:NA,2017
Bhaskar Mitra:Fernando Diaz:Nick Craswell,"Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favourable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or 'duet' performs significantly better than either neural network individually on a Web page ranking task, and significantly outperforms traditional baselines and other recently proposed models based on neural networks.",Learning to Match using Local and Distributed Representations of Text for Web Search,NA:NA:NA,2017
Alexey Drutsa:Gleb Gusev:Pavel Serdyukov,"State-of-the-art user engagement metrics (such as session-per-user) are widely used by modern Internet companies to evaluate ongoing updates of their web services via A/B testing. These metrics are predictive of companies' long-term goals, but suffer from this property due to slow user learning of an evaluated treatment, which causes a delay in the treatment effect. That, in turn, causes low sensitivity of the metrics and requires to conduct A/B experiments with longer duration or larger set of users from a limited traffic. In this paper, we study how the delay property of user learning can be used to improve sensitivity of several popular metrics of user loyalty and activity. We consider both novel and previously known modifications of these metrics, including different methods of quantifying a trend in a metric's time series and delaying its calculation. These modifications are analyzed with respect to their sensitivity and directionality on a large set of A/B tests run on real users of Yandex. We discover that mostly loyalty metrics gain profit from the considered modifications. We find such modifications that both increase sensitivity of the source metric and are consistent with the sign of its average treatment effect as well.",Using the Delay in a Treatment Effect to Improve Sensitivity and Preserve Directionality of Engagement Metrics in A/B Experiments,NA:NA:NA,2017
Qian Zhao:Yue Shi:Liangjie Hong,"Latent factor models and decision tree based models are widely used in tasks of prediction, ranking and recommendation. Latent factor models have the advantage of interpreting categorical features by a low-dimensional representation, while such an interpretation does not naturally fit numerical features. In contrast, decision tree based models enjoy the advantage of capturing the nonlinear interactions of numerical features, while their capability of handling categorical features is limited by the cardinality of those features. Since in real-world applications we usually have both abundant numerical features and categorical features with large cardinality (e.g. geolocations, IDs, tags etc.), we design a new model, called GB-CENT, which leverages latent factor embedding and tree components to achieve the merits of both while avoiding their demerits. With two real-world data sets, we demonstrate that GB-CENT can effectively (i.e. fast and accurately) achieve better accuracy than state-of-the-art matrix factorization, decision tree based models and their ensemble.",GB-CENT: Gradient Boosted Categorical Embedding and Numerical Trees,NA:NA:NA,2017
Jun Hu:Ping Li,"We propose a new pointwise collaborative ranking approach for recommender systems, which focuses on improving ranking performance at the top of recommended list. Our approach is different from common pointwise methods in that we consider user ratings as ordinal rather than viewing them as real values or categorical labels. In addition, positively rated items (higher rating scores) are emphasized more in our method in order to improve the performance at the top of recommended list. In our method, user ratings are modeled based on an ordinal classification framework, which is made up of a sequence of binary classification problems in which one discriminates between ratings no less than a specific ordinal category c and ratings below that category ({̥ c}vs.{< c}). The results are used subsequently to generate a ranking score that puts higher weights on the output of those binary classification problems concerning high values of c so as to improve the ranking performance at the top of list. As our method crucially builds on a decomposition into binary classification problems, we call our proposed method as Decoupled Collaborative Ranking (DCR). As an extension, we impose pairwise learning on DCR, which yields further improvement with regard to the ranking performance of the proposed method. We demonstrate through extensive experiments on benchmark datasets that our method outperforms many considered state-of-the-art collaborative ranking algorithms in terms of the NDCG metric.",Decoupled Collaborative Ranking,NA:NA,2017
Avradeep Bhowmik:Joydeep Ghosh,"Learning the true rank ordering among objects by aggregating a set of expert opinion rank order lists is an important and ubiquitous problem in many applications ranging from social choice theory to recommendation systems and search aggregation. We study the problem of unsupervised rank aggregation where no ground truth ordering information in available, neither about the true preference ordering among any set of objects nor about the quality of individual rank lists. Aggregating the often inconsistent and poor quality rank lists in such an unsupervised manner is a highly challenging problem, and standard consensus-based methods fall short in terms of both quality and scalability. In this manuscript we propose a novel framework to bypass these issues by using object attributes to augment the standard rank aggregation framework. We design algorithms that learn joint models on both rank lists and object features to obtain an aggregated rank ordering that is more accurate and robust, and also helps weed out rank lists of dubious validity. We validate our techniques on synthetic datasets where our algorithm is able to estimate the true rank ordering even when the rank lists are corrupted. Experiments on three real datasets, MQ2008, MQ2007 and OHSUMED, show that using object features can result in significant improvement in performance over existing rank aggregation methods that do not use object information. Furthermore, when at least some of the rank lists are of high quality, our methods are able to effectively exploit such information to output an aggregated rank ordering of high accuracy.",LETOR Methods for Unsupervised Rank Aggregation,NA:NA,2017
Immanuel Bayer:Xiangnan He:Bhargav Kanagal:Steffen Rendle,"In recent years, interest in recommender research has shifted from explicit feedback towards implicit feedback data. A diversity of complex models has been proposed for a wide variety of applications. Despite this, learning from implicit feedback is still computationally challenging. So far, most work relies on stochastic gradient descent (SGD) solvers which are easy to derive, but in practice challenging to apply, especially for tasks with many items. For the simple matrix factorization model, an efficient coordinate descent (CD) solver has been previously proposed. However, efficient CD approaches have not been derived for more complex models. In this paper, we provide a new framework for deriving efficient CD algorithms for complex recommender models. We identify and introduce the property of k-separable models. We show that k-separability is a sufficient property to allow efficient optimization of implicit recommender problems with CD. We illustrate this framework on a variety of state-of-the-art models including factorization machines and Tucker decomposition. To summarize, our work provides the theory and building blocks to derive efficient implicit CD algorithms for complex recommender models.",A Generic Coordinate Descent Framework for Learning from Implicit Feedback,NA:NA:NA:NA,2017
Roy Ka-Wei Lee:Tuan-Anh Hoang:Ee-Peng Lim,"Topic modeling has traditionally been studied for single text collections and applied to social media data represented in the form of text documents. With the emergence of many social media platforms, users find themselves using different social media for posting content and for social interaction. While many topics may be shared across social media platforms, users typically show preferences of certain social media platform(s) over others for certain topics. Such platform preferences may even be found at the individual level. To model social media topics as well as platform preferences of users, we propose a new topic model known as MultiPlatform-LDA (MultiLDA). Instead of just merging all posts from different social media platforms into a single text collection, MultiLDA keeps one text collection for each social media platform but allowing these platforms to share a common set of topics. MultiLDA further learns the user-specific platform preferences for each topic. We evaluate MultiLDA against TwitterLDA, the state-of-the-art method for social media content modeling, on two aspects: (i) the effectiveness in modeling topics across social media platforms, and (ii) the ability to predict platform choices for each post. We conduct experiments on three real-world datasets from Twitter, Instagram and Tumblr sharing a set of common users. Our experiments results show that the MultiLDA outperforms in both topic modeling and platform choice prediction tasks. We also show empirically that among the three social media platforms, ""Daily matters"" and ""Relationship matters"" are dominant topics in Twitter, ""Social gathering"", ""Outing"" and ""Fashion"" are dominant topics in Instagram, and ""Music"", ""Entertainment"" and ""Fashion"" are dominant topics in Tumblr.",On Analyzing User Topic-Specific Platform Preferences Across Multiple Social Media Sites,NA:NA:NA,2017
Rahmtin Rotabi:Cristian Danescu-Niculescu-Mizil:Jon Kleinberg,"In many domains, a latent competition among different conventions determines which one will come to dominate. One sees such effects in the success of community jargon, of competing frames in political rhetoric, or of terminology in technical contexts. These effects have become widespread in the on-line domain, where the ease of information transmission makes them particularly forceful, and where the available data offers the potential to study competition among conventions at a fine-grained level. In analyzing the dynamics of conventions over time, however, even with detailed on-line data, one encounters two significant challenges. First, as conventions evolve, the underlying substance of their meaning tends to change as well; and such substantive changes confound investigations of social effects. Second, the selection of a convention takes place through the complex interactions of individuals within a community, and contention between the users of competing conventions plays a key role in the convention's evolution. Any analysis of the overall dynamics must take place in the presence of these two issues. In this work we study a setting in which we can cleanly track the competition among conventions while explicitly taking these sources of complexity into account. Our analysis is based on the spread of low-level authoring conventions in the e-print arXiv over 24 years and roughly a million posted papers: by tracking the spread of macros and other author-defined conventions, we are able to study conventions that vary even as the underlying meaning remains constant. We find that the interaction among co-authors over time plays a crucial role in the selection of conventions; the distinction between more and less experienced members of the community, and the distinction between conventions with visible versus invisible effects, are both central to the underlying processes. Through our analysis we make predictions at the population level about the ultimate success of different synonymous conventions over time --- and at the individual level about the outcome of ``fights'' between people over convention choices.",Competition and Selection Among Conventions,NA:NA:NA,2017
George Berry:Sean J. Taylor,"Studies of online social influence have demonstrated that friends have important effects on many types of behavior in a wide variety of settings. However, we know much less about how influence works among relative strangers in digital public squares, despite important conversations happening in such spaces. We present the results of a study on large public Facebook Pages where we randomly used two different methods---most recent and social feedback---to order comments on posts. We find that the social feedback condition results in higher quality viewed comments and response comments. After measuring the average quality of comments written by users before the study, we find that social feedback has a positive effect on response quality for both low and high quality commenters. We draw on a theoretical framework of social norms to explain this empirical result. In order to examine the influence mechanism further, we measure the similarity between comments viewed and written during the study, finding that similarity increases for the highest quality contributors under the social feedback condition. This suggests that, in addition to norms, some individuals may respond with increased relevance to high-quality comments.",Discussion Quality Diffuses in the Digital Public Square,NA:NA,2017
Liye Fu:Lillian Lee:Cristian Danescu-Niculescu-Mizil,"Group discussions are a way for individuals to exchange ideas and arguments in order to reach better decisions than they could on their own. One of the premises of productive discussions is that better solutions will prevail, and that the idea selection process is mediated by the (relative) competence of the individuals involved. However, since people may not know their actual competence on a new task, their behavior is influenced by their self-estimated competence -- that is, their confidence -- which can be misaligned with their actual competence. Our goal in this work is to understand the effects of confidence-competence misalignment on the dynamics and outcomes of discussions. To this end, we design a large-scale natural setting, in the form of an online team-based geography game, that allows usto disentangle confidence from competence and thus separate their effects. We find that in task-oriented discussions, the more-confident individuals have a larger impact on the group's decisions even when these individuals are at the same level of competence as their teammates. Furthermore, this unjustified role of confidence in the decision-making process often leads teams to under-perform. We explore this phenomenon by investigating the effects of confidence on conversational dynamics. For example, we take up the question: do more-confident people introduce more ideas than the less-confident, or do they introduce the same number of ideas but their ideas get more uptake? Moreover, we show that the language people use is more predictive of a person's confidence level than their actual competence. This also suggests potential practical applications, given that in many settings, true competence cannot be assessed before the task is completed, whereas the conversation can be tracked during the course of the problem-solving process.",When Confidence and Competence Collide: Effects on Online Decision-Making Discussions,NA:NA:NA,2017
Ellery Wulczyn:Nithum Thain:Lucas Dixon,"The damage personal attacks cause to online discourse motivates many platforms to try to curb the phenomenon. However, understanding the prevalence and impact of personal attacks in online platforms at scale remains surprisingly difficult. The contribution of this paper is to develop and illustrate a method that combines crowdsourcing and machine learning to analyze personal attacks at scale. We show an evaluation method for a classifier in terms of the aggregated number of crowd-workers it can approximate. We apply our methodology to English Wikipedia, generating a corpus of over 100k high quality human-labeled comments and 63M machine-labeled ones from a classifier that is as good as the aggregate of 3 crowd-workers, as measured by the area under the ROC curve and Spearman correlation. Using this corpus of machine-labeled scores, our methodology allows us to explore some of the open questions about the nature of online personal attacks. This reveals that the majority of personal attacks on Wikipedia are not the result of a few malicious users, nor primarily the consequence of allowing anonymous contributions from unregistered users.",Ex Machina: Personal Attacks Seen at Scale,NA:NA:NA,2017
Dominik Kowald:Subhash Chandra Pujari:Elisabeth Lex,"Hashtags have become a powerful tool in social platforms such as Twitter to categorize and search for content, and to spread short messages across members of the social network. In this paper, we study temporal hashtag usage practices in Twitter with the aim of designing a cognitive-inspired hashtag recommendation algorithm we call BLLi,s. Our main idea is to incorporate the effect of time on (i) individual hashtag reuse (i.e., reusing own hashtags), and (ii) social hashtag reuse (i.e., reusing hashtags, which has been previously used by a followee) into a predictive model. For this, we turn to the Base-Level Learning (BLL) equation from the cognitive architecture ACT-R, which accounts for the time-dependent decay of item exposure in human memory. We validate BLLI,S using two crawled Twitter datasets in two evaluation scenarios. Firstly, only temporal usage patterns of past hashtag assignments are utilized and secondly, these patterns are combined with a content-based analysis of the current tweet. In both evaluation scenarios, we find not only that temporal effects play an important role for both individual and social hashtag reuse but also that our BLLI,S approach provides significantly better prediction accuracy and ranking results than current state-of-the-art hashtag recommendation methods.",Temporal Effects on Hashtag Reuse in Twitter: A Cognitive-Inspired Hashtag Recommendation Approach,NA:NA:NA,2017
Sihem Amer-Yahia:Sofia Kleisarchaki:Naresh Kumar Kolloju:Laks V.S. Lakshmanan:Ruben H. Zamar,"Online rated datasets have become a source for large-scale population studies for analysts and a means for end-users to achieve routine tasks such as finding a book club. Existing systems however only provide limited insights into the opinions of different segments of the rater population. In this paper, we develop a framework for finding and exploring population segments and their opinions. We propose rating maps, a collection of (population segment, rating distribution) pairs, where a segment, e.g., {18-29 year old males in CA} has a rating distribution in the form of a histogram that aggregates its ratings for a set of items (e.g., movies starring Russel Crowe). We formalize the problem of building rating maps dynamically given desired input distributions. Our problem raises two challenges: (i) the choice of an appropriate measure for comparing rating distributions, and (ii) the design of efficient algorithms to find segments. We show that the Earth Mover's Distance (EMD) is well-adapted to comparing rating distributions and prove that finding segments whose rating distribution is close to input ones is NP-complete. We propose an efficient algorithm for building Partition Decision Trees and heuristics for combining the resulting partitions to further improve their quality. Our experiments on real and synthetic datasets validate the utility of rating maps for both analysts and end-users.",Exploring Rated Datasets with Rating Maps,NA:NA:NA:NA:NA,2017
Charalampos Mavroforakis:Isabel Valera:Manuel Gomez-Rodriguez,"People are increasingly relying on social media and the Web to find solutions to their problems in a wide range of domains. In this setting, closely related problems often lead to the same characteristic learning pattern --- people sharing a similar problem visit closely related pieces of information, perform almost identical queries or, more generally, take a series of similar actions at a similar pace. In this paper, we introduce a novel modeling framework for clustering continuous-time grouped streaming data, the Hierarchical Dirichlet Hawkes process (HDHP), which allows us to automatically uncover a wide variety of learning patterns from detailed traces of learning activity. Our model allows for efficient inference, scaling to millions of actions and thousands of users. Experiments on real data from Stack Overflow reveal that our framework recovers meaningful learning patterns, accurately tracks users' interests and goals over time and achieves better predictive performance than the state of the art.",Modeling the Dynamics of Learning Activity on the Web,NA:NA:NA,2017
Ali Pinar:C. Seshadhri:Vaidyanathan Vishal,"Counting the frequency of small subgraphs is a fundamental technique in network analysis across various domains, most notably in bioinformatics and social networks. The special case of triangle counting has received much attention. Getting results for 4-vertex or 5-vertex patterns is highly challenging, and there are few practical results known that can scale to massive sizes. We introduce an algorithmic framework that can be adopted to count any small pattern in a graph and apply this framework to compute exact counts for all 5-vertex subgraphs. Our framework is built on cutting a pattern into smaller ones, and using counts of smaller patterns to get larger counts. Furthermore, we exploit degree orientations of the graph to reduce runtimes even further. These methods avoid the combinatorial explosion that typical subgraph counting algorithms face. We prove that it suffices to enumerate only four specific subgraphs (three of them have less than 5 vertices) to exactly count all 5-vertex patterns. We perform extensive empirical experiments on a variety of real-world graphs. We are able to compute counts of graphs with tens of millions of edges in minutes on a commodity machine. To the best of our knowledge, this is the first practical algorithm for 5-vertex pattern counting that runs at this scale. A stepping stone to our main algorithm is a fast method for counting all 4-vertex patterns. This algorithm is typically ten times faster than the state of the art 4-vertex counters.",ESCAPE: Efficiently Counting All 5-Vertex Subgraphs,NA:NA:NA,2017
Priya Govindan:Chenghong Wang:Chumeng Xu:Hongyu Duan:Sucheta Soundarajan,"The structure of real-world complex networks has long been an area of interest, and one common way to describe the structure of a network has been with the k-core decomposition. The core number of a node can be thought of as a measure of its centrality and importance, and is used by applications such as community detection, understanding viral spreads, and detecting fraudsters. However, we observe that the k-core decomposition suffers from an important flaw: namely, it is calculated globally, and so if the network contains distinct regions of different densities, the sparser among these regions may be neglected. To resolve this issue, we propose the k-peak graph decomposition method, based on the k-core algorithm, which finds the centers of distinct regions in the graph. Our contributions are as follows: (1) We present a novel graph decomposition- the k-peak decomposition- and corresponding algorithm, and perform a theoretical analysis of its properties. (2) We describe a new visualization method, the ""Mountain Plot"", which can be used to better understand the global structure of a graph. (3) We perform an extensive empirical analysis of real-world graphs, including technological, social, biological, and collaboration graphs, and show how the k-peak decomposition gives insight into the structures of these graphs. (4) We demonstrate the advantage of using the k-peak decomposition in various applications, including community detection, contagion and identifying essential proteins.",The k-peak Decomposition: Mapping the Global Structure of Graphs,NA:NA:NA:NA:NA,2017
Charalampos E. Tsourakakis:Jakub Pachocki:Michael Mitzenmacher,"We develop new methods based on graph motifs for graph clustering, allowing more efficient detection of communities within networks. We focus on triangles within graphs, but our techniques extend to other clique motifs as well. Our intuition, which has been suggested but not formalized similarly in previous works, is that triangles are a better signature of community than edges. We therefore generalize the notion of conductance for a graph to triangle conductance, where the edges are weighted according to the number of triangles containing the edge. This methodology allows us to develop variations of several existing clustering techniques, including spectral clustering, that minimize triangles split by the cluster instead of edges cut by the cluster. We provide theoretical results in a planted partition model to demonstrate the potential for triangle conductance in clustering problems. We then show experimentally the effectiveness of our methods to multiple applications in machine learning and graph mining.",Scalable Motif-aware Graph Clustering,NA:NA:NA,2017
Aaron Archer:Silvio Lattanzi:Peter Likarish:Sergei Vassilvitskii,"We consider the reachability indexing problem for private-public directed graphs. In these graphs nodes come in three flavors: public--nodes visible to all users, private--nodes visible to a specific set of users, and protected--nodes visible to any user who can see at least one of the node's parents. We are interested in computing the set of nodes visible to a specific user online. There are two obvious algorithms: precompute the result for every user, or run a reachability algorithm at query time. This paper explores the trade-off between these two strategies. Our approach is to identify a set of additional visible seed nodes for each user. The online reachability algorithm explores the graph starting at these nodes. We first formulate the problem as asymmetric k-center with outliers, and then give an efficient and practical algorithm. We prove new theoretical guarantees for this problem and show empirically that it performs very well in practice.",Indexing Public-Private Graphs,NA:NA:NA:NA,2017
Danny Yuxing Huang:Doug Grundman:Kurt Thomas:Abhishek Kumar:Elie Bursztein:Kirill Levchenko:Alex C. Snoeren,"In this paper, we investigate a new form of blackhat search engine optimization that targets local listing services like Google Maps. Miscreants register abusive business listings in an attempt to siphon search traffic away from legitimate businesses and funnel it to deceptive service industries---such as unaccredited locksmiths---or to traffic-referral scams, often for the restaurant and hotel industry. In order to understand the prevalence and scope of this threat, we obtain access to over a hundred-thousand business listings on Google Maps that were suspended for abuse. We categorize the types of abuse affecting Google Maps; analyze how miscreants circumvented the protections against fraudulent business registration such as postcard mail verification; identify the volume of search queries affected; and ultimately explore how miscreants generated a profit from traffic that necessitates physical proximity to the victim. This physical requirement leads to unique abusive behaviors that are distinct from other online fraud such as pharmaceutical and luxury product scams.",Pinning Down Abuse on Google Maps,NA:NA:NA:NA:NA:NA:NA,2017
Oleksii Starov:Nick Nikiforakis,"Users have come to rely on browser extensions to realize features that are not implemented by browser vendors. Extensions offer users the ability to, among others, block ads, de-clutter websites, enrich pages with third-party content, and take screenshots. At the same time, because of their privileged position inside a user's browser, extensions have access to content and functionality that is not available to webpages, such as, the ability to conduct and read cross-origin requests, as well as get access to a browser's history and cookie jar. In this paper, we report on the first large-scale study of privacy leakage enabled by extensions. By using dynamic analysis and simulated user interactions, we investigate the leaking happening by the 10,000 most popular browser extensions of Google Chrome and find that a non-negligible fraction leaks sensitive information about the user's browsing habits, such as, their browsing history and search-engine queries. We identify common ways that extensions use to obfuscate this leakage and discover that, while some leakage happens on purpose, a large fraction of it is accidental because of the way that extensions attempt to introduce third-party content to a page's DOM. To counter the inference of a user's interests and private information enabled by this leakage, we design, implement, and evaluate BrowsingFog, a browser extension that automatically browses the web in a way that conceals a user's true interests, from a vantage point of history-stealing, third-party trackers.",Extended Tracking Powers: Measuring the Privacy Diffusion Enabled by Browser Extensions,NA:NA,2017
Li Chang:Hsu-Chun Hsiao:Wei Jeng:Tiffany Hyun-Jin Kim:Wei-Hsi Lin,"URL redirection is a popular technique that automatically navigates users to an intended destination webpage with- out user awareness. However, such a seemingly advantageous feature may offer inadequate protection from security vulnerabilities unless every redirection is performed over HTTPS. Even worse, as long as the final redirection to a website is performed over HTTPS, the browser's URL bar indicates that the website is secure regardless of the security of prior redirections, which may provide users with a false sense of security. This paper reports a well-rounded investigation to analyze the wellness of URL redirection security. As an initial large-scale investigation, we screened the integrity and consistency of URL redirections for the Alexa top one million (1M) websites, and further examined 10,000 (10K) websites with their login features. Our results suggest that 1) the majority (83.3% in the 1M dataset and 78.6% in the 10K dataset) of redirection trails among web- sites that support only HTTPS are vulnerable to attacks, and 2) current incoherent practices (e.g., naked domains and www subdomains being redirected to different destinations with varying security levels) undermine the security guarantees provided by HTTPS and HSTS.",Security Implications of Redirection Trail in Popular Websites Worldwide,NA:NA:NA:NA:NA,2017
Milijana Surbatovich:Jassim Aljuraidan:Lujo Bauer:Anupam Das:Limin Jia,"The use of end-user programming, such as if-this-then-that (IFTTT), is becoming increasingly common. Services like IFTTT allow users to easily create new functionality by connecting arbitrary Internet-of-Things (IoT) devices and online services using simple if-then rules, commonly known as recipes. However, such convenience at times comes at the cost of security and privacy risks for end users. To gain an in-depth understanding of the potential security and privacy risks, we build an information-flow model to analyze how often IFTTT recipes involve potential integrity or secrecy violations. Our analysis finds that around 50% of the 19,323 unique recipes we examined are potentially unsafe, as they contain a secrecy violation, an integrity violation, or both. We next categorize the types of harm that these potentially unsafe recipes can cause to users. After manually examining a random selection of potentially unsafe recipes, we find that recipes can not only lead to harms such as personal embarrassment but can also be exploited by an attacker, e.g., to distribute malware or carry out denial-of-service attacks. The use of IoT devices and services like IFTTT is expected only to grow in the near future; our analysis suggests users need to be both informed about and protected from these emerging threats to which they could be unwittingly exposing themselves.",Some Recipes Can Do More Than Spoil Your Appetite: Analyzing the Security and Privacy Risks of IFTTT Recipes,NA:NA:NA:NA:NA,2017
Qingyao Ai:Susan T. Dumais:Nick Craswell:Dan Liebling,"As the number of email users and messages continues to grow, search is becoming more important for finding information in personal archives. In spite of its importance, email search is much less studied than web search, particularly using large-scale behavioral log analysis. In this paper we report the results of a large-scale log analysis of email search and complement this with a survey to better understand email search intent and success. We characterize email search behaviors and highlight differences from web search. When searching for email, people know many attributes about what they are looking for; they often look for specific known items; their queries are shorter and they click on fewer items than in web search. Although repeat queries are common in both email and web search, repeat visits to the same search result are much less common in email search suggesting that the same query is used for different search intents over time. We consider search intent from multiple angles. In email search logs, we find that people use email search not just to find information but also to perform tasks such as cleanup or organization, and that the distribution of actions they perform depends on the type of query. In our survey, people reported that they looked for specific information in both email search and web search, but they were much less likely to search for general information on a topic in email. The differences in overall behavior, re-finding patterns and search intents we observed between email and web search have important implications for the design of email search algorithms and interfaces.",Characterizing Email Search using Large-scale Behavioral Logs and Surveys,NA:NA:NA:NA,2017
Julia Proskurnia:Marc-Allen Cartright:Lluis Garcia-Pueyo:Ivo Krka:James B. Wendt:Tobias Kaufmann:Balint Miklos,"Unsupervised template induction over email data is a central component in applications such as information extraction, document classification, and auto-reply. The benefits of automatically generating such templates are known for structured data, e.g. machine generated HTML emails. However much less work has been done in performing the same task over unstructured email data. We propose a technique for inducing high quality templates from plain text emails at scale based on the suffix array data structure. We evaluate this method against an industry-standard approach for finding similar content based on shingling, running both algorithms over two corpora: a synthetically created email corpus for a high level of experimental control, as well as user-generated emails from the well-known Enron email corpus. Our experimental results show that the proposed method is more robust to variations in cluster quality than the baseline and templates contain more text from the emails, which would benefit extraction tasks by identifying transient parts of the emails. Our study indicates templates induced using suffix arrays contain approximately half as much noise (measured as entropy) as templates induced using shingling. Furthermore, the suffix array approach is substantially more scalable, proving to be an order of magnitude faster than shingling even for modestly-sized training clusters. Public corpus analysis shows that email clusters contain on average 4 segments of common phrases, where each of the segments contains on average 9 words, thus showing that templatization could help users reduce the email writing effort by an average of 35 words per email in an assistance or auto-reply related task.",Template Induction over Unstructured Email Corpora,NA:NA:NA:NA:NA:NA:NA,2017
Hamed Zamani:Michael Bendersky:Xuanhui Wang:Mingyang Zhang,"Modern search engines leverage a variety of sources, beyond the conventional query-document content similarity, to improve their ranking performance. Among them, query context has attracted attention in prior work. Previously, query context was mainly modeled by user search history, either long-term or short-term, to help the ranking of future queries. In this paper, we focus on situational context, i.e., the contextual features of the current search request that are independent from both query content and user history. As an example, situational context can depend on search request time and location. We propose two context-aware ranking models based on neural networks. The first model learns a low-dimensional deep representation from the combination of contextual features. The second model extends the first one by leveraging binarized contextual features in addition to the high-level abstractions learned using a deep network. The existing context-aware ranking models are mainly based on search history, especially click data that can be gathered from the search engine logs. Although context-aware models have been widely explored in web search, their influence on search scenarios where click data is highly sparse is relatively unstudied. The focus of this paper, personal search (e.g., email search or on-device search), is one of such scenarios. We evaluate our models using the click data collected from one of the world's largest personal search engines. The experiments demonstrate that the proposed models significantly outperform the baselines which do not take context into account. These results indicate the importance of situational context for personal search, and open up a venue for further exploration of situational context in other search scenarios.",Situational Context for Ranking in Personal Search,NA:NA:NA:NA,2017
David Carmel:Liane Lewin-Eytan:Alex Libov:Yoelle Maarek:Ariel Raviv,"Web mail search is an emerging topic, which has not been the object of as many studies as traditional Web search. In particular, little is known about the characteristics of mail searchers and of the queries they issue. We study here the characteristics of Web mail searchers, and explore how demographic signals such as location, age, gender, and inferred income, influence their search behavior. We try to understand for instance, whether women exhibit different mail search patterns than men, or whether senior people formulate more precise queries than younger people. We compare our results, obtained from the analysis of a Yahoo Web mail search query log, to similar work conducted in Web and Twitter search. In addition, we demonstrate the value of the user's personal query log, as well as of the global query log and of the demographic signals, in a key search task: dynamic query auto-completion. We discuss how going beyond users' personal query logs (their search history) significantly improves the quality of suggestions, in spite of the fact that a user's mailbox is perceived as being highly personal. In particular, we note the striking value of demographic features for queries relating to companies/organizations, thus verifying our assumption that query completion benefits from leveraging queries issued by ``people like me"". We believe that demographics and other such global features can be leveraged in other mail applications, and hope that this work is a first step in this direction.",The Demographics of Mail Search and their Application to Query Suggestion,NA:NA:NA:NA:NA,2017
David Carmel:Liane Lewin-Eytan:Alex Libov:Yoelle Maarek:Ariel Raviv,"Mail search has traditionally served time-ranked results, even if it has been shown that relevance ranking provides higher retrieval quality on average. Some Web mail services have recently started to provide relevance ranking options such as the relevance toggle in the search results page of Yahoo Mail, or the ``top results"" section in Inbox by Gmail. Yet, ranking results by relevance is not accepted by all, either in mail search, or in in other domains such as social media, where it has even triggered some public outcry. Given the sensitivity of the topic, we propose here to investigate a mixed approach of promoting the most relevant results, to which we refer as ``heroes'', on top of time-ranked results. We argue that this approach represents a good compromise to mail searchers, supporting on one hand the time sorted paradigm they are familiar with, while being almost as effective as full relevance ranking view that Web mail users seem to be reluctant to adopt. We describe three hero-selection algorithms we have devised and the associated experiments we have conducted in Yahoo mail. We measure retrieval success via two metrics: MRR (Mean Reciprocal Rank) and [email protected], and verify agreement between these metrics and users' direct feedback. We demonstrate that supplementing time-sorted results with hero results leads to a higher MRR than the traditional time-sorted view. We additionally show that MRR better reflects users' perception of quality than [email protected] Finally, we report on online results following the successful launch of one of our hero-selection algorithms for all Yahoo enterprise mail users and a few million Yahoo Web mail users.",Promoting Relevant Results in Time-Ranked Mail Search,NA:NA:NA:NA:NA,2017
Jinyuan Jia:Binghui Wang:Le Zhang:Neil Zhenqiang Gong,"In the attribute inference problem, we aim to infer users' private attributes (e.g., locations, sexual orientation, and interests) using their public data in online social networks. State-of-the-art methods leverage a user's both public friends and public behaviors (e.g., page likes on Facebook, apps that the user reviewed on Google Play) to infer the user's private attributes. However, these methods suffer from two key limitations: 1) suppose we aim to infer a certain attribute for a target user using a training dataset, they only leverage the labeled users who have the attribute, while ignoring the label information of users who do not have the attribute; 2) they are inefficient because they infer attributes for target users one by one. As a result, they have limited accuracies and applicability in real-world social networks. In this work, we propose AttriInfer, a new method to infer user attributes in online social networks. AttriInfer can leverage both friends and behaviors, as well as the label information of training users who have an attribute and who do not have the attribute. Specifically, we model a social network as a pairwise Markov Random Field (pMRF). Given a training dataset, which consists of some users who have a certain attribute and some users who do not have a certain attribute, we compute the posterior probability that a target user has the attribute and use the posterior probability to infer attributes. In the basic version of AttriInfer, we use Loopy Belief Propagation (LBP) to compute the posterior probability. However, LBP is not scalable to very large-scale real-world social networks and not guaranteed to converge. Therefore, we further optimize LBP to be scalable and guaranteed to converge. We evaluated our method and compare it with state-of-the-art methods using a real-world Google+ dataset with 5.7M users. Our results demonstrate that our method substantially outperforms state-of-the-art methods in terms of both accuracy and efficiency.",AttriInfer: Inferring User Attributes in Online Social Networks Using Markov Random Fields,NA:NA:NA:NA,2017
Ajaya Neupane:Nitesh Saxena:Leanne Hirshfield,"In this paper, we study the neural underpinnings relevant to user-centered web security through the lens of functional near-infrared spectroscopy (fNIRS). Specifically, we design and conduct an fNIRS study to pursue a thorough investigation of users' processing of legitimate vs. illegitimate and familiar vs. unfamiliar websites. We pinpoint the neural activity in these tasks as well as the brain areas that control such activity. We show that, at the neurological level, users process the legitimate websites differently from the illegitimate websites when subject to phishing attacks. Similarly, we show that users exhibit marked differences in the way their brains process the previously familiar websites from unfamiliar websites. These findings have several defensive and offensive implications. In particular, we discuss how these differences may be used by the system designers in the future to differentiate between legitimate and illegitimate websites automatically based on neural signals. Similarly, we discuss the potential for future malicious attackers, with access to neural signals, in compromising the privacy of users by detecting whether a website is previously familiar or unfamiliar to the user. Compared to prior research, our novelty lies in several aspects. First, we employ a neuroimaging methodology (fNIRS) not tapped into by prior security research for the problem domain we are studying. Second, we provide a focused study design and comprehensive investigation of the neural processing underlying the specific tasks of legitimate vs. illegitimate and familiar vs. unfamiliar websites. Third, we use an experimental set-up much more amenable to real-world settings, compared to previous fMRI studies. Beyond these scientific innovations, our work also serves to corroborate and extend several of the findings of the prior literature with independent methodologies, tools, and settings.",Neural Underpinnings of Website Legitimacy and Familiarity Detection: An fNIRS Study,NA:NA:NA,2017
Sungchul Kim:Nikhil Kini:Jay Pujara:Eunyee Koh:Lise Getoor,"Personalization -- the customization of experiences, interfaces, and content to individual users -- has catalyzed user growth and engagement for many web services. A critical prerequisite to personalization is establishing user identity. However the variety of devices, including mobile phones, appliances, and smart watches, from which users access web services from both anonymous and logged-in sessions poses a significant obstacle to user identification. The resulting entity resolution task of establishing user identity across devices and sessions is commonly referred to as ``visitor stitching.'' We introduce a general, probabilistic approach to visitor stitching using features and attributes commonly contained in web logs. Using web logs from two real-world corporate websites, we motivate the need for probabilistic models by quantifying the difficulties posed by noise, ambiguity, and missing information in deployment. Next, we introduce our approach using probabilistic soft logic (PSL), a statistical relational learning framework capable of capturing similarities across many sessions and enforcing transitivity. We present a detailed description of model features and design choices relevant to the visitor stitching problem. Finally, we evaluate our PSL model on binary classification performance for two real-world visitor stitching datasets. Our model demonstrates significantly better performance than several state-of-the-art classifiers, and we show how this advantage results from collective reasoning across sessions.",Probabilistic Visitor Stitching on Cross-Device Web Logs,NA:NA:NA:NA:NA,2017
Philipp Singer:Florian Lemmerich:Robert West:Leila Zia:Ellery Wulczyn:Markus Strohmaier:Jure Leskovec,"Wikipedia is one of the most popular sites on the Web, with millions of users relying on it to satisfy a broad range of information needs every day. Although it is crucial to understand what exactly these needs are in order to be able to meet them, little is currently known about why users visit Wikipedia. The goal of this paper is to fill this gap by combining a survey of Wikipedia readers with a log-based analysis of user activity. Based on an initial series of user surveys, we build a taxonomy of Wikipedia use cases along several dimensions, capturing users' motivations to visit Wikipedia, the depth of knowledge they are seeking, and their knowledge of the topic of interest prior to visiting Wikipedia. Then, we quantify the prevalence of these use cases via a large-scale user survey conducted on live Wikipedia with almost 30,000 responses. Our analyses highlight the variety of factors driving users to Wikipedia, such as current events, media coverage of a topic, personal curiosity, work or school assignments, or boredom. Finally, we match survey responses to the respondents' digital traces in Wikipedia's server logs, enabling the discovery of behavioral patterns associated with specific use cases. For instance, we observe long and fast-paced page sequences across topics for users who are bored or exploring randomly, whereas those using Wikipedia for work or school spend more time on individual articles focused on topics such as science. Our findings advance our understanding of reader motivations and behavior on Wikipedia and can have implications for developers aiming to improve Wikipedia's user experience, editors striving to cater to their readers' needs, third-party services (such as search engines) providing access to Wikipedia content, and researchers aiming to build tools such as recommendation engines.",Why We Read Wikipedia,NA:NA:NA:NA:NA:NA:NA,2017
Xin Wang:Steven C.H. Hoi:Martin Ester:Jiajun Bu:Chun Chen,"Recent years have seen a surge of research on social recommendation techniques for improving recommender systems due to the growing influence of social networks to our daily life. The intuition of social recommendation is that users tend to show affinities with items favored by their social ties due to social influence. Despite the extensive studies, no existing work has attempted to distinguish and learn the personalized preferences between strong and weak ties, two important terms widely used in social sciences, for each individual in social recommendation. In this paper, we first highlight the importance of different types of ties in social relations originated from social sciences, and then propose anovel social recommendation method based on a new Probabilistic Matrix Factorization model that incorporates the distinction of strong and weak ties for improving recommendation performance. The proposed method is capable of simultaneously classifying different types of social ties in a social network w.r.t. optimal recommendation accuracy, and learning a personalized tie type preference for each user in addition to other parameters. We conduct extensive experiments on four real-world datasets by comparing our method with state-of-the-art approaches, and find encouraging results that validate the efficacy of the proposed method in exploiting the personalized preferences of strong and weak ties for social recommendation.",Learning Personalized Preference of Strong and Weak Ties for Social Recommendation,NA:NA:NA:NA:NA,2017
Xiaokai Wei:Linchuan Xu:Bokai Cao:Philip S. Yu,"Link Prediction has been an important task for social and information networks. Existing approaches usually assume the completeness of network structure. However, in many real-world networks, the links and node attributes can usually be partially observable. In this paper, we study the problem of Cross View Link Prediction (CVLP) on partially observable networks, where the focus is to recommend nodes with only links to nodes with only attributes (or vice versa). We aim to bridge the information gap by learning a robust consensus for link-based and attribute-based representations so that nodes become comparable in the latent space. Also, the link-based and attribute-based representations can lend strength to each other via this consensus learning. Moreover, attribute selection is performed jointly with the representation learning to alleviate the effect of noisy high-dimensional attributes. We present two instantiations of this framework with different loss functions and develop an alternating optimization framework to solve the problem. Experimental results on four real-world datasets show the proposed algorithm outperforms the baseline methods significantly for cross-view link prediction.",Cross View Link Prediction by Learning Noise-resilient Representation Consensus,NA:NA:NA:NA,2017
Xiang Li:Yao Wu:Martin Ester:Ben Kao:Xin Wang:Yudian Zheng,"A heterogeneous information network (HIN) is one whose nodes model objects of different types and whose links model objects' relationships. In many applications, such as social networks and RDF-based knowledge bases, information can be modeled as HINs. To enrich its information content, objects (as represented by nodes) in an HIN are typically associated with additional attributes. We call such an HIN an Attributed HIN or AHIN. We study the problem of clustering objects in an AHIN, taking into account objects' similarities with respect to both object attribute values and their structural connectedness in the network. We show how supervision signal, expressed in the form of a must-link set and a cannot-link set, can be leveraged to improve clustering results. We put forward the SCHAIN algorithm to solve the clustering problem. We conduct extensive experiments comparing SCHAIN with other state-of-the-art clustering algorithms and show that SCHAIN outperforms the others in clustering quality.",Semi-supervised Clustering in Attributed Heterogeneous Information Networks,NA:NA:NA:NA:NA:NA,2017
Filip Radlinski:Serena Villata,NA,Session details: Posters,NA:NA,2018
Dominik Kowald:Paul Seitlinger:Tobias Ley:Elisabeth Lex,"In this paper, we present the results of an online study with the aim to shed light on the impact that semantic context cues have on the user acceptance of tag recommendations. Therefore, we conducted a work-integrated social bookmarking scenario with 17 university employees in order to compare the user acceptance of a context-aware tag recommendation algorithm called 3Layers with the user acceptance of a simple popularity-based baseline. In this scenario, we validated and verified the hypothesis that semantic context cues have a higher impact on the user acceptance of tag recommendations in a collaborative tagging setting than in an individual tagging setting. With this paper, we contribute to the sparse line of research presenting online recommendation studies.",The Impact of Semantic Context Cues on the User Acceptance of Tag Recommendations: An Online Study,NA:NA:NA:NA,2018
Ryan A. Rossi:Nesreen K. Ahmed:Eunyee Koh,"This paper describes a general framework for learning Higher-Order Network Embeddings (HONE) from graph data based on network motifs. The HONE framework is highly expressive and flexible with many interchangeable components. The experimental results demonstrate the effectiveness of learning higher-order network representations. In all cases, HONE outperforms recent embedding methods that are unable to capture higher-order structures with a mean relative gain in AUC of 19% (and up to 75% gain) across a wide variety of networks and embedding methods.",Higher-order Network Representation Learning,NA:NA:NA,2018
Peng Bao:Jiahui Wang,"With the rapid development of scientific impact quantification in the field of science of success, the ability to identify the representative work of a researcher has important implications in a wide range of areas, including hiring, funding, and promotion systems. In this paper, we propose a two-step credit allocation algorithm (TSCA) for identifying the representative work of a researcher. This algorithm explicitly captures the importance of a paper, its relevance to other papers, and the unequally distributed contribution of each citation. We validate TSCA by applying it on the citation data from American Physical Society (APS) in the scenario of identifying the Nobel prize winning papers of the Nobel laureates. Experiments demonstrate that the proposed algorithm can significantly outperform the existing methods.",Identifying Your Representative Work Based on Credit Allocation,NA:NA,2018
Hongru Liang:Qian Li:Haozheng Wang:Hang Li:Jin-Mao Wei:Zhenglu Yang,"Learning rap lyrics is an important area of music information retrieval because it is the basis of many applications, such as recommendation systems, automatic classification. In this paper, we tackle the issue pertaining to the lack of an effective approach to aggregate various features of lyrics by proposing an attention-based autoencoder for rap lyrics representation learning (AttAE-RL²). The proposed method appropriately integrates the semantic and prosodic features of rap lyrics. The preliminary experimental results demonstrate that our approach outperforms the state-of-the-art ones.",AttAE-RL²: Attention based Autoencoder for Rap Lyrics Representation Learning,NA:NA:NA:NA:NA:NA,2018
Yuta Takata:Mitsuaki Akiyama:Takeshi Yagi:Kunio Hato:Shigeki Goto,"Threats of abusing websites that webmasters have stopped updating have increased. In this poster, we propose a method of predicting potentially abusable websites by retrospectively analyzing updates of software that composes websites. The method captures webmaster behaviors from archived snapshots of a website and analyzes the changes of web servers and web applications used in the past as update histories. A classifier that predicts website abuses is finally built by using update histories from snapshots of known malicious websites before the detections. Evaluation results showed that the classifier could predict various website abuses, such as drive-by downloads, phishes, and defacements, with accuracy: a 76% true positive rate and a 26% false positive rate.",POSTER: Predicting Website Abuse Using Update Histories,NA:NA:NA:NA:NA,2018
Yufei Wen:Lei Guo:Zhumin Chen:Jun Ma,"With the advent of online social networks, the use of information hidden in social networks for recommendation has been extensively studied. Unlike previous work regarded social influence as regularization terms, we take advantage of network embedding techniques and propose an embedding based recommendation method. Specifically, we first pre-train a network embedding model on the users' social network to map each user into a low dimensional space, and then incorporate them into a matrix factorization model, which combines both latent and pre-learned features for recommendation. The experimental results on two real-world datasets indicate that our proposed model is more effective and can reach better performance than other related methods.",Network Embedding Based Recommendation Method in Social Networks,NA:NA:NA:NA,2018
Jingtao Ding:Fuli Feng:Xiangnan He:Guanghui Yu:Yong Li:Depeng Jin,"Bayesian Personalized Ranking (BPR) is a representative pairwise learning method for optimizing recommendation models. It is widely known that the performance of BPR depends largely on the quality of the negative sampler. In this short paper, we make two contributions with respect to BPR. First, we find that sampling negative items from the whole space is unnecessary and may even degrade the performance. Second, focusing on the purchase feedback of the E-commerce domain, we propose a simple yet effective sampler for BPR by leveraging the additional view data. Compared to the vanilla BPR that applies a uniform sampler on all candidates, our view-aware sampler enhances BPR with a relative improvement of 27.36% and 69.54% on two real-world datasets respectively.",An Improved Sampler for Bayesian Personalized Ranking by Leveraging View Data,NA:NA:NA:NA:NA:NA,2018
Ting-Yu Yen:Yang-Yin Lee:Hen-Hsen Huang:Hsin-Hsi Chen,"While recent word embedding models demonstrate their abilities to capture syntactic and semantic information, the demand for sense level embedding is getting higher. In this study, we propose a novel joint sense embedding learning model that retrofits the word representation into sense representation from contextual and ontological information. The experiment shows the effectiveness and robustness of our model that outperforms previous approaches in four public available benchmark datasets.",That Makes Sense: Joint Sense Retrofitting from Contextual and Ontological Information,NA:NA:NA:NA,2018
Qiang Xu:Xin Wang:Yueqi Xin:Zhiyong Feng:Renhai Chen,"This paper presents a novel Pregel-based Distributed Subgraph Matching method PDSM to answer subgraph matching queries on big RDF graphs. In our method, the query graph is transformed to a spanning tree based on the breadth-first search (BFS). Two optimization techniques are proposed to filter out part of the unpromising intermediate results and postpone the Cartesian product operations in the Pregel iterative computation. The extensive experiments on both synthetic and real-world datasets show that PDSM outperforms the state-of-the-art methods by an order of magnitude.",PDSM: Pregel-Based Distributed Subgraph Matching on Large Scale RDF Graphs,NA:NA:NA:NA:NA,2018
Yueqi Xin:Bingyi Zhang:Xin Wang:Qiang Xu:Zhiyong Feng,"This paper proposes a novel method for answering Pregel-based Parallel Provenance-aware Regular Path Queries (P3RPQ) on large RDF graphs. Our method is developed using the Pregel framework, which utilizes Glushkov automata to keep track of the matching process of RPQs in parallel. Meanwhile, four optimization strategies are devised, which can reduce the response time of the basic algorithm dramatically and overcome the counting paths problem to some extent. The experiments are conducted to verify the performance of our algorithms on both synthetic and real-world datasets.",P3RPQ: Pregel-Based Parallel Provenance-Aware Regular Path Query Processing on Large RDF Graphs,NA:NA:NA:NA:NA,2018
An-Zi Yen:Hen-Hsen Huang:Hsin-Hsi Chen,"People are used to log their life on the social media platform. Life event can be expressed explicitly or implicitly in a text description. However, a description does not always contain life events related to a specific individual. To tell if there exist any life events and further know their categories is indispensable for event retrieval. This paper explores various LSTM models to detect and classify life events in tweets. Experiments show that the proposed Multi-Task LSTM model with attention achieves the best performance.",Detecting Personal Life Events from Twitter by Multi-Task LSTM,NA:NA:NA,2018
Reut Apel:Elad Yom-Tov:Moshe Tennenholtz,"Users of social networks often focus on specific areas of that network, leading to the well-known ""filter bubble"" effect. Connecting people to a new area of the network in a way that will cause them to become active in that area could help alleviate this effect and improve social welfare. Here we present preliminary analysis of network referrals, that is, attempts by users to connect peers to other areas of the network. We classify these referrals by their efficiency, i.e., the likelihood that a referral will result in a user becoming active in the new area of the network. We show that by using features describing past experience of the referring author and the content of their messages we are able to predict whether referral will be effective, reaching an AUC of 0.87 for those users most experienced in writing efficient referrals. Our results represent a first step towards algorithmically constructing efficient referrals with the goal of mitigating the ""filter bubble"" effect pervasive in on line social networks.",Characterizing Efficient Referrals in Social Networks,NA:NA:NA,2018
Moumita Basu:Anurag Shandilya:Kripabandhu Ghosh:Saptarshi Ghosh,"During a disaster event, it is essential to know about needs and availabilities of different types of resources, for coordinating relief operations. Microblogging sites are frequently used for aiding post-disaster relief operations, and there have been prior attempts to identify tweets that inform about resource needs and availabilities (termed as need-tweets and availability-tweets respectively). However, there has not been much attempt to effectively utilise such tweets. We introduce the problem of automatically matching need-tweets with appropriate availability-tweets, which is practically important for coordination of post-disaster relief operations. We also experiment with several methodologies for automatically matching need-tweets and availability-tweets.",Automatic Matching of Resource Needs and Availabilities in Microblogs for Post-Disaster Relief,NA:NA:NA:NA,2018
Gaoyang Guo:Chaokun Wang:Xiang Ying,"A myriad of community detection methods have been designed to discover communities based on specific network features in different disciplines, such as sociology, physics, and computer science. Consequentially, we have to face the problem of Algorithm Selection for Community Detection (ASCD): Given a specific network, which algorithm should we select to reveal its latent community structures In this study, we propose a model called CYDES to address the ASCD problem. CYDES consists of two parts, namely feature matrix generation and algorithm classification. We combine three effective feature extraction methods with the idea of BOW model to construct a fixed-size feature matrix. After a nonlinear transformation to the feature matrix, a softmax regression model is utilized to generate a classification label representing the best community detection algorithm we select. Extensive experimental results demonstrate that CYDES has high algorithm selection quality for community detection in networks.",Which Algorithm Performs Best: Algorithm Selection for Community Detection,NA:NA:NA,2018
Longtao Huang:Shangwen Lv:Liangjun Zang:Yipeng Su:Jizhong Han:Songlin Hu,"This paper proposes a novel approach to retrieve news articles related to a specific event and generate a storyline to help people understand the event evolution. First, a similarity calculation method is proposed to retrieve news articles related to the specific event, which combines textual similarity, temporal similarity and entity similarity. Then a multi-view attribute graph is constructed to represent the relationship between retrieved articles. Finally, a community detection algorithm is developed to segment and chain subevents in the graph. Experimental results on real-world datasets demonstrate that the proposed approach achieve better results than existing methods.0F0F",A Fresh Look at Understanding News Events Evolution,NA:NA:NA:NA:NA:NA,2018
Yu Fu:Dongliang Zhang:Hao Jiang,"Besides usability, visual appearance also plays an important role in influencing users' attitudes towards the mobile shopping apps. This article presents a pilot study that explores users' and designers' preferences for mobile shopping app user interfaces (UIs). The study consisted of two phases. (1) Eliciting participants' perception of UIs similarity by sorting, using DISTATIS and cluster analysis, the UIs similarity perceptual space was identified. (2) Eliciting participants' overall preference by rating. The results identified three typical UIs and the distribution of ideal preference UIs for users and designers. Last, users and designers' differences in the UI preference were discussed.",Comparison of Users' and Designers' Differences in Mobile Shopping App Interface Preferences and Designs,NA:NA:NA,2018
Saurabh Sohoney:Nikita Prabhu:Vineet Chaoji,"Inverse Propensity Score estimator (IPS) is a basic, unbiased, off-policy evaluation technique to measure the impact of a user-interactive system without serving live traffic. We present our work on applying IPS to real-world settings by addressing some practical challenges, thereby enabling successful policy evaluation. In particular, we show that off-policy evaluation can be impossible in the absence of a complete context and we describe a systematic way of defining the context.",Handling Confounding for Realistic Off-Policy Evaluation,NA:NA:NA,2018
Yunqi Qiu:Manling Li:Yuanzhuo Wang:Yantao Jia:Xiaolong Jin,"Topic entity detection is to find out the main entity asked in a question, which is significant in question answering. Traditional methods ignore the information of entities, especially entity types and their hierarchical structures, restricting the performance. To take full advantage of Knowledge Base(KB) and detect topic entities correctly, we propose a deep neural model to leverage type hierarchy and relations of entities in KB. Experimental results demonstrate the effectiveness of the proposed method.",Hierarchical Type Constrained Topic Entity Detection for Knowledge Base Question Answering,NA:NA:NA:NA:NA,2018
Dhruv Gupta:Klaus Berberich,"Knowledge graphs capture very little temporal information associated with facts. In this work, we address the problem of identifying time intervals of knowledge graph facts from large document collections annotated with temporal expressions. Prior approaches in this direction have leveraged limited metadata associated with documents in large collections (e.g., publication dates) or have limited techniques to model the uncertainty and dynamics of temporal expressions. Our approach to identify time intervals for time-sensitive facts in knowledge graphs leverages a time model that incorporates uncertainty and models them at different levels of granularity (i.e., day, month, and year). Evaluation on a temporal fact benchmark using two large news archives amounting to more than eleven million documents show the quality of our results.",Identifying Time Intervals for Knowledge Graph Facts,NA:NA,2018
Fei Wu:Yanen Li:Ning Xu,"The affinity of a user to a type of items (e.g., stories from the same publisher, and movies of the same genre) is an important signal reflecting the user's interests. Accurately estimating of the user type affinity has various applications in ranking and recommendation systems. For frequent users, simply dividing the number of interactions with content (e.g., clicks) by the number of impressions (e.g., the number of times the content is presented to each user) would be a good estimate. However, such estimates are erroneous for users who have sparse interaction history, (e.g., new users). To alleviate the problem, feature-based approaches aim to learn functions predicting the affinity score using only none-click features, such as user demographics, locations, and interests. Likewise, such approaches do not take full advantage of the interaction history of frequent users. Motivated by the limitations of the two approaches, we propose a Gamma-Poisson model that aims at utilizing the interaction history of frequent users, as well as leveraging a feature-based model for infrequent users. Our intuition is that we should rely more on the interaction history when estimating affinity for frequent users, and weigh more on feature-based model for infrequent users. We present experimental results on large-scale real-world data in a publisher content clicks prediction task to demonstrate the effectiveness of the proposed method in estimating user type affinity scores.",User Type Affinity Estimation Using Gamma-Poisson Model,NA:NA:NA,2018
Qiang Zhang:Emine Yilmaz:Shangsong Liang,"A valuable step towards news veracity assessment is to understand stance from different information sources, and the process is known as the stance detection. Specifically, the stance detection is to detect four kinds of stances (""agree'', ""disagree'', ""discuss'' and ""unrelated'') of the news towards a claim. Existing methods tried to tackle the stance detection problem by classification-based algorithms. However, classification-based algorithms make a strong assumption that there is clear distinction between any two stances, which may not be held in the context of stance detection. Accordingly, we frame the detection problem as a ranking problem and propose a ranking-based method to improve detection performance. Compared with the classification-based methods, the ranking-based method compare the true stance and false stances and maximize the difference between them. Experimental results demonstrate the effectiveness of our proposed method.",Ranking-based Method for News Stance Detection,NA:NA:NA,2018
Anurag Roy:Kripabandhu Ghosh:Moumita Basu:Parth Gupta:Saptarshi Ghosh,"The Web has several information sources on which an ongoing event is discussed. To get a complete picture of the event, it is important to retrieve information from multiple sources. We propose a novel neural network based model which integrates the embeddings from multiple sources, and thus retrieves information from them jointly, %all the sources together, as opposed to combining multiple retrieval results. The importance of the proposed model is that no document-aligned comparable data is needed. Experiments on posts related to a particular event from three different sources - Facebook, Twitter and WhatsApp - exhibit the efficacy of the proposed model.",Retrieving Information from Multiple Sources,NA:NA:NA:NA:NA,2018
Tanmoy Chakraborty:Zhe Cui:Noseong Park,"A community detection (CD) method is usually evaluated by what extent it is able to discover the 'ground-truth' community structure of a network. A certain 'node-centric metadata' is used to define the ground-truth partition. However, nodes in real networks often have multiple metadata types (e.g., occupation, location); each can potentially form a ground-truth partition. Our experiment with 10 CD methods on 5 datasets (having multiple metadata-based ground-truth partitions) show that the metadata-based evaluation is misleading because there is no single CD method that can outperform others by detecting all types of metadata-based partitions. We further show that the community structure obtained from the CD methods is usually topologically stronger than any metadata-based partitions. Finally, we suggest a new task-based evaluation framework for CD methods and show that a certain type of CD methods is useful for a certain type of task.",Metadata vs. Ground-truth: A Myth behind the Evolution of Community Detection Methods,NA:NA:NA,2018
Zeyang Lei:Yujiu Yang:Yi Liu,"Sentiment analysis of social media and comment data is an important issue in opinion monitoring. In this work, we propose a Linguistic-Aware Attention Network (LANN) to enhance the performance of convolution neural network (CNN). LANN adopts a two-stage strategy to model the sentiment-specific sentence representation. First, an interactive attention mechanism is designed to model word-level semantics. Second, to capture phrase-level linguistic structure, a dynamic semantic attention is adopted to select the crucial phrase chunks in the sentence. The experiments demonstrate that LANN has robust superiority over competitors and has reached the state-of-the-art performance.",LAAN: A Linguistic-Aware Attention Network for Sentiment Analysis,NA:NA:NA,2018
Daniel Alexandrov:Viktor Karepin:Ilya Musabirov:Daria Chuprina,"We use social media and WWW data to analyse international educational migration from Russia. We find substantial regional differences in migration patterns for three contrast directions: the Nordic countries, China and the Middle East. We built a model of migration flows with geographic distances to destination countries, various socio-demographic data and institutional characteristics of educational organisations.","Educational Migration from Russia to the Nordic Countries, China and the Middle East. Social Media Data",NA:NA:NA:NA,2018
Mengjing Chen:Weiran Shen:Pingzhong Tang:Song Zuo,"Over the past few years, ride-sharing has been proven to be an effective way to relieve urban traffic congestion, as evidenced by several emerging ride-sharing platforms such as Uber and Didi. A key economic problem for these platforms is to design a revenue-optimal (or welfare-optimal) pricing scheme and a corresponding vehicle dispatching policy that incorporates geographic information, and more importantly, dynamic supply and demand. In this paper, we aim to solve this problem by introducing a unified model that takes into account both travel time and driver redirection. We tackle the non-convexity problem using the ""ironing"" technique and formulate the optimization problem as a Markov decision process (MDP), where the states are the driver distributions and the decision variables are the prices. Our main finding is to give an efficient algorithm that computes the exact revenue (or welfare) optimal randomized pricing schemes. We characterize the optimal solutions of the MDP by primal-dual analysis of a convex program. We also conduct empirical analysis of our solution with real data of a major ride-sharing platform and show its significant advantages over fixed pricing schemes as well as those prevalent surge-based pricing schemes.",Optimal Vehicle Dispatching for Ride-sharing Platforms via Dynamic Pricing,NA:NA:NA:NA,2018
Deguang Kong:Xiannian Fan:Konstantin Shmakov:Jian Yang,"Bid optimization, which aims to find the competitive bid to achieve the best performance for the advertiser, is an important problem in online advertising. The optimal bid recommendation enables the advertisers to make informed decisions without actually spending the budget. In this paper, we consider a bid optimization scenario that the advertiser's budget can be split across multiple campaigns. To achieve the optimal performance, we formalize the bid optimization problem as a constraint combinational optimization problem, and derive an effective method to solve it. Experiment studies on real- world ad campaigns demonstrate the effectiveness of our method.",A Combinational Optimization Approach for Advertising Budget Allocation,NA:NA:NA:NA,2018
Zhile Jiang:Shuai Yu:Qiang Qu:Min Yang:Junyu Luo:Juncheng Liu,"Author profiling is an important but challenging task. In this paper, we propose a novel Multi-Task learning framework for Author Profiling (MTAP), in which a document modeling module is shared across three different author profiling tasks (i.e., age, gender and job classification tasks). To further boost author profiling, we integrate hierarchical features learned by different models. Concretely, we employ CNN, LSTM and topic model to learn the character-level, word-level and topic-level features, respectively. MTAP thus leverages the benefits of supervised deep neural neural networks as well as an unsupervised probabilistic generative model to enhance the document representation learning. Experimental results on a real-life blog dataset show that MTAP has robust superiority over competitors and sets state-of-the-art for all the three author profiling tasks",Multi-task Learning for Author Profiling with Hierarchical Features,NA:NA:NA:NA:NA:NA,2018
Qi Zhu:Xiang Ren:Jingbo Shang:Yu Zhang:Frank F. Xu:Jiawei Han,"Extracting entities and their relations from text is an important task for understanding massive text corpora. Open information extraction (IE) systems mine relation tuples (i.e., entity arguments and a predicate string to describe their relation) from sentences. However, current open IE systems ignore the fact that global statistics in a large corpus can be collectively leveraged to identify high-quality sentence-level extractions. In this paper, we propose a novel open IE system, called ReMine, which integrates local context signal and global structural signal in a unified framework with distant supervision. The new system can be efficiently applied to different domains as it uses facts from external knowledge bases as supervision; and can effectively score sentence-level tuple extractions based on corpus-level statistics. Specifically, we design a joint optimization problem to unify (1) segmenting entity/relation phrases in individual sentences based on local context; and (2) measuring the quality of sentence-level extractions with a translating-based objective. Experiments on real-world corpora from different domains demonstrate the effectiveness and robustness of ReMine when compared to other open IE systems.",Open Information Extraction with Global Structure Constraints,NA:NA:NA:NA:NA:NA,2018
Wenyu Du:Shuai Yu:Min Yang:Qiang Qu:Jia Zhu,"In this paper, we propose GPSP, a novel Graph Partition and Space Projection based approach, to learn the representation of a heterogeneous network that consists of multiple types of nodes and links. Concretely, we first partition the heterogeneous network into homogeneous and bipartite subnetworks. Then, the projective relations hidden in bipartite subnetworks are extracted by learning the projective embedding vectors. Finally, we concatenate the projective vectors from bipartite subnetworks with the ones learned from homogeneous subnetworks to form the final representation of the heterogeneous network. Extensive experiments are conducted on a real-life dataset. The results demonstrate that GPSP outperforms the state-of-the-art baselines in two key network mining tasks: node classification and clustering.",GPSP: Graph Partition and Space Projection based Approach for Heterogeneous Network Embedding,NA:NA:NA:NA:NA,2018
Yong-Yeon Jo:Myung-Hwan Jang:Hyungsoo Jung:Sang-Wook Kim,Existing single-machine based graph engines do not leverage the characteristic of social networks following the power-law degree distribution. We propose a new graph engine tailored for processing and analyzing large-scale social networks efficiently by exploiting the power-law degree property,A High-Performance Graph Engine for Efficient Social Network Analysis,NA:NA:NA:NA,2018
Suman Kalyan Maity:Santosh K. C.:Arjun Mukherjee,"In this paper, we propose a semi-supervised framework Spam2Vec to identify spammers in Twitter. This algorithmic framework learns the spam representations of the node in the network by leveraging biased random walks. Our spammer detection method yields an AUC of 0.54 with [email protected] as 0.12 and performs significantly better with 7.77% increase in AUC and a 2.4 times improvement on precision over the best performing baseline.",Spam2Vec: Learning Biased Embeddings for Spam Detection in Twitter,NA:NA:NA,2018
Zhitao Wang:Chengyao Chen:Wenjie Li,"In this paper, we propose an attention network for diffusion prediction problem. The developed diffusion attention module can effectively explore the implicit user-to-user diffusion dependency among information cascade users. Besides, the user-to-cascade importance and the time-decay effect are captured and utilized by the model. The superiority of the proposed model over state-of-the-art methods is demonstrated by experiments on real diffusion data.",Attention Network for Information Diffusion Prediction,NA:NA:NA,2018
Po-Cheng Huang:Hen-Hsen Huang:Hsin-Hsi Chen,"Knowledge base completion (KBC) involves in discovering missing facts. However, knowledge changes over time. Some facts need to be removed from knowledge base (KB) to keep knowledge base integrity (KBI) while new facts are inserted or old facts are deleted. This paper proposes a path-based learning model to learn the dependency of dynamic relations automatically. In this way, we can eliminate the conflicting facts and keep KB clean. That would be a significant benefit for KBC and other tasks using KB.",Path Ranking with Path Difference Sets for Maintaining Knowledge Base Integrity,NA:NA:NA,2018
Chen Ling:Lei Wang:Jun Lang:Qiufen Xia:Guoxuan Chang:Kun Wang:Peng Zhao,"Internet access restriction in various areas unceasingly poses inconvenience for users. Unblocked websites or web pages always accompany couple fate-to-fail links from blocked servers due to such restrictions, while users wait for a long time to see contents of these blocked/invalid links. Therefore, it is better to directly show timeout links to users for better experiences rather than making users excessively wait. In this paper, we present LinCa (Links Catcher), a novel approach that fully considers the Internet access restriction and reduces page loading time on client-side by parsing all HTTP requests, and intercepting all invalid links when a web navigation starts. Thus, we first create and maintain a Rule Base to store invalid links under given access restriction rules. We then update the Rule Base periodically to cover as many invalid links as possible and remove links that become valid. We finally demonstrate the effectiveness of LinCa through experiments by building and deploying a Chrome extension. Experimental results show that LinCa can reduce page loading time with average 28.12% of original page loading time for our data sets.",LinCa: A Page Loading Time Optimization Approach for Users Subject to Internet Access Restriction,NA:NA:NA:NA:NA:NA:NA,2018
Wouter Lightenberg:Yulong Pei:George Fletcher:Mykola Pechenizkiy,"We introduce the Tink library for distributed temporal graph analytics. Increasingly, reasoning about temporal aspects of graph-structured data collections is an important aspect of analytics. For example, in a communication network, time plays a fundamental role in the propagation of information within the network. Whereas existing tools for temporal graph analysis are built stand alone, Tink is a library in the Apache Flink ecosystem, thereby leveraging its advanced mature features such as distributed processing and query optimization. Furthermore, Flink requires little effort to process and clean the data without having to use different tools before analyzing the data. Tink focuses on interval graphs in which every edge is associated with a starting time and an ending time. The library provides facilities for temporal graph creation and maintenance, as well as standard temporal graph measures and algorithms. Furthermore, the library is designed for ease of use and extensibility.",Tink: A Temporal Graph Analytics Library for Apache Flink,NA:NA:NA:NA,2018
Debasis Ganguly:Kripabandhu Ghosh,"Effective clustering of short documents, such as tweets, is difficult because of the lack of sufficient semantic context. Word embedding is a technique that is effective in addressing this lack of semantic context. However, the process of word vector embedding, in turn, relies on the availability of sufficient contexts to learn the word associations. To get around this problem, we propose a novel word vector training approach that leverages topically similar tweets to better learn the word associations. We test our proposed word embedding approach by clustering a collection of tweets on disasters. We observe that the proposed method improves clustering effectiveness by up to 14%.",Contextual Word Embedding: A Case Study in Clustering Tweets about Emergency Situations,NA:NA,2018
Markus Schedl:Eelco Wiechert:Christine Bauer,"We approach the research question whether real-world events, such as sport events or product launches, influence music consumption behavior. To this end, we consider events of different categories from Google Trends and model listening events as time series using Last.fm data. Performing an auto-regressive integrated moving average analysis to decompose the signal and subsequently an intervention time series analysis, we find significant signal discontinuities, in particular for the Google news category. We found that news and events are likely to increase the number of songs listened to per person per day by about 2%, while tech events commonly cause 1% less music being consumed.",The Effects of Real-world Events on Music Listening Behavior: An Intervention Time Series Analysis,NA:NA:NA,2018
Abhijnan Chakraborty:Mohammad Luqman:Sidhartha Satapathy:Niloy Ganguly,"With a large number of stories emerging from the newsrooms, media websites need to curate interesting news for their readers. Although traditionally news was curated solely by human editors, increasing news volume has led media outlets to adopt editorial algorithms. However, such algorithms are often proprietary, and smaller outlets do not have the resources to build them from scratch. In this paper, we present a novel framework 'Samar' to automatically curate news by optimizing recency, relevance and diversity of the selected stories. Evaluations over two real-world news datasets show that Samar outperforms several state-of-the-art baselines in matching the news curation performed by human editors.","Editorial Algorithms: Optimizing Recency, Relevance and Diversity for Automated News Curation",NA:NA:NA:NA,2018
Hoang-Long Nguyen:Claudia-Lavinia Ignat:Olivier Perrin,"Public key server is a simple yet effective way of key management in secure end-to-end communication. To ensure the trustworthiness of a public key server, transparent log systems such as CONIKS employ a tamper-evident data structure on the server and a gossiping protocol among clients in order to detect compromised servers. However, due to lack of incentive and vulnerability to malicious clients, a gossiping protocol is hard to implement in practice. Meanwhile, alternative solutions such as EthIKS are not scalable. This paper presents Trusternity, an auditing scheme relying on Ethereum blockchain that is easy to implement, scalable and inexpensive to operate.",Trusternity: Auditing Transparent Log Server with Blockchain,NA:NA:NA,2018
Jiaming Song:Xiaowang Zhang:Peng Peng:Zhiyong Feng:Lei Zou,"In this paper, we present a plugin-based framework (MapSQ) with three parts for SPARQL queries utilizing high-performance of GPU to accelerate answering in a convenient way. Selector chooses suitable join order according to characteristics of data and queries. Executor answers subqueries and returns intermediate solutions and GPU Computing obtains the join result of intermediate solutions through MapReduce. Finally, we evaluate MapSQ bulit on gStore and RDF-3X on the LUBM benchmark and YAGO datasets (over 200 million triples). The experimental results show that MapSQ significantly improves the performance of SPARQL query engines with speedup up to 33.",MapSQ: A Plugin-based MapReduce Framework for SPARQL Queries on GPU,NA:NA:NA:NA:NA,2018
Deguang Kong:Konstantin Shmakov:Jian Yang,"In cost-per-click (CPC) or cost-per-impression (CPM) advertising campaigns, advertisers always run the risk of spending the bud- get without getting enough conversions. Moreover, the bidding on advertising inventory has few connections with propensity that can reach to cost-per-acquisition (CPA) goals. To address this problem, this paper presents a bid optimization scenario to achieve the desired CPA goals for advertisers. In particular, we build the optimization engine to make a decision by solving the constrained optimization problem. The proposed model can naturally recommend the bid that meets the advertisers' expectations by making inference over history auction behaviors. The bid optimization model outperforms the baseline methods on real-world campaigns, and can be applied into a wide range of scenarios for performance improvement and revenue liftup.",Demystifying Advertising Campaign for CPA Goal Optimization,NA:NA:NA,2018
Ugo Tanielian:Anne-Marie Tousch:Flavian Vasile,"Over the last decade, the number of devices per person has increased substantially. This poses a challenge for cookie-based personalization applications, such as online search and advertising, as it narrows the personalization signal to a single device environment. A key task is to find which cookies belong to the same person to recover a complete cross-device user journey. Recent work on the topic has shown the benefits of using unsupervised embeddings learned on user event sequences. In this paper, we extend this approach to a supervised setting and introduce the Siamese Cookie Embedding Network (SCEmNet), a siamese convolutional architecture that leverages the multi-modal aspect of sequences, and show significant improvement over the state-of-the-art.",Siamese Cookie Embedding Networks for Cross-Device User Matching,NA:NA:NA,2018
Saket Maheshwary:Hemant Misra,"In this paper we investigate the important and challenging task of recommending appropriate jobs for job seeking candidates by matching semi structured resumes of candidates to job descriptions. To perform this task, we propose to use a siamese adaptation of convolutional neural network. The proposed approach effectively captures the underlying semantics thus enabling to project similar resumes and job descriptions closer to each other, and make dissimilar resumes and job descriptions distant from each other in the semantic space. Our experimental results on a set of 1314 resumes and a set of 3809 job descriptions (5,005,026 resume-job description pairs) demonstrate that our approach is better than the current state-of-the-art approaches.",Matching Resumes to Jobs via Deep Siamese Network,NA:NA,2018
Baoxu Shi:Tim Weninger,"Understanding and visualizing human discourse has long being a challenging task. Although recent work on argument mining have shown success in classifying the role of various sentences, the task of recognizing concepts and understanding the ways in which they are discussed remains challenging. Given an email thread or a transcript of a group discussion, our task is to extract the relevant concepts and understand how they are referenced and re-referenced throughout the discussion. In the present work, we present a preliminary approach for extracting and visualizing group discourse by adapting Wikipedia's category hierarchy to be an external concept ontology. From a user study, we found that our method achieved better results than 4 strong alternative approaches, and we illustrate our visualization method based on the extracted discourse flows.",Visualizing the Flow of Discourse with a Concept Ontology,NA:NA,2018
Deguang Kong:Konstantin Shmakov:Jian Yang,"In online advertising, a common objective for advertisers is to get the maximum returns on investment given the budget. On one hand, if the bid is too high, the advertiser pays more money than he should pay for the same number of clicks. On the other hand, it the bid is too low, the advertiser cannot win in auctions and therefore it loses the opportunity. A challenging problem is how to recommend the bid to achieve the maximum values for advertisers. In this paper, we present an inflection point approach for bid recommendation from discovering the bid price of click(bid)1 function at which the function changes from significant increase (i.e. concave downward) to slow increase (convex upward). We derive the optimal solution using history sparse and noisy observations given the budget limit. In real word advertising campaign evaluations, the proposed bid recommendation scenario brings in 15.37% bid increase and 30.24% click increase over the baselines.",An Inflection Point Approach for Advertising Bid Optimization,NA:NA:NA,2018
Helen Spiers:Alexandra Swanson:Lucy Fortson:Brooke D. Simmons:Laura Trouille:Samantha Blickhan:Chris Lintott,"Human-computer systems are increasingly applied to data reduction problems; citizen science platforms (e.g. the Zooniverse) are one type of such a system. These platforms function as social machines, combining volunteer efforts with automated processes to enable distributed data analysis. The rapid growth of this approach is increasing the need to understand how we can improve volunteer interaction and engagement. Here, we utilize the most comprehensive collection of online citizen science data gathered to date to examine multiple variables across 63 Zooniverse projects. Our analyses reveal how subtle design changes can influence many facets of volunteer interaction, generating insights that have implications for the design and study of citizen science projects, and future research.",Patterns of Volunteer Behaviour Across Online Citizen Science,NA:NA:NA:NA:NA:NA:NA,2018
Evgeny Krivosheev:Bahareh Harandizadeh:Fabio Casati:Boualem Benatallah,"In this paper we describe how crowd and machine classifier can be efficiently combined to screen items that satisfy a set of predicates. We show that this is a recurring problem in many domains, present machine-human (hybrid) algorithms that screen items efficiently and estimate the gain over human-only or machine-only screening in terms of performance and cost.",Crowd-Machine Collaboration for Item Screening,NA:NA:NA:NA,2018
Anurag Shandilya:Kripabandhu Ghosh:Saptarshi Ghosh,"We propose to evaluate extractive summarization algorithms from a completely new perspective. Considering that an extractive summarization algorithm selects a subset of the textual units in the input data for inclusion in the summary, we investigate whether this selection is fair. We use several summarization algorithms over datasets that have a sensitive attribute (e.g., gender, political leaning) associated with the textual units, and find that the generated summaries often have very different distributions of the said attribute. Specifically, some classes of the textual units are under-represented in the summaries according to the fairness notion of adverse impact. To our knowledge, this is the first work on fairness of summarization, and is likely to open up interesting research problems.",Fairness of Extractive Text Summarization,NA:NA:NA,2018
Elias Moons:Tinne Tuytelaars:Marie-Francine Moens,"Images have a prominent role in the communication of news on the Web. We propose a novel method for image classification with subject categories when limited annotated images are available for training the classifier. A neural network based encoder learns image representations from paired news images and their texts. Once trained, this encoder transforms any image to a text-enriched representation of the image, which is then used as input for the classifier that categorizes an image according to its subject category. We have trained classifiers with different amounts of annotated images and found that the image classifier that uses the text-enriched image representations outperforms a baseline model that only uses image features especially in cases with limited training examples.",Text-Enriched Representations for News Image Classification,NA:NA:NA,2018
Jurek Leonhardt:Avishek Anand:Megha Khosla,Recent works in recommendation systems have focused on diversity in recommendations as an important aspect of recommendation quality. In this work we argue that the post-processing algorithms aimed at only improving diversity among recommendations lead to discrimination among the users. We introduce the notion of user fairness which has been overlooked in literature so far and propose measures to quantify it. Our experiments on two diversification algorithms show that an increase in aggregate diversity results in increased disparity among the users.,User Fairness in Recommender Systems,NA:NA:NA,2018
Tomoki Sato:Hiroaki Shiokawa:Yuto Yamaguchi:Hiroyuki Kitagawa,"ObjectRank is one of the popular graph mining methods that enables us to evaluate the importance of each vertex on heterogeneous graphs. However, it is computationally expensive to apply it to large graphs since ObjectRank needs to compute the importance of all vertices iteratively. In this work, we present a fast ObjectRank algorithm,FORank, that accurately approximates the keyword search results. FORank iteratively prunes vertices whose convergence score likely has less impact on the results during iterative computation. The experiments showed that FORank runs 7 times faster than ObjectRank computation with over 90% accuracy approximation.",FORank: Fast ObjectRank for Large Heterogeneous Graphs,NA:NA:NA:NA,2018
Amit Sarkar:G. Srinivasaraghavan,"We investigate the task of reading context-biased web summarization, where the goal is to extract information relevant to the current reading context from a cited web article. In certain kind of linked document sets such as Wikipedia articles, scientific papers as well as news and blogs, such contextual summaries can be useful in providing additional related information to the user helping in the reading task. In this work, we focus on web articles only and try to find out the set of key components that contribute to building up the reading context. We build a supervised model for ranking sentences from the cited document according to their contextual salience. Initial evaluation based on annotated data-set of web articles show that our ranking model performs better than the generic summaries as well as baseline context-biased summaries.",Contextual Web Summarization: A Supervised Ranking Approach,NA:NA,2018
Madian Khabsa:Ahmed El Kholy:Ahmed Hassan Awadallah:Imed Zitouni:Milad Shokouhi,"Digital assistants are emerging to become more prevalent in our daily lives. In interacting with these assistants, users may engage in multiple tasks within a short period of time. Identifying task boundaries and isolating them within a session is critical for measuring the performance of the system on each individual task. In this paper we aim to automatically identify sequences of interactions that together form a task. To this end, we sample interactions from a real world digital assistant and use crowd judges to segment a session into multiple tasks. After that, we use a machine learned model to identify task boundaries. Our learned model with its features significantly outperform the baselines. To the best of our knowledge, this is the first work that aims to identify tasks within digital assistant sessions.",Identifying Task Boundaries in Digital Assistants,NA:NA:NA:NA:NA,2018
Martin Atzmueller:Florian Lemmerich,"Academic conferences are a backbone for the exchange of ideas in scientific communities. However, so far little is known about the communication networks emerging at those venues. Besides personal knowledge, network homophily has been identified as a driving factor for establishing contacts and followerships in social networks, i.e., people are more likely to engage with others if they are similar with respect to certain attributes. In this paper, we describe work in progress on investigating homophily at four academic conferences based on face-to-face (F2F) contact data collected using wearable sensors between conference participants. In particular, we study which personal attributes are predictive for face-to-face contacts. For that purpose, we obtained diverse personal attributes from online sources in order to elicit a variety of hypotheses, which can then be compared using descriptive statistics and a Bayesian method for comparing hypotheses in networks. Our results suggest that personal knowledge (as derived from DBLP and ResearchGate networks) and homophilic behavior with respect to several attributes, e.g., gender or country of origin, are important factors for contacts at academic conferences.",Homophily at Academic Conferences,NA:NA,2018
William Brendel:Fangqiu Han:Luis Marujo:Luo Jie:Aleksandra Korolova,"Making friend recommendations is an important task for social networks, as having more friends typically leads to a better user experience. Most current friend recommendations systems grow the existing network at the cost of privacy. In particular, any given user's friend graph may be directly or indirectly leaked as a result of such recommendations. In many situations this is not desirable, as the friend list may reveal much about the user--from their identity to their sexual orientation and interests. In this work, we focus on the ""cold start"" problem of making friend recommendations for new users while raising the bar on protecting the privacy of the friend list of all users. We propose a practical friend recommendation framework, tested on the Snapchat social network, that preserves the privacy of users' friends lists with respect to brute-force attacks and scales to millions of users.",Practical Privacy-Preserving Friend Recommendations on Social Networks,NA:NA:NA:NA:NA,2018
Yingying Wu:Yiqun Liu:Ke Zhou:Xiaochuan Wang:Min Zhang:Shaoping Ma,"Diversifying search results to satisfy as many users' intentions as possible is NP-hard. Some research employs a pruned exhaustive search, and some uses a greedy approach. However, the objective function of the result diversification problem adopts the cascade assumption which assumes users' information needs will drop once their subtopic search intents are satisfied. As a result, the intent distribution of diversified results deviates from the actual distribution of user intentions, and each subtopic tends to be chosen equally. This phenomenon is unreasonable, especially when the original distribution of user intent is unbalanced. In this paper, we present empirical evidence of the diversification equilibrium by showing that the standard deviations of subtopic distribution approach zero.",Treating Each Intent Equally: The Equilibrium of IA-Select,NA:NA:NA:NA:NA:NA,2018
Peizhi Wu:Yi Tu:Zhenglu Yang:Adam Jatowt:Masato Odagaki,"Modeling the evolution of user preferences and item attributes in a dynamic social network is important because it is the basis for many applications, including recommendation systems and user behavior analysis. This study introduces a comprehensive general neural framework with several optimal strategies to jointly model the evolution of user preferences and item attributes in dynamic social networks. Preliminary experimental results conducted on real-world datasets demonstrate that our model performs better than the state-of-the-art methods.",Deep Modeling of the Evolution of User Preferences and Item Attributes in Dynamic Social Networks,NA:NA:NA:NA:NA,2018
Yingru Lin:Soyeon Caren Han:Byeong Ho Kang,"The peer assessment approach is considered to be one of the best solutions for scaling both assessment and peer learning to global classrooms, such as MOOCs. However, some academic staff hesitate to use a peer assessment approach for their classes due to concerns about its credibility and reliability. The focus of our research is to detect the credibility level of each assessment performed by students during peer assessment. We found three major scopes in assessing the credibility level of evaluations, 1) Informativity, 2) Accuracy, and 3) Consistency. We collect assessments, including comments and grades provided by students during the peer assessment process and then each feedback-and-grade pair is labeled with its credibility level by Mechanical Turk evaluators. We extract relevant features from each labeled assessment and use them to build a classifier that attempts to automatically assess its level of credibility in C5.0 Decision Tree classifier. The evaluation results show that the model can be used to automatically classify peer assessments as credible or non-credible, with accuracy in the range of 88%.",Machine Learning for the Peer Assessment Credibility,NA:NA:NA,2018
Qian Li:Ziwei Li:Jin-Mao Wei:Zhenglu Yang:Yanhui Gu:R. Uday Kiran,"Predicting the ending of a story is an interesting issue that has attracted considerable attention, as in case of the ROC Story Cloze Task (SCT). Although several studies have addressed this issue, the performance remains unsatisfactory due to ineffectiveness of story comprehension. In this paper, we propose to construct a story coherence based neural network model (SCNN) with well-designed optimizations. The preliminary evaluation demonstrates the effectiveness of our model which is superior to that of state-of-the-art approaches.",A Story Coherence based Neural Network Model for Predicting Story Ending,NA:NA:NA:NA:NA:NA,2018
Min Gui:Zhengkun Zhang:Zhenglu Yang:Yanhui Gu:Guandong Xu,"Document summarization is an important research issue and has attracted much attention from the academe. The approaches for document summarization can be classified as extractive and abstractive. In this work, we introduce an effective joint framework that integrates extractive and abstractive summarization models, which is much closer to the way human write summaries (first underlining important information). Preliminary experiments on real benchmark dataset demonstrate that our model is competitive with the state-of-the-art methods.",An Effective Joint Framework for Document Summarization,NA:NA:NA:NA:NA,2018
Paul Groth:Amélie Gyrard,"The Demo Track is one of the most exciting parts of any Web Conference. It allows researchers and practitioners to demonstrate new systems in an engaging and hands-on manner to the community. The Web has been driven forward by building systems and technology. The demo track is a venue that encourages this sort of important type of result This year the track received 71 submissions of those 30 were accepted for a 42% accept rate. We had a comprehensive review procedure that looked at a number of dimensions including the novelty of the demo, its fit with the conference, its research content, and its potential for audience engagement. We were pleased by the number of submissions that included links to online demonstrations and/or videos. This gave reviewers additional information about how the demo would be presented. Overall, we had 232 reviews across all submissions. Many of the reviews provided not only a their expert judgement but ways in which the submissions could be improved. It is often difficult judging demonstrations as there are multiple factors to be taken into account. We want to thank the entire committee for taking the time to support the track. The resulting set of selected demos reflects the wide-variety of technology and research interests impacting the wide. Demonstrations cover topics such as using data on the web, the integration of the web and the physical world, knowledge graphs, search engines, security and privacy, and dealing with multimedia data. We believe that these demos provide an exciting taste of the future of the Web.",Demo Track Chairs' Welcome & Organization,NA:NA,2018
Ruben Taelman:Miel Vander Sande:Ruben Verborgh,"The Linked Open Data cloud is evergrowing and many datasets are frequently being updated. In order to fully exploit the potential of the information that is available in and over historical dataset versions, such as discovering evolution of taxonomies or diseases in biomedical datasets, we need to be able to store and query the different versions of Linked Datasets efficiently. In this demonstration, we introduce OSTRICH, which is an efficient triple store with supported for versioned query evaluation. We demonstrate the capabilities of OSTRICH using a Web-based graphical user interface in which a store can be opened or created. Using this interface, the user is able to query in, between, and over different versions, ingest new versions, and retrieve summarizing statistics.",OSTRICH: Versioned Random-Access Triple Store,NA:NA:NA,2018
Hong-Han Shuai:Yueh-Hsue Li:Chun-Chieh Feng:Wen-Chih Peng,"Nowadays, people get used to buying a variety of commodities from e-commerce platforms because of the convenience and easy access to Internet. As a result, the sales on e-commerce platforms have grown exponentially. It is promising to customize the online shops for different users under VR environments since users do not need to waste time of going upstairs or downstairs for finding interesting commodities. Therefore, in this demo paper, we propose a novel four-dimensional shopping mall that offers online group shopping services with the customized shop recommendation, where a group of friends in the proposed four-dimensional shopping mall can teleport to the next shop by one click. We formulate a Sequential group wIllingness OptimizatioN (SION) problem, prove SION is NP-hard, and provide an efficient algorithm called ζ-GSS. The experimental results show that the solution quality of the proposed ζ-GSS is close to the optimal solution, while the execution time only requires 3.3%. Finally, we build the prototype of the four-dimensional shopping mall, which can be demonstrated for users to experience the next-generation online shopping.",Four-Dimensional Shopping Mall: Sequential Group Willingness Optimization under VR Environments,NA:NA:NA:NA,2018
Benjamin Klotz:Raphaël Troncy:Daniel Wilms:Christian Bonnet,"In this paper, we use semantic technologies for enriching trajectory data in the automotive industry for offline analysis. We proposed to re-use a combination of existing ontologies and we designed a Vehicle Signal Specification ontology to provide an environment in which we developed an application that analyzes the variations of signal values and enables to infer the ""driving smoothness'' that we represent as additional annotations of semantic trajectories.",Generating Semantic Trajectories Using a Car Signal Ontology,NA:NA:NA:NA,2018
Johannes Doleschal:Nico Höllerich:Wim Martens:Frank Neven,"Chisel is a tool for flexible manipulation of CSV-like data, motivated by the recent effort of the World Wide Web Consortium (W3C) towards a recommendation for tabular data and metadata on the Web. In brief, Chisel supports an expressive built-in schema language for CSV-like data, that can handle both tabular and non-tabular data. Furthermore, it supports a simple programming language for transforming tabular and non-tabular CSV-like data. In the demo, we showcase the system for specifying and validating schemas, building transformations, and setting up a pipeline for automatic conversion of ""wild"" CSV-like data into structured tabular data. We present use cases for Chisel specifically targeted at exemplifying the ease of specifying, modifying, and understanding Sculpt schemas as well as extracting and transforming data.",Chisel: Sculpting Tabular and Non-Tabular Data on the Web,NA:NA:NA:NA,2018
Ram G. Athreya:Axel-Cyrille Ngonga Ngomo:Ricardo Usbeck,"In this demo, we introduce the DBpedia chatbot, a knowledge-graph-driven chatbot designed to optimize community interaction. The bot was designed for integration into community software to facilitate the answering of recurrent questions. Four main challenges were addressed when building the chatbot, namely (1) understanding user queries, (2) fetching relevant information based on the queries, (3) tailoring the responses based on the standards of each output platform (i.e. Web, Slack, Facebook) as well as (4) developing subsequent user interactions with the DBpedia chatbot. With this demo, we will showcase our solutions to these four challenges.",Enhancing Community Interactions with Data-Driven Chatbots--The DBpedia Chatbot,NA:NA:NA,2018
Mayank Kejriwal:Daniel Gilley:Pedro Szekely:Jill Crisman,"In this demonstration, we present the Text-enabled Humanitarian Operations in Real-time (THOR) framework, which is being prototyped to provide visual and analytical situational awareness to humanitarian and disaster relief (HADR) planners. THOR is a collaborative effort between industrial and university research laboratories, designed with an intent to support both military and civilian HADR operations. At its core, THOR is powered by a domain-specific knowledge graph, which is derived from natural language outputs and is amenable to real-time analytics. THOR is designed to operate in low-resource linguistic environments, process heterogeneous data, including news and social media, reason about arbitrary disasters not knowable in advance, and provide advanced graphical interaction capabilities. We will demo the latest prototype of THOR using an interactive case study situation.",THOR: Text-enabled Analytics for Humanitarian Operations,NA:NA:NA:NA,2018
Mehdi Bahrami:Junhee Park:Lei Liu:Wei-Peng Chen,"Application Programming Interface (API) exposes data and functions of a software application to third-party users. In digital business, API economy is one of the key component for determining the value of provided services. With the rise in number of publicly available APIs, understanding each API endpoint manually is not only labor intensive but it is also an error prone task for software engineers. Due to the complexity of understanding the sheer number of APIs, it is difficult for software developers to find the best possible API combinations (i.e. API Mashups). In this demonstration, we introduce API Learning platform which employs machine-learning based technologies to efficiently search APIs, validate APIs, and generate API mashups. These technologies enable a machine to automatically generate machine-readable API specification from API documentations, understand variety of APIs, validate extracted information through automated API validation, and finally recommend API mashups for a specific purpose. As of now, API Learning platform collected over 14,000 API documentations and generates a machine readable format for REST APIs with an accuracy of 84%. The proposed demo prototype shows how it enables users to quickly find relevant APIs, automatically verify API availability, and get the best possible API mashup recommendations.",API Learning: Applying Machine Learning to Manage the Rise of API Economy,NA:NA:NA:NA,2018
Kashyap Popat:Subhabrata Mukherjee:Jannik Strötgen:Gerhard Weikum,"Rapid increase of misinformation online has emerged as one of the biggest challenges in this post-truth era. This has given rise to many fact-checking websites that manually assess doubtful claims. However, the speed and scale at which misinformation spreads in online media inherently limits manual verification. Hence, the problem of automatic credibility assessment has attracted great attention. In this work, we present CredEye, a system for automatic credibility assessment. It takes a natural language claim as input from the user and automatically analyzes its credibility by considering relevant articles from the Web. Our system captures joint interaction between language style of articles, their stance towards a claim and the trustworthiness of the sources. In addition, extraction of supporting evidence in the form of enriched snippets makes the verdicts of CredEye transparent and interpretable.",CredEye: A Credibility Lens for Analyzing and Explaining Misinformation,NA:NA:NA:NA,2018
Arseny Kurnikov:Klaudia Krawiecka:Andrew Paverd:Mohammad Mannan:N. Asokan,"Although passwords are by far the most widely-used user authentication mechanism on the web, their security is threatened by password phishing and password database breaches. SafeKeeper is a system for protecting web passwords against very strong adversaries, including sophisticated phishers and compromised servers. Compared to other approaches, one of the key differentiating aspects of SafeKeeper is that it provides web users with verifiable assurance that their passwords are being protected. In this paper, we demonstrate precisely how SafeKeeper can be used to protect web passwords in real-world systems. We first explain two important deployability aspects: i) how SafeKeeper can be integrated into the popular WordPress platform, and ii) how ordinary web users can use Intel SGX remote attestation to verify that SafeKeeper is running on a particular server. We then describe three demonstrations to illustrate the use of SafeKeeper: i) showing the user experience when visiting a legitimate website; ii) showing the encryption of the password in transit via live packet-capture; and iii) showing how SafeKeeper performs in the presence of phishing.",Using SafeKeeper to Protect Web Passwords,NA:NA:NA:NA:NA,2018
Welderufael B. Tesfay:Peter Hofmann:Toru Nakamura:Shinsaku Kiyomoto:Jetzabel Serna,"With the continuing growth of the Internet landscape, users share large amount of personal, sometimes, privacy sensitive data. When doing so, often, users have little or no clear knowledge about what service providers do with the trails of personal data they leave on the Internet. While regulations impose rather strict requirements that service providers should abide by, the defacto approach seems to be communicating data processing practices through privacy policies. However, privacy policies are long and complex for users to read and understand, thus failing their mere objective of informing users about the promised data processing behaviors of service providers. To address this pertinent issue, we propose a machine learning based approach to summarize the rather long privacy policy into short and condensed notes following a risk-based approach and using the European Union (EU) General Data Protection Regulation (GDPR) aspects as assessment criteria. The results are promising and indicate that our tool can summarize lengthy privacy policies in a short period of time, thus supporting users to take informed decisions regarding their information disclosure behaviors.",I Read but Don't Agree: Privacy Policy Benchmarking using Machine Learning and the EU GDPR,NA:NA:NA:NA:NA,2018
Alo Allik:Florian Thalmann:Mark Sandler,"MusicLynx is a web application for music discovery that enables users to explore an artist similarity graph constructed by linking together various open public data sources. It provides a multifaceted browsing platform that strives for an alternative, graph-based representation of artist connections to the grid-like conventions of traditional recommendation systems. Bipartite graph filtering of the Linked Data cloud, content-based music information retrieval, machine learning on crowd-sourced information and Semantic Web technologies are combined to analyze existing and create new categories of music artists through which they are connected. The categories can uncover similarities between artists who otherwise may not be immediately associated: for example, they may share ethnic background or nationality, common musical style or be signed to the same record label, come from the same geographic origin, share a fate or an affliction, or have made similar lifestyle choices. They may also prefer similar musical keys, instrumentation, rhythmic attributes, or even moods their music evokes. This demonstration is primarily meant to showcase the graph-based artist discovery interface of MusicLynx: how artists are connected through various categories, how the different graph filtering methods affect the topology and geometry of linked artists graphs, and ways in which users can connect to external services for additional content and information about objects of their interest.",MusicLynx: Exploring Music Through Artist Similarity Graphs,NA:NA:NA,2018
Yihong Zhang:Panote Siriaraya:Yuanyuan Wang:Shoko Wakamiya:Yukiko Kawai:Adam Jatowt,"For a traveler to enjoy a trip in a city, one important factor is the diversity of sceneries and facilities along the route. Current navigation systems can provide the shortest route between two points, as well as scenic or safe routes. However, diversity is largely ignored in existing works. In this paper, we present a system that provides diversity-based route recommendation. It measures visual-based diversity and facility-based diversity with information extracted from publicly available data such as Google Street View images and FourSquare venues. As we will show, the current prototype system is able to provide diversity-based route recommendation for city areas in San Fransisco and Kyoto.",Walking down a Different Path: Route Recommendation based on Visual and Facility based Diversity,NA:NA:NA:NA:NA:NA,2018
Quyu Kong:Marian-Andrei Rizoiu:Siqi Wu:Lexing Xie,"What makes content go viral Which videos become popular and why others don't Such questions have elicited significant attention from both researchers and industry, particularly in the context of online media. A range of models have been recently proposed to explain and predict popularity; however, there is a short supply of practical tools, accessible for regular users, that leverage these theoretical results. HIPie--an interactive visualization system--is created to fill this gap, by enabling users to reason about the virality and the popularity of online videos. It retrieves the metadata and the past popularity series of Youtube videos, it employs the Hawkes Intensity Process, a state-of-the-art online popularity model for explaining and predicting video popularity, and it presents videos comparatively in a series of interactive plots. This system will help both content consumers and content producers in a range of data-driven inquiries, such as to comparatively analyze videos and channels, to explain and to predict future popularity, to identify viral videos, and to estimate responses to online promotion.",Will This Video Go Viral: Explaining and Predicting the Popularity of Youtube Videos,NA:NA:NA:NA,2018
Michel Buffa:Jerome Lebrun,"The ANR project WASABI will last 42 months and consists in developing a 2 million songs database with interactive WebAudio enhanced client applications. Client applications target composers, music schools, sound engineering schools, musicologists, music streaming services and journalists. In this paper, we present a virtual pedal board (a set of chainable audio effects on the form of ""pedals""), and a guitar tube amplifier simulation for guitarists, that will be associated with songs from the WASABI database. Music schools and music engineering schools are interested in such tools that can be run in a Web page, without the need to install any further software. Take a classic rock song: isolate the guitar solo, study it, then mute it and play guitar real-time along the other tracks using an online guitar amplifier that reproduces the real guitar amp model used in the song, with its signature sound, proper dynamic and frequency response. Add some audio effects such as a reverberation, a delay, a flanger, etc. in order to reproduce Pink Floyd's guitar sound or Eddie Van Halen famous ""Brown Sound"". Learn interactively, guitar in hands, how to fine tune a compressor effect, or how to shape the sound of a tube guitar amp, how to get a ""modern metal"" or a ""Jimi Hendrix"" sound, using only your Web browser.","Real-Time Emulation of a Marshall JCM 800 Guitar Tube Amplifier, Audio FX Pedals, in a Virtual Pedal Board",NA:NA,2018
Giulio Rossetti:Letizia Milli:Salvatore Rinzivillo,"Nowadays the analysis of dynamics of and on networks represents a hot topic in the Social Network Analysis playground. To support students, teachers, developers and researchers we introduced a novel framework, named NDlib, an environment designed to describe diffusion simulations. NDlib is designed to be a multi-level ecosystem that can be fruitfully used by different user segments. Upon NDlib, we designed a simulation server that allows remote execution of experiments as well as an online visualization tool that abstracts its programmatic interface and makes available the simulation platform to non-technicians.",NDlib: A Python Library to Model and Analyze Diffusion Processes over Complex Networks,NA:NA:NA,2018
Angela Bonifati:Wim Martens:Thomas Timm,"In this demonstration, we showcase DARQL, the first tool for deep, large-scale analysis of SPARQL queries. We have harvested a large corpus of query logs with different lineage and sizes, from DBPedia to BioPortal and Wikidata, whose total number of queries amounts to 180M. We ran a wide range of analyses on the corpus, spanning from simple tasks (keyword counts, triple counts, operator distributions), moderately deep tasks (projection test, query classification), and deep analysis (shape analysis, well-designedness, weakly well-designedness, hypertreewidth, and fractional edge cover). The key goal of our demonstration is to let the users dive into the SPARQL query logs of our corpus and let them discover the inherent characteristics of the queries. The entire corpus of SPARQL queries is stored in a DBMS. The tool has a GUI that allows users to ask sophisticated analytical queries on the SPARQL logs. These analytical queries can both be directly written in SQL or composed by a visual query builder tool. The results of the analytical queries are represented both textually (as SPARQL queries) and visually. The DBMS performs the searches within the corpus quite efficiently. To the best of our knowledge, this is the first demonstration of this kind on such a large corpus and with such a number of varied tests.",DARQL: Deep Analysis of SPARQL Queries,NA:NA:NA,2018
Sepideh Mesbah:Alessandro Bozzon:Christoph Lofi:Geert-Jan Houben,"This demo presents SmartPub, a novel web-based platform that supports the exploration and visualization of shallow meta-data (e.g., author list, keywords) and deep meta-data--long tail named entities which are rare, and often relevant only in specific knowledge domain--from scientific publications. The platform collects documents from different sources (e.g. DBLP and Arxiv), and extracts the domain-specific named entities from the text of the publications using Named Entity Recognizers (NERs) which we can train with minimal human supervision even for rare entity types. The platform further enables the interaction with the Crowd for filtering purposes or training data generation, and provides extended visualization and exploration capabilities. SmartPub will be demonstrated using sample collection of scientific publications focusing on the computer science domain and will address the entity types Dataset (i.e. dataset presented or used in a publication), and Methods (i.e. algorithms used to create/enrich/analyse a data set)",SmartPub: A Platform for Long-Tail Entity Extraction from Scientific Publications,NA:NA:NA:NA,2018
Andrea Mauri:Achilleas Psyllidis:Alessandro Bozzon,"Having a thorough understanding of energy consumption behavior is an important element of sustainability studies. Traditional sources of information about energy consumption, such as smart meter devices and surveys, can be costly to deploy, may lack contextual information or have infrequent updates. In this paper, we examine the possibility of extracting energy consumption-related information from user-generated content. More specifically, we develop a pipeline that helps identify energy-related content in Twitter posts and classify it into four categories (dwelling, food, leisure, and mobility), according to the type of activity performed. We further demonstrate a web-based application--called Social Smart Meter--that implements the proposed pipeline and enables different stakeholders to gain an insight into daily energy consumption behavior, as well as showcase it in case studies involving several world cities.",Social Smart Meter: Identifying Energy Consumption Behavior in User-Generated Content,NA:NA:NA,2018
Freya Behrens:Sebastian Bischoff:Pius Ladenburger:Julius Rückin:Laurenz Seidel:Fabian Stolp:Michael Vaichenker:Adrian Ziegler:Davide Mottin:Fatemeh Aghaei:Emmanuel Müller:Martin Preusse:Nikola Müller:Michael Hunger,"We present MetaExp, a system that assists the user during the exploration of large knowledge graphs, given two sets of initial nodes. At its core, MetaExp presents a small set of meta-paths to the user, which are sequences of relationships among node types. Such meta-paths do not overwhelm the user with complex structures, yet they preserve semantically-rich relationships in a graph. MetaExp engages the user in an interactive procedure, which involves simple meta-paths evaluations to infer a user-specific similarity measure. This similarity measure incorporates the domain knowledge and the preferences of the user, overcoming the fundamental limitations of previous methods based on local node neighborhoods or statically determined similarity scores. Our system provides a user-friendly interface for searching initial nodes and guides the user towards progressive refinements of the meta-paths. The system is demonstrated on three datasets, Freebase, a movie database, and a biological network.",MetaExp: Interactive Explanation and Exploration of Large Knowledge Graphs,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Rudolf Schneider:Sebastian Arnold:Tom Oberhauser:Tobias Klatt:Thomas Steffek:Alexander Löser,"We demonstrate Smart-MD, an information retrieval system for medical professionals. The system supports topical queries in the form [disease topic], such as [""lyme disease"", treatments]. In contrast to document-oriented retrieval systems, Smart-MD retrieves relevant paragraphs and reduces the reading load of a medical doctor drastically. We recognize diseases and topical aspects with a novel paragraph retrieval method based on bidirectional LSTM neural networks. We demonstrate Smart-MD on a dataset that contains 3,469 diseases from the English language part of Wikipedia and 6,876 distinct medical aspects extracted from Wikipedia headlines.",Smart-MD: Neural Paragraph Retrieval of Medical Topics,NA:NA:NA:NA:NA:NA,2018
Yanyan Wang:Qun Chen:Xin Liu:Murtadha Ahmed:Zhanhuai Li:Wei Pan:Hailong Liu,"The state-of-the-art techniques for aspect-level sentiment analysis focus on feature modeling using a variety of deep neural networks (DNN). Unfortunately, their practical performance may fall short of expectations due to semantic complexity of natural languages. Motivated by the observation that linguistic hints (e.g. explicit sentiment words and shift words) can be strong indicators of sentiment, we present a joint framework, SenHint, which integrates the output of deep neural networks and the implication of linguistic hints into a coherent reasoning model based on Markov Logic Network (MLN). In SenHint, linguistic hints are used in two ways: (1) to identify easy instances, whose sentiment can be automatically determined by machine with high accuracy; (2) to capture implicit relations between aspect polarities. We also empirically evaluate the performance of SenHint on both English and Chinese benchmark datasets. Our experimental results show that SenHint can effectively improve accuracy compared with the state-of-the-art alternatives.",SenHint: A Joint Framework for Aspect-level Sentiment Analysis by Deep Neural Networks and Linguistic Hints,NA:NA:NA:NA:NA:NA:NA,2018
Michele Ruta:Floriano Scioscia:Giuseppe Loseto:Filippo Gramegna:Saverio Ieva:Agnese Pinto:Eugenio Di Sciascio,"ThePhysical Semantic Web (PSW) is a novel paradigm built upon the Google Physical Web (PW) approach and devoted to improve the quality of interactions in the Web of Things. Beacons expose semantic annotations instead of basic identifiers, ıe\ machine-understandable descriptions of physical resources. This enables novel ontology-based object advertisement and discovery and --in turn-- advanced user-to-thing and autonomous thing-to-thing interactions. The demo shows the evolution from the PW to the PSW in a discovery scenario set in a winery, where bottles are equipped with Bluetooth Low Energy beacons and a customer can discover them using her smartphone. The final goal is to prove benefits of PSW over basic PW, including: rich semantic-based object annotation; dynamic annotations exploiting on-board sensors; enhanced discovery and ranking of nearby objects through semantic matchmaking; availability of interactions even without working Internet infrastructure, by means of point-to-point data exchanges.",A journey from the Physical Web to the Physical Semantic Web,NA:NA:NA:NA:NA:NA:NA,2018
Mohamed Abdel Maksoud:Gaurav Pandey:Shuaiqiang Wang,"This paper addresses a novel tour discovery problem in the domain of travel search. We create a ranking of tours for a set of travel interests, where a tour is a group of city documents and a travel interest is a query. While generating and ranking tours, it is aimed that each interest (from the interest set) is satisfied by at least one city in a tour and the distance traveled to cover the tour is not too large. Firstly, we generate tours for the interest set, by utilizing the available ranking of cities for the individual interests and the distances between the cities. Then, in absence of existing methods directly related to our problem, we devise our novel techniques to calculate ranking scores for the tours and present a comparison of these techniques in our results. We demonstrate our web application Travición, that utilizes the best tour scoring technique.",Finding Tours for a Set of Interests,NA:NA:NA,2018
Mirko Marras:Matteo Manca:Ludovico Boratto:Gianni Fenu:David Laniado,"The massive amount and variety of city-related data raise equally big challenges to enable citizens to make sense of such data for improving their daily life and fostering collective decision making. The existing dashboards include limited pre-defined use cases which can only address the most common needs of citizens, but do not allow for personalization. In this work, we propose an open source environment that enables citizens to easily explore city-related data. A backend continuously collects heterogeneous data, turns them into a unified format and exposes them through an API; a front-end allows users to create interactive visualizations combining different data sources and visual models, and to share them to engage others. In this way, citizens can build up a data-driven public awareness supporting an open, transparent, and collaborative city.",BarcelonaNow: Empowering Citizens with Interactive Dashboards for Urban Data Exploration,NA:NA:NA:NA:NA,2018
Giorgos Argyriou:George Papadakis:George Stamoulis:Efi Karra Taniskidou:Nikiforos Pittaras:George Giannakopoulos:Sergio Albani:Michele Lazzarini:Emanuele Angiuli:Anca Popescu:Argyros Argyridis:Manolis Koubarakis,"GeoSensor is a novel system that enriches change detection over satellite images with event detection over news items and social media content. GeoSensor faces the major challenges of Big Data: volume (a single satellite image may be a few GBs), variety (its data sources include two different types of satellite images and various types of user-generated content) and veracity, as the accuracy of the end result is crucial for the usefulness of our system. To overcome these three challenges, while offering on-line functionality, GeoSensor comprises a complex architecture that is based on the open-source platform developed in the H2020 project Big Data Europe. Through the presented demonstration, both the effectiveness and the efficiency of GeoSensor's functionalities are highlighted.",GeoSensor: On-line Scalable Change and Event Detection over Big Data,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Weijian Zhang:Jonathan Deakin:Nicholas J. Higham:Shuaiqiang Wang,"We present Etymo (https://etymo.io), a discovery engine to facilitate artificial intelligence (AI) research and development. It aims to help readers navigate a large number of AI-related papers published every week by using a novel form of search that finds relevant papers and displays related papers in a graphical interface. Etymo constructs and maintains an adaptive similarity-based network of research papers as an all-purpose knowledge graph for ranking, recommendation, and visualisation. The network is constantly evolving and can learn from user feedback to adjust itself. A screencast is available at: https://youtu.be/T4FDPk_TmN0",Etymo: A New Discovery Engine for AI Research,NA:NA:NA:NA,2018
Tanya Chowdhury:Aashay Mittal:Tanmoy Chakraborty,"In this demo, we present VIZ-Wiki, a browser extension which generates an overview of summarizable threads in Question Answering forums. It reduces a user's effort to go through lengthy text-based, sarcastic and highly critiqued answers. Our tool can be used to collect community opinion from popular discussion sites like Quora, Yahoo! Answers, Reddit etc. as well as topic-centric ones such as Askubuntu, Stackoverflow. We rely on textual information of these forums to extract insightful summaries for a reader. VIZ-Wiki provides users a pie-graph view marking popular choices when such a question link is raised. A button guides them to detailed statistics and relevant list of answers. It further highlights sentences relevant to an answer choice in the text. VIZ-Wiki deals with answers contradicted by other users, prioritizes highly-recommended ones and avoids sarcasm. We test our model on the factoid questions dataset of Yahoo! Answers and obtain a macro precision of 0.6 on displayed answers and a macro recall of 0.69, beating the baseline significantly. To the best of our knowledge, VIZ-Wiki is the first attempt to visualize answers for questions in community question answering services. In the spirit of reproducibility, we have released the code and a demonstration video public at \urlhttp://goo.gl/cyx3EF and \urlhttps://youtu.be/XNmRa_jtmC8 respectively",VIZ-Wiki: Generating Visual Summaries to Factoid Threads in Community Question Answering Services,NA:NA:NA,2018
Benjamin D. Horne:William Dron:Sara Khedr:Sibel Adali,"Today, journalist, information analyst, and everyday news consumers are tasked with discerning and fact-checking the news. This task has became complex due to the ever-growing number of news sources and the mixed tactics of maliciously false sources. To mitigate these problems, we introduce the The News Landscape (NELA) Toolkit: an open source toolkit for the systematic exploration of the news landscape. NELA allows users to explore the credibility of news articles using well-studied content-based markers of reliability and bias, as well as, filter and sort through article predictions based on the user's own needs. In addition, NELA allows users to visualize the media landscape at different time slices using a variety of features computed at the source level. NELA is built with a modular, pipeline design, to allow researchers to add new tools to the toolkit with ease. Our demo is an early transition of automated news credibility research to assist human fact-checking efforts and increase the understanding of the news ecosystem as a whole.",Assessing the News Landscape: A Multi-Module Toolkit for Evaluating the Credibility of News,NA:NA:NA:NA,2018
Debabrata Mahapatra:Ragunathan Mariappan:Vaibhav Rajan:Kuldeep Yadav:Seby A.:Sudeshna Roy,"The number of high quality online videos is increasing rapidly. Online courses as well as universities do not fully leverage the content due to several open challenges in video search, indexing, summarization and customization requirements for specific courses, instructors or learners. We present a new web-based social learning platform called Videoken. Using novel video summarization algorithms, Videoken automatically creates Table of Contents for videos. This allows a textbook-like facility for non-linear search and navigation through the video, enables extraction of semantically coherent clips from within a video and improves video search through better semantic indexing. The platform also allows new ways of course creation and sharing of learning modules; and can be both integrated with existing Learning Management Systems and used independently.",VideoKen: Automatic Video Summarization and Course Curation to Support Learning,NA:NA:NA:NA:NA:NA,2018
Harshita Jhavar:Paramita Mirza,"We present EMOFIEL, a system that identifies characters and scenes in a story from a fictional narrative summary, generates appropriate scene descriptions, identifies the emotion flow between a given directed pair of story characters in each interaction, and organizes them along the story timeline. These emotions are identified using two emotion modelling approaches: categorical and dimensional emotion models. The generated plots show that in a particular scene, two characters can share multiple emotions together with different intensity. Furthermore, the directionality of the emotion can be captured as well, depending on which character is more dominant in each interaction. EMOFIEL provides a web-based GUI that allows users to query the annotated stories to explore the emotion mapping of a given character pair throughout a given story, and to explore scenes for which a certain emotion peaks.",EMOFIEL: Mapping Emotions of Relationships in a Story,NA:NA,2018
Miltiadis Lytras:Naif Radi Aljohani:Amir Hussain:Jiebo Luo:Jacky Xi Zhang,"Cognitive Computing has received increasing attention from academia and industries as it brings cognitive science and computing together for the development of new computational platforms, infrastructures, systems and algorithms. Artificial intelligence and computational intelligence are key elements to succeed in cognitive computing. The Cognitive Computing track was successfully organized at WWW2017. This year, WWW2018 continues this track with a focus on all applications. Particularly, applications in healthcare, environment, education, sustainability, smart cities and food science are among the important global challenges in 21st Century.",Cognitive Computing Track Chairs' Welcome & Organization,NA:NA:NA:NA:NA,2018
Iqra Safder:Saeed-Ul Hassan:Naif Radi Aljohani,"Although, over the years, information retrieval systems have shown tremendous improvements in searching for relevant scientific literature, human cognition is still required to search for specific document elements in full text publications. For instance, pseudocodes pertaining to algorithms published in scientific publications cannot be correctly matched against user queries, hence the process requires human involvement. AlgorithmSeer, a state-of-the-art technique, claims to replace humans in this task, but one of the limitations of such an algorithm search engine is that the metadata is simply a textual description of each pseudocode, without any algorithm-specific information. Hence, the search is performed merely by matching the user query to the textual metadata and ranking the results using conventional textual similarity techniques. The ability to automatically identify algorithm-specific metadata such as precision, recall, or f-measure would be useful when searching for algorithms. In this article, we propose a set of algorithms to extract further information pertaining to the performance of each algorithm. Specifically, sentences in an article that convey information about the efficiency of the corresponding algorithm are identified and extracted using a recurrent convolutional neural network (RCNN). Furthermore, we propose improving the efficacy of the pseudocode detection task by using a multi-layer perceptron (MLP) classification trained with 15 features, which improves the classification performance of the state-of-the-art pseudocode detection methods used in AlgorithmSeer by 27%. Finally, we show the advantages of the AI-enabled search engine (based on RCNN and MLP models) over conventional text-retrieval models.","AI Cognition in Searching for Relevant Knowledge from Scholarly Big Data, Using a Multi-layer Perceptron and Recurrent Convolutional Neural Network Model",NA:NA:NA,2018
Ali Daud:Akbar Hussain:Rabeeh Ayaz Abbasi:Naif Radi Aljohani:Tehmina Amjad:Hassan Dawood,"Players are ranked in various sports to show their importance over other players. Existing methods only consider intra-type links (e.g., player to player and team to team), but ignore inter-type links (e.g., one type of player to another type of player, such as batsman to bowler and player to team) based on cognitive aspects. They also ignore the spatiality of the players. There is a strong relationship among players and their teams, which can be represented as a network consisting of multi-type interrelated objects. In this paper, we propose a players' ranking method, called Region-wise Players Link Fusion (RPLF) which is applied to the sport of cricket. RPLF considers players' region-wise intra-type and inter-type relation-based features to rank the players. Considering multi-type interrelated objects is based on the intuition that a batsman scoring high against top bowlers of a strong team or a bowler taking wickets against top batsmen of a strong team is considered as a good player. The experimental results show that RPLF provides promising insights of players' rankings. RLFP is a generic method and can be applied to different sports for ranking players.",Region-wise Ranking of Sports Players based on Link Fusion,NA:NA:NA:NA:NA:NA,2018
Debabrata Mahapatra:Ragunathan Mariappan:Vaibhav Rajan,"The number of freely available online educational videos from universities and other organizations is growing rapidly. Accurate indexing and summarization are essential for efficient search, recommendation and effective consumption of videos. In this paper, we describe a new method of automatically creating a hierarchical table of contents for a video. It provides a summary of the video content along with a textbook--like facility for nonlinear navigation and search through the video. Our multimodal approach combines new methods for shot level video segmentation and for hierarchical summarization. Empirical results demonstrate the efficacy of our approach on many educational videos.",Automatic Hierarchical Table of Contents Generation for Educational Videos,NA:NA:NA,2018
Kwan Hui Lim:Kate E. Lee:Dave Kendal:Lida Rashidi:Elham Naghizade:Stephan Winter:Maria Vasardani,"Green spaces are believed to improve the well-being of users in urban areas. While there are urban research exploring the emotional benefits of green spaces, these works are based on user surveys and case studies, which are typically small in scale, intrusive, time-intensive and costly. In contrast to earlier works, we utilize a non-intrusive methodology to understand green space effects at large-scale and in greater detail, via digital traces left by Twitter users. Using this methodology, we perform an empirical study on the effects of green spaces on user sentiments and emotions in Melbourne, Australia and our main findings are: (i) tweets in green spaces evoke more positive and less negative emotions, compared to those in urban areas; (ii) each season affects various emotion types differently; (iii) there are interesting changes in sentiments based on the hour, day and month that a tweet was posted; and (iv) negative sentiments are typically associated with large transport infrastructures such as train interchanges, major road junctions and railway tracks. The novelty of our study is the combination of psychological theory, alongside data collection and analysis techniques on a large-scale Twitter dataset, which overcomes the limitations of traditional methods in urban research.",The Grass is Greener on the Other Side: Understanding the Effects of Green Spaces on Twitter User Sentiments,NA:NA:NA:NA:NA:NA:NA,2018
Lin Mu:Peiquan Jin:Lizhou Zheng:En-Hong Chen:Lihua Yue,"Microblog like Twitter and Sina Weibo has been an important information source for event detection and monitoring. In many decision-making scenarios, it is not enough to only provide a structural tuple for an event, e.g., a 5W1H record like <who, where, when, what, whom, how>. However, in addition to event structural tuples, people need to know the evolution lifecycle of an event. The lifecycle description of an event is more helpful for decision making because people can focus on the progress and trend of events. In this paper, we propose a novel method for efficiently detecting and tracking event evolution on microblogging platforms. The major features of our study are: (1) It provides a novel event-type-driven method to extract event tuples, which forms the foundation for event evolution analysis. (2) It describes the lifecycle of an event by a staged model, and provides effective algorithms for detecting the stages of an event. (3) It offers emotional analysis over the stages of an event, through which people are able to know the public emotional tendency over a specific event at different time periods. We build a prototype system and present its architecture and implemental details in the paper. In addition, we conduct experiments on real microblog datasets and the results in terms of precision, recall, and F-measure suggest the effectiveness and efficiency of our proposal.",Lifecycle-Based Event Detection from Microblogs,NA:NA:NA:NA:NA,2018
Jiongqian Liang:Peter Jacobs:Srinivasan Parthasarathy,"Hurricane-induced flooding can lead to substantial loss of life and huge damage to infrastructure. Mapping flood extent from satellite or aerial imagery is essential for prioritizing relief efforts and for assessing future flood risk. Identification of water extent in such images can be challenging considering the heterogeneity in water body size and shape, cloud cover, and natural variations in land cover. In this effort, we introduce a novel cognitive framework based on a semi-supervised learning algorithm, called HUman-Guided Flood Mapping (HUG-FM), specifically designed to tackle the flood mapping problem. Our framework first divides the satellite or aerial image into patches leveraging a graph-based clustering approach. A domain expert is then asked to provide labels for a few patches (as opposed to pixels which are harder to discern). Subsequently, we learn a classifier based on the provided labels to map flood extent. We test the efficacy and efficiency of our framework on imagery from several recent flood-induced emergencies and results show that our algorithm can robustly and correctly detect water areas compared to the state-of-the-art. We then evaluate whether expert guidance can be replaced by the wisdom of a crowd (e.g., crisis volunteers). We design an online crowdsourcing platform based on HUG-FM and propose a novel ensemble method to leverage crowdsourcing efforts. We conduct an experiment with over $50$ participants and show that crowdsourced HUG-FM (CHUG-FM) can approach or even exceed the performance of a single expert providing guidance (HUG-FM).",Human-Guided Flood Mapping: From Experts to the Crowd,NA:NA:NA,2018
Patrick Watson:TengFei Ma:Ravi Tejwani:Maria Chang:JaeWook Ahn:Sharad Sundararajan,"The availability of open educational resources (OER) has enabled educators and researchers to access a variety of learning assessments online. OER communities are particularly useful for gathering multiple choice questions (MCQs), which are easy to grade, but difficult to design well. To account for this, OERs often rely on crowd-sourced data to validate the quality of MCQs. However, because crowds contain many non-experts, and are susceptible to question framing effects, they may produce ratings driven by guessing on the basis of surface-level linguistic features, rather than deep topic knowledge. Consumers of OER multiple choice questions (and authors of original multiple choice questions) would benefit from a tool that automatically provided feedback on assessment quality, and assessed the degree to which OER MCQs are susceptible to framing effects. This paper describes a model that is trained to use domain-naive strategies to guess which multiple choice answer is correct. The extent to which this model can predict the correct answer to an MCQ is an indicator that the MCQ is a poor measure of domain-specific knowledge. We describe an integration of this model with a front-end visualizer and MCQ authoring tool.",Human-level Multiple Choice Question Guessing Without Domain Knowledge: Machine-Learning of Framing Effects,NA:NA:NA:NA:NA:NA,2018
Rui Yan:Dongyan Zhao,"To establish an automatic conversation system between human and computer is regarded as one of the most hardcore problems in computer science. It requires interdisciplinary techniques of information retrieval, natural language processing, data management as well as artificial intelligence. The arrival of big data era reveals the feasibility to create a conversation system empowered by data-driven approaches. Now we are able to collect extremely large conversational data on Web, and organize them to launch a human-computer conversation system. Owing to the diversity of Web resources available, a retrieval-based conversation system will be able to find at least some responses from the massive data repository for any user inputs. Given a human issued utterance, i.e., a query, a retrieval-based conversation system will search for appropriate replies, conduct a relevance ranking, and then output the highly relevant one as the response. In this paper, we propose a novel retrieval model named NeuRetrieval for short text understanding, representation and semantic matching. The proposed model is general and unified for both single-turn and multi-turn conversation scenarios in open domain. In the experiments, we investigate the effectiveness of the proposed deep neural network model for human-computer conversations. We demonstrate performance improvement against a series of baseline methods in several evaluation metrics. In contrast with previously proposed methods, NeuRetrieval is tailored for conversation scenarios and demonstrated to be more effective.",A NeuRetrieval Model for Human-Computer Conversations,NA:NA,2018
Yihang Cheng:Xi Zhang:Hao Wang:Shang Jiang,"Community Question Answering (CQA) has emerged recently and it becomes popular among people. During the process of the communication, different knowledge can be merged. Recently, several vendors use the business model of paying for knowledge to make these knowledge to the monetary benefits, then the Pay-for-Knowledge Communities (PKC) have been applied. Even PKC has interesting business model, there are several problems to be solved, and one of the most salient problem is that questioners may takes too long time to choose the most valuable answers, leading to questioners not able to pay for suitable answerers and many problems about platforms' operation There are several previous research has focused on this problem but still have not found satisfactory solutions as questions and answers are more and more complex in PKC platforms. With the development of cognitive computing techniques, applying an intelligent QA system in PKC to improve the answering effectiveness may be possible. In this paper, we tried to investigate how to apply the intelligent QA system into PKC platform to improve the answering effectiveness. For solving the problems of matching complex questions and answers, we present a Four Module QA Model based on the normal intelligent QA System. Compared to normal intelligent QA System, our model uses categories to classify the questions with traditional machine learning methods. We use answers in each category of corresponding questions as one dataset, answers in each entity of corresponding question as the other dataset, finally, these two datasets make up the document database. Then we got the best answer among past answers through comparing the TF-IDF weighted bag-of word vectors of two datasets or the new answer including key words through Long Short-Term Memory (LSTM) algorithm with PKC's features composed of centrality and money. Experiments were developed on a dataset with 1222 users' QA sites collected from a QA community. The model we proposed is expected to increase QA's effectiveness and improve the business model of Pay-for-Knowledge Communities.",How to Improve the Answering Effectiveness in Pay-for-Knowledge Community: An Exploratory Application of Intelligent QA System,NA:NA:NA:NA,2018
Sylvio Barbon Junior:Gabriel Marques Tavares:Victor G. Turrisi da Costa:Paolo Ceravolo:Ernesto Damiani,"One of the main challenges of Cognitive Computing (CC) is reacting to evolving environments in near-real time. Therefore, it is expected that CC models provide solutions by examining a summary of past history, rather than using full historical data. This strategy has significant benefits in terms of response time and space complexity but poses new challenges in term of concept-drift detection, where both long term and short terms dynamics should be taken into account. In this paper, we introduce the Concept-Drift in Event Stream Framework (CDESF) that addresses some of these challenges for data streams recording the execution of a Web-based business process. Thanks to CDESF support for feature transformation, we perform density clustering in the transformed feature space of the process event stream, observe track concept-drift over time and identify anomalous cases in the form of outliers. We validate our approach using logs of an e-healthcare process.",A Framework for Human-in-the-loop Monitoring of Concept-drift Detection in Event Log Stream,NA:NA:NA:NA:NA,2018
Peijun Zhao:Jia Jia:Yongsheng An:Jie Liang:Lexing Xie:Jiebo Luo,"Emojis can be regarded as a language for graphical expression of emotions, and have been widely used in social media. They can express more delicate feelings beyond textual information and improve the effectiveness of computer-mediated communication. Recent advances in machine learning make it possible to automatic compose text messages with emojis. However, the usages of emojis can be complicated and subtle so that analyzing and predicting emojis is a challenging problem. In this paper, we first construct a benchmark dataset of emojis with tweets and systematically investigate emoji usages in terms of tweet content, tweet structure and user demographics. Inspired by the investigation results, we further propose a multitask multimodality gated recurrent unit (mmGRU) model to predict the categories and positions of emojis. The model leverages not only multimodality information such as text, image and user demographics, but also the strong correlations between emoji categories and their positions. Our experimental results show that the proposed method can significantly improve the accuracy for predicting emojis for tweets (+9.0% in F1-value for category and +4.6% in F1-value for position). Based on the experimental results, we further conduct a series of case studies to unveil how emojis are used in social media.",Analyzing and Predicting Emoji Usages in Social Media,NA:NA:NA:NA:NA:NA,2018
Vitobha Munigala:Abhijit Mishra:Srikanth G. Tamilselvam:Shreya Khare:Riddhiman Dasgupta:Anush Sankaran,"Persuasiveness is a creative art which aims at inducing certain set of beliefs in the target audience. In an e-commerce setting, for a newly launched product, persuasive descriptions are often composed to motivate an online buyer towards a successful purchase. Such descriptions can be catchy taglines, product-summaries, style-tipsetc.. In this paper, we present PersuAIDE! - a persuasive system based on linguistic creativity to generate various forms of persuasive sentences from the input product specification. To demonstrate the effectiveness of the proposed system, we have applied the technology to fashion domain, where, for a given fashion product like""red collar shirt"" we were able to generate descriptive sentences that not only explain the item but also garner positive attention, making it persuasive. PersuAIDE! identifies fashion related keywords from input specifications and intelligently expands the keywords to creative phrases. Once such compatible phrases are obtained, persuasive descriptions are synthesized from the set of phrases and input keywords with the help of a neural language model trained on a large domain-specific fashion corpus. We evaluate the system on a large fashion corpus collected from different sources using (a) automatic text generation metrics used for Machine Translation and Automatic Summarization evaluation and Readability measurement, and (b) human judgment scores evaluating the persuasiveness and fluency of the generated text. Experimental results and qualitative analysis show that an unsupervised system like ours can produce more creative and better constructed persuasive output than supervised generative counterparts based on neural sequence-to-sequence models and statistical machine translation.",PersuAIDE ! An Adaptive Persuasive Text Generation System for Fashion Domain,NA:NA:NA:NA:NA:NA,2018
Tianlang Chen:Yuxiao Chen:Han Guo:Jiebo Luo,"WeChat Business, developed on WeChat, the most extensively used instant messaging platform in China, is a new business model that bursts into people's lives in the e-commerce era. As one of the most typical WeChat Business behaviors, WeChat users can advertise products, advocate companies and share customer feedback to their WeChat friends by posting a WeChat Moment--a public status that contains images and a text. Given its popularity and significance, in this paper, we propose a novel Bilateral-Attention LSTM network (BiATT-LSTM) to identify WeChat Business Moments based on their texts and images. In particular, different from previous schemes that equally consider visual and textual modalities for a joint visual-textual classification task, we start our work with a text classification task based on an LSTM network, then we incorporate a bilateral-attention mechanism that can automatically learn two kinds of explicit attention weights for each word, namely 1) a global weight that is insensitive to the images in the same Moment with the word, and 2) a local weight that is sensitive to the images in the same Moment. In this process, we utilize visual information as a guidance to figure out the local weight of a word in a specific Moment. Two-level experiments demonstrate the effectiveness of our framework. It outperforms other schemes that jointly model visual and textual modalities. We also visualize the bilateral-attention mechanism to illustrate how this mechanism helps joint visual-textual classification.",When E-commerce Meets Social Media: Identifying Business on WeChat Moment Using Bilateral-Attention LSTM,NA:NA:NA:NA,2018
Hogun Park:Hamid Reza Motahari Nezhad,"A lot of knowledge about procedures and how-tos are described in text. Recently, extracting semantic relations from the procedural text has been actively explored. Prior work mostly has focused on finding relationships among verb-noun pairs or clustering of extracted pairs. In this paper, we investigate the problem of learning individual procedure-specific relationships (e.g. is method of, is alternative of, or is subtask of) among sentences. To identify the relationships, we propose an end-to-end neural network architecture, which can selectively learn important procedure-specific relationships. Using this approach, we could construct a how-to knowledge base from the largest procedure sharing-community, wiki-how.com. The evaluation of our approach shows that it outperforms the existing entity relationship extraction algorithms.",Learning Procedures from Text: Codifying How-to Procedures in Deep Neural Networks,NA:NA,2018
Zara Mansoor:Mustansar Ali Ghazanfar:Syed Muhammad Anwar:Ahmed S. Alfakeeh:Khaled H. Alyoubi,"This research article focuses on the analysis of electroencephalography (EEG) signals of the brain during pain perception. The proposed system is based on the hypothesis that a noticeable change occurs in mental conditions while experiencing pain. When the human body is injured, sensory receptors in the brain enter a stimulated state. The injury may be the result of attention or an accident. Pain warnings are natural in humans and protect the body from further negative effects. In this article, an innovative and robust system based on prominent features extracted from the brain activity recorded using EEG, is proposed to predict the state of pain perception. The brain signals of subjects are observed using two low-cost EEG headsets including neurosky mindwave mobile and emotiv insight. Time and frequency domain features are selected to represent the observed signals. The results show that a combination of time and frequency domain features is the most informative approach for pain prediction using the observed brain activity.",Pain Prediction in Humans using Human Brain Activity Data,NA:NA:NA:NA:NA,2018
Yunhao Zheng:Xi Zhang:Yuting Xiao,"Recommending proper experts to knowledge buyers is a significant problem in online paid Q&A community (OPQC). Existing approaches for online expert recommendation have been mainly focused on exploiting semantic similarities and social network influence, while personalizing recommendation according to individuals' motivations has not received much attention. In this paper, we propose a personalized expert recommender system, which integrates buyer's motivation for knowledge, social influence, and money in a unified framework. As an innovative application of cognitive computing, our recommender system is capable of providing users with the best matching experts so as to help them make the most cost-effective choice in OPQC. To this end, Paragraph Vector technique is implemented to construct domain knowledge base (KB) in a multilayer information retrieval (IR) framework. Then we perform knowledge pricing based on buyer's query and bid in the context of bilateral monopoly knowledge market. After that, a Markov Chain based method with user motivation learning is introduced to find the best matching experts. Finally, we evaluate the proposed approach using datasets collected from two OPQC. The experimental results show encouraging success as effectively offering reasonable personalization options. As an innovative approach to solve the expert matching problem in OPQC, this research provides flexibility in customizing the recommendation heuristics based on user motivation, and demonstrate its contribution to a higher rate of optimal knowledge seller-buyer matching.",Making the Most Cost-effective Decision in Online Paid Q&A Community: An Expert Recommender System with Motivation Modeling and Knowledge Pricing,NA:NA:NA,2018
Tehmina Amjad:Ali Daud:Min Song,"With the increase in collaboration among researchers of various disciplines, changing the research topic or working on multiple topics is not an unusual behavior. Several comprehensive efforts have been made for predicting, quantifying, and studying the researcher's impact. The question, that how the change in the field of interest over time or working in more than one topics can influence the scientific impact, remains unanswered. In this research, we study the effect of topic drift on the scientific impact of an author. We apply Author Conference Topic (ACT) model to extract topic distribution of individual authors who are working on multiple topics to compare and analyze with authors who work on a single topic. We analyze the productivity of the authors on the basis of publication count, citation count and h-index. We find that authors who stick to one topic, produce a higher impact and gain more attention. To further strengthen our results we gather the h-index of top-ranked authors working on one topic and top-ranked authors working on multiple topics and examine whether there are similar trends in their progress. The results show an evidence of significant impact of topic drift on career choices of researchers.",Measuring the Impact of Topic Drift in Scholarly Networks,NA:NA:NA,2018
Aditya Mogadala:Bhargav Kanuparthi:Achim Rettinger:York Sure-Vetter,"Growth of multimodal content on the web and social media has generated abundant weakly aligned image-sentence pairs. However, it is hard to interpret them directly due to intrinsic intension. In this paper, we aim to annotate such image-sentence pairs with connotations as labels to capture the intrinsic intension. We achieve it with a connotation multimodal embedding model (CMEM) using a novel loss function. It's unique characteristics over previous models include: (i) the exploitation of multimodal data as opposed to only visual information, (ii) robustness to outlier labels in a multi-label scenario and (iii) works effectively with large-scale weakly supervised data. With extensive quantitative evaluation, we exhibit the effectiveness of CMEM for detection of multiple labels over other state-of-the-art approaches. Also, we show that in addition to annotation of image-sentence pairs with connotation labels, byproduct of our model inherently supports cross-modal retrieval i.e. image query - sentence retrieval.",Discovering Connotations as Labels for Weakly Supervised Image-Sentence Data,NA:NA:NA:NA,2018
Zhiyuan He:Su Yang:Weishan Zhang:Jiulong Zhang,"Different urban regions usually have different commercial hotness due to the different social contexts inside. As satellite imagery promises high-resolution, low-cost, real-time, and ubiquitous data acquisition, this study aims to solve commercial hotness prediction as well as the correlated social contexts mining problem via visual pattern analysis on satellite images. The goal is to reveal the underlying law correlating visual patterns of satellite images with commercial hotness so as to infer the commercial hotness map of a whole city for government regulation and business planning. We propose a novel deep learning-based model, which learns semantic information from raw satellite images to enable predicting regional commercial hotness. First, we collect satellite images from Google Map and label such images with POI categories according to the annotations from OpenStreetMap. Then, we train a model of deep convolutional networks that leverage raw images to infer the social attributes of the region of interest. Finally, we use three classical regression methods to predict regional commercial hotness from the corresponding social contexts reflected in satellite images in Shanghai, where the applied deep features are learned from the examples of Beijing to guarantee the generality. The result shows that the proposed model is robust enough to reach 82% precision at average. To the best of our knowledge, it is the first work focused on discovering relations between commercial hotness and satellite images. A web service is developed to demonstrate how business planning can be done in reference to the predicted commercial hotness of a given region.",Perceiving Commerial Activeness Over Satellite Images,NA:NA:NA:NA,2018
Wen Hua Lin:Kuan-Ting Chen:Hung Yueh Chiang:Winston Hsu,"Recently, deep neural network models have achieved promising results in image captioning task. Yet, ""vanilla'' sentences, only describing shallow appearances (e.g., types, colors), generated by current works are not satisfied netizen style resulting in lacking engagements, contexts, and user intentions. To tackle this problem, we propose Netizen Style Commenting (NSC), to automatically generate characteristic comments to a user-contributed fashion photo. We are devoted to modulating the comments in a vivid ""netizen'' style which reflects the culture in a designated social community and hopes to facilitate more engagement with users. In this work, we design a novel framework that consists of three major components: (1) We construct a large-scale clothing dataset named NetiLook, which contains 300K posts (photos) with 5M comments to discover netizen-style comments. (2) We propose three unique measures to estimate the diversity of comments. (3) We bring diversity by marrying topic models with neural networks to make up the insufficiency of conventional image captioning works. Experimenting over Flickr30k and our NetiLook datasets, we demonstrate our proposed approaches benefit fashion photo commenting and improve image captioning tasks both in accuracy and diversity.",Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures,NA:NA:NA:NA,2018
Mu-Yen Chen:Tien-Chi Huang:Yu Shu:Chia-Chen Chen:Tsung-Che Hsieh:Neil Y. Yen,"This study retains the meanings of the original text using Autoencoder (AE) in this regard. This study uses the different loss (includes three types) to train the neural network model, hopes that after compressing sentence features, it can still decompress the original input sentences and classify the correct targets, such as positive or negative sentiment. In this way, it supposed to get the more relative features (compressing sentence features) in the sentences to classify the targets, rather than using the classification loss that may classify by the meaningless features (words). In the result, this study discovers that adding additional features for correction of errors does not interfere with the learning. Also, not all words are needed to be restored without distortion after applying the AE method.",Learning the Chinese Sentence Representation with LSTM Autoencoder,NA:NA:NA:NA:NA:NA,2018
Shreya Ghosh:Soumya K. Ghosh:Rahul Deb Das:Stephan Winter,"Several studies have shown that the spatio-temporal mobility traces of human movements can be used to identify an individual. However, this work presents a novel framework for activity-based mobility profiling of individuals using only the temporal information. The proposed framework is conducive to model individuals' activity patterns in temporal scale, and quantifies the uniqueness measures based on certain temporal features of the activity sequence.",Activity-based Mobility Profiling: A Purely Temporal Modeling Approach,NA:NA:NA:NA,2018
Muhammad Usman Ilyas:Jalal Suliman Alowibdi,"Several prior studies have demonstrated the possibility of tracking the outbreak and spread of diseases using public tweets and other social media platforms. However, almost all such prior studies were restricted to geographically filtered English language tweets only. This study is the first to attempt a similar approach for Arabic language tweets originating from the Gulf Cooperation Council (GCC) countries. We obtained a list of commonly occurring diseases in the region from the Saudi Ministry of Health. We used both the English disease names as well as their Arabic translations to filter the stream of tweets. We acquired old tweets for a period spanning 29 months. All tweets were geographically filtered for the Middle East and the list of disease names in both English and Arabic languages. We observed that only a small fraction of tweets were in English, demonstrating that prior approaches to disease tracking relying on English language features are less effective for this region. We also demonstrate how Arabic language tweets can be used rather effectively to track the spread of some infectious diseases in the region. We verified our approach by demonstrating that a high degree of correlation between the occurrence of MERS-Coronavirus cases and Arabic language tweets on the disease. We also show that infectious diseases generating fewer tweets and non-infectious diseases do not exhibit the same high correlation. We also verify the usefulness of tracking cases using Twitter mentions by comparing against a ground truth data set of MERS-CoV cases obtained from the Saudi Ministry of Health.",Disease Tracking in GCC Region Using Arabic Language Tweets,NA:NA,2018
Manuel Tomas Carrasco Benitez:Pascal Hitzler:Irwin King:Moussa Lo:Erik Mannens:Daniel Schwabe,NA,Session details: International Project,NA:NA:NA:NA:NA:NA,2018
Martin Serrano:Amelie Gyrard:Elias Tragos:Hung Nguyen,"FIESTA-IoT project provides a blueprint experimental infrastructure, software tools, semantic techniques, certification processes and best practices enabling IoT testbed/platforms to interconnect their facility's resources in an interoperable semantic way. FIESTA-IoT project enables the integration of IoT platform's resources, testbeds infrastructure and their associated applications. FIESTA-IoT opens up new opportunities in the development and deployment of experiments using data from IoT testbeds. The FIESTA-IoT infrastructure enables experimenters to use a single EaaS API (i.e. the FIESTA-IoT EaaS API) for executing experiments over multiple IoT federated testbeds in a testbed agnostic way i.e. like accessing a single large scale virtualized testbed. The main goal of the FIESTA-IoT project is to open new horizons in the development and deployment of IoT applications and experiments at a EU (and global) scale, based on the interconnection and interoperability of diverse IoT platforms and testbeds. FIESTA-IoT project's experimental infrastructure provides to the European experimenters in the IoT domain with the unique capability for accessing and sharing IoT semantically annotated datasets in a testbed-agnostic way. FIESTA-IoT enables execution of experiments across multiple IoT testbeds, based on a single API for submitting the experiment and a single set of credentials for the researcher and the portability of IoT experiments across different testbeds and the provision of interoperable standards-based IoT/cloud interfaces over diverse IoT experimental facilities.",FIESTAIoT Project: Federated Interoperable Semantic IoT/cloud Testbeds and Applications,NA:NA:NA:NA,2018
Mark A. Musen:Susanna-Assunta Sansone:Kei-Hoi Cheung:Steven H. Kleinstein:Morgan Crafts:Stephan C. Schürer:John Graybeal,"There is an expectation that scientists will archive their experimental data online in public repositories to enable other investigators to verify their work and to re-explore their data in search of new discoveries. When left to their own devices, however, scientists do a poor job creating the metadata that describe their datasets. A lack of standardization makes it difficult for other investigators to find relevant datasets and to perform secondary analyses. The Center for Expanded Data Annotation and Retrieval (CEDAR) was founded with the goal of enhancing the authoring of experimental metadata to make online datasets more useful to the scientific community. CEDAR technology includes Web-based methods for creating and managing libraries of templates for representing metadata. CEDAR's templates interoperate with a repository of scientific ontologies to standardize the way in which the templates may be filled out. Collaborations with several major research projects are allowing us to explore how CEDAR may ease access to scientific data sets stored in public repositories.",CEDAR: Semantic Web Technology to Support Open Science,NA:NA:NA:NA:NA:NA:NA,2018
Philip E. Schreur,"Linked Data for Production (LD4P) is a collaboration between six institutions (Columbia, Cornell, Harvard, Library of Congress, Princeton, and Stanford) to begin the transition of technical services production workflows from a series of library-centric data formats (MARC) to ones based in Linked Open Data (LOD). This first phase of the transition focuses on the development of the ability to produce metadata as LOD communally, the enhancement of the BIBFRAME ontology to encompass the multiple resource formats that academic libraries must process, and the engagement of the broader academic library community to ensure a sustainable and extensible environment. As its name implies, LD4P focuses on the immediate needs of metadata production such as ontology coverage and workflow transition. The LD4P partners' work will be based, in part, on a collection of tools that currently exist, such as those developed by the Library of Congress. The cyclical feedback of use and enhancement request to the developers of these tools will allow for their enhancement based on use in an actual production environment. The six institutions involved will focus on materials ranging from art to rare books, from cartographic materials to music, from annotations to workflows. Tool development and enhancement will also be a key aspect of the project. By the end of the first phase of this project (Spring 2018), the partners will have the minimal tooling, workflows, and standards developed to begin the transformation from MARC to LOD in Phase 2 of the project.",Linked Data for Production (LD4P): a Multi-Institutional Approach to Technical Services Transformation,NA,2018
Hsin-Hsi Chen:Manabu Okumura,"Memory loss, common seen in elderly people, affects their social interaction in the daily life very much. This 3-year international project jointly funded by Taiwan Ministry of Science and Technology (MOST) and Japan Science and Technology Agency (JST) investigates together the crucial issues behind the hyper aged societies. We aim at developing technologies and systems to provide information recall support for elderly people at the right time and at the right place.",Information Recall Support for Elderly People in Hyper Aged Societies,NA:NA,2018
Pedro Szekely:Mayank Kejriwal,"The DARPA Memex program was established with the goal of funding research into building domain-specific search systems that integrated state-of-the-art focused crawling (domain discovery) information extraction and semantic search, and that could be used by users and domain experts with no programming or technical experience. Domain-specific Insight Graphs (DIG) was proposed and funded under Memex and has led to an end-to-end search system currently being used by over 200 law enforcement for combating human trafficking, by investigators from the Securities and Exchange Commission (SEC) in the US for investigating securities fraud, and for numerous other domains of a difficult, socially consequential (e.g., investigative) and unusual nature.",Domain-specific Insight Graphs (DIG),NA:NA,2018
Manolis Koubarakis:Herve Caumont:Ulrike Daniels:Erwin Goor:Lara König:Valentijn Venus,"Copernicus App Lab is a two year project (November 2016 to October 2018) funded by the European Commission under the H2020 program. The consortium consists of AZO (project coordinator), National and Kapodistrian University of Athens, Terradue, RAMANI and VITO. The main objective of Copernicus App Lab is to make Earth observation data produced by the Copernicus program of the European Union available on the Web as linked data to aid their use by mobile developers.",Copernicus App Lab: A Platform for Easy Data Access Connecting the Scientific Earth Observation Community with Mobile Developers,NA:NA:NA:NA:NA:NA,2018
Fosca Giannotti:Roberto Trasarti:Kalina Bontcheva:Valerio Grossi,"One of the most pressing and fascinating challenges scientists face today, is understanding the complexity of our globally interconnected society. The big data arising from the digital breadcrumbs of human activities has the potential of providing a powerful social microscope, which can help us understand many complex and hidden socio-economic phenomena. Such challenge requires high-level analytics, modeling and reasoning across all the social dimensions above. There is a need to harness these opportunities for scientific advancement and for the social good, compared to the currently prevalent exploitation of big data for commercial purposes or, worse, social control and surveillance. The main obstacle to this accomplishment, besides the scarcity of data scientists, is the lack of a large-scale open ecosystem where big data and social mining research can be carried out. The SoBigData Research Infrastructure (RI) provides an integrated ecosystem for ethic-sensitive scientific discoveries and advanced applications of social data mining on the various dimensions of social life as recorded by ""big data"". The research community uses the SoBigData facilities as a ""secure digital wind-tunnel"" for large-scale social data analysis and simulation experiments. SoBigData promotes repeatable and open science and supports data science research projects by providing: (i) an ever-growing, distributed data ecosystem for procurement, access and curation and management of big social data, to underpin social data mining research within an ethic-sensitive context; (ii) an ever-growing, distributed platform of interoperable, social data mining methods and associated skills: tools, methodologies and services for mining, analysing, and visualising complex and massive datasets, harnessing the techno-legal barriers to the ethically safe deployment of big data for social mining; (iii) an ecosystem where protection of personal information and the respect for fundamental human rights can coexist with a safe use of the same information for scientific purposes of broad and central societal interest. SoBigData has a dedicated ethical and legal board, which is implementing a legal and ethical framework.",SoBigData: Social Mining & Big Data Ecosystem,NA:NA:NA:NA,2018
Mathieu d'Aquin:Dominik Kowald:Angela Fessl:Elisabeth Lex:Stefan Thalmann,"The goal of AFEL is to develop, pilot and evaluate methods and applications, which advance informal/collective learning as it surfaces implicitly in online social environments. The project is following a multi-disciplinary, industry-driven approach to the analysis and understanding of learner data in order to personalize, accelerate and improve informal learning processes. Learning Analytics and Educational Data Mining traditionally relate to the analysis and exploration of data coming from learning environments, especially to understand learners' behaviours. However, studies have for a long time demonstrated that learning activities happen outside of formal educational platforms, also. This includes informal and collective learning usually associated, as a side effect, with other (social) environments and activities. Relying on real data from a commercially available platform, the aim of AFEL is to provide and validate the technological grounding and tools for exploiting learning analytics on such learning activities. This will be achieved in relation to cognitive models of learning and collaboration, which are necessary to the understanding of loosely defined learning processes in online social environments. Applying the skills available in the consortium to a concrete set of live, industrial online social environments, AFEL will tackle the main challenges of informal learning analytics through 1) developing the tools and techniques necessary to capture information about learning activities from (not necessarily educational) online social environments; 2) creating methods for the analysis of such informal learning data, based on combining feature engineering and visual analytics with cognitive models of learning and collaboration; and 3) demonstrating the potential of the approach in improving the understanding of informal learning, and the way it is better supported; 4) evaluate all the former items in real world large scale applications and platforms.",AFEL - Analytics for Everyday Learning,NA:NA:NA:NA:NA,2018
Valerio Basile:Roberto Navigli,"The exponential growth of the Web is resulting in vast amounts of online content. However, the information expressed therein is not at easy reach: what we typically browse is only an infinitesimal part of the Web. And even if we had time to read all the Web we could not understand it, as most of it is written in languages we do not speak. Rather than time, a key problem for a machine is language comprehension, that is, enabling a machine to transform sentences, i.e., sequences of characters, into machine-readable semantic representations linked to existing meaning inventories such as computational lexicons and knowledge bases.",From MultiJEDI to MOUSSE: Two ERC Projects for Innovating Multilingual Disambiguation and Semantic Parsing of Text,NA:NA,2018
Claudia d'Amato:Francesco Marcelloni:Rudi Studer,"It is our great pleasure to welcome you to the first edition of WWW 2018 Journal Track. The track is new track within WWW conference series and it is intended as a forum for presentations of significant Web-related research results that have been published recently in well-known and established journals, and have never been presented at any Web-related conference. The goal is to give visibility of these results to a conference audience as well as to promote discussions concerning such results. The call for papers of the journal track was open for two categories of papers: 1) self-nominations from authors promoting their own journal publication(s) and matching the prerequisites reported above; 2) invited papers, selected on the basis of interest, appropriateness and attractiveness for the WWW 2018 audience, from articles published since January 1st 2015 (even only in the electronic version) in the following journals: Journal of Network and Computer Applications, Journal of Web Semantics, Semantic Web Journal, IEEE Transactions on Fuzzy Systems, IEEE Transactions on Neural Networks and Learning Systems, Journal of Machine Learning Research, Data Mining and Knowledge Discovery, ACM Transactions on the Web, ACM Computing Surveys, Knowledge-based systems, Artificial Intelligence. Exceptions have been also considered for papers judged as potentially very influential but published before January 2015 or in a journal not included in the list. We received 61 submissions and accepted 12 of them for presentation to the WWW 2018 Journal Track. The papers have been evaluated according to the following criteria: novelty, relevance to the conference, quality of the extended abstract, metrics values (e.g. number of citations) computed for the original journal paper, representativeness and impact factor of the journals in which the papers have been published (secondarily). We also took in account the coverage of the different areas related to WWW.",Journal Track Chairs' Welcome & Organization,NA:NA:NA,2018
Ulle Endriss:Umberto Grandi,"Graph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs, each provided by a different source. One needs to perform graph aggregation in a wide variety of situations, e.g., when applying a voting rule (graphs as preference orders), when consolidating conflicting views regarding the relationships between arguments in a debate (graphs as abstract argumentation frameworks), or when computing a consensus between several alternative clusterings of a given dataset (graphs as equivalence relations). Other potential applications include belief merging, data integration, and social network analysis. In this short paper, we review a recently introduced formal framework for graph aggregation that is grounded in social choice theory. Our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule. Our main result is a powerful impossibility theorem that generalises Arrow's seminal result regarding the aggregation of preference orders to a large collection of different types of graphs. We also provide a discussion of existing and potential applications of graph aggregation.",Graph Aggregation,NA:NA,2018
Stefano Calzavara:Riccardo Focardi:Marco Squarcina:Mauro Tempesta,"We survey the most common attacks against web sessions, i.e., attacks which target honest web browser users establishing an authenticated session with a trusted web application. We then review existing security solutions which prevent or mitigate the different attacks, by evaluating them along four different axes: protection, usability, compatibility and ease of deployment. Based on this survey, we identify five guidelines that, to different extents, have been taken into account by the designers of the different proposals we reviewed. We believe that these guidelines can be helpful for the development of innovative solutions approaching web security in a more systematic and comprehensive way.",Surviving the Web: A Journey into Web Session Security,NA:NA:NA:NA,2018
Cataldo Musto:Pasquale Lops:Marco de Gemmis:Giovanni Semeraro,"In this contribution we propose a hybrid recommendation framework based on classification algorithms such as Random Forests and Naive Bayes, which are fed with several heterogeneous groups of features. We split our features into two classes: classic features, as popularity-based, collaborative and content-based ones, and extended features gathered from the Linked Open Data (LOD) cloud, as basic ones (i.e. genre of a movie or the writer of a book) and graph-based features calculated on the ground of the different topological characteristics of the tripartite representation connecting users, items and properties in the LOD cloud. In the experimental session we evaluate the effectiveness of our framework on varying of different groups of features, and results show that both LOD-based and graph-based features positively affect the overall performance of the algorithm, especially in highly sparse recommendation scenarios. Our approach also outperforms several state-of-the-art recommendation techniques, thus confirming the insights behind this research. This extended abstract summarizes the content of the journal paper published on Knowledge-based Systems.",Semantics-aware Recommender Systems Exploiting Linked Open Data and Graph-based Features,NA:NA:NA:NA,2018
Rathachai Chawuthai:Hideaki Takeda:Vilas Wuwongse:Utsugi Jinbo,"Linked Open Data (LOD) technology enables web of data and exchangeable knowledge graphs through the Internet. However, the change in knowledge is happened everywhere and every time, and it becomes a challenging issue of linking data precisely because the misinterpretation and misunderstanding of some terms and concepts may be dissimilar under different context of time and different community knowledge. To solve this issue, we introduce an approach to the preservation of knowledge graph, and we select the biodiversity domain to be our case studies because knowledge of this domain is commonly changed and all changes are clearly documented. Our work produces an ontology, transformation rules, and an application to demonstrate that it is feasible to present and preserve knowledge graphs and provides open and accurate access to linked data. It covers changes in names and their relationships from different time and communities as can be seen in the cases of taxonomic knowledge.",Presenting and Preserving the Change in Taxonomic Knowledge for Linked Data (Extended Abstract),NA:NA:NA:NA,2018
Lorenz Bühmann:Jens Lehmann:Patrick Westphal:Simon Bin,"The following paper is an extended summary of the journal paper ""DL-Learner A framework for inductive learning on the Semantic Web"". In this system paper, we describe the DL-Learner framework. It is beneficial in various data and schema analytic tasks with applications in different standard machine learning scenarios, e.g. life sciences, as well as Semantic Web specific applications such as ontology learning and enrichment. Since its creation in 2007, it has become the main OWL and RDF-based software framework for supervised structured machine learning and includes several algorithm implementations, usage examples and has applications building on top of the framework.",DL-Learner Structured Machine Learning on Semantic Web Data,NA:NA:NA:NA,2018
Mehrdad Farajtabar:Manuel Gomez-Rodriguez:Yichen Wang:Shuang Li:Hongyuan Zha:Le Song,"Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when they are exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes---information diffusion and network evolution---have been typically studied separately, ignoring their co-evolutionary dynamics. In this work, we propose a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. The model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Moreover, we develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. Experiments in both synthetic data and real data gathered from Twitter show that our model provides a good fit to the data as well as more accurate predictions than alternatives.",COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution,NA:NA:NA:NA:NA:NA,2018
Valeria Fionda:Giuseppe Pirro:Claudio Gutierrez,"We research the problem of building knowledge maps of graph-like information. We live in the digital era and similarly to the Earth, the Web is simply too large and its interrelations too complex for anyone to grasp much of it through direct observation. Thus, the problem of applying cartographic principles also to digital landscapes is intriguing. We introduce a mathematical formalism that captures the general notion of map of a graph and enables its development and manipulation in a semi-automated way. We describe an implementation of our formalism on the Web of Linked Data graph and discuss algorithms that efficiently generate and combine (via an algebra) regions and maps. Finally, we discuss examples of knowledge maps built with a tool implementing our framework.",Building Knowledge Maps of Web Graphs,NA:NA:NA,2018
Deepayan Chakrabarti:Stanislav Funiak:Jonathan Chang:Sofus A. Macskassy,"We consider the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Existing approaches such as Label Propagation fail to consider interactions between the label types. Our proposed method, called EdgeExplain, explicitly models these interactions, while still allowing scalable inference under a distributed message-passing architecture. On a large subset of the Facebook social network, collected in a previous study, EdgeExplain outperforms label propagation for several label types, with lifts of up to $120%$ for [email protected] and 60% for [email protected]",Joint Label Inference in Networks,NA:NA:NA:NA,2018
Elaheh Momeni:Claire Cardie:Nicholas Diakopoulos,"User-generated content (UGC) on the Web, especially on social media platforms, facilitates the association of additional information with digital resources and online social topics and it can provide valuable supplementary content. However, UGC varies in quality and, consequently, raises the challenge of how to maximize its utility for a variety of end-users, in particular in the age of misinformation. This study aims to provide researchers and Web data curators with answers to the following questions: (1) What are the existing approaches and methods for assessing and ranking UGC (2) What features and metrics have been used successfully to assess and predict UGC value across a range of application domains This survey is composed of a systematic review of approaches for assessing and ranking UGC: results obtained by identifying and comparing methodologies within the context of short text-based UGC on the Web. This survey categorizes existing assessment and ranking approaches into four framework types and discusses the main contributions and considerations of each type. Furthermore, it suggests a need for further experimentation and encourages the development of new approaches for the assessment and ranking.",How to Assess and Rank User-Generated Content on Web,NA:NA:NA,2018
Jianguo Lu:Hao Wang:Dingding Li,"We show that uniform random sampling is not as effective as PPS (probability proportional to size) sampling in many estimation tasks. In the setting of (graph) size estimation, this paper demonstrates that random edge sampling outperforms random node sampling, with a performance ratio proportional to the normalized graph degree variance. This result is particularly important in the era of big data, when data are typically large and scale-free, resulting in large degree variance. We derive the result by first giving the variances of random node and random edge estimators. A simpler and more intuitive result is obtained by assuming that the data is large and degree distribution follows a power law.",Uniform Random Sampling Not Recommended,NA:NA:NA,2018
Maribel Acosta:Elena Simperl:Fabian Flöck:Maria-Esther Vidal,"We propose HARE, a SPARQL query engine that encompasses human-machine query processing to augment the completeness of query answers. We empirically assessed the effectiveness of HARE on 50 SPARQL queries over DBpedia. Experimental results clearly show that our solution accurately enhances answer completeness.",HARE: An Engine for Enhancing Answer Completeness of SPARQL Queries via Crowdsourcing,NA:NA:NA:NA,2018
Muhammad Imran:Carlos Castillo:Fernando Diaz:Sarah Vieweg,"Millions of people use social media to share information during disasters and mass emergencies. Information available on social media, particularly in the early hours of an event when few other sources are available, can be extremely valuable for emergency responders and decision makers, helping them gain situational awareness and plan relief efforts. Processing social media content to obtain such information involves solving multiple challenges, including parsing brief and informal messages, handling information overload, and prioritizing different types of information. These challenges can be mapped to information processing operations such as filtering, classifying, ranking, aggregating, extracting, and summarizing. This work highlights these challenges and presents state of the art computational techniques to deal with social media messages, focusing on their application to crisis scenarios.",Processing Social Media Messages in Mass Emergency: Survey Summary,NA:NA:NA:NA,2018
Giovanni Luca Ciampaglia:Kristina Lerman:Panagiotis Metaxas,"It is our pleasure to welcome you to the WWW 2018 Journalism, Misinformation and Fact Checking Alternate Track. Although the problem of misinformation and deceptive information is as old as Web itself, the topic has gained a lot of attention recently. Phenomena, such as misinformation propagation, fabricated news reports (also known as ""fake news"",) computational propaganda, astroturf, and ideological polarization have become more common on the Web and the social Web, calling for a cross-cutting approach to better understand the topic. One approach that has gained some traction is that of the establishment of fact-checking organizations. This track solicited contributions that explore the range of computational, social, cognitive, economic, and communication topics related to the above phenomena. We received submissions covering a broad range of topics, including computational approaches for detecting misinformation and propaganda on the Web and social media, as well as proposals to improve fact checking, critical thinking, information and media literacy, crowdsourcing, and societal decision-making processes.","Journalism, Misinformation and Fact Checking Chairs' Welcome & Organization",NA:NA:NA,2018
Sebastian Tschiatschek:Adish Singla:Manuel Gomez Rodriguez:Arpit Merchant:Andreas Krause,"Our work considers leveraging crowd signals for detecting fake news and is motivated by tools recently introduced by Facebook that enable users to flag fake news. By aggregating users' flags, our goal is to select a small subset of news every day, send them to an expert (e.g., via a third-party fact-checking organization), and stop the spread of news identified as fake by an expert. The main objective of our work is to minimize the spread of misinformation by stopping the propagation of fake news in the network. It is especially challenging to achieve this objective as it requires detecting fake news with high-confidence as quickly as possible. We show that in order to leverage users' flags efficiently, it is crucial to learn about users' flagging accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian inference for detecting fake news and jointly learns about users' flagging accuracy over time. Our algorithm employs posterior sampling to actively trade off exploitation (selecting news that maximize the objective value at a given epoch) and exploration (selecting news that maximize the value of information towards learning about users' flagging accuracy). We demonstrate the effectiveness of our approach via extensive experiments and show the power of leveraging community signals for fake news detection.",Fake News Detection in Social Networks via Crowd Signals,NA:NA:NA:NA:NA,2018
Xuezhi Wang:Cong Yu:Simon Baumgartner:Flip Korn,"With the support of major search platforms such as Google and Bing, fact-checking articles, which can be identified by their adoption of the schema.org ClaimReview structured markup, have gained widespread recognition for their role in the fight against digital misinformation. A claim-relevant document is an online document that addresses, and potentially expresses a stance towards, some claim. The claim-relevance discovery problem, then, is to find claim-relevant documents. Depending on the verdict from the fact check, claim-relevance discovery can help identify online misinformation. In this paper, we provide an initial approach to the claim-relevance discovery problem by leveraging various information retrieval and machine learning techniques. The system consists of three phases. First, we retrieve candidate documents based on various features in the fact-checking article. Second, we apply a relevance classifier to filter away documents that do not address the claim. Third, we apply a language feature based classifier to distinguish documents with different stances towards the claim. We experimentally demonstrate that our solution achieves solid results on a large-scale dataset and beats state-of-the-art baselines. Finally, we highlight a rich set of case studies to demonstrate the myriad of remaining challenges and that this problem is far from being solved.",Relevant Document Discovery for Fact-Checking Articles,NA:NA:NA:NA,2018
Dylan Bourgeois:Jérémie Rappaz:Karl Aberer,"News entities must select and filter the coverage they broadcast through their respective channels since the set of world events is too large to be treated exhaustively. The subjective nature of this filtering induces biases due to, among other things, resource constraints, editorial guidelines, ideological affinities, or even the fragmented nature of the information at a journalist's disposal. The magnitude and direction of these biases are, however, widely unknown. The absence of ground truth, the sheer size of the event space, or the lack of an exhaustive set of absolute features to measure make it difficult to observe the bias directly, to characterize the leaning's nature and to factor it out to ensure a neutral coverage of the news. In this work, we introduce a methodology to capture the latent structure of media's decision process on a large scale. Our contribution is multi-fold. First, we show media coverage to be predictable using personalization techniques, and evaluate our approach on a large set of events collected from the GDELT database. We then show that a personalized and parametrized approach not only exhibits higher accuracy in coverage prediction, but also provides an interpretable representation of the selection bias. Last, we propose a method able to select a set of sources by leveraging the latent representation. These selected sources provide a more diverse and egalitarian coverage, all while retaining the most actively covered events.","Selection Bias in News Coverage: Learning it, Fighting it",NA:NA:NA,2018
Shweta Bhatt:Sagar Joglekar:Shehar Bano:Nishanth Sastry,"This paper aims to shed light on alternative news media ecosystems that are believed to have influenced opinions and beliefs by false and/or biased news reporting during the 2016 US Presidential Elections. We examine a large, professionally curated list of 668 hyper-partisan websites and their corresponding Facebook pages, and identify key characteristics that mediate the traffic flow within this ecosystem. We uncover a pattern of new websites being established in the run up to the elections, and abandoned after. Such websites form an ecosystem, creating links from one website to another, and by 'liking' each others' Facebook pages. These practices are highly effective in directing user traffic internally within the ecosystem in a highly partisan manner, with right-leaning sites linking to and liking other right-leaning sites and similarly left-leaning sites linking to other sites on the left, thus forming a filter bubble amongst news producers similar to the filter bubble which has been widely observed among consumers of partisan news. Whereas there is activity along both left- and right-leaning sites, right-leaning sites are more evolved, accounting for a disproportionate number of abandoned websites and partisan internal links. We also examine demographic characteristics of consumers of hyper partisan news and find that some of the more populous demographic groups in the US tend to be consumers of more right-leaning sites.",Illuminating an Ecosystem of Partisan Websites,NA:NA:NA:NA,2018
Andreas Spitz:Michael Gertz,"The increasing number of news outlets and the frequency of the news cycle have made it all but impossible to obtain the full picture from online news. Consolidating news from different sources has thus become a necessity in online news processing. Despite the amount of research that has been devoted to different aspects of new event detection and tracking in news streams, solid solutions for such entangled streams of full news articles are still lacking. Many existing works focus on streams of microblogs since the analysis of news articles raises the additional problem of summarizing or extracting the relevant sections of articles. For the consolidation of identified news snippets, schemes along numerous different dimensions have been proposed, including publication time, temporal expressions, geo-spatial references, named entities, and topics. The granularity of aggregated news snippets then includes such diverse aspects as events, incidents, threads, or topics for various subdivisions of news articles. To support this variety of granularity levels, we propose a comprehensive network model for the representation of multiple entangled streams of news documents. Unlike previous methods, the model is geared towards entity-centric explorations and enables the consolidation of news along all dimensions, including the context of entity mentions. Since the model also serves as a reverse index, it supports explorations along the dimensions of sentences or documents for an encompassing view on news events. We evaluate the performance of our model on a large collection of entangled news streams from major news outlets of English speaking countries and a ground truth that we generate from event summaries in the Wikipedia Current Events portal.",Exploring Entity-centric Networks in Entangled News Streams,NA:NA,2018
Sylvie Cazalens:Philippe Lamarre:Julien Leblay:Ioana Manolescu:Xavier Tannier,"Fact checking has captured the attention of the media and the public alike; it has also recently received strong attention from the computer science community, in particular from data and knowledge management, natural language processing and information retrieval; we denote these together under the term ""content management"". In this paper, we identify the fact checking tasks which can be performed with the help of content management technologies, and survey the recent research works in this area, before laying out some perspectives for the future. We hope our work will provide interested researchers, journalists and fact checkers with an entry point in the existing literature as well as help develop a roadmap for future research and development work.",A Content Management Perspective on Fact-Checking,NA:NA:NA:NA:NA,2018
Svitlana Volkova:Jin Yea Jang,"Deceptive information in online news and social media has had dramatic effect on our society in recent years. This study is the first to gain deeper insights into writers' intent behind digital misinformation by analyzing psycholinguistic signals: moral foundations and connotations extracted from different types of deceptive news ranging from strategic disinformation to propaganda and hoaxes. To ensure consistency of our findings and generalizability across domains, we experiment with data from: (1) confirmed cases of disinformation in news summaries, (2) propaganda, hoax, and disinformation news pages, and (3) social media news. We first contrast lexical markers of biased language, syntactic and stylistic signals, and connotations across deceptive news types including disinformation, propaganda, and hoaxes, and deceptive strategies including misleading or falsification. We then incorporate these insights to build machine learning and deep learning predictive models to infer deception strategies and deceptive news types. Our experimental results demonstrate that unlike earlier work on deception detection, content combined with biased language markers, moral foundations, and connotations leads to better predictive performance of deception strategies compared to syntactic and stylistic signals (as reported in earlier work on deceptive reviews). Falsification strategy is easier to identify than misleading strategy. Disinformation is more difficult to predict than to propaganda or hoaxes. Deceptive news types (disinformation, propaganda, and hoaxes), unlike deceptive strategies (falsification and misleading), are more salient, and thus easier to identify in tweets than in news reports. Finally, our novel connotation analysis across deception types provides deeper understanding of writers' perspectives and therefore reveals the intentions behind digital misinformation.",Misleading or Falsification: Inferring Deceptive Strategies and Types in Online News and Social Media,NA:NA,2018
Jing Ma:Wei Gao:Kam-Fai Wong,"In recent years, an unhealthy phenomenon characterized as the massive spread of fake news or unverified information (i.e., rumors) has become increasingly a daunting issue in human society. The rumors commonly originate from social media outlets, primarily microblogging platforms, being viral afterwards by the wild, willful propagation via a large number of participants. It is observed that rumorous posts often trigger versatile, mostly controversial stances among participating users. Thus, determining the stances on the posts in question can be pertinent to the successful detection of rumors, and vice versa. Existing studies, however, mainly regard rumor detection and stance classification as separate tasks. In this paper, we argue that they should be treated as a joint, collaborative effort, considering the strong connections between the veracity of claim and the stances expressed in responsive posts. Enlightened by the multi-task learning scheme, we propose a joint framework that unifies the two highly pertinent tasks, i.e., rumor detection and stance classification. Based on deep neural networks, we train both tasks jointly using weight sharing to extract the common and task-invariant features while each task can still learn its task-specific features. Extensive experiments on real-world datasets gathered from Twitter and news portals demonstrate that our proposed framework improves both rumor detection and stance classification tasks consistently with the help of the strong inter-task connections, achieving much better performance than state-of-the-art methods.",Detect Rumor and Stance Jointly by Neural Multi-task Learning,NA:NA:NA,2018
Miriam Fernandez:Harith Alani,"Misinformation has become a common part of our digital media environments and it is compromising the ability of our societies to form informed opinions. It generates misperceptions, which have affected the decision making processes in many domains, including economy, health, environment, and elections, among others. Misinformation and its generation, propagation, impact, and management is being studied through a variety of lenses (computer science, social science, journalism, psychology, etc.) since it widely affects multiple aspects of society. In this paper we analyse the phenomenon of misinformation from a technological point of view. We study the current socio-technical advancements towards addressing the problem, identify some of the key limitations of current technologies, and propose some ideas to target such limitations. The goal of this position paper is to reflect on the current state of the art and to stimulate discussions on the future design and development of algorithms, methodologies, and applications.",Online Misinformation: Challenges and Future Directions,NA:NA,2018
Amy X. Zhang:Aditya Ranganathan:Sarah Emlen Metz:Scott Appling:Connie Moon Sehat:Norman Gilmore:Nick B. Adams:Emmanuel Vincent:Jennifer Lee:Martin Robbins:Ed Bice:Sandro Hawke:David Karger:An Xiao Mina,"The proliferation of misinformation in online news and its amplification by platforms are a growing concern, leading to numerous efforts to improve the detection of and response to misinformation. Given the variety of approaches, collective agreement on the indicators that signify credible content could allow for greater collaboration and data-sharing across initiatives. In this paper, we present an initial set of indicators for article credibility defined by a diverse coalition of experts. These indicators originate from both within an article's text as well as from external sources or article metadata. As a proof-of-concept, we present a dataset of 40 articles of varying credibility annotated with our indicators by 6 trained annotators using specialized platforms. We discuss future steps including expanding annotation, broadening the set of indicators, and considering their use by platforms and the public, towards the development of interoperable standards for content credibility.",A Structured Response to Misinformation: Defining and Annotating Credibility Indicators in News Articles,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Michele Bedard:Chianna Schoenthaler,"Ever since the surprising results from the 2016 U.S. presidential race, the subject of Fake News in our worldwide media consumption has grown steadily. On a smaller scale, mainstream media have taken a closer look at the relatively narrow genre of satirical news content. Ed Koltonski of Kent State, defines satirical news as designed specifically to entertain the reader, usually with irony or wit, to critique society or a social figure and invoke change or reform. Using field experiment, survey and focus group methods we sought to determine if media consumers' ability to differentiate between satirical news and fake news is tied to socio-demographic factors. We found that age, education, sex, and political affiliation predict understanding of ""fake news"" and satire. Furthermore, the ability to identify different types of misinformation when presented with screen shots from social media posts appears to be related to these variables. Focus group comments were also analyzed to gain a richer perspective on how participants interpreted the SMS screen shots. Using our primary research, we seek to determine if there is a correlation between social media consumers understanding of the difference between satirical news versus fake news and their varying socio-demographic factors",Satire or Fake News: Social Media Consumers' Socio-Demographics Decide,NA:NA,2018
Fred Morstatter:Yunqiu Shao:Aram Galstyan:Shanika Karunasekera,"In the 2017 German Federal elections, the ""Alternative for Deutschland'', or AfD, party was able to take control of many seats in German parliament. Their success was credited, in part, to their large online presence. Like other ""alt-right'' organizations worldwide, this party is tech savvy, generating a large social media footprint, especially on Twitter, which provides an ample opportunity to understand their online behavior. In this work we present an analysis of Twitter data related to the aforementioned election. We show how users self-organize into communities, and identify the themes that define those communities. Next we analyze the content generated by those communities, and the extent to which these communities interact. Despite these elections being held in Germany, we note a substantial impact from the English-speaking Twittersphere. Specifically, we note that many of these accounts appear to be from the American alt-right movement, and support the German alt-right movement.",From Alt-Right to Alt-Rechts: Twitter Analysis of the 2017 German Federal Election,NA:NA:NA:NA,2018
Evgeniy Gabrilovich:Kira Radinsky:Kuansan Wang,"It is our great pleasure to welcome you to the BIG Web Track of the Web Conference 2018. Many of today's most successful enterprises in business and in science are built on the collection and analysis of data. The sheer volume and richness of these data sets has stimulated a massive wave of innovation. In addition, this revolution has also sparked important debate on data privacy policies, ethics, and governance. This track started as a co-located event called BigData Innovators Gathering (BIG) with a vision to bring together academic and industry leaders in the Big Data space to share the state of the art and its successful applications in business. This event will be co-located with the Web conference for the fifth time, but now as a fully fledged alternate track named The BIG Web in The Web Conference 2018 in Lyon. This year's track consists of two keynotes, a panel on machine learning in the field of medicine, and 11 invited talks. In addition, we have accepted 6 papers from 35 submissions (with an acceptance ratio of 17%).",The BIG Web Track Chairs' Welcome & Organization,NA:NA:NA,2018
Kunwoo Park:Meeyoung Cha:Eunhee Rhim,"Customer ratings are valuable sources to understand their satisfaction and are critical for designing better customer experiences and recommendations. The majority of customers, however, do not respond to rating surveys, which makes the result less representative. To understand overall satisfaction, this paper aims to investigate how likely customers without responses had satisfactory experiences compared to those respondents. To infer customer satisfaction of such unlabeled sessions, we propose models using recurrent neural networks (RNNs) that learn continuous representations of unstructured text conversation. By analyzing online chat logs of over 170,000 sessions from Samsung's customer service department, we make a novel finding that while labeled sessions contributed by a small fraction of customers received overwhelmingly positive reviews, the majority of unlabeled sessions would have received lower ratings by customers. The data analytics presented in this paper not only have practical implications for helping detect dissatisfied customers on live chat services but also make theoretical contributions on discovering the level of biases in online rating platforms.",Positivity Bias in Customer Satisfaction Ratings,NA:NA:NA,2018
Ya-Lin Zhang:Longfei Li:Jun Zhou:Xiaolong Li:Zhi-Hua Zhou,"In this paper, we consider the problem of anomaly detection. Previous studies mostly deal with this task in either supervised or unsupervised manner according to whether label information is available. However, there always exists settings which are different from the two standard manners. In this paper, we address the scenario when anomalies are partially observed, i.e., we are given a large amount of unlabeled instances as well as a handful labeled anomalies. We refer to this problem as anomaly detection with partially observed anomalies, and proposed a two-stage method ADOA to solve it. Firstly, by addressing the difference between the anomalies, the observed anomalies are clustered, while the unlabeled instances are filtered to get potential anomalies and reliable normal instances. Then, with the above instances, a weight is attached to each instance according to the confidence of its label, and a weighted multi-class model is built, which will be further used to distinguish different anomalies to the normal instances. Experimental results show that in the aforementioned setting, existing methods behave unsatisfactorily and the proposed method performs significantly better than all these methods, which validates the effectiveness of the proposed approach.",Anomaly Detection with Partially Observed Anomalies,NA:NA:NA:NA:NA,2018
Wenshan Wang:Su Yang:Zhiyuan He:Minjie Wang:Jiulong Zhang:Weishan Zhang,"People can percept social attributes from streetscapes such as safety, richness, and happiness by means of visual perception, which inspires the research in terms of urban perception. To the best of our knowledge, this is the first work focused on revealing the relationship between visual patterns of satellite images as well as streetscapes and commercial activeness. We propose to make use of bag of features (BoF) in the context of computer vision and sparse representation in the sense of machine learning to predict commercial activeness of urban commercial districts. After obtaining the urban commercial districts via clustering, we predict the commercial activeness degrees of them using four image features, namely, Histogram of Oriented Gradients (HOG), Autoencoder, GIST, and multifractal spectra for satellite images and street view images, respectively. The performance evaluation with four large-scale datasets demonstrates that the presented computational framework can not only predict the commercial activeness with satisfactory precision compared with that based on Point of Interest (POI) data but also discover the visual patterns related.",Urban Perception of Commercial Activeness from Satellite Images and Streetscapes,NA:NA:NA:NA:NA:NA,2018
Dotan Di Castro:Iftah Gamzu:Irena Grabovitch-Zuyev:Liane Lewin-Eytan:Abhinav Pundir:Nil Ratan Sahoo:Michael Viderman,"Mail extraction is a critical task whose objective is to extract valuable data from the content of mail messages. This task is key for many types of applications including re-targeting, mail search, and mail summarization, which utilize the important personal data pieces in mail messages to achieve their objectives. We focus on machine generated traffic, which comprises most of the Web mail traffic today, and use its structured and large-scale repetitive nature to devise a fully automated extraction method. Our solution builds on an advanced structural clustering technique previously presented by some of the authors of this work. The heart of our solution is an offline process that leverages the structural mail-specific characteristics of the clustering, and automatically creates extraction rules that are later applied online for each new arriving message. We provide of a full description of our process, which has been productized in Yahoo mail backend. We complete our work with large-scale experiments carried over real Yahoo mail traffic, and evaluate the performance of our automatic extraction method.",Automated Extractions for Machine Generated Mail,NA:NA:NA:NA:NA:NA:NA,2018
Qianyun Zhang:Shawndra Hill:David Rothschild,"Although consumer behavior in response to search engine marketing has been studied extensively, few efforts have been made to understand how consumers search and respond to ads post purchase. Advertising to existing customers the same way as to prospective customers inevitably leads to wasteful and inefficient marketing. Employing a unique dataset that combines both search query and purchase data, we examine consumers' searching behavior and response to search engine marketing after purchase. We study large advertising campaigns for two popular technology products. We find that over half of the branded keyword searches come from consumers who already purchased the products, and that advertising response varies based on whether searchers are pre- or post-purchase. In general, post-purchase searchers are less likely to click on focal brand ads (i.e., they are less responsive to ads for products they already own). However, post-purchase searchers are still responsive to advertising and much more likely to click on ads for complementary products (i.e., they are more responsive to ads for relevant products other than the focal product).",Post Purchase Search Engine Marketing,NA:NA:NA,2018
Xinpeng Chen:Jingyuan Chen:Lin Ma:Jian Yao:Wei Liu:Jiebo Luo:Tong Zhang,"Nowadays, billions of videos are online ready to be viewed and shared. Among an enormous volume of videos, some popular ones are widely viewed by online users while the majority attract little attention. Furthermore, within each video, different segments may attract significantly different numbers of views. This phenomenon leads to a challenging yet important problem, namely fine-grained video attractiveness prediction, which only relies on video contents to forecast video attractiveness at fine-grained levels, specifically video segments of several second length in this paper. However, one major obstacle for such a challenging problem is that no suitable benchmark dataset currently exists. To this end, we construct the first fine-grained video attractiveness dataset (FVAD), which is collected from one of the most popular video websites in the world. In total, the constructed FVAD consists of 1,019 drama episodes with 780.6 hours covering different categories and a wide variety of video contents. Apart from the large amount of videos, hundreds of millions of user behaviors during watching videos are also included, such as view counts, ""fast-forward, ""fast-rewind, and so on, where ""view counts"" reflects the video attractiveness while other engagements capture the interactions between the viewers and videos. First, we demonstrate that video attractiveness and different engagements present different relationships. Second, FVAD provides us an opportunity to study the fine-grained video attractiveness prediction problem. We design different sequential models to perform video attractiveness prediction by relying solely on video contents. The sequential models exploit the multimodal relationships between visual and audio components of the video contents at different levels. Experimental results demonstrate the effectiveness of our proposed sequential models with different visual and audio representations, the necessity of incorporating the two modalities, and the complementary behaviors of the sequential prediction models at different levels.",Fine-grained Video Attractiveness Prediction Using Multimodal Deep Learning on a Large Real-world Dataset,NA:NA:NA:NA:NA:NA:NA,2018
Manuel Serrano:Sukyoung Ryu,"It is our great pleasure to welcome you to the WWW 2018 Web Programming alternate track. We received 14 proposals from all around the world covering a broad range of topics. We evaluated them regarding relevance, quality, and novelty, selecting 1 full-day talks.",Web Programming Chairs' Welcome & Organization,NA:NA,2018
Gabriel Radanne:Jérôme Vouillon,"Tierless Web programming languages allow combining client-side and server-side programming in a single program. This allows defining expressions with both client and server parts, and at the same time provides good static guarantees regarding client-server communication. However, these nice properties come at a cost: most tierless languages offer very poor support for modularity and separate compilation. To regain this modularity and offer a larger-scale notion of composition, we propose to leverage a well-known tool: ML-style modules. In modern ML languages, the module system is a layer separate from the expression language. Eliom is an extension of OCaml for tierless Web programming which provides type-safe communication and an efficient execution model. In this article, we present how the Eliom module system combines the flexibility of tierless Web programming languages with a powerful module system, thus providing good support for abstraction, modularity and separate compilation. We also show that we can provide all these advantages while providing seamless integration with OCaml and its ecosystem.",Tierless Web Programming in the Large,NA:NA,2018
Arthur Charguéraud:Alan Schmitt:Thomas Wood,"We present JSExplain, a reference interpreter for JavaScript that closely follows the specification and that produces execution traces. These traces may be interactively investigated in a browser, with an interface that displays not only the code and the state of the interpreter, but also the code and the state of the interpreted program. Conditional breakpoints may be expressed with respect to both the interpreter and the interpreted program. In that respect, JSExplain is a double-debugger for the specification of JavaScript.",JSExplain: A Double Debugger for JavaScript,NA:NA:NA,2018
Stéphane Letz:Yann Orlarey:Dominique Fober,"\beginabstract This paper demonstrates how FAUST, a functional programming language for sound synthesis and audio processing, can be used to develop efficient audio code for the Web. After a brief overview of the language, its compiler and the architecture system allowing to deploy the same program as a variety of targets, the generation of WebAssembly code and the deployment of specialized WebAudio nodes will be explained. Several use cases will be presented. Extensive benchmarks to compare the performance of native and WebAssembly versions of the same set of DSP have be done and will be commented. \endabstract",FAUST Domain Specific Audio DSP Language Compiled to WebAssembly,NA:NA:NA,2018
Véronique Benzaken:Giuseppe Castagna:Laurent Daynès:Julien Lopez:Kim Nguyen:Romain Vernoux,"We present BOLDR, a modular framework that enables the evaluation in databases of queries containing application logic and, in particular, user-defined functions. BOLDR also allows the nesting of queries for different databases of possibly different data models. The framework detects the boundaries of queries present in an application, translates them into an intermediate representation together with the relevant language environment, rewrites them in order to avoid query avalanches and to make the most out of database optimizations, and converts the results back to the application. Our experiments show that the techniques we implemented are applicable to real-world database applications, successfully handling a variety of language-integrated queries with good performances.",Language-Integrated Queries: a BOLDR Approach,NA:NA:NA:NA:NA:NA,2018
Nick ten Veen:Daco C. Harkes:Eelco Visser,"Modern web applications are interactive. Reactive programming languages and libraries are the state-of-the-art approach for declara- tively specifying these interactive applications. However, programs written with these approaches contain error-prone boilerplate code for e ciency reasons. In this paper we present PixieDust, a declarative user-interface language for browser-based applications. PixieDust uses static de- pendency analysis to incrementally update a browser-DOM at run- time, without boilerplate code. We demonstrate that applications in PixieDust contain less boilerplate code than state-of-the-art ap- proaches, while achieving on-par performance.",PixieDust: Declarative Incremental User Interface Rendering Through Static Dependency Tracking,NA:NA:NA,2018
Minh Ngo:Nataliia Bielova:Cormac Flanagan:Tamara Rezk:Alejandro Russo:Thomas Schmitz,"Multiple Facets (MF) is a dynamic enforcement mechanism which has proved to be a good fit for implementing information flow security for JavaScript. It relies on multi executing the program, once per each security level or view, to achieve soundness. By looking inside programs, MF encodes the views to reduce the number of needed multi-executions. In this work, we extend Multiple Facets in three directions. First, we propose a new version of MF for arbitrary lattices, called Generalised Multiple Facets, or GMF. GMF strictly generalizes MF, which was originally proposed for a specific lattice of principals. Second, we propose a new optimization on top of GMF that further reduces the number of executions. Third, we strengthen the security guarantees provided by Multiple Facets by proposing a termination sensitive version that eliminates covert channels due to termination.",A Better Facet of Dynamic Information Flow Control,NA:NA:NA:NA:NA:NA,2018
Achim D. Brucker:Michael Herzberg,"At its core, the Document Object Model (DOM) defines a tree-like data structure for representing documents in general and HTML documents in particular. It is the heart of any modern web browser. Formalizing the key concepts of the DOM is a prerequisite for the formal reasoning over client-side JavaScript programs and for the analysis of security concepts in modern web browsers. We present a formalization of the core DOM, with focus on the node-tree and the operations defined on node-trees, in Isabelle/HOL. We use the formalization to verify the functional correctness of the most important functions defined in the DOM standard. Moreover, our formalization is extensible, i.e., can be extended without the need of re-proving already proven properties and executable, i.e., we can generate executable code from our specification.",A Formal Semantics of the Core DOM in Isabelle/HOL,NA:NA,2018
Amy Guy:Thomas Steiner,"It is our great pleasure to welcome you to the WWW 2018 Developers' Track. We had 12 submissions in total, out of which 7 were papers for the proceedings (5 long, 2 short), and 5 were free-form formats that did not go in the proceedings. A lot of great publications are accompanied by great implementations that sometimes risk going almost unnoticed in favor of the more glamorous research results they helped produce. Likewisethe Web being a moving and ever-developing targeta lot of sometimes tedious and oftentimes less obvious work happens on standardization of future Web APIs and programming languages. The Developers' Track aims to put this implementation and standardization work front and center. It highlights research submissions that describe technically challenging Web applications of all sorts. Apart from classic papers (that we do understand are a fixed requirement for some people in order be allowed to the conference), the Developers' Track was not limited to formats that can be printed, and authors were encouraged to be creative in finding the most effective way to communicate their work, and we have included dynamic or interactive contributions.",Developers' Track Chairs' Welcome & Organization,NA:NA,2018
Ksenia Peguero:Nan Zhang:Xiuzhen Cheng,"\textitBackground: JavaScript frameworks are widely used to create client-side and server-side parts of contemporary web applications. Vulnerabilities like cross-site scripting introduce significant risks in web applications.\\ \textitAim: The goal of our study is to understand how the security features of a framework impact the security of the applications written using that framework.\\ \textitMethod: In this paper, we present four locations in an application, relative to the framework being used, where a mitigation can be applied. We perform an empirical study of JavaScript applications that use the three most common template engines: Jade/Pug, EJS, and Angular. Using automated and manual analysis of each group of applications, we identify the number of projects vulnerable to cross-site scripting, and the number of vulnerabilities in each project, based on the framework used.\\ \textitResults: We analyze the results to compare the number of vulnerable projects to the mitigation locations used in each framework and perform statistical analysis of confounding variables.\\ \textitConclusions: The location of the mitigation impacts the application's security posture, with mitigations placed within the framework resulting in more secure applications.",An Empirical Study of the Framework Impact on the Security of JavaScript Web Applications,NA:NA:NA,2018
Michel Buffa:Jérôme Lebrun:Jari Kleimola:Oliver larkin:Stéphane Letz,"Web Audio is a recent W3C API that brings the world of computer music applications to the browser. Although developers have been actively using it since the first beta implementations in 2012, the number of web apps built using Web Audio API cannot yet compare to the number of commercial and open source audio software tools available on native platforms. Many of the sites using this new technology are of an experimental nature or are very limited in their scope. While JavaScript and Web standards are increasingly flexible and powerful, C and C++ are the languages most often used for real-time audio applications and domain specific languages such as FAUST facilitate rapid development with high performance. Our work aims to create a continuum between native and browser based audio app development and to appeal to programmers from both worlds. This paper presents our proposal including guidelines and proof of concept implementations for an open Web Audio plug-in standard - essentially the infrastructure to support high level audio plug-ins for the browser.",Towards an open Web Audio plugin standard,NA:NA:NA:NA:NA,2018
Andrea Gallidabino:Cesare Pautasso,"In the past years the average number of Web-enabled devices owned by each user has significantly increased. Liquid Web applications enable users to take advantage of all their devices sequentially to migrate their running applications across them or simultaneously when running different views of the same application at the same time on each device. Developers of liquid Web application need to control how to expose the liquid behavior of their cross-device Web applications to the users. To do so, they can use the API of Liquid.js we describe in this paper. Liquid.js is a framework for building component-based rich Web applications which run across multiple Web-enabled devices. The framework is based on technologies such as Polymer, WebRTC, WebWorkers, PouchDB and Yjs. Liquid.js helps to build decentralized Web applications whose components can seamlessly flow directly between Web browsers carrying along their execution state. The Liquid.js API gives developers fine-grained control over the liquid user experience primitives, device discovery, and the lifecycle of liquid Web components.",The Liquid User Experience API,NA:NA,2018
Pasquale Lisena:Raphaël Troncy,"SPARQL endpoints are one possible access method to linked data. The results of SPARQL queries serialized in JSON are, however, not suitable to be directly used by web developers in end-user applications who often need to merge the values resulting from variable bindings. In this work, we propose a generic approach implemented in a JavaScript module that takes as input a JSON file describing both the SPARQL query and the shape of the expected output at the same time.",Transforming the JSON Output of SPARQL Queries for Linked Data Clients,NA:NA,2018
István Koren:Ralf Klamma,"New Internet-enabled devices and Web services are introduced on a daily basis. Documentation formats are available that describe their functionalities in terms of API endpoints and parameters. In particular, the OpenAPI specification has gained considerable influence over the last years. Web-based solutions exist that generate interactive OpenAPI documentation with HTML5 & JavaScript. They allow developers to quickly get an understanding what the services and devices do and how they work. However, the generated user interfaces are far from real-world practices of designers and end users. We present an approach to overcome this gap, by using a model-driven methodology resulting in state-of-the-art responsive Web user interfaces. To this end, we use the Interaction Flow Modeling Language (IFML) as intermediary model specification to bring together APIs and frontends. Our implementation is based on open standards like Web Components and SVG. A screencast of our tool is available at https://youtu.be/KFOPmPShak4",The Exploitation of OpenAPI Documentation for the Generation of Web Frontends,NA:NA,2018
Thomas Steiner,"Progressive Web Apps (PWA) are a new class of Web applications, enabled for the most part by the Service Workers APIs. Service Workers allow apps to work offline by intercepting network requests to deliver programmatic or cached responses, Service Workers can receive push notifications and synchronize data in the background even when the app is not running, andtogether with Web App Manifestsallow users to install PWAs to their devices' home screens. Service Workers being a Web standard, support has landed in several stand-alone Android Web browsersamong them (but not limited to) Chrome and its open-source foundation Chromium, Firefox, Edge, Opera, UC Browser, Samsung Internet, andeagerly awaitediOS Safari. In this paper, we examine the PWA feature support situation in Web Views, that is, in-app Web experiences that are explicitly not stand-alone browsers. Such in-app browsers can commonly be encountered in chat applications like WeChat or WhatsApp, online social networks like Facebook or Twitter, but also email clients like Gmail, or simply anywhere where Web content is displayed inside native apps. We have developed an open-source application called PWA Feature Detector that allows for easily testing in-app browsers (and naturally stand-alone browsers), and have evaluated the level of support for PWA features on different devices and Web Views. On the one hand, our results show that there are big differences between the various Web View technologies and the browser engines they are based upon, but on the other hand, that for Android the results are independent from the devices' operating systems, which is good news given the problematic update policy of many device manufacturers. These findings help developers make educated choices when it comes to determining whether a PWA is the right approach given their target users' means of Web access.",What is in a Web View: An Analysis of Progressive Web App Features When the Means of Web Access is not a Web Browser,NA,2018
Erik Wilde,"The Web is based on numerous standards that together make up the surface of the Web: By knowing and supporting those standards, problems can be solved in well-known ways. This general design pattern on the Web applies to APIs in the very same way as it does to the human Web: By using an (evolving) set of standards, API developers benefit by not having to reinvent the wheel, and developers benefit by the same problem being solved in the same way across a variety of APIs. The evolving set of standards for Web APIs can be regarded as a set of building blocks or vocabularies for API design. Web Concepts is a site (webconcepts.info) and a repository (github.com/dret/webconcepts) that can be used to manage how within organizations these building blocks are used, thus helping to establish a Web API design culture. The main idea of Web Concepts is to promote reuse of existing standards and technologies, and to therefore make it easier for teams to understand which options are available generally speaking, and maybe which ones are popular choices within their organization.",Surfing the API Web: Web Concepts,NA,2018
Sylvie Calabretto:Lalana Kagal:Maria Maleshkova,"It is our great pleasure to welcome you to the WWW 2018 PhD Symposium. The goal of the PhD Symposium is to provide a supportive atmosphere for PhD students to present and receive feedback on their ongoing work. Students at different stages in their research get an opportunity to present and discuss their problem statements, goals, methods and results. The symposium aims to provide students with useful guidance on various aspects of their research from established researchers and other PhD students, working in areas related to the World Wide Web. Finally, the symposium also aims to enable PhD students to interact with other participants and potential collaborators in order to stimulate an exchange of ideas, suggestions and experiences. We received 25 PhD proposal submissions from all around the world covering a broad range of topics. We evaluated them with respect to methodology, approach, relevance and novelty, selecting 8 papers for long presentation and 4 papers for short presentation. We also took in account the coverage of the different areas related to WWW as well as the potential audience and the possibility to receive feedback from seniors and peers.",PhD Symposium Chairs' Welcome,NA:NA:NA,2018
Lucas Azevedo,"In the actual scenario of ever-growing data consumption speed and quantity, factors like news source decentralization, citizen journalism and democratization of media, make the task of manually checking and correcting disinformation across the internet impractical or infeasible . Here, there is an imperative need for a fast and reliable way to account for the veracity of what is produced and spread as information: Automatic fact-checking. In this work we present the problem of fact-checking in the era of big data and post-truth. Some existing approaches for this task are presented and their main features discussed and compared. Concluding, a new approach inspired on the best components of the existing ones is presented.",Truth or Lie: Automatically Fact Checking News,NA,2018
Tobias Grubenmann,"Inspired by the World Wide Web, the Web of Data is a network of interlinked data fragments. One of the main advantages of the Web of Data is that all of its content is processable by machines. However, this also has its drawbacks when it comes to monetization of the content: advertisements and donationstwo important financial motors in the World Wide Webdo not translate into the Web of Data as they rely on exposing the user to advertisement/call for donations. The remedy this situation, we propose two different monetization strategies for the Web of Data. The first strategy involves a marketplace where users can buy data in an integrated way. The second strategy allows third parties to promote certain data. In return, the sponsors pay money whenever a user follows a link contained in the sponsored data. We identified two different kind of datacommercial and sponsored datawhich can benefit from the two respective monetization strategies. With our work, we propose solutions to the problem of financing the creation and maintenance of content in the Web of Data.",Monetization Strategies for the Web of Data,NA,2018
Tobias Weller,"The number of users of the world wide web is constantly increasing. However, this also increases the risks. There is the possibility that other users illegally gain access to a users' account of social networks, web shops or other web services. Previous work use graph-based methods to identify hijacked or compromised accounts. Most often posts are used in social networks to detect fraudulences. However, not every compromised account is used to spread propaganda information or phishing attacks. Therefore, we restrict ourselves to the clickstreams from the accounts. In order to identify compromised accounts by means of clickstreams, we will also consider a temporal aspect, since the preferences of a user change over time. We choose a hybrid approach consisting of methods from subsymbolic and symbolic AI to detect fraudulences in clickstreams. We will also take into account the experience of domain experts. Our approach can also be used to identify not only compromised accounts but also shared accounts on instance streaming sites.",Compromised Account Detection Based on Clickstream Data,NA,2018
Robin Marx,"Web performance is important for the user experience and can heavily influence web page revenues. While there are many established Web Performance Optimization (WPO) methods, our work so far has clearly shown that new network protocols, optimized browsers and cutting-edge web standards can have a significant impact on known best practices. Additionally, there is still low-hanging fruit to be exploited, in the form of personalizing performance based on user context (i.e., current device, network, browser) and user preferences (e.g., text reading vs multimedia experience). In our PhD project, we strive to integrate this user-specific metadata into dynamic configurations for both existing and new automated WPO techniques. An intermediate server can (pre)generate optimized versions of a web page, which are then selected based on user context and preferences. Additional metadata is also passed along to the browser, enabling improvements on that side, and used to steer new network protocols to speed up the incremental delivery of page resources. We use the Speeder platform to perform and evaluate full-factorial objective measurements and use subjective user studies across a range of groups to assess the applicability of our methods to end users. Our aim is to provide insights in how WPO can be tweaked for specific users, in the hopes of leading to new web standards that enable this behavior.",Web Performance Automation for the People,NA,2018
Dakshi Tharanga Kapugama Geeganage,"Text contents are overloaded with the digitization of the data and new contents are transmitted through many sources by generating a large volume of information, which spreads all over the world through different communication media. Therefore, text data is available everywhere and reading, understanding and analysing the text data has become a main activity in daily routine. With the increment of the volume and the variety of information, organizing and searching, the required information has become vital. Topic modelling is the state of the art for information organization, understanding and extracting the content. Most of the prevailing topic models use the probabilistic approaches and consider the frequency and the co-occurrence to discover the topics from collections of documents. The proposed research aims to address the existing problems of topic modeling by introducing a concept embedded topic model which generates the most relevant and meaningful topics by understanding the content. The research includes approaches to understand the semantic elements from the content, domain identification of concepts and provide most suitable topics without getting the number of topics from the user beforehand. Capturing the semantics of document collections and generating the most related set of topics according to the actual meaning will be the significance of this research.",Concept Embedded Topic Modeling Technique,NA,2018
Reshmi Gopalakrishna Pillai,"The ability to detect human stress and relaxation is central for timely diagnosing stress-related diseases, ensuring customer satisfaction in services and managing human-centric applications such as traffic management. Traditional methods employ stress measuring scales or physiological monitoring which may be intrusive and inconvenient. Instead, the ubiquitous nature of social media can be leveraged to identify stress and relaxation. In this PhD research, we introduce an improved method to detect expressions of stress and relaxation in social media content. It uses word sense vectors for word sense disambiguation to improve the performance of the first ever lexicon-based stress/relaxation detection algorithm TensiStrength. Experimental results show that TensiStrength with word sense disambiguation performs better than the original TensiStrength and state-of-the-art machine learning methods in terms of Pearson's correlation and accuracy. We also suggest a novel, word-vector based approach for detecting causes of stress and relaxation in social media content.",Detection of Strength and Causal Agents of Stress and Relaxation for Tweets,NA,2018
Xuanxing Yang,"Recently with dynamic information being ubiquitous on the Web, there have been efforts to extend RDF and SPARQL for representing streaming information and continuous querying functionalities, respectively. While existing works focusing on formalization and implementation of continuous querying process over RDF streams, little attention has deserved the problem of querying the complex temporal correlations among RDF stream tuples and the effective, scalable implementation of RDF stream processing system. To fill this gap, in this paper we propose CT-SPARQL and AIMRS, a language specifying for the compositional stream patterns and an architecture for adaptive incremental maintenance of RDF stream tuples defined in CT-SPARQL. We believe that this work will benefit a wide range of real-time analyzing and future predicting applications.",Query for Streaming Information: Dynamic Processing and Adaptive Incremental Maintenance of RDF Stream,NA,2018
Laura Koesten,"Structured data is becoming critical in every domain and its availability on the web is increasing rapidly. Despite its abundance and variety of applications, we know very little about how people find data, understand it, and put it to use. This work aims to inform the design of data discovery tools and technologies from a user centred perspective. We aim to better understand what type of information supports people in finding and selecting data relevant for their respective tasks. We conducted a mixed-methods study looking at the workflow of data practitioners when searching for data. From that we identified textual summaries as a key element that supports the decision making process in information seeking activities for data. Based on these results we performed a mixed-methods study to identify attributes people consider important when summarising a dataset. We found text summaries are laid out according to common structures, contain four main information types, and cover a set of dataset features. We describe follow-up studies that are planned to validate these findings and to evaluate their applicability in a dataset search scenario.",A User Centred Perspective on Structured Data Discovery,NA,2018
Dawid Wisniewski,"The process of ontology authoring is inseparably connected with the quality assurance phase. One can verify the maturity and correctness of a given ontology by evaluating how many competency questions give correct answers. Competency questions are defined as a set of questions expressed in natural language that the finished ontology should be able to answer to correctly. Although this method can easily indicate what is the development status of an ontology, one has to translate competency questions from natural language into an ontology query language. This task is very hard and time consuming. To overcome this problem, my PhD thesis focuses on methods for automatically checking answerability of competency questions for a given ontology and proposing SPARQL-OWL query (OWL-aware SPARQL query) for each question where it is possible to create the query. Because the task of automatic translation from competency questions to SPARQL-OWL queries is a novel one, besides a method, we have proposed a new benchmark to evaluate such translation.",Automatic Translation of Competency Questions into SPARQL-OWL Queries,NA,2018
Maulik R. Kamdar,"The vision of the Semantic Web has stimulated the development of Web-scale architectures for discovering implicit associations from multiple heterogeneous data and knowledge sources. In biomedicine, using W3C-established standards and Linked Data principles, data publishers have transformed and linked several datasets to create a huge web of Life Sciences Linked Open Data (LSLOD). However, mining the LSLOD cloud is still very difficult and often impossible for biomedical researchers due to several challenges: structural heterogeneity, lack of vocabulary reuse, inconsistencies, and incompleteness. To discover drug-adverse reaction associations and their mechanistic explanations, I have developed a novel architecture that combines information retrieval and association discovery. The architecture demonstrates favorable AUROC statistics against baseline methods in pharmacovigilance, and provides confidence values on underlying biological mechanisms. I quantify the several challenges associated with mining the LSLOD cloud for biomedical applications through an empirical analysis of more than 40 different sources. Ideally, the architecture can be extended in other domains to realize the goal of implicit association discovery.",Mining the Web of Life Sciences Linked Open Data for Mechanism-Based Pharmacovigilance,NA,2018
Isa Inuwa-Dutse,"Contemporary social media networks can be viewed as a break to the early two-step flow model in which influential individuals act as intermediaries between the media and the public for information diffusion. Today's social media platforms enable users to both generate and consume online contents. Users continuously engage and disengage in discussions with varying degrees of interaction leading to formation of distinct online communities. Such communities are often formed at high-level either based on metadata, such as hashtags on Twitter, or popular content triggered by few influential users. These online communities often do not reflect true connectivity and lack the cohesiveness of traditional communities. In this study, we investigate real-time formation of temporal communities on Twitter. We aim at defining both high and low levels connections and to reveal the magnitude of clustering cohesion on temporal basis. Inspired by a real-life event center sitting arrangement scenario, the proposed method aims to cluster users into distinct and cohesive online temporal communities. Membership to a community relies on intrinsic tweet properties to define similarity as the basis for interaction networks. The proposed method can be useful for local event monitoring and clique-based marketing among other applications.",Modelling Formation of Online Temporal Communities,NA,2018
Varsha Bhat Kukkala,"Social networks have been a popular choice of study, given the surge of online data on friendship networks, communication networks, collaboration networks etc. This popularity, however, is not true for all types of social networks. In the current work, we draw the reader's attention to a class of social networks which are investigated to a limited extent, classified as distributed sensitive social networks. It constitutes of networks where the presence or absence of edges in the network is distributedly known to a set of parties, who regard this information as their private data. Supply chain networks, informal networks such as trust network, advice network, enmity network, etc. are a few examples of the same. A major reason for the lack of any substantial study on these networks has been the unavailability of data. As a solution, we propose a privacy preserving approach to investigating these networks. We show the feasibility of using secure multiparty computation techniques to perform the required analysis, while preserving the privacy of every individual's data. The possible approaches that can be considered to ensure the design of efficient secure protocols are discussed such as efficient circuit design, ORAM based secure computation, use of oblivious data structures, etc. The results obtained in the direction of secure network analysis algorithms are also presented.",Privacy Preserving Distributed Analysis of Social Networks,NA,2018
Angela Bonifati:Romain Wuillemot,"It is our great pleasure to welcome you to the WWW 2018 Panels Track. We have selected three panel proposals among the ones received. We also took in account the coverage of the different areas related to WWW, inputs from the community with a shared document, as well as the suggestions of the authors of the research tracks in deciding the subjects of the three panels.",Panels Track Chairs' Welcome,NA:NA,2018
Davood Rafiei:Eugene Agichtein:Ricardo Baeza-Yates:Jon Kleinberg:Jure Leskovec,"The Web's content has been going through major changes, triggered by multiple factors including changes in user demographic and authoring behaviour, a shift in device types that access the Web, and changes in common use cases of the Web. More specifically, the number of mobile internet users has surpassed the desktop users according to different statistics; a considerable portion of web use cases are in the form of social interactions rather than information seeking; and the authoring behaviour has transformed from compiling a page and linking resources to sharing content with like-minded followers and leaving likes and comments on posts. Those changes have influenced and are expected to shape the way the content is organized, searched, ranked and analyzed. This panel brings together researchers who have been working in different established areas related to web search and mining, web content and social network analysis, and semantics and knowledge management. The panel will draw from the experience of the panellists, dealing with changes in their respective fields. In the first (role-playing) round, each panellist will strongly take a side on where the changes are heading, arguing that one form of content will dominate in the near future. In the second round, the panellists will counter each other and will share their vision on what future holds in terms of research problems and directions. The members of the audience will participate, in a QA session with the panellists, bringing their own perspectives to the discussion.","The Shifting Landscape of Web Search and Mining: Past, Present, and Future",NA:NA:NA:NA:NA,2018
Boualem Benatallah:Fabio Casati,"Cognitive services and conversational digital assistants are emerging as the engine that powers natural interactions between humans, software services, devices and ""things"" - supported by advances in AI and human computations. Not surprisingly, many large and small tech companies are rushing to occupy this space by providing platforms for building cognitive services and conversational bots. Digital assistants interact in a natural way (through text or voice) with both software and humans to get information and perform actions, from checking the weather to booking a restaurants and a cab ride, managing cloud resources, answering simple scientific questions, and preparing a decaf latte using IoT enabled coffee machines. User requests or tasks are often expressed in natural language, an interaction ensues to clarify the intent and the details, and the answer is sought - or the appropriate service or device is invoked - based on the cognitive service understanding. While the potential of this new wave of services is exciting, it also brings significant challenges: we are far away from the comfort of developing deterministic software that responds to API calls by invoking other APIs. Now we have to understand, guess, explore options, take decisions based on probabilistic models over a large set of possible intents and services, all while engaging with users. Doing so brings a large set of engineering challenges related to the development, training, tuning and evolution of such services. This panel will discuss such challenges and identify interesting opportunities for research as well as promising trends.",Panel on Cognitive Service Engineering,NA:NA,2018
Steffen Staab:Jens Lehmann:Ruben Verborgh,"Structured Knowledge on the Web had an intriguing history before it has become successful. We briefly revisit this history, before we go into the longer discussion about how structured knowledge on the Web should be devised such that it benefits even more applications. Core to this discussion will be issues like trust, information infrastructure usability and resilience, promising realms of structured knowledge and principles and practices of data sharing.",Structured Knowledge on the Web 7.0,NA:NA:NA,2018
Eyhab Al-Masri:Marie-Christine Rousset,"It is our great pleasure to welcome you to the WWW 2018 Workshops. This year's workshops of WWW 2018 feature a number of co-located workshops that are intended to provide a forum for researchers and practitioners in Web technologies to discuss and exchange positions on current and emergent Web topics. We received forty proposals from all around the world covering a broad range of topics. We evaluated them regarding relevance, quality, and novelty selecting eighteen full-day workshops and ten half-day workshops. We also took into account the coverage of the different areas related to WWW as well as the potential audience, to schedule them in two consecutive days with the minimal audience interest overlap.",Workshop Chairs' Welcome,NA:NA,2018
Leonidas Anthopoulos:Marijn Janssen:Vishanth Weerakkody,"Following up the success of the past events at WWW2015, WWW2016 and WWW2017, the 4th AW4City 2018 aims to keep on attracting a significant international attention with regard to web applications for smart cities. More specifically, the aim of this workshop is to focus on the applications smart city component and more specifically on the design and implementation of web-based applications and Apps that deliver smart services or address smart city challenges. This year, the proposed workshop will emphasize on the contribution of web applications and Apps to citizen centricity. In the era of cities, municipal leaders, service and utility providers are making an important shift regarding thinking of people as customers and of customer experience. This shift is not a simple task since it demands a continuous service monitoring, assessment and improvement [1;2;3], which normally is based on accurate data analysis and appears as a thinking that makes government and providers more personal and responsive.",AW4City 2018 Chairs' Welcome & Organization,NA:NA:NA,2018
Zeenat Rehena:Marijn Janssen,"In the last few years, the smart city concept resulted in the development and deployment of platforms for providing innovative services to improve sustainability and the living standards. These platforms integrate data collected from devices and citizen-generated data and thereafter employ big data analytics to create insights from the data. These platform enable the creation of context-aware Intelligent traffic management systems (ITMS), however the involvement of various actors at different stages hampers development. In this paper, we propose a framework to support sustainable traffic management system for providing better commute, safety and security during travel based on real-time information. The framework should help to integrate the activities performed by the various actors. The main key elements of this framework are Datasets, Traffic Management Analytics, Actors and Actions which are taken by these users. The framework helps to create an overall overview of the activities needed. In this way it can be used to improve the quality of the traffic flow, increase efficient use of resources, smooth and safe commute of the citizens.",Towards a Framework for Context-Aware Intelligent Traffic Management System in Smart Cities,NA:NA,2018
Auriol Degbelo:Tomi Kauppinen,"Recent years have witnessed progress of public institutions in making their datasets available online, free of charge, for re-use. This notwithstanding, there is still a long way to go to put the power of data in the hands of citizens. This article suggests that transparency in the context of open government can be increased through web maps featuring: i) Application Programming Interfaces (APIs) which support app and data usage tracking; and (ii) 'transparency badges' which inform the users about the presence/absence of extra, useful contextual information. Eight examples of web maps are introduced as proof of concept for the idea. Designing and implementing these web maps has reminded of the need of interactive guidelines to help non-experts select vocabularies, and datasets to link to. The ideas presented are relevant to making existing open data more user friendly (and ultimately more usable).",Increasing Transparency through Web Maps,NA:NA,2018
Vaia Moustaka:Zenonas Theodosiou:Athena Vakali:Anastasis Kounoudes,"As smart cities infrastructures mature, data becomes a valuable asset which can radically improve city services and tools. Registration, acquisition and utilization of data, which will be transformed into smart services, are becoming more necessary than ever. Online social networks with their enormous momentum are one of the main sources of urban data offering heterogeneous real-time data at a minimal cost. However, various types of attacks often appear on them, which risk users' privacy and affect their online trust. The purpose of this article is to investigate how risks on online social networks affect smart cities and study the differences between privacy and security threats with regard to smart people and smart living dimensions.",Smart Cities at Risk!: Privacy and Security Borderlines from Social Networking in Cities,NA:NA:NA:NA,2018
Gabriela Viale Pereira:Gregor Eibl:Peter Parycek,"This paper presents the analysis of the SmartGov project as a case of smart technologies application, such as expert-based Fuzzy Cognitive Maps, social media applications and open data, to promote citizen engagement and support decision-making. The objective of this paper is to analyze the role of digital technologies as inputs to achieve smart city governance. The main results are illustrated in a framework that combines the smart city governance elements in a real case description.",The Role of Digital Technologies in Promoting Smart City Governance,NA:NA:NA,2018
Agnes Mainka:Tobias Siebenlist:Lisa Beutelspacher,"Participatory smartphone apps empower citizens to interact with the city's administration. The purpose of this case study is to investigate the current state of participatory apps in Germany. The 29 apps that have been found can be categorized into four topics: Information Awareness, City Service, Transparency and Public Safety. Most citizen apps can be assigned to the category Travel & Local. None of the identified apps is based on open-source code, and the citizens' reports are not publicly visible, e.g., for other citizens. It is unclear to whom the data generated by citizens belong.",Citizen Participation: Case Study on Participatory Apps in Germany,NA:NA:NA,2018
Thivya Kandappu:Archan Misra:Desmond Koh:Randy Daratan Tandriansyah:Nikita Jaiman,"Active citizenry, whereby citizens actively participate in reporting and addressing challenges in urban service delivery is a strategic goal of smart cities such as Singapore. In spite of the promise, we believe that the success of such large-scale nation-wide crowdsourcing deployments depend on the real-word user preferences and behavioral characteristics of citizens. In this paper, we first present our findings on behavioral preferences and key concerns of citizens regarding smart-city services via an opinion survey conducted with 1300 participants. We then propose a ""citizen-controlled"" urban services reporting platform where citizens actively report on the status of various municipal resources. We advocate the importance of matching user mobility patterns against task locations to make the platform more efficient (i.e., higher task completion rate and lower detour overhead).",A Feasibility Study on Crowdsourcing to Monitor Municipal Resources in Smart Cities,NA:NA:NA:NA:NA,2018
Leonidas Anthopoulos:Amel Attour,"An increasing amount of applications can be located in several cities that attempt to deal with mobility issues like traffic management, transportation safety, congestion control, taxi booking, car sharing, carpooling etc. The aim of this work in progress article is to collect information with regard to carpooling applications and attempt to recognize the underlying business models.",Smart Transportation Applications' Business Models: A Comparison,NA:NA,2018
Pernilla Bergmark,"This paper looks into the use of Information and Communication Technology (ICT) for Smart Sustainable Cities (SSC). It specifically points towards ICT's potential to help cities mitigate climate change and to support a 2ºC or lower trajectory and to involve citizens in city planning and when implementing solutions. The paper also focuses on the modelling, assessment methodologies and indicators for ICT and sustainability aspects of cities, not least with regards to climate change mitigation. Especially, it highlights the city GG emissions assessment standard by ITU, and the indicator sets from ITU and ISO and their enhancement. The paper emphasizes a sustainability and citizen-centric perspective, seeing ICT as instrumental in this respect. For assessments, the above mentioned ITU-T standard is considered reusable for impacts beyond global warming. For indicators, further research on outcome and impact indicators is suggested, and also strengthening of the socio-economic and cultural aspects.",Reflections Regarding ICT and a Citizen-centric Future Path of Smart Sustainable Cities: AW4City 2018 Keynote,NA,2018
Vinay Kandpal,"Nagpur has emerged as the topmost smart city in India. In just five months, Nagpur has beaten other cities chosen before it to get the best implementation of smart city plan. A recent stock-taking exercise conducted by urban development ministry has revealed that Nagpur, though chosen as a smart city in September 2016 much after 33 smart cities in two previous rounds has achieved the best investment conversion ratio. India's smart city program hopes to revolutionize city life and improve the quality of life for India's urban population. In the absence of a zonal plan, many parts of Dehradun have witnessed haphazard development over the years, which has already caused much damage to the vision of a planned smart city. Smart City would require smart economy, bright people, smart organization, smart communication, smart engineering, smart transit, fresh environment and bright living. Nevertheless, with mass migration leading to basic problems, like water shortages and overcrowding, the rate at which these cities will be developed will be the key. Several initiatives are being led by the Government of India to convert 100 Cities into Smart Cities. Government to Actively Use PPP Route and Encourage FDI for Effective Implementation of Smart Cities Project in India.","A Case Study on Smart City Projects in India: An Analysis of Nagpur, Allahabad and Dehradun",NA,2018
Jie Tang:Michalis Vazirgiannis:Yuxiao Dong:Fragkiskos D. Malliaros:Michael Cochez:Mayank Kejriwal:Achim Rettinger,"It is our great pleasure to welcome you to the 2018 International Workshop on Learning Representations for Big Networks ([email protected]). This is the third edition of the BigNet workshop series, following its inauguration at the 25th ACM International Conference on Information and Knowledge Management (CIKM 2016) and the second edition at the 26th International World Wide Web Conference (WWW 2017). Recent years have witnessed the emergence of network representation learning research. Different from the feature engineering process in conventional analysis, network representation learning, also know as network embedding, aims to learn the latent low-dimensional representations for objects in networks, such as nodes, links, and groups. Its ultimate objective is to encode networks' structural properties into the latent representations, benefiting all existing network mining tasks, such as node classification, link prediction, community detection, etc. In the BigNet 2018 workshop, we aim to provide a forum for presenting the most recent advances in network representation learning to unearth rich knowledge.",BigNet 2018 Chairs' Welcome & Organization,NA:NA:NA:NA:NA:NA:NA,2018
Ayushi Dalmia:Ganesh J:Manish Gupta,"Recently there have been a large number of studies on embedding large-scale information networks using low-dimensional, neighborhood and community aware node representations. Though the performance of these embedding models have been better than traditional methods for graph mining applications, little is known about what these representations encode, or why a particular node representation works better for certain tasks. Our work presented here constitutes the first step in decoding the black-box of vector embeddings of nodes by evaluating their effectiveness in encoding elementary properties of a node such as page rank, degree, closeness centrality, clustering coefficient, etc. We believe that a node representation is effective for an application only if it encodes the application-specific elementary properties of nodes. To unpack the elementary properties encoded in a node representation, we evaluate the representations on the accuracy with which they can model each of these properties. Our extensive study of three state-of-the-art node representation models (DeepWalk, node2vec and LINE) on four different tasks and six diverse graphs reveal that node2vec and LINE best encode the network properties of sparse and dense graphs respectively. We correlate the model performance obtained for elementary property prediction tasks with the high-level downstream applications such as link prediction and node classification, and visualize the task performance vector of each model to understand the semantic similarity between the embeddings learned by various models. Our first study of the node embedding models for outlier detection reveals that node2vec and DeepWalk identify outliers well for sparse and dense graphs respectively. Our analysis highlights that the proposed elementary property prediction tasks help in unearthing the important features responsible for the given node embedding model to perform well for a given downstream task. This understanding would facilitate in picking the right model for a given downstream task.",Towards Interpretation of Node Embeddings,NA:NA:NA,2018
Ryan A. Rossi:Rong Zhou:Nesreen K. Ahmed,"This paper presents a general inductive graph representation learning framework called DeepGL for learning deep node and edge features that generalize across-networks. In particular, DeepGL begins by deriving a set of base features from the graph (e.g., graphlet features) and automatically learns a multi-layered hierarchical graph representation where each successive layer leverages the output from the previous layer to learn features of a higher-order. Contrary to previous work, DeepGL learns relational functions (each representing a feature) that naturally generalize across-networks and are therefore useful for graph-based transfer learning tasks. Moreover, DeepGL naturally supports attributed graphs, learns interpretable inductive graph representations, and is space-efficient (by learning sparse feature vectors). In addition, DeepGL is expressive, flexible with many interchangeable components, efficient with a time complexity of O(|E|), and scalable for large networks via an efficient parallel implementation. Compared with recent methods, DeepGL is (1) effective for across-network transfer learning tasks and large (attributed) graphs, (2) space-efficient requiring up to 6x less memory, (3) fast with up to 182x speedup in runtime performance, and (4) accurate with an average improvement in AUC of 20% or more on many learning tasks and across a wide variety of networks.",Deep Inductive Network Representation Learning,NA:NA:NA,2018
Ivan Brugere:Tanya Y. Berger-Wolf,"Networks are fundamental models for data used in practically every application domain. In most instances, several implicit or explicit choices about the network definition impact the translation of underlying data to a network representation, and the subsequent question(s) about the underlying system being represented. Users of downstream network data may not even be aware of these choices or their impacts. We propose a task-focused network model selection methodology which addresses several key challenges. Our approach constructs network models from underlying data and uses minimum description length (MDL) criteria for selection. Our methodology measures efficiency, a general and comparable measure of the network's performance of a local (i.e. node-level) predictive task of interest. Selection on efficiency favors parsimonious (e.g. sparse) models to avoid overfitting and can be applied across arbitrary tasks and representations. We show stability, sensitivity, and significance testing in our methodology.",Network Model Selection Using Task-Focused Minimum Description Length,NA:NA,2018
Giang Hoang Nguyen:John Boaz Lee:Ryan A. Rossi:Nesreen K. Ahmed:Eunyee Koh:Sungchul Kim,"Networks evolve continuously over time with the addition, deletion, and changing of links and nodes. Although many networks contain this type of temporal information, the majority of research in network representation learning has focused on static snapshots of the graph and has largely ignored the temporal dynamics of the network. In this work, we describe a general framework for incorporating temporal information into network embedding methods. The framework gives rise to methods for learning time-respecting embeddings from continuous-time dynamic networks. Overall, the experiments demonstrate the effectiveness of the proposed framework and dynamic network embedding approach as it achieves an average gain of 11.9% across all methods and graphs. The results indicate that modeling temporal dependencies in graphs is important for learning appropriate and meaningful network representations.",Continuous-Time Dynamic Network Embeddings,NA:NA:NA:NA:NA:NA,2018
Andriy Nikolov:Peter Haase:Daniel M. Herzig:Johannes Trame:Artem Kozlov,"Vector embedding models have recently become popular for encoding both structured and unstructured data. In the context of knowledge graphs such models often serve as additional evidence supporting various tasks related to the knowledge base population: e.g., information extraction or link prediction to expand the original dataset. However, the embedding models themselves are often not used directly alongside structured data: they merely serve as additional evidence for structured knowledge extraction. In the metaphactory knowledge graph management platform, we use federated hybrid SPARQL queries for combining explicit information stated in the graph, implicit information from the associated embedding models, and information extracted using vector embeddings in a transparent way for the end user. In this paper we show how we integrated RDF data with vector space models to construct an augmented knowledge graph to be used in customer applications.",Combining RDF Graph Data and Embedding Models for an Augmented Knowledge Graph,NA:NA:NA:NA:NA,2018
Richard Han:Jeremy Blackburn:Homa Hosseinmardi:Qin Lv:Bert Huang:Shivakant Mishra,"It is our great pleasure to welcome you to the WWW 2018 Workshop on Computational Methods in Cybersafety, Online Harassment, and Misinformation. The theme of cybersafety is an important emerging research topic on the Internet that manifests itself daily as users navigate the Web and networked applications. After two successful workshops on cybersafety, the main goal of this third edition of this workshop on cybersafety is to build and grow the cybersafety research community by bringing together the leading researchers and practitioners from academia, industry, government, and research labs working in the general area of cybersafety to discuss the unique challenges in addressing various cybersafety issues and to share experiences, solutions, tools, and techniques. The focus is on the detection, prevention, and mitigation of various cybersafety issues, as well as education and promoting safe practices. The focus of this workshop is on computational methods in cybersafety, including new algorithms, tools, data mining techniques, analysis, systems, and applications for the detection, prevention and mitigation of various cybersafety issues, as well as education and promoting safe practices. Our program features two invited keynote speakers. We will have Dr. April Edwards, Vice President for Academic Affairs and Dean of the Faculty at Elmhurst College, speak about Racial and Gender Differences in Cyberbullying Behavior. And we will have Dr. Neil Shah, Research Scientist at Snap Inc., speak about Anomaly Detection on Large Social Graphs. We will also feature four contributed presentations and publications selected from papers submitted to our workshop.",International Workshop on Cybersafety Chairs' Welcome & Organization,NA:NA:NA:NA:NA:NA,2018
Anna Sapienza:Sindhu Kiranmai Ernala:Alessandro Bessi:Kristina Lerman:Emilio Ferrara,"Widespread adoption of networking technologies has brought about tremendous economic and social growth, but also exposed individuals and organization to new threats from malicious cyber actors. Recent attacks by WannaCry and NotPetya ransomware crypto-worms, infected hundreds of thousands of computer systems world wide, compromising data and critical infrastructure. In order to limit their impact, it is, therefore, critical to detect---and even predict---cyber attacks before they spread. Here, we introduce DISCOVER, an early cyber threat warning system, that mines online chatter from cyber actors on social media, security blogs, and darkweb forums, to identify words that signal potential cyber attacks. We evaluate DISCOVER and find that it can identify terms related to emerging cyber threats with precision above $80%$. DISCOVER also generates a time line of related online discussions on different Web sources that can be useful for analyzing emerging cyber threats.",DISCOVER: Mining Online Chatter for Emerging Cyber Threats,NA:NA:NA:NA:NA,2018
Emeric Bernard-Jones:Jeremiah Onaolapo:Gianluca Stringhini,"We set out to understand the effects of differing language on the ability of cybercriminals to navigate webmail accounts and locate sensitive information in them. To this end, we configured thirty Gmail honeypot accounts with English, Romanian, and Greek language settings. We populated the accounts with email messages in those languages by subscribing them to selected online newsletters. We also hid email messages about fake bank accounts in fifteen of the accounts to mimic real-world webmail users that sometimes store sensitive information in their accounts. We then leaked credentials to the honey accounts via paste sites on the Surface Web and the Dark Web, and collected data for fifteen days. Our statistical analyses on the data show that cybercriminals are more likely to discover sensitive information (bank account information) in the Greek accounts than the remaining accounts, contrary to the expectation that Greek ought to constitute a barrier to the understanding of non-Greek visitors to the Greek accounts. We also extracted the important words among the emails that cybercriminals accessed (as an approximation of the keywords that they possibly searched for within the honey accounts), and found that financial terms featured among the top words. In summary, we show that language plays a significant role in the ability of cybercriminals to access sensitive information hidden in compromised webmail accounts.",BABELTOWER: How Language Affects Criminal Activity in Stolen Webmail Accounts,NA:NA:NA,2018
Arpita Chakraborty:Yue Zhang:Arti Ramesh,"The possibility of anonymity and lack of effective ways to identify inappropriate messages have resulted in a significant amount of online interaction data that attempt to harass, bully, or offend the recipient. In this work, we perform a preliminary linguistic study on messages exchanged using one such popular web/smartphone application---Sarahah, that allows friends to exchange messages anonymously. Since messages exchanged via Sarahah are private, we collect them when the recipient shares it on Twitter. We then perform an analysis of the different kinds of messages exchanged through this application. Our linguistic analysis reveals that a significant number of these messages (~20%) include inappropriate, hurtful, or profane language intended to embarrass, offend, or bully the recipient. Our analysis helps in understanding the different ways in which anonymous message exchange platforms are used and the different types of bullying present in such exchanges.",Understanding Types of Cyberbullying in an Anonymous Messaging Application,NA:NA:NA,2018
Savvas Zannettou:Barry Bradlyn:Emiliano De Cristofaro:Haewoon Kwak:Michael Sirivianos:Gianluca Stringini:Jeremy Blackburn,"Over the past few years, a number of new ""fringe"" communities, like 4chan or certain subreddits, have gained traction on the Web at a rapid pace. However, more often than not, little is known about how they evolve or what kind of activities they attract, despite recent research has shown that they influence how false information reaches mainstream communities. This motivates the need to monitor these communities and analyze their impact on the Web's information ecosystem. In August 2016, a new social network called Gab was created as an alternative to Twitter. It positions itself as putting ""people and free speech first"", welcoming users banned or suspended from other social networks. In this paper, we provide, to the best of our knowledge, the first characterization of Gab. We collect and analyze 22M posts produced by 336K users between August 2016 and January 2018, finding that Gab is predominantly used for the dissemination and discussion of news and world events, and that it attracts alt-right users, conspiracy theorists, and other trolls. We also measure the prevalence of hate speech on the platform, finding it to be much higher than Twitter, but lower than 4chan's Politically Incorrect board.",What is Gab: A Bastion of Free Speech or an Alt-Right Echo Chamber,NA:NA:NA:NA:NA:NA:NA,2018
Inaya Lahoud:Elsa Cardoso:Nada Matta,"Welcome to the 3rd Educational Knowledge Management (EKM) workshop, which takes place at the WWW18 conference, in Lyon, France. The first edition was in 2014 in conjunction with the International Conference on Knowledge Engineering and Knowledge Management (EKAW), which was held in Linköping, Sweden, and the second one with EKAW 2016, in Bologna, Italy. We received 6 papers from all around the world covering a broad range of topics. Each paper was reviewed by three members of the program committee. After the reviewing process, three papers were accepted for inclusion into the WWW proceedings volume, two of them as full papers and one as a short paper. The EKM2018 workshop will run as a half-day event, including the papers' presentation and two invited speaker sessions. A Best Paper Award will be assigned, and authors will be invited to submit an extended version of their work to a special issue that will be published as part of the ""International Journal of Continuing Engineering Education and Lifelong Learning."" The first paper Construction and Applications of TeKnowbase A Knowledge Base of Computer Science concepts' by Rajna Upadhyay, Ashutosh Bindal, Manjeet Kumar, and Maya Ramanath describes the development and evaluation of TeKnowbase, and how to use it in a variety of applications for learning a new topic, classification of technical text and querying and ranking computer science articles. The second one untitled ""Ontology-based recommender system in higher education"" by Charbel Obeid, Inaya Lahoud, Hicham El Khoury, and Pierre-Antoine Champin, is a position paper discussing an ontology-based recommender system to support a student's choice of major and university. The third paper ""Automatic Generation of Quizzes from DBpedia According to Educational Standards"" by Oscar Rodríguez Rocha and Catherine Faron Zucker focuses on educational quizzes. The authors present an approach to generate quizzes automatically from existing knowledge bases available on the Web of Linked Open Data (LOD), according to the official French educational standards.",3rd EKM Workshop Chairs' Welcome & Organization,NA:NA:NA,2018
Serge Garlatti:Jean Marie Gilliot:Sacha Kieffer:Jérôme Eneau:Genevieve Lameul:Partricia Serrano-Alvarado:Hala Skaf-Molli:Emmanuel Desmontils,NA,"Open Learner Models, Trust and Knowledge Management for Life Long Learning",NA:NA:NA:NA:NA:NA:NA:NA,2018
Elsa Cardoso,"Learning Analytics (LA) is a recent research field, in which Business Intelligence and Analytics techniques are applied to learners and their contexts, with the purpose of acquiring a greater insight about the entire learning process (including outcomes). In this talk, we explore the LA landscape, delving into the definitions, techniques, challenges, and lessons learned.","The Past, Present, and Future of Learning Analytics",NA,2018
Prajna Upadhyay:Ashutosh Bindal:Manjeet Kumar:Maya Ramanath,"In this paper, we make two main contributions. First, we describe the construction and evaluation of TeKnowbase, a knowledge-base of technical concepts in computer science. And second, we show how to use TeKnowbase in a variety of applications, including, generation of pre-requisite concepts for learning a new topic, classification of technical text and querying and ranking computer science articles.",Construction and Applications of TeKnowbase: A Knowledge Base of Computer Science Concepts,NA:NA:NA:NA,2018
Charbel Obeid:Inaya Lahoud:Hicham El Khoury:Pierre-Antoine Champin,"Academic advising is limited in its ability to assist students in identifying academic pathways. Selecting a major and a university is a challenging process rife with anxiety. Students at high school are not sure how to match their interests with their working future or major. Therefore, high school students need guidance and support. Moreover, students need to filter, prioritize and efficiently get appropriate information from the web in order to solve the problem of information overload. This paper represents an approach for developing ontology-based recommender system improved with machine learning techniques to orient students in higher education. The proposed recommender system is an assessment tool for students' vocational strengths and weaknesses, interests and capabilities. The main objective of our ontology-based recommender system is to identify the student requirements, interests, preferences and capabilities to recommend the appropriate major and university for each one.",Ontology-based Recommender System in Higher Education,NA:NA:NA:NA,2018
Oscar Rodríguez Rocha:Catherine Faron Zucker,"Educational quizzes are a powerful and popular tool to test the knowledge acquired by a learner and also to deepen her/his knowledge about a specific subject in an informal and entertaining way. Their production is a time-consuming task that can be automated by taking advantage of existing knowledge bases available on the Web of Linked Open Data (LOD). For these quizzes to be useful to learners, they must be generated according to the knowledge and skills defined by official educational standards for each subject and school year. This paper shows an approach to generate quizzes automatically according to the official French educational standards, from two different knowledge bases. Likewise, we show an evaluation of both knowledge bases.",Automatic Generation of Quizzes from DBpedia According to Educational Standards,NA:NA,2018
Franz Baader:Brigitte Grau:Yue Ma,NA,HQA18 Workshop Chairs' Welcome & Organization,NA:NA:NA,2018
Eric Gaussier,"Semantic annotation in the biomedical domain raises the problem of classifying texts with large-scale taxonomies, a problem sometimes referred to as extreme classification. In this presentation, we will give an overview of this problem and the main solutions proposed, with a focus on textual collections and the BioASQ challenge.",Semantic Annotation in the Biomedical Domain: Large-scale Classification and BioASQ,NA,2018
Andreas Both,"In the past, the research, as well as industry, brought Question Answering (QA) into daily use. However, there is still an obvious gap between the claim of providing access to any--structured or unstructured--knowledge stored in the world using an interface fitting the demands of regular users. On the one hand side, implementing Question Answering Systems is still hard and time-consuming, on the other hand side, the QA community is still struggling on defining a common ground for collaboration across research fields regarding, for example, realistic benchmarks, maintainability, and broad coverage of knowledge sources. In the talk, challenges of Question Answering will be highlight w.r.t. hybrid QA, domain-specific QA, cross knowledge base QA, etc. Particularly the industry perspective is also presented while aiming at a Question Answering platform which can be assembled from industry components as well as components of the research community. First steps towards this long-term vision are provided by the Qanary framework and similar frameworks aiming at a collaborative approach for the development QA systems which should lead to effective implementations, improved research results as well as a platform economy for QA. Such a platform would industry lead to a tighter collaboration while academics would have the opportunity of accessing precious data for further improvements.","Towards Component-based, Domain-specific, Efficient Question Answering Systems",NA,2018
Phuong Le-Hong:Duc-Thien Bui,"In this paper, we describe the development of an end-to-end factoid question answering system for the Vietnamese language. This system combines both statistical models and ontology-based methods in a chain of processing modules to provide high-quality mappings from natural language text to entities. We present the challenges in the development of such an intelligent user interface for an isolating language like Vietnamese and show that techniques developed for inflectional languages cannot be applied as is. Our question answering system can answer a wide range of general knowledge questions with promising accuracy on a test set.",A Factoid Question Answering System for Vietnamese,NA:NA,2018
Zhen Jia:Abdalghani Abujabal:Rishiraj Saha Roy:Jannik Strötgen:Gerhard Weikum,"Answering complex questions is one of the challenges that question-answering (QA) systems face today. While complexity has several facets, question dimensions like temporal and spatial intents necessitate specialized treatment. Methods geared towards such questions need benchmarks that reflect the desired aspects and challenges. Here, we take a key step in this direction, and release a new benchmark, TempQuestions, containing 1,271 questions, that are all temporal in nature, paired with their answers. As a key contribution that enabled the creation of this resource, we provide a crisp definition for temporal questions. Most questions require decomposing them into sub-questions, and the questions are of a kind that they would be best evaluated on a combination of structured data and unstructured text sources. Experiments with two QA systems demonstrate the need for further research on complex questions.",TempQuestions: A Benchmark for Temporal Question Answering,NA:NA:NA:NA:NA,2018
Atsushi Otsuka:Kyosuke Nishida:Katsuji Bessho:Hisako Asano:Junji Tomita,"We propose a novel Frequently Asked Question (FAQ) retrieval technique with a neural query expansion model. With the growth in Question Answering systems and mobile communications, FAQ retrieval systems have become widely used in site searches and call center support. However, FAQ retrieval often has lexical gaps between queries and answer documents. To bridge these gaps, we design a query expansion model on the basis of an Encoder-Decoder model as a type of deep neural network. The model learns the words that appear in answers for questions using Q&A pair documents and generates the expanded queries from inputted queries to retrieve answer documents. We evaluate our proposed technique in a multi-domain FAQ retrieval task. Experimental results show that our technique retrieves FAQs more accurately than the previous methods.",Query Expansion with Neural Question-to-Answer Translation for FAQ-based Question Answering,NA:NA:NA:NA:NA,2018
Franz Baader:Stefan Borgwardt:Walter Forkel,"Finding suitable candidates for clinical trials is a labor-intensive task that requires expert medical knowledge. Our goal is to design (semi-)automated techniques that can support clinical researchers in this task. We investigate the issues involved in designing formal query languages for selecting patients that are eligible for a given clinical trial, leveraging existing ontology-based query answering techniques. In particular, we propose to use a temporal extension of existing approaches for accessing data through ontologies written in Description Logics. We sketch how such a query answering system could work and show that eligibility criteria and patient data can be adequately modeled in our formalism.",Patient Selection for Clinical Trials Using Temporalized Ontology-Mediated Query Answering,NA:NA:NA,2018
Martino Mensio:Giuseppe Rizzo:Maurizio Morisio,"QA systems offer a human friendly interface to navigate through knowledge, which can range from encyclopedic to domain-specific. Generally, a QA system is designed to provide an answer to a specific question once (so-called single turn) and state-of-the-art systems reach nowadays robust performance in such a scenario. However, most of the interactions with QA systems are based on multiple handshakes of question/answer pairs, where the human being refines the questions further, while the system can collect the necessary information and generate a compelling final answer through multiple turns. In this paper, we investigate and experiment a multi-turn QA system that is suited to work given a particular domain of knowledge and configurable goals. Our approach models the entire dialogue as a sequence of turns, i.e. questions and answers, using a Recurrent Neural Network which is firstly trained to understand natural language, classifying entities and intents using prior knowledge of domain-specific interactions, and provide answers according to the domain used as background knowledge. We have compared our approach with state-of-the-art sequence-based intent classification using a well-known and standardized gold standard observing an increase of 17.16% of F1. Results show the robustness of the approach and the competitive results motivate the adoption in multi-turn QA scenarios.",Multi-turn QA: A RNN Contextual Approach to Intent Classification for Goal-oriented Systems,NA:NA:NA,2018
Brigitte Grau:Anne-Laure Ligozat,"Question answering has been the focus of a lot of researches and evaluation campaigns, either for text-based systems (TREC and CLEF evaluation campaigns for example), or for knowledge-based systems (QALD, BioASQ). Few systems have effectively combined both types of resources and methods in order to exploit the fruitfulness of merging the two kinds of information repositories. The only evaluation QA track that focuses on hybrid QA is QALD since 2014. As it is a recent task, few annotated data are available (around 150 questions). In this paper, we present a question answering dataset that was constructed to develop and evaluate hybrid question answering systems. In order to create this corpus, we collected several textual corpora and augmented them with entities and relations of a knowledge base by retrieving paths in the knowledge base which allow to answer the questions. The resulting corpus contains 4300 question-answer pairs and 1600 have a true link with DBpedia.",A Corpus for Hybrid Question Answering Systems,NA:NA,2018
Dennis Diefenbach:Kamal Singh:Pierre Maret,"In the last two decades a new part of the web grew significantly, namely the Semantic Web. It contains many Knowledge Bases (KB) about different areas like music, books, publications, live science and many more. Question Answering (QA) over KBs is seen as the most promising approach to bring this data to end-users. We describe WDAqua-core1, a QA service for querying RDF knowledge-bases. It is multilingual, it supports different RDF knowledge bases and it understands both full natural language questions and keyword questions.",WDAqua-core1: A Question Answering service for RDF Knowledge Bases,NA:NA:NA,2018
Sanjay Kamath:Brigitte Grau:Yue Ma,"Extractive Question Answering (QA) focuses on extracting precise answers from a given paragraph to questions posed in natural language. Deep learning models are widely used to address this problem and can fetch good results, provided there exists enough data for learning. Such large datasets have been released in open domain, but not in specific domains, such as the medical domain. However, the medical domain has a great amount of resources such as UMLS thesaurus, ontologies such as SNOMED CT, and tools such as Metamap etc that could be useful. In this paper, we apply transfer learning for getting a DNN baseline system on biomedical questions and we study if structured resources can help in selecting the answers based on the recognition of the Expected Answer Type (EAT), which has been proved useful in open domain QA systems. This study relies on different representations for LAT and we study if gold standard answers and answers of our model have some positive impact from the LAT.",Verification of the Expected Answer Type for Biomedical Question Answering,NA:NA:NA,2018
Lora Aroyo:Gianluca Demartini:Anna Lisa Gentile:Chris Welty,"It is our great pleasure to welcome you to the WWW 2018 Augmenting Intelligence with Humans-in-the-loop ([email protected]WWW2018), http://w3id.org/huml/HumL-WWW2018/ The workshop program includes two invited talks. Praveen Paritosh (Google Research) explores the right incentives to motivate human contribution to create knowledge resources. Elena Simperl (University of Southampton) surveys how humans and bots contribute together to the development of the Wikidata knowledge graph. Seven full papers and one short paper were accepted, covering a wide range of topics related to the efficient and effective combination of the strong sides of both machine and crowd computation. Empirical results were provided and discussed with respect to (1) methods for data quality ensurance and labeling task efficiency, (2) the role of gamification elements for improving crowd performance as well as the role of quantum mathematics to simulate human behavior.",Augmenting Intelligence with Humans-in-the-Loop ([email protected]) Chairs' Welcome & Organization,NA:NA:NA:NA,2018
Lora Aroyo:Chris Welty,"AI and collective intelligence systems universally suffer from a deficiency of context. There are innumerable possible contexts that may possibly change the interpretation of some signal, that may change the proper response to some stimulus. For example, an image understanding system that does not recognize an arrest event in a zoomed image of a person's face. How is it possible to know there is more information, outside of what the system can access, that affects the interpretation of data The solution to the context problem in practice today is a pragmatic, engineering one: analyze errors (in recommendations, question answers, image recognition, etc.), classify the kinds of contextual information that caused the wrong behavior, find the most common type of context that causes errors, and add information about that kind of context to the system. Clearly this approach is neither general nor scalable, and ignores the infamous long tail of possible contextual information that may affect a system's understanding and its behavior. In this paper we outline a new, more general, approach to recognizing context. The approach is grounded in a fairly simple intuition: the mathematics underlying quantum mechanics is far more appropriate for modeling, and therefore simulating, human cognitive behavior than the standard toolset from classical statistics. Notions such as Heisenberg's uncertainty principle, superpositions of states, and entanglement have direct and measurable analogs in collective intelligence.",The Quantum Collective,NA:NA,2018
Praveen Paritosh,"Dictionaries, encyclopedias, knowledge graphs, annotated corpora, library classification systems and world maps are all examples of human-curated knowledge resources that have been highly valuable to science as well as amortized across multiple large-scale systems in practice. Many of these were started and built even before a crowdsourcing research community existed. While the last decade has seen unprecedented growth in research and practice in building crowdsourcing systems to do increasingly complex tasks at scale, many of these resources are still woefully incompletelacking coverage in languages and subject matter domains. Moreover, many knowledge resources needed to fill other semantic gaps for artificial intelligence systems simply don't exist or arent being built. Why I argue that we don't have the right incentives, and that in order to improve the incentives, we have some fundamental scientific questions to answer. While building a large knowledge resource, we have little more than intuitions when it comes to estimating the reusability, maintainability, and long-term value of the effort. These make it difficult to fund or manage such projects, often requiring herculean personalities or fortunate businesses. Building or expanding a resource is often not seen as ""sexy,"" which results in lack of resources to answer those questions in any principled manner. These problems begin to outline a new science of curation, making progress on which could help improve the discussion around and funding for building sorely needed knowledge resources.",The Missing Science of Knowledge Curation: Improving Incentives for Large-scale Knowledge Curation,NA,2018
Elena Simperl,"Wikidata is one of most successful knowledge graphs ever created. It expresses knowledge in the form of subject-property-value statements accompanied by provenance information. A project of the Wikimedia Foundation, Wikidata is supported by a community of currently 19 thousand active users and 234 bots, who together are responsible for editing more than 45 million entities since the start of the project in 2012. This makes Wikidata a prime example for what human-in-the-loop technology can achieve. In this talk, we are going to present several studies that aim to understand the links between its socio-technical fabric and its success.",Loops of Humans and Bots in Wikidata,NA,2018
Amrapali Zaveri:Pedro Hernandez Serrano:Manisha Desai:Michel Dumontier,"Crowdsourcing involves the creating of HITs (Human Intelligent Tasks), submitting them to a crowdsourcing platform and providing a monetary reward for each HIT. One of the advantages of using crowdsourcing is that the tasks can be highly parallelized, that is, the work is performed by a high number of workers in a decentralized setting. The design also offers a means to cross-check the accuracy of the answers by assigning each task to more than one person and thus relying on majority consensus as well as reward the workers according to their performance and productivity. Since each worker is paid per task, the costs can significantly increase, irrespective of the overall accuracy of the results. Thus, one important question when designing such crowdsourcing tasks that arise is how many workers to employ and how many tasks to assign to each worker when dealing with large amounts of tasks. That is, the main research questions we aim to answer is: 'Can we a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks'. Thus, we introduce a two-staged statistical guideline, CrowdED, for optimal crowdsourcing experimental design in order to a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks. We describe the algorithm and present preliminary results and discussions. We implement the algorithm in Python and make it openly available on Github, provide a Jupyter Notebook and a R Shiny app for users to re-use, interact and apply in their own crowdsourcing experiments.",CrowdED: Guideline for Optimal Crowdsourcing Experimental Design,NA:NA:NA:NA,2018
Alexandros Chortaras:Anna Christaki:Nasos Drosopoulos:Eirini Kaldeli:Maria Ralli:Anastasia Sofou:Arne Stabenau:Giorgos Stamou:Vassilis Tzouvaras,"The transformation that has been accomplished in Cultural Heritage (CH) during the last decades has resulted in the production of vast amounts of content from many different cultural institutions, such as museums, libraries and archives. A large part of this rich content has been aggregated in digital platforms that serve as cross-domain hubs, which however offer limited usability and accessibility of content due to insufficient data and metadata quality. In our effort to make CH more accessible and reusable, we introduce WITH, an aggregation platform that provides enhanced services and enables human-computer collaboration for data annotations and enrichment. WITH excels existing cultural content aggregation platforms by advancing digital cultural data through the combination of artificial intelligence automation and creative user engagement, thus facilitating its accessibility, visibility, and re-use. In particular, by using image and free text analysis methodologies for automatic metadata enrichment, in accordance to the human expertise for enrichment and validation through crowdsourcing approaches with gamification elements, WITH combines the intelligence of humans and computers to improve the quality of digital cultural content and its presentation, establishing new ways of collaboration between cultural organizations and their audiences.",WITH: Human-Computer Collaboration for Data Annotation and Enrichment,NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Rafael Zequeira Jiménez:Laura Fernández Gallardo:Sebastian Möller,"Crowdsourcing provides an exceptional opportunity for the rapid collection of human input for data acquisition and labelling. This approach have been adopted in multiple domains and researchers are now able to reach a demographically diverse audience at low cost. However, it remains the question of whether the results are still valid and reliable. Previous work have introduced different mechanisms to ensure data reliability in crowdsourcing. This work examines to which extend, ""trapping question"" or ""outliers detection"" assure reliable results to the detriment of, overloading task content with stimuli that are not of interest for the researcher, or by discarding data points that might be the true opinion of a worker. To this end, a speech quality assessment study have been conducted in a web crowdsourcing platform, following the ITU-T Rec. P.800. Workers assessed the speech stimuli of the database 501 from the ITU-T Rec. P.863. We examine results' validity in terms of correlations to previous ratings collected in laboratory. Our outcomes shows that neither of the techniques under investigation improve results accuracy by itself, but a combination of both. Our goal is to provide empirical guidance for designing experiments in crowdsourcing while ensuring data reliability.",Outliers Detection vs. Control Questions to Ensure Reliable Results in Crowdsourcing.: A Speech Quality Assessment Case Study,NA:NA:NA,2018
Ismini Lourentzou:Daniel Gruhl:Steve Welch,"Domain-specific relation extraction requires training data for supervised learning models, and thus, significant labeling effort. Distant supervision is often leveraged for creating large annotated corpora however these methods require handling the inherent noise. On the other hand, active learning approaches can reduce the annotation cost by selecting the most beneficial examples to label in order to learn a good model. The choice of examples can be performed sequentially, i.e. select one example in each iteration, or in batches, i.e. select a set of examples in each iteration. The optimization of the batch size is a practical problem faced in every real-world application of active learning, however it is often treated as a parameter decided in advance. In this work, we study the trade-off between model performance, the number of requested labels in a batch and the time spent in each round for real-time, domain specific relation extraction. Our results show that the use of an appropriate batch size produces competitive performance, even compared to a fully sequential strategy, while reducing the training time dramatically.",Exploring the Efficiency of Batch Active Learning for Human-in-the-Loop Relation Extraction,NA:NA:NA,2018
Giorgio Maria Di Nunzio:Maria Maistro:Federica Vezzani,"Supervised machine learning algorithms require a set of labelled examples to be trained; however, the labelling process is a costly and time consuming task which is carried out by experts of the domain who label the dataset by means of an iterative process to filter out non-relevant objects of the dataset. In this paper, we describe a set of experiments that use gamification techniques to transform this labelling task into an interactive learning process where users can cooperate in order to achieve a common goal. To this end, first we use a geometrical interpretation of Naïve Bayes (NB) classifiers in order to create an intuitive visualization of the current state of the system and let the user change some of the parameters directly as part of a game. We apply this visualization technique to the classification of newswire and we report the results of the experiments conducted with different groups of people: PhD students, Master Degree students and general public. Then, we present a preliminary experiment of query rewriting for systematic reviews in a medical scenario, which makes use of gamification techniques to collect different formulation of the same query. Both the experiments show how the exploitation of gamification approaches help to engage the users in abstract tasks that might be hard to understand and/or boring to perform.",A Gamified Approach to Naïve Bayes Classification: A Case Study for Newswires and Systematic Medical Reviews,NA:NA:NA,2018
Roberto Enea:Maria Teresa Pazienza:Andrea Turbati:Alessandro Colantonio,"Creating ontologies is an essential while challenging task to be performed by either a human or a system: on one hand it is excessively burdensome for a human operator, on the other it is very complex also for a machine due to the not negligible amount of ""uncertainty"" that it must be able to manage. In the last years, some attempts have been made to automate this process, but at present, due to the large number of aspects to be covered in the automatic creation of an ontology (such as Domain terminology extraction, Concept discovery, Concept hierarchy derivation, "") satisfactory solutions have not been reached yet. In order to produce efficient tools for both creation and enrichment of ontologies, the participation of the human in such a process still seems necessary. Our approach, that foresees a broader framework for ontology learning, is based by first on the automatic extraction of triples from heterogeneous sources, then on the presentation of the most reliable triples to the human operator for validation purposes. The system provides the user with a series of graphical representations that can give him an overview of the level of uncertainty of the automatically generated ontology. Then provides the user with the possibility to perform SPARQL what-if queries, (i.e. assuming as true the triples filtered according to the level of confidence, the source and the structure of the triples).","How to Support Human Operator in ""Uncertainty"" Managing during the Ontology Learning Process",NA:NA:NA:NA,2018
Wei Sun:Ying Li:Anshul Sheopuri:Thales Teixeira,"Making successful video advertisements has long been considered a combination of art and business acumen. In this work, we propose a system to assist human designers to produce more effective advertisements with predictable outcomes. We formalize this concept with a dynamic Bayesian network (DBN), where we represent the knowledge base with data collected from large-scale field experiments in a novel setting. Face and eye tracking which continuously measures viewers emotional responses and viewing interest on 169 television advertisements for 2334 participants, along with moment-to-moment branding activities in the advertisements are used to estimate the model. The resulting DBN represents relationships across advertisement content, viewers emotional responses, as well as effectiveness metrics such as ad avoidance, sharing and influence on purchase. Conditioned on the specified requirement on the ad, a human designer can draw high scoring samples from the DBN, which represent the optimized sequences of branding activities and entertainment content.",Computational Creative Advertisements,NA:NA:NA:NA,2018
Luis-Daniel Ibáñez:John Domingue:Pascal Molli,"It is our great pleasure to welcome you to the WWW 2018 3rd International Workshop on Linked Data and Distributed Ledgers (LD-DL). We envision the workshop as a forum for researchers and practitioners from Distributed Ledgers and Linked Data to come together to discuss common challenges; propose solutions to shortcomings of existing architectures; and identify synergies for joint initiatives. The ultimate goal is the creation of a Web of Interoperable Ledgers. We received 6 submissions from all around the world. We evaluated them regarding relevance, quality, and novelty, selecting 3 short papers and 1 long paper (66% acceptance rate) --ScienceMiles - Digital currency for researchers--Can Blockchains and Linked Data Advance Taxation? --A distributed database with explicit semantics and chained RDF graphs--When trust saves energy: A Reference Framework for Proof of Trust (PoT) Blockchains. We hope that you will find the tutorial program interesting, providing you with a valuable opportunity to learn and share ideas with other researchers and practitioners from institutions around the world.",3rd International Workshop on Linked Data and Distributed Ledgers Chairs' Welcome & Organization,NA:NA:NA,2018
Leila Bahri:Sarunas Girdzijauskas,"Blockchains are attracting the attention of many technical, financial, and industrial parties, as a promising infrastructure for achieving secure peer-to-peer (P2P) transactional systems. At the heart of blockchains is proof-of-work (PoW), a trustless leader election mechanism based on demonstration of computational power. PoW provides blockchain security in trusless P2P environments, but comes at the expense of wasting huge amounts of energy. In this research work, we question this energy expenditure of PoW under blockchain use cases where some form of trust exists between the peers. We propose a Proof-of-Trust (PoT) blockchain where peer trust is valuated in the network based on a trust graph that emerges in a decentralized fashion and that is encoded in and managed by the blockchain itself. This trust is then used as a waiver for the difficulty of PoW; that is, the more trust you prove in the network, the less work you do.",When Trust Saves Energy: A Reference Framework for Proof of Trust (PoT) Blockchains,NA:NA,2018
Mirek Sopek:Przemyslaw Gradzki:Witold Kosowski:Dominik Kuziski:Rafa Trójczak:Robert Trypuz,"In this paper we present a new idea of creating a Blockchain compliant distributed database which exposes its data with explicit semantics, is easily and natively accessible, and which applies Blockchain securitization mechanisms to the RDF graph data model directly, without additional packaging or specific serialisation. Essentially, the resulting database forms the linked chain of named RDF graphs and is given a name: GraphChain. Such graphs can then be published with the help of any standard mechanisms using triplestores or as linked data objects accessible via standard web mechanisms using the HTTP protocol to make them available on the web. They can also be easily queried using techniques like SPARQL or methods typical to available RDF graphs frameworks (like rdflib, Apache Jena, RDF4J, OWL API, RDF HDT, dotnetRDF and others). The GraphChain concept comes with its own, OWL-compliant ontology that defines all the structural, invariant elements of the GraphChain and defines their basic semantics. The paper describes also a few simple, prototypical GraphChain implementations with examples created using Java, .NET/C# and JavaScript/Node.js frameworks.",GraphChain: A Distributed Database with Explicit Semantics and Chained RDF Graphs,NA:NA:NA:NA:NA:NA,2018
Michał R. Hoffman,"Permissioned distributed ledgers (permissioned blockchains) supporting smart contracts that automatically adjust accounts and coordinate records among multiple parties, present a valid platform opportunity for establishing a fully digital tax regime. We propose a permissioned blockchain-based system aimed at eliminating some of the losses that tax authorities globally are currently struggling with. These multi-billion flaws manifest themselves as the tax gap, or the inability to collect the full amount that is owed by a given entity to a particular authority. Illegitimate or inefficient tax operations could be prevented with a global suite of smart contracts deployed on top of a consortium distributed ledger with on-chain governance. We also introduce the vision for a VAT Invoice 2.0 modelled as a Linked Data document. A tax reference generated by a smart contract would allow anyone with the right permissions to immediately investigate the entire commercial chain for any taxable item on an ontology-based tax document.",Can Blockchains and Linked Data Advance Taxation,NA,2018
Zeeshan Jan:Allan Third:Luis-Daniel Ibanez:Michelle Bachler:Elena Simperl:John Domingue,"Peer-reviewing is a community-driven activity where volunteer researchers assess the work of other researchers. Peer-reviewing is an important and time-consuming activity that has very little recognition. This lack of incentive may lead to poor-quality reviews and frustration from researchers. In this paper, we envision ScienceMiles, a Blockchain-based platform to manage the incentivization of peer-reviewers through a crypto-currency.",ScienceMiles: Digital Currency for Researchers,NA:NA:NA:NA:NA:NA,2018
Dirk Ahlers:Erik Wilde:Rossano Schifanella:Jalal S. Alowibdi:Muhammad Zubair Shafiq,"It is our great pleasure to welcome you to the 8th International Workshop on Location and the Web (LocWeb2018) at WWW 2018. LocWeb 2018 will continue a successful workshop series at the intersection of location-based services and Web architecture. It focuses on Web-scale services and systems facilitating location-aware information access as well as on Spatial Social Behavior Analytics on the Web as part of social computing. The location topic is seen as a cross-cutting issue equally concerning information access, semantics and standards, social analysis and mining, and Web-scale systems and services. The workshop is an integrated venue where location and spatio-social aspects can be discussed in depth with an interested community. New application areas for Web architecture, such as the Internet of Things (IoT) and the Web of Things (WoT), will lead to increasingly rich and large sets of applications for which location is highly relevant as the connection to the physical world. Location has high importance in Web-based designs, and it continues to provide challenging research questions.",LocWeb2018 Chairs' Welcome & Organization,NA:NA:NA:NA:NA,2018
Luca Rossi:Eric Boscaro:Andrea Torsello,"The last decade has seen a huge expansion in the use of social media to extract data about human behaviour. While metadata and textual information have taken the lion's share as data sources for social media analysis, geotagged image-based platforms represent an unprecedented and as yet almost untapped source of data to analyse human behaviour and characterise the physical space we live in. In this paper we investigate the use of Instagram photos to analyse tourism consumption. We take the city of Venice (Italy) as a case study and we collect a dataset of about 90k photos taken between January 2014 and December 2015. Using computer vision techniques, we build a supervised classifier which assigns each photo to one of six different categories. We then observe how the frequency and spatial distribution of these categories varies with time. This in turn allows us to confirm the existence of a number of touristic hotspots associated with different events, such as Venice Carnival and Biennale. Our analysis also uncovers the existence of touristic flows associated with these events, such as the Folklore Line that marks the path of tourists from ""Santa Lucia"" railway station to ""San Marco"" square during the Carnival period. Overall, our findings confirm the effectiveness of the proposed framework to investigate tourism consumption using Instagram data.",Venice through the Lens of Instagram: A Visual Narrative of Tourism in Venice,NA:NA:NA,2018
Kendall Taylor:Kwan Hui Lim:Jeffrey Chan,"Travelling and touring are popular leisure activities enjoyed by millions of tourists around the world. However, the task of travel itinerary recommendation and planning is tedious and challenging for tourists, who are often unfamiliar with the various Points-of-Interest (POIs) in a city. Apart from identifying popular POIs, the tourist needs to construct a travel itinerary comprising a subset of these POIs, and to order these POIs as a sequence of visits that can be completed within his/her available touring time. For a more realistic itinerary, the tourist also has to account for travelling time between POIs and visiting times at individual POIs. Furthermore, this itinerary should incorporate tourist preferences such as desired starting and ending POIs (e.g., POIs that are near the tourist's hotel) and a subset of must-see POIs (e.g., popular POIs that a tourist must visit). We term this the TourMustSee problem, which is based on a variant of the Orienteering problem. Following which, we propose the LP+M algorithm for solving the TourMustSee problem as an Integer Linear Program (ILP). Using a Flickr dataset of POI visits in seven touristic cities, we compare LP+M against various ILP-based baselines, and the results show that LP+M recommends better travel itineraries in terms of POI popularity, total POIs visited, total touring time utilized and must-visit POI(s) inclusion.",Travel Itinerary Recommendations with Must-see Points-of-Interest,NA:NA:NA,2018
Antonio La Salandra:Piero Fraternali:Darian Frajberg,"Location-based mobile outdoor applications are powerful tools that can engage users in social and environmental tasks and support the emerging paradigm of citizen science. In this paper we present PeakLensVR, a virtual reality location-based mobile app that enables users to capture with their mobile phone panoramic mountain images and later visualize such images, enriched with metadata about the peaks visible from the capture point, with a low-end VR device. The goal of PeakLensVR is to harness the emerging trend of geo-located augmented and virtual reality applications to foster a community of environmentally conscious users who volunteer in the collection of mountain images for environment monitoring purposes.",A Location-Based Virtual Reality Application for Mountain Peak Detection,NA:NA:NA,2018
Guido Boella:Louise Francis:Elena Grassi:Axel Kistner:Andreas Nitsche:Alexey Noskov:Luigi Sanasi:Adriano Savoca:Claudio Schifanella:Ioannis Tsampoulatidis,"In this paper we describe the advancement of WeGovNow, an Horizon 2020 European Union project involving twelve partners from Germany, Sweden, Greece, Italy and United Kingdom, aimed at using state-of-the-art digital technologies in community engagement platforms to involve citizens in decision making processes within their local neighbourhood. Different software components, both previously existing and developed specially for the project and covering separate aspects of community engagement, were integrated in a single web platform offering an homogeneous experience to the users. One of the main common threads beyond this integration process is the ability to collect crowd mapped information and show them back to the users in an engaging way on maps, harmonizing data coming from the different components and making the mapped space easily explorable.",WeGovNow: A Map Based Platform to Engage the Local Civic Society,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Eduardo Graells-Garrido:Diego Caro:Omar Miranda:Rossano Schifanella:Oscar F. Peredo,"People fulfill their informational needs through smartphones, however, little is known regarding how the urban fabric and the activities that take place in it affect the usage of mobile applications. In this regard, starting from an anonymized dataset of Deep Packet Inspection (DPI) data from the largest telecommunications operator in Chile, we focus on the following questions: What are the most popular applications used in the city Where are they spatially clustered When does an application is more frequently used And How does the urban context and the mobility patterns relate to application usage As a result, we observed that specific applications present high spatial clustering, while the most popular services are geographically dispersed throughout the entire city. Clusters appear in places of high floating population; however, hotspots vary in space depending on the application. Interestingly, we found that commuting plays an important role, both in terms of rush hours and transportation infrastructure. We present a discussion on these results, focusing on how the physical space and the daily commuting routine affect the pattern of data consumption and represent an important aspect in mobile users behavioral studies.","The WWW (and an H) of Mobile Application Usage in the City: The What, Where, When, and How",NA:NA:NA:NA:NA,2018
Martin Atzmueller:Sabrina Gaito:Roberto Interdonato:Rushed Kanawati:Christine Largeron:Matteo Magnani:Alessandra Sala,"Attributed network models have seen an increasing success in recent years, thanks to their informative power and to their ability to model complex networked relations that characterize most real-world phenomena. Their use has been attractive to communities in different disciplines such as computer science, physics, social science, as well as in interdisciplinary research environments. The use of such models has been also supported by the increasing easiness in collecting multirelational data from the Web, e.g., from online social media platforms, crowdsourced data, online knowledge bases; within this view, the World Wide Web is an inestimable source of information, which can be conveniently represented with feature-rich network models, e.g., enclosing temporal aspects of the data, quantitative and/or qualitative properties of nodes, different relations between a common set of entities, different existence probabilities, or modeling connection between different entity types.",International Workshop on Mining Attributed Networks (MATNET 2018) Chairs' Welcome,NA:NA:NA:NA:NA:NA:NA,2018
Amani H. B. Eissa:Mohamed E. El-Sharkawi:Hoda M. O. Mokhtar,"Social networks can be modeled as attributed networks whose nodes represent users, edges represent relationships among users (e.g. friendship/follow) and attribute vectors hold properties of nodes and/or edges. In this paper, we consider friends' recommendation based on interest-based communities generated from topic based attributed social networks (TbASN). In our model, an attribute vector is not just a container for explicit users' profile data that is stored in social network's dataset, but rather holds topic vectors that are derived from analyzing the implicit interest of users' that are aggregated from his/her posts on the social network (e.g. tweets in Twitter, posts in Facebook). In our framework, topics of interest are represented as a hierarchy of topics (Topics/Subtopics) forming hierarchical interest-based communities. Users within each interest-based community are clustered according to their profile features (age, location, education etc.). Those clusters are later used in recommendations where recommendations target members of the same cluster to guarantee the quality and coherence of recommendations. In addition, we propose a recommendation selection approach to handle the large number of recommended candidates. The main advantage of the proposed approach is that it considers multiple criteria for candidate selection including the number of common communities, the resemblance in basic features, as well as network proximity. In addition to recommending friends of similar interests, frequent pattern mining is used to discover frequently occurring interests in order to be used in recommending communities for users to join. Although our approach is generic and can be applied to most of the existing social networks, we used Twitter as our target social network.",Towards Recommendation Using Interest-Based Communities in Attributed Social Networks,NA:NA:NA,2018
Jihwan Lee:Sunil Prabhakar,"Network embedding aims to learn low-dimensional vector representations for nodes in a network that preserve structural characteristics. It has been shown that such representations are helpful in several graph mining tasks such as node classification, link prediction, and community detection. Some recent works have attempted to extend the approach to attributed networks in which each node is associated with a set of attribute values. They have focused on homophily relationships by forcing nodes with similar attribute values to obtain similar vector representations. This is unnecessarily restrictive and misses the opportunity to harness other types of relationships revealed by patterns in attribute values of connected nodes for learning insightful relationships. In this paper, we propose a new network attributed embedding framework called A3embed that is aware of attribute associations. A3embed favors significant attribute associations, not merely homophily relationships, which contributes to its robustness to diverse attribute vectors and noisy links. The experimental results on real-world datasets demonstrate that the proposed framework achieves better performance on different graph mining tasks compared to existing models.",A3embed: Attribute Association Aware Network Embedding,NA:NA,2018
Rajesh Sharma:Danilo Montesi,"One of the important problems in the domain of network science is the community detection. In the past, various topological based community detection algorithms have been proposed. Recently, researchers have taken into account at- tributes of the nodes while proposing community detection algorithms. In this work, we investigate if the nodes in a community, identified through topology based algorithms al- so exhibit attribute similarity. Using four different kinds of similarity metrics, we analyse the attribute similarity of the nodes within the communities derived using five different types of topological based community detection algorithms. Based on our analysis of three real social network datasets, we found on an average of 50% attribute similarity among the nodes in the communities.",Investigating Similarity of Nodes' Attributes in Topological Based Communities.,NA:NA,2018
Ryuta Matsuno:Tsuyoshi Murata,"Network embedding is a method for converting nodes in a network into low dimensional vectors, preserving its structure and the similarities among the nodes. Embedding is widely used in many applications, e.g., social network analysis and knowledge discovery. Because of its wide usage, many studies have been proposed, such as DeepWalk, LINE and node2vec. These works are designed for single-layer networks, however, real world networks often possess not just one, but multiple types of connections. Hence it is more appropriate to represent them as multiplex networks, which consist of multiple layers each of which represents one type of relationship. Embedding multiplex networks is difficult because all layer structures have to be taken into consideration. In this paper, we propose MELL, a novel embedding method for multiplex networks, which incorporates an idea of layer vector that captures and characterizes each layer's connectivity. This method exploits the overall structure effectively, and embeds both directed and undirected multiplex networks, whether their layer structures are similar or complementary. We focus on link prediction tasks and test our method and other baseline methods using five data sets from different domains. The results show that our method outperforms all of the baseline methods for all of the data sets.",MELL: Effective Embedding Method for Multiplex Networks,NA:NA,2018
Manish Kumar:Anurag Singh:Hocine Cherifi,"When an epidemic occurs, it is often impossible to vaccinate the entire population due to limited amount of resources. Therefore, it is of prime interest to identify the set of influential spreaders to immunize, in order to minimize both the cost of vaccine resource and the disease spreading. While various strategies based on the network topology have been introduced, few works consider the influence of the community structure in the epidemic spreading process. Nowadays, it is clear that many real-world networks exhibit an overlapping community structure, in which nodes are allowed to belong to more than one community. Previous work shows that the numbers of communities to which a node belongs is a good measure of its epidemic influence. In this work, we address the effect of nodes in the neighborhood of the overlapping nodes on epidemics spreading. The proposed immunization strategy provides highly connected neighbors of overlapping nodes in the network to immunize. The whole process requires information only at the node level and is well suited to large-scale networks. Extensive experiments on four real-world networks of diverse nature have been performed. Comparisons with alternative local immunization strategies using the fraction of the Largest Connected Component (LCC) after immunization,show that the proposed method is much more efficient. Additionally, it compares favorably to global measures such as degree and betweenness centrality.",An Efficient Immunization Strategy Using Overlapping Nodes and Its Neighborhoods,NA:NA:NA,2018
Lisette Espín-Noboa:Claudia Wagner:Fariba Karimi:Kristina Lerman,"Relational inference leverages relationships between entities and links in a network to infer information about the network from a small sample. This method is often used when global information about the network is not available or difficult to obtain. However, how reliable is inference from a small labeled sample How should the network be sampled, and what effect does it have on inference error How does the structure of the network impact the sampling strategy We address these questions by systematically examining how network sampling strategy and sample size affect accuracy of relational inference in networks. To this end, we generate a family of synthetic networks where nodes have a binary attribute and a tunable level of homophily. As expected, we find that in heterophilic networks, we can obtain good accuracy when only small samples of the network are initially labeled, regardless of the sampling strategy. Surprisingly, this is not the case for homophilic networks, and sampling strategies that work well in heterophilic networks lead to large inference errors. This finding suggests that the impact of network structure on relational classification is more complex than previously thought.",Towards Quantifying Sampling Bias in Network Inference,NA:NA:NA:NA,2018
Henry Soldano:Guillaume Santini:Dominique Bouthinon:Sophie Bary:Emmanuel Lazega,"In two-mode networks there are two kinds of vertices, i.e objects, each being possibly described with a proper attribute set. This means that to select a subnetwork according to vertex descriptions we have to consider a pair of vertex subsets. A common technique is to extract from a network an essential subnetwork, the core subgraph of the network. Formal Concept Analysis and closed pattern mining were previously applied to networks with the purpose of reducing extensions of patterns to be core subgraphs. To apply this methodology to two-mode networks, we need to consider the two vertex subsets of two-mode cores and define accordingly abstract closed bi-patterns. Each component of a bi-pattern is then associated to one mode. We also show that the same methodology applies to hub-authority cores of directed networks in which each vertex subset is associated to a role (in or out). We illustrate the methodology both on a two-mode network of epistemological data and on a directed advice network of lawyers.",Bi-Pattern Mining of Two Mode and Directed Networks,NA:NA:NA:NA:NA,2018
Didier Henry:Erick Stattner:Martine Collard,"Today, social media are one of the fastest ways to have access to information related to several topics. Indeed, a diffused information on these supports can travel thousands of kilometres in only few seconds contrary to an article posted on a news site. Despite the fact that a large variety of studies have been conducted to understand how fast and how scale information spreads in social media, we observe that they have not yet been interested in the geographical aspect. In this paper, we perform a geographical and temporal analysis of Twitter trends spread between May and June 2017. We introduce interesting patterns which deal with the paths taken by information between countries. In addition, we observe relevant results by taking into account the topic. Finally, we conclude and give perspectives of research of this work.",Information Propagation Routes between Countries in Social Media,NA:NA:NA,2018
Issam Falih:Nistor Grozavu:Rushed Kanawati:Younès Bennani,"Graph clustering techniques are very useful for detecting densely connected groups in large graphs. Many existing graph clustering methods mainly focus on the topological structure, but ignore the vertex properties. Existing graph clustering methods have been recently extended to deal with nodes attribute. First we motivate the interest in the study of this issue. Then we review the main approaches proposed to deal with this problem. We propose a comparative study of some existing attributed network community detection algorithm on both synthetic data and on real world data.",Community detection in Attributed Network,NA:NA:NA:NA,2018
Martin Atzmueller:Alvin Chin:Christoph Trattner,"Bienvenue! It is our great pleasure to welcome you to the WWW 2018 International Workshop on Modeling Social Media (MSM'2018) - Applying Machine Learning and AI for Modeling Social Media. This is our 9th edition of our workshop. Social networks such as Facebook, Twitter, and LinkedIn have paved the way for generating huge amount of diverse, streaming bit data in a short period of time. Such social media data require the application of big data analytics to produce meaningful information to both information consumers and data generators. Machine learning and AI techniques are particularly effective in situations where deep and predictive insights need to be uncovered from such social media data sets that are large, diverse and fast changing. The workshop aims to address machine learning and AI methods, frameworks, algorithms, and the applications and evaluation of these approaches on social media, big data and the web. We received 11 papers from all around the world covering a broad range of topics, and we accepted 8 papers resulting in a 72% acceptance rate. We evaluated them regarding relevance, quality, and novelty, selecting 4 full papers and 4 short papers. Each paper was reviewed by 3 reviewers and then decisions were made from the reviews and the workshop chairs.",International Workshop on Modeling Social Media (MSM 2018) Chairs' Welcome & Organization,NA:NA:NA,2018
Hong Wei:Hao Zhou:Jangan Sankaranarayanan:Sudipta Sengupta:Hanan Samet,"The tweet count prediction of a local spatial region is to forecast the number of tweets that are likely to be posted from that area over a relatively short period of time. It has many applications such as human mobility analysis, traffic planning, and abnormal event detection. In this paper, we formulate tweet count prediction as a spatiotemporal sequence forecasting problem and design an end-to-end convolutional LSTM based network with skip connection for this problem. Such a model enables us to exploit the unique properties of spatiotemporal data, consisting of not only the temporal characteristics such as temporal closeness, period and trend properties but also spatial dependencies. Our experiments on the city of Seattle, WA as well as a larger city of New York City show that the proposed method consistently outperforms the competitive baseline approaches.",Residual Convolutional LSTM for Tweet Count Prediction,NA:NA:NA:NA:NA,2018
Ashwini Tonge:Cornelia Caragea,"Online image sharing in social networking sites such as Facebook, Flickr, and Instagram can lead to unwanted disclosure and privacy violations, when privacy settings are used inappropriately. Despite that social networking sites allow users to set their privacy preferences, this can be cumbersome for the vast majority of users. In this paper, we explore privacy prediction models for social media that can automatically identify private (or sensitive) content from images, before they are shared online, in order to help protect users' privacy in social media. More precisely, we study ""deep"" visual features that are extracted from various layers of a pre-trained deep Convolutional Neural Network (CNN) as well as ""deep"" image tags generated from the CNN. Experimental results on a Flickr dataset of thousands of images show that the deep visual features and deep image tags can successfully identify images' private content and substantially outperform previous models for this task.","On the Use of ""Deep"" Features for Online Image Sharing",NA:NA,2018
Yutaro Miura:Fujio Toriumi:Toshiharu Sugawara,"We propose a model of a social networking service (SNS) with diminishing marginal utility in the framework of evolutionary computing and present our investigation on the effect of diminishing marginal utility on the dominant structure of strategies in all agents. SNSs such as Twitter and Facebook have been growing rapidly, but why they are prospering is unknown. SNSs have the characteristics of a public goods game because they are maintained by users posting many articles that incur some cost and because users can also be free riders, who just read articles. Thus, a number of studies aimed at understanding the conditions or mechanisms that keep social media thriving theoretically by introducing the meta-rewards game, which is a variation of a public goods game. The meta-rewards games assume constant marginal utility, meaning that the rewards by receiving comments increase linearly according to the number of comments, but describing the psychological rewards of humans is often inappropriate. In this paper, we present our modification of the model using the diminishing marginal utility and our comparison of the experimental results with those of the original meta-rewards game. We demonstrate that the structure of dominant strategies of all agents in our game is quite different from that in the original meta-rewards game and is more reasonable to explain the users' behavior in SNSs because their efforts in SNSs are limited even if they have many friends.",Evolutionary Learning Model of Social Networking Services with Diminishing Marginal Utility,NA:NA:NA,2018
Mehwish Nasim:Andrew Nguyen:Nick Lothian:Robert Cope:Lewis Mitchell,"Content polluters, or bots that hijack a conversation for political or advertising purposes are a known problem for event prediction, election forecasting and when distinguishing real news from fake news in social media data. Identifying this type of bot is particularly challenging, with state-of-the-art methods utilising large volumes of network data as features for machine learning models. Such datasets are generally not readily available in typical applications which stream social media data for real-time event prediction. In this work we develop a methodology to detect content polluters in social media datasets that are streamed in real-time. Applying our method to the problem of civil unrest event prediction in Australia, we identify content polluters from individual tweets, without collecting social network or historical data from individual accounts. We identify some peculiar characteristics of these bots in our dataset and propose metrics for identification of such accounts. We then pose some research questions around this type of bot detection, including: how good Twitter is at detecting content polluters and how well state-of-the-art methods perform in detecting bots in our dataset.",Real-time Detection of Content Polluters in Partially Observable Twitter Networks,NA:NA:NA:NA:NA,2018
Vedant Nanda:Hemank Lamba:Divyansh Agarwal:Megha Arora:Niharika Sachdeva:Ponnurangam Kumaraguru,"Selfies have become a prominent medium for self-portrayal on social media. Unfortunately, certain social media users go to extreme lengths to click selfies, which puts their lives at risk. Two hundred and sixteen individuals have died since March 2014 until January 2018 while trying to click selfies. It is imperative to be able to identify dangerous selfies posted on social media platforms to be able to build an intervention for users going to extreme lengths for clicking such selfies. In this work, we propose a convolutional neural network based classifier to identify dangerous selfies posted on social media using only the image (no metadata). We show that our proposed approach gives an accuracy of $98%$ and performs better than previous methods.",Stop the KillFies! Using Deep Learning Models to Identify Dangerous Selfies,NA:NA:NA:NA:NA:NA,2018
Javier Sanz-Cruzado:Sofía M. Pepa:Pablo Castells,"Link prediction has mainly been addressed as an accuracy-targeting problem in the social networks field. We discuss different perspectives on the problem considering other dimensions and effects that the link prediction methods may have on the social network where they are applied. Specifically, we consider the structural effects the prediction can have if the predicted links are added to the network. We consider further utility dimensions beyond prediction accuracy, namely novelty and diversity. We discuss the adaptation, for this purpose, of specific network, novelty and diversity metrics from social network analysis, recommender systems, and information retrieval.",Structural Novelty and Diversity in Link Prediction,NA:NA:NA,2018
Gaurav Bhatt:Aman Sharma:Shivam Sharma:Ankush Nagpal:Balasubramanian Raman:Ankush Mittal,"Identifying the veracity of a news article is an interesting problem while automating this process can be a challenging task. Detection of a news article as fake is still an open question as it is contingent on many factors which the current state-of-the-art models fail to incorporate. In this paper, we explore a subtask to fake news identification, and that is stance detection. Given a news article, the task is to determine the relevance of the body and its claim. We present a novel idea that combines the neural, statistical and external features to provide an efficient solution to this problem. We compute the neural embedding from the deep recurrent model, statistical features from the weighted n-gram bag-of-words model and handcrafted external features with the help of feature engineering heuristics. Finally, using deep neural layer all the features are combined, thereby classifying the headline-body news pair as agree, disagree, discuss, or unrelated. Through extensive experiments, we find that the proposed model outperforms all the state-of-the-art techniques including the submissions to the fake news challenge.","Combining Neural, Statistical and External Features for Fake News Stance Identification",NA:NA:NA:NA:NA:NA,2018
Marco Brambilla:Stefano Ceri:Florian Daniel:Marco Di Giovanni:Andrea Mauri:Giorgia Ramponi,"Knowledge in the world continuously evolves, and ontologies are largely incomplete, especially regarding data belonging to the so-called long tail. We propose a method for discovering emerging knowledge by extracting it from social content. Once initialized by domain experts, the method is capable of finding relevant entities by means of a mixed syntactic-semantic method. The method uses seeds, i.e. prototypes of emerging entities provided by experts, for generating candidates; then, it associates candidates to feature vectors built by using terms occurring in their social content and ranks the candidates by using their distance from the centroid of seeds, returning the top candidates. Our method can run iteratively, using the results as new seeds. as new seeds. In this paper we address the following research questions: (1) How does the reconstructed domain knowledge evolve if the candidates of one extraction are recursively used as seeds (2) How does the reconstructed domain knowledge spread geographically (3) Can the method be used to inspect the past, present, and future of knowledge (4) Can the method be used to find emerging knowledge",Iterative Knowledge Extraction from Social Networks,NA:NA:NA:NA:NA:NA,2018
Alípio Jorge:João Vinagre:Pawel Matuszyk:Myra Spiliopoulou,"It is our great pleasure to welcome you to the WWW 2018 Workshop on Online Recommender Systems and User Modeling (ORSUM). We have received eleven submissions covering highly relevant topics in the research related to recommender systems and user modeling. During this workshop its participants will have the opportunity to see eight presentations corresponding to the accepted papers and to discuss the recent advances on these topics both with authors and other researchers in the audience. The topics of the talks include, among others, location and news recommendation, local models for online recommendations, recommendations' diversity, page optimization, context-aware recommender systems, crowdsourcing and incremental matrix factorization methods.",ORSUM Chairs' Welcome & Organization,NA:NA:NA:NA,2018
Julien Subercaze:Christophe Gravier:Frederique Laforest,"Real-time recommendation of Twitter users based on the content of their profiles is a very challenging task. Traditional IR methods such as TF-IDF fail to handle efficiently large datasets. In this paper we present a scalable approach that allows real time recommendation of users based on their tweets. Our model builds a graph of terms, driven by the fact that users sharing similar interests will share similar terms. We show how this model can be encoded as a compact binary footprint, that allows very fast comparison and ranking, taking full advantage of modern CPU architectures. We validate our approach through an empirical evaluation against the Apache Lucene's implementation of TF-IDF. We show that our approach is in average two hundred times faster than standard optimised implementation of TF-IDF with a precision of 58%. The work presented here has been published in The Web Intelligence Journal.","Real-time, Scalable, Content-based Twitter Users Recommendation",NA:NA:NA,2018
George Karypis,"Recommender systems are designed to identify the items that a user will like or find useful based on the user's prior preferences and activities. These systems have become ubiquitous and are an essential tool for information filtering and (e-)commerce. Over the years, collaborative filtering, which derive these recommendations by leveraging past activities of groups of users, has emerged as the most prominent approach for solving this problem. This talk will present some of our recent work towards improving the performance of collaborative filtering-based recommender systems and understanding some of their fundamental limitations and characteristics. It will start by analyzing how the ratings that users provide to a set of items relate to their ratings of the set's individual items and, using these insights, will present rating prediction approaches that utilize distant supervision. It will then discuss extensions to approaches based on sparse linear and latent factor models that postulate that users' preferences are a combination of global and local preferences, which are shown to lead to better user modeling and as such improved prediction performance. Finally, the talk will conclude by discussing what can be accurately predicted by latent factor approaches and by analyzing the estimation error of sparse linear and latent factor models and how its characteristics impacts the performance of top N recommendation algorithms.","Recent Advances in Recommender Systems: Sets, Local Models, Coverage, and Errors",NA,2018
Eiman Aldahari:Vivek Shandilya:Sajjan Shiva,"Crowdsourcing is an approach whereby employers call for workers online with different capabilities to process a task for monetary reward. With a vast amount of tasks posted every day, satisfying the workers, employers, and service providers who are the stakeholders of any crowdsourcing system is critical to its success. To achieve this, the system should address three objectives: (1) match the worker with suitable tasks that fit the worker's interests and skills and raise the worker's rewards and rating, (2) give the employer more acceptable solutions with lower cost and time and raise the employer's rating, and (3) raise the rate of accepted tasks, which will raise the aggregated commissions to the service provider and improve the average rating of the registered users (employers and workers) accordingly. For these objectives, we present a mechanism design that is capable of reaching holistic satisfaction using a multi-objective recommendation system. In contrast, all previous crowdsourcing recommendation systems are designed to address one stakeholder who could be either the worker or the employer. Moreover, our unique contribution is to consider each stakeholder to be self serving. Considering selfish behavior from every stakeholder, we provide a more qualified recommendation for each stakeholder.",Crowdsourcing Multi-Objective Recommendation System*,NA:NA:NA,2018
Michele Zanitti:Sokol Kosta:Jannick Sørensen,"Recommender systems (RS) have seen widespread adoption across the Internet. However, by emphasizing personalization through the optimization of accuracy-focused metrics, over-personalization may emerge, with negative effects on the user experience. A countermeasure to the problem is to diversify recommendations. In this paper, we present a solution that addresses the problem in the context of a movie application domain. The solution enhances diversity on four related dimensions, namely global coverage, local coverage, novelty, and redundancy. The proposed solution is designed to diversify users profiles, modeled on categorical preferences, within the same group in the recommendation filtering. We evaluate our approach on the Movielens dataset and show that our algorithm yields better results compared to random selection distant neighbors and performs comparably to one of the current state of the art solutions.",A User-Centric Diversity by Design Recommender System for the Movie Application Domain,NA:NA:NA,2018
Leonardo Cella,"Traditional collaborative filtering, and content-based approaches attempt to learn a static recommendation model in a batch fashion. These approaches are not suitable in highly dynamic recommendation scenarios, like news recommendation and computational advertisement. Due to this well-known limitation, in the last decade a lot of efforts have been spent over the study of online learning techniques. Currently, a lot of attention has been devoted to improvements on the theoretical guarantees, without caring too much about computational cost and memory footprint. However, in the era of big-data content features tend to be high-dimensional, which leads to a direct challenge for traditional on-line learning algorithms (e.g., multi-armed bandits) since these are mostly designed for low-dimensional feature spaces. In this work we face the aforementioned problem, investigating an approximated context-aware bandit learner. Our model takes into account the problem of finding the actual low-dimensional manifold spanned by data content-features. In particular, we propose to store the covariance matrix of the previously seen contexts in a compressed space, without losing too much in terms of recommendation quality. With this work we provide an overview over the main properties, describe the adopted techniques, and report on preliminary experimental results on a synthetic dataset. We also discuss a drawback of the proposed method that may appear in typical scenarios and suggest future research avenues.",Efficient Context-Aware Sequential Recommender System,NA,2018
Gabriele Sottocornola:Panagiotis Symeonidis:Markus Zanker,"In the context of news recommendations, many time-aware approaches were proposed. These approaches have tried to capture the recency of news with respect to their short life span, by using either decaying weights on past articles or even forgetting them. However, most of these approaches have missed to consider sessions, which encapsulate inside them the articles that a user has interacted with in a short time period. In this paper, we provide news recommendations based on user sessions to reveal their short-term intentions. We also combine content-based with collaborative filtering to deal with the severe data sparsity problem that exists in our real-life data set. We have experimentally seen that the users' interests evolve over time and that our strategies can adapt fast to these changes.",Session-based News Recommendations,NA:NA:NA,2018
Jia Wang:Yungang Feng:Elham Naghizade:Lida Rashidi:Kwan Hui Lim:Kate Lee,"Studying large, widely spread Twitter data has laid the foundation for many novel applications from predicting natural disasters and epidemics to understanding urban dynamics. Recent studies have focused on exploring people's emotional response to their urban environment, e.g., green spaces versus built up areas, through analysing the sentiment of tweets within that area. Since green spaces have the capacity to improve citizen's well-being, we developed a system that is capable of recommending green spaces to users. Our system is unique in the sense that the recommendations are tailored with regard to users' preferred activity as well as the degree of positive sentiments in each green space. We show that the incoming flow of tweets can be used to refine the recommendations over time. Furthermore, We implemented a web-based, user-friendly interface to solicit user inputs and display recommendation results.",Happiness is a Choice: Sentiment and Activity-Aware Location Recommendation,NA:NA:NA:NA:NA:NA,2018
Weiru Zhang:Chao Wei:Xiaonan Meng:Yi Hu:Hao Wang,"Modern search engines present result pages composed of two most prominent types of information: sponsored and organic search results. The whole-page results must satisfy user's information inquiry while sponsored ad alongside the search results has become a key monetization strategy for the platform. Against the backdrop of this situation, a basic question has received comparatively little attention: how many ads are good enough to get higher user satisfaction and better monetization Most search engines always display a fixed number of ads or use heuristic rules to determine the number of ads. In this paper, we formulate the task of finding the best number of ads into a linear programming optimization problem, for which we propose a novel online algorithm to solve. We have conducted several offline experiments and tested our approach in Alibaba E-commerce platform. The experimental results show that the platform could achieve higher revenue and more clicks simultaneously by the proposed algorithm.",The Whole-Page Optimization via Dynamic Ad Allocation,NA:NA:NA:NA:NA,2018
Susan C. Anyosa:João Vinagre:Alípio M. Jorge,"Recommender systems try to predict which items a user will prefer. Traditional models for recommendation only take into account the user-item interaction, usually expressed by explicit ratings. However, in these days, web services continuously generate auxiliary data from users and items that can be incorporated into the recommendation model to improve recommendations. In this work, we propose an incremental Matrix Co-factorization model with implicit user feedback, considering a real-world data-stream scenario. This model can be seen as an extension of the conventional Matrix Factorization that includes additional dimensions to be decomposed in the common latent factor space. We test our proposal against a baseline algorithm that relies exclusively on interaction data, using prequential evaluation. Our experimental results show a significant improvement in the accuracy of recommendations, after incorporating an additional dimension in three music domain datasets.",Incremental Matrix Co-factorization for Recommender Systems with Implicit Feedback,NA:NA:NA,2018
Marie Al-Ghossein:Talel Abdessalem:Anthony Barré,"With the explosion of the volume of user-generated data, designing online recommender systems that learn from data streams has become essential. These systems rely on incremental learning that continuously update models as new observations arrive and they should be able to adapt to drifts in real-time. User preferences evolve over time and tracking their evolution is not an easy task. In addition to the low number of observations available per user, the preferences change at different moments and in different ways for each individual. In this paper, we propose a novel approach based on local models to address this problem. Local models are known for their ability to capture diverse preferences among user subsets. Our approach automatically detects the drift of preferences that leads a user to adopt a behavior closer to the users of another subset, and adjusts the models accordingly. Our experiments on real world datasets show promising results and prove the effectiveness of using local models to adapt to changes in user preferences.",Dynamic Local Models for Online Recommendation,NA:NA:NA,2018
Rémy Cazabet:Andrea Passarella:Giulio Rossetti:Fabrizio Silvestri,NA,OSNED 2018 Chairs' Welcome & Organization,NA:NA:NA:NA,2018
Gaku Morio:Katsuhide Fujita,"Large-scale online civic engagements (OCEs) with more than 100 participants have become possible due to recent developments in online social media technology. OCEs have the potential to achieve consensus building and collective decision-making with a large number of citizens, which is difficult to achieve in face-to-face contexts. However, most users in a large-scale OCE are rarely constantly active. Therefore, an important problem for the activation of a large-scale OCE is to facilitate the discussion by predicting which citizens will have significant influence in the discussion. This paper examines the activation prediction problem in a large-scale OCE. We propose a novel influence model based on the impulse response of activity histories and argumentative pressures, as well as an effective testing algorithm. The experimental results demonstrate that the proposed models with impulse response and the lexical pressures show better accuracy compared with baselines. In addition, the testing time required by the proposed method can be reduced significantly by employing a node-cutting algorithm.",Predicting Argumentative Influence Probabilities in Large-Scale Online Civic Engagement,NA:NA,2018
Caitlin Gray:Lewis Mitchell:Matthew Roughan,"Modelling information cascades over online social networks is important in fields from marketing to civil unrest prediction, however the underlying network structure strongly affects the probability and nature of such cascades. Even with simple cascade dynamics the probability of large cascades are almost entirely dictated by network properties, with well-known networks such as Erdos-Renyi and Barabasi-Albert producing wildly different cascades from the same model. Indeed, the notion of 'superspreaders' has arisen to describe highly influential nodes promoting global cascades in a social network. Here we use a simple model of global cascades to show that the presence of locality in the network increases the probability of a global cascade due to the increased vulnerability of connecting nodes. Rather than 'super-spreaders', we find that the presence of these highly connected 'super-blockers' in heavy-tailed networks in fact reduces the probability of global cascades, while promoting information spread when targeted as the initial spreader.",Super-blockers and the Effect of Network Structure on Information Cascades,NA:NA:NA,2018
Clemens Deusser:Nora Jansen:Jan Reubold:Benjamin Schiller:Oliver Hinz:Thorsten Strufe,"Social media interaction happens in a broad variety of context and magnitude. The vast majority of posts cause little to no discussion, while some start trends and become viral. We study the virality, explicitly of ""Buzzes"" - posts that evoke intense interaction over a short period of time, as they have been observed frequently, some- times with severe consequences for individuals and companies in the physical world. Early detection of a Buzz may help mitigate or prevent negative consequences of large scale social media outrage against companies or persons, by giving them a chance to react at an early stage. Collecting a labeled set of over 100,000 posts on Facebook pages, we first explore properties that define a Buzz using logistic regres- sion. This method helps us to interpret the results and derive prac- tical recommendations. We subsequently train classifiers and apply machine learning based classification techniques to demonstrate the potential capabilities of automated prediction. We achieve high recall with moderate precision, where feature boosting on broad feature sets yields the most promising results. Our study reveals that Buzzes are well described by a high num- ber of comments from previously passive users, a high number of likes given to comments, and a prolonged discussion period - properties that can be used to distinguish inconsequential posts from potentially volatile ones.",Buzz in Social Media: Detection of Short-lived Viral Phenomena,NA:NA:NA:NA:NA:NA,2018
Qiu Fang Ying:Dah Ming Chiu:Srinivasan Venkatramanan:Xiaopeng Zhang,"In this paper, we study the posting behavior of OSN users, in particular the posting frequency and temporal patterns, and consider possible interpretations of how users use the platform. At the aggregate (macro) level, we find two distinct peaks, one during morning working hours, and one in the evening. The morning peak is more pronounced for frequent posters, while the evening peak is pronounced in the remaining users. We postulate that this difference results from qualitatively different usage of the OSN platform (e.g. for work, with customers, etc.) than purely social interactions (e.g., friends, family, etc.). We also study user posting behavior at an individual (micro) level and apply LDA to cluster user temporal patterns, interpret our results. Our study provides possibly new insights into user activity in today's OSNs, and suggests a framework for profiling users based on their posting activities. In the process, we provide a novel application of LDA, to temporal user posting behavior by equating the time epochs of posts to words in documents. We believe our approach will complement other methods of user profiling based on static demographic information and friendship network information.",Profiling OSN Users Based on Temporal Posting Patterns,NA:NA:NA:NA,2018
Sebastian Schams:Jan Hauffa:Georg Groh,"To improve the quality of communication in Online Social Networks and Media (OSNEM), we envision a system that models a person's contributive social capital (CSC), which encompasses their competence, trustworthiness, and social responsibility. Having the CSC score available may inspire social behavior and mutual support. The system is based on three pillars: the analysis of OSNEM activity, interactions in virtual social capital market systems, and personal endorsements. In this paper we present our investigations regarding the first pillar. To obtain a dataset, we ran an experiment where 165 participants interacted on a custom social networking platform and assessed each other. Ground truth data was derived from these assessments. The dataset shows characteristics that are similar to larger OSNs. With different machine learning algorithms we investigated the hypothesis that contributive social capital can be extracted from network properties and networking activity, which were assessed with features such as the number of contributions of each participant. The prediction of contributive social capital showed an improvement over the baseline. A ranking of the participants following their predicted CSC scores showed a moderate correlation with the ranking according to the ground truth assessment. We also investigated the relative importance of the features for the analysis, and the effect of excluding inactive users to better understand network dynamics on a micro level. The selected features are also available in most other OSNEM platforms, like Facebook and Twitter. This allows a large-scale application of our investigations.",Analyzing a User's Contributive Social Capital Based on Acitivities in Online Social Networks and Media,NA:NA:NA,2018
Mauro Coletto:Claudio Lucchese:Salvatore Orlando,"The popularity of online social platforms has also determined the emergence of violent and abusive behaviors reflecting real life issues into the digital arena. Cyberbullying, Internet banging, pedopornography, sexting are examples of these behaviors, as witnessed in the social media environments. Several studies have shown how to approximately detect those behaviors by analyzing the social interactions and in particular the content of the exchanged messages. The features considered in the models basically include detection of o ensive language through NLP techniques and vocabularies, social network structural measures and, if available, user context information. Our goal is to investigate those users who adopt offensive language and hate speech in Twitter by analyzing their profile pictures. Results show that violent people smile less and they are dominating by anger, fear and sadness.",Do Violent People Smile: Social Media Analysis of their Profile Pictures,NA:NA:NA,2018
Stefano Cresci:Marinella Petrocchi:Angelo Spognardi:Stefano Tognazzi,"We envisage a revolutionary change in the approach to spambot detection: instead of taking countermeasures only after having collected evidence of new spambot mischiefs, in a near future techniques will be able to anticipate the ever-evolving spammers.",From Reaction to Proaction: Unexplored Ways to the Detection of Evolving Spambots,NA:NA:NA:NA,2018
Chiara Boldrini:Mustafa Toprak:Marco Conti:Andrea Passarella,"Ego networks have proved to be a valuable tool for understanding the relationships that individuals establish with their peers, both in offline and online social networks. Particularly interesting are the cognitive constraints associated with the interactions between the ego and the members of their ego network, whereby individuals cannot maintain meaningful interactions with more than 150 people, on average. In this work, we focus on the ego networks of journalists on Twitter, and we investigate whether they feature the same characteristics observed for other relevant classes of Twitter users, like politicians and generic users. Our findings are that journalists are generally more active and interact with more people than generic users. Their ego network structure is very aligned with reference models derived from the social brain hypothesis and observed in general human ego networks. Remarkably, the similarity is even higher than the one of politicians and generic users ego networks. This may imply a greater cognitive involvement with Twitter than with other social interaction means. Moreover, the ego networks of journalists are much stabler than those of politicians and generic users, and the ego-alter ties are often information-driven.",Twitter and the Press: an Ego-Centred Analysis,NA:NA:NA:NA,2018
Laura Koesten:Elena Demidova:Vadim Savenkov:John Breslin:Oscar Corcho:Stefan Dietze:Elena Simperl,"The web of data has seen tremendous growth recently. New forms of structured data have emerged in the form of web markup, such as schema.org and web tables. Exploiting these rich, heterogeneous and evolving data sources has become increasingly important for many different types of applications, including (federated) search, question answering and fact verification. The objective of the PROFILES & DATA:SEARCH Workshop is to bring together researchers and practitioners interested in the development of data search techniques, data profiling, and dataset retrieval on the web. This includes looking at the specifics of data-centric information seeking behaviours, understanding interaction challenges in data search on the web, and analysing the cognitive processes involved in the consumption of structured data by users. At the same time, we aim to discuss technologies addressing data search including semantics, information retrieval for web data (ranking algorithms and indexing), in particular in the context of decentralised and distributed systems, such as the web. We are interested in approaches to analyse, characterise and discover data sources. We want to facilitate a discussion around data search across formats and domain-specific applications. The PROFILES & DATA:SEARCH Workshop includes papers on a variety of topics such as profiling and data search, including querying and searching for structured data, profiling applications for cultural heritage, as well as data quality improvements through schema inference, content analysis and communities.",PROFILES & DATA: SEARCH International Workshop on Profiling and Searching Data on the Web Chairs' Welcome & Organization,NA:NA:NA:NA:NA:NA:NA,2018
Aidan Hogan,"Graphs are being increasingly adopted as a flexible data model in scenarios (e.g., Google's Knowledge Graph, Facebook's Graph API, Wikidata, etc.) where multiple editors are involved in content creation, where the schema is ever changing, where data are incomplete, where the connectivity of resources plays a key rolescenarios where relational models traditionally struggle. But with this flexibility comes a conceptual cost: it can be difficult to summarise and understand, at a high level, the content that a given graph contains. Hence profiling graphs becomes of increasing importance to extract order, a posteriori, from the chaotic processes by which such graphs are generated. This talk will motivate the use of graphs as a data model, abstract recent trends in graph data management, and then turn to the issue of profiling and summarising graphs: what are the goals of such profiling, the principles by which graphs can be summarised, the main techniques by which this can/could be achieved The talk will emphasise the importance of profiling graphs while highlighting a variety of open research questions yet to be tackled.",Profiling Graphs: Order from Chaos,NA,2018
Maarten de Rijke,"Over the years, search engines have developed to return a broad range of retrievable items, from documents to answers, people, locations, and products. Research datasets are increasingly being turned in retrievable items too. This raises a number of interesting challenges. Starting from the user end (What do users want from datasets) to increasing the retrievability of datasets (What kind of contextual information is available to enrich datasets so as to make the more easily retrieval) to optimizing rankers for datasets in the absence of large volumes of interaction data (How can we train learning to rank datasets algorithms in weakly supervised ways).",Learning to Search for Datasets,NA,2018
Emilia Kacprzak:Laura Koesten:Jeni Tennison:Elena Simperl,"The amount of data generated and published on the web is increasing rapidly, but search for structured data on the web still presents challenges. In this paper we explore dataset search by analysing queries specifically generated for this work through a crowdsourcing experiment and comparing them to a search log analysis of queries on data portals. The change in search environment together with the task we gave people altered the generated queries. We found that queries issued in our experiment were much longer than search queries for datasets on data portals. They further contained seven times more mentions of geospatial and of temporal information and are more likely to be structured as questions. These insights can be used to tailor search functionalities to the particular information needs and characteristics of dataset search.",Characterising Dataset Search Queries,NA:NA:NA:NA,2018
Mohamed Ben Ellefi:Odile Papini:Djamal Merad:Jean-Marc Boi:Jean-Philip Royer:Jérôme Pasquet:Jean-Christophe Sourisseau:Filipe Castro:Mohammad Motasem Nawaf:Pierre Drap,"Cultural heritage (CH) resources are very heterogeneous since the information was collected from vast diversity of cultural sites and digitally recorded in different formats. With the progress of 3D technologies, photogrammetry techniques become the adopted solution for representing CH artifacts by turning photos from small finds, to entire landscapes, into accurate 3D models. To meet knowledge representation with cultural heritage photogrammetry, this paper proposes an ontology-profiling method for modeling a real case of archaeological amphorae. The ontological profile consists of all needed information to represent a CH resource including typology attributes, geo-spatial information and photogrammetry process. An example illustrating the applicability of this profiling method to the problem of CH resources conceptualization is presented. We also outline our perspectives for using ontologies in data-driven science, in particular on modeling a complete pipeline that manages both the photogrammetric process and the archaeological knowledge.",Cultural Heritage Resources Profiling: Ontology-based Approach,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Semih Yumusak:Andreas Kamilaris:Erdogan Dogdu:Halife Kodaz:Elif Uysal:Riza Emre Aras,"The Semantic Web promotes common data formats and exchange protocols on the web towards better interoperability among systems and machines. Although Semantic Web technologies are being used to semantically annotate data and resources for easier reuse, the ad hoc discovery of these data sources remains an open issue. Popular Semantic Web endpoint repositories such as SPARQLES, Linking Open Data Project (LOD Cloud), and LODStats do not include recently published datasets and are not updated frequently by the publishers. Hence, there is a need for a web-based dynamic search engine that discovers these endpoints and datasets at frequent intervals. To address this need, a novel web meta-crawling method is proposed for discovering Linked Data sources on the Web. We implemented the method in a prototype system named SPARQL Endpoints Discovery (SpEnD). In this paper, we describe the design and implementation of SpEnD, together with an analysis and evaluation of its operation, in comparison to the aforementioned static endpoint repositories in terms of time performance, availability, and size. Findings indicate that SpEnD outperforms existing Linked Data resource discovery methods.",A Discovery and Analysis Engine for Semantic Web,NA:NA:NA:NA:NA:NA,2018
Sean Soderman:Anusha Kola:Maksim Podkorytov:Michael Geyer:Michael Gubanov,"Variety of Big data is a significant impediment for anyone who wants to search inside a large-scale structured dataset. For example, there are millions of tables available on the Web, but the most relevant search result does not necessarily match the keyword-query exactly due to a variety of ways to represent the same information. Here we describe Hybrid.AI, a learning search engine for large-scale structured data that uses automatically generated machine learning classifiers and Unified Famous Objects (UFOs) to return the most relevant search results from a large-scale Web tables corpora. We evaluate it over this corpora, collecting 99 queries and their results from users, and observe significant relevance gain.",Hybrid.AI: A Learning Search Engine for Large-scale Structured Data,NA:NA:NA:NA:NA,2018
Zhiyu Chen:Haiyan Jia:Jeff Heflin:Brian D. Davison,"Impoverished descriptions and convoluted schema labels are common challenges in data-centric tasks such as schema matching and data linking, especially when datasets can span domains. To address these issues, we consider the task of schema label generation. Typically, schema labels are created by dataset providers and are useful for users to understand a dataset. The motivation behind the task is that a lot of data linking systems require overlapping information between two datasets and rely on unique identifiers of schema labels. Moreover, it is common for schema labels in different datasets to have different identifiers even when they refer to the same concept. With no naming standard for schema labels, unintelligible labels are widely found in real-world datasets. For example, many schema labels contain abbreviations and compound nouns that hinder automated matching of attributes in corresponding datasets. Through schema label generation, more common (and thus understandable) schema labels can be provided to allow for broader schema matches in contexts such as dataset search and data linking. We develop a variety of features based on analysis of dataset content to enable machine learning methods to recommend useful labels. We test our approach on two real-world data collections and demonstrate that our method is able to outperform the alternative approach.",Generating Schema Labels through Dataset Content Analysis,NA:NA:NA:NA,2018
Sebastian Neumaier:Lörinc Thurnay:Thomas J. Lampoltshammer:Tomá Knap,"The present work describes the ADEQUATe platform: a framework to monitor the quality of (Governmental) Open Data catalogs, to re-publish improved and linked versions of the datasets and their respective metadata descriptions, and to include the community in the quality improvement process. The information acquired by the linking and (meta)data improvement steps is then integrated in a semantic search engine. In the paper, we first describe the requirements of the platform, which are based on focus group interviews and a web-based survey. Second, we use these requirements to formulate the goals and show the architecture of the overall platform, and third, we showcase the potential and relevance of the platform to resolve the requirements by describing exemplary user journeys exploring the system. The platform is available at: https://www.adequate.at/","Search, Filter, Fork, and Link Open Data: The ADEQUATe platform: data- and community-driven quality improvements",NA:NA:NA:NA,2018
Pinelopi Troullinou:Mathieu d'Aquin:Ilaria Tiddi,"This volume of proceedings presents the papers from the 2nd edition of the interdisciplinary workshop Re-coding Black Mirror, held on April 24, 2018 in Lyon, France and co-located with The WEB Conference (WWW2018). Participating to the topical debate of data ethics and algorithmic governance, Re-coding Black Mirror offers the research community tools to reflect on its role in the construction of the technological future and the potential societal implications. The workshop becomes a venue for computer scientists, data scientists and social scientists to create bridges of knowledge. The complexity of the societal phenomena emerging from the development in web technologies urge for interdisciplinary collaboration. Following the slightly futuristic approach to technology of the British-made sci-fi series Black Mirror, we called scientists to create their dystopic scenarios developed from their own existing technologies. Through this thought experiment, researchers considered potential ethical and social risks of technological advancements offering in some cases possible solutions.",Re-coding Black Mirror Chairs' Welcome & Organization,NA:NA:NA,2018
Sven Helmer,"We analyze the scenario depicted in the ""Black Mirror"" episode ""Fifteen Million Merits"" from an economic point of view, focusing on treating the attention of a user or consumer as a commodity. We continue by sketching the technological requirements for building such an economic framework, looking at advertisement platforms, payment schemes, and surveillance technology. As we show, a lot of the technology already exists and we expect the gaps to be filled in the very near future. Additionally, we briefly discuss the impact on social and work environments. While we believe that a scenario as extreme as shown in the episode is unlikely, we think that certain facets of it could find their way into our society.","May I Have Your Attention, Please: - Building a Dystopian Attention Economy",NA,2018
Tabea Tietz:Francesca Pichierri:Maria Koutraki:Dara Hallinan:Franziska Boehm:Harald Sack,"What happens to our social media profiles when we die The episode ""Be Right Back"" as part of Netflix's series ""Black Mirror"" provides a possible scenario. A digital avatar is created to communicate with close relatives which learns from past social media activities of the deceased user. While the users entrust their social media content to one or more companies, even after their death, it may be reasonable to ask: What will the company really do with a deceased user's data: sell it to manipulate users or create advertisements In this paper we tackle the issues of ownership, ethics, and transparency of post mortem user data.",Digital Zombies - the Reanimation of our Digital Selves,NA:NA:NA:NA:NA:NA,2018
Martino Mensio:Giuseppe Rizzo:Maurizio Morisio,"A future where the conversation with machines can potentially involve mutual emotions between the parties may be not so far in time. Inspired by the episode of Black Mirror ""Be Right Back'' and Replika, a futuristic app that promises to be ""your best friend'', in this work we are considering the positive and negative points of including an automated learning conversational agent inside the personal world of feelings and emotions. These systems can impact both single individuals and society, worsening an already critical situation. Our conclusion is that a regulation on the artificial emotional content should be considered before actually going beyond some one-way-only limits.",The Rise of Emotion-aware Conversational Agents: Threats in Digital Emotions,NA:NA:NA,2018
Kevin Koidl,"Social Media has transformed modern day society. It can be argued that one of the main drivers behind this transformation are novel ways to effectively distribute content in a highly targeted fashion and at scale. Recently, this effectiveness has come under attack based on new phenomena known as Fake News, Filter Bubble and Echo Chambers. The public debate about the impact of these phenomena on modern day society ranges from demanding a complete social media shutdown to government intervention and censorship. Furthermore, it appears that Social Media Platform providers are not sure what countermeasures are needed to address these new challenges. The main concern is that Black Mirror like scenarios will emerge simply by allowing privately held companies decide what content is conforming to public norms leading to a distortion of values. This paper presents an alternative solution by focusing on empowering meaningful relationships and not content engagement. The main motivation behind the proposed solution is to create social networks that follow a Trust by Design paradigm. This paper introduces and discusses the above-­mentioned challenges and presents a novel new social media concept seeking to overcome current challenges.",Towards Trust­-based Decentralized Ad-Hoc Social Networks,NA,2018
Linda Anticoli:Marco Basaldella,"In this paper we explore possible negative drawbacks in the use of wearable sensors, i.e., wearable devices used to detect different kinds of activity, e.g., from step and calories counting to heart rate and sleep monitoring. These technologies, which in the latter years witnessed a rapid development in terms of accuracy and diffusion, are now available on different platforms at reasonable prices and can lead to an healthier behavior in people using them. Nevertheless, we will try to investigate possibly harming behaviors related to these devices. We will provide different scenarios in which wearable sensors, in connection with social media, data mining, or other technologies, could prove harmful for their users.",Shut Up and Run: the Never-ending Quest for Social Fitness,NA:NA,2018
Patrick Wang:Rafael Angarita:Ilaria Renna,"Social media is an amazing platform for enhancing public exposure. Anyone, even social bots, can reach out to a vast community and expose one's opinion. But what happens when fake news is (un)intentionally spread within a social media This paper reviews techniques that can be used to fabricate fake news and depicts a scenario where social bots evolve in a fully semantic Web to infest social media with automatically generated deceptive information.",Is this the Era of Misinformation yet: Combining Social Bots and Fake News to Deceive the Masses,NA:NA:NA,2018
Martin Cooney:Sepideh Pashami:Anita Sant'Anna:Yuantao Fan:Slawomir Nowaczyk,"What would happen in a world where people could ""see'' others' hidden emotions directly through some visualizing technology Would lies become uncommon and would we understand each other better Or to the contrary, would such forced honesty make it impossible for a society to exist The science fiction television show Black Mirror has exposed a number of darker scenarios in which such futuristic technologies, by blurring the lines of what is private and what is not, could also catalyze suffering. Thus, the current paper first turns an eye towards identifying some potential pitfalls in emotion visualization which could lead to psychological or physical harm, miscommunication, and disempowerment. Then, some countermeasures are proposed and discussed--including some level of control over what is visualized and provision of suitably rich emotional information comprising intentions--toward facilitating a future in which emotion visualization could contribute toward people's well-being. The scenarios presented here are not limited to web technologies, since one typically thinks about emotion recognition primarily in the context of direct contact. However, as interfaces develop beyond today's keyboard and monitor, more information becomes available also at a distance--for example, speech-to-text software could evolve to annotate any dictated text with a speaker's emotional state.","Pitfalls of Affective Computing: How can the automatic visual communication of emotions lead to harm, and what can be done to mitigate such risks",NA:NA:NA:NA:NA,2018
Luca Viganò:Diego Sempreboni,What if we delegated so much to autonomous AI and intelligent machines that They passed a law that forbids humans to carry out a number of professions We conceive the plot of a new episode of Black Mirror to reflect on what might await us and how we can deal with such a future.,Gnirut: The Trouble With Being Born Human In An Autonomous World,NA:NA,2018
Diego Sempreboni:Luca Viganò,"Consider the following set-up for the plot of a possible future episode of the TV series Black Mirror: human brains can be connected directly to the net and MiningMind Inc. has developed a technology that merges a reward system with a cryptojacking engine that uses the human brain to mine cryptocurrency (or to carry out some other mining activity). Part of our brain will be committed to cryptographic calculations (mining), leaving the remaining part untouched for everyday operations, i.e., for our brain's normal daily activity. In this short paper, we briefly argue why this set-up might not be so far fetched after all, and explore the impact that such a technology could have on our lives and our society.",MMM: May I Mine Your Mind,NA:NA,2018
Harshvardhan J. Pandit:Dave Lewis,"The use of personal data is a double-edged sword that on one side provides benefits through personalisation and user profiling, while the other raises several ethical and moral implications that impede technological progress. Laws often try to reflect the shifting values of social perception, such as the General Data Protection Regulation (GDPR) catering to explicit consent over use of personal data, though actions may still be legal without being perceived as acceptable. Black Mirror is a TV series that serves to imagine scenarios that test the boundary of such perceptions, and is often described as being futuristic. In this paper, we discuss how existing technologies have already coalesced towards calculating a probability metric or rating as presented by the episode 'Nosedive'. We present real-world instances of such technologies and their applications, and how they can be easily expanded using the interminable web. The dilemma posed by the ethics of such technological applications is discussed using the 'Ethics Canvas', our methodology and tool for encouraging discussions on ethical implications in responsible innovation.",Ease and Ethics of User Profiling in Black Mirror,NA:NA,2018
Marie-Laure Mugnier:Catherine Roussey:Pierre Senellart,"It is our great pleasure to welcome you to the WWW 2018 Reasoning on Data Workshop. This workshop will gather people on a timely issue at the crossroad on knowledge representation and reasoning, data management, and the Semantic Web: How to use knowledge to make better use of data The workshop will more precisely focus on reasoning techniques that allow us to exploit domain knowledge in data access. An emblematic task is query answering, but knowledge can be exploited within the whole data lifecycle.",Reasoning on Data Workshop Chairs' Welcome and Organization,NA:NA:NA,2018
Mehdi Terdjimi:Lionel Médini:Michael Mrissa,"Today's Web applications tend to reason about cyclic data (i.e. facts that re-occur periodically) on the client side. Although they can benefit from efficient incremental maintenance algorithms capable of handling frequent data updates, existing rule-based algorithms cause successive re-derivations of previously inferred information. In this paper, we propose an incremental maintenance approach for rule-based reasoning that prevents successive re-computations of fact derivations. We tag (i.e. annotate) facts to keep trace of their provenance and validity. We compare our solution with the DRed-based incremental reasoning algorithm and show that it significantly outperforms this algorithm for fact updates in re-occurring situations, to the cost of tagging facts at their first insertion. Our experiments show that this cost can be recovered within a small number of cycles of deletions and reinsertions of explicit facts. We discuss the utility and limitations of our approach on Web clients and provide implementation packages of this reasoner that can be directly integrated in Web applications, on both server and client sides.",Web Reasoning Using Fact Tagging,NA:NA:NA,2018
Viet-Phi Huynh:Paolo Papotti,"Fact checking is the task of determining if a given claim holds. Several algorithms have been developed to check facts with reference information in the form of knowledge bases. While individual algorithms have been experimentally evaluated, we provide a first publicly available benchmark evaluating fact checking implementations across a range of assumptions about the properties of the facts and the reference data. We used our benchmark to compare algorithms designed on different principles and assumptions, as well as algorithms that can solve similar tasks developed in closely related communities. Our evaluation provided us with a number of new insights concerning the factors that impact the performance of the different methods.",Towards a Benchmark for Fact Checking with Knowledge Bases,NA:NA,2018
Jérôme David:Jérôme Euzenat:Pierre Genevès:Nabil Layaïda,"Query transformations are ubiquitous in semantic web query processing. For any situation in which transformations are not proved correct by construction, the quality of these transformations has to be evaluated. Usual evaluation measures are either overly syntactic and not very informative the result being: correct or incorrect or dependent from the evaluation sources. Moreover, both approaches do not necessarily yield the same result. We suggest that grounding the evaluation on query containment allows for a data-independent evaluation that is more informative than the usual syntactic evaluation. In addition, such evaluation modalities may take into account ontologies, alignments or different query languages as soon as they are relevant to query evaluation.",Evaluation of Query Transformations without Data: Short paper,NA:NA:NA:NA,2018
Franz Baader:Pavlos Marantidis:Maximilian Pensel,"Ontology-mediated query answering can be used to access large data sets through a mediating ontology. It has drawn considerable attention in the Description Logic (DL) community where both the complexity of query answering and practical query answering approaches based on rewriting were investigated in detail. Surprisingly, there is still a gap in what is known about the data complexity of query answering w.r.t. ontologies formulated in the inexpressive DL FL0. While it is known that the data complexity of answering conjunctive queries w.r.t. FL0 ontologies is coNP-complete, the exact complexity of answering instance queries was open until now. In the present paper, we show that answering instance queries w.r.t. FL0 ontologies is in P for data complexity. Together with the known lower bound of P-completeness for a fragment of FL0, this closes the gap mentioned above.",The Data Complexity of Answering Instance Queries in FL0,NA:NA:NA,2018
Marie-Francine Moens:Gareth J. F. Jones:Saptarshi Ghosh:Debasis Ganguly:Tanmoy Chakraborty:Kripabandhu Ghosh,"User-generated content on online social media (OSM) platforms has become an important source of real-time information during emergency events. The SMERP workshop series aims to provide a forum for researchers working on utilizing OSM for emergency preparedness and aiding post-emergency relief operations. The workshop aims to bring together researchers from diverse fields - Information Retrieval, Data Mining and Machine Learning, Natural Language Processing, Social Network Analysis, Computational Social Science, Human Computer Interaction - who can potentially contribute to utilizing social media for emergency relief and preparedness. The first SMERP workshop was held in April 2017 in conjunction with the ECIR 2017 conference. This 2nd SMERP Workshop with The Web Conference 2018 includes two keynote talks, a peer-reviewed research paper track, and a panel discussion.",WWW'18 Workshop on Exploitation of Social Media for Emergency Relief and Preparedness: Chairs' Welcome & Organization,NA:NA:NA:NA:NA:NA,2018
Dheeraj Kumar:Satish V. Ukkusuri,"Hurricane evacuation is a complex process and a better understanding of the evacuation behavior of the coastal residents could be helpful in planning better evacuation policy. Traditionally, various aspects of the household evacuation decisions have been determined by post-evacuation questionnaire surveys, which are usually time-consuming and expensive. Increased activity of users on social media, especially during emergencies, along with the geo-tagging of the posts, provides an opportunity to gain insights into user's decision-making process, as well as to gauge public opinion and activities using the social media data as a supplement to the traditional survey data. This paper leverages the geo-tagged Tweets posted in the New York City (NYC) in wake of Hurricane Sandy to understand the evacuation behavior of the residents. Based on the geo-tagged Tweet locations, we classify the NYC Twitter users into one of the three categories: outside evacuation zone, evacuees, and non-evacuees and examine the types of Tweets posted by each group during different phases of the hurricane. We establish a strong link between the social connectivity with the decision of the users to evacuate or stay. We analyze the geo-tagged Tweets to understand evacuation and return time and evacuation location patterns of evacuees. The analysis presented in this paper could be useful for authorities to plan a better evacuation campaign to minimize the risk to the life of the residents of the emergency hit areas.",Utilizing Geo-tagged Tweets to Understand Evacuation Dynamics during Emergencies: A case study of Hurricane Sandy,NA:NA,2018
Anastasia Moumtzidou:Stelios Andreadis:Ilias Gialampoukidis:Anastasios Karakostas:Stefanos Vrochidis:Ioannis Kompatsiaris,"Disaster monitoring based on social media posts has raised a lot of interest in the domain of computer science the last decade, mainly due to the wide area of applications in public safety and security and due to the pervasiveness not solely on daily communication but also in life-threating situations. Social media can be used as a valuable source for producing early warnings of eminent disasters. This paper presents a framework to analyse social media multimodal content, in order to decide if the content is relevant to flooding. This is very important since it enhances the crisis situational awareness and supports various crisis management procedures such as preparedness. Evaluation on a benchmark dataset shows very good performance in both text and image classification modules.",Flood Relevance Estimation from Visual and Textual Content in Social Media Streams,NA:NA:NA:NA:NA:NA,2018
Samujjwal Ghosh:Maunendra Sankar Desarkar,"Proper formulation of features plays an important role in short-text classification tasks as the amount of text available is very little. In literature, Term Frequency - Inverse Document Frequency (TF-IDF) is commonly used to create feature vectors for such tasks. However, TF-IDF formulation does not utilize the class information available in supervised learning. For classification problems, if it is possible to identify terms that can strongly distinguish among classes, then more weight can be given to those terms during feature construction phase. This may result in improved classifier performance with the incorporation of extra class label related information. We propose a supervised feature construction method to classify tweets, based on the actionable information that might be present, posted during different disaster scenarios. Improved classifier performance for such classification tasks can be helpful in the rescue and relief operations. We used three benchmark datasets containing tweets posted during Nepal and Italy earthquakes in 2015 and 2016 respectively. Experimental results show that the proposed method obtains better classification performance on these benchmark datasets.",Class Specific TF-IDF Boosting for Short-text Classification: Application to Short-texts Generated During Disasters,NA:NA,2018
Ribhav Soni:Sukomal Pal,"Microblogging sites like Twitter, Facebook, etc., are important sources of first-hand accounts during disaster situations, and have the potential to significantly aid disaster relief efforts. The IRMiDis track at FIRE 2017 focused on developing and comparing IR approaches to automatically identify and match tweets that indicate the need or availability of a resource, leading to the creation of a benchmark dataset for future improvements in this task. However, based on our experiments, we argue that the gold standard data obtained in the track is substantially incomplete. We also discuss some reasons why it may have been so, and provide some suggestions for making more robust ground truth data in such tasks.",Gold Standard Creation for Microblog Retrieval: Challenges of Completeness in IRMiDis 2017,NA:NA,2018
Ritam Dutt:Kaustubh Hiware:Avijit Ghosh:Rameshwar Bhaskaran,"We present SAVITR, a system that leverages the information posted on the Twitter microblogging site to monitor and analyse emergency situations. Given that only a very small percentage of microblogs are geo-tagged, it is essential for such a system to extract locations from the text of the microblogs. We employ natural language processing techniques to infer the locations mentioned in the microblog text, in an unsupervised fashion and display it on a map-based interface. The system is designed for efficient performance, achieving an F-score of 0.81, and is approximately two orders of magnitude faster than other available tools for location extraction.",SAVITR: A System for Real-time Location Extraction from Microblogs during Emergencies,NA:NA:NA:NA,2018
Cheng-Te Li:Lun-Wei Ku,"With the rapid growing of social networking services (e.g., Facebook and Twitter), being able to process data come from such platforms has gained much attention in recent years. SocialNLP is a new inter-disciplinary area of natural language processing (NLP) and social computing. There are three plausible directions of SocialNLP: (1) addressing issues in social computing using NLP techniques; (2) solving NLP problems using information from social media; and (3) handling new problems related to both social computing and natural language processing. Several challenges are foreseeable in SocialNLP. First, the message lengths on social media are usu-ally short, and thus it is difficult to apply traditional NLP approaches directly. Second, social media contains heterogeneous information (e.g. tags, friends, followers, likes, and retweets) that should be considered together with the contents for better quality of analysis. Finally, social media contents always involve multiple persons with slangs and jargons, and usually require special techniques to process. We organize SocialNLP in WWW 2018 with three goals. First, social media data is essentially generated and collected from online social services that are functioned based on Web techniques. One can leverage Web techniques to investigate various user behaviors and investigate the interactions between users. Second, user-generated data in social media is mainly in the form of text. Theories and techniques on Web information retrieval and natural language processing are desired for semantic understanding, accurate search, and efficient processing of big social media data. Third, from the perspective of application, if social media data can be effectively processed to distill the collective knowledge of users, novel Web applications, such as emergency management, social recommendation, and future prediction, can be developed with higher accuracy and better user experience. We expect SocialNLP workshop in WWW community can provide mutually-reinforced benefits for researchers in areas of Web techniques, information retrieval and social media analytics.",[email protected] 2018 Chairs' Welcome & Organization,NA:NA,2018
Xuetong Chen:Martin D. Sykora:Thomas W. Jackson:Suzanne Elayan,"Depression is among the most commonly diagnosed mental disorders around the world. With the increasing popularity of online social network platforms and the advances in data science, more research efforts have been spent on understanding mental disorders through social media by analysing linguistic style, sentiment, online social networks and other activity traces. However, the role of basic emotions and their changes over time, have not yet been fully explored in extant work. In this paper, we proposed a novel approach for identifying users with or at risk of depression by incorporating measures of eight basic emotions as features from Twitter posts over time, including a temporal analysis of these features. The results showed that emotion-related expressions can reveal insights of individuals' psychological states and emotions measured from such expressions show predictive power of identifying depression on Twitter. We also demonstrated that the changes in an individual's emotions as measured over time bear additional information and can further improve the effectiveness of emotions as features, hence, improve the performance of our proposed model in this task.",What about Mood Swings: Identifying Depression on Twitter with Temporal Measures of Emotions,NA:NA:NA:NA,2018
Sinya Peng:Vincent S. Tseng:Che-Wei Liang:Man-Kwan Shan,"Social media provides a vast continuous supply of dynamic and diverse information contents from the crowd, which serves as useful resources for predictive analytical applications. Although there exist already a number of studies on emerging topics detection, they focused on modelling of textual contents and emerging detection mechanism over topic popularity. To meet the real-life demands, prediction of emerging product topic, rather than detection, in the early stage is required. Besides, despite that some relevant studies considered social structure information, they suffer from the assumption that the complete network is available and the diffusion process only depends on social influence among members of networks. Moreover, not all social media sites provide the functionality to facilitate the development of online social networks. In this paper, we tackle the problem of emerging product topics prediction in social network with implicit networks. Two tasks, one for long-term forecast in pre-production stage and the other for short-term forecast in post-release stage, are investigated. We present a novel framework named Emerging Topics Predictor (ETP). Two novel features, namely author diversity and competition features, are also proposed to accommodate the diffusion process with implicit networks based on the rationale of product marketing. Through empirical evaluation on movie reviews from two real social media sites, ETP is shown to provide effective and efficient performance in predicting the emerging topics as early as possible. In particular, the experiment results show the promising effect of author diversity in emerging prediction. To the best of our knowledge, this work is among the very first studies on emerging product topic prediction in social media with considerations of implicit networks.",Emerging Product Topics Prediction in Social Media without Social Structure Information,NA:NA:NA:NA,2018
Saratchandra Indrakanti:Gyanit Singh:Justin House,"Product reviews on modern e-commerce websites have evolved into repositories of valuable firsthand opinions on products. Showcasing the opinions that reviewers express on a product in a succinct way can not only promote the product, but also provide an engaging experience that simplifies the shopping journey for online shoppers. In the case of traditional media such as movies and books, employingblurbs or excerpts from critic reviews for promotional purposes is an established practice among movie publicists and book editors that has proven to be an effective way of capturing attention of customers. Such excerpts can be discovered from e-commerce product reviews to highlight interesting reviewer opinions and add emotive elements to otherwise bland e-commerce product pages. While traditional movie or book blurbs are manually extracted, they must be automatically extracted from e-commerce product reviews owing to the scale of catalogues. Further, traditional blurbs are generally phrased to be very positive in tone and sometimes may take some words out of context. However, excerpts for e-commerce products must represent the true opinions of the reviewers and must capture the context in which the words were used to retain trust of users. To that end, we introduce the problem of extracting engaging excerpts from e-commerce product reviews in this paper. We present methods to automatically discover such excerpts from reviews at scale by leveraging natural language properties such as syntactic structure of sentences and sentiment, and discuss some of the underlying challenges. We further present an evaluation of the effectiveness of the proposed methods in terms of the quality of the blurbs generated and their ranking orders produced.",Blurb Mining: Discovering Interesting Excerpts from E-commerce Product Reviews,NA:NA:NA,2018
Reshmi Gopalakrishna Pillai:Mike Thelwall:Constantin Orasan,"The ability to automatically detect human stress and relaxation is crucial for timely diagnosing stress-related diseases, ensuring customer satisfaction in services and managing human-centric applications such as traffic management. Traditional methods employ stress-measuring scales or physiological monitoring which may be intrusive and inconvenient. Instead, the ubiquitous nature of the social media can be leveraged to identify stress and relaxation, since many people habitually share their recent life experiences through social networking sites. This paper introduces an improved method to detect expressions of stress and relaxation in social media content. It uses word sense disambiguation by word sense vectors to improve the performance of the first and only lexicon-based stress/relaxation detection algorithm TensiStrength. Experimental results show that incorporating word sense disambiguation substantially improves the performance of the original TensiStrength. It performs better than state-of-the-art machine learning methods too in terms of Pearson correlation and percentage of exact matches. We also propose a novel framework for identifying the causal agents of stress and relaxation in tweets as future work.",Detection of Stress and Relaxation Magnitudes for Tweets,NA:NA:NA,2018
Lipika Dey:Tirthanker Dasgupta:Priyanka Sinha,"Breakthroughs in Artificial Intelligence (AI) and World Wide Web technologies have opened a new direction in Enterprise Intelligence, that is gradually transforming the way enterprises perform business and interact with their customers. This change is largely driven by the widespread consumer adoption of sophisticated AI technologies and web based social media. Consequently, almost all business enterprises face a number of challenges such as adoption of new business paradigms; customer centric business processes; issues with large, multi-modal, multilingual, and multicultural data, analyzing behavioral signals from social media; agility, security and many more. Therefore, the primary goal of this workshop is to bring together industry professionals and researchers working in the area of AI, Natural Language Processing (NLP), Machine-Learning, linguistics, social science, HCI, design and computer vision and those whose work concerns the intersection of these areas, together to provide a venue for the multidisciplinary discussion of how ubiquitous AI technologies can help in extracting social and enterprise intelligence for smart enterprise transformation while addressing the aforementioned challenges.",Social Sensing and Enterprise Intelligence: Towards a Smart Enterprise Transformation (SSEI 2018) Chairs' Welcome and Organization,NA:NA:NA,2018
Dominik Slezak,"The AI methods are regaining a lot of attention in the areas of data analytics and decision support. Given the increasing amount of information and computational resources available, it is now possible for intelligent algorithms to learn from the data and assist humans more efficiently. Still, there is a question about the goals of learning and a form of the resulting data-driven knowledge. It is evident that humans do not operate with precise information in decision-making and, thus, it might be unnecessary to provide them with complete outcomes of analytical processes. Consequently, the next question arises whether approximate results of computations or results derived from the approximate data could be delivered more efficiently than their standard counterparts. Such questions are analogous to the ones about precision of calculations conducted by machine learning and KDD methods, whereby heuristic algorithms could be boosted by letting them rely on approximate computations. This leads us toward discussion of the importance of approximations in the areas of machine intelligence and business intelligence and, more broadly, the meaning of approximate derivations for various aspects of AI. In this talk, this discussion is supported by four industry-related case studies\footnoteThe first case study refers entirely to the author's work for Security On-Demand (\urlhttps://www.securityondemand.com/ ). The work on the second case study was co-financed by the EU Smart Growth Operational Programme 2014-2020 under the Innovation Voucher project POIR.02.03.02-14-0009/15-00. The work on the third case study is co-financed by the EU Smart Growth Operational Programme 2014-2020 under the project POIR.01.01.01-00-0831/17-00. The work on the fourth case study is co-financed by the EU Smart Growth Operational Programme 2014-2020 under the GameINN project POIR.01.02.00-00-0184/17-00 : \beginenumerate ıtem The approximate database engine based on the paradigms of rough-granular computing applied in the area of cyber-security analytics \citeslezak:queryengine,\citeslezak:scalablefeature, \citeslezak:cyberengine ıtem The similarity-based feature engineering methodology embedded into an HR support system working with heterogeneous information sources \citeslezak:jobs ; ıtem The ensemble-based attribute approximation approach that will be used in the area of online health support \citeOvu ; ıtem The process of approximate data generation that will be used for tuning an online gaming coaching platform \citeEsensei.\endenumerate",Toward Approximate Intelligence: Approximate Query Engines & Approximate Data Exploration,NA,2018
Galit B. Yom-Tov:Shelly Ashtar:Daniel Altman:Michael Natapov:Neta Barkay:Monika Westphal:Anat Rafaeli,"We adjust sentiment analysis techniques to automatically detect customer emotion in on-line service interactions of multiple business domains. Then we use the adjusted sentiment analysis tool to report insights about the dynamics of emotion in on-line service chats, using a large data set of Telecommunication customer service interactions. Our analyses show customer emotions starting out negative and evolving into positive as the interaction ends. Also, we identify a close relationship between customer emotion dynamicsduring the service interaction and the concepts of service failure and recovery. This connection manifests in customer service quality evaluationsafter the interaction ends. Our study shows the connection between customer emotion and service quality as service interactions unfold, and suggests the use of sentiment analysis tools for real-time monitoring and control of web-based service quality.",Customer Sentiment in Web-Based Service Interactions: Automated Analyses and New Insights,NA:NA:NA:NA:NA:NA:NA,2018
Changzhou Li:Yao Lu:Junfeng Wu:Yongrui Zhang:Zhongzhou Xia:Tianchen Wang:Dantian Yu:Xurui Chen:Peidong Liu:Junyu Guo,"Clustering narrow-domain short texts, such as academic abstracts, is an extremely difficult clustering problem. Firstly, short texts lead to low frequency and sparseness of words, making clustering results highly unstable and inaccurate; Secondly, narrow domain leads to great overlapping of insignificant words and makes it hard to distinguish between sub-domains, or fine-grained clusters. The vocabulary size is also too small to construct a good word bag needed by traditional clustering algorithms like LDA to give a meaningful topic distribution. A novel clustering model, Partitioned Word2Vec-LDA (PW-LDA), is proposed in this paper to tackle the described problems. Since the purpose sentences of an abstract contain crucial information about the topic of the paper, we firstly implement a novel algorithm to extract them from the abstracts according to its structural features. Then high-frequency words are removed from those purpose sentences to get a purified-purpose corpus and LDA and Word2Vec models are trained. After combining the results of both models, we can cluster the abstracts more precisely. Our model uses abstract text instead of keywords to cluster because keywords may be ambiguous and cause unsatisfied clustering results shown by previous work. Experimental results show that the clustering results of PW-LDA are much more accurate and stable than state-of-the-art techniques.",LDA Meets Word2Vec: A Novel Model for Academic Abstract Clustering,NA:NA:NA:NA:NA:NA:NA:NA:NA:NA,2018
Pankaj Trivedi:Arvind Singh,"Payment transaction engine at PayU processes multimillion trans- actions every day through multiple payment gateways. Routing a transaction through an appropriate payment gateway is crucial to the engine for optimizing the availability and cost. The problem is that every transaction needs to choose one of K available payment gateways characterized by an unknown probability reward distri- bution. The reward for a gateway is a combination of its health and cost factors. The reward for a gateway is only realized when transaction is processed by the gateway i.e. by its success or failure. The objective of dynamic routing is to maximize the cumulative expected rewards over some given horizon of transactions' life. To do this, the dynamic switching system needs to acquire informa- tion about gateways (exploration) while simultaneously optimizing immediate rewards by selecting the best gateway at the moment (exploitation); the price paid due to this trade o is referred to as the regret. The main objective is to minimize the regret and maximize the rewards. The basic idea is to choose a gateway according to its probability of being the best gateway. The routing problem is a direct formulation of reinforcement learning (RL) problem. In an RL problem, an agent interacts with a dynamic, stochastic, and incompletely known environment, with the goal of finding an action-selection strategy, or policy, that optimizes some long-term performance measure. Thompson Sampling algorithm has experimentally been shown to be close to optimal.",Stochastic Multi-path Routing Problem with Non-stationary Rewards: Building PayU's Dynamic Routing,NA:NA,2018
Nabeel Albishry:Tom Crick:Theo Tryfonas:Tesleem Fagade,"With an increasing number of consumers using social media platforms to share both their satisfaction and displeasure about the products and services they use every day, organisations with a customer service focus are recognising the importance of rapid--and genuine--online engagement with their customers. In turn, consumers increasingly judge organisations on the quality of customer service and degree of responsiveness to online queries. This paper presents an extensible framework for evaluating direct engagements of customer service teams with customers on Twitter. Furthermore, this framework provides the capability to measure and analyse indirect engagement with industry sector rivals, especially their patterns, frequency and intensity. By applying graph analysis to these Twitter interactions, our framework generates various analytical measures and visual representations, exemplified through a case study based on seven major UK telecoms companies. With a dataset consisting of 15,000 tweets and 3,500 user profiles, the results provide sustained evidence for indirect engagements between business rivals, with customer queries acting as a trigger for intense competition between companies based in the same industry sub-domain.",An Evaluation of Performance and Competition in Customer Services on Twitter: A UK Telecoms Case Study,NA:NA:NA:NA,2018
Manish Puri:Xu Du:Aparna S. Varde:Gerard de Melo,"This research focuses on mining ordinances (local laws) and public reactions to them expressed on social media. We place particular emphasis on ordinances and tweets relating to Smart City Characteristics (SCCs), since an important aim of our work is to assess how well a given region heads towards a Smart City. We rely on SCCs as a nexus between a seemingly infinite number of ordinances and tweets to be able to map them, and also to facilitate SCC-based opinion mining later for providing feedback to urban agencies based on public reactions. Common sense knowledge is harnessed in our approach to reflect human judgment in mapping. This paper presents our research in ordinance and tweet mapping with SCCs, including the proposed mapping approach, our initial experiments, related discussion, and future work emerging therein. To the best of our knowledge, ours is among the first works to conduct mining on ordinances and tweets for Smart Cities. This work has a broader impact with a vision to enhance Smart City growth.",Mapping Ordinances and Tweets using Smart City Characteristics to Aid Opinion Mining,NA:NA:NA:NA,2018
Marc Spaniol:Ricardo Baeza-Yates:Julien Masanès,"Time is a key dimension to understand the Web. It is fair to say that it has not received yet all the attention it deserves and TempWeb is an attempt to help remedy this situation by putting time as the center of its reflection. Studying time in this context actually covers a large spectrum, from the extraction of temporal information and knowledge, to diachronic studies for the design of infrastructural and experimental settings enabling a proper observation of this dimension.",TempWeb 2018 Chairs' Welcome and Organization,NA:NA:NA,2018
Andreas Spitz:Jannik Strötgen:Michael Gertz,"For the temporal analysis of news articles or the extraction of temporal expressions from such documents, accurate document creation times are indispensable. While document creation times are available as time stamps or HTML metadata in many cases, depending on the document collection in question, this data can be inaccurate or incomplete in others. Especially in digitally published online news articles, publication times are often missing from the article or inaccurate due to (partial) updates of the content at a later time. In this paper, we investigate the prediction of document creation times for articles in citation networks of digitally published news articles, which provide a network structure of knowledge flows between individual articles in addition to the contained temporal expressions. We explore the evolution of such networks to motivate the extraction of suitable features, which we utilize in a subsequent prediction of document creation times, framed as a regression task. Based on our evaluation of several established machine learning regressors on a large network of English news articles, we show that the combination of temporal and local structural features allows for the estimation of document creation times from the network.",Predicting Document Creation Times in News Citation Networks,NA:NA:NA,2018
Bowen Zhang:Wing Cheong Lau,"Predicting the popularity of a discussion topic in an online social network (OSN) or the responses to an online fund-raising campaign is a practical challenge of immense value. Previous work tries to predict the popularity of an online campaign by modeling information diffusion as a homogeneous temporal point process within a network of a single-type of actors. However, real-world information propagation often involved multiple types of actors. In particular, there are the so-called opinion leaders, e.g. online celebrities or influential OSN users with a huge number of followers, who can create a great impact on the visibility and thus the final popularity of an event by simply mentioning it in their tweets or postings. In this paper, we propose MASEP, a Multi-actor Self-exciting Process, to model and predict the popularity of different online campaigns involving multiple types of actors. MASEP combines a self-exciting branching process with a periodical decay process to capture the dynamics and interdependent relationship between opinion leaders and ordinary users during an online campaign. A closed-form expression is derived for the temporal campaign popularity under the MASEP model. Based on this closed-form expression, we can efficiently perform regression against the empirical activity measurements of an online campaign during its early stage to estimate the parameters of the corresponding MASEP model. The final popularity of the campaign can then be predicted. To demonstrate the efficacy of the MASEP-based approach, we apply it to predict the popularity of three types of online campaigns from different large-scale real-world datasets, namely, the total number of posts in retweeting cascades, the overall count of individual hashtags in posting streams, and the final number of sponsors for crowd-funding campaigns. In particular, using the initial 30% of each campaign data trace for training, our approach can achieve absolute prediction error (APE) of 13.25%, 15.7%, and 36.9% respectively for datasets of 3 different types of campaigns. This corresponds to a 26.1% to 63.2% reduction in prediction error when comparing to state-of-the-art approaches including SEISMIC, SpikeM, and STRM.",Temporal Modeling of Information Diffusion using MASEP: Multi-Actor Self-Exciting Processes,NA:NA,2018
Behrooz Mansouri:Mohammad Sadegh Zahedi:Ricardo Campos:Mojgan Farhoodi:Alireza Yari,"The development of information retrieval algorithms and temporal information retrieval ones has been extensively carried out over the last few years. While several studies have been conducted, most of these researches relate to English, leading to a lack of knowledge in several other important languages. This includes the Persian one. In this work, we aim to shorten this gap by contributing, disseminating and enlarging the knowledge we have on temporal information retrieval aspects in Persian, which is one of the dominant languages in the Middle East, widely spoken in several countries. To achieve this objective, we propose to understand the use of temporal expressions on a large-scale Persian search engine query log consisting of 27M queries. In particular, we focus on explicit (e.g., June 2017) and relative temporal expressions (e.g., tomorrow) and try to understand (1) how often temporal expressions are used in web queries; (2) which type of temporal expressions (Date, Time, Duration and Set) are commonly used; (3) to which time (past, current or future) do temporal expressions mostly refer to; (4) to which category they often belong; (5) how often do user's reformulate their queries by adding temporal expressions; and (6) how using temporal expressions affects user's satisfaction. We believe that answering these questions may be beneficial for a large number of tasks including, user's behavior understanding and search engines' improvement effectiveness.",Understanding the use of Temporal Expressions on Persian Web Search,NA:NA:NA:NA:NA,2018
Henry S. Thompson:Jian Tong,"We report here on the results of two studies using two and four monthly web crawls respectively from the Common Crawl (CC) initiative between 2014 and 2017, whose initial goal was to provide empirical evidence for the changing patterns of use of so-called persistent identifiers. This paper focusses on the tooling needed for dealing with CC data, and the problems we found with it. The first study is based on over 10^12 URIs from over 5 x 10^9 pages crawled in April 2014 and April 2017, the second study adds a further 3 x 10^9 pages from the April 2015 and April 2016 crawls. We conclude with suggestions on specific actions needed to enable studies based on CC to give reliable longitudinal information.",Can Common Crawl Reliably Track Persistent Identifier (PID) Use Over Time,NA:NA,2018
Melisachew Wudage Chekol:Heiner Stuckenschmidt,"The emergence of open information extraction as a tool for constructing and expanding knowledge graphs has aided the growth of temporal data, for instance, YAGO, NELL and Wikidata. While YAGO and Wikidata maintain the valid time of facts, NELL records the time point at which a fact is retrieved from some Web corpora. Collectively, these knowledge graphs (KGs) store facts extracted from Wikipedia and other sources. Due to the imprecise nature of the extraction tools that are used to build and expand KGs, such as NELL, the facts in the KGs are weighted (a confidence value representing the correctness of a fact). Additionally, NELL can be considered as a transaction time KG because every fact is associated with extraction date. On the other hand, YAGO and Wikidata use the valid time model because they only maintain facts together with their validity time (temporal scope). In this paper, we propose a bitemporal model (that combines transaction and valid time models) for maintaining and querying probabilistic temporal knowledge graphs. We report our evaluation results of the proposed approach.",Towards Probabilistic Bitemporal Knowledge Graphs,NA:NA,2018
Behrooz Mansouri:Mohammad Sadegh Zahedi:Ricardo Campos:Mojgan Farhoodi:Maseud Rahgozar,"Web searches are done by users every day on a million-daily basis. Many of these web searches are related to events, social occasions that attracts society's attention. Events may happen multiple times on cyclic or non-periodic occasions. These are known as spiky events. When these events occur, multiple spikes can be observed in query logs triggered by a change in the user's behaviour and an increase in the frequency of the user's queries. In this paper, we aim to understand the user's search behaviour towards this kind of events. To this regard, we propose a new taxonomy of spiky events which categorizes queries into two groups: periodic (ongoing, historical, traditional) and aperiodic (predictable and unpredictable), and study how various features concerning the query and the clicked web pages describe the user's behaviour, before, during, and after the event. To conduct this research, we consider 100 spiky events and rely on a two-year Persian search engine query log to analyse their related queries and associated information. The results obtained show that users have a different behaviour regarding the query frequency, length and temporality, depending on the category of the spiky event and that query formulation and clicked pages are also different for each category before, during and after the event. Understanding these user's behaviours and their relationship with the different categories may play an important role for any search engine looking to provide better services for their users.",Understanding User's Search Behavior towards Spiky Events,NA:NA:NA:NA:NA,2018
Julien Leblay:Melisachew Wudage Chekol,"Knowledge Graphs (KGs) are a popular means to represent knowledge on the Web, typically in the form of node/edge labelled directed graphs. We consider temporal KGs, in which edges are further annotated with time intervals, reflecting when the relationship between entities held in time. In this paper, we focus on the task of predicting time validity for unannotated edges. We introduce the problem as a variation of relational embedding. We adapt existing approaches, and explore the importance example selection and the incorporation of side information in the learning process. We present our experimental evaluation in details.",Deriving Validity Time in Knowledge Graph,NA:NA,2018
Robert West:Leila Zia:Dario Taraborelli:Jure Leskovec,"It is our great pleasure to welcome you to the Wiki Workshop at the Web Conference 2018. The goal of this workshop is to bring together researchers exploring all aspects of Wikimedia projects such as Wikipedia, Wikidata, and Wikimedia Commons. With members of the Wikimedia Foundation's Research team on the organizing committee and with the experience of successful workshops in 2015, 2016, and 2017, we aim to continue facilitating a direct pathway for exchanging ideas between the organization that coordinates Wikimedia projects and the researchers interested in studying them. We received 17 paper submissions (and counting) from all around the world covering a broad range of topics. Our program committee evaluated them regarding relevance, quality, and novelty, selecting 8 to be presented as posters and published in the workshop proceedings, and numerous others for poster presentation only.",Welcome on Behalf of the Wiki Workshop Chairs & Organization,NA:NA:NA:NA,2018
Christoph Hube:Besnik Fetahu,"Quality in Wikipedia is enforced through a set of editing policies and guidelines recommended for Wikipedia editors. Neutral point of view (NPOV) is one of the main principles in Wikipedia, which ensures that for controversial information all possible points of view are represented proportionally. Furthermore, language used in Wikipedia should be neutral and not opinionated. However, due to the large number of Wikipedia articles and its operating principle based on a voluntary basis of Wikipedia editors; quality assurances and Wikipedia guidelines cannot always be enforced. Currently, there are more than 40,000 articles, which are flagged with NPOV or similar quality tags. Furthermore, these represent only the portion of articles for which such quality issues are explicitly flagged by the Wikipedia editors, however, the real number may be higher considering that only a small percentage of articles are of good quality or featured as categorized by Wikipedia. In this work, we focus on the case of language bias at the sentence level in Wikipedia. Language bias is a hard problem, as it represents a subjective task and usually the linguistic cues are subtle and can be determined only through its context. We propose a supervised classification approach, which relies on an automatically created lexicon of bias words, and other syntactical and semantic characteristics of biased statements. We experimentally evaluate our approach on a dataset consisting of biased and unbiased statements, and show that we are able to detect biased statements with an accuracy of 74%. Furthermore, we show that competitors that determine bias words are not suitable for detecting biased statements, which we outperform with a relative improvement of over 20%.",Detecting Biased Statements in Wikipedia,NA:NA,2018
Vevake Balaraman:Simon Razniewski:Werner Nutt,"The collaborative knowledge base Wikidata is the central storage of Wikimedia projects, containing over 45 million data items. It acts as the hub for interlinking Wikipedia pages about a specific item in different languages, automates features such as infoboxes in Wikipedia, and is increasingly used for other applications such as data enrichment and question answering. Tracking the quality of Wikidata is an important issue for this project. In this paper we focus particularly on the completeness aspect. Several automated techniques have been adopted by Wikis to track and manage completeness, yet these techniques are generally subjective and do not provide a clear quality estimate at the level of entities. In this paper, we present an approach towards measuring Relative Completeness in Wikidata by comparison with data present for similar entities. This relative completeness approach is easily scalable with the introduction of new classes in the knowledge base, and has been implemented for all available entities in Wikidata. The results provide an intuition on the completeness of an entity comparing it with other similar entities. Here, we present our implementation approach along with a discussion on strategies and open challenges.",Recoin: Relative Completeness in Wikidata,NA:NA:NA,2018
Lijun Lyu:Besnik Fetahu,"Wikipedia is one of the top visited resources on the Web, furthermore, it is used extensively as the main source of information for applications like Web search, question & answering etc. This is mostly attributed to Wikipedia's coverage in terms of topics and real-world entities and the fact that Wikipedia articles are constantly updated with new and emerging facts. However, only a small fraction of articles are considered to be of good quality. The large majority of articles are incomplete and have other quality issues. A strong quality indicator is the presence of external references from third-party sources (e.g. news sources) as suggested by the verifiability principle in Wikipedia. Even for the existing references in Wikipedia there is an inherent lag in terms of the publication time of cited resources and the time they are cited in Wikipedia articles. We propose a near real-time suggestion of news references for Wikipedia from a daily news stream. We model daily news into specific events, spanning from a day up to year. Thus, we construct an event-chain from which we determine when the information in an event has converged and consequentially based on a learning-to-rank approach suggest the most authoritative and complete news article to Wikipedia articles involved in a specific event. We evaluate our news suggestion approach on a set of 41 events extracted from Wikipedia currents event portal, and on new corpus consisting of daily news between the period of 2016-2017 with more than 14 million news articles. We are able to suggest news articles to Wikipedia pages with an overall accuracy of MAP=0.77 and with a minimal lag w.r.t the publication time of the news article.",Real-time Event-based News Suggestion for Wikipedia Pages from News Streams,NA:NA,2018
Thomas Pellissier Tanon:Lucie-Aimée Kaffee,"Stability in Wikidata's schema is essential for the reuse of its data. In this paper, we analyze the stability of the data based on the changes in labels of properties in six languages. We find that the schema is overall stable, making it a reliable resource for external usage.",Property Label Stability in Wikidata: Evolution and Convergence of Schemas in Collaborative Knowledge Bases,NA:NA,2018
Laxmi Amulya Gundala:Francesca Spezzano,"In this paper, we describe our on-going research on the problem of predicting needed hyperlinks between pairs of Wikipedia pages (u,v) that are not connected, yet show readers' search navigation from u to v. We propose a solution that first estimates how long will these searches last and then predicts new hyperlinks according to descending order of duration. Our initial experimental results show that our best solution achieves an AUROC of 0.77 on the Wikipedia Clickstream dataset and a [email protected]% of 1.0 and significantly beats the baselines.",Readers' Demanded Hyperlink Prediction in Wikipedia,NA:NA,2018
Finn Årup Nielsen,The linkage of ImageNet WordNet synsets to Wikidata items will leverage deep learning algorithm with access to a rich multilingual knowledge graph. Here I will describe our on-going efforts in linking the two resources and issues faced in matching the Wikidata and WordNet knowledge graphs. I show an example on how the linkage can be used in a deep learning setting with real-time image classification and labeling in a non-English language and discuss what opportunities lies ahead.,Linking ImageNet WordNet Synsets with Wikidata,NA,2018
Sebastián Ferrada:Nicolás Bravo:Benjamin Bustos:Aidan Hogan,"Despite its importance to the Web, multimedia content is often neglected when building and designing knowledge-bases: though descriptive metadata and links are often provided for images, video, etc., the multimedia content itself is often treated as opaque and is rarely analysed. IMGpedia is an effort to bring together the images of Wikimedia Commons (including visual information), and relevant knowledge-bases such as Wikidata and DBpedia. The result is a knowledge-base that incorporates similarity relations between the images based on visual descriptors, as well as links to the resources of Wikidata and DBpedia that relate to the image. Using the IMGpedia SPARQL endpoint, it is then possible to perform visuo-semantic queries, combining the semantic facts extracted from the external resources and the similarity relations of the images. This paper presents a new web interface to browse and explore the dataset of IMGpedia in a more friendly manner, as well as new visuo-semantic queries that can be answered using 6 million recently added links from IMGpedia to Wikidata. We also discuss future directions we foresee for the IMGpedia project.",Querying Wikimedia Images using Wikidata Facts,NA:NA:NA:NA,2018
Tomás Sáez:Aidan Hogan,"Info-boxes provide a summary of the most important meta-data relating to a particular entity described by a Wikipedia article. However, many articles have no info-box or have info-boxes with only minimal information; furthermore, there is a huge disparity between the level of detail available for info-boxes in English articles and those for other languages. Wikidata has been proposed as a central repository of facts to try to address such disparities, and has been used as a source of information to generate info-boxes. However, current processes still rely on human intervention either to create generic templates for entities of a given type or to create a specific info-box for a specific article in a specific language. As such, there are still many articles of Wikipedia without info-boxes but where relevant data are provided by Wikidata. In this paper, we investigate fully automatic methods to generate info-boxes for Wikipedia from the Wikidata knowledge graph. The primary challenge is to create ranking mechanisms that provide an intuitive prioritisation of the facts associated with an entity. We discuss this challenge, propose several straightforward metrics to prioritise information in info-boxes, and present an initial user evaluation to compare the quality of info-boxes generated by various metrics.",Automatically Generating Wikipedia Info-boxes from Wikidata,NA:NA,2018
Payam Barnaghi:Jean-Paul Calbimonte:Daniele Dell'Aglio,"Applications in different domains require reactive processing of massive, dynamically generated streams of data. This trend is increasingly visible also on the Web, where more and more streaming sources are becoming available. These originate from social networks, sensor networks, the Internet of Things (IoT) and many other technologies that use the Web as a platform for sharing data. This has resulted in new Web-centric efforts such as the Web of Things (WoT), which focuses on exposing and describing the IoT resources on the Web; or the Social Web which provides protocols, vocabularies, and APIs to facilitate access to social communications and interactions on the Web.",Web Stream Processing Workshop Chairs' Welcome & Organization,NA:NA:NA,2018
Mathias De Brouwer:Femke Ongenae:Glenn Daneels:Esteban Municio:Jeroen Famaey:Steven Latré:Filip De Turck,"Enabling real-time collection and analysis of cyclist sensor data could allow amateur cyclists to continuously monitor themselves, receive personalized feedback on their performance, and communicate with each other during cycling events. Semantic Web technologies enable intelligent consolidation of all available context and sensor data. Stream reasoning techniques allow to perform advanced processing tasks by correlating the consolidated data to enable personalized and context-aware real-time feedback. In this paper, these technologies are leveraged and evaluated to design a Proof-of-Concept application of a personalized real-time feedback platform for amateur cyclists. Real-time feedback about the user's heart rate and heart rate training zones is given through a web application. The performance and scalability of the platform is evaluated on a Raspberry Pi. This shows the potential of the framework to be used in real-life cycling by small groups of amateur cyclists, who can only access low-end devices during events and training.",Personalized Real-Time Monitoring of Amateur Cyclists on Low-End Devices: Proof-of-Concept & Performance Evaluation,NA:NA:NA:NA:NA:NA:NA,2018
Maria Bermudez-Edo:Payam Barnaghi,"The data gathered from smart cities can help citizens and city manager planners know where and when they should be aware of the repercussions regarding events happening in different parts of the city. Most of the smart city data analysis solutions are focused on the events and occurrences of the city as a whole, making it difficult to discern the exact place and time of the consequences of a particular event. We propose a novel method to model the events in a city in space and time. We apply our methodology for vehicular traffic data basing our models in (convolutional) neuronal networks.",Spatio-Temporal Analysis for Smart City Data,NA:NA,2018
Julián Andrés Rojas Meléndez:Brecht Van de Vyvere:Arne Gevaert:Ruben Taelman:Pieter Colpaert:Ruben Verborgh,"For smart decision making, user agents need live and historic access to open data from sensors installed in the public domain. In contrast to a closed environment, for Open Data and federated query processing algorithms, the data publisher cannot anticipate in advance on specific questions, nor can it deal with a bad cost-efficiency of the server interface when data consumers increase. When publishing observations from sensors, different fragmentation strategies can be thought of depending on how the historic data needs to be queried. Furthermore, both publish/subscribe and polling strategies exist to publish live updates. Each of these strategies come with their own trade-offs regarding cost-efficiency of the server-interface, user-perceived performance and cpu use. A polling strategy where multiple observations are published in a paged collection was tested in a proof of concept for parking spaces availability. In order to understand the different resource trade-offs presented by publish/subscribe and polling publication strategies, we devised an experiment on two machines, for a scalability test. The preliminary results were inconclusive and suggest more large scale tests are needed in order to see a trend. While the large-scale tests will be performed in future work, the proof of concept helped to identify the technical Open Data principles for the 13 biggest cities in Flanders.",A Preliminary Open Data Publishing Strategy for Live Data in Flanders,NA:NA:NA:NA:NA:NA,2018
Gustavo Gonçalves:Flávio Martins:João Magalhães,"The rise of large data streams introduces new challenges regarding the delivery of relevant content towards an information need. This need can be seen as a broad topic of information. By identifying sub-streams within a broader data stream, we can retrieve relevant content that matches the multiple facets of the topic; thus summarizing information, and matching the initial need. In this paper, we propose to study the generation of sub-streams over time and compare various aggregation methods to summarize information. Our experiments were made using the standard TREC Real-Time Summarization (RTS) 2017 dataset.",Analysis of Subtopic Discovery Algorithms for Real-time Information Summarization,NA:NA:NA,2018
Danh Le-Phuoc,"The join operator is a core component of an RDF Stream Processing engine. The join operations usually dominate the processing load of a query execution plan. Due to the constantly updating nature of continuous queries, the query optimiser has to frequently change the optimal execution plan for a query. However, optimising the join executing plan for every execution step might be prohibitively expensive, hence, dynamic optimisation of continuous join operations is still a challenging problem so far. Therefore, this paper proposes the first adaptive optimisation approach towards this problem in the context of RDF Stream Processing. The approach comes with two dynamic cost-based optimisation algorithms which use a light-weight process to search for the best execution plan for every execution step. The experiments show the encouraging results towards this direction.",Adaptive Optimisation For Continuous Multi-Way Joins Over RDF Streams,NA,2018
Erik Wilde:Mike Amundsen:Mehdi Medjaoui,"Welcome to the 9th International Workshop on Web APIs and Service Architecture (WS-REST). First held in 2010 at WWW in Raleigh, North Carolina, USA, this 2018 edition of the WS-REST Workshop Series is proud to be a part of the renowned WWW conference series in Lyon, France. WS-REST 2018 brings together a community of researcher and practitioners interested in Web APIs and service architecture. Bringing Research and Industry Together In keeping with the history of WS-REST events, the 2018 edition strives to bring together vital content from both the Web Services and REST communities. This year we are hosting two individual tracks (Research and Industry) as a way to continue and strengthen this collaboration between academic and applied experience. The research track submissions received careful peer-review and will be published in the Web Conference proceedings. Industry track submissions focus on field-tested examples and use-cases in the form of extended abstracts or position papers and were selected by the organizers with advice from select program committee members. APIs, Services, and REST APIs have become the connective fabric of the Web and any application area that uses Internet or Web technologies. The goal of WS-REST 2018 is to provide a forum for researchers and practitioners where they can openly and freely exchange ideas about how they are using Web technologies in their APIs, what works and what does not work for them, and what challenges they see in the current landscape of standards and technologies. Our goal is to capture both the state of the art when it comes to Web APIs and service architecture, but to also provide a forum that identifies some of the most pressing issues in that space, and can help solving them.",Workshop on Web APIs and Service Architecture (WS-REST) Chairs' Welcome & Organization,NA:NA:NA,2018
Anastasios Dimanidis:Kyriakos C. Chatzidimitriou:Andreas L. Symeonidis,"Speeding up the development process of Web Services, while adhering to high quality software standards is a typical requirement in the software industry. This is why industry specialists usually suggest ""driven by"" development approaches to tackle this problem. In this paper, we propose such a methodology that employs Specification Driven Development and Behavior Driven Development in order to facilitate the phases of Web Service requirements elicitation and specification. Furthermore, we introduce gherkin2OAS, a software tool that aspires to bridge the aforementioned development approaches. Through the suggested methodology and tool, one may design and build RESTful services fast, while ensuring proper functionality.",A Natural Language Driven Approach for Automated Web API Development: Gherkin2OAS,NA:NA:NA,2018
Tobias Fertig:Peter Braun,"Representational State Transfer (REST) is an efficient and by now established architectural style for distributed hypermedia systems. However, REST has not been designed for offline operations, yet many applications must also keep functioning when going offline for more than a few seconds. Burdening the programmer with knowledge about offline status is undesirable. RESTful applications can be described by a formal model. Therefore, we define a function to derive a formal model of the proxy for handling offline support on the client-side. We then extend existing caching approaches so that a client-side proxy can transparently hide the offline status from the application. We validate our solution with a proxy layer that covers all test cases derived from the model. Using our model and proxy, clients do not have to know and worry about whether they are online or offline.",Towards Offline Support for RESTful Systems,NA:NA,2018
Henry Vu:Tobias Fertig:Peter Braun,"Being an architectural style rather than a specification or a standard, the proper design of REpresentational State Transfer (REST) APIs is not trivial, since developers have to deal with a flood of recommendations and best practices, especially the proper application of the hypermedia constraint requires some decent experience. Furthermore, testing RESTful APIs is a missing topic within literature and especially, hypermedia testing is not mentioned at all. To deal with this state of affairs, we have elaborated a Model-Driven Software Development (MDSD) approach for creating RESTful APIs. As this project matured, we also explored the possibility of Model-Driven Testing (MDT). This work addresses the challenges of hypermedia testing and proposes approaches to overcome them with MDT techniques. We present the results of hypermedia testing for RESTful APIs using a model verification approach that were discovered within our research. MDT enables the verification of the underlying model of a RESTful API and ensuring its correctness before initiating any code generation. Therefore, we can prevent a poorly designed model from being transformed into a poorly designed RESTful API.",Verification of Hypermedia Characteristic of RESTful Finite-State Machines,NA:NA:NA,2018
Sebastian R. Bader:Maria Maleshkova,"A central vision of the Internet of Things is the representation of the physical world in a consistent virtual environment. Especially in the context of smart factories the connection of the different, heterogeneous production modules through a digital shop floor promises faster conversion rates, data-driven maintenance or automated machine configurations for use cases which haven't been recognized at design time. Nevertheless, these scenarios demand IoT representations of all participating machines and components, which requires high installation efforts and hardware adjustments. We propose an incremental process for bringing the shop floor closer to the IoT vision. Currently the majority of systems, components or parts are not yet connected with the internet and might not even provide the possibility to be technically equipped with sensors. However, those could be essential parts for a realistic digital shop floor representation. We therefore propose Virtual Representations, which are capable of independently calculating a physical object's condition by dynamically collecting and interpreting already available data through RESTful Web APIs. The internal logic of such Virtual Representations are further adjustable at runtime since changes to its respective physical object, its environment or updates to the resource itself should not cause any downtime.",Virtual Representations for an Iterative IoT Deployment,NA:NA,2018
Akshay Soni:Aasish Pappu:Robert Busa-Fekete:Krzysztof Dembczynski,"Extreme Multilabel Classification (XMLC) is a very active and rapidly growing research area that deals with the problem of labeling an item with a small set of tags out of an extremely large number of potential tags. Applications include content understanding, document tagging, image tagging, biological sequence tagging, recommendation, etc. While the difficulty and the potential applications of XMLC are well understood in the core machine learning community, to the best of our knowledge, XMLC has not made inroads in the field of Information Retrieval (IR) and related areas. The aim of this workshop is to bring researchers from academia and industry in order to further advance this very exciting field and come up with potential applications of XMLC in new areas.",Extreme Multilabel Classification for Social Media Chairs' Welcome and Organization,NA:NA:NA:NA,2018
Anshumali Shrivastava,"In this talk, I will present Merged-Averaged Classifiers via Hashing (MACH) for K-classification with ultra-large values of K. Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(dłogK)$ (d is dimensionality) memory while only requiring $O(KłogK + dłogK )$ operation for inference. MACH is a generic K-classification algorithm, with provably theoretical guarantees, without any assumption on the relationship between classes. MACH uses universal hashing to reduce classification with a large number of classes to few (logarithmic many) independent classification tasks with small (constant) number of classes. I will show the first quantification of discriminability-memory tradeoff in multi-class classification. Using the simple idea of hashing, we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU, with the classification accuracy of 19.28%, which is the best-reported accuracy on this dataset. Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (160 GB model size) and achieves 9% accuracy. In contrast, MACH can achieve 9% accuracy with 480x reduction in the model size (of mere 0.3GB). With MACH, we also demonstrate complete training of feature extracted fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU. To the best of our knowledge, this is the first work to demonstrate complete training of these extreme-class datasets on a single Titan X. Furthermore, the algorithm is trivially parallelizable. Our experiments show that we can train ODP datasets in 7 hours on a single GPU or in 15 minutes with 25 GPUs. Similarly, we can train classifiers over the fine-grained imagenet dataset in 24 hours on a single GPU which can be reduced to little over 1 hour with 20 GPUs.","Training 100,000 Classes on a Single Titan X in 7 Hours or 15 Minutes with 25 Titan Xs",NA,2018
Manik Varma,"I will introduce extreme classi cation which is a new area of machine learning research focusing on multi-class & multi-label problems involving millions of categories. Extreme classification has opened up a new paradigm for thinking about key applications such as tagging, ranking and recommendation. I will discuss algorithms for some of these applications and present results on tagging on Wikipedia, product recommendation on Amazon and search and advertising on the Bing search engine. More details can be found on The Extreme Classification Repository webpage at http://manikvarma.org/downloads/XC/XMLRepository.html","Extreme Classification: Tagging on Wikipedia, Recommendation on Amazon & Advertising on Bing",NA,2018
Marius Kloft,"Training of multi-class or multi-label classification machines are embarrassingly parallelizable via the one-vs.-rest approach. However, training of all-in-one multi-class learning machines such as multinomial logistic regression or all-in-one multi-class SVMs (MC-SVMs) is not parallelizable out of the box. In my talk, I present optimization strategies to distribute the training of some all-in-one multi-class SVMs over the classes, which makes them appealing for the use in extreme classification.",Distributed Optimization of All-in-one SVMs for Extreme Classfication,NA,2018
Lu Tang:Sougata Chaudhuri:Abraham Bagherjeiran:Ling Zhou,"Structured prediction, where outcomes have a precedence order, lies at the heart of machine learning for information retrieval, movie recommendation, product review prediction, and digital advertising. Ordinal ranking, in particular, assumes that the structured response has a linear ranked order. Due to the extensive applicability of these models, substantial research has been devoted to understanding them, as well as developing efficient training techniques. One popular and widely cited technique of training ordinal ranking models is to exploit the linear precedence order and systematically reduce it to a binary classification problem. This facilitates the usage of readily available, powerful binary classifiers, but necessitates an expansion of the original training data, where the training data increases by $K-1$ times of its original size, with K being the number of ordinal classes. Due to prevalent nature of problems with large number of ordered classes, the reduction leads to datasets which are too large to train on single machines. While approximation methods like stochastic gradient descent are typically applied here, we investigate exact optimization solutions that can scale. In this paper, we present a divide-and-conquer (DC) algorithm, which divides large scale binary classification data into a cluster of machines and trains logistic models in parallel, and combines them at the end of the training phase to create a single binary classifier, which can then be used as an ordinal ranker. It requires no synchronization between the parallel learning algorithms during the training period, which makes training on large datasets feasible and efficient. We prove consistency and asymptotic normality property of the learned models using our proposed algorithm. We provide empirical evidence, on various ordinal datasets, of improved estimation and prediction performance of the model learnt using our algorithm, over several standard divide-and-conquer algorithms.",Learning Large Scale Ordinal Ranking Model via Divide-and-Conquer Technique,NA:NA:NA:NA,2018
Kishaloy Halder:Lahari Poddar:Min-Yen Kan,"In public online discussion forums, the large user base and frequent posts can create challenges for recommending threads to users. Importantly, traditional recommender systems, based on collaborative filtering, are not capable of handlingnever-seen-before items (threads). We can view this task as a form of Extreme Multi-label Classification (XMLC), where for a newly-posted thread, we predict the set of users (labels) who will want to respond to it. Selecting a subset of users from the set of all users in the community poses significant challenges due to scalability, and sparsity. We propose a neural network architecture to solve thisnew thread recommendation task. Our architecture uses stacked bi-directional Gated Recurrent Units (GRU) for text encoding along with cluster sensitive attention for exploiting correlations among the large label space. Experimental evaluation with four datasets from different domains show that our model outperforms both the state-of-the-art recommendation systems as well as other XMLC approaches for this task in terms of MRR, Recall, and NDCG.",Cold Start Thread Recommendation as Extreme Multi-label Classification,NA:NA:NA,2018
Mathieu d'Aquin:Elena Cabrio,"It is our great pleasure to welcome you to the WWW 2018 Challenges Track. It is the first time that the WWW conference includes such a track, which aim was to showcase the maturity of the state of the art on tasks common to the Web community and adjacent academic communities, in a controlled setting of rigorous evaluation. Through our call for challenge organisation, we also wanted to see which open questions might be seen as most relevant in this community today, and how groups of researchers might come together around shared resources (e.g. datasets) to address those questions in a hands-on, practical way. We received five proposals for challenges, and selected four of them, which were proposed by well established researchers in their respective domains. The topics addressed varied from being purely focused on a domain specific task, without constraints on the technical approach to take (e.g. music genre recognition) to fundamentally technical tasks, which can be seen as useful across domains (question answering), with two of the challenges representing a mix of both.",Challenges Track Chairs' Welcome,NA:NA,2018
Michaël Defferrard:Sharada P. Mohanty:Sean F. Carroll:Marcel Salathé,"We here summarize our experience running a challenge with open data for musical genre recognition. Those notes motivate the task and the challenge design, show some statistics about the submissions, and present the results.",Learning to Recognize Musical Genre from Audio: Challenge Overview,NA:NA:NA:NA,2018
Benjamin Murauer:Günther Specht,"This paper summarizes our contribution to the CrowdAI music genre classification challenge ""Learning to Recognise Musical Genre from Audio on the Web'' as part of the WebConference 2018. We utilize different approaches from the field of music analysis to predict the music genre of given mp3 music files, including a convolutional neural network for spectrogram classification, deep neural networks and ensemble methods using various numerical audio features. Our best results were obtained by an extreme gradient boosting classifier.",Detecting Music Genre Using Extreme Gradient Boosting,NA:NA,2018
Jaehun Kim:Minz Won:Xavier Serra:Cynthia C. S. Liem,"The automated recognition of music genres from audio information is a challenging problem, as genre labels are subjective and noisy. Artist labels are less subjective and less noisy, while certain artists may relate more strongly to certain genres. At the same time, at prediction time, it is not guaranteed that artist labels are available for a given audio segment. Therefore, in this work, we propose to apply the transfer learning framework, learning artist-related information which will be used at inference time for genre classification. We consider different types of artist-related information, expressed through artist group factors, which will allow for more efficient learning and stronger robustness to potential label noise. Furthermore, we investigate how to achieve the highest validation accuracy on the given FMA dataset, by experimenting with various kinds of transfer methods, including single-task transfer, multi-task transfer and finally multi-task learning.",Transfer Learning of Artist Group Factors to Musical Genre Classification,NA:NA:NA:NA,2018
Amelie Gyrard:Manas Gaur:Swati Padhee:Amit Sheth:Mihaela Juganaru-Mathieu,"The Web of Things (WoT) is an extension of the Internet of Things (IoT) to ease the access to data generated by things/devices using the benefits of Web technologies. Data is exploited by WoT applications to monitor healthcare or even control home automation devices. The purpose of the Knowledge Extraction for the Web of Things (KE4WoT) challenge is to automatically extract the relevant knowledge from already designed smart WoT applications in various applicative domains. Those applications design and release Knowledge Bases (e.g., datasets and/or models) on the web.",Knowledge Extraction for the Web of Things (KE4WoT): WWW 2018 Challenge Summary,NA:NA:NA:NA:NA,2018
Fabricio F. de Faria:Ricardo Usbeck:Alessio Sarullo:Tingting Mu:Andre Freitas,"This challenge focuses on the use of semantic representation methods to support Visual Question Answering: given a large image collection, find a set of images matching natural language queries. The task supports advancing the state-of-the-art in Visual Question Answering by focusing on methods which explore the interplay between contemporary machine learning techniques, semantic representation and reasoning mechanisms.",Question Answering Mediated by Visual Clues and Knowledge Graphs,NA:NA:NA:NA:NA,2018
Macedo Maia:Siegfried Handschuh:André Freitas:Brian Davis:Ross McDermott:Manel Zarrouk:Alexandra Balahur,"The growing maturity of Natural Language Processing (NLP) techniques and resources is dramatically changing the landscape of many application domains which are dependent on the analysis of unstructured data at scale. The finance domain, with its reliance on the interpretation of multiple unstructured and structured data sources and its demand for fast and comprehensive decision making is already emerging as a primary ground for the experimentation of NLP, Web Mining and Information Retrieval (IR) techniques for the automatic analysis of financial news and opinions online. This challenge focuses on advancing the state-of-the-art of aspect-based sentiment analysis and opinion-based Question Answering for the financial domain.",WWW'18 Open Challenge: Financial Opinion Mining and Question Answering,NA:NA:NA:NA:NA:NA:NA,2018
Chung-Chi Chen:Hen-Hsen Huang:Hsin-Hsi Chen,"This paper decribes our experimental methods and results in FiQA 2018 Task 1. There are two subtasks : (1) to predict continuous sentiment score between -1 to 1, and (2) to determine which aspect(s) are related to the content of financial tweets. First, we propose a preprocessing procedure for decomposing financial tweets. Second, we collect over 334K labeled financial tweets to enlarge the scale of the experiments. Third, the sentiment prediction task is separated into two steps in this paper, i.e., (1) bullish/bearish and (2) sentiment degree. We compare the results of the CNN, CRNN and Bi-LSTM models. Besides, we further combine the results of the best models in both steps as the model of subtask 1. Finally, we make an investigation of aspects in depth, and propose some clues for dealing with the 14 aspects.",Fine-Grained Analysis of Financial Tweets,NA:NA:NA,2018
Shijia E.:Li Yang:Mohan Zhang:Yang Xiang,"Aspect-based financial sentiment analysis, which aims to classify the text instance into a pre-defined aspect class and predict the sentiment score for the mentioned target. In this paper, we propose a neural network model, Attention-based LSTM model with the Aspect information (ALA), to solve the financial opinion mining problem introduced by the WWW 2018 shared task. The proposed neural network model can adapt to the financial dataset so that the neural network can effectively understand the semantic information of the short text. We evaluate our model with the 10-fold cross-validation, and we compare our model with a variety of related deep neural network models.",Aspect-based Financial Sentiment Analysis with Deep Neural Networks,NA:NA:NA:NA,2018
Shijia E.:Shiyao Xu:Yang Xiang,"The goal of question answering with financial data is selecting sentences as answers from the given documents for a question. The core of the task is computing the similarity score between the question and answer pairs. In this paper, we incorporate statistical features such as the term frequency-inverse document frequency (TF-IDF) and the word overlap in convolutional neural networks to learn optimal vector representations of question-answering pairs. The proposed model does not depend on any external resources and can be easily extended to other domains. Our experiments show that the TF-IDF and the word overlap features can improve the performance of basic neural network models. Also, with our experimental results, we can prove that models based on the margin loss training achieve better performance than the traditional classification models. When the number of candidate answers for each question is 500, our proposed model can achieve 0.622 in Top-1 accuracy (Top-1), 0.654 in mean average precision (MAP), 0.767 in normalized discounted cumulative gain (NDCG), and 0.701 in bilingual evaluation understudy (BLEU). If the number of candidate answers is 30, all the values of the evaluation metrics can reach more than 90%.",Incorporating Statistical Features in Convolutional Neural Networks for Question Answering with Financial Data,NA:NA:NA,2018
Hitkul Jangid:Shivangi Singhal:Rajiv Ratn Shah:Roger Zimmermann,Aspect based sentiment analysis aims to detect an aspect (i.e. features) in a given text and then perform sentiment analysis of the text with respect to that aspect. This paper aims to give a solution for the FiQA 2018 challenge subtask 1. We perform aspect-based sentiment analysis on the microblogs and headlines of financial domain. We use a multi-channel convolutional neural network for sentiment analysis and a recurrent neural network with bidirectional long short-term memory units to extract aspect from a given headline or microblog. Our proposed model produces a weighted average F1 score of 0.69 for the aspect extraction task and predicts sentiment intensity scores with a mean squared error of 0.112 on 10-fold cross validation. We believe that the developed system has direct applications in the financial domain.,Aspect-Based Financial Sentiment Analysis using Deep Learning,NA:NA:NA:NA,2018
Dayan de França Costa:Nadia Felix Felipe da Silva,"This paper describes our system which participate in Task 1 of FiQA 2018. The task's focuses was to predict sentiment and aspects of financial microblog posts and headlines. The sentiment analysis for a specific company had to be predicted using a scale between -1 and 1, while the aspect prediction had to be predicted using a set of aspects which was given in train data. We had used Support Vector Regression (SVR) to predict the sentiments in both cases (microblog posts and headlines).",INF-UFG at FiQA 2018 Task 1: Predicting Sentiments and Aspects on Financial Tweets and News Headlines,NA:NA,2018
Guangyuan Piao:John G. Breslin,"In this paper, we describe our ensemble approach for sentiment and aspect predictions in the financial domain for a given text. This ensemble approach uses Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) with a ridge regression and a voting strategy for sentiment and aspect predictions, and therefore, does not rely on any handcrafted feature. Based on 5-cross validation on the released training set, the results show that CNNs overall perform better than RNNs on both tasks, and the ensemble approach can boost the performance further by leveraging different types of deep learning approaches.",Financial Aspect and Sentiment Predictions with Deep Neural Networks: An Ensemble Approach,NA:NA,2018
