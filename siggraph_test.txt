Fei Wang:Shujin Lin:Ruomei Wang:Yi Li:Baoquan Zhao:Xiaonan Luo	Our method shortens the time of fluid simulation by coupling the two conditions of density-invariant and divergence-free, and achieves the same simulation effect compared with other methods. Further, we regard the displacement of particles as the only basic variable of the continuity equation, which improves the stability of the fluid to a certain extent.	Improving incompressible SPH simulation efficiency by integrating density-invariant and divergence-free conditions	NA:NA:NA:NA:NA:NA	2018
Jiangyan Han:Ishtiaq Rasool Khan:Susanto Rahardja	We propose an adaptive tone mapping method for displaying HDR images according to ambient light conditions. To compensate the loss of perceived luminance in brighter viewing conditions, we enhance the HDR image by an algorithm based on the Naka-Rushton model. Changes of the HVS response under different adaptation levels are considered and we match the response under the ambient conditions with the plateau response to the original HDR scene. The enhanced HDR image is tone mapped through a tone mapping curve constructed by the original image luminance histogram to produce visually pleasing images under given viewing conditions.	Lighting condition adaptive tone mapping method	NA:NA:NA	2018
Tobias Bertel:Christian Richardt	Capturing 360° panoramas has become straightforward now that this functionality is implemented on every phone. However, it remains difficult to capture immersive 360° panoramas with motion parallax, which provide different views for different viewpoints. Alternatives such as omnidirectional stereo panoramas provide different views for each eye (binocular disparity), but do not support motion parallax, while Casual 3D Photography [Hedman et al. 2017] reconstructs textured 3D geometry that provides motion parallax but suffers from reconstruction artefacts. We propose a new image-based approach for capturing and rendering high-quality 360° panoramas with motion parallax. We use novel-view synthesis with flow-based blending to turn a standard monoscopic video into an enriched 360° panoramic experience that can be explored in real time. Our approach makes it possible for casual consumers to capture and view high-quality 360° panoramas with motion parallax.	MegaParallax: 360° panoramas with motion parallax	NA:NA	2018
Antoine Toisoul:Daljit Singh J. Dhillon:Abhijeet Ghosh	We present a novel approach to measure the appearance of commonly found spatially varying holographic surfaces. Such surfaces are made of one dimensional diffraction gratings that vary in orientations and periodicities over a sample to create impressive visual effects. Our method is able to recover the orientation and periodicity maps simply using a flash illumination and a DSLR camera. We present real-time renderings under environmental illumination using the measured maps that match the observed appearance.	Practical acquisition and rendering of common spatially varying holographic surfaces	NA:NA:NA	2018
Yuliya Gitlina:Daljit Singh J. Dhillon:Jan Hansen:Dinesh K. Pai:Abhijeet Ghosh	Realistic appearance modeling of human skin is an important research topic with a variety of application in computer graphics. Various diffusion based BSSRDF models [Jensen et al. 2001, Donner and Jensen 2005, Donner and Jensen 2006] have been introduced in graphics to efficiently simulate subsurface scattering in skin including modeling its layered structure. These models however assume homogeneous subsurface scattering parameters and produce spatial color variation using an albedo map. In this work, we build upon the spectral scattering model of [Donner and Jensen 2006] and target a practical measurement-based rendering approach for such a spectral BSSRDF. The model assumes scattering in the two primary layers of skin (epidermis and dermis respectively) can be modeled with relative melanin and hemoglobin chromophore concentrations respectively. To drive this model for realistic rendering, we employ measurements of skin patches using an off-the-shelf Miravex Antera 3D camera which provides spatially varying maps of these chromophore concentrations as well as corresponding 3D surface geometry (see Figure 1) using a custom imaging setup.	Practical measurement-based spectral rendering of human skin	NA:NA:NA:NA:NA	2018
Markus Schuetz:Michael Wimmer	Rendering tens of millions of points in real time usually requires either high-end graphics cards, or the use of spatial acceleration structures. We introduce a method to progressively display as many points as the GPU memory can hold in real time by reprojecting what was visible and randomly adding additional points to uniformly converge towards the full result within a few frames. Our method heavily limits the number of points that have to be rendered each frame and it converges quickly and in a visually pleasing way, which makes it suitable even for notebooks with low-end GPUs. The data structure consists of a randomly shuffled array of points that is incrementally generated on-the-fly while points are being loaded. Due to this, it can be used to directly view point clouds in common sequential formats such as LAS or LAZ while they are being loaded and without the need to generate spatial acceleration structures in advance, as long as the data fits into GPU memory.	Progressive real-time rendering of unprocessed point clouds	NA:NA	2018
Anastasia Feygina:Dmitry I. Ignatov:Ilya Makarov	In this talk, we show a realistic post-processing rendering based on generative adversarial network CycleWGAN. We propose to use CycleGAN architecture and Wasserstein loss function with additional identity component in order to transfer graphics from Grand Theft Auto V to the older version of GTA video-game, Grand Theft Auto: San Andreas. We aim to present the application of modern art style transfer and unpaired image-to-image translations methods for graphics improvement using deep neural networks with adversarial loss.	Realistic post-processing of rendered 3D scenes	NA:NA:NA	2018
Yen-Chih Chiang:Shih-Song Cheng:Huei-Siou Chen:Le-Jean Wei:Li-Min Huang:David KT Chu	Currently1, Visual Reality Head-mounted Display has several problems that need to be overcome, such as insufficient resolution of the display, latency, Vergence-accommodation Conflict, etc., while the resolution is not high enough, causing the virtual image of the display to have graininess or Screen-door Effect. These problems have brought VR users an imperfect image quality experience and are unable to achieve a good sense of immersion. Therefore, it is necessary to solve the problem of insufficient display resolution. INT TECH Co., is working towards this goal and has made very good progress.	Retinal resolution display technology brings impact to VR industry	NA:NA:NA:NA:NA:NA	2018
Keiko Nakamoto:Takafumi Koike	We present an improved method for rendering heterogeneous translucent materials with existing BSSRDF models. In the general BSSRDF models, the optical properties of the target object are constant. Sone et al. have proposed a method to combine with existing BSSRDF models for rendering heterogeneous materials. However, the method generates more bright and blurred images compared with correctly simulated images. We have experimented with various BSSRDF models by the method and rendered heterogeneous materials. As a result, the rendered image with the better dipole model is the closest to the result of Monte carlo simulation. If incorporating the better dipole model into the method proposed by Sone et al., we can render more realistic images of heterogeneous materials.	Which BSSRDF model is better for heterogeneous materials?	NA:NA	2018
Nao Asano:Katsutoshi Masai:Yuta Sugiura:Maki Sugimoto	Facial performance capture is used for animation production that projects a performer's facial expression to a computer graphics model. Retro-reflective markers and cameras are widely used for the performance capture. To capture expressions, we need to place markers on the performer's face and calibrate the intrinsic and extrinsic parameters of cameras in advance. However, the measurable space is limited to the calibrated area. In this study, we propose a system to capture facial performance using a smart eyewear with photo-reflective sensors and machine learning technique. Also, we show a result of principal components analysis of facial geometry to determine a good estimation parameter set.	3D facial geometry analysis and estimation using embedded optical sensors on smart eyewear	NA:NA:NA:NA	2018
Paul Canada:George Ventura:Christopher Iossa:Orquidia Moreno:William J. Joel	Motion capture (MoCap) has been one of the leading and most useful tools within the field of animation to capture fluid and detailed motion. However, it can be quite expensive for animators, game developers and educators on a tight budgets. By using Raspberry Pi Zeros, with NoIR cameras and IR LED light rings, the cost of a four-camera system can potentially be reduced to less than 1000 USD. The research described should lead to an effective and useful system, able to detect multiple markers, record their coordinates, and keep track of them as they move. With a setup of three or more cameras, one would be able to triangulate the data on a low-cost host computer. All software and hardware designs will be disseminated open source, providing anyone who is interested in MoCap, whether it be for hobbyist, semi-professional, or educational purposes, a system for a fraction of the typical cost.	Development of an open source motion capture system	NA:NA:NA:NA:NA	2018
Kaizhang Kang:Zimin Chen:Jiaping Wang:Kun Zhou:Hongzhi Wu	Digitally acquiring high-quality material appearance from the real-world is challenging, with applications in visual effects, e-commerce and entertainment. One popular class of existing work is based on hand-derived illumination multiplexing [Ghosh et al. 2009], using hundreds of patterns in the most general case [Chen et al. 2014].	Learning optimal lighting patterns for efficient SVBRDF acquisition	NA:NA:NA:NA:NA	2018
Yoichi Ochiai:Kazuki Otao:Yuta Itoh:Shouki Imai:Kazuki Takazawa:Hiroyuki Osone:Atsushi Mori:Ippei Suzuki	Retinal projection is required for xR applications that can deliver immersive visual experience throughout the day. If general-purpose retinal projection methods can be realized at a low cost, not only could the image be displayed on the retina using less energy, but there is also a possibility of cutting off the weight of projection unit itself from the AR goggles. Several retinal projection methods have been previously proposed. Maxwellian optics based retinal projection was proposed in 1990s [Kollin 1993]. Laser scanning [Liao and Tsai 2009], laser projection using spatial light modulator (SLM) or holographic optical elements were also explored [Jang et al. 2017]. In the commercial field, QD Laser1 with a viewing angle of 26 degrees is available. However, as the lenses and iris of an eyeball are in front of the retina, which is a limitation of a human eyeball, the proposal of retinal projection is generally fraught with narrow viewing angles and small eyebox problems. Due to these problems, retinal projection displays are still a rare commodity because of their difficulty in optical schematics design.	Make your own retinal projector: retinal near-eye displays via metamaterials	NA:NA:NA:NA:NA:NA:NA:NA	2018
Kenta Yamamoto:Kotaro Omomo:Kazuki Takazawa:Yoichi Ochiai	The sun is the most universal, powerful and familiar energy available on the planet. Every organism and plant has evolved over the years, corresponding to the energy brought by the sun. Humanity is no exception. We have invented many artificial lights since Edison invented light bulbs. In recent years, LEDs are one of the most representative examples. Displays and projectors using LEDs are still being actively developed. However, it is difficult to reproduce ideal light with high brightness and wide wavelength like sunlight. Furthermore, considering low energy sustainability and environmental contamination in the manufacturing process, artificial light can not surpass the sunlight. Against this backdrop, projects that utilize sunlight have been actively carried out in the world. Concentrating Solar Power (CSP) generate electricity using the heat of sunlight to turn turbines [Müller-Steinhagen and Trieb 2004]. [Koizumi 2017] is an aerial image presentation system using the sun as a light source. Digital sundials use the shadow of sunlight to inform digital time [Scharstein et al. 1996]. These projects attempt to use the direct sunlight without any conversion and minimize the energy loss.	Solar projector	NA:NA:NA:NA	2018
Simone Barbieri:Tao Jiang:Ben Cawthorne:Zhidong Xiao:Xiaosong Yang	While 3D animation is constantly increasing its popularity, 2D is still largely in use in animation production. In fact, 2D has two main advantages. The first one is economic, as it is more rapid to produce, having a dimension less to consider. The second one is important for the artists, as 2D characters usually have highly distinctive traits, which are lost in a 3D transposition. An iconic example is Mickey Mouse, whom ears appear circular no matter which way he is facing.	3D content creation exploiting 2D character animation	NA:NA:NA:NA:NA	2018
Huiyi Fang:Kenji Funahashi	Human eyes have an adjustment function to adjust for different distances of seeing. However, it becomes weaker as you get older. When you move paper closer to read small letters, it is not in focus. When you move it away to bring it into focus, it is too small to read. This condition is called Presbyopia. People suffering from presbyopia also suffer from this condition when they use a smartphone or tablet. Although they can magnify the display using the pinch operation, it is a bother. A method for automatic display zoom, to see detail and an overview, was proposed in [Satake et al. 2016]. This method measures the distance between a face and a screen to judge whether you want to see detail or an overview. When you move it close to your face, it judges you want to see detail and zooms in. When you move it away from your face, it judges that you want to see overview and zooms out. In this paper, we improve and apply this method for presbyopia. First we observe and analyze the behavior of presbyopic people when trying to read small letters. Then we propose a suitable zooming function, for example, a screen is zoomed in also when it is moved away if the person suffers from presbyopia.	Automatic display zoom for people suffering from Presbyopia	NA:NA	2018
Buck Barbieri:Naomi Hutchens:Kayleigh Harrison	Massive Collaborative Animation Projects (MCAP) was founded in 2016 by Dr. William Joel (Western Connecticut State University) to test students' collaborative abilities and provide experience that will allow them to grow professionally and academically. The MCAP 1 production is a children's ghost story designed to test the massive collaborative structure. The goal of MCAP 2 is to create an animation for use in planetariums worldwide. Currently, there are nearly one hundred student contributors from universities in Alaska, California, Colorado, Connecticut, Japan, Michigan, South Korea, and Taiwan.	Collaborative animation production from students' perspective: creating short 3D CG films through international team-work	NA:NA:NA	2018
Martin Kilian:Hui Wang:Eike Schling:Jonas Schikore:Helmut Pottmann	The computation and construction of curved beams along freeform skins pose many challenges. We show how to use surfaces of constant mean curvature (CMC) to compute beam networks with beneficial properties, both aesthetically and from a fabrication perspective. To explore variations of such networks we introduce a new discretization of CMC surfaces as quadrilateral meshes with spherical vertex stars and right node angles. The computed non-CMC surface variations can be seen as a path in design space - exploring possible solutions in a neighborhood, or represent an actual erection sequence exploiting elastic material behavior.	Curved support structures and meshes with spherical vertex stars	NA:NA:NA:NA:NA	2018
Or Fleisher:Shirin Anlen	This paper presents Volume, a software toolkit that enables users to experiment with expressive reconstructions of archival and/or historical materials as volumetric renderings. Making use of contemporary deep learning methods, Volume re-imagines 2D images as volumetric 3D assets. These assets can then be incorporated into virtual, augmented and mixed reality experiences.	Volume: 3D reconstruction of history for immersive platforms	NA:NA	2018
Vincent Gaubert:Enki Londe:Thibaut Poittevin:Alain Lioret	We propose a new approach to 3D mesh fracturing for the fields of animation and game production. Through the use of machine learning and computer vision to analyze real fractures we produced a solution capable of creating realistic fractures in real-time.	3D-mesh cutting based on fracture photographs	NA:NA:NA:NA	2018
Hye sun Kim:Yun ji Ban:Chang joon Park	We present a technique to generate realistic high quality texture with no seams suitable to reconstruct large-scale 3D terrains. We focused on adjusting color difference caused by camera variations and illumination transition for texture reconstruction pipelines. Seams between separated processing areas should also be considered important in large terrain models. The proposed technique corrects these problems by normalizing texture colors and interpolating texture adjustment colors.	A seamless texture color adjustment method for large-scale terrain reconstruction	NA:NA:NA	2018
Kenta Yamamoto:Riku Iwasaki:Tatsuya Minagawa:Ryota Kawamura:Bektur Ryskeldiev:Yoichi Ochiai	3D printing failures can occur without completion of printing process due to shaking, errors in printer settings, and shape of the support material and 3D model. In such case it could be difficult to restart printing process from the last printed layer in conventional 3D printers, as the printing parts to which the nozzles are supposed to be attached are lost. In order to restart printing from the middle layer, Wu et al.[Wu et al. 2017] proposed a method of printing while rotating the base of a 3D printer. However, such approach required time for two objects to bond after segmentation, with limited availability of methods for adhesion between parts. Wu et al.[Wu et al. 2016] have also proposed a method to print 3D models at any angle through 5-axis rotation of the base of a 3D printer, but the manufacturing cost of such approach was relatively high. Therefore, we propose a system that prints 3D models on existing object by utilizing an infrared depth camera. Our method makes it possible to attach a 3D-printed object into a free-formed object in the middle of printing by recognizing its shape with a depth camera.	BOLCOF: base optimization for middle layer completion of 3D-printed objects without failure	NA:NA:NA:NA:NA:NA	2018
Byungjun Kwon:Moonwon Yu:Hanyoung Jang:KyuHyun Cho:Hyundong Lee:Taesung Hahn	This paper presents a novel motion transfer algorithm that copies content motion into a specific style character. The input consists of two motions. One is a content motion such as walking or running, and the other is movement style such as zombie or Krall. The algorithm automatically generates the synthesized motion such as walking zombie, walking Krall, running zombie, or running Krall. In order to obtain natural results, the method adopts the generative power of deep neural networks. Compared to previous neural approaches, the proposed algorithm shows better quality, runs extremely fast, does not require big data, and supports user-controllable style weights.	Deep motion transfer without big data	NA:NA:NA:NA:NA:NA	2018
Xiaodong Cun:Feng Xu:Chi-Man Pun:Hao Gao	Synthesizing images of novel viewpoints is widely investigated in computer vision and graphics. Most works in this topic focus on using multi-view images to synthesize viewpoints in-between. In this paper, we consider extrapolation, and we take a step further to do extrapolation from one single input image. This task is very challenging for two major reasons. First, some parts of the scene may not be observed in the input viewpoint but are required for novel ones. Second, 3D information is lacking for single view input but is crucial to determine pixel movements between viewpoints. Although very challenging, we observe that human brains are always able to imagine novel viewpoints. The reason is that human brains have learned in our daily lives to understand the depth order of objects in a scene [Chen et al. 2016] and infer what the scene looks like when viewing from another viewpoint.	Depth assisted full resolution network for single image-based view synthesis	NA:NA:NA:NA	2018
Sherzod Salokhiddinov:Seungkyu Lee	Depth estimation from differently focused set of images has been a practical approach for 3D reconstruction with existing color cameras. In this paper, we propose a depth from focus (DFF) method for accurate depth estimation using single commodity color camera. We investigate the appearance changes in spatial and frequency domain along the focused image frames in iterative manner. In order to achieve sub-frame level accuracy in depth estimation, optimal location of in-focus frame is estimated by fitting a parameterized polynomial curve on the dissimilarity measurements of each pixel. Quantitative and qualitative evaluations on various test image sets show promising performance of the proposed method in depth estimation.	Depth from focus for 3D reconstruction by iteratively building uniformly focused image set	NA:NA	2018
Chloe LeGendre:Kalle Bladin:Bipin Kishore:Xinglei Ren:Xueming Yu:Paul Debevec	We propose a variant to polarized gradient illumination facial scanning which uses monochrome instead of color cameras to achieve more efficient and higher-resolution results. In typical polarized gradient facial scanning, sub-millimeter geometric detail is acquired by photographing the subject in eight or more polarized spherical gradient lighting conditions made with white LEDs, and RGB cameras are used to acquire color texture maps of the subject's appearance. In our approach, we replace the color cameras and white LEDs with monochrome cameras and multispectral, colored LEDs, leveraging that color images can be formed from successive monochrome images recorded under different illumination colors. While a naive extension of the scanning process to this setup would require multiplying the number of images by number of color channels, we show that the surface detail maps can be estimated directly from monochrome imagery, so that only an additional n photographs are required, where n is the number of added spectral channels. We also introduce a new multispectral optical flow approach to align images across spectral channels in the presence of slight subject motion. Lastly, for the case where a capture system's white light sources are polarized and its multispectral colored LEDs are not, we introduce the technique of multispectral polarization promotion, where we estimate the cross- and parallel-polarized monochrome images for each spectral channel from their corresponding images under a full sphere of even, unpolarized illumination. We demonstrate that this technique allows us to efficiently acquire a full color (or even multispectral) facial scan using monochrome cameras, unpolarized multispectral colored LEDs, and polarized white LEDs.	Efficient multispectral facial capture with monochrome cameras	NA:NA:NA:NA:NA:NA	2018
Nobuhiko Mukai:Taishi Nishikawa:Youngha Chang	In this paper, we report evaluation of thin stretched thread lengths in spinnability simulations. There are many previous studies related to viscoelastic fluid, however, there are few studies that represent "spinnability", which is a feature that the material is stretched thin and long. Although some studies represented thread-forming property, they did not evaluate the stretched length of the material. We also tried to represent spinnability of viscoelastic fluid, however, the simulation results were not similar to a real material. Therefore, we try to perform spinnability simulations with three kinds of models, and evaluate stretched thread lengths by comparison of simulation results with a literature datum.	Evaluation of stretched thread lengths in spinnability simulations	NA:NA:NA	2018
Yuji Suzuki:Jotaro Shigeyama:Shigeo Yoshida:Takuji Narumi:Tomohiro Tanikawa:Michitaka Hirose	Food texture plays an important role in the experience of food. Researchers have proposed various methods to manipulate the perception of food texture using auditory and physical stimulation. In this paper, we demonstrate a system to present visually modified mastication movements in real-time to manipulate the perception of food texture, because visual stimuli efficiently work to enrich other food-related perceptions and showing someone their deformed posture changes somatosensory perception. The result of our experiments suggested that adding real-time feedback of facial deformation when participants open their mouths can increase the perceived chewiness of foods. Moreover, perceptions of hardness and adhesiveness were improved when the participants saw their modified face or listened to their non-modified chewing sound, while both perceptions were decreased when participants were presented with both stimuli. These results indicate the occurrence of the contrast effect.	Food texture manipulation by face deformation	NA:NA:NA:NA:NA:NA	2018
Quan Qi:Qingde Li	Converting a surface-based objects into a thin-surface solid representation is an essential problem for additive manufacturing. This paper proposes a simple way to thicken surfaces to thin solids based on implicit modelling technique. With the proposed technique, any surface-based object can be converted into a 3D printing friendly form that seamlessly combines both the geometric shape and its interior material structures in one single representation.	From visible to printable: thin surface with implicit interior structures	NA:NA	2018
Ivo Aluízio Stinghen Filho:Estevam Nicolas Chen:Jucimar Maia da Silva Junior:Ricardo da Silva Barboza	In this paper we compare the effectiveness of various methods of machine learning algorithms for real-time hand gesture recognition, in order to find the most optimal way to identify static hand gestures, as well as the most optimal sample size for use during the training step of the algorithms. In our framework, Leap Motion and Unity were used to extract the data. The data was then used to be trained using Python and scikit-learn. Utilizing normalized information regarding the hands and fingers, we managed to get a hit rate of 97% using the decision tree classifier.	Gesture recognition using leap motion: a comparison between machine learning algorithms	NA:NA:NA:NA	2018
Yifan Men:Zeyu Shen:Dawar Khan:Dong-Ming Yan	We present a novel method for valence optimization of the Centroidal Voronoi Tessellation (CVT). We first identify three commonly appeared atomic configurations of local irregular Voronoi cells, and then design specific atomic operations for each configuration to improve the regularity within the CVT framework.	Improving regularity of the centoridal voronoi tessellation	NA:NA:NA:NA	2018
Yeonho Kim:Daijin Kim	This paper presents a dance performance evaluation how well a learner mimics the teacher's dance as follows. We estimate the human skeletons, then extract dance features such as torso and first and second-degree feature, and compute the similarity score between the teacher and the learner dance sequence in terms of timing and pose accuracies. To validate the proposed dance evaluation method, we conducted several experiments on a large K-Pop dance database. The proposed methods achieved 98% concordance with experts' evaluation on dance performance.	Interactive dance performance evaluation using timing and accuracy similarity	NA:NA	2018
Ming-Shiuan Chen:I-Chao Shen:Chun-Kai Hunag:Bing-Yu Chen	In recent years, personalized fabrication has attracted many attentions due to the widespread of consumer-level 3D printers. However, consumer 3D printers still suffer from shortcomings such as long production time and limited output size, which are undesirable factors to large-scale rapid-prototyping. We propose a hybrid 3D fabrication method that combines 3D printing and Zometool structure for both time/cost-effective fabrication of large objects. The key of our approach is to utilize compact, sturdy and re-usable internal structure (Zometool) to infill fabrications and replace both time and material-consuming 3D-printed materials. Unlike the laser-cutted shape used in [Song et al. 2016], we are able to reuse the inner structure. As a result, we can significantly reduce the cost and time by printing thin 3D external shells only.	Large-scale fabrication with interior zometool structure	NA:NA:NA:NA	2018
Ryota Natsume:Tatsuya Yatagawa:Shigeo Morishima	This abstract introduces a generative neural network for face swapping and editing face images. We refer to this network as "region-separative generative adversarial network (RSGAN)". In existing deep generative models such as Variational autoencoder (VAE) and Generative adversarial network (GAN), training data must represent what the generative models synthesize. For example, image inpainting is achieved by training images with and without holes. However, it is difficult or even impossible to prepare a dataset which includes face images both before and after face swapping because faces of real people cannot be swapped without surgical operations. We tackle this problem by training the network so that it synthesizes synthesize a natural face image from an arbitrary pair of face and hair appearances. In addition to face swapping, the proposed network can be applied to other editing applications, such as visual attribute editing and random face parts synthesis.	RSGAN: face swapping and editing using face and hair representation in latent spaces	NA:NA:NA	2018
Danny Huang:Ian Stavness	Many thin tissues, such as leaves and flower petals, exhibit rippling and buckling patterns along their edge as they grow (Figure 1). Experiments with plastic materials have replicated the rippling patterns found in nature and shown that such patterns exhibit a fractal quality of ripples upon ripples --- a so called "buckling cascade" [Eran et al. 2004]. Such patterns are influenced by many physical mechanisms, including stress forces, physical properties of materials (e.g., stiffness), and space constraints [Prusinkiewicz and Barbier de Reuille 2010]. Physics-based computer animation that produces emergent rippling patterns on thin surface can improve the realism of virtual flowers and leaves, and also help to explain which physical mechanisms are most important for controlling the morphology of tissues with buckling cascades.	Simulation of emergent rippling on growing thin-shells	NA:NA	2018
Feier Cao:MHD Yamen Saraiji:Kouta Minamizawa	Wearable technologies have been supporting and augmenting our body and sensory functions for a long time. Skin+ introduces a novel bidirectional on-skin interface that serve not only as haptic feedback to oneself but also as a visual display to mediate touch sensation to others as well. In this paper, we describe the design of Skin+ and its usability in a variety of applications. We use a shape-changing auxetic structure to build this programmable coherent visuo-tactile interface. The combination of shape-memory alloy with an auxetic structure enables a lightweight haptic device that can be worn seamlessly on top of our skin.	Skin+: programmable skin as a visuo-tactile interface	NA:NA:NA	2018
Abdelhak Saouli:Mohamed Chaouki Babahenini	The human brain is constantly solving enormous and challenging optimization problems in vision. Due to the formidable meta-heuristics engine our brain equipped with, in addition to the widespread associative inputs from all other senses that act as the perfect initial guesses for a heuristic algorithm, the produced solutions are guaranteed to be optimal. By the same token, we address the problem of computing the depth and normal maps of a given scene under a natural but unknown illumination utilizing particle swarm optimization (PSO) to maximize a sophisticated photo-consistency function. For each output pixel, the swarm is initialized with good guesses starting with SIFT features as well as the optimal solution (depth, normal) found previously during the optimization. This leads to significantly better accuracy and robustness to textureless or quite specular surfaces.	Towards a stochastic depth maps estimation for textureless and quite specular surfaces	NA:NA	2018
Dominic Branchaud:Walter Muskovic:Maria Kavallaris:Daniel Filonik:Tomasz Bednarz	An innovative fully interactive and ultra-high resolution navigation tool has been developed to browse and analyze gene expression levels from human cancer cells, acting as a visual microscope on data. The tool uses high-performance visualization and computer graphics technology to enable genome scientists to observe the evolution of regulatory elements across time and gain valuable insights from their dataset as never before.	Visual microscope for massive genomics datasets, expanded perception and interaction	NA:NA:NA:NA:NA	2018
Richard Clegg:Richard Hoover:Chris McLaughlin	35 years after the release of the original "Blade Runner" film, the visual effects teams behind "Blade Runner 2049" were tasked with the challenge of crafting a dystopian world in the next phase of one of the most-beloved sci-fi films of all time. Set 30 years after the first film, the sequel follows a new blade runner as he unearths a long-buried secret that has the potential to plunge what's left of society into chaos. From the creation of the LA cityscapes, Las Vegas, and Trash Mesa environments to the development of a holographic Joi and the return of Rachael, join the filmmakers from DNEG, Framestore, and MPC as they discuss their Academy-Award winning work that paid tribute to the original picture while creating a film of the future.	DNEG, framestore, and MPC present: the visual effects of "Blade Runner 2049"	NA:NA:NA	2018
Thomas Hullin:Isabelle Langlois	In this production session, we will share our story of working on the legendary show, "Game of Thrones, "since the series' fourth season, detailing the learnings and knowledge we have gained from our multi-season experience on the groundbreaking show. We will go in depth on two of season 7's most intense sequences, starting from the concept art and working through the processes that got us to the final shots.	"Game of Thrones" season 7: orchestrating sea battles and blowing up a big wall	NA:NA	2018
Ian Failes:Rob Bredow:Matt Estela:Mark Hodgkins:Michael Kaschalk:Andy Hayes	In 1996, SideFX released Houdini version 1.0, bringing the power of procedural methods to visual effects artists around the world. This year, more than two decades since Houdini's original release, SideFX was awarded a Scientific and Technical Academy Award of Merit to recognize its continual innovation and dedication to visual effects artists.	Generations of Houdini in film	NA:NA:NA:NA:NA:NA	2018
Rob Bredow:Patrick Tubach:Greg Kegel:Joseph Kasparian	Join the visual effects team as they take you behind the scenes on one of 2018's biggest films. The team will showcase the innovative shooting techniques developed for the film and the unique collaboration with Director Ron Howard that allowed this chapter in the Star Wars universe to be brought to the screen. The team will also pull back the curtain on how they took old school methodologies and combined them with cutting edge technologies to create the film's groundbreaking visual effects work.	Making the kessel run in less than 12 parsecs: the VFX of "Soloa: Star Wars Story"	NA:NA:NA:NA	2018
Mahyar Abousaeedi:Beth Albright:Evan Bonifacio:Chris Burrows:Gordon Cameron:Ralph Eggleston:Nathan Fariss:Fran Kalal:Paul Kanyuk:Ted Mathot:Philip Metschan:Tom Nettleship:Bret Parker:Darwyn Peachey:Reid Sandros:Rick Sayre:Stephen Schaffer:Erik Smitt:Esdras Varagnolo:Bill Watral:Bill Wise	In a conversation that will not only span multiple disciplines, but also multiple years of technological advancement at Pixar, the team behind "Incredibles 2" - many of whom also worked on the first film - will compare and contrast the filmmaking process then and now. With a sequel, there's always the challenge of making a film true to the original, yet different in every detail. In building the world of "Incredibles 2" the team tackled one of the most technically daunting films in Pixar's canon, all while needing it to hue to the familiar tone established by the first film. Hear from this super group as they examine how they used the past to inform the present and, incredibly, achieved the near-impossible.	The Incredibles 2: suit up, it might get weird!	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Grady Cofer:David Shirk:David Dally:Scott Meadows:Ryan Magid	In this deep dive into Steven Spielberg's "Ready Player One" teams from Industrial Light & Magic and Digital Domain will showcase the break through virtual production techniques and technology deployed for the film and the visual effects involved in bringing the film's dystopian vision of life in 2045 to the screen. In addition, the teams will delve into the immense artistic and technical challenges of designing, building and animating every aspect of the expansive virtual universe known as the OASIS.	Three keys to creating the world of "ready player one" visual effects & virtual production	NA:NA:NA:NA:NA	2018
Koki Nagano:Jaewoo Seo:Kyle San:Aaron Hong:Mclean Goldwhite:Jun Xing:Stuti Rastogi:Jiale Kuang:Aviral Agarwal:Hanwei Kung:Caleb Arthur:Carrie Sun:Stephen Chen:Jens Fursund:Hao Li	A deep learning-based technology for generating photo-realistic 3D avatars with dynamic facial textures from a single input image is presented. Real-time performance-driven animations and renderings are demonstrated on an iPhone X and we show how these avatars can be integrated into compelling virtual worlds and used for 3D chats.	Deep learning-based photoreal avatars for online virtual worlds in iOS	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Cory Strassberger:Remco Sikkema	Kite & Lighting reveals how Xsens inertial mocap technology, used in tandem with an iPhone X, can be used for full body and facial performance capture - wirelessly and without the need for a mocap volume - with the results live-streamed to Autodesk Maya in real time.	Democratising mocap: real-time full-performance motion capture with an iPhone X, Xsens, and Maya	NA:NA	2018
Sam Glassenberg:Matthew Yaeger	Enter Gastro Ex for on smartphones and VR. The entire environment surrounding you is interactable and "squishy," featuring advanced soft-body physics and 3D interactive fluid dynamics. Grab anything. Cut anything. Inject anywhere. Unleash argon plasma. Enjoy emergent surgical gameplay, rendered with breathtaking real-time GI and subsurface scattering.	Gastro Ex: real-time interactive fluids and soft tissues on mobile and VR	NA:NA	2018
Tobias Soffner:Christopher Baumbach	IKEA Immerse is available in select IKEA stores in Germany. This application enables consumers to create, experience, and share their own configurations in a virtual living and kitchen room set. With seamless e-commerce integration, a high level of detail, and real-time interaction, the VR experience represents an engaging, valuable touch-point.	IKEA immerse interior designer	NA:NA	2018
Taehyun Rhee:Andrew Chalmers:Ian Loh:Ben Allen:Lohit Petikam:Stephen Thompson:Tom Revill	An interactive mixed reality system using live streamed 360° panoramic videos is presented. A live demo for real-time image-based lighting, light detection, mixed reality rendering, and composition of 3D objects into a live-streamed 360° video of a real-world environment with dynamically changing real-world lights is shown.	Mixed reality 360 live: live blending of virtual objects into 360° streamed video	NA:NA:NA:NA:NA:NA:NA	2018
Chris Harvey:Mike Blomkamp:Isabelle Riva:Neill Blomkamp	Come see how Oats Studios modified their traditional VFX pipeline to create the breakthrough real-time shorts ADAM Chapter 2 & 3 using Photogrammetry, Alembic, and the Unity real-time engine.	Oats studios VFX workflow for real-time production with photogrammetry, alembic, and unity	NA:NA:NA:NA	2018
Jean-Colas Prunier:Armelle Bauer:Yvain Raeymaekers:Stephane Tayeb	PocketStudio is designed to allow filmmakers to easily create, play, and stream 3D animation sequences in real time using real-time collaborative editing, a unified workflow, and other real-time technologies, such as augmented reality.	The power of real-time collaborative filmmaking	NA:NA:NA:NA	2018
Gavin Moran:Mohen Leo	Epic Games, Nvidia, and ILMxLAB would like to present 2018's GDC demo, "Reflections," set in the "Star Wars" universe. In addition, we will record a character performance live using virtual production/virtual reality directly into Unreal Engine Sequencer, and then play the demo with real-time ray tracing live at 24fps.	The 'reflections' ray-tracing demo presented in real time and captured live using virtual production techniques	NA:NA	2018
Francesco Giordana:Veselin Efremov:Gael Sourimant:Silvia Rasheva:Natasha Tatarchuk:Callum James	We demonstrate a Unity-powered virtual production platform that pushes the boundaries of real-time technologies to empower filmmakers with full multi-user collaboration and live manipulation of whole environments and characters. Special attention is dedicated to high-quality real-time graphics, as evidenced by Unity's "Book of the Dead."	Virtual production in 'book of the dead': technicolor's genesis platform, powered by unity	NA:NA:NA:NA:NA:NA	2018
Ayaka Ebisu:Satoshi Hashizume:Yoichi Ochiai	A sense of rhythm is essential for playing instruments. However, many beginners learning how to play musical instruments have difficulty with rhythm. We have proposed "Stimulated Percussions," which is a musical instrument performance system using electrical muscle stimulation (EMS) in the past. In this study, we apply it to the learning of rhythm. By the movement of muscles stimulated using EMS, users are able to acquire what kind of arms and legs to move at what timing. In addition to small percussion instruments such as castanets, users can play the rhythm patterns of drums that the require the simultaneous movement of their limbs.	Building a feedback loop between electrical stimulation and percussion learning	NA:NA:NA	2018
Matthew Griffin:Lizabeth Arum	Since its release, "The Design Engine" has been played by groups of students, teachers, and individuals looking to spark self-guided training. "The Design Engine" is a direct response to educators' requests for better classroom tools surrounding inspiration and 3D printing. By prompting participants to create their own original, imaginative works-instead of using pre-selected examples-teachers can keep their students better motivated through the process of mastering desktop 3D printing. We are hosting a brand new SIGGRAPH-edition of "The Design Engine," a constantly evolving series of challenges hosted within the Studio. Participants of all backgrounds can join for a short startup round, or stick around to design and develop their projects using the tools available in the SIGGRAPH Studio Workshop.	Design engine community project: generate quick adhoc inventions to explore at SIGGRAPH and in the studio	NA:NA	2018
Kengo Tanaka:Kohei Ogawa:Tatsuya Minagawa:Yoichi Ochiai	In this study, We propose a method to develop a spring glass dip pen by using a 3D printer and reproduce different types of writing feeling. There have been several studies on different types of pens to change the feel of writing. For example, EV-Pen [Wang et al. 2016] and haptics pens [Lee et al. 2004] changes the feel of pen writing with using vibration. However, our proposed method does not reproduce tactile sensation of softness by using vibrations.	Design method of digitally fabricated spring glass pen	NA:NA:NA:NA	2018
Quentin Galvane:I-Sheng Lin:Marc Christie:Tsai-Yen Li	Creatives in animated and real movie productions have been exploring new modalities to visually design filmic sequences before realizing them in studios, through techniques like hand-drawn storyboards, physical mockups or more recently virtual 3D environments. A central issue in using virtual 3D environments is the complexity of content creation tools for non technical film creatives. To overcome this issue, we present One Man Movie, a VR authoring system which enables the crafting of filmic sequences with no prior knowledge in 3D animation. The system is designed to reflect the traditional creative process in film pre-production through stages like (i) scene layout (ii) animation of characters, (iii) placement and control of cameras and (iv) montage of the filmic sequence, while enabling a fully novel and seamless back-and-forth between all stages of the process thanks to real-time engines. This research tool has been designed and evaluated with students and experts from film schools, and should therefore raise a significant interest among Siggraph participants.	Immersive previz: VR authoring for film previsualisation	NA:NA:NA:NA	2018
Robin Mange:Kepa Iturrioz Zabala	NA	IMVERSE livemaker: create a 3D model from a single 2D photo inside VR	NA:NA	2018
Brittany Factura:Laura LaPerche:Phil Reyneri:Brett Jones:Kevin Karsch	Projected augmented reality, also called projection mapping or video mapping, is a form of augmented reality that uses projected light to directly augment 3D surfaces, as opposed to using pass-through screens or headsets. The value of projected AR is its ability to add a layer of digital content directly onto physical objects or environments in a way that can be instantaneously viewed by multiple people, unencumbered by a screen or additional setup.	Lightform: procedural effects for projected AR	NA:NA:NA:NA:NA	2018
Alexandra Ion:Patrick Baudisch	In our hands-on demonstration, we show several objects, the functionality of which is defined by the objects' internal micro-structure. Such metamaterial machines can (1) be mechanisms based on their microstructures, (2) employ simple mechanical computation, or (3) change their outside to interact with their environment. They are 3D printed from one piece and we support their creating by providing interactive software tools.	Metamaterial devices	NA:NA	2018
Wataru Date:Yasuaki Kakehi	In this research, we propose a system which makes paper through additive manufacturing process by using a dispenser mounted on XY plotter. By using our system, graphic designers can design and output paper itself which is hard in an existing paper production process. This time, we designed and implemented a machine for fabricating paper and created several output examples. In SIGGRAPH, we will provide a workshop for participants to design their original paper using our machines.	Paperprinting: a machine for prototyping paper and its applications for graphic design	NA:NA	2018
Kevin Watters:Fernando Ramallo	Raymarching signed distance fields is a technique used by graphics experts and demoscene enthusiasts to construct scenes with features unusual in traditional polygonal workflows-blending shapes, kaleidoscopic patterns, reflections, and infinite fractal detail all become possible and are represented in compact representations that live mostly on the graphics card. Until now these scenes have had to be constructed in shaders by hand, but the Raymarching Toolkit for Unity is an extension that combines Unity's highly visual scene editor with the power of raymarched visuals by automatically generating the raymarching shader for the scene an artist is creating, live.	Raymarching toolkit for unity: a highly interactive unity toolkit for constructing signed distance fields visually	NA:NA	2018
